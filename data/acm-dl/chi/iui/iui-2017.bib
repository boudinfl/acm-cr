@inproceedings{10.1145/3025171.3026367,
author = {Zhai, Shumin},
title = {Modern Touchscreen Keyboards as Intelligent User Interfaces: A Research Review},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3026367},
doi = {10.1145/3025171.3026367},
abstract = {Essential to mobile communication, the touchscreen keyboard is the most ubiquitous intelligent user interface on modern mobile phones. Developing smarter, more efficient, easy to learn, and fun to use keyboards has presented many fascinating IUI research and design questions. Some have been addressed by academic research and practitioners in industry, while others remain significant ongoing research challenges. In this IUI 2017 keynote address I will review and synthesize the progress and open research questions of the past 15 years in text input, focusing on those my co-authors and I have directly dealt with through publications, such as the cost-benefit equations of automation and prediction [9], the power of machine/statistical intelligence [4, 7, 12], the human performance models fundamental to the design of error-correction algorithms [1, 2, 8], spatial scaling from a phone to a watch and the implications on human-machine labor division [5], user behavior and learning innovation [7, 11, 12, 13], and the challenges of evaluating the longitudinal effects of personalization and adaptation [4]. Through this research program review, I will illustrate why intelligent user interfaces, or the combination of machine intelligence and human factors, holds the future of human-computer interaction, and information technology at large.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {1–2},
numpages = {2},
keywords = {gesture interface, intelligent user interface, language modelling, machine intelligence, prediction, shape writing, touch screen keyboard, text input},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025189,
author = {Kunkel, Johannes and Loepp, Benedikt and Ziegler, J\"{u}rgen},
title = {A 3D Item Space Visualization for Presenting and Manipulating User Preferences in Collaborative Filtering},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025189},
doi = {10.1145/3025171.3025189},
abstract = {While conventional Recommender Systems perform well in automatically generating personalized suggestions, it is often difficult for users to understand why certain items are recommended and which parts of the item space are covered by the recommendations. Also, the available means to influence the process of generating results are usually very limited. To alleviate these problems, we suggest a 3D map-based visualization of the entire item space in which we position and present sample items along with recommendations. The map is produced by mapping latent factors obtained from Collaborative Filtering data onto a 2D surface through Multidimensional Scaling. Then, areas that contain items relevant with respect to the current user's preferences are shown as elevations on the map, areas of low interest as valleys. In addition to the presentation of his or her preferences, the user may interactively manipulate the underlying profile by raising or lowering parts of the landscape, also at cold-start. Each change may lead to an immediate update of the recommendations. Using a demonstrator, we conducted a user study that, among others, yielded promising results regarding the usefulness of our approach.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {3–15},
numpages = {13},
keywords = {3D visualizations, interactive recommending, user experience, recommender systems, user profiles, user interfaces, matrix factorization},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025173,
author = {Chen, Li and Wang, Feng},
title = {Explaining Recommendations Based on Feature Sentiments in Product Reviews},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025173},
doi = {10.1145/3025171.3025173},
abstract = {The explanation interface has been recognized important in recommender systems as it can help users evaluate recommendations in a more informed way for deciding which ones are relevant to their interests. In different decision environments, the specific aim of explanation can be different. In high-investment product domains (e.g., digital cameras, laptops) for which users usually attempt to avoid financial risk, how to support users to construct stable preferences and make better decisions is particularly crucial. In this paper, we propose a novel explanation interface that emphasizes explaining the tradeoff properties within a set of recommendations in terms of both their static specifications and feature sentiments extracted from product reviews. The objective is to assist users in more effectively exploring and understanding product space, and being able to better formulate their preferences for products by learning from other customers' experiences. Through two user studies (in form of both before-after and within-subjects experiments), we empirically identify the practical role of feature sentiments in combination with static specifications in producing tradeoff-oriented explanations. Specifically, we find that our explanation interface can be more effective to increase users' product knowledge, preference certainty, perceived information usefulness, recommendation transparency and quality, and purchase intention.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {17–28},
numpages = {12},
keywords = {sentiment analysis, product reviews, recommender systems, user study, explanation interfaces},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025215,
author = {Zheng, Yong},
title = {Criteria Chains: A Novel Multi-Criteria Recommendation Approach},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025215},
doi = {10.1145/3025171.3025215},
abstract = {Recommender systems (RSs) have been successfully applied to alleviate the problem of information overload and assist users' decision makings. Multi-criteria recommender systems is one of the RSs which utilizes users' multiple ratings on different aspects of the items (i.e., multi-criteria ratings) to predict user preferences. Traditional approaches usually predict ratings on each criterion individually and aggregate them together to estimate the user preferences. In this paper, we propose an approach named as "Criteria Chains", where each combination of the criteria can be utilized in a way of contextual situations in order to better predict the multi-criteria ratings. Our experimental results based on the TripAdvisor and YahooMovies rating data sets demonstrate that our proposed approach is able to improve the performance of multi-criteria item recommendations.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {29–33},
numpages = {5},
keywords = {decision making, multi-criteria, context-awareness, recommender systems},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025185,
author = {Maccatrozzo, Valentina and Terstall, Manon and Aroyo, Lora and Schreiber, Guus},
title = {SIRUP: Serendipity In Recommendations via User Perceptions},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025185},
doi = {10.1145/3025171.3025185},
abstract = {In this paper, we propose a model to operationalise serendipity in content-based recommender systems. The model, called SIRUP, is inspired by the Silvia's curiosity theory, based on the fundamental theory of Berlyne, aims at (1) measuring the novelty of an item with respect to the user profile, and (2) assessing whether the user is able to manage such level of novelty (coping potential). The novelty of items is calculated with cosine similarities between items, using Linked Open Data paths. The coping potential of users is estimated by measuring the diversity of the items in the user profile. We deployed and evaluated the SIRUP model in a use case with TV recommender using BBC programs dataset. Results show that the SIRUP model allows us to identify serendipitous recommendations, and, at the same time, to have 71% precision.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {35–44},
numpages = {10},
keywords = {recommender system, entertainment, design methods, serendipity, television/video, personalization, user and cognitive models, qualitative methods, user studies, curiosity},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025211,
author = {Yuan, Fajie and Guo, Guibing and Jose, Joemon M. and Chen, Long and Yu, Haitao and Zhang, Weinan},
title = {BoostFM: Boosted Factorization Machines for Top-N Feature-Based Recommendation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025211},
doi = {10.1145/3025171.3025211},
abstract = {Feature-based matrix factorization techniques such as Factorization Machines (FM) have been proven to achieve impressive accuracy for the rating prediction task. However, most common recommendation scenarios are formulated as a top-N item ranking problem with implicit feedback (e.g., clicks, purchases)rather than explicit ratings. To address this problem, with both implicit feedback and feature information, we propose a feature-based collaborative boosting recommender called BoostFM, which integrates boosting into factorization models during the process of item ranking. Specifically, BoostFM is an adaptive boosting framework that linearly combines multiple homogeneous component recommenders, which are repeatedly constructed on the basis of the individual FM model by a re-weighting scheme. Two ways are proposed to efficiently train the component recommenders from the perspectives of both pairwise and listwise Learning-to-Rank (L2R). The properties of our proposed method are empirically studied on three real-world datasets. The experimental results show that BoostFM outperforms a number of state-of-the-art approaches for top-N recommendation.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {45–54},
numpages = {10},
keywords = {factorization machines, listwise, feature-based, top-n ranking, boostfm, pairwise},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025228,
author = {Kotowick, Kyle and Shah, Julie},
title = {Intelligent Sensory Modality Selection for Electronic Supportive Devices},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025228},
doi = {10.1145/3025171.3025228},
abstract = {Humans operating in stressful environments, such as in military or emergency first-responder roles, are subject to high sensory input loads and must often switch their attention between different modalities. Conventional supportive devices that assist users in such situations typically provide information using a single, static sensory modality; however, this carries the risk of overload when the modalities for the primary task and the supportive device overlap. Effective feedback modality selection is essential in order to avoid such a risk. One potential method for accomplishing this is to intelligently select the supportive device's feedback modality based on the user's environment and given task; however, this may result in delayed or lost information due to the performance cost resulting from switching attention from one modality to another. This paper describes the design and results of a human-participant study designed to evaluate the benefits and risks of various intelligent modality-selection strategies. Our findings suggest complex interactions between strategies, sensory input load levels and feedback modalities, with numerous significant effects across many different performance metrics.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {55–66},
numpages = {12},
keywords = {sensory modality, sensory overload, switchingcost, multimodal, intelligent selection},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025186,
author = {Pham, Phuong and Wang, Jingtao},
title = {Understanding Emotional Responses to Mobile Video Advertisements via Physiological Signal Sensing and Facial Expression Analysis},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025186},
doi = {10.1145/3025171.3025186},
abstract = {Understanding a target audience's emotional responses to video advertisements is crucial to stakeholders. However, traditional methods for collecting such information are slow, expensive, and coarse-grained. We propose AttentiveVideo, an intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising. AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to predict viewers' attention, engagement, and sentiment when watching video advertisements on unmodified smartphones. In a 24-participant study, we found that AttentiveVideo achieved good accuracies on a wide range of emotional measures (the best average accuracy = 73.59%, kappa = 0.46 across 9 metrics). We also found that the PPG sensing channel and the FEA technique are complimentary. While FEA works better for strong emotions (e.g., joy and anger), the PPG channel is more informative for subtle responses or emotions. These findings show the potential for both low-cost collection and deep understanding of emotional responses to mobile video advertisements.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {67–78},
numpages = {12},
keywords = {affective computing, heart rate, physiological signal, computational advertisement, mobile interfaces},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025220,
author = {Chao, Beibei and Zhao, Xiaoyan and Shi, Dapeng and Feng, Guihuan and Luo, Bin},
title = {Eyes Understand the Sketch! Gaze-Aided Stroke Grouping of Hand-Drawn Flowcharts},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025220},
doi = {10.1145/3025171.3025220},
abstract = {Stroke grouping in sketch recognition is both difficult and time-consuming. Our preliminary experiment indicates that, when people drawing flowcharts, their gaze focused on non-arrow areas, which providing a spatial cue for stroke grouping. Therefore, we present a novel stroke grouping method aided by gaze information. Based on gaze data that is collected simultaneously during natural drawing process, we generate hotspot areas serving as the position reference of semantic symbols. Strokes are first roughly grouped by the hotspot areas, so as to efficiently decrease the searching space. Experiment on a dataset of 54 flowcharts shows that time efficiency of stroke grouping can be greatly improved in our method and there is much potential for introducing eye-gaze data in sketch recognition.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {79–83},
numpages = {5},
keywords = {multimodal interaction, stroke grouping, sketch recognition, eye-hand coordination pattern},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025214,
author = {Wang, Chao and Terken, Jacques and Hu, Jun},
title = {CarNote: Reducing Misunderstanding between Drivers by Digital Augmentation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025214},
doi = {10.1145/3025171.3025214},
abstract = {The road environment can be seen as a social situation: Drivers need to coordinate with each other to share the infrastructure. In addition to the driving behaviour itself, lights, horn and speed are the most frequently used means to exchange information, limiting both the range and the bandwidth of the connectivity and leading to misunderstanding and conflict. With everywhere available connectivity and the broad penetration of social network services, the relationship between drivers on the road may gain more transparency, enabling social information to pass through the steel shell of the cars and giving opportunities to reduce misunderstanding and strengthen empathy. In this study, we present "CarNote", a concept that aims to reduce misunderstanding and conflict between drivers by showing their emergency driving status to others. This concept was prototyped and evaluated with users in a driving simulator. The results showed that CarNote enhances drivers' empathy, increases forgiveness and decreases anger to others on the road.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {85–94},
numpages = {10},
keywords = {social computing., driving violations, connected car, computer-mediated communication},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025196,
author = {Pereira Santos, Carlos and Hutchinson, Kevin and Khan, Vassilis-Javed and Markopoulos, Panos},
title = {Measuring Self-Esteem with Games},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025196},
doi = {10.1145/3025171.3025196},
abstract = {Self-esteem is a personality trait utilized to support the diagnosis of several psychological conditions. With this study we investigate the potential that computer games can have in assessing self-esteem. To that end, we designed and developed a platformer game and analyzed how in-game behavior relates to Rosenberg's Self-Esteem Scale. We examined: i) how a player's self-esteem influences game performance, ii) how a player's self-esteem generally influences in-game behavior iii) the possible game mechanics that assist in inferring a player's self-esteem. The study was conducted in two phases (N=98 and N=85). Results indicate that self-esteem does not have any impact on the player's performance, on the other hand, we found that players' self-evaluation of game performance correlates with their self-esteem.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {95–105},
numpages = {11},
keywords = {player modeling, games with a purpose, self-esteem, player profiling, games user research, game design},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025174,
author = {Lefort, S\'{e}bastien and Zibetti, Elisabetta and Lesot, Marie-Jeanne and Detyniecki, Marcin and Tijus, Charles},
title = {Dimensions for Automatic Interpretation of Approximate Numerical Expressions: An Empirical Study},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025174},
doi = {10.1145/3025171.3025174},
abstract = {Imprecise numerical expressions, such as "about 100 meters", are pervasive in natural language. Mobile robotics, Geographic Information Systems, intelligent personal assistants as well as database querying applications are required to automatically and accurately interpret such expressions, called Approximate Numerical Expressions (ANE). The main challenge is to determine their numerical boundaries that sound plausible to users. The aim of this paper is to provide guidelines to interpret ANEs that are independent from the domain and the formal representations. We identified three arithmetical properties and examined their involvement in ANE interpretation as intervals of denoted values. The implicit assumption of symmetry of the intervals was also tested. To do so, 146 participants were asked to provide the intervals corresponding to 24 ANEs in a semantically neutral context. Results suggest that the properties of ANEs we identified are key factors in their interpretation while symmetry is not always maintained. This study contributes towards an understanding of how users process ANEs and its results can be used to improve intelligent interfaces that lead to better users' satisfaction and natural interaction between him/her and the system.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {107–117},
numpages = {11},
keywords = {imprecision, about, database querying, approximate numerical expressions},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025230,
author = {Gao, Mingkun and Do, Hyo Jin and Fu, Wai-Tat},
title = {An Intelligent Interface for Organizing Online Opinions on Controversial Topics},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025230},
doi = {10.1145/3025171.3025230},
abstract = {An enormous amount of posts and comments are shared in online social forums, which often organize these online social opinions based on semantic contents. However, for controversial topics, people with different attitudes and stances often have very distinct perspectives, reactions, and emotions to the same post. Organization by semantic contents often encourages selective exposure to information, which may exacerbate opinion polarization. To address this problem, we design a novel interface that allows people to better understand and appreciate people with different stances in social forums. Our interface was developed to allow interactive visualization and categorization of original posts about a controversial topic with crowd workers' reactions and emotions from different stances. We evaluated the interface using Reddit posts about US presidential candidates. Results demonstrate that the interface can mitigate selective exposure and help users to adopt a broader spectrum of opinions than the traditional Reddit interface.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {119–123},
numpages = {5},
keywords = {selective exposure, social opinion, interface design},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025225,
author = {Wauck, Helen and Xiao, Ziang and Chiu, Po-Tsung and Fu, Wai-Tat},
title = {Untangling the Relationship Between Spatial Skills, Game Features, and Gender in a Video Game},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025225},
doi = {10.1145/3025171.3025225},
abstract = {Certain commercial video games, such as Portal 2 and Tetris, have been empirically shown to train spatial reasoning skills, a subset of cognitive skills essential for success in STEM disciplines. However, no research to date has attempted to understand which specific features in these games tap into players' spatial ability or how individual player differences interact with these game features. This knowledge is crucially important as a first step towards understanding what makes these games effective and why, especially for subpopulations with lower spatial ability such as women and girls. We present the first empirical study analyzing the relationship between spatial ability, specific game features, and individual player differences using a custom-built computer game. Twenty children took a pretest of spatial skills and then played our game for 2 hours. We found that spatial ability pretest scores predicted several player behaviors related to in-game tasks involving 3D object construction and first person navigation. However, when analyzed by gender, girls' pretest scores were much less predictive of player behavior.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {125–136},
numpages = {12},
keywords = {stem, video games, player behavior, spatial reasoning, children, education, game features, gender},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025224,
author = {Wauck, Helen and Fu, Wai-Tat},
title = {A Data-Driven, Multidimensional Approach to Hint Design in Video Games},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025224},
doi = {10.1145/3025171.3025224},
abstract = {Hint systems are designed to adjust a video game's difficulty to suit the individual player, but too often they are designed without analyzing player behavior and lack intelligence and adaptability, resulting in hints that are at best ineffective and at worst hurt player experience. We present an alternative approach to hint design focusing on player experience rather than performance. We had 25 participants play a difficult spatial puzzle game and collected player behavior, demographics, and self-reported player experience measures. We found that more exploratory behavior improved player experience, so we designed three types of hints encouraging this behavior: adaptive, automatic, and on-demand. We found that certain players found hints more helpful regardless of whether the hints changed their behavior, and players seemed to prefer seeing fewer hints than the adaptive and automatic conditions gave them. Our findings contribute a deeper empirical understanding of hint design strategies and their effect on player behavior and experience, with practical recommendations for designers of interactive systems.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {137–147},
numpages = {11},
keywords = {game design, hint design, hint systems, hints, player behavior, player experience, video games},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025222,
author = {Peltonen, Jaakko and Strahl, Jonathan and Flor\'{e}en, Patrik},
title = {Negative Relevance Feedback for Exploratory Search with Visual Interactive Intent Modeling},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025222},
doi = {10.1145/3025171.3025222},
abstract = {In difficult information seeking tasks, the majority of top-ranked documents for an initial query may be non-relevant, and negative relevance feedback may then help find relevant documents. Traditional negative relevance feedback has been studied on document results; we introduce a system and interface for negative feedback in a novel exploratory search setting, where continuous-valued feedback is directly given to keyword features of an inferred probabilistic user intent model. The introduced system allows both positive and negative feedback directly on an interactive visual interface, by letting the user manipulate keywords on an optimized visualization of modeled user intent. Feedback on the interactive intent model lets the user direct the search: Relevance of keywords is estimated from feedback by Bayesian inference, influence of feedback is increased by a novel propagation step, documents are retrieved by likelihoods of relevant versus non-relevant intents, and the most relevant keywords (having the highest upper confidence bounds of relevance) and the most non-relevant ones (having the smallest lower confidence bounds of relevance) are shown as options for further feedback. We carry out task-based information seeking experiments with real users on difficult real tasks; we compare the system to the nearest state of the art baseline allowing positive feedback only, and show negative feedback significantly improves the quality of retrieved information and user satisfaction for difficult tasks.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {149–159},
numpages = {11},
keywords = {interactive exploratory search, user intent model, difficult queries, novelty in information retrieval, negative relevance feedback, query intent, presentation of retrieval results, search interfaces, query reformulation, language model},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025210,
author = {Hoque, Enamul and Joty, Shafiq and Marquez, Luis and Carenini, Giuseppe},
title = {CQAVis: Visual Text Analytics for Community Question Answering},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025210},
doi = {10.1145/3025171.3025210},
abstract = {Community question answering (CQA) forums can provide effective means for sharing information and addressing a user's information needs about particular topics. However, many such online forums are not moderated, resulting in many low quality and redundant comments, which makes it very challenging for users to find the appropriate answers to their questions. In this paper, we apply a user-centered design approach to develop a system, CQAVis, which supports users in identifying high quality comments and get their questions answered. Informed by the user's requirements, the system combines both text analytics and interactive visualization techniques together in a synergistic way. Given a new question posed by the user, the text analytic module automatically finds relevant answers by exploring existing related questions and the comments within their threads. Then the visualization module presents the search results to the user and supports the exploration of related comments. We have evaluated the system in the wild by deploying it within a CQA forum among thousands of real users. Through the online study, we gained deeper insights about the potential utility of the system, as well as learned generalizable lessons for designing visual text analytics systems for the domain of CQA forums.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {161–172},
numpages = {12},
keywords = {asynchronous conversation, computer-mediated communication, text visualization, community question answering},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025183,
author = {Hirosue, Kauzki and Ukawa, Shohei and Itoh, Yuichi and Onoye, Takao and Hashimoto, Masanori},
title = {GPGPU-Based Highly Parallelized 3D Node Localization for Real-Time 3D Model Reproduction},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025183},
doi = {10.1145/3025171.3025183},
abstract = {This paper proposes a highly parallelized 3D node localization method based on cross-entropy method for the 3D modeling system. Cross-entropy localization statistically estimates node positions from node-to-node distance information by sampling, and each sample evaluation and internal computation of objective function can be processed in parallel. Experimental results show our GPGPU-based implementation achieved 5,163x and 61.5x speed up compared to a single processor and 80-processor implementations. In addition, for enhancing model reproduction accuracy, this work introduces a penalty function to mitigate flip ambiguity.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {173–178},
numpages = {6},
keywords = {cross-entropy method, node localization, wireless sensor network, parallel computing, 3D modeling},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025233,
author = {Sen, Shilad and Swoap, Anja Beth and Li, Qisheng and Boatman, Brooke and Dippenaar, Ilse and Gold, Rebecca and Ngo, Monica and Pujol, Sarah and Jackson, Bret and Hecht, Brent},
title = {Cartograph: Unlocking Spatial Visualization Through Semantic Enhancement},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025233},
doi = {10.1145/3025171.3025233},
abstract = {This paper introduces Cartograph, a visualization system that harnesses the vast amount of world knowledge encoded within Wikipedia to create thematic maps of almost any data. Cartograph extends previous systems that visualize non-spatial data using geographic approaches. While these systems required data with an existing semantic structure, Cartograph unlocks spatial visualization for a much larger variety of datasets by enhancing input datasets with semantic information extracted from Wikipedia. Cartograph's map embeddings use neural networks trained on Wikipedia article content and user navigation behavior. Using these embeddings, the system can reveal connections between points that are unrelated in the original data sets, but are related in meaning and therefore embedded close together on the map. We describe the design of the system and key challenges we encountered, and we present findings from an exploratory user study},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {179–190},
numpages = {12},
keywords = {wikidata, wikipedia, semantic relatedness, thematic cartography, neural networks, maps},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3026366,
author = {Samaras, George},
title = {Utilizing Human Cognitive and Emotional Factors for User-Centered Computing},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3026366},
doi = {10.1145/3025171.3026366},
abstract = {Intelligent interactive systems should not ignore the individuality of the user. The "one-size-fits-all" approach, especially in user interaction, is not appropriate when user satisfaction and acceptability is a primary goal. Each user has unique human cognitive processing styles and abilities. In addition, emotions change over time, which possibly affect the user's cognitive state and the overall interaction process. Unsurprisingly, the users' ability to control their emotions is another essential factor in adapting user interfaces, applications and data delivery. How can an interactive system adapt to human cognitive and emotional factors with the aim to deliver a personalized and more usable interface? Is there a user interface to an application or system that is equally effective to all types of users? How can we place the human in the center of every day's interaction and task activity? This keynote speech will present some approaches that our work at the DMAC Lab/SCRAT Group has addressed on how individual differences in human cognitive processing and emotional factors place the user in the center of every day interaction.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {191–192},
numpages = {2},
keywords = {personalization, adaptation, human emotions, human cognitive factors, wearable sensors, individual differences},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025202,
author = {Nayyar, Aanand and Dwivedi, Utkarsh and Ahuja, Karan and Rajput, Nitendra and Nagar, Seema and Dey, Kuntal},
title = {OptiDwell: Intelligent Adjustment of Dwell Click Time},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025202},
doi = {10.1145/3025171.3025202},
abstract = {Gaze based navigation with digital screens offer a hands-free and touchless interaction, which is often useful in providing a hygienic interaction experience in a public kiosk scenario. The goodness of such a navigation system depends not only on the accuracy of detecting the eye gaze but also on the ability to determine whether a user is interested in clicking a button or is just looking at the button. The time for which a user needs to gaze at a particular button before it is considered as a click action is called the dwell time. In this paper, we explore intelligent adjustment of dwell times, where mouse click events on the buttons of a given application are emulated with user gaze. A constant dwell-time for all buttons and for all users may not provide an efficient and intuitive interface. We thereby propose a model to dynamically adjust dwell-time values used to emulate user mouse click events, exploiting the user's experience with different portions of a given application. The adjustment happens at a per-user, per-button granularity, as a function of the user's (a) prior usage experience of the given button within the application and (b) Midas touch characteristics for the given button. We propose OptiDwell, inspired by the action-value method based solutions to the Multi-Armed Bandits problem, for dwell click time adaptation. We experiment OptiDwell using an interactive TV channel browsing interface application, constituting of a mix of text and image buttons, over 10 computer-savvy users generating over 9000 click tasks. We observe significant improvement of user comfort level over the sessions, quantified by (a) improved (reduced) dwell times and (b) reduced number of Midas touches in spite of faster dwell-clicks, as high as 10-fold reduction in the best case. Our work is useful for creating an interface, with accurate, fast and comfortable dwell-clicks for each interface element (e.g., buttons), and each user.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {193–204},
numpages = {12},
keywords = {adaptive dwell time, gaze tracking, dwell click, midas touch},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025199,
author = {Mioch, Tina and Kroon, Liselotte and Neerincx, Mark A.},
title = {Driver Readiness Model for Regulating the Transfer from Automation to Human Control},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025199},
doi = {10.1145/3025171.3025199},
abstract = {In the collaborative driving scenario of truck platooning, the first car is driven by its chauffeur and the next cars follow automatically via a so-called 'virtual tow-bar'. The chauffeurs of the following cars do not drive 'in the towbar mode', but need to be able to take back control in foreseen emph{and} unforeseen conditions. It is crucial that this transfer of control only takes place when the chauffeur is ready for it. This paper presents a Driver Readiness (DR) ontological model that specifies the core factors, with their relationships, of a chauffeur's current and near-future readiness for taking back the control of driving. A first model was derived from a literature study and an analysis of truck driving data, which was refined subsequently based on an expert review. This DR model distinguishes (a) current and required states for the physical (hand, feet, head, and seating position) and mental readiness (attention and situation awareness), (b) agents (human and machine actor), (c) policies for agent behaviors, and (d) states of the vehicle and its environment. It provides the knowledge base of a Control Transfer Support (CTS) agent that assesses the current and predicted chauffeur state and guides the transition of control in an adaptive and personalized manner. The DR model will be fed by information from the network and in-car sensors. The behaviors of the CTS agent will be generated and constrained by the instantiated policies, providing an important step towards a safe transfer of control from automation to human driver.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {205–213},
numpages = {9},
keywords = {collaborative driving, ontology, knowledge representation, transfer of control, driver readiness, reusability},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025226,
author = {Rajaonarivo, Landy and Courgeon, Matthieu and Maisel, Eric and De Loor, Pierre},
title = {Inline Co-Evolution between Users and Information Presentation for Data Exploration},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025226},
doi = {10.1145/3025171.3025226},
abstract = {This paper presents an intelligent user interface model dedicated to the exploration of complex databases. This model is implemented on a 3D metaphor: a virtual museum. In this metaphor, the database elements are embodied as museum objects. The objects are grouped in rooms according to their semantic properties and relationships and the rooms organization forms the museum. Rooms? organization is not predefined but defined incrementally by taking into account not only the relationships between objects, but also the user's centers of interest. The latter are evaluated in real-time through user interactions within the virtual museum. This interface allows for a personal reading and favors the discovery of unsuspected links between data. In this paper, we present our model's formalization as well as its application to the context of cultural heritage.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {215–219},
numpages = {5},
keywords = {user interface, cultural heritage, database exploration, visual metaphor, real-time adaptation},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025187,
author = {Toker, Dereck and Lall\'{e}, S\'{e}bastien and Conati, Cristina},
title = {Pupillometry and Head Distance to the Screen to Predict Skill Acquisition During Information Visualization Tasks},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025187},
doi = {10.1145/3025171.3025187},
abstract = {In this paper we investigate using a variety of behavioral measures collectible with an eye tracker to predict a user's skill acquisition phase while performing various information visualization tasks with bar graphs. Our long term goal is to use this information in real-time to create user-adaptive visualizations that can provide personalized support to facilitate visualization processing based on the user's predicted skill level. We show that leveraging two additional content-independent data sources, namely information on a user's pupil dilation and head distance to the screen, yields a significant improvement for predictive accuracies of skill acquisition compared to predictions made using content-dependent information related to user eye gaze attention patterns, as was done in previous work. We show that including features from both pupil dilation and head distance to the screen improve the ability to predict users' skill acquisition state, beating both the baseline and a model using only content-dependent gaze information.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {221–231},
numpages = {11},
keywords = {classification, information visualization, user modeling, skill acquisition, eye tracking, pupil dilation, distance to the screen},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025176,
author = {Intharah, Thanapong and Turmukhambetov, Daniyar and Brostow, Gabriel J.},
title = {Help, It Looks Confusing: GUI Task Automation Through Demonstration and Follow-up Questions},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025176},
doi = {10.1145/3025171.3025176},
abstract = {Non-programming users should be able to create their own customized scripts to perform computer-based tasks for them, just by demonstrating to the machine how it's done. To that end, we develop a system prototype which learns-by-demonstration called HILC (Help, It Looks Confusing). Users train HILC to synthesize a task script by demonstrating the task, which produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions. We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. While pure programming-by-demonstration is still unrealistic, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system. Our models of events and appearance are surprisingly simple, but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slides, struggled with the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple executables. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {233–243},
numpages = {11},
keywords = {action segmentation and recognition, GUI automation, programming by demonstration},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025190,
author = {Chen, Chun-Fu (Richard) and Pistoia, Marco and Shi, Conglei and Girolami, Paolo and Ligman, Joseph W. and Wang, Yong},
title = {UI X-Ray: Interactive Mobile UI Testing Based on Computer Vision},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025190},
doi = {10.1145/3025171.3025190},
abstract = {User Interface/eXperience (UI/UX) significantly affects the lifetime of any software program, particularly mobile apps. A bad UX can undermine the success of a mobile app even if that app enables sophisticated capabilities. A good UX, however, needs to be supported of a highly functional and user friendly UI design. In spite of the importance of building mobile apps based on solid UI designs, UI discrepancies---inconsistencies between UI design and implementation---are among the most numerous and expensive defects encountered during testing. This paper presents UI X-Ray, an interactive UI testing system that integrates computer-vision methods to facilitate the correction of UI discrepancies---such as inconsistent positions, sizes and colors of objects and fonts. Using UI X-Ray does not require any programming experience; therefore, UI X-Ray can be used even by non-programmers---particularly designers---which significantly reduces the overhead involved in writing tests. With the feature of interactive interface, UI testers can quickly generate defect reports and revision instructions---which would otherwise be done manually. We verified our UI X-Ray on 4 developed mobile apps of which the entire development history was saved. UI X-Ray achieved a 99.03% true-positive rate, which significantly surpassed the 20.92% true-positive rate obtained via manual analysis. Furthermore, evaluating the results of our automated analysis can be completed quickly (&lt; 1 minute per view on average) compared to hours of manual work required by UI testers. On the other hand, UI X-Ray received the appreciations from skilled designers and UI X-Ray improves their current work flow to generate UI defect reports and revision instructions. The proposed system, UI X-Ray, presented in this paper has recently become part of a commercial product.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {245–255},
numpages = {11},
keywords = {software engineering, interactive interface, user interface testing},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025203,
author = {Lee, Tak Yeon and Dugan, Casey and Bederson, Benjamin B.},
title = {Towards Understanding Human Mistakes of Programming by Example: An Online User Study},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025203},
doi = {10.1145/3025171.3025203},
abstract = {Programming-by-Example (PBE) enables users to create programs without writing a line of code. However, there is little research on people's ability to accomplish complex tasks by providing examples, which is the key to successful PBE solutions. This paper presents an online user study, which reports observations on how well people decompose complex tasks, and disambiguate sub-tasks. Our findings suggest that disambiguation and decomposition are difficult for inexperienced users. We identify seven types of mistakes made, and suggest new opportunities for actionable feedback based on unsuccessful examples, with design implications for future PBE systems.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {257–261},
numpages = {5},
keywords = {user study, programming-by-example},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025184,
author = {Dev, Himel and Liu, Zhicheng},
title = {Identifying Frequent User Tasks from Application Logs},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025184},
doi = {10.1145/3025171.3025184},
abstract = {In the light of continuous growth in log analytics, application logs remain a valuable source to understand and analyze patterns in user behavior. Today, almost every major software company employs analysts to reveal user insights from log data. To understand the tasks and challenges of the analysts, we conducted a background study with a group of analysts from a major software company. A fundamental analytics objective that we recognized through this study involves identifying frequent user tasks from application logs. More specifically, analysts are interested in identifying operation groups that represent meaningful tasks performed by many users inside applications. This is challenging, primarily because of the nature of modern application logs, which are long, noisy and consist of events from high-cardinality set. In this paper, we address these challenges to design a novel frequent pattern ranking technique that extracts frequent user tasks from application logs. Our experimental study shows that our proposed technique significantly outperforms state of the art for real-world data.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {263–273},
numpages = {11},
keywords = {application log, frequent pattern mining, pattern ranking, user task},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025206,
author = {Li, Jingyi and Zhou, Michelle X. and Yang, Huahai and Mark, Gloria},
title = {Confiding in and Listening to Virtual Agents: The Effect of Personality},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025206},
doi = {10.1145/3025171.3025206},
abstract = {We present an intelligent virtual interviewer that engages with a user in a text-based conversation and automatically infers the user's psychological traits, such as personality. We investigate how the personality of a virtual interviewer influences a user's behavior from two perspectives: the user's willingness to confide in, and listen to, a virtual interviewer. We have developed two virtual interviewers with distinct personalities and deployed them in a real-world recruiting event. We present findings from completed interviews with 316 actual job applicants. Notably, users are more willing to confide in and listen to a virtual interviewer with a serious, assertive personality. Moreover, users' personality traits, inferred from their chat text, influence their perception of a virtual interviewer, and their willingness to confide in and listen to a virtual interviewer. Finally, we discuss the implications of our work on building hyper- personalized, intelligent agents based on user traits.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {275–286},
numpages = {12},
keywords = {virtual interviewer, personality analytics, individual differences, computer personality, chatbot, human-machine trust},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025209,
author = {Berkovsky, Shlomo and Taib, Ronnie and Conway, Dan},
title = {How to Recommend? User Trust Factors in Movie Recommender Systems},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025209},
doi = {10.1145/3025171.3025209},
abstract = {How much trust a user places in a recommender is crucial to the uptake of the recommendations. Although prior work established various factors that build and sustain user trust, their comparative impact has not been studied in depth. This paper presents the results of a crowdsourced study examining the impact of various recommendation interfaces and content selection strategies on user trust. It evaluates the subjective ranking of nine key factors of trust grouped into three dimensions and examines the differences observed with respect to users' personality traits.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {287–300},
numpages = {14},
keywords = {recommender systems, user-system trust, presentation of recommendations, user study},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025192,
author = {Gajos, Krzysztof Z. and Chauncey, Krysta},
title = {The Influence of Personality Traits and Cognitive Load on the Use of Adaptive User Interfaces},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025192},
doi = {10.1145/3025171.3025192},
abstract = {One of the problems adaptive interfaces must solve is the issue of stability---users must be able to complete a familiar task reliably. Split Adaptive Interfaces, where a limited part of the screen contains copies of the interface elements predicted to be of immediate use, are one technique for resolving this difficulty. While prior work demonstrated that Split Adaptive Interfaces improve performance on average, the results of our study demonstrate systematic individual differences in the utilization of the adaptive features, which correlate with the stable user traits of Need for Cognition and Extraversion. Specifically, higher Need for Cognition (a willingness to undertake difficult mental activities) is correlated with increased utilization rates, while higher Extraversion (a general orientation towards seeking gratification from the external world) is negatively correlated with utilization rates. Our results also demonstrate a significant negative correlation between cognitive load induced by a secondary task and the utilization of the adaptive features. This effect, however, is very small (less than two percentage points). Together, these results provide additional evidence of the usefulness of the split adaptive interface approach and a negligible effect of additional cognitive load, but also demonstrate that the approach does not benefit all users equally.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {301–306},
numpages = {6},
keywords = {adaptive user interfaces, cognitive load, extraversion, need for cognition},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025219,
author = {Yu, Kun and Berkovsky, Shlomo and Taib, Ronnie and Conway, Dan and Zhou, Jianlong and Chen, Fang},
title = {User Trust Dynamics: An Investigation Driven by Differences in System Performance},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025219},
doi = {10.1145/3025171.3025219},
abstract = {Trust is a key factor affecting the way people rely on automated systems. On the other hand, system performance has comprehensive implications on a user's trust variations. This paper examines systems of varied levels of accuracy, in order to reveal the relationship between system performance, a user's trust and reliance on the system. In particular, it is identified that system failures have a stronger effect on trust than system successes. We also describe how patterns of trust change according to a number of consecutive system failures or successes. Importantly, we show that increasing user familiarity with the system decreases the rate of trust change, which provides new insights on the development of user trust. Finally, our analysis established a correlation between a user's reliance on a system and their trust level. Combining all these findings can have important implications in general system design and implementation, by predicting how trust builds and when it stabilizes, as well as allowing for indirectly reading a user's trust in real time based on system reliance.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {307–317},
numpages = {11},
keywords = {acquisition and extinction, trust dynamics, temporal examination, system performance, reliance},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025198,
author = {H\"{a}uslschmid, Renate and von B\"{u}low, Max and Pfleging, Bastian and Butz, Andreas},
title = {SupportingTrust in Autonomous Driving},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025198},
doi = {10.1145/3025171.3025198},
abstract = {Autonomous cars will likely hit the market soon, but trust into such a technology is one of the big discussion points in the public debate. Drivers who have always been in complete control of their car are expected to willingly hand over control and blindly trust a technology that could kill them. We argue that trust in autonomous driving can be increased by means of a driver interface that visualizes the car's interpretation of the current situation and its corresponding actions. To verify this, we compared different visualizations in a user study, overlaid to a driving scene: (1) a chauffeur avatar, (2) a world in miniature, and (3) a display of the car's indicators as the baseline. The world in miniature visualization increased trust the most. The human-like chauffeur avatar can also increase trust, however, we did not find a significant difference between the chauffeur and the baseline.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {319–329},
numpages = {11},
keywords = {avatar, trustworthiness, antropomorphism, automated driving, trust},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025195,
author = {Mihoub, Alaeddine and Lefebvre, Gr\'{e}goire},
title = {Social Intelligence Modeling Using Wearable Devices},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025195},
doi = {10.1145/3025171.3025195},
abstract = {Social Signal Processing techniques have given the opportunity to analyze in-depth human behavior in social face-to-face interactions. With recent advancements, it is henceforth possible to use these techniques to augment social interactions, especially the human behavior in oral presentations. The goal of this paper is to train a computational model able to provide a relevant feedback to a public speaker concerning his coverbal communication. Hence, the role of this model is to augment the social intelligence of the orator and then the relevance of his presentation. To this end, we present an original interaction setting in which the speaker is equipped with only wearable devices. Several coverbal modalities have been extracted and automatically annotated namely speech volume, intonation, speech rate, eye gaze, hand gestures and body movements. An offline report was addressed to participants containing the performance scores on the overall modalities. In addition, a post-experiment study was conducted to collect participant's opinions on many aspects of the studied interaction and the results were rather positive. Moreover, we annotated recommended feedbacks for each presentation session, and to retrieve these annotations, a Dynamic Bayesian Network model was trained using as inputs the multimodal performance scores. We will show that our assessment behavior model presents good performances compared to other models.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {331–341},
numpages = {11},
keywords = {social intelligence, feedback, DBN, multimodal behavior assessment, oral presentation, wearable devices},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025191,
author = {Oraby, Shereen and Gundecha, Pritam and Mahmud, Jalal and Bhuiyan, Mansurul and Akkiraju, Rama},
title = {"How May I Help You?": Modeling Twitter Customer ServiceConversations Using Fine-Grained Dialogue Acts},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025191},
doi = {10.1145/3025171.3025191},
abstract = {Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understand trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained "dialogue acts" frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real-time. We characterize differences between customer and agent behavior in Twitter customer service conversations, and investigate the effect of testing our system on different customer service industries. Finally, we use a data-driven approach to predict important conversation outcomes: customer satisfaction, customer frustration, and overall problem resolution. We show that the type and location of certain dialogue acts in a conversation have a significant effect on the probability of desirable and undesirable outcomes, and present actionable rules based on our findings. The patterns and rules we derive can be used as guidelines for outcome-driven automated customer service platforms.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {343–355},
numpages = {13},
keywords = {dialogue, conversation modeling, customer service, twitter},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025221,
author = {Kumar, Abhishek and Srivastava, Kushal and Yadav, Kuldeep and Deshmukh, Om},
title = {Multi-Faceted Index Driven Navigation for Educational Videos in Mobile Phones},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025221},
doi = {10.1145/3025171.3025221},
abstract = {One of the challenges that is holding back wide spread consumption of educational videos on mobile devices is the lack of mobile interfaces which can provide efficient video navigation capabilities. In this paper, we utilize multi-modal data analysis techniques which include analysis of the spoken content and the written content of the video, to create a multi-faceted index. We present a novel and first-of-its-kind mobile interface which uses aforementioned multi-faceted index to provide intuitive, usable, and efficient way to navigate through a video. The efficacy of the proposed multi-faceted index driven mobile interface for non-linear navigation is demonstrated through a preliminary user study of 15 participants. We demonstrate that the proposed interface leads to statistically significant savings in navigation time as compared to that of a baseline interface used by leading e-learning providers.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {357–361},
numpages = {5},
keywords = {video navigation, moocs, mobile system, mobile interface},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025204,
author = {Fan, Xiangmin and Luo, Wencan and Menekse, Muhsin and Litman, Diane and Wang, Jingtao},
title = {Scaling Reflection Prompts in Large Classrooms via Mobile Interfaces and Natural Language Processing},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025204},
doi = {10.1145/3025171.3025204},
abstract = {We present the iterative design, prototype, and evaluation of CourseMIRROR (Mobile In-situ Reflections and Review with Optimized Rubrics), an intelligent mobile learning system that uses natural language processing (NLP) techniques to enhance instructor-student interactions in large classrooms. CourseMIRROR enables streamlined and scaffolded reflection prompts by: 1) reminding and collecting students' in-situ written reflections after each lecture; 2) continuously monitoring the quality of a student's reflection at composition time and generating helpful feedback to scaffold reflection writing; and 3) summarizing the reflections and presenting the most significant ones to both instructors and students. Through a combination of a 60-participant lab study and eight semester-long deployments involving 317 students, we found that the reflection and feedback cycle enabled by CourseMIRROR is beneficial to both instructors and students. Furthermore, the reflection quality feedback feature can encourage students to compose more specific and higher-quality reflections, and the algorithms in CourseMIRROR are both robust to cold start and scalable to STEM courses in diverse topics.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {363–374},
numpages = {12},
keywords = {natural language processing, mobile learning, collaborative learning, reflection prompts},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3026365,
author = {Markopoulos, Panos},
title = {Interaction Design for Rehabiliation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3026365},
doi = {10.1145/3025171.3026365},
abstract = {Well-known trends pertaining to the aging of population and the rising costs of healthcare motivate the development of rehabilitation technology. There is a considerable body of work in this area including efforts to make serious games, virtual reality and robotic applications. While innovative technologies have been introduced over the years, and often researchers produce promising experimental results, these technologies have not yet delivered the anticipated benefits. The causes for this apparent failure are evident when looking a closer look at the case of stroke rehabilitation, which is one of the heaviest researched topics for developing rehabilitation technologies. It is argued that improvements should be sought by centering the design on an understanding of patient needs, allowing patients, therapists and care givers in general to personalize solutions to the need of patients, effective feedback and motivation strategies to be implemented, and an in depth understanding of the socio-technical system in which the rehabilitation technology will be embedded. These are classic challenges that human computer interaction (HCI) researchers have been dealing with for years, which is why the field of rehabilitation technology requires considerable input from HCI researchers, and which explains the growing number of relevant HCI publications pertaining to rehabilitation. The talk reviews related research carried out at the Eindhoven University of Technology together with collaborating institutes, which has examined the value of tangible user interfaces and embodied interaction in rehabilitation, how designing playful interactions or games with a functional purpose., feedback design. I shall discuss the work we have done to develop rehabilitation technologies for the TagTrrainer system in the doctoral research of Daniel Tetteroo [2,3,4] and the explorations on wearable solutions in the doctoral research of Wang Qi.[5,6]. With our research being design driven and explorative, I will discuss also the current state of the art for the field and the challenges that need to be addressed for human computer interaction research to make a larger impact in the domain of rehabilitation technology.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {375–376},
numpages = {2},
keywords = {motor learning, motivation, feedback, rehabilitation, behavior change},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025201,
author = {di Sciascio, Cecilia and Strohmaier, David and Errecalde, Marcelo and Veas, Eduardo},
title = {WikiLyzer: Interactive Information Quality Assessment in Wikipedia},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025201},
doi = {10.1145/3025171.3025201},
abstract = {Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success, but also a hindrance to good quality: contributions can be of poor quality because anyone, even anonymous users, can participate. Though Wikipedia has defined guidelines as to what makes the perfect article, authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. However, little has been done to support quality assessment of user-generated content through interactive tools that combine automatic methods and human intelligence. We developed WikiLyzer, a Web toolkit comprising three interactive applications designed to assist (i) knowledge discovery experts in creating and testing metrics for quality measurement, (ii) Wikipedia users searching for good articles, and (iii) Wikipedia authors that need to identify weaknesses to improve a particular article. A design study sheds a light on how experts could create complex quality metrics with our tool, while a user study reports on its usefulness to identify high-quality content.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {377–388},
numpages = {12},
keywords = {text analytics, visual analytics, wikipedia, user-generated content, text quality},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025175,
author = {Pons, Patricia and Jaen, Javier and Catala, Alejandro},
title = {Towards Future Interactive Intelligent Systems for Animals: Study and Recognition of Embodied Interactions},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025175},
doi = {10.1145/3025171.3025175},
abstract = {User-centered design applied to non-human animals is showing to be a promising research line known as Animal Computer Interaction (ACI), aimed at improving animals' wellbeing using technology. Within this research line, intelligent systems for animal entertainment could have remarkable benefits for their mental and physical wellbeing, while providing new ways of communication and amusement between humans and animals. In order to create user-centered interactive intelligent systems for animals, we first need to understand how they spontaneously interact with technology, and develop suitable mechanisms to adapt to the animals' observed interactions and preferences. Therefore, this paper describes a pioneer study on cats' preferences and behaviors with different technological devices. It also presents the design and evaluation of a promising depth-based tracking system for the detection of cats' body parts and postures. The contributions of this work lay foundations towards providing a framework for the development of future intelligent systems for animal entertainment.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {389–400},
numpages = {12},
keywords = {animal-computer interaction, entertainment, behavior recognition, intelligent environment, interactive system, depth tracking},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025188,
author = {Peters, Rifca and Broekens, Joost and Neerincx, Mark A.},
title = {Guidelines for Tree-Based Collaborative Goal Setting},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025188},
doi = {10.1145/3025171.3025188},
abstract = {Educational technology needs a model of learning goals to support motivation, learning gain, tailoring of the learning process, and sharing of the personal goals between different types of users (i.e., learner and educator) and the system. This paper proposes a tree-based learning goal structuring to facilitate personal goal setting to shape and monitor the learning process. We developed a goal ontology and created a user interface representing this knowledge-base for the self-management education for children with Type 1 Diabetes Mellitus. Subsequently, a co-operative evaluation was conducted with healthcare professionals to refine and validate the ontology and its representation. Presentation of a concrete prototype proved to support professionals' contribution to the design process. The resulting tree-based goal structure enables three important tasks: ability assessment, goal setting and progress monitoring. Visualization should be clarified by icon placement and clustering of goals with the same difficulty and topic. Bloom's taxonomy for learning objectives should be applied to improve completeness and clarity of goal content.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {401–405},
numpages = {5},
keywords = {visualization, personalization, healthcare, diabetes, education, collaboration, learning goal -setting -attainment, knowledge-base},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025229,
author = {Ashok, Vikas and Puzis, Yury and Borodin, Yevgen and Ramakrishnan, I.V.},
title = {Web Screen Reading Automation Assistance Using Semantic Abstraction},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025229},
doi = {10.1145/3025171.3025229},
abstract = {A screen reader's sequential press-and-listen interface makes for an unsatisfactory and often times painful web-browsing experience for blind people. To help alleviate this situation, we introduce Web Screen Reading Automation Assistant (SRAA) for automating users' screen-reading actions (e.g., finding price of an item) on demand, thereby letting them focus on what they want to do rather than on how to get it done. The key idea is to elevate the interaction from operating on (syntactic) HTML elements, as is done now, to operating on web entities (which are semantically meaningful collections of related HTML elements, e.g., search results, menus, widgets, etc.). SRAA realizes this idea of semantic abstraction by constructing a Web Entity Model (WEM), which is a collection of web entities of the underlying webpage, using an extensive generic library of custom-designed descriptions of commonly occurring web entities across websites. The WEM brings blind users closer to how sighted people perceive and operate on web entities, and together with a natural-language user interface, SRAA relieves users from having to press numerous shortcuts to operate on low-level HTML elements - the principal source of tedium and frustration. This paper describes the design and implementation of SRAA. Evaluation with 18 blind subjects demonstrates its usability and effectiveness.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {407–418},
numpages = {12},
keywords = {blindness, blind users, screen-reader, assistant, automation, accessibility, natural interfaces, web browsing},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025179,
author = {Thomason, John and Ratsamee, Photchara and Kiyokawa, Kiyoshi and Kriangkomol, Pakpoom and Orlosky, Jason and Mashita, Tomohiro and Uranishi, Yuki and Takemura, Haruo},
title = {Adaptive View Management for Drone Teleoperation in Complex 3D Structures},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025179},
doi = {10.1145/3025171.3025179},
abstract = {Drone navigation in complex environments poses many problems to teleoperators. Especially in 3D structures like buildings or tunnels, viewpoints are often limited to the drone's current camera view, nearby objects can be collision hazards, and frequent occlusion can hinder accurate manipulation. To address these issues, we have developed a novel interface for teleoperation that provides a user with environment-adaptive viewpoints that are automatically configured to improve safety and smooth user operation. This real-time adaptive viewpoint system takes robot position, orientation, and 3D pointcloud information into account to modify user-viewpoint to maximize visibility. Our prototype uses simultaneous localization and mapping (SLAM) based reconstruction with an omnidirectional camera and we use resulting models as well as simulations in a series of preliminary experiments testing navigation of various structures. Results suggest that automatic viewpoint generation can outperform first and third-person view interfaces for virtual teleoperators in terms of ease of control and accuracy of robot operation.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {419–426},
numpages = {8},
keywords = {drone, navigation, adaptive view, virtual reality},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025182,
author = {Bin Hannan, Nabil and Tearo, Khalid and Malloch, Joseph and Reilly, Derek},
title = {Once More, With Feeling: Expressing Emotional Intensity in Touchscreen Gestures},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025182},
doi = {10.1145/3025171.3025182},
abstract = {In this paper, we explore how people use touchscreens to express emotional intensity, and whether these intensities can be understood by oneself at a later date or by others. In a controlled study, 26 participants were asked to express a set of emotions mapped to predefined gestures, at range of different intensities. One week later, participants were asked to identify the emotional intensity visualized in animations of the gestures made by themselves and by other participants. Our participants expressed emotional intensity using gesture length, pressure, and speed primarily; the choice of attributes was impacted by the specific emotion, and the range and rate of increase of these attributes varied by individual and by emotion. Recognition accuracy of emotional intensity was higher at extreme ends, and was higher for one's own gestures than those made by others. The attributes of size and pressure (mapped to color in the animation) were most readily interpreted, while speed was more difficult to differentiate. We discuss human gesture drawing patterns to express emotional intensities and implications for developers of annotation systems and other touchscreen interfaces that wish to capture affect.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {427–437},
numpages = {11},
keywords = {emotional intensity, touch gestures},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025234,
author = {Katsuragawa, Keiko and Kamal, Ankit and Lank, Edward},
title = {Effect of Motion-Gesture Recognizer Error Pattern on User Workload and Behavior},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025234},
doi = {10.1145/3025171.3025234},
abstract = {Bi-level thresholding is a motion gesture recognition technique that mediates between false positives, and false negatives by using two threshold levels: a tighter threshold that limits false positives and recognition errors, and a looser threshold that prevents repeated errors (false negatives) by analyzing movements in sequence. In this paper, we examine the effects of bi-level thresholding on the workload and acceptance of end-users. Using a wizard-of-Oz recognizer, we hold recognition rates constant and adjust for fixed versus bi-level thresholding. Given identical recognition rates, we show that systems using bi-level thresholding result in significant lower workload scores on the NASA-TLX and accelerometer variance. Overall, these results argue for the viability of bi-level thresholding as an effective technique for balancing between false positives, recognition errors and false negatives.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {439–449},
numpages = {11},
keywords = {recognition, gesture, usability testing and evaluation, interaction design, thresholding, handheld devices and mobile computing},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025217,
author = {Su Yin, Myat and Haddawy, Peter and Suebnukarn, Siriwan and Schultheis, Holger and Rhienmora, Phattanapon},
title = {Use of Haptic Feedback to Train Correct Application of Force in Endodontic Surgery},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025217},
doi = {10.1145/3025171.3025217},
abstract = {With the minute margins of error in endodontic surgery, training in manual dexterity and proper instrument handling are crucial components in the dental curriculum. Important parameters include tool path, tool angulation, and force applied. In this work, we focus on training of correct application of force. This is particularly challenging since the amounts of force used are on the order of tenths of Newtons, requiring a highly refined tactile sense and incorrect force can cause irreversible damage. Too great a force can cause overdrilling or in extreme cases perforation of the tooth. Too small a force can cause thermal irritation possibly resulting in tissue necrosis. Despite the importance of correct use of force, this is the dimension on which students receive the least tutorial feedback since force information is typically not available in traditional training settings. In this paper, we present an approach to using haptic feedback as a means to convey formative feedback on the correct application of force. Feedback is conveyed to the student graphically and the correct amount of force to apply is trained haptically. The simulator is rewound and the student is asked to redo the stage where the error occurred. Preliminary evaluation against a control group of students who received only feedback concerning outcome shows the feedback mechanism to be effective.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {451–455},
numpages = {5},
keywords = {intelligent tutorings, virtual reality, haptic feedback, surgical simulation, formative feedback},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025216,
author = {Paudyal, Prajwal and Lee, Junghyo and Banerjee, Ayan and Gupta, Sandeep K.S.},
title = {DyFAV: Dynamic Feature Selection and Voting for Real-Time Recognition of Fingerspelled Alphabet Using Wearables},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025216},
doi = {10.1145/3025171.3025216},
abstract = {Recent research has shown that reliable recognition of sign language words and phrases using user-friendly and non-invasive armbands is feasible and desirable. This work provides an analysis and implementation of including fingerspelling recognition (FR) in such systems, which is a much harder problem due to lack of distinctive hand movements. A novel algorithm called DyFAV (Dynamic Feature Selection and Voting) is proposed for this purpose that exploits the fact that fingerspelling has a finite corpus (26 letters for ASL). The system uses an independent multiple agent voting approach to identify letters with high accuracy. The independent voting of the agents ensures that the algorithm is highly parallelizable and thus recognition times can be kept low to suit real-time mobile applications. The results are demonstrated on the entire ASL alphabet corpus for nine people with limited training and average recognition accuracy of 95.36% is achieved which is better than the state-of-art for armband sensors. The mobile, non-invasive, and real time nature of the technology is demonstrated by evaluating performance on various types of Android phones and remote server configurations.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {457–467},
numpages = {11},
keywords = {assistive technology, wearable and pervasive computing, sign language processing, gesture-based interfaces},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025212,
author = {Beltran, Juan Felipe and Huang, Ziqi and Abouzied, Azza and Nandi, Arnab},
title = {Don't Just Swipe Left, Tell Me Why: Enhancing Gesture-Based Feedback with Reason Bins},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025212},
doi = {10.1145/3025171.3025212},
abstract = {Despite several advances in information retrieval systems and user interfaces, the specification of queries over text-based document collections remains a challenging problem. Query specification with keywords is a popular solution. However, given the widespread adoption of gesture-driven interfaces such as multitouch technologies in smartphones and tablets, the lack of a physical keyboard makes query specification with keywords inconvenient. We present BinGO, a novel gestural approach to querying text databases that allows users to refine their queries using a swipe gesture to either "like" or "dislike" candidate documents as well as express the reasons they like or dislike a document by swiping through automatically generated "reason bins". Such reasons refine a user's query with additional keywords. We present an online and efficient bin generation algorithm that presents reason bins at gesture articulation. We motivate and describe BinGo's unique interface design choices. Based on our analysis and user studies, we demonstrate that query specification by swiping through reason bins is easy and expressive.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {469–480},
numpages = {12},
keywords = {reason bins, interactive search refinement, gesture-based feedback},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025177,
author = {Jones, Paul and Sharma, Shivani and Moon, Changsung and Samatova, Nagiza F.},
title = {A Network-Fusion Guided Dashboard Interface for Task-Centric Document Curation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025177},
doi = {10.1145/3025171.3025177},
abstract = {Knowledge workers are being exposed to more information than ever before, as well as having to work in multi-tasking and collaborative environments. There is an increasing need for interfaces and algorithms to help automatically keep track of documents that are associated with both individual and team tasks. Previous approaches to the problem of automatically applying task labels to documents have been limited to small feature spaces or have not taken into account multi-user environments. Many different clues to potential task associations are available through user, task and document similarity metrics, as well as through temporal patterns in individual and team workflows. We present a network-fusion algorithm for automatic task-centric document curation, and show how this can guide a recent-work dashboard interface, which organizes user's documents and gathers feedback from them. Our approach efficiently computes representations of users, tasks and documents in a common vector space, and can easily take into account many different types of associations through the creation of edges in a multi-layer graph. We have demonstrated the effectiveness of this approach using labelled document corpora from three empirical studies with students and intelligence analysts. We have also shown how to leverage relationships between different entity types to increase classification accuracy by up to 20% over a simpler baseline, and with as little as 10% labelled data.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {481–491},
numpages = {11},
keywords = {vector embedding, interface, deepwalk, network-fusion, workflow, knowledge workers, information recall, tasks, dashboard, multi-layer graph, instrumentation},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025227,
author = {Dhamdhere, Kedar and McCurley, Kevin S. and Nahmias, Ralfi and Sundararajan, Mukund and Yan, Qiqi},
title = {Analyza: Exploring Data with Conversation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025227},
doi = {10.1145/3025171.3025227},
abstract = {We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The first is a question-and-answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza's ability to democratize access to data. We discuss the key design decisions in implementing this system. For instance, how to mix structured and natural language modalities, how to use conversation to disambiguate and simplify querying, how to rely on the ``semantics' of the data to compensate for the lack of syntactic structure, and how to efficiently curate the data.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {493–504},
numpages = {12},
keywords = {exploratory data analysis, natural language},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025232,
author = {Alsulmi, Mohammad R. and Carterette, Benjamin A.},
title = {Learning to Rate Clinical Concepts Using Simulated Clinician Feedback},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025232},
doi = {10.1145/3025171.3025232},
abstract = {We present a user-based model for rating concepts (i.e., words and phrases) in clinical queries based on their relevance to clinical decision making. Our approach can be adopted by information retrieval systems (e.g., search engines) to identify the most important concepts in user queries in order to better understand user intent and to improve search results. In our experiments, we examine several learning algorithms and show that by using simulated user feedback, our approach can predict the ratings of the clinical concepts in newly unseen queries with high prediction accuracy.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {505–509},
numpages = {5},
keywords = {user simulated modeling, clinical concept rating, clinical decision support (cds)},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025218,
author = {Wasserman Pritsker, Evgenia and Kuflik, Tsvi and Minkov, Einat},
title = {Assessing the Contribution of Twitter's Textual Information to Graph-Based Recommendation},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025218},
doi = {10.1145/3025171.3025218},
abstract = {Graph-based recommendation approaches can model associations between users and items alongside additional contextual information. Recent studies demonstrated that representing features extracted from social media (SM) auxiliary data, like friendships, jointly with traditional users/items ratings in the graph, contribute to recommendation accuracy. In this work, we take a step further and propose an extended graph representation that includes socio-demographic and personal traits extracted from the content posted by the user on SM. Empirical results demonstrate that processing unstructured textual information collected from Twitter and representing it in structured form in the graph improves recommendation performance, especially in cold start conditions.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {511–516},
numpages = {6},
keywords = {social media, twitter, graph-based recommendation, ppr, information extraction},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025178,
author = {Su'a, Tavita and Licorish, Sherlock A. and Savarimuthu, Bastin Tony Roy and Langlotz, Tobias},
title = {QuickReview: A Novel Data-Driven Mobile User Interface for Reporting Problematic App Features},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025178},
doi = {10.1145/3025171.3025178},
abstract = {User-reviews of mobile applications provide information that benefits other users and developers. Even though reviews contain feedback about an app's performance and problematic features, users and app developers need to spend considerable effort reading and analyzing the feedback provided. In this work, we introduce and evaluate QuickReview, an intelligent user interface for reporting problematic app features. Preliminary user evaluations show that QuickReview facilitates users to add reviews swiftly with ease, and also helps developers with quick interpretation of submitted reviews by presenting a ranked list of commonly reported features.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {517–522},
numpages = {6},
keywords = {mobile devices, app reviews, data driven, user interface, intelligent user interfaces, android},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025208,
author = {Sun, Yunjia and Lank, Edward and Terry, Michael},
title = {Label-and-Learn: Visualizing the Likelihood of Machine Learning Classifier's Success During Data Labeling},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025208},
doi = {10.1145/3025171.3025208},
abstract = {While machine learning is a powerful tool for the analysis and classification of complex real-world datasets, it is still challenging, particularly for developers with limited expertise, to incorporate this technology into their software systems. The first step in machine learning, data labeling, is traditionally thought of as a tedious, unavoidable task in building a machine learning classifier. However, in this paper, we argue that it can also serve as the first opportunity for developers to gain insight into their dataset. Through a Label-and-Learn interface, we explore visualization strategies that leverage the data labeling task to enhance developers' knowledge about their dataset, including the likely success of the classifier and the rationale behind the classifier's decisions. At the same time, we show that the visualizations also improve users' labeling experience by showing them the impact they have made on classifier performance. We assess the visualizations in Label-and-Learn and experimentally demonstrate their value to software developers who seek to assess the utility of machine learning during the data labeling process.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {523–534},
numpages = {12},
keywords = {machine learning, data labeling, visualization},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025172,
author = {Jiang, Biye and Canny, John},
title = {Interactive Machine Learning via a GPU-Accelerated Toolkit},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025172},
doi = {10.1145/3025171.3025172},
abstract = {Machine learning is growing in importance in industry, sciences, and many other fields. In many and perhaps most of these applications, users need to trade off competing goals. Machine learning, however, has evolved around the optimization of a single, usually narrowly-defined criterion. In most cases, an expert makes (or should be making) trade-offs between these criteria which requires high-level (human) intelligence. With interactive customization and optimization the expert can incorporate secondary criteria into the model-generation process in an interactive way. In this paper we develop the techniques to perform customized and interactive model optimization, and demonstrate the approach on several examples. The keys to our approach are (i) a machine learning architecture which is modular and supports primary and secondary loss functions, while users can directly manipulate its parameters during training (ii) high-performance training so that non-trivial models can be trained in real-time (using roofline design and GPU hardware), and (iii) highly-interactive visualization tools that support dynamic creation of visualizations and controls to match various optimization criteria.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {535–546},
numpages = {12},
keywords = {hyper-parameters tuning, machine learning, multiple objective optimization, GPU, interactive},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025181,
author = {Micallef, Luana and Sundin, Iiris and Marttinen, Pekka and Ammad-ud-din, Muhammad and Peltola, Tomi and Soare, Marta and Jacucci, Giulio and Kaski, Samuel},
title = {Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025181},
doi = {10.1145/3025171.3025181},
abstract = {Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert's knowledge of the relevance of different features for a prediction task. In particular, based on the expert's earlier input, the user model guides the selection of the features on which to elicit user's knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {547–552},
numpages = {6},
keywords = {interactive knowledge elicitation, prediction, user model},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025231,
author = {Kim, Bongjun and Pardo, Bryan},
title = {I-SED: An Interactive Sound Event Detector},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025231},
doi = {10.1145/3025171.3025231},
abstract = {Tagging of sound events is essential in many research areas. However, finding sound events and labeling them within a long audio file is tedious and time-consuming. Building an automatic recognition system using machine learning techniques is often not feasible because it requires a large number of human-labeled training examples and fine tuning the model for a specific application. Fully automated labeling is also not reliable enough for all uses. We present I-SED, an interactive sound detection interface using a human-in-the-loop approach that lets a user reduce the time required to label audio that is tediously long (e.g. 20 hours) to do manually and has too few prior labeled examples (e.g. one) to train a state-of-the-art machine audio labeling system. We performed a human-subject study to validate its effectiveness and the results showed that our tool helped participants label all target sound events within a recording twice as fast as labeling them manually.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {553–557},
numpages = {5},
keywords = {sound event detection, interactive machine learning, human-in-the-loop system},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025194,
author = {Watanabe, Kento and Matsubayashi, Yuichiroh and Inui, Kentaro and Nakano, Tomoyasu and Fukayama, Satoru and Goto, Masataka},
title = {LyriSys: An Interactive Support System for Writing Lyrics Based on Topic Transition},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025194},
doi = {10.1145/3025171.3025194},
abstract = {This paper presents LyriSys, a novel lyric-writing support system. Previous systems for lyric writing can fully automatically only generate a single line of lyrics that satisfies given constraints on accent and syllable patterns or an entire lyric. In contrast to such systems, LyriSys allows users to create and revise their work incrementally in a trial-and-error manner. Through fine-grained interactions with the system, the user can create the specifications of the musical structure and the story of the lyrics in terms of the verse-bridge-chorus structure, the number of lines, words and syllables, and most importantly, the transition over semantic topics such as "scene", "dark" and "sweet love". This paper provides an overview of the design of the system and its user interface and describes how the writing process is guided by a state-of-the-art probabilistic generative topic model that is trained without supervision. The system works for both Japanese and English.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {559–563},
numpages = {5},
keywords = {semantic, song lyrics, computational creativity, linguistic creativity, topic transition},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025193,
author = {Gil, Yolanda and Garijo, Daniel},
title = {Towards Automating Data Narratives},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025193},
doi = {10.1145/3025171.3025193},
abstract = {We propose a new area of research on automating data narratives. Data narratives are containers of information about computationally generated research findings. They have three major components: 1) A record of events, that describe a new result through a workflow and/or provenance of all the computations executed; 2) Persistent entries for key entities involved for data, software versions, and workflows; 3) A set of narrative accounts that are automatically generated human-consumable renderings of the record and entities and can be included in a paper. Different narrative accounts can be used for different audiences with different content and details, based on the level of interest or expertise of the reader. Data narratives can make science more transparent and reproducible, because they ensure that the text description of the computational experiment reflects with high fidelity what was actually done. Data narratives can be incorporated in papers, either in the methods section or as supplementary materials. We introduce DANA, a prototype that illustrates how to generate data narratives automatically, and describe the information it uses from the computational records. We also present a formative evaluation of our approach and discuss potential uses of automated data narratives.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {565–576},
numpages = {12},
keywords = {computational workflows, reproducibility, explanation, semantic workflows, data narratives, wings},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025213,
author = {Gattupalli, Srujana and Ebert, Dylan and Papakostas, Michalis and Makedon, Fillia and Athitsos, Vassilis},
title = {CogniLearn: A Deep Learning-Based Interface for Cognitive Behavior Assessment},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025213},
doi = {10.1145/3025171.3025213},
abstract = {This paper proposes a novel system for assessing physical exercises specifically designed for cognitive behavior monitoring. The proposed system provides decision support to experts for helping with early childhood development. Our work is based on the well-established framework of Head-Toes-Knees-Shoulders (HTKS) that is known for its sufficient psychometric properties and its ability to assess cognitive dysfunctions. HTKS serves as a useful measure for behavioral self-regulation. Our system, CogniLearn, automates capturing and motion analysis of users performing the HTKS game and provides detailed evaluations using state-of-the-art computer vision and deep learning based techniques for activity recognition and evaluation. The proposed system is supported by an intuitive and specifically designed user interface that can help human experts to cross-validate and/or refine their diagnosis. To evaluate our system, we created a novel dataset, that we made open to the public to encourage further experimentation. The dataset consists of 15 subjects performing 4 different variations of the HTKS task and contains in total more than 60,000 RGB frames, of which 4,443 are fully annotated.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {577–587},
numpages = {11},
keywords = {head-toes-knees-shoulders (htks), cognitive assessment, human computer interaction (hci), computer vision, deep learning},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025207,
author = {Soh, Harold and Sanner, Scott and White, Madeleine and Jamieson, Greg},
title = {Deep Sequential Recommendation for Personalized Adaptive User Interfaces},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025207},
doi = {10.1145/3025171.3025207},
abstract = {Adaptive user-interfaces (AUIs) can enhance the usability of complex software by providing real-time contextual adaptation and assistance. Ideally, AUIs should be personalized and versatile, i.e., able to adapt to each user who may perform a variety of complex tasks. But this is difficult to achieve with many interaction elements when data-per-user is sparse. In this paper, we propose an architecture for personalized AUIs that leverages upon developments in (1) deep learning, particularly gated recurrent units, to efficiently learn user interaction patterns, (2) collaborative filtering techniques that enable sharing of data among users, and (3) fast approximate nearest-neighbor methods in Euclidean spaces for quick UI control and/or content recommendations. Specifically, interaction histories are embedded in a learned space along with users and interaction elements; this allows the AUI to query and recommend likely next actions based on similar usage patterns across the user base. In a comparative evaluation on user-interface, web-browsing and e-learning datasets, the deep recurrent neural-network (DRNN) outperforms state-of-the-art tensor-factorization and metric embedding methods.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {589–593},
numpages = {5},
keywords = {personalization, adaptive user interface, deep learning},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025200,
author = {Gasparic, Marko and Janes, Andrea and Ricci, Francesco and Zanellati, Marco},
title = {GUI Design for IDE Command Recommendations},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025200},
doi = {10.1145/3025171.3025200},
abstract = {This paper describes a novel design of a graphical user interface (GUI) to recommend useful command within an integrated development environment. The recommendation GUI contains a description of the suggested command, an explanation why the command is recommended, and a command usage example. The proposed design is based on the analysis of relevant guidelines identified in the literature. Its perceived usability and acceptance were evaluated in a live user study with 36 software developers. Our findings, partially contradicting existing literature, indicate that the presentation of the command-the description and the example-is perceived as more useful than the explanation of the rationale for the recommendation.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {595–599},
numpages = {5},
keywords = {recommender system, integrated development environment, command, user interface},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025180,
author = {Kumar, Arun and Schrater, Paul},
title = {Novelty Learning via Collaborative Proximity Filtering},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025180},
doi = {10.1145/3025171.3025180},
abstract = {The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for spontaneous changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {601–610},
numpages = {10},
keywords = {implicit preferences, user behaviors, boredom, recommender systems, user preferences, novelty, latent tastes},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025223,
author = {Peltonen, Jaakko and Belorustceva, Kseniia and Ruotsalo, Tuukka},
title = {Topic-Relevance Map: Visualization for Improving Search Result Comprehension},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025223},
doi = {10.1145/3025171.3025223},
abstract = {We introduce topic-relevance map, an interactive search result visualization that assists rapid information comprehension across a large ranked set of results. The topic-relevance map visualizes a topical overview of the search result space as keywords with respect to two essential information retrieval measures: relevance and topical similarity. Non-linear dimensionality reduction is used to embed high-dimensional keyword representations of search result data into angles on a radial layout. Relevance of keywords is estimated by a ranking method and visualized as radiuses on the radial layout. As a result, similar keywords are modeled by nearby points, dissimilar keywords are modeled by distant points, more relevant keywords are closer to the center of the radial display, and less relevant keywords are distant from the center of the radial display. We evaluated the effect of the topic-relevance map in a search result comprehension task where 24 participants were summarizing search results and produced a conceptualization of the result space. The results show that topic-relevance map significantly improves participants' comprehension capability compared to a conventional ranked list presentation.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {611–622},
numpages = {12},
keywords = {visualization, exploratory search, sense-making, dimensionality reduction},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025205,
author = {Medlar, Alan and Pyykk\"{o}, Joel and Glowacka, Dorota},
title = {Towards Fine-Grained Adaptation of Exploration/Exploitation in Information Retrieval},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025205},
doi = {10.1145/3025171.3025205},
abstract = {Lookup and exploratory search tasks can be distinguished using individuals' information search behaviour. Previous work, however, has treated these search tasks as belonging to homogeneous categories, ignoring the specific information needs between users and even between search sessions for the same user. In this work, we avoid this dichotomy by considering each search task to exist on a spectrum between lookup and exploratory. In doing so, our approach aims to dynamically adapt exploration and exploitation in a manner commensurate with the user's individual requirements for each search session. We present a novel study design together with a regression model for predicting the optimal exploration rate based on simple metrics from the first iteration, such as clicks and reading time, that can be collected without special hardware. We perform model selection based on the data collected from a user study and show that predictions are consistent with user feedback.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {623–627},
numpages = {5},
keywords = {system optimisation, exploratory search, user modelling},
location = {Limassol, Cyprus},
series = {IUI '17}
}

@inproceedings{10.1145/3025171.3025197,
author = {Greenstein-Messica, Asnat and Rokach, Lior and Friedman, Michael},
title = {Session-Based Recommendations Using Item Embedding},
year = {2017},
isbn = {9781450343480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025171.3025197},
doi = {10.1145/3025171.3025197},
abstract = {Recent methods for learning vector space representations of words, word embedding, such as GloVe and Word2Vec have succeeded in capturing fine-grained semantic and syntactic regularities. We analyzed the effectiveness of these methods for e-commerce recommender systems by transferring the sequence of items generated by users' browsing journey in an e-commerce website into a sentence of words. We examined the prediction of fine-grained item similarity (such as item most similar to iPhone 6 64GB smart phone) and item analogy (such as iPhone 5 is to iPhone 6 as Samsung S5 is to Samsung S6) using real life users' browsing history of an online European department store. Our results reveal that such methods outperform related models such as singular value decomposition (SVD) with respect to item similarity and analogy tasks across different product categories. Furthermore, these methods produce a highly condensed item vector space representation, item embedding, with behavioral meaning sub-structure. These vectors can be used as features in a variety of recommender system applications. In particular, we used these vectors as features in a neural network based models for anonymous user recommendation based on session's first few clicks. It is found that recurrent neural network that preserves the order of user's clicks outperforms standard neural network, item-to-item similarity and SVD (recall@10 value of 42% based on first three clicks) for this task.},
booktitle = {Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {629–633},
numpages = {5},
keywords = {e-commerce, glove, item embedding, recurrent neural network, session-based recommender system, word embedding, word2vec, deep learning},
location = {Limassol, Cyprus},
series = {IUI '17}
}

