@inproceedings{10.1145/3172944.3176183,
author = {Landay, James A.},
title = {From on Body to Out of Body User Experience},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3176183},
doi = {10.1145/3172944.3176183},
abstract = {Today's most common user interfaces represent an incremental change from the GUI popularized by the Apple Macintosh in 1984. Over the last 30 years the dominant hardware has changed drastically while the user interface has barely moved: from one hand on a mouse to two fingers on a panel of glass. I will illustrate how we are building on-body interfaces of the future that further engage our bodies by using muscle sensing for input and vibrotactile output, offering discrete and natural interaction on the go. I will also show how other interfaces we are designing take an even more radical approach, moving the interface off the human body altogether and onto drones that project into the space around them. Finally, I will introduce a new project where we envision buildings as hybrid physical-digital spaces that both sense and actuate to improve human wellbeing.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {1–2},
numpages = {2},
keywords = {behavior change, smart buildings, human-drone interaction, ambient display, vibrotactile feedback, muscle sensing},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3176184,
author = {Goto, Masataka},
title = {Intelligent Music Interfaces},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3176184},
doi = {10.1145/3172944.3176184},
abstract = {Automatic music-understanding technologies (automatic analysis of music signals) make possible the creation of intelligent music interfaces that enrich music experiences and open up new ways of listening to music. In the past, it was common to listen to music in a somewhat passive manner; in the future, people will be able to enjoy music in a more active manner by using music technologies. Listening to music through active interactions is called active music listening. In this keynote speech I first introduce active music listening interfaces demonstrating how end users can benefit from music-understanding technologies based on signal processing and/or machine learning. By analyzing the music structure (chorus sections), for example, the SmartMusicKIOSK interface enables people to access their favorite part of a song directly (skipping other parts) while viewing a visual representation of the song's structure. I then introduce our recent challenge of deploying such research-level music interfaces as web services open to the public. Those services augment people's understanding of music, enable music-synchronized control of computer-graphics animation and robots, and provide various bird's-eye views on a large music collection. In the future, further advances in music-understanding technologies and music interfaces based on them will make interaction between people and music even more active and enriching.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {3–4},
numpages = {2},
keywords = {music interface, music understanding, active music listening, music analysis, web service, music information research},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3176185,
author = {Golbeck, Jennifer},
title = {Surveillance or Support? When Personalization Turns Creepy},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3176185},
doi = {10.1145/3172944.3176185},
abstract = {Personalization, recommendations, and user modeling can be powerful tools to improve people's experiences with technology and to help them find information. However, we also know that people underestimate how much of their personal information is used by our technology and they generally do not understand how much algorithms can discover about them. Both privacy and ethical technology have issues of consent at their heart. While many personalization systems assume most users would consent to the way they employ personal data, research shows this is not necessarily the case. This talk will look at how to consider issues of privacy and consent when users cannot explicitly state their preferences, The Creepy Factor, and how to balance users' concerns with the benefits personalized technology can offer.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {5},
numpages = {1},
keywords = {user preference, recommender systems, privacy, personalization},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247905,
author = {O'Donovan, John},
title = {Session Details: Session 1A: Reccomender Systems},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247905},
doi = {10.1145/3247905},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172981,
author = {Yakura, Hiromu and Nakano, Tomoyasu and Goto, Masataka},
title = {FocusMusicRecommender: A System for Recommending Music to Listen to While Working},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172981},
doi = {10.1145/3172944.3172981},
abstract = {This paper proposes FocusMusicRecommender, an automated system recommending background music to listen to while working. Recommendation systems matching user preferences have been widely researched even though research has shown that music that listeners strongly like is not suitable background music because it interferes with their concentration. FocusMusicRecommender plays songs that users may "neither like nor dislike" instead of "like very much." It is designed to by default summarize a song automatically so that users can give "like very much" feedback by pressing a "keep listening" button or "dislike very much" feedback by pressing a "skip" button. It uses this feedback, along with users» concentration levels estimated from their behavior history, to distinguish between the preference levels "like" and "like very much." It then estimates the preference levels of unplayed songs and selects the most suitable song by considering the user»s current concentration level. The effectiveness of the proposed feedback method and suitability of the recommendation results were verified experimentally and in user studies. Furthermore, it is confirmed that the proposed method can estimate the user»s concentration level more accurately than the previous methods.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {7–17},
numpages = {11},
keywords = {concentration level estimation, background music, music recommendation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172967,
author = {Brod\'{e}n, Bj\"{o}rn and Hammar, Mikael and Nilsson, Bengt J. and Paraschakis, Dimitris},
title = {Ensemble Recommendations via Thompson Sampling: An Experimental Study within e-Commerce},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172967},
doi = {10.1145/3172944.3172967},
abstract = {This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {19–29},
numpages = {11},
keywords = {streaming recommendations, Thompson sampling, e-commerce recommender systems, bandit ensembles, session-based recommendations, reinforcement learning},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173011,
author = {Derezinski, Michal and Rohanimanesh, Khashayar and Hydrie, Aamer},
title = {Discovering Surprising Documents with Context-Aware Word Representations},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173011},
doi = {10.1145/3172944.3173011},
abstract = {User experiences can be made more engaging by incorporating surprise. For example, online shoppers may like to view unique products. In this paper we propose an approach for detecting surprising documents, such as product titles. As the concept of surprise is subjective, there is currently no principled method for measuring the surprisingness score of a document. We present such a method; an unsupervised approach for automatically discovering surprising documents in an unlabeled corpus. Our approach is based on a probabilistic model of surprise, and a construction of effective distributional word embeddings, which can be adapted to the semantic context in which the word appears. As the performance of our model does not degrade with the length of the document, it is particularly well suited for very short documents (even a single sentence). We evaluate our model both in supervised and unsupervised settings, demonstrating its state-of-the-art performance on two real-world data sets: a collection of e-commerce products from eBay, and a corpus of NSF proposals. These experiments show that our surprisingness score exhibits high correlation with human annotated labels.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {31–35},
numpages = {5},
keywords = {recommender systems, product discovery, topic modeling, text surprisingness, information theory},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173001,
author = {Choi, Saemi and Aizawa, Kiyoharu and Sebe, Nicu},
title = {FontMatcher: Font Image Paring for Harmonious Digital Graphic Design},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173001},
doi = {10.1145/3172944.3173001},
abstract = {One of the important aspects in graphic design is choosing the font of the caption that matches aesthetically the associated image. To obtain a good match, users would exhaustively examine a long font list requiring them a substantial effort. This paper presents FontMatcher, which supports users to design digital graphic works harmoniously pairing fonts with an image. The system provides three features, recommendation, explaination and feedback. If a warm feeling image is given as input, the system recommends warm feeling fonts, and then explains what is the distinguishing features of the recommendation, e.g. a cursive shape. Users can also provide feedback to find fonts which correspond to their intention. Our evaluation results show that the recommended fonts scored better than selected fonts by novices and provides competing results with the ones chosen by experienced graphic designers. The system also provides explanations that help increasing the reliability of the recommended results.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {37–41},
numpages = {5},
keywords = {graphic design, matching fonts with images, font search},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247906,
author = {Mokryn, Osnat},
title = {Session Details: Session 1B: Multimodal Interfaces},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247906},
doi = {10.1145/3247906},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172977,
author = {Kapur, Arnav and Kapur, Shreyas and Maes, Pattie},
title = {AlterEgo: A Personalized Wearable Silent Speech Interface},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172977},
doi = {10.1145/3172944.3172977},
abstract = {We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user's intention to speak and internal speech is characterized by neuromuscular signals in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. We demonstrate robustness of the system through user studies and report 92% median word accuracy levels.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {43–53},
numpages = {11},
keywords = {human-machine symbiosis, peripheral nerve interface, silent speech interface, intelligence augmentation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172958,
author = {Ali, Mohammad Rafayet and Van Orden, Kimberly and Parkhurst, Kimberly and Liu, Shuyang and Nguyen, Viet-Duy and Duberstein, Paul and Hoque, M. Ehsan},
title = {Aging and Engaging: A Social Conversational Skills Training Program for Older Adults},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172958},
doi = {10.1145/3172944.3172958},
abstract = {We developed 'Aging and Engaging, a web-based intelligent interface, to improve communication skills among older adults. The interface allows users to practice conversations with a virtual assistant and receive feedback on eye contact, speaking volume, smiling, and valence of speech content. Feedback is generated automatically by analyzing the temporal properties of the conversation using the hidden Markov model. The interface was designed with the assistance of an expert advisory panel that works with geriatric patients, as well as a focus group of 12 older adults. To evaluate its effectiveness, we conducted a study with 25 older adults, each of whom participated in four conversations. Participants' response times to questions, as well as the amount of positive feedback, increased gradually through these interactions, as assessed by human judges. Participants found the feedback useful, easy to interpret, and fairly accurate, and expressed their interest in using the system at home. We plan to enroll subjects with difficulties in social communication; have them use the system over time at home in a randomized, controlled study; and measure any changes in their behavior.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {55–66},
numpages = {12},
keywords = {aging, social skills, virtual agent},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172994,
author = {Chiu, Po-Tsung and Wauck, Helen and Xiao, Ziang and Yao, Yuqi and Fu, Wai-Tat},
title = {Supporting Spatial Skill Learning with Gesture-Based Embodied Design},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172994},
doi = {10.1145/3172944.3172994},
abstract = {Prior research has shown that spatial abilities are crucial for STEM achievement and attainment. The connection between the digital and physical worlds provided by embodied interaction has been shown to enhance performance and engagement in educational contexts. Spatial reasoning is a domain that lends itself naturally to embodied, physical interaction; however, there is little understanding of how embodied interaction could be incorporated into educational technology designed to train spatial reasoning skills. We propose several guidelines for gestural interaction design in spatial reasoning education games based on an empirical study with students at a local afterschool program using a custom-built computer game for training spatial skills. We present a series of gesture sets derived from an iterative design approach that are easy for children to acquire, show sufficient congruency to specific spatial operations, and enable robust recognition from the system. We also compared children's behaviors when playing the game with our gestural interface and a traditional mouse-based interface and found that children take more time but fewer steps to complete game levels when using gestures.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {67–71},
numpages = {5},
keywords = {stem education, spatial reasoning, interaction, design guidelines},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172991,
author = {Salminen, Mikko and J\"{a}rvel\"{a}, Simo and Ruonala, Antti and Timonen, Janne and Mannermaa, Kristiina and Ravaja, Niklas and Jacucci, Giulio},
title = {Bio-Adaptive Social VR to Evoke Affective Interdependence: DYNECOM},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172991},
doi = {10.1145/3172944.3172991},
abstract = {The effects of bio-adaptation-based visual cues in conveying affective information was studied while conducting social meditation in a virtual reality (VR) environment. In a laboratory study, 22 dyads practiced a short duration, modified version of empathy-evoking compassion meditation. The immersive VR provided electroencephalography (EEG) -based real-time visual feedback, as changing color, on the user's brain activation; the breathing rate of the users was visualized as a movement cue. In addition, the synchronization of the feedback signals between the users was also visualized. Initial results suggest that the perceived affective interdependence, a component of social presence, was increased when both of the bio-adaptive visual feedbacks were presented to the users, compared to when there was no visual feedback. Also, the EEG-based visual feedback led to higher affective interdependence than the respiration based visual feedback. The findings suggest that affective contagion may occur by mediated adaptive cues that are based on physiological signals.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {73–77},
numpages = {5},
keywords = {neurofeedback, affective contagion, bio-feedback, virtual reality, empathy, meditation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172973,
author = {Biswas, Pradipta and DV, Jeevithashree},
title = {Eye Gaze Controlled MFD for Military Aviation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172973},
doi = {10.1145/3172944.3172973},
abstract = {Multi-Function Displays (MFD) are essential part of glass cockpit of modern military and civilian aircrafts. They can display more information in less space than traditional analog displays. Interacting with MFDs is still now limited to a joystick system attached to throttle (called Target Des-ignation System, TDS). This paper explored using gaze con-trolled interface for MFD and proposed two algorithms based on hotspots and adaptable zooming for improving response times in a gaze controlled interface. Three user studies confirmed gaze controlled MFD can significantly reduce response times for big peripheral buttons compared to touchscreen in a head down configuration and com-pared to existing TDS in a head up configuration.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {79–89},
numpages = {11},
keywords = {eye gaze tracker, MFD, gaze controlled interface, flight simulator, HUD},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247907,
author = {Stumpf, Simone},
title = {Session Details: Session 2A: Evaluation of IUIs},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247907},
doi = {10.1145/3247907},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172954,
author = {Xiao, Ziang and Wauck, Helen and Peng, Zeya and Ren, Hanfei and Zhang, Lei and Zuo, Shiliang and Yao, Yuqi and Fu, Wai-Tat},
title = {Cubicle: An Adaptive Educational Gaming Platform for Training Spatial Visualization Skills},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172954},
doi = {10.1145/3172944.3172954},
abstract = {Research has demonstrated that spatial visualization skills are crucial for success in Science, technology, engineering, and mathematics (STEM) disciplines. With an increasing number of students entering STEM disciplines, the question of how to effectively train students» spatial visualization skills has become very important. While a scalable existing solution is to implement online workshops for students, the problem of how to motivate students to participate in these online workshops remains unsolved. In this study, we studied gamification as a way to motivate first year engineering students to take part in an online workshop designed to train their spatial visualization skills. Our game contains eight modules, each designed to train a different component of spatial visualization. The game records players» in-game behavior with high granularity, which allows us to provide automated, scalable feedback on players» problem-solving strategies. Ten students with different levels of spatial ability played our game and expressed a strong interest in using the game to train their spatial visualization skills in the future. In addition, our analysis of players» in-game behaviors shows the potential benefits of implementing adaptive and personalized learning guidance.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {91–101},
numpages = {11},
keywords = {video games, stem, player behavior, spatial visualization skills, education, learning analytics, game features},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172953,
author = {Donoso-Guzm\'{a}n, Ivania and Parra, Denis},
title = {An Interactive Relevance Feedback Interface for Evidence-Based Health Care},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172953},
doi = {10.1145/3172944.3172953},
abstract = {We design, implement and evaluate EpistAid, an interactive relevance feedback system to support physicians towards a more efficient citation screening process for Evidence Based Health Care (EBHC). The system combines a relevance feedback algorithm with an interactive interface inspired by Tinder-like swipe interaction. To evaluate its efficiency and effectiveness in the citation screening process we conducted a user study with real users (senior medicine students) using a large EBHC dataset (Epistemonikos), with around 400,000 documents. We compared two relevance feedback algorithms, Rocchio and BM25-based. The combination of Rocchio relevance feedback with the document visualization yielded the best recall and F-1 scores, which are the most important metrics for EBHC document screening. In terms of cognitive demand and effort, BM25 relevance feedback without visualization was perceived as needing more physical and cognitive effort. EpistAid has the potential of improving the process for answering clinical questions by reducing the time needed to classify documents, as well as promoting user interaction. Our results can inform the development of intelligent user interfaces for screening research articles in the clinical domain and beyond.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {103–114},
numpages = {12},
keywords = {intelligent user interfaces, human in the loop, evidence-based health care, information filtering, visualization},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172980,
author = {Kotowick, Kyle and Shah, Julie},
title = {Modality Switching for Mitigation of Sensory Adaptation and Habituation in Personal Navigation Systems},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172980},
doi = {10.1145/3172944.3172980},
abstract = {When humans need to navigate across terrain accurately and quickly, they often use portable electronic navigation systems for directional guidance. Prior work in this field has focused on selecting either the visual or haptic sensory modality for providing such guidance and has indicated that either option may be preferable depending on the user's specific goals. However, basing the selection of visual or haptic guidance on static criteria of this type discounts important time-varying effects, primarily stimulus-specific adaptation (SSA) and habituation. Here, we propose a navigation system design that mitigates these detrimental effects by periodically switching between visual and haptic navigation guidance. While this is likely to incur an undesirable switching cost, we hypothesize that the long-term benefits of counteracting SSA and habituation will outweigh this cost. In this paper, we describe the design and results of a human-participant study intended to evaluate this hypothesis. Our findings indicate that modality switching results in a transient cost to performance, but also that switching modalities lessens the SSA and habituation effects over time as compared with single-modality systems. The results support the hypothesis that an alternating-modality system would outperform a single-modality system for long-duration navigation tasks.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {115–127},
numpages = {13},
keywords = {sensory modality, navigation, multimodal, habituation, adaptation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172984,
author = {Axtell, Benett and Munteanu, Cosmin and Demmans Epp, Carrie and Aly, Yomna and Rudzicz, Frank},
title = {Touch-Supported Voice Recording to Facilitate Forced Alignment of Text and Speech in an E-Reading Interface},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172984},
doi = {10.1145/3172944.3172984},
abstract = {Reading a book together with a family member who has impaired vision or other difficulties reading is an important social bonding activity. However, for the person being read to, there is little support in making these experiences repeatable. While audio can easily be recorded, synchronizing it with the text for later playback requires the use of forced alignment algorithms, which do not perform well on amateur read-aloud speech. We propose a human-in-the-loop approach to augmenting such algorithms, in the form of touch metaphors during collocated read-aloud sessions using tablet e-readers. The metaphor is implemented as a finger-follows-text tracker. We explore how this could better handle the variability of amateur reading, which poses accuracy challenges for existing forced alignment techniques. Data collected from users reading aloud as assisted by touch metaphors show increases in the accuracy of forced alignment algorithms and reveal opportunities for how to better support reading aloud.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {129–140},
numpages = {12},
keywords = {forced alignment, natural language and speech processing, multi-modal interfaces, assistive technology},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172999,
author = {Gonzalez, Diego and Gordon, Andrew S.},
title = {Comparing Speech and Text Input in Interactive Narratives},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172999},
doi = {10.1145/3172944.3172999},
abstract = {Intelligent user interfaces are finding new applications in interactive narratives, where players take on the role of a character in a fictional storyline. A recent example is the interactive audio narrative "Traveler", in which a combination of technologies for speech recognition and unsupervised text classification allow players to navigate a branching storyline via open-vocabulary spoken input. We hypothesize that the affordances of audio-based interaction in interactive narratives are different than text-based interaction, and that these differences change the player experience and their understanding of their fictional role. To test this hypothesis, we conducted a controlled experiment (n=39) to compare player interaction in "Traveler" with a text-only variant of the same storyline. We found significant differences in the types of input provided by players, suggesting that interaction modality impacts how players conceive of their relation to narrators of fictional storylines.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {141–145},
numpages = {5},
keywords = {interactive narratives, speech interaction, natural language interfaces},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172996,
author = {Katsini, Christina and Fidas, Christos and Raptis, George E. and Belk, Marios and Samaras, George and Avouris, Nikolaos},
title = {Eye Gaze-Driven Prediction of Cognitive Differences during Graphical Password Composition},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172996},
doi = {10.1145/3172944.3172996},
abstract = {Evidence suggests that individual cognitive differences affect users' memorability, visual behavior, and graphical passwords' security. Such knowledge denotes the added value of personalizing graphical password schemes towards the unique cognitive characteristics of the users. However, real-time and accurate cognition-based predictive user models are necessary to reach such a breakthrough. In this paper, we present the results of such an attempt, where an in-lab eye-tracking study was conducted with 36 participants who completed a recall-based graphical password composition task. We adopted a credible cognitive style theory, and investigated a variety of eye-tracking metrics to predict participants' cognitive styles. Results' analysis reveals that inferring individual cognitive differences in real-time during graphical password composition is feasible within a few seconds and that specific eye-tracking metrics correlate stronger with certain cognitive style groups. The findings further support the vision of incorporating real-time adaptive mechanisms in graphical password schemes for the benefit of service providers and end-users.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {147–152},
numpages = {6},
keywords = {user modeling, graphical user authentication, classification, human cognitive differences, eye-tracking},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247908,
author = {Nichols, Jeff},
title = {Session Details: Session 2B: Modelling and Predicting User Behavior},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247908},
doi = {10.1145/3247908},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172969,
author = {M\"{u}ller, Philipp and Huang, Michael Xuelin and Bulling, Andreas},
title = {Detecting Low Rapport During Natural Interactions in Small Groups from Non-Verbal Behaviour},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172969},
doi = {10.1145/3172944.3172969},
abstract = {Rapport, the close and harmonious relationship in which interaction partners are "in sync" with each other, was shown to result in smoother social interactions, improved collaboration, and improved interpersonal outcomes. In this work, we are first to investigate automatic prediction of low rapport during natural interactions within small groups. This task is challenging given that rapport only manifests in subtle non-verbal signals that are, in addition, subject to influences of group dynamics as well as inter-personal idiosyncrasies. We record videos of unscripted discussions of three to four people using a multi-view camera system and microphones. We analyse a rich set of non-verbal signals for rapport detection, namely facial expressions, hand motion, gaze, speaker turns, and speech prosody. Using facial features, we can detect low rapport with an average precision of 0.7 (chance level at 0.25), while incorporating prior knowledge of participants' personalities can even achieve early prediction without a drop in performance. We further provide a detailed analysis of different feature sets and the amount of information contained in different temporal segments of the interactions.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {153–164},
numpages = {12},
keywords = {personality traits, speech prosody, dominance, body posture, leadership, facial expressions, social signal processing, affective computing},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172982,
author = {Bahirat, Paritosh and He, Yangyang and Menon, Abhilash and Knijnenburg, Bart},
title = {A Data-Driven Approach to Developing IoT Privacy-Setting Interfaces},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172982},
doi = {10.1145/3172944.3172982},
abstract = {User testing is often used to inform the development of user interfaces (UIs). But what if an interface needs to be developed for a system that does not yet exist? In that case, existing datasets can provide valuable input for UI development. We apply a data-driven approach to the development of a privacy-setting interface for Internet-of-Things (IoT) devices. Applying machine learning techniques to an existing dataset of users' sharing preferences in IoT scenarios, we develop a set of "smart" default profiles. Our resulting interface asks users to choose among these profiles, which capture their preferences with an accuracy of 82%---a 14% improvement over a naive default setting and a 12% improvement over a single smart default setting for all users.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {165–176},
numpages = {12},
keywords = {internet of things, data-driven design, privacy settings, machine learning},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172985,
author = {Bohari, Umema and Chen, Ting-Ju and Vinayak},
title = {To Draw or Not to Draw: Recognizing Stroke-Hover Intent in Non-Instrumented Gesture-Free Mid-Air Sketching},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172985},
doi = {10.1145/3172944.3172985},
abstract = {Drawing curves in mid-air with fingers is a fundamental task with applications to 3D sketching, geometric modeling, handwriting recognition, and authentication. Mid-air curve input is most commonly accomplished through explicit user input; akin to click-and-drag, the user may use a hand posture (e.g. pinch) or a button-press on an instrumented controller to express the intention to start and stop sketching. In this paper, we present a novel approach to recognize the user's intention to draw or not to draw in a mid-air sketching task without the use of postures or controllers. For every new point recorded in the user's finger trajectory, the idea is to simply classify this point as either hover or stroke. Our work is motivated by a behavioral study that demonstrates the need for such an approach due to the lack of robustness and intuitiveness while using hand postures and instrumented devices. We captured sketch data from users using a haptics device and trained multiple binary classifiers using feature vectors based on the local geometric and motion profile of the trajectory. We present a systematic comparison of these classifiers and discuss the advantages of our approach to spatial curve input applications.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {177–188},
numpages = {12},
keywords = {3d sketching, intent recognition, random forest, curve modeling, mid-air interactions},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172987,
author = {Robal, Tarmo and Zhao, Yue and Lofi, Christoph and Hauff, Claudia},
title = {Webcam-Based Attention Tracking in Online Learning: A Feasibility Study},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172987},
doi = {10.1145/3172944.3172987},
abstract = {A main weakness of the open online learning movement is retention: a small minority of learners (on average 5-10%, in extreme cases &lt;1%) that start a so-called Massive Open Online Course (MOOC) complete it successfully. There are many reasons why learners are unsuccessful, among the most important ones is the lack of self-regulation: learners are often not able to self-regulate their learning behavior. Designing tools that provide learners with a greater awareness of their learning is vital to the future success of MOOC environments. Detecting learners' loss of focus during learning is particularly important, as this can allow us to intervene and return the learners' attention to the learning materials. One technological affordance to detect such loss of focus are webcams---ubiquitous pieces of hardware available in almost all laptops today. In recent years, researchers have begun to exploit eye tracking and gaze data generated from webcams as part of complex machine learning solutions to detect inattention or loss of focus. Those approaches however tend to have a high detection lag, can be inaccurate, and are complex to design and maintain. In contrast, in this paper, we explore the possibility of a simple alternative---the presence or absence of a face---to detect a loss of focus in the online learning setting. To this end, we evaluate the performance of three consumer and professional eye/face-tracking frameworks using a benchmark suite we designed specifically for this purpose: it contains a set of common xMOOC user activities and behaviours. The results of our study show that even this basic approach poses a significant challenge to current hardware and software-based tracking solutions.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {189–197},
numpages = {9},
keywords = {online learning, eye tracking, moocs, face detection},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173009,
author = {Toker, Dereck and Conati, Cristina and Carenini, Giuseppe},
title = {User-Adaptive Support for Processing Magazine Style Narrative Visualizations: Identifying User Characteristics That Matter},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173009},
doi = {10.1145/3172944.3173009},
abstract = {In this paper we present results from an exploratory user study to uncover which user characteristics (e.g., perceptual speed, verbal working memory, etc.) play a role in how users process textual documents with embedded visualizations (i.e., Magazine Style Narrative Visualizations). We present our findings as a step toward developing user-adaptive support, and provide suggestions on how our results can be leveraged for creating a set of meaningful interventions for future evaluation.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {199–204},
numpages = {6},
keywords = {individual differences, user-adaptation, personalization., user study, user characteristics, narrative visualization},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173006,
author = {Putze, Felix and Salous, Mazen and Schultz, Tanja},
title = {Detecting Memory-Based Interaction Obstacles with a Recurrent Neural Model of User Behavior},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173006},
doi = {10.1145/3172944.3173006},
abstract = {A memory-based interaction obstacle is a condition which impedes human memory during Human-Computer Interaction, for example a memory-loading secondary task. In this paper, we present an approach to detect the presence of such memory-based interaction obstacles from logged user behavior during system use. For this purpose, we use a recurrent neural network which models the resulting temporal sequences. To acquire a sufficient number of training episodes, we employ a cognitive user simulation. We evaluate the approach with data from a user test and on which we outperform a non-sequential baseline by up to 42% relative.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {205–209},
numpages = {5},
keywords = {interaction obstacles, memory, LSTMs, classification of user behavior},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247909,
author = {Liao, Vera},
title = {Session Details: Session 3A: XAI: Explainable IUIs},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247909},
doi = {10.1145/3247909},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172961,
author = {Eiband, Malin and Schneider, Hanna and Bilandzic, Mark and Fazekas-Con, Julian and Haug, Mareike and Hussmann, Heinrich},
title = {Bringing Transparency Design into Practice},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172961},
doi = {10.1145/3172944.3172961},
abstract = {Intelligent systems, which are on their way to becoming mainstream in everyday products, make recommendations and decisions for users based on complex computations. Researchers and policy makers increasingly raise concerns regarding the lack of transparency and comprehensibility of these computations from the user perspective. Our aim is to advance existing UI guidelines for more transparency in complex real-world design scenarios involving multiple stakeholders. To this end, we contribute a stage-based participatory process for designing transparent interfaces incorporating perspectives of users, designers, and providers, which we developed and validated with a commercial intelligent fitness coach. With our work, we hope to provide guidance to practitioners and to pave the way for a pragmatic approach to transparency in intelligent systems.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {211–223},
numpages = {13},
keywords = {explanation interfaces, mental models, transparency, participatory design, intelligent systems, intelligibility, scrutability},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172946,
author = {Penney, Sean and Dodge, Jonathan and Hilderbrand, Claudia and Anderson, Andrew and Simpson, Logan and Burnett, Margaret},
title = {Toward Foraging for Understanding of StarCraft Agents: An Empirical Study},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172946},
doi = {10.1145/3172944.3172946},
abstract = {Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called "Explainable AI," is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study -- using the lens of Information Foraging Theory -- into how experienced users foraged in the domain of StarCraft to assess an agent. Our results showed that participants faced difficult foraging problems. These foraging problems caused participants to entirely miss events that were important to them, reluctantly choose to ignore actions they did not want to ignore, and bear high cognitive, navigation, and information costs to access the information they needed.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {225–237},
numpages = {13},
keywords = {video games, starcraft, intelligent agents, intelligibility, information foraging, explainable ai, content analysis},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172959,
author = {Tsai, Chun-Hua and Brusilovsky, Peter},
title = {Beyond the Ranked List: User-Driven Exploration and Diversification of Social Recommendation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172959},
doi = {10.1145/3172944.3172959},
abstract = {The beyond-relevance objectives of recommender systems have been drawing more and more attention. For example, a diversity-enhanced interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about how users adopt diversity-enhanced interfaces to accomplish various real-world tasks. In this paper, we present two attempts at creating a visual diversity-enhanced interface that presents recommendations beyond a simple ranked list. Our goal was to design a recommender system interface to help users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two within-subject user studies in the context of social recommendation at academic conferences were conducted to compare our visual interfaces. Results from our user study show that the visual interfaces significantly reduced the exploration efforts required for given tasks and helped users to perceive the recommendation diversity. We show that the users examined a diverse set of recommended items while experiencing an improvement in overall user satisfaction. Also, the users» subjective evaluations show significant improvement in many user-centric metrics. Experiences are discussed that shed light on avenues for future interface designs.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {239–250},
numpages = {12},
keywords = {social recommendation, diversification, user interface, user-driven exploration, diversity},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172968,
author = {Alkan, Oznur and Daly, Elizabeth M. and Vejsbjerg, Inge},
title = {Opportunity Team Builder for Sales Teams},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172968},
doi = {10.1145/3172944.3172968},
abstract = {Sellers work together as a team on sales opportunities, using their expertise in different roles to increase the probability of a win. These roles include managing the relationship with the client, overall architecture support or deep knowledge of a particular product depending on the seller's expertise, and the current opportunity requirements. Forming the right team for an incoming opportunity is vital and depends on several factors including understanding the required roles for the opportunity and assigning the right person to fulfill these roles, taking into consideration the seller's social network. In this paper, we present the Opportunity Team Builder solution, which supports sellers in this work by dividing the process into the following sub-tasks; identifying the required roles for the opportunity based on the products that the client is interested in, recommending the best people to fulfill these roles, and providing a win probability figure to guide users in team formation. This supports the sellers in forming the bestfitting team for current opportunity dynamics. Each task in the solution is implemented as a model using historical data from previous sales opportunities. Models work in coordination with each other to ultimately maximize the probability of win over loss. The solution not only recommends the best person to join a team taking into account a combination of inferred skills and social relationships, but also the predicted impact the person can have on the overall performance of the team. We present how the whole solution is realized with an intelligent user interface enabling interaction with the user throughout the team formation process. Substantial experiments with real world data show that win/loss prediction is performed accurately and the Opportunity Team Builder solution can recommend teams that achieve a higher win probability.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {251–261},
numpages = {11},
keywords = {prediction model, explanation interfaces, team recommendation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172988,
author = {Ma, Xinyao and Yao, Zhaolin and Wang, Yijun and Pei, Weihua and Chen, Hongda},
title = {Combining Brain-Computer Interface and Eye Tracking for High-Speed Text Entry in Virtual Reality},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172988},
doi = {10.1145/3172944.3172988},
abstract = {Gaze interaction provides an efficient way for users to communicate and control in virtual reality (VR) presented by head-mounted displays. In gaze-based text-entry systems, eye tracking and brain-computer interface (BCI) are the two most commonly used approaches. This paper presents a hybrid BCI system for text entry in VR by combining steady-state visual evoked potentials (SSVEP) and eye tracking. The user interface in VR designed a 40-target virtual keyboard using a joint frequency-phase modulation method for SSVEP. Eye position was measured by an eye-tracking accessory in the VR headset. Target-related gaze direction was detected by combining simultaneously recorded SSVEP and eye position data. Offline and online experiments indicate that the proposed system can type at a speed around 10 words per minute, leading to an information transfer rate (ITR) of 270 bits per minute. The results further demonstrate the superiority of the hybrid method over single-modality methods for VR applications.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {263–267},
numpages = {5},
keywords = {virtual reality, eye tracking, brain-computer interface, text entry, steady-state visual evoked potentials},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247910,
author = {Daly, Elizabeth},
title = {Session Details: Session 3B: Interactive Machine Learning and Analysis},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247910},
doi = {10.1145/3247910},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172950,
author = {Chen, Nan-Chen and Suh, Jina and Verwey, Johan and Ramos, Gonzalo and Drucker, Steven and Simard, Patrice},
title = {AnchorViz: Facilitating Classifier Error Discovery through Interactive Semantic Data Exploration},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172950},
doi = {10.1145/3172944.3172950},
abstract = {When building a classifier in interactive machine learning, human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This paper presents AnchorViz, an interactive visualization that facilitates error discovery through semantic data exploration. By creating example-based anchors, users create a topology to spread data based on their similarity to the anchors and examine the inconsistencies between data points that are semantically related. The results from our user study show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {269–280},
numpages = {12},
keywords = {semantic data exploration, visualization, error discovery, interactive machine learning, unlabeled data},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172964,
author = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
title = {Interactive Document Clustering Revisited: A Visual Analytics Approach},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172964},
doi = {10.1145/3172944.3172964},
abstract = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user»s perspectives. To incorporate the user»s perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {281–292},
numpages = {12},
keywords = {user study, interactive document clustering, key-term, document projection, visualization, text},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172965,
author = {Smith, Alison and Kumar, Varun and Boyd-Graber, Jordan and Seppi, Kevin and Findlater, Leah},
title = {Closing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172965},
doi = {10.1145/3172944.3172965},
abstract = {Human-in-the-loop topic modeling allows users to guide the creation of topic models and to improve model quality without having to be experts in topic modeling algorithms. Prior work in this area has focused either on algorithmic implementation without understanding how users actually wish to improve the model or on user needs but without the context of a fully interactive system. To address this disconnect, we implemented a set of model refinements requested by users in prior work and conducted a study with twelve non-expert participants to examine how end users are affected by issues that arise with a fully interactive, user-centered system. As these issues mirror those identified in interactive machine learning more broadly, such as unpredictability, latency, and trust, we also examined interactive machine learning challenges with non-expert end users through the lens of human-in-the-loop topic modeling. We found that although users experience unpredictability, their reactions vary from positive to negative, and, surprisingly, we did not find any cases of distrust, but instead noted instances where users perhaps trusted the system too much or had too little confidence in themselves.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {293–304},
numpages = {12},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172989,
author = {Daee, Pedram and Peltola, Tomi and Vehtari, Aki and Kaski, Samuel},
title = {User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172989},
doi = {10.1145/3172944.3172989},
abstract = {In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {305–310},
numpages = {6},
keywords = {bayesian inference, overfitting, interactive machine learning, probabilistic modelling, human-in-the-loop machine learning, expert prior elicitation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172993,
author = {Gomez-Zara, Diego and Boon, Miriam and Birnbaum, Larry},
title = {Who is the Hero, the Villain, and the Victim? Detection of Roles in News Articles Using Natural Language Techniques},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172993},
doi = {10.1145/3172944.3172993},
abstract = {News articles often use narrative frames to present people, organizations, and facts. These narrative frames follow cultural archetypes, enabling readers to associate each of the presented elements with familiar stereotypes, well-known characters, and recognizable outcomes. In this way, authors can cast real people or organizations as heroes, villains, or victims. We present a system that identifies the main entities of a news article, and determines which is being cast as a hero, a villain, or a victim. As currently implemented, this system interacts directly with news consumers through a browser extension. Our hope is that by informing readers when an entity is cast in one of these roles, we can make implicit bias explicit, and thereby assist readers in applying their media literacy skills. This approach can also be used to identify roles in well-understood event sequences in a more prosaic manner, e.g., for information extraction.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {311–315},
numpages = {5},
keywords = {sentiment analysis, computational journalism, contextual information, entity recognition, information extraction, role detection},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247911,
author = {Paris, Cecile},
title = {Session Details: Session 4A: Information Retrieval and Search},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247911},
doi = {10.1145/3247911},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172975,
author = {Vanderdonckt, Jean and Bouzit, Sara and Calvary, Ga\"{e}lle and Ch\^{e}ne, Denis},
title = {Cloud Menus: A Circular Adaptive Menu for Small Screens},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172975},
doi = {10.1145/3172944.3172975},
abstract = {This paper presents Cloud Menus, a split adaptive menu for small screens where the predicted menu items are arranged in a circular tag cloud with a location consistent with their corresponding position in the static menu and a font size depending on their prediction level. This layout results from a 3-step design process: (i) defining an initial design space on Bertin's 8 visual variables and 4 quality properties, (ii) identifying the most preferred layout based on agreement rate, and (iii) implementing it into Cloud Menus, a new widget for Android with circular layout. An empirical study suggests that cloud menus reduce item selection time and error rate when prediction is correct without penalizing it when prediction is incorrect, compared to two baselines: a non-adaptive static menu and an adaptive linear menu. From this study, design guidelines for cloud menus are elaborated.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {317–328},
numpages = {12},
keywords = {split menu, tag cloud, adaptive menu, prediction window},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172983,
author = {Clark, Elizabeth and Ross, Anne Spencer and Tan, Chenhao and Ji, Yangfeng and Smith, Noah A.},
title = {Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172983},
doi = {10.1145/3172944.3172983},
abstract = {As the quality of natural language generated by artificial intelligence systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed two case studies using two system prototypes, one for short story writing and one for slogan writing. Participants in our studies were asked to write with a machine in the loop or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, machine suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design choices that may better support creative writing.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {329–340},
numpages = {12},
keywords = {machine in the loop, creative writing, natural language processing},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172957,
author = {Mauro, Noemi and Ardissono, Liliana},
title = {Session-Based Suggestion of Topics for Geographic Exploratory Search},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172957},
doi = {10.1145/3172944.3172957},
abstract = {Exploratory information search can challenge users in the formulation of efficacious search queries. Moreover, complex information spaces, such as those managed by Geographical Information Systems, can disorient people, making it difficult to find relevant data. In order to address these issues, we developed a session-based suggestion model that proposes concepts as a em "you might also be interested in»» function, by taking the user»s previous queries into account. Our model can be applied to incrementally generate suggestions in interactive search. It can be used for query expansion, and in general to guide users in the exploration of possibly complex spaces of data categories. Our model is based on a concept co-occurrence graph that describes how frequently concepts are searched together in search sessions. Starting from an ontological domain representation, we generated the graph by analyzing the query log of a major search engine. Moreover, we identified clusters of ontology concepts which frequently co-occur in the sessions of the log via community detection on the graph. The evaluation of our model provided satisfactory accuracy results.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {341–352},
numpages = {12},
keywords = {geographical information retrieval, session-based concept suggestion, query expansion},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172986,
author = {di Sciascio, Cecilia and Brusilovsky, Peter and Veas, Eduardo},
title = {A Study on User-Controllable Social Exploratory Search},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172986},
doi = {10.1145/3172944.3172986},
abstract = {Information-seeking tasks with learning or investigative purposes are usually referred to as exploratory search. Exploratory search unfolds as a dynamic process where the user, amidst navigation, trial-and-error and on-the-fly selections, gathers and organizes information (resources). A range of innovative interfaces with increased user control have been developed to support exploratory search process. In this work we present our attempt to increase the power of exploratory search interfaces by using ideas of social search, i.e., leveraging information left by past users of information systems. Social search technologies are highly popular nowadays, especially for improving ranking. However, current approaches to social ranking do not allow users to decide to what extent social information should be taken into account for result ranking. This paper presents an interface that integrates social search functionality into an exploratory search system in a user-controlled way that is consistent with the nature of exploratory search. The interface incorporates control features that allow the user to (i) express information needs by selecting keywords and (ii) to express preferences for incorporating social wisdom based on tag matching and user similarity. The interface promotes search transparency through color-coded stacked bars and rich tooltips. In an online study investigating system accuracy and subjective aspects with a structural model we found that, when users actively interacted with all its control features, the hybrid system outperformed a baseline content-based-only tool and users were more satisfied.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {353–364},
numpages = {12},
keywords = {exploratory search, information filtering, preference elicitation, hybrid ranking, social search},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172998,
author = {Zhang, Yunfeng and Liao, Q. Vera and Srivastava, Biplav},
title = {Towards an Optimal Dialog Strategy for Information Retrieval Using Both Open- and Close-Ended Questions},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172998},
doi = {10.1145/3172944.3172998},
abstract = {The emerging paradigm of dialogue interfaces for information retrieval systems opens new opportunities for interactively narrowing down users' information query and improving search results. Prior research has largely focused on methods that use a set of close-ended questions, such as decision tree, to learn about the user's search target. However, when there is a myriad of documents or items to search, solely relying on close-ended questions can lead to long and undesirable dialogues. We propose an adaptive dialogue strategy framework that incorporates open-ended questions at the optimal timing to reduce the length of the dialogue. We propose a method to estimate the information gain of open-ended questions, and in each dialog turn, we compare it with that of close-ended questions to decide which question to ask. We present experiments using several synthetic datasets designed to explore the behavior of such an adaptive dialogue strategy under different environments, and compare the system's performance with that of a close-ended-questions-only strategy.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {365–369},
numpages = {5},
keywords = {interactive information retrieval, question asking, dialog systems, decision-tree induction},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247912,
author = {Lim, Brian},
title = {Session Details: Session 4B: Persuasive and Assistive IUIs},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247912},
doi = {10.1145/3247912},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172970,
author = {Gao, Mingkun and Do, Hyo Jin and Fu, Wai-Tat},
title = {Burst Your Bubble! An Intelligent System for Improving Awareness of Diverse Social Opinions},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172970},
doi = {10.1145/3172944.3172970},
abstract = {Social media users are overloaded with diverse opinions by people with opposing stances. Previous research shows that people often look for opinions that reinforce their pre-existing beliefs and stances, which may lead to social polarization. Traditional social media present opinions in a linear list format, which not only lacks structures for people to explore diverse viewpoints but also aggravates their selective exposure to agreeable opinions. To address this problem, we designed an intelligent system that improves awareness of diverse social opinions by providing visual hints and recommendations of opinions (e.g. news articles and comments) on different sides with different indicators. We evaluated our system with news articles about Obamacare repeal issue and their corresponding user comments from Facebook. Results demonstrate that our system could increase people»s awareness of their stances and opinion selection preferences, which mitigates selective exposure and thereby leads to a more balanced perception of social opinions.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {371–383},
numpages = {13},
keywords = {selective exposure, intelligent system, social opinion},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172971,
author = {Yen, Chi-Hsien and Lee, Yi-Chieh and Fu, Wai-Tat},
title = {Visible Hearts, Visible Hands: A Smart Crowd Donation Platform},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172971},
doi = {10.1145/3172944.3172971},
abstract = {On existing crowdfunding platforms, the allocation of money is often not regulated, which leads to less-than-ideal distribution of resources. For example, recent donations to hurricane victims through their crowdfunding campaigns often lead to overfunding of certain victims while underfunding others. Inspired by algorithms from economic theories, we proposed a Smart Crowd Donate system encourages donors to express preferences to multiple projects and reallocates funds dynamically across these preferences over time. We conducted a user study in which recruited 452 participants to simulate a small scale of crowdfunding. The findings of our user study supported the idea that the Smart Crowd Donate system has potential to efficiently distribute funds to projects and allows more projects to receive the amount of money they need.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {385–395},
numpages = {11},
keywords = {resource allocation, donate, crowdfunding, fundraising},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173002,
author = {Kwok, Tiffany C.K. and Fu, Eugene Yujun and Wu, Erin You and Huang, Michael Xuelin and Ngai, Grace and Leong, Hong-Va},
title = {Every Little Movement Has a Meaning of Its Own: Using Past Mouse Movements to Predict the Next Interaction},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173002},
doi = {10.1145/3172944.3173002},
abstract = {User experience could be enhanced if the computer could understand human interaction intention. For instance, it could react to intercept and prevent interaction errors. This paper presents an approach to predicting users intention in interaction tasks based on past mouse movements. We adopt a long short-term memory (LSTM) model to predict the users» intention via their next mouse click interaction, upon being trained with past mouse interaction behaviors. To evaluate, we consider two scenarios in daily computer usage: a more structured crowdsourcing annotation task and a more free-form, open-ended web search task. Our results indicate that we could predict the next interaction event with reasonable accuracy. We also conducted a pilot study to investigate the possibility of applying our model for non-intentional mouse click detection. We believe that our findings would be beneficial towards the development of better intelligent agents.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {397–401},
numpages = {5},
keywords = {human-computer interaction, mouse interaction, user intention, non-intentional mouse click detection},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173008,
author = {Ohn-Bar, Eshed and Guerreiro, Jo\~{a}o and Ahmetovic, Dragan and Kitani, Kris M. and Asakawa, Chieko},
title = {Modeling Expertise in Assistive Navigation Interfaces for Blind People},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173008},
doi = {10.1145/3172944.3173008},
abstract = {Evaluating the impact of expertise and route knowledge on task performance can guide the design of intelligent and adaptive navigation interfaces. Expertise has been relatively unexplored in the context of assistive indoor navigation interfaces for blind people. To quantify the complex relationship between the user»s walking patterns, route learning, and adaptation to the interface, we conducted a study with 8 blind participants. The participants repeated a set of navigation tasks while using a smartphone-based turn-by-turn navigation guidance app. The results demonstrate the gradual evolution of user skill and knowledge throughout the route repetitions, significantly impacting the task completion time. In addition to the exploratory analysis, we take a step towards tailoring the navigation interface to the user»s needs by proposing a personalized recurrent neural network-based behavior model for expertise level classification.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {403–407},
numpages = {5},
keywords = {interface adaptation, knowledge, recurrent neural networks, experience, personalization, blind navigation, user modeling},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173010,
author = {Parekh, Viral and Foong, Pin Sym and Zhao, Shengdong and Subramanian, Ramanathan},
title = {AVEID: Automatic Video System for Measuring Engagement In Dementia},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173010},
doi = {10.1145/3172944.3173010},
abstract = {Engagement in dementia is typically measured using behavior observational scales (BOS) that are tedious and involve intensive manual labor to annotate, and are therefore not easily scalable. We propose AVEID, a low cost and easy-to-use video-based engagement measurement tool to determine the engagement level of a person with dementia (PwD) during digital interaction. We show that the objective behavioral measures computed via AVEID correlate well with subjective expert impressions for the popular MPES and OME BOS, confirming its viability and effectiveness. Moreover, AVEID measures can be obtained for a variety of engagement designs, thereby facilitating large-scale studies with PwD populations.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {409–413},
numpages = {5},
keywords = {dementia, video-based analytics, engagement},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172995,
author = {Hagiya, Toshiyuki and Hoashi, Keiichiro and Kawahara, Tatsuya},
title = {Voice Input Tutoring System for Older Adults Using Input Stumble Detection},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172995},
doi = {10.1145/3172944.3172995},
abstract = {Many older adults are interested in smartphones but encounter difficulties in self-instruction and need support, especially text input. Voice input is a useful option for text input, but also presents some difficulties for older adults.In this paper, we propose a tutoring system for voice input that detects input stumbles using a statistical approach and provides instructions to overcome them. We construct the tutoring system based on the data from a user study with novice older adults. In an evaluation experiment, the number of input stumble and the sentence completion time of the participants using the tutoring system were significantly smaller than those without it. The results showed that the tutoring system resulted in the improvement of the efficiency of voice input for novice older adults.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {415–419},
numpages = {5},
keywords = {older adults, voice input, tutoring system},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172997,
author = {Zhang, Yanxia and Olenick, Jeffrey and Chang, Chu-Hsiang and Kozlowski, Steve W. J. and Hung, Hayley},
title = {The I in Team: Mining Personal Social Interaction Routine with Topic Models from Long-Term Team Data},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172997},
doi = {10.1145/3172944.3172997},
abstract = {Social interaction plays a key role in assessing teamwork and collaboration. It becomes particularly critical in team performance when coupled with isolated, confined, and extreme conditions such as undersea missions. This work investigates how social interactions of individual members in a small team evolve during the course of a long duration mission. We propose to use a topic model to mine individual social interaction patterns and examine how the dynamics of these patterns have an effect on self-assessment of mood and team cohesion. Specifically, we analyzed data from a 6-person crew wearing Sociometric badges over a 4-month mission. Our results show that our method can extract the latent structure of social contexts without supervision. We demonstrate how the extracted patterns based on probabilistic models can provide insights on common behaviors at various temporal resolutions and exhibit links with self-report affective states and team cohesion.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {421–426},
numpages = {6},
keywords = {machine learning, team dynamics, wearable},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247913,
author = {Kunze, Kai},
title = {Session Details: Session 5A: IUIs for Wearable, Mobile and Ubiquitious Computing},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247913},
doi = {10.1145/3247913},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173005,
author = {Billah, Syed Masum and Ashok, Vikas and Ramakrishnan, IV},
title = {Write-It-Yourself with the Aid of Smartwatches: A Wizard-of-Oz Experiment with Blind People},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173005},
doi = {10.1145/3172944.3173005},
abstract = {Working with non-digital, standard printed materials has always been a challenge for blind people, especially writing. Blind people very often depend on others to fill out printed forms, write checks, sign receipts and documents. Extant assistive technologies for working with printed material have exclusively focused on reading, with little to no support for writing. Also, these technologies employ special-purpose hardware that are usually worn on fingers, making them unsuitable for writing. In this paper, we explore the idea of using off-the-shelf smartwatches (paired with smartphones) to assist blind people in both reading and writing paper forms including checks and receipts. Towards this, we performed a Wizard-of-Oz evaluation of different smartwatch-based interfaces that provide user-customized audio-haptic feedback in real-time, to guide blind users to different form fields, narrate the field labels, and help them write straight while filling out these fields. Finally, we report the findings of this study including the technical challenges and user expectations that can potentially inform the design of Write-it-Yourself aids based on smartwatches.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {427–431},
numpages = {5},
keywords = {wearables, visual impairments, smartwatch., accessibility, directional guidance, blind, writing aid, audio-haptic},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172992,
author = {Chongtay, Rocio and Last, Mark and Berendt, Bettina},
title = {Responsive News Summarization for Ubiquitous Consumption on Multiple Mobile Devices},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172992},
doi = {10.1145/3172944.3172992},
abstract = {With the proliferation of online news read on devices ranging from desktops to smart watches, the need for meaningful summaries of long texts is growing. Manual summaries are labour-intensive and cannot be offered for all display sizes, whereas today's abstracts of most news texts are teasers designed to attract the reader's interest more than to provide an overview of an article's content suited to the reader's information needs. We propose responsive news summarization as a technological approach for filling this gap. Responsive news summarization provides an automatically generated content summary that has the right length for the device requesting the article, plus access to the full text. We describe the system prototype available at multisizenews.com along with the initial user study results and give an outlook on future work.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {433–437},
numpages = {5},
keywords = {responsive news, automatic text summarization, adaptive text, responsive web design},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172962,
author = {Avrahami, Daniel and Patel, Mitesh and Yamaura, Yusuke and Kratz, Sven},
title = {Below the Surface: Unobtrusive Activity Recognition for Work Surfaces Using RF-Radar Sensing},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172962},
doi = {10.1145/3172944.3172962},
abstract = {Activity recognition is a core component of many intelligent and context-aware systems. In this paper, we present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in two work domains; recognizing work activities at a convenience-store counter (useful for post-hoc analytics) and recognizing common office deskwork activities (useful for real-time applications). We classify seven clerk activities with 94.9% accuracy using data collected in a lab environment, and recognize six common deskwork activities collected in real offices with 95.3% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users privacy concerns associated with cameras and is useful for a wide range of intelligent systems.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {439–451},
numpages = {13},
keywords = {radio frequency radar sensor, retail, activity recognition, deskwork, sensing, IMU},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172951,
author = {Luzhnica, Granit and Veas, Eduardo},
title = {Investigating Interactions for Text Recognition Using a Vibrotactile Wearable Display},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172951},
doi = {10.1145/3172944.3172951},
abstract = {Vibrotactile skin-reading uses wearable vibrotactile displays to convey dynamically generated textual information. Such wearable displays have potential to be used in a broad range of applications. Nevertheless, the reading process is passive, and users have no control over the reading flow. To compensate for such drawback, this paper investigates what kind of interactions are necessary for vibrotactile skin reading and the modalities of such interactions. An interaction concept for skin reading was designed by taking into account the reading as a process. We performed a formative study with 22 participants to assess reading behaviour in word and sentence reading using a six-channel wearable vibrotactile display. Our study shows that word based interactions in sentence reading are more often used and preferred by users compared to character-based interactions and that users prefer gesture-based interaction for skin reading. Finally, we discuss how such wearable vibrotactile displays could be extended with sensors that would enable recognition of such gesture-based interaction. This paper contributes a set of guidelines for the design of wearable haptic displays for text communication.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {453–465},
numpages = {13},
keywords = {interaction design, vibrotactile feedback, skin reading, wearable, interaction, haptic display, gesture interaction, user study, stimulation, HCI, gesture recognition},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172963,
author = {H\"{a}uslschmid, Renate and Fritzsche, Benjamin and Butz, Andreas},
title = {Can a Helmet-Mounted Display Make Motorcycling Safer?},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172963},
doi = {10.1145/3172944.3172963},
abstract = {In this paper we investigate whether a helmet-mounted display for motorcyclists can provide similar benefits as the established head-up display (HUD) in cars. HUDs can increase driving safety by efficiently informing the driver. Given their high safety risks, such a technology seems overdue for motorcyclists. Only recently, several crowdfunding projects have raised substantial funding for helmet-mounted displays, however, none of them has released a commercial product yet. In order to investigate display concepts, we developed an easy-to-reproduce and low-cost helmet-mounted display prototype and compared it to a conventional display setup. The helmet-mounted display was rated more attractive and induced a lower workload. Those motorcyclists who generally exceeded the speed limit before also rode significantly slower when using the head-mounted display. We argue that an intelligent HUD application that adapts to the rider»s behavior and preferences could further enhance safety and improve user experience for motorcyclists. We encourage other researchers to replicate our setup and to investigate the impact of HUDs on motorcyclists' behavior and road safety.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {467–476},
numpages = {10},
keywords = {head-mounted displays, motorcycling, rider-assistance system, head-down display, head-up display, prototyping},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172952,
author = {Buschek, Daniel},
title = {A Model for Detecting and Locating Behaviour Changes in Mobile Touch Targeting Sequences},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172952},
doi = {10.1145/3172944.3172952},
abstract = {Touch offset models capture users' targeting behaviour patterns across the screen. We present and evaluate the first extension of these models to explicitly address behaviour changes. We focus on user changes in particular: Given only a series of touch/target locations (x, y), our model detects 1) if the user has changed therein, and if so, 2) at which touch. We evaluate our model on smartphone targeting and typing data from the lab (N=28) and field (N=30). The results show that our model can exploit touch targeting sequences to reveal user changes. Our model outperforms existing non-sequence touch offset models and does not require training data. We discuss the model's limitations and ideas for further improvement. We conclude with recommendations for its integration into future touch biometric systems.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {477–486},
numpages = {10},
keywords = {change point model, regression, touch biometrics, computational interaction},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172978,
author = {Rateau, Hanae and Rekik, Yosra and Lank, Edward and Grisoni, Laurent},
title = {Ether-Toolbars: Evaluating Off-Screen Toolbars for Mobile Interaction},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172978},
doi = {10.1145/3172944.3172978},
abstract = {In mobile touchscreebn interaction, an important challenge is to find solutions to balance the size of individual widgets against the number of widgets needed during interaction. In this work, to address display space limitations, we explore the design of invisible off-screen toolbars (ether-toolbars) that leverage computer vision to expand application features by placing widgets adjacent to the display screen. We show how simple computer vision algorithms can be combined with a natural human ability to estimate physical placing to support highly accurate targeting. Our ether-toolbar design promises targeting accuracy approximating on-screen widget accuracy while significantly expanding the interaction space of mobile devices. Through two experiments, we examine off-screen content placement metaphors and off-screen precision of participants accessing these toolbars. From the data of the second experiment, we provide a basic model that reflects how users perceive mobile surroundings for ether-widgets and validate it. We also demonstrate a prototype system consisting of an inexpensive 3D printed mount for a mirror that supports ether-toolbar implementations. Finally, we discuss the implications of our work and potential design extensions that can increase the usability and the utility of ether-toolbars.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {487–495},
numpages = {9},
keywords = {around-device interaction, ether-toolbar, deformation estimation, mobile interaction, model, user study, ether-widgets},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247914,
author = {Jacucci, Giulio},
title = {Session Details: Session 5B: Intelligent Visualization and Smart Environments},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247914},
doi = {10.1145/3247914},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173004,
author = {Perez-Messina, Ignacio and Gutierrez, Claudio and Graells-Garrido, Eduardo},
title = {Organic Visualization of Document Evolution},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173004},
doi = {10.1145/3172944.3173004},
abstract = {Recent availability of data about writing processes at keystroke-granularity has enabled research on the evolution of document writing. A natural task is to develop systems that can actually show this data, that is, user interfaces that transform the data of the process of writing --today a black box-- into intelligible forms. On this line, we propose a data structure that captures a document's fine-grained history and an organic visualization that serves as an interface to it. We evaluate a proof-of-concept implementation of the system through a pilot study using documents written by students at a public university. Our results are promising and reveal facets such as general strategies adopted, local edition density and hierarchical structure of the final text.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {497–501},
numpages = {5},
keywords = {information visualization, text production, writing process},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173007,
author = {Metoyer, Ronald and Zhi, Qiyu and Janczuk, Bart and Scheirer, Walter},
title = {Coupling Story to Visualization: Using Textual Analysis as a Bridge Between Data and Interpretation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173007},
doi = {10.1145/3172944.3173007},
abstract = {Online writers and journalism media are increasingly combining visualization (and other multimedia content) with narrative text to create narrative visualizations. Often, however, the two elements are presented independently of one another. We propose an approach to automatically integrate text and visualization elements. We begin with a writer»s narrative that presumably can be supported with visual data evidence. We leverage natural language processing, quantitative narrative analysis, and information visualization to (1) automatically extract narrative components (who, what, when, where) from data-rich stories, and (2) integrate the supporting data evidence with the text to develop a narrative visualization. We also employ bidirectional interaction from text to visualization and visualization to text to support reader exploration in both directions. We demonstrate the approach with a case study in the data-rich field of sports journalism.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {503–507},
numpages = {5},
keywords = {text analysis, text visualization interaction, narrative visualization, deep coupling},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172972,
author = {Marti, Marcel and Vieli, Jodok and Wito\'{n}, Wojciech and Sanghrajka, Rushit and Inversini, Daniel and Wotruba, Diana and Simo, Isabel and Schriber, Sasha and Kapadia, Mubbasir and Gross, Markus},
title = {CARDINAL: Computer Assisted Authoring of Movie Scripts},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172972},
doi = {10.1145/3172944.3172972},
abstract = {We present Cardinal, a tool for computer-assisted authoring of movie scripts. Cardinal provides a means of viewing a script through a variety of perspectives, for interpretation as well as editing. This is made possible by virtue of intelligent automated analysis of natural language scripts and generating different intermediate representations. Cardinal generates 2-D and 3-D visualizations of the scripted narrative and also presents interactions in a timeline-based view. The visualizations empower the scriptwriter to understand their story from a spatial perspective, and the timeline view provides an overview of the interactions in the story. The user study reveals that users of the system demonstrated confidence and comfort using the system.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {509–519},
numpages = {11},
keywords = {scriptwriting, natural language processing, computer-assisted authoring, previz},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172979,
author = {Wegba, Kodzo and Lu, Aidong and Li, Yuemeng and Wang, Wencheng},
title = {Interactive Storytelling for Movie Recommendation through Latent Semantic Analysis},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172979},
doi = {10.1145/3172944.3172979},
abstract = {Recommendation is essential to many online services; however current systems often provide limited interaction and visualization mechanisms, affecting the user satisfaction of recommendation. This paper presents an interactive recommendation approach for the general public without any knowledge of recommendation or visualization algorithms. Our approach emphasizes interactivity, explicit user input, and semantic information convey with the following two components. First, we propose a Latent Semantic Model that captures the statistical features of semantic concepts on 2D domains and abstracts user preferences for personal recommendation, so that high-dimensional spectral space from the rating records can be understood and interacted with directly. Second, we propose an interactive recommendation approach through a storytelling mechanism for promoting the communication between the user and the recommendation system. We demonstrate and evaluate our approach with a real dataset. Our approach can also be extended to other applications including various online recommendation systems.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {521–533},
numpages = {13},
keywords = {recommender systems, interactive storytelling, latent semantic analysis},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172976,
author = {De Raffaele, Clifford and Smith, Serengul and Gemikonakli, Orhan},
title = {An Active Tangible User Interface Framework for Teaching and Learning Artificial Intelligence},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172976},
doi = {10.1145/3172944.3172976},
abstract = {Interactive and tangible computing platforms have garnered increased interest in the pursuit of embedding active learning pedagogies within curricula through educational technologies. Whilst Tangible User Interface (TUI) systems have successfully been developed to edutain children in various research, TUI architectures have seen limited deployment in more complex and abstract domains. In light of these limitations, this paper proposes an active TUI framework that addresses the challenges experienced in teaching and learning artificial intelligence (AI) within higher educational institutions. The proposal extends an aptly designed tabletop TUI architecture with the novel interactive paradigm of active tangible manipulatives to provide a more engaging and effective user interaction. The paper describes the deployment of the proposed TUI framework within an undergraduate laboratory session to aid in the teaching and learning of artificial neural networks. The experiment is assessed against currently adopted educational computer software and the obtained results highlight the potential of the proposed TUI framework to augment students? gain in knowledge and understanding of abstracted threshold concepts in higher education.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {535–546},
numpages = {12},
keywords = {tangible user interface, higher education, computer-aided instruction, artificial neural networks.},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172949,
author = {Todi, Kashyap and Jokinen, Jussi and Luyten, Kris and Oulasvirta, Antti},
title = {Familiarisation: Restructuring Layouts with Visual Learning Models},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172949},
doi = {10.1145/3172944.3172949},
abstract = {In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) in familiar locations. This paper contributes computational approaches to restructuring layouts such that features on a new, unvisited interface can be found quicker. We explore four concepts of familiarisation, inspired by the human visual system (HVS), to automatically generate a familiar design for each user.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {547–558},
numpages = {12},
keywords = {adaptive user interfaces, visual search, graphical layouts, computational design},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247915,
author = {Smith, Alison},
title = {Session Details: Session 6A: IUIs for Complex Tasks},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247915},
doi = {10.1145/3247915},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172948,
author = {Song, Jean Y. and Fok, Raymond and Lundgard, Alan and Yang, Fan and Kim, Juho and Lasecki, Walter S.},
title = {Two Tools Are Better Than One: Tool Diversity as a Means of Improving Aggregate Crowd Performance},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172948},
doi = {10.1145/3172944.3172948},
abstract = {Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this paper, we introduce a novel crowdsourcing workflow that leverages multiple tools for the same task to increase output accuracy by reducing systematic error biases introduced by the tools themselves. When a task can no longer be broken down into more-tractable subtasks (the conventional approach taken by microtask crowdsourcing), our multi-tool approach can be used to further improve accuracy by assigning different tools to different workers. We present a series of studies that evaluate our multi-tool approach and show that it can significantly improve aggregate accuracy in semantic image segmentation.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {559–570},
numpages = {12},
keywords = {semantic image segmentation, crowdsourcing, multi-tool aggregation, computer vision, human computation, tool diversity},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172960,
author = {Higuchi, Keita and Matsuda, Soichiro and Kamikubo, Rie and Enomoto, Takuya and Sugano, Yusuke and Yamamoto, Junichi and Sato, Yoichi},
title = {Visualizing Gaze Direction to Support Video Coding of Social Attention for Children with Autism Spectrum Disorder},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172960},
doi = {10.1145/3172944.3172960},
abstract = {This paper presents a novel interface to support video coding of social attention in the assessment of children with autism spectrum disorder. Video-based evaluations of social attention during therapeutic activities allow observers to find target behaviors while handling the ambiguity of attention. Despite the recent advances in computer vision-based gaze estimation methods, fully automatic recognition of social attention under diverse environments is still challenging. The goal of this work is to investigate an approach that uses automatic video analysis in a supportive manner for guiding human judgment. The proposed interface displays visualization of gaze estimation results on videos and provides GUI support to allow users to facilitate agreement between observers by defining social attention labels on the video timeline. Through user studies and expert reviews, we show how the interface helps observers perform video coding of social attention and how human judgment compensates for technical limitations of the automatic gaze analysis.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {571–582},
numpages = {12},
keywords = {social attention, video coding support, children with asd},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172974,
author = {Asadi, Reza and Trinh, Ha and Fell, Harriet J. and Bickmore, Timothy W.},
title = {Quester: A Speech-Based Question Answering Support System for Oral Presentations},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172974},
doi = {10.1145/3172944.3172974},
abstract = {Current slideware, such as PowerPoint, reinforces the delivery of linear oral presentations. In settings such as question answering sessions or review lectures, more extemporaneous and dynamic presentations are required. An intelligent system that can automatically identify and display the slides most related to the presenter's speech, allows for more speaker flexibility in sequencing their presentation. We present Quester, a system that enables fast access to relevant presentation content during a question answering session and supports nonlinear presentations led by the speaker. Given the slides contents and notes, the system ranks presentation slides based on semantic closeness to spoken utterances, displays the most related slides, and highlights the corresponding content keywords in slide notes. The design of our system was informed by findings from interviews with expert presenters and analysis of recordings of lectures and conference presentations. In a within-subjects study comparing our dynamic support system with a static slide navigation system during a question answering session, presenters expressed a strong preference for our system and answered the questions more efficiently using our system.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {583–593},
numpages = {11},
keywords = {natural language and speech processing, question answering support systems, presentation assistance},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173003,
author = {Wang, Isaac and Narayana, Pradyumna and Smith, Jesse and Draper, Bruce and Beveridge, Ross and Ruiz, Jaime},
title = {EASEL: Easy Automatic Segmentation Event Labeler},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173003},
doi = {10.1145/3172944.3173003},
abstract = {Video annotation is a vital part of research examining gestural and multimodal interaction as well as computer vision, machine learning, and interface design. However, annotation is a difficult, time-consuming task that requires high cognitive effort. Existing tools for labeling and annotation still require users to manually label most of the data, limiting the tools helpfulness. In this paper, we present the Easy Automatic Segmentation Event Labeler (EASEL), a tool supporting gesture analysis. EASEL streamlines the annotation process by introducing assisted annotation, using automatic gesture segmentation and recognition to automatically annotate gestures. To evaluate the efficacy of assisted annotation, we conducted a user study with 24 participants and found that assisted annotation decreased the time needed to annotate videos with no difference in accuracy compared with manual annotation. The results of our study demonstrate the benefit of adding computational intelligence to video and audio annotation tasks.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {595–599},
numpages = {5},
keywords = {gesture segmentation, gesture analysis, data annotation tools},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172990,
author = {Politis, Ioannis and Langdon, Patrick and Adebayo, Damilola and Bradley, Mike and Clarkson, P. John and Skrypchuk, Lee and Mouzakitis, Alexander and Eriksson, Alexander and Brown, James W. H. and Revell, Kirsten and Stanton, Neville},
title = {An Evaluation of Inclusive Dialogue-Based Interfaces for the Takeover of Control in Autonomous Cars},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172990},
doi = {10.1145/3172944.3172990},
abstract = {This paper presents formative research to inform the design of intelligent automotive user interfaces. It describes an evaluation of dialogue-based interfaces, mediating the driver to take back control from the autonomous mode of a car. Four concepts designed to increase driver Situation Awareness were evaluated in a driving simulator. They used dialogue-based interaction, where driving-related information was either asked from or repeated by the driver, with the alternative of a countdown-based interface with no additional information. An inclusive set of participants, with a wide age spectrum, tested the interfaces. The shorter and simpler interaction of the countdown timer was most accepted. The interface seeking answers to driving-related questions came next, and the interface requiring repetition of driving-related information, even when augmented by visual and tactile cues, was least accepted. Design guidelines on utilizing dialogue as a means of keeping the driver in the loop during a takeover were thus derived.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {601–606},
numpages = {6},
keywords = {takeover of control, dialogue system, inclusive design., autonomous cars, evaluation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247916,
author = {Itoh, Takayuki},
title = {Session Details: Session 6B: Social Media and Reccomenders},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247916},
doi = {10.1145/3247916},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172947,
author = {Danone, Yaakov and Kuflik, Tsvi and Mokryn, Osnat},
title = {Visualizing Reviews Summaries as a Tool for Restaurants Recommendation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172947},
doi = {10.1145/3172944.3172947},
abstract = {Online customers opinions about products and services, in the form of reviews, are a major part of today's web culture. However, customers, when looking for a product or service, do not have the time or the desire to read even a small part of the available product reviews (which themselves may be lengthy and not easy to read). Moreover, they often would like to examine reviews of similar products, and get a comprehensive picture of how different aspects of these products compare. In this work, by introducing a generic framework for analyzing and presenting a visual summary based on comparative sentences extracted from customer reviews, we offer the user an easy and intuitive understanding of the differences between a set of products. The contribution of this study is twofold: First, it focuses on reviews of intangible services (using the restaurant domain as a case study), unlike most of the related studies that consider physical products. Second, it combines state-of-the-art text analysis techniques with an intuitive visualization into an easy to use prototype to visualize summarized service comparisons to the users.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {607–616},
numpages = {10},
keywords = {reviews summarization, information visualization, visualizing comparisons},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172955,
author = {Atreja, Shubham and Aggarwal, Pooja and Mohapatra, Prateeti and Dumrewal, Amol and Basu, Anwesh and Dasgupta, Gargi B.},
title = {Citicafe: An Interactive Interface for Citizen Engagement},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172955},
doi = {10.1145/3172944.3172955},
abstract = {Community engagement is a new and emerging trend in urban cities driven by the mission of developing responsible citizenship. The platform ingests data from different sources, which is exploited by a virtual agent to enable informed interactions. It can help citizens to (a) report problems and (b) gather information related to civic issues for different locations and their neighborhoods. We report the results of a user study carried out to establish the effectiveness of our interface and draw a comparison with an existing platform. A detailed qualitative and quantitative analysis of the survey results shows a definite and statistically significant (p &lt; 0.05) preference for our interface over the existing platform.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {617–628},
numpages = {12},
keywords = {CRF, natural language, clustering, social good, knowledge mining, topic modeling, citizen engagement, conversational agent},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172956,
author = {Kremer-Davidson, Shiri and Ronen, Inbal and Leiba, Lior and Kaplan, Avi and Barnea, Maya},
title = {Personal Recommendations for Raising Social Eminence in an Enterprise},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172956},
doi = {10.1145/3172944.3172956},
abstract = {Social media sites have become very popular within large enterprises. Still, employees are experiencing difficulties in engaging efficiently. In this paper, we present a study of a personalized action recommendation system in an enterprise social network. Following a previous study on how to raise one's social eminence in the enterprise and a set of interviews, we built an innovative recommendation system which provides employees with concrete personalized recommendations on how and where to engage. Differently from other systems, it presents recommendations in context of limiting social network behavioral patterns. The recommendations goal is to assist employees in growing out of these patterns. The paper presents the interview findings, the innovative recommendation system and results of a wide survey investigating the effectiveness of such a system.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {629–639},
numpages = {11},
keywords = {action recommendation, social engagement, social eminence, enterprise social media},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3172966,
author = {Liao, Zhenyu and Xian, Yikun and Yang, Xiao and Zhao, Qinpei and Zhang, Chenxi and Li, Jiangfeng},
title = {TSCSet: A Crowdsourced Time-Sync Comment Dataset for Exploration of User Experience Improvement},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3172966},
doi = {10.1145/3172944.3172966},
abstract = {Time-Sync Comment (TSC) is a type of crowdsourced user review embedded in online video websites, which provides better real-time user interaction than traditional user comment type. Various TSC-related problems and approaches have been studied to improve user experience by taking advantage of special characteristics of TSCs such as strong time reliance. However, there are three major drawbacks to these TSC researches. First, they did not explicitly show advantage of TSC features over the traditional features in terms of users' experience. Second, the experiments were conducted on some inconsistent TSC datasets crawled from different source, which makes the effectiveness of their methods less convincing. Third, the methods were manually evaluated by a limited number of so-called "experts" in these experiments, so it is hard for other researchers to obtain the data labels and reproduce the results. In order to overcome these drawbacks, this paper aims to explore the usefulness of TSC data for for the improvement of user experience online by exploiting the TSC pattern inside a new dataset. Specifically, we present a larger-scale TSC dataset with four-level structures and rich self-labeled attributes and formally define a group of TSC-related research problems based on this dataset. The problems are solved by adapted state-of-the-art methods and evaluated through crowdsourced labels in the dataset. The result can be regarded as a baseline for further research.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {641–652},
numpages = {12},
keywords = {crowdsourced time-sync comment, storyline prediction, episode representation learning, hierarchical structured dataset},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173012,
author = {Sato, Masahiro and Ahsan, Budrul and Nagatani, Koki and Sonoda, Takashi and Zhang, Qian and Ohkuma, Tomoko},
title = {Explaining Recommendations Using Contexts},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173012},
doi = {10.1145/3172944.3173012},
abstract = {Recommender systems support user decision-making, and explanations of recommendations further facilitate their usefulness. Previous explanation styles are based on similar users, similar items, demographics of users, and contents of items. Contexts, such as usage scenarios and accompanying persons, have not been used for explanations, although they influence user decisions. In this paper, we propose a context style explanation method, presenting contexts suitable for consuming recommended items. The expected impacts of context style explanations are 1) persuasiveness: recognition of suitable context for usage motivates users to consume items, and 2) usefulness: envisioning context helps users to make right choices because the values of items depend on contexts. We evaluate context style persuasiveness and usefulness by a crowdsourcing-based user study in a restaurant recommendation setting. The context style explanation is compared to demographic and content style explanations. We also combine context style and other explanation styles, confirming that hybrid styles improve persuasiveness and usefulness of explanation.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {659–664},
numpages = {6},
keywords = {context-awareness, recommender system, explanation, user evaluation},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3247917,
author = {Orlosky, Jason and Yamashita, Naomi},
title = {Session Details: Doctoral Consortium},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247917},
doi = {10.1145/3247917},
booktitle = {23rd International Conference on Intelligent User Interfaces},
numpages = {1},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173146,
author = {Lee, Min Hun},
title = {A Technology for Computer-Assisted Stroke Rehabilitation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173146},
doi = {10.1145/3172944.3173146},
abstract = {Improving functional ability after a stroke requires task-oriented physical rehabilitation with the supervision of a therapist. However, with increasing medical costs and a shortage of rehabilitation specialists, post-stroke survivors sometimes receive a limited amount of individual treatment. This paper proposes a low-cost computer-assisted rehabilitation system, called Virtual Coach that evaluates and guides a post-stroke survivor to engage in rehabilitation correctly at home with minimal supervision of a therapist. This system includes two major components: 1) motion analysis modules that evaluate exercise performance and guide a desirable joint trajectory and 2) a dialogue interface to provide feedback. The evaluation function of motion analysis modules computed exercise performance scores of 15 post-stroke survivors and achieved a 78% agreement with a practicing clinician. After developing a guidance function, the usability of the system will be evaluated with post-stroke survivors and therapists and iteratively improved.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {665–666},
numpages = {2},
keywords = {stroke rehabilitation., motion modeling &amp; analysis, uncertainty quantification, intelligent agent},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173147,
author = {Lara-Garduno, Raniero A.},
title = {Machine Learning Behavioral Recognition to Support Neuropsychological Diagnosis of Cognitive Decline},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173147},
doi = {10.1145/3172944.3173147},
abstract = {With nearly 93,500 deaths were attributed to Alzheimer's Disease in 2014 and numbers projected to climb, medical experts have recently placed increasing emphasis in the early detection of Alzheimer's disease and other types of dementia. An open challenge in early detection is developing standardized testing that helps clinicians detect subtle signs of Mild Cognitive Impairment that occur prior to the development of said dementias. In this regard we present our preliminary work on leveraging machine-learning behavioral classification to detect subtle behavioral abnormalities. Our framework consists of detecting behavioral abnormalities through the computerization of an existing paper-based clinical neuropsychological tests as well as the development of novel tests that could only be realized with modern touch tablet technology.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {667–668},
numpages = {2},
keywords = {Alzheimer's disease, clinical neuropsychology, human-computer interaction, machine learning classification, mild cognitive impairment},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173148,
author = {Mauro, Noemi},
title = {Suggestion Models in Geographic Exploratory Search},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173148},
doi = {10.1145/3172944.3173148},
abstract = {My PhD project focuses on the suggestion of information categories in exploratory search in a geographical domain. Geographical maps may challenge the user in the exploration of possibly complex information spaces, making difficult to find all the relevant data for the completion of her/his search task. I propose different models for concepts suggestion which, given a search query, allow the creation of clusters of categories useful for query expansion as a "you might be interested in" function. The training of these models is done by exploiting different types of information: search sessions, users» preferences and social data coming from Twitter.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {669–670},
numpages = {2},
keywords = {query expansion, geographical information retrieval and search, session-based concept suggestion},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173149,
author = {Kim, Bongjun},
title = {Leveraging User Input and Feedback for Interactive Sound Event Detection and Annotation},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173149},
doi = {10.1145/3172944.3173149},
abstract = {Tagging of environment audio events is essential in many areas. However, finding sound events and labeling them within a long audio file is tedious and time-consuming. Building an automatic recognition system using modern machine learning is often not feasible because it requires a large number of human-labeled training examples and it is not reliable enough for all uses. I propose interactive sound event detection to solve the issue by combining machine search with human tagging, specifically focusing on the effectiveness of various types of user-inputs to the interactive sound searching. The types of user inputs that I will explore include binary relevance feedback, segmentation, and vocal imitation. I expect that leveraging one or combination of these user inputs would help users find audio contents of interest quickly and accurately, even in the situation where there are not enough training examples for a typical automated system.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {671–672},
numpages = {2},
keywords = {sound event detection, interactive machine learning, human-in-the-loop system},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173150,
author = {Crowell, Ciera},
title = {Analysis of Interaction Design and Evaluation Methods in Full-Body Interaction for Special Needs},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173150},
doi = {10.1145/3172944.3173150},
abstract = {This work is focused on the specific properties and evaluation of full-body interaction design of multi-user mixed reality environments. The main goal is to study how full-body interaction can aid in intervention strategies for children with autism, to improve their understanding and adoption of social behaviors with peers and with society in general. The research is based upon HCI theory, aided by general theories of embodied cognition, embodiment and developmental psychology. The main setting of the research is large scale floor-projected mixed environments, which will allow for testing interaction strategies and evaluation methods of experiences based on collocation of multiple users within a full-body interactive scenario, where they can practice interaction in a natural and uninhibited manner. The research consists of designing playful experiences for the target users in order to promote socialization, collaboration and social inclusion. Topics for analysis include understanding the dynamics of goal-oriented and open-ended gameplay, proxemics, and encouraged group collaboration, on the design of these systems. Assessment methods take into account multimodal analysis, including physiology-based data such as electrodermal activity and heart rate, of the children's behavioral and affective states in the experience.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {673–674},
numpages = {2},
keywords = {encouraged collaboration, open-ended gameplay, ASD, full-body interaction, autism spectrum disorder},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173151,
author = {Massimo, David},
title = {User Preference Modeling and Exploitation in IoT Scenarios},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173151},
doi = {10.1145/3172944.3173151},
abstract = {Recommender Systems are commonly used in web applications to support users in finding items of their interest. We here propose to use Recommender Systems in Internet of Things scenarios to support human decision making in the physical world. For instance, users' choices (visit actions) for points of interests (POIs) while they explore a sensor enabled city can be tracked and used to generate recommendations for not yet visited POIs. In this PhD research, we propose a novel learning approach for generating an explainable human behaviour model and relevant recommendations in sensor enabled areas. Moreover, we propose techniques for simulating user behaviour and analyse the collective dynamics of a population of users.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {675–676},
numpages = {2},
keywords = {recommender system, behaviour learning, inverse reinforcement learning, internet of things},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173152,
author = {Malla, Adil Hamid and Hammond, Tracy},
title = {Eye Tracking- Single Technology to Handle Multiple Domains},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173152},
doi = {10.1145/3172944.3173152},
abstract = {Interdisciplinary research always yields an enhanced and different perspective on the problem. Solving a problem using many modalities and technologies gives us the complete picture of the problem and the best solution possible. My thesis is about solving the various problem in different domains using the Eye and Gaze Tracking. My research area currently limits to the application of Eye tracking to three domains i.e. Identification and Authentication, Page Ranking based on the Gaze Interaction, and Data Analytics based on the Gaze Interaction data. In the Authentication and Identification, I have developed different authentication systems which use Gaze and Eye tracking to authenticate a user with 97% accuracy. For Page ranking of the pages, I have used the eye-tracking interaction to enhance the ranking of the page/link on Google and Bing backed Search Emulator. This project is in progress and will be mostly done in coming two months. The third and the last project is related to using the Eye tracking and Gaze Interaction data for inferences which range from detecting the unique eye moment pattern of the user, the saccades and micro-saccades in both horizontal as well as vertical directions, moreover detecting the eye diseases using the Gaze Interaction data.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {677–678},
numpages = {2},
keywords = {identification, eye tracking, authentication, page ranking, gaze analysis, data analytics},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173153,
author = {Cherian, Josh},
title = {Automatic Recognition of Hygiene Activities and Personalized Interventions for Chronic Care},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173153},
doi = {10.1145/3172944.3173153},
abstract = {The number of individuals living with chronic conditions continues to rise. As a result, a significant emphasis has been placed on both improving their quality of life as well as decreasing the cost and burden of caring for them. One particularly promising avenue for achieving this is the use of wearable devices, as they have become both affordable and reliable in recognizing fitness activities. However, while the existing algorithms reliably recognize physically intensive activities (e.g., walking vs. swimming), they fail to recognize personal hygiene actives that have more subtle differences (e.g., brushing teeth vs. washing hands). This research aims to develop novel features and intelligent, multi-stage algorithms that can reliably recognize such personal hygiene activities for chronic care. Additionally, we aim to further supplement this activity recognition with personalized interventions that enable individuals to manage their own personal health.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {679–680},
numpages = {2},
keywords = {personalized interventions, activity recognition, human-computer interaction, machine learning},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173154,
author = {Vannaprathip, Narumol},
title = {An Intelligent Tutoring System for Situated Decision Making in Dental Surgery},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173154},
doi = {10.1145/3172944.3173154},
abstract = {We present the design and the partial implementation of a simulation-based intelligent tutoring system for training surgical decision making skill. The simulation combines an immersive virtual environment with a conversational intelligent tutoring system. The pedagogical module has been largely implemented and the result of an initial evaluation of the implemented tutorial interventions is presented.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {681–682},
numpages = {2},
keywords = {knowledge representation, surgical decision making, situation awareness, intelligent tutoring system, pedagogical intervention},
location = {Tokyo, Japan},
series = {IUI '18}
}

@inproceedings{10.1145/3172944.3173156,
author = {Kocielnik, Rafal and Hsieh, Gary},
title = {Facilitating Self-Learning in Behavior Change Through Long-Term Intelligent Conversational Assistance},
year = {2018},
isbn = {9781450349451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172944.3173156},
doi = {10.1145/3172944.3173156},
abstract = {Despite much recent progress in conversational systems, the vision of a truly "intelligent" agent is still far from being realized. Part of the reason is that current applications focus on offering conversational alternatives to GUI supported tasks. In my research I am focusing on application of dialogue-based interaction in area of self-learning in health behavior change, domain in which conversation can offer unique value. Yet, to be effective in this domain a conversational system needs to be "intelligent". In my research I define aspects of intelligence crucial for supporting long-term self-learning in behavior change through conversation and address the technical as well as design challenges of enabling such intelligent applications.},
booktitle = {23rd International Conference on Intelligent User Interfaces},
pages = {683–684},
numpages = {2},
keywords = {human-computer interaction, reflection, self-learning, behavior change, conversational AI, crowd-sourcing},
location = {Tokyo, Japan},
series = {IUI '18}
}

