@inproceedings{10.1145/2559184.2559194,
author = {Ben Shimon, David and Friedman, Michael and Hoerle, Johannes and Tsikinovsky, Alexander and Gude, Roland and Aluchanov, Rodion},
title = {Deploying Recommender System for the Masses},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559194},
doi = {10.1145/2559184.2559194},
abstract = {Many small and mid-sized e-businesses wish to integrate a recommender system into their website. Integrating an existing recommender system to a website often requires certain expertise and programming efforts, thus incurs substantial investments and may not be justified by the added value of the recommender system. This demo presents a solution for integrating a recommender system as a service to an existing e-business without any programming efforts. The integration method is analogue to the way of the Google AdSense integration and the business model is adapted from the advertisements world. Initial feedback from real website owners indicates that such integration has a great benefit for both sides; the website owner and the Recommender System (RS) provider.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {1–4},
numpages = {4},
keywords = {recommender system as a service, collaborative filtering, integration},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559195,
author = {Roitman, Haggai and Raviv, Ariel and Hummel, Shay and Erera, Shai and Konopniki, David},
title = {Microcosm: Visual Discovery, Exploration and Analysis of Social Communities},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559195},
doi = {10.1145/2559184.2559195},
abstract = {Social communities play an important role in many domains. While a lot of attention has been given to developing efficient methods for detecting and analyzing social communities, it still remains a great challenge to provide intuitive search interfaces for end-users who wish to discover and explore such communities. Trying to fill the gaps, in this demonstration we present Microcosm: a holistic solution for visual discovery, exploration and analysis of social communities.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {5–8},
numpages = {4},
keywords = {visualization, social communities, exploration, discovery},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559196,
author = {AlTarawneh, Ragaad and Bauer, Jens and Humayoun, Shah Rukh and Ebert, Achim and Liggesmeyer, Peter},
title = {Enhancing Understanding of Safety Aspects in Embedded Systems through an Interactive Visual Tool},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559196},
doi = {10.1145/2559184.2559196},
abstract = {In this work, we present a demonstration of a visual interactive tool called ESSAVis that helps different engineers in collaborating together for understanding the failure mechanisms in complex embedded systems. ESSAVis provides a 2Dplus3D visual user interface that integrates intuitively between different data sets related with embedded systems failure mechanisms. The tool accepts a CFT model describing a specific hazard in the underlying system, and a CAD model describing the geometry of system components. In this paper, we present different interaction options of ESSAVis that are used for intuitively extracting safety aspects of the underlying embedded system.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {9–12},
numpages = {4},
keywords = {collaborative environment, immersive environment, safety aspects visualization, embedded systems, essavis},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559197,
author = {Sagara, Satoshi and Higuchi, Masakazu and Komuro, Takashi},
title = {Multi-Finger AR Typing Interface for Mobile Devices},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559197},
doi = {10.1145/2559184.2559197},
abstract = {In this paper, we propose a user interface that enables multi-finger typing in the space behind a mobile device. By using the augmented reality (AR) technology, a virtual keyboard is superimposed on the rear camera image, and a hand region of the camera image is again superimposed on that image, which makes it possible to perform input operation as if there were a real keyboard. The system recognizes only key pressing actions and does not recognize a hand or fingers, which enables stable recognition and multi-finger input. Further, key typing at any place on a plane and in the air is possible. Demonstration using an experimental device showed that multi-finger input using a virtual keyboard displayed on the screen was realized.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {13–16},
numpages = {4},
keywords = {augmented reality, multi-finger input, virtual keyboard},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559198,
author = {Unuma, Yuko and Niikura, Takehiro and Komuro, Takashi},
title = {See-through Mobile AR System for Natural 3D Interaction},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559198},
doi = {10.1145/2559184.2559198},
abstract = {In this paper, we propose an interaction system which displays see-through images on the mobile display and that allows a user to interact with virtual objects overlaid on the see-through image using the user's hand. In this system, the camera which tracks the user's viewpoint is attached to the front of the mobile display and the depth camera which captures color and depth images of the user's hand and the background scene is attached to the back of the mobile display. Natural interaction with virtual objects using the user's hand is realized by displaying images so that the appearance of a space through the mobile display is consistent with that of the real space from the user's viewpoint. We implemented two applications to the system and showed the usefulness of this system in various AR applications.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {17–20},
numpages = {4},
keywords = {geometric consistency, augmented reality, mobile device, depth camera},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559200,
author = {Leuski, Anton and Gowrisankar, Rasiga and Richmond, Todd and Shapiro, Ari and Xu, Yuyu and Feng, Andrew},
title = {Mobile Personal Healthcare Mediated by Virtual Humans},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559200},
doi = {10.1145/2559184.2559200},
abstract = {We demonstrate Ally -- a prototype interface for a consumer-level medical diagnostic device. It is an interactive virtual character, -- Virtual Human (VH), -- that listens to user's concern, collects and processes sensor data, offers advice, guides the user through a self-administered medical tests, and answers the user's questions. The primary focus of this demo is on the VH, we describe and demonstrate the technologies for language analysis, dialogue management, response generation and presentation. The sensing and medical decision making components are simulated in the current system, but possible applications and extensions are discussed.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {21–24},
numpages = {4},
keywords = {mobile, interface for healthcare, virtual human},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559201,
author = {Paraskevopoulos, Fotis and Taramigkou, Maria and Bothos, Efthimios and Apostolou, Dimitris and Mentzas, Gregoris},
title = {Creative User Centric Inspirational Search},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559201},
doi = {10.1145/2559184.2559201},
abstract = {The demo paper describes the Creative User Centric Inspirational Search, which aims to leverage user inspiration in information seeking activities. CRUISE is an interactive exploratory search tool that combines diversification of content and sources with a user interface design that visualizes cues from the social chatter generated with microblogging services such as Twitter and lets users interactively explore the available information space. The tool is based on the observation that users often use the social chatter to follow links and initiate information seeking activities which can lead to unexpected discoveries which can in turn inspire them.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {25–28},
numpages = {4},
keywords = {exploratory search, inspirational systems, social information filtering},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559202,
author = {Tintarev, Nava and Kutlak, Roman},
title = {Demo: Making Plans Scrutable with Argumentation and Natural Language Generation},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559202},
doi = {10.1145/2559184.2559202},
abstract = {Autonomous systems perform tasks without human guidance. Techniques for making autonomous systems scrutable and, hence, more transparent are required in order to support humans working with such systems. The Scrutable Autonomous Systems (SAsSy) demo shows a novel way of combining argumentation and natural language to generate a human understandable explanation dialogue. By interacting with SAsSy users are able to ask why a certain plan was selected for execution, why other alternatives were not selected, also allowing users to modify information in the system.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {29–32},
numpages = {4},
keywords = {natural language, agents, argumentation, explanations},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559203,
author = {Impett, Leonardo and Robinson, Peter and Baltrusaitis, Tadas},
title = {A Facial Affect Mapping Engine},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559203},
doi = {10.1145/2559184.2559203},
abstract = {Facial expressions play a crucial role in human interaction. Interactive digital games can help teaching people to both express and recognise them. Such interactive games can benefit from the ability to alter user expressions dynamically and in real-time. In this demonstration, we present the Facial Affect Mapping Engine (FAME), a framework for mapping and manipulating facial expressions across images and video streams. Our system is fully automatic runs in real-time and does not require any specialist hardware. FAME presents new possibilities for the designers of intelligent interactive digital games.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {33–36},
numpages = {4},
keywords = {intelligent games., augmented reality, facial puppetry, face tracking, facial affect, face swapping},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559204,
author = {Wecker, Alan J. and Minkov, Einat and Mokryn, Osnat and Lanir, Joel and Kuflik, Tsvi},
title = {Visualizing Sentiment: Do You See What i Mean?},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559204},
doi = {10.1145/2559184.2559204},
abstract = {Many tools exist for extracting and visualizing key information from a corpus of text documents. However often, one would like to assess the sentiment and feelings that arise from a single document. This paper describes an interactive service that visualizes the sentiment of a specific document. The service enables the user to visualize the sentimental polarity of each paragraph to get a detailed impression; to quickly detect the polarity of emotional words; to identify subjective sentences within the text, and the grade level of language used in each sentence. Participants in an initial qualitative evaluation found the service fast and useful.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {37–40},
numpages = {4},
keywords = {sentiment analysis, emotion polarity, subjectivity, visualize, user interface},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559205,
author = {Gatti, Lorenzo and Guerini, Marco and Stock, Oliviero and Strapparava, Carlo},
title = {SUBVERTISER: Mocking Ads through Mobile Phones},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559205},
doi = {10.1145/2559184.2559205},
abstract = {As advertisements on posters in the street get more and more aggressive, our basic cognitive defense -aimed at not perceiving those messages- is not enough. One advanced defensive technique is based on transforming the perceived message into something different from what was originally meant in the message. The demo is based on an application for smartphones that creatively modifies the linguistic expression in a virtual copy of a poster. The mobile system is inspired by the counter-cultural art practice of "subvertising", and aims at experiencing aesthetic pleasure that relaxes the cognitive tension of the user.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {41–44},
numpages = {4},
keywords = {mobile apps, computational humour, affective nlp},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559940,
author = {Schnelle-Walka, Dirk and Huber, Jochen and Radomski, Stefan and Brdiczka, Oliver and Luyten, Kris and M\"{u}hlh\"{a}user, Max},
title = {SmartObjects: Third Workshop on Interacting with Smart Objects},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559940},
doi = {10.1145/2559184.2559940},
abstract = {The increasing number of smart objects in our everyday life shapes how we interact beyond the desktop. In this workshop we discuss how interaction with these smart objects should be designed from various perspectives. },
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {45–46},
numpages = {2},
keywords = {hci, multimodal and adapter interaction, embodied interaction, tangible interaction, enabling techologies, novel interaction, smart objects, context-awareness},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559941,
author = {Oomen, Johan and Aroyo, Lora and Gena, Cristina and Wecker, Alan},
title = {Personalized Access to Cultural Heritage (PATCH2014): The Future of Experiencing Cultural Heritage},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559941},
doi = {10.1145/2559184.2559941},
abstract = {Since 2007, the PATCH workshop series have been gathering successfully researchers and professionals from various countries and institutions to discuss the topics of digital access to cultural heritage and specifically the personalization aspects in this process. Due to this rich history, the reach of the PATCH workshop in various research communities is extensive.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {47–48},
numpages = {2},
keywords = {mobile, personalized},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559943,
author = {Paletta, Lucas and Schuller, Bjoern W. and Robinson, Peter and Sabouret, Nicolas},
title = {IDGEI 2014: 2nd International Workshop on Intelligent Digital Games for Empowerment and Inclusion},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559943},
doi = {10.1145/2559184.2559943},
abstract = {Digital Games for Empowerment and Inclusion have the potential to improve our society by preparing particular groups of people to meet social challenges in their everyday lives, and to do so in an enjoyable way through games. These games are developing rapidly to exploit new algorithms for computational intelligence supported by increasing availability of computing power to help analyze players' behavior, monitor their motivation and interest, and to adapt the progress of the games accordingly. Intelligent Digital Games for Empowerment and Inclusion (IDGEI) explore the use of machine intelligence in serious digital games. In this introduction and in this context, we summarize the second international workshop on IDGEI held at the International Conference on Intelligent User Interfaces (IUI) 2014.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {49–50},
numpages = {2},
keywords = {serious games, machine intelligence, digital games for empowerment and inclusion, affective computing},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559944,
author = {Davis, Richard C. and Adler, Aaron},
title = {Sketch: Pen and Touch Recognition},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559944},
doi = {10.1145/2559184.2559944},
abstract = {Sketch recognition has technically been around for 40 years, but it has come and gone several times due to the difficulty of the problem. With the rise of touch and pen enabled phones and tablets, sketch recognition is regaining popularity and public presence, and more people are becoming aware of and interested in this difficult, but valuable, problem. It is important to harness the Sketch Recognition community at this time to encourage the flourishing of this topic.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {51–52},
numpages = {2},
keywords = {sketch, touch, pen, recognition},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559185,
author = {Taele, Paul and Hammond, Tracy},
title = {Developing Sketch Recognition and Interaction Techniques for Intelligent Surfaceless Sketching User Interfaces},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559185},
doi = {10.1145/2559184.2559185},
abstract = {As commercial motion-tracking sensors achieve greater reliability and ubiquity, intelligent sketching user interfaces can expand beyond traditional surface environments for richer surfaceless sketching interactions. However, relevant techniques for automatically recognizing sketches in surfaceless interaction spaces are either largely constrained, due to limited gesture input vocabularies from existing gesture recognition techniques; or unexplored, due to being adapted specifically for surface environments by existing sketch recognition techniques. This dissertation research therefore proposes to investigate techniques for developing intelligent surfaceless sketching user interfaces. The core research work will focus on investigating automated recognition techniques for better understanding the content of surfaceless sketches, and determining optimal interaction techniques for improving related intuitive sketching cues in those surfaceless interaction spaces.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {53–56},
numpages = {4},
keywords = {sketch recognition, natural user interfaces, surfaceless interaction},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559186,
author = {de Rooij, Alwin},
title = {Toward Emotion Regulation via Physical Interaction},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559186},
doi = {10.1145/2559184.2559186},
abstract = {Emotions can be regulated to fit a task in order to enhance task performance. Motor expressions can help regulate emotion. This paper briefly reports ongoing work on the design of physical interactions based on motor expressions that can help regulate emotion to fit a task. We argue that to be effective, such interactions must be made meaningful in relation to ongoing appraisal processes, and that such interactions can help regulate emotion via congruence, suppression, or incompatibility. We present previous work on the validation of these arguments within the context of supporting idea generation, and develop a roadmap for research that aims to translate these results to the design of physical interactions under device constraints. The research will enable designers of interactive technology to develop physical interactions that help regulate emotion with the aim to help people get the most out of their own capabilities.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {57–60},
numpages = {4},
keywords = {emotion regulation, affective computing, emotion elicitation, embodied interaction, motor expression},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559187,
author = {Yogev, Sivan},
title = {Exploratory Search Interfaces: Blending Relevance, Diversity, Relationships and Categories},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559187},
doi = {10.1145/2559184.2559187},
abstract = {Exploratory search of scientific literature plays an essential part of a researcher's work. Efforts to provide interfaces supporting this task accomplished significant progress, but the field is open for further evolution. In this paper I present four basic design concepts identified in exploratory search interfaces: relevance, diversity, relationships and categories, and propose a novel browsing layout featuring a unique combination of these concepts.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {61–64},
numpages = {4},
keywords = {exploratory search interfaces, citation networks},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559188,
author = {Schiavo, Gianluca},
title = {Socially-Aware Interfaces for Supporting Co-Located Interaction},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559188},
doi = {10.1145/2559184.2559188},
abstract = {Ambient intelligence refers to a vision of technology where physical environments are sensitive and responsive to people. One of the challenges to realize this vision is to leverage information available in the social context. My doctoral research focuses on how to design interfaces that support co-located multi-user interactions taking into account individual and group nonverbal behavior, such as proxemics, gaze direction and body movements. In particular, the research activities are twofold: to understand which nonverbal cues and social signals reflect engagement, cooperation and cohesion in co-located group activities and to design systems that can handle and manage this social information. I present an integrated research approach for designing multi-user interactions based on social signal processing and I discuss the progress-to-date toward the development of systems that can sense and respond to social context.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {65–68},
numpages = {4},
keywords = {co-located interaction, multi-user interfaces, social context, nonverbal behavior, context-awareness},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559189,
author = {Gu, Yecheng and Sosnovsky, Sergey},
title = {Recognition of Student Intentions in a Virtual Reality Training Environment},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559189},
doi = {10.1145/2559184.2559189},
abstract = {This paper introduces a novel method for detecting and modeling intentions of students performing training tasks in a Virtual Reality (VR) environment enhanced with intelligent tutoring capabilities. Our VR-setup provides students with an immersive user interface, but produces noisy and low-level input, from which we need to recognize higher-level cognitive information about the student. The complexity of this task is amplified by the requirements of the target domain (child pedestrian safety), where students need to train complex skills in dynamic settings. We present an approach for this task, which combines the logic-based Event Calculus (EC) and probabilistic modeling.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {69–72},
numpages = {4},
keywords = {intention recognition, virtual reality, intelligent tutoring systems, student modeling},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559190,
author = {Matsumoto, Mariko},
title = {Silent Speech Decoder Using Adaptive Collection},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559190},
doi = {10.1145/2559184.2559190},
abstract = {We investigated a classification method using brain computer interfaces (BCIs) for silent speech. Event-related potentials (ERPs) obtained when four subjects imagined the vocalization of two Japanese vowels while they remained silent and immobilized were recorded. We used an adaptive collection (AC) that adaptively selects suitable output signals of common spatial patterns (CSP) filters and its time duration for classification. The classification accuracies (CAs) were 73-92% for the pairwise classification /a/ vs. /u/ in the use of 63 channels and significantly better than previous study.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {73–76},
numpages = {4},
keywords = {common spatial patter (csp), support vector machine (svm), adaptive collection, eeg, brain computer interface (bci), silent speech},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559191,
author = {Lopez-Tovar, Hugo and Dowell, John},
title = {A Non-Command Interface for Automatic Document Provision during Meetings},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559191},
doi = {10.1145/2559184.2559191},
abstract = {This research presents the concept of a non-command interface for a smart room to automatically detect when people talk about a document and whether it is present or not, as a fundamental prerequisite for missing document provision that doesn't require explicit requests, avoiding distraction from the main discourse. A study on how observers judge document usage in meetings is presented as a baseline and the conceptual framework is briefly explained. Finally, an exploratory experiment is reported. These elements demonstrate the research feasibility and define the techniques needed to build the agent.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {77–80},
numpages = {4},
keywords = {intelligent user interfaces, meetings, proactive and agent-based paradigms for user interaction, ambient intelligence, computer supported cooperative work},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559192,
author = {Smith, Kirsten},
title = {Supporting Carers through Intelligent Technology},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559192},
doi = {10.1145/2559184.2559192},
abstract = {Informal carers lack adequate practical and emotional support. This PhD investigates how a software agent could be used to help maintain a carer's personal social network by mediating communication and facilitating the provision of emotional and practical support. The agent should use features of the carer and their social network to provide a personalized support interface.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {81–84},
numpages = {4},
keywords = {social networks, ehealth, agents, emotional support},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

@inproceedings{10.1145/2559184.2559193,
author = {Jain, Devyani and Hariharan Kala, Manikandan},
title = {Wearable Audio Journal and Mobile Application to Capture Automatic Thoughts in Patients Undergoing Cognitive Behavioral Therapy},
year = {2014},
isbn = {9781450327299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559184.2559193},
doi = {10.1145/2559184.2559193},
abstract = {By replacing the hand-written 'thought records', used by Cognitive behavioral therapy (CBT) patients, with a Wearable audio journal that works in tandem with a smartphone, we can help patients capture their automatic thoughts at the moment of its occurrence and facilitate in the posterior analysis of the data along with the therapist. Speech provides richer clues about the emotional state of mind of the patient and thus could possibly help in better therapy.},
booktitle = {Proceedings of the Companion Publication of the 19th International Conference on Intelligent User Interfaces},
pages = {85–88},
numpages = {4},
keywords = {mindfulness, reflective journalism, technology facilitated self-awareness, wearable technology, aids for cognitive behavioral therapy},
location = {Haifa, Israel},
series = {IUI Companion '14}
}

