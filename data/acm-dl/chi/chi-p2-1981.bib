@inproceedings{10.1145/800275.810922,
author = {Greenberg, Edward A. and Ivey, Wm. Max and Lewis, Bruce R.},
title = {Comparison of Some Available Packages for Use in Research Data Management},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810922},
doi = {10.1145/800275.810922},
abstract = {Data management features of SIR, SAS, and SPSS were applied to a sample hierarchical data base. For each package, the areas investigated included the logical definition of the data base, data entry, data retrieval, data integrity, security, reporting, and updating.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {1–8},
numpages = {8},
keywords = {SAS, SIR, SPSS, Data base, Data retrieval, Information retrieval, Data management},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810922,
author = {Greenberg, Edward A. and Ivey, Wm. Max and Lewis, Bruce R.},
title = {Comparison of Some Available Packages for Use in Research Data Management},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810922},
doi = {10.1145/1015528.810922},
abstract = {Data management features of SIR, SAS, and SPSS were applied to a sample hierarchical data base. For each package, the areas investigated included the logical definition of the data base, data entry, data retrieval, data integrity, security, reporting, and updating.},
journal = {SIGSOC Bull.},
month = may,
pages = {1–8},
numpages = {8},
keywords = {SIR, SPSS, Information retrieval, Data base, SAS, Data management, Data retrieval}
}

@inproceedings{10.1145/800275.810923,
author = {Beveridge, Andrew A. and Norris, Jennifer A.},
title = {Organizing the Annual Housing Surveys as a Very Large Relationally Oriented Data Base},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810923},
doi = {10.1145/800275.810923},
abstract = {Since 1973, the Department of Housing and Urban Development, through the Bureau of the Census, has conducted a yearly nationwide survey of housing. Data on a wide range of topics are collected during face to face interviews with over 190,000 individuals. Plainly, the Annual Housing Surveys represent one of the largest longitudinal general social and economic data collection efforts ever undertaken.Due to changing policy and substantive interests, as well as government requirements, the interview schedules have changed significantly from year to year. Since the great potential for data from the Annual Housing Survey is in longitudinal analysis, it is necessary to have common variable definitions and consistent formats.To accomplish this, we have developed and implemented a system which includes: 1) documentation of the variables, questionnaires, and files across all years and surveys; 2) files created using one homogeneously defined data structure; 3) a simple system to produce custom user files; 4) a method to easily produce routine custom analyses and tabulations using the data.We have applied the relational model to create a small data base which documents the interview schedules, files and variable definitions. From this we produce up to date documentation and computer programs which are used to update the Annual Housing Survey data base, to handle custom file requests, and to perform analyses.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {9–15},
numpages = {7},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810923,
author = {Beveridge, Andrew A. and Norris, Jennifer A.},
title = {Organizing the Annual Housing Surveys as a Very Large Relationally Oriented Data Base},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810923},
doi = {10.1145/1015528.810923},
abstract = {Since 1973, the Department of Housing and Urban Development, through the Bureau of the Census, has conducted a yearly nationwide survey of housing. Data on a wide range of topics are collected during face to face interviews with over 190,000 individuals. Plainly, the Annual Housing Surveys represent one of the largest longitudinal general social and economic data collection efforts ever undertaken.Due to changing policy and substantive interests, as well as government requirements, the interview schedules have changed significantly from year to year. Since the great potential for data from the Annual Housing Survey is in longitudinal analysis, it is necessary to have common variable definitions and consistent formats.To accomplish this, we have developed and implemented a system which includes: 1) documentation of the variables, questionnaires, and files across all years and surveys; 2) files created using one homogeneously defined data structure; 3) a simple system to produce custom user files; 4) a method to easily produce routine custom analyses and tabulations using the data.We have applied the relational model to create a small data base which documents the interview schedules, files and variable definitions. From this we produce up to date documentation and computer programs which are used to update the Annual Housing Survey data base, to handle custom file requests, and to perform analyses.},
journal = {SIGSOC Bull.},
month = may,
pages = {9–15},
numpages = {7}
}

@inproceedings{10.1145/800275.810924,
author = {Cohn, Richard M. and Prouse, Howard R.},
title = {The 1940 and 1950 Public Use Sample Project: Data Quality Issues},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810924},
doi = {10.1145/800275.810924},
abstract = {The 1940 and 1950 Public Use Sample Project is the creation of 1/100 household samples from the 1940 and 1950 Censuses of Population. The data source for the samples is the microfilmed original Population Schedules which contain the census enumerator's recording of household information. The procedure to sample the universe of household listings and transcribe the sample households' data is described in the paper. A pretest of the 1940 Public Use Sample included a comparison of three methods of sampling and transcription. The results of this comparison are reported. The applicability of these procedures to similar projects is discussed.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {16–19},
numpages = {4},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810924,
author = {Cohn, Richard M. and Prouse, Howard R.},
title = {The 1940 and 1950 Public Use Sample Project: Data Quality Issues},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810924},
doi = {10.1145/1015528.810924},
abstract = {The 1940 and 1950 Public Use Sample Project is the creation of 1/100 household samples from the 1940 and 1950 Censuses of Population. The data source for the samples is the microfilmed original Population Schedules which contain the census enumerator's recording of household information. The procedure to sample the universe of household listings and transcribe the sample households' data is described in the paper. A pretest of the 1940 Public Use Sample included a comparison of three methods of sampling and transcription. The results of this comparison are reported. The applicability of these procedures to similar projects is discussed.},
journal = {SIGSOC Bull.},
month = may,
pages = {16–19},
numpages = {4}
}

@inproceedings{10.1145/800275.810925,
author = {Gordon, Christopher J. and Zartman, Michael B.},
title = {The Automation of Data Processing, Analysis, and Reporting in a Large Survey Time-Series Database.},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810925},
doi = {10.1145/800275.810925},
abstract = {The May 1981 Survey will mark the 152nd Survey of Consumer Attitudes. Initiated in 1946, the purpose of the surveys is to measure changes in consumer attitudes and expectations, to understand why these changes occur, and to evaluate how they relate to consumer decisions to save, to borrow, or to make discretionary purchases under changing conditions.Each survey contains approximately 40 core questions, each of which probes a different aspect of consumer confidence. Open-ended questions are asked concerning evaluations of expectations about personal finances, employment, price changes, and the national business situation. Additional questions probe for the respondents appraisal of present market conditions for houses, and other durables. Demographic data obtained in these surveys include income, age, sex, race, education, and occupation, among others. While many questions designed to measure change in attitudes and behavior are repeated in identical form in each survey, special questionnaire supplements are added to most surveys by outside sponsors on a time share basis. Supplements to the ongoing surveys give sponsors prompt turnaround to survey materials while taking advantage of shared field expenses. When the research task is first undertaken, a maximum amount of time and effort can be spent in developing these survey materials, not in establishing and setting in motion standard sampling and interviewing procedures, questionnaire and code development for standard demographic items, and so forth.Although each survey task is unique in its time requirements, shared time participation on the ongoing Surveys of Consumer Attitudes is an effective and flexible approach for meeting many research needs. Current procedures include production of a fully documented computer data file available for analytic use within 48 hours of the close of interviewing. Within one week of the close of the survey, a report containing tabulations and charts of questions asked is sent to the sponsors.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {20–23},
numpages = {4},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810925,
author = {Gordon, Christopher J. and Zartman, Michael B.},
title = {The Automation of Data Processing, Analysis, and Reporting in a Large Survey Time-Series Database.},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810925},
doi = {10.1145/1015528.810925},
abstract = {The May 1981 Survey will mark the 152nd Survey of Consumer Attitudes. Initiated in 1946, the purpose of the surveys is to measure changes in consumer attitudes and expectations, to understand why these changes occur, and to evaluate how they relate to consumer decisions to save, to borrow, or to make discretionary purchases under changing conditions.Each survey contains approximately 40 core questions, each of which probes a different aspect of consumer confidence. Open-ended questions are asked concerning evaluations of expectations about personal finances, employment, price changes, and the national business situation. Additional questions probe for the respondents appraisal of present market conditions for houses, and other durables. Demographic data obtained in these surveys include income, age, sex, race, education, and occupation, among others. While many questions designed to measure change in attitudes and behavior are repeated in identical form in each survey, special questionnaire supplements are added to most surveys by outside sponsors on a time share basis. Supplements to the ongoing surveys give sponsors prompt turnaround to survey materials while taking advantage of shared field expenses. When the research task is first undertaken, a maximum amount of time and effort can be spent in developing these survey materials, not in establishing and setting in motion standard sampling and interviewing procedures, questionnaire and code development for standard demographic items, and so forth.Although each survey task is unique in its time requirements, shared time participation on the ongoing Surveys of Consumer Attitudes is an effective and flexible approach for meeting many research needs. Current procedures include production of a fully documented computer data file available for analytic use within 48 hours of the close of interviewing. Within one week of the close of the survey, a report containing tabulations and charts of questions asked is sent to the sponsors.},
journal = {SIGSOC Bull.},
month = may,
pages = {20–23},
numpages = {4}
}

@inproceedings{10.1145/800275.810926,
author = {Austin, Erik W. and Barge, Sylvia J. and Horvath, Susan M. and Traugott, Santa M.},
title = {A New Process for Documenting and Checking Archival Data},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810926},
doi = {10.1145/800275.810926},
abstract = {The Inter-university Consortium for Political and Social Research (ICPSR) is a data archive and repository for social science data. A major function of the ICPSR is to disseminate the data holdings in a reasonably standard format. For holdings that will be extensively used, additional effort is made to prepare comprehensive, machine-readable documentation, to cross-check the documentation against the data for accuracy and consistency, and to correct or document any inconsistencies discovered.In the past, "cleaning" and documenting the data involved using a number of different computer programs. A great deal of human time was expended on procedural matters: which programs to use, when to use them, and how to coordinate the various stages of the cleaning process. As staff costs and the number of new acquisitions skyrocketed, and computers increased in power and decreased in cost, it became imperative to automate as much as possible the procedure for preparing data for distribution. The GIDO software was developed to meet this need.GIDO is an interactive multi-function program package that guides staff members through the procedure for documenting and cleaning social science data. A cohesive history of the processing operations performed on the data is maintained automatically in machine-readable form. Video terminals are used to display "forms" which the staff fill out with the textual and technical documentation for the data. GIDO immediately verifies the contents of each form and provides an opportunity to make corrections. The forms allow the input of information without requiring knowledge of specialized syntax and conventions. After all documentary materials have been entered, GIDO checks the data for consistency with the original documentation, corrects or flags discrepancies encountered, reformats the data using uniform conventions, and produces machine-readable documentation in a form ready for dissemination.Use of GIDO enables the ICPSR archive to perform its data processing functions more efficiently and at lower cost, thus permitting the organization to meet ever-increasing demands on its resources.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {24–31},
numpages = {8},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810926,
author = {Austin, Erik W. and Barge, Sylvia J. and Horvath, Susan M. and Traugott, Santa M.},
title = {A New Process for Documenting and Checking Archival Data},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810926},
doi = {10.1145/1015528.810926},
abstract = {The Inter-university Consortium for Political and Social Research (ICPSR) is a data archive and repository for social science data. A major function of the ICPSR is to disseminate the data holdings in a reasonably standard format. For holdings that will be extensively used, additional effort is made to prepare comprehensive, machine-readable documentation, to cross-check the documentation against the data for accuracy and consistency, and to correct or document any inconsistencies discovered.In the past, "cleaning" and documenting the data involved using a number of different computer programs. A great deal of human time was expended on procedural matters: which programs to use, when to use them, and how to coordinate the various stages of the cleaning process. As staff costs and the number of new acquisitions skyrocketed, and computers increased in power and decreased in cost, it became imperative to automate as much as possible the procedure for preparing data for distribution. The GIDO software was developed to meet this need.GIDO is an interactive multi-function program package that guides staff members through the procedure for documenting and cleaning social science data. A cohesive history of the processing operations performed on the data is maintained automatically in machine-readable form. Video terminals are used to display "forms" which the staff fill out with the textual and technical documentation for the data. GIDO immediately verifies the contents of each form and provides an opportunity to make corrections. The forms allow the input of information without requiring knowledge of specialized syntax and conventions. After all documentary materials have been entered, GIDO checks the data for consistency with the original documentation, corrects or flags discrepancies encountered, reformats the data using uniform conventions, and produces machine-readable documentation in a form ready for dissemination.Use of GIDO enables the ICPSR archive to perform its data processing functions more efficiently and at lower cost, thus permitting the organization to meet ever-increasing demands on its resources.},
journal = {SIGSOC Bull.},
month = may,
pages = {24–31},
numpages = {8}
}

@inproceedings{10.1145/800275.810927,
author = {Bixby, Tina G. and Vavra, Janet K.},
title = {An Automated System for Responding to Data Service Requests},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810927},
doi = {10.1145/800275.810927},
abstract = {During the 1970s, there was a steady decline in the cost and size of computing hardware with a corresponding phenomenal growth in computing capability. Computers now help store, manage, duplicate and interpret vast quantities of data with an ease and relative economy undreamed of in the past. These developments have, over the years, fostered the growth of new research methods in a variety of fields, including the social sciences. Large and more complex bodies of quantitative data have been collected as social scientists seek ways to understand human behavior with empirical research methods and scientific sampling techniques. In addition to collecting their own data, researchers have also utilized vast amounts of machine-readable data that have been prepared by other researchers, governmental agencies, and private organizations.The changes in the computing industry combined with the increased demand for services have made it feasible for organizations to consider automating as many tasks as possible. FAST (Facility to Aid Servicing Transactions) is one system that was created in response to these conditions. This paper describes FAST and its impact on the organization which developed it.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {32–35},
numpages = {4},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810927,
author = {Bixby, Tina G. and Vavra, Janet K.},
title = {An Automated System for Responding to Data Service Requests},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810927},
doi = {10.1145/1015528.810927},
abstract = {During the 1970s, there was a steady decline in the cost and size of computing hardware with a corresponding phenomenal growth in computing capability. Computers now help store, manage, duplicate and interpret vast quantities of data with an ease and relative economy undreamed of in the past. These developments have, over the years, fostered the growth of new research methods in a variety of fields, including the social sciences. Large and more complex bodies of quantitative data have been collected as social scientists seek ways to understand human behavior with empirical research methods and scientific sampling techniques. In addition to collecting their own data, researchers have also utilized vast amounts of machine-readable data that have been prepared by other researchers, governmental agencies, and private organizations.The changes in the computing industry combined with the increased demand for services have made it feasible for organizations to consider automating as many tasks as possible. FAST (Facility to Aid Servicing Transactions) is one system that was created in response to these conditions. This paper describes FAST and its impact on the organization which developed it.},
journal = {SIGSOC Bull.},
month = may,
pages = {32–35},
numpages = {4}
}

@inproceedings{10.1145/800275.810928,
author = {Janda, Ann and Janda, Kenneth},
title = {Online Searches of Social Science Data Sets: The RIQS System and ICPSR Data},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810928},
doi = {10.1145/800275.810928},
abstract = {Every solution seems to generate a new problem. The problem of accurately assessing public opinion led to the invention of the sample survey. The subsequent problem of analyzing survey responses brought widespread use of machine-readable data. The problem of preserving machine-readable data for secondary analysis stimulated the creation of data depositories or "archives." Growth over time in the holdings of these social science data archives, however, has aroused needs for improved retrieval of data. This paper explains one method of dealing with such needs. It involves an interactive search of the holdings of the most diversified social science data archive, the Inter-University Consortium for Political and Social Research, using a general-purpose information retrieval system, RIQS, written for CDC computers.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {36–44},
numpages = {9},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810928,
author = {Janda, Ann and Janda, Kenneth},
title = {Online Searches of Social Science Data Sets: The RIQS System and ICPSR Data},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810928},
doi = {10.1145/1015528.810928},
abstract = {Every solution seems to generate a new problem. The problem of accurately assessing public opinion led to the invention of the sample survey. The subsequent problem of analyzing survey responses brought widespread use of machine-readable data. The problem of preserving machine-readable data for secondary analysis stimulated the creation of data depositories or "archives." Growth over time in the holdings of these social science data archives, however, has aroused needs for improved retrieval of data. This paper explains one method of dealing with such needs. It involves an interactive search of the holdings of the most diversified social science data archive, the Inter-University Consortium for Political and Social Research, using a general-purpose information retrieval system, RIQS, written for CDC computers.},
journal = {SIGSOC Bull.},
month = may,
pages = {36–44},
numpages = {9}
}

@inproceedings{10.1145/800275.810929,
author = {Zartman, Michael B. and Gordon, Christopher J.},
title = {Developing an Aggregated Survey/Macro-Economic Database for Statistical and Graphical Social Science Applications},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810929},
doi = {10.1145/800275.810929},
abstract = {The Survey Research Center at The University of Michigan has routinely conducted surveys of consumer attitudes since 1946. The May 1981 survey is the 152nd in this series which provides regular assessments of consumer attitudes and expectations. The surveys are designed to explore why changes in consumer attitudes and expectations occur, and how these changes influence consumer spending and saving decisions. A major research objective of the project is to use this collected data to evaluate economic trends and prospects.Each survey contains "standard" questions asked at regular intervals, many of which have been included from the project's inception. The aggregated results of these surveys provide a wealth of time-series data with the potential to be an important factor in forecasting consumer behavior. The "standard" questions themselves can be disseminated into approximately 190 separate data series (including index transformations). When "nonstandard" (or non-core) questions are included, this total jumps considerably. With such a large number of data variables, many different areas of analysis are available to be researched. When the many macro-economic data series (e.g., Federal Reserve, Census, or Retail Sales data) are added to this compilation, the data management problems increase. The research results which could be achieved, then, are directly related to the development of a flexible method of data storage and retrieval.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {45–47},
numpages = {3},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810929,
author = {Zartman, Michael B. and Gordon, Christopher J.},
title = {Developing an Aggregated Survey/Macro-Economic Database for Statistical and Graphical Social Science Applications},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810929},
doi = {10.1145/1015528.810929},
abstract = {The Survey Research Center at The University of Michigan has routinely conducted surveys of consumer attitudes since 1946. The May 1981 survey is the 152nd in this series which provides regular assessments of consumer attitudes and expectations. The surveys are designed to explore why changes in consumer attitudes and expectations occur, and how these changes influence consumer spending and saving decisions. A major research objective of the project is to use this collected data to evaluate economic trends and prospects.Each survey contains "standard" questions asked at regular intervals, many of which have been included from the project's inception. The aggregated results of these surveys provide a wealth of time-series data with the potential to be an important factor in forecasting consumer behavior. The "standard" questions themselves can be disseminated into approximately 190 separate data series (including index transformations). When "nonstandard" (or non-core) questions are included, this total jumps considerably. With such a large number of data variables, many different areas of analysis are available to be researched. When the many macro-economic data series (e.g., Federal Reserve, Census, or Retail Sales data) are added to this compilation, the data management problems increase. The research results which could be achieved, then, are directly related to the development of a flexible method of data storage and retrieval.},
journal = {SIGSOC Bull.},
month = may,
pages = {45–47},
numpages = {3}
}

@inproceedings{10.1145/800275.810930,
author = {Meyer, Garry S.},
title = {On-Line Manipulation of Small Area Demographic Data: AmericanProfile<sup>sm</sup>},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810930},
doi = {10.1145/800275.810930},
abstract = {A new approach to the way in which users interact with the computer in an on-line environment is presented. The method is designed specifically to provide both a friendly and highly productive means of communicating user requirements. Unlike systems which are targeted toward either novice computer users or experienced programmers, the approach we take is well suited for all levels along this continuum. AmericanProfilesm, a system which provides access to demographic and economic data for both standard geo-political units of analysis (states, counties, SMSA's, Zip codes, etc.) and unique small areas (polygons, circles, etc.), is discussed as an actual case point to illustrate our approach.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {48–57},
numpages = {10},
keywords = {Small area demographics, Flex-prompt, American Profilesm, On-line, User-friendly, Census data, Interactive},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810930,
author = {Meyer, Garry S.},
title = {On-Line Manipulation of Small Area Demographic Data: AmericanProfile<sup>sm</sup>},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810930},
doi = {10.1145/1015528.810930},
abstract = {A new approach to the way in which users interact with the computer in an on-line environment is presented. The method is designed specifically to provide both a friendly and highly productive means of communicating user requirements. Unlike systems which are targeted toward either novice computer users or experienced programmers, the approach we take is well suited for all levels along this continuum. AmericanProfilesm, a system which provides access to demographic and economic data for both standard geo-political units of analysis (states, counties, SMSA's, Zip codes, etc.) and unique small areas (polygons, circles, etc.), is discussed as an actual case point to illustrate our approach.},
journal = {SIGSOC Bull.},
month = may,
pages = {48–57},
numpages = {10},
keywords = {On-line, Small area demographics, American Profilesm, Census data, User-friendly, Flex-prompt, Interactive}
}

@inproceedings{10.1145/800275.1016160,
author = {Hofland, Knut and Arhus, Sigbjorn},
title = {A TEXT-RETRIEVAL SYSTEM USED IN HUMANISTIC ARCHIVE APPLICATIONS (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.1016160},
doi = {10.1145/800275.1016160},
abstract = {NOVA*STATUS is a text-retrieval system which is available at all
Norwegian universities and several government institutions, running
on computers from different manufacturers.NOVA*STATUS is a full-text retrieval system originally developed
by AERE, Harwell, England, and redeveloped at the Norwegian
Computing Centre for the Humanities and other Norwegian
institutions. The data is divided into documents and each word (or
a truncated part of it) in each document is potentially a key word.
The format of the document is free. By use of prefixes it is
possible to divide each document into specific fields of
information.A request to the system can consist of a Boolean expression of
words and prefixes and relational expressions between prefixed
words. The system allows for macros which can be permanently stored
which makes, for example, synonym lists possible. The system also
contains procedures for off-line sorting and printing of catalogues
and for the coding of data for statistical analysis by SPSS.At the Norwegian Computing Centre for the Humanities,
NOVA*STATUS has been, and is still being used in a variety of
humanistic archive applications. Several of these have a common
data format which consists of 20-30 defined fields of fixed
information and one or more fields of free-text description of
e.g., photographs, paintings, archaeological and cultural
artifacts, old buildings and documents. The second part of the
paper will describe the actual use of the system in these various
applications.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {58–59},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@inproceedings{10.1145/800275.810933,
author = {Malin, Morton V. and Dean, Martha C.},
title = {Bibliometric Analysis of Isi's Arts &amp; Humanities Citation Index},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810933},
doi = {10.1145/800275.810933},
abstract = {Bibliometrics, the quantitative study of literature, has made a considerable contribution to the management of scientific libraries and to the understanding of the sociology of science. It has joined operations research as effective management tools in making substantial impact on optimization of library services, resources sharing and allocation in library networks. With the common belief that humanistic literatures are different, these analytic techniques have not been applied to any extent. We believe that although sociological factors may impact on the humanities in different ways, patterns of regularity in communication, individual contribution, and literature growth shall be studied. Knowledge of the characteristics of humanistic literatures may enable us to improve information access and to maximize available resources in space, manpower, and funds for books. A pilot study has been attempted on a bibliography of the history of the American Revolution.The data base is constructed from the Writings on American History: A Subject Bibliography of Articles under the heading "Revolution &amp; Confederation (1763-1789)". To facilitate the analysis, the data management program FAMULUS was used.Selected Results: 1095 articles were published between 1962-1976 by 790 authors in 224 journals or publishing sources. The average author productivity is 1.39 which is similar to other fields studied. As expected, a few highly prolific authors dominated the field, and their productivity distribution follows the well-known Lotka's Law. The dispersion of publications over journals also adheres to Bradford's distribution, identifying the most productive journal in this field as the William and Mary Quarterly. Thus, scholarship in general assumes certain common characteristics regardless of the discipline. Yet differences exist. There are only 77 (10%) authors who have ever co-authored in a mere 47 (4%) of the total literature. This is a dramatic departure from the intensely collaborative enterprise of science in which almost all scientific writers co-author.From our experience, FAMULUS is a useful tool in bibliometric analysis in addition to being a good personal documentation system to replace our shoebox of cards. We have also used it in the teaching of indexing and retrieval of documents.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {58},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810933,
author = {Malin, Morton V. and Dean, Martha C.},
title = {Bibliometric Analysis of Isi's Arts &amp; Humanities Citation Index},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810933},
doi = {10.1145/1015528.810933},
abstract = {Bibliometrics, the quantitative study of literature, has made a considerable contribution to the management of scientific libraries and to the understanding of the sociology of science. It has joined operations research as effective management tools in making substantial impact on optimization of library services, resources sharing and allocation in library networks. With the common belief that humanistic literatures are different, these analytic techniques have not been applied to any extent. We believe that although sociological factors may impact on the humanities in different ways, patterns of regularity in communication, individual contribution, and literature growth shall be studied. Knowledge of the characteristics of humanistic literatures may enable us to improve information access and to maximize available resources in space, manpower, and funds for books. A pilot study has been attempted on a bibliography of the history of the American Revolution.The data base is constructed from the Writings on American History: A Subject Bibliography of Articles under the heading "Revolution &amp; Confederation (1763-1789)". To facilitate the analysis, the data management program FAMULUS was used.Selected Results: 1095 articles were published between 1962-1976 by 790 authors in 224 journals or publishing sources. The average author productivity is 1.39 which is similar to other fields studied. As expected, a few highly prolific authors dominated the field, and their productivity distribution follows the well-known Lotka's Law. The dispersion of publications over journals also adheres to Bradford's distribution, identifying the most productive journal in this field as the William and Mary Quarterly. Thus, scholarship in general assumes certain common characteristics regardless of the discipline. Yet differences exist. There are only 77 (10%) authors who have ever co-authored in a mere 47 (4%) of the total literature. This is a dramatic departure from the intensely collaborative enterprise of science in which almost all scientific writers co-author.From our experience, FAMULUS is a useful tool in bibliometric analysis in addition to being a good personal documentation system to replace our shoebox of cards. We have also used it in the teaching of indexing and retrieval of documents.},
journal = {SIGSOC Bull.},
month = may,
pages = {58},
numpages = {2}
}

@inproceedings{10.1145/800275.810932,
author = {Pao, Miranda Lee},
title = {Bibliometric Analysis of Amrican History Data by FAMULUS (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810932},
doi = {10.1145/800275.810932},
abstract = {NOVA*STATUS is a text-retrieval system which is available at all Norwegian universities and several government institutions, running on computers from different manufacturers.NOVA*STATUS is a full-text retrieval system originally developed by AERE, Harwell, England, and redeveloped at the Norwegian Computing Centre for the Humanities and other Norwegian institutions. The data is divided into documents and each word (or a truncated part of it) in each document is potentially a key word. The format of the document is free. By use of prefixes it is possible to divide each document into specific fields of information.A request to the system can consist of a Boolean expression of words and prefixes and relational expressions between prefixed words. The system allows for macros which can be permanently stored which makes, for example, synonym lists possible. The system also contains procedures for off-line sorting and printing of catalogues and for the coding of data for statistical analysis by SPSS.At the Norwegian Computing Centre for the Humanities, NOVA*STATUS has been, and is still being used in a variety of humanistic archive applications. Several of these have a common data format which consists of 20-30 defined fields of fixed information and one or more fields of free-text description of e.g., photographs, paintings, archaeological and cultural artifacts, old buildings and documents. The second part of the paper will describe the actual use of the system in these various applications.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {58},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810932,
author = {Pao, Miranda Lee},
title = {Bibliometric Analysis of Amrican History Data by FAMULUS (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810932},
doi = {10.1145/1015528.810932},
abstract = {NOVA*STATUS is a text-retrieval system which is available at all Norwegian universities and several government institutions, running on computers from different manufacturers.NOVA*STATUS is a full-text retrieval system originally developed by AERE, Harwell, England, and redeveloped at the Norwegian Computing Centre for the Humanities and other Norwegian institutions. The data is divided into documents and each word (or a truncated part of it) in each document is potentially a key word. The format of the document is free. By use of prefixes it is possible to divide each document into specific fields of information.A request to the system can consist of a Boolean expression of words and prefixes and relational expressions between prefixed words. The system allows for macros which can be permanently stored which makes, for example, synonym lists possible. The system also contains procedures for off-line sorting and printing of catalogues and for the coding of data for statistical analysis by SPSS.At the Norwegian Computing Centre for the Humanities, NOVA*STATUS has been, and is still being used in a variety of humanistic archive applications. Several of these have a common data format which consists of 20-30 defined fields of fixed information and one or more fields of free-text description of e.g., photographs, paintings, archaeological and cultural artifacts, old buildings and documents. The second part of the paper will describe the actual use of the system in these various applications.},
journal = {SIGSOC Bull.},
month = may,
pages = {58–59},
numpages = {2}
}

@inproceedings{10.1145/800275.810931,
author = {Malin, Morton V. and Dean, Martha C.},
title = {Bibliometric Analysis of ISI's Arts &amp; Humanities Citation Index (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810931},
doi = {10.1145/800275.810931},
abstract = {The most frequently cited journal articles in the Arts &amp; Humanities Citation Index (A&amp;HCI) are analyzed in terms of their disciplinary classification. Four years (1976-79) of the A&amp;HCI data base yielded 144 journal articles which were cited 10 or more times during the period. These articles are predominantly from the disciplines of Language and Linguistics (31%), Philosophy (23%), History (13%), Religion (8%), and Archeology (6%).However, most citations in the A&amp;HCI are to books rather than journal articles (96% versus 4% among those items cited 10 or more times). The discipline of Literature (or Literary Criticism) is predominant in the list of highly cited books.These statistics suggest systematic variation in the resources used by the different disciplines. Various other aspects of the A&amp;HCI data base are explored, and some specific examples are discussed in depth.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {58},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810931,
author = {Malin, Morton V. and Dean, Martha C.},
title = {Bibliometric Analysis of ISI's Arts &amp; Humanities Citation Index (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810931},
doi = {10.1145/1015528.810931},
abstract = {The most frequently cited journal articles in the Arts &amp; Humanities Citation Index (A&amp;HCI) are analyzed in terms of their disciplinary classification. Four years (1976-79) of the A&amp;HCI data base yielded 144 journal articles which were cited 10 or more times during the period. These articles are predominantly from the disciplines of Language and Linguistics (31%), Philosophy (23%), History (13%), Religion (8%), and Archeology (6%).However, most citations in the A&amp;HCI are to books rather than journal articles (96% versus 4% among those items cited 10 or more times). The discipline of Literature (or Literary Criticism) is predominant in the list of highly cited books.These statistics suggest systematic variation in the resources used by the different disciplines. Various other aspects of the A&amp;HCI data base are explored, and some specific examples are discussed in depth.},
journal = {SIGSOC Bull.},
month = may,
pages = {58–59},
numpages = {2}
}

@inproceedings{10.1145/800275.810934,
author = {Castonguay, Denis},
title = {A Thesaurus for Canadian Iconography (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810934},
doi = {10.1145/800275.810934},
abstract = {The Picture Division of The Public Archives of Canada has undertaken the construction of a thesaurus of iconographic terms as part of its preparations for a computerized inventory system. The thesaurus which complements an existing set of descriptive standards will serve as a terminological control device enabling indexers and researchers to translate natural language into a more restrained and logical system language. General characteristics of the thesaurus will be described. Special emphasis will be given to the impact of on-line information retrieval computer technology on the design and development of the system language. Sample pages of the thesaurus will be available for examination and further discussion. An overview of other Canadian experiments in the field of subject access to visual records will also be provided.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {59–60},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810934,
author = {Castonguay, Denis},
title = {A Thesaurus for Canadian Iconography (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810934},
doi = {10.1145/1015528.810934},
abstract = {The Picture Division of The Public Archives of Canada has undertaken the construction of a thesaurus of iconographic terms as part of its preparations for a computerized inventory system. The thesaurus which complements an existing set of descriptive standards will serve as a terminological control device enabling indexers and researchers to translate natural language into a more restrained and logical system language. General characteristics of the thesaurus will be described. Special emphasis will be given to the impact of on-line information retrieval computer technology on the design and development of the system language. Sample pages of the thesaurus will be available for examination and further discussion. An overview of other Canadian experiments in the field of subject access to visual records will also be provided.},
journal = {SIGSOC Bull.},
month = may,
pages = {59–60},
numpages = {2}
}

@inproceedings{10.1145/800275.810935,
author = {Preston, Michael},
title = {The Text's the Thing: Concordances to Literary Texts (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810935},
doi = {10.1145/800275.810935},
abstract = {The history of computer-generated concordances is already one-third of a century long. Thousands of concordances have been generated; many have been published. Most of these are useful, but there are limitations to all of them. In this presentation I discuss a number of variations on concordance-making based on specific projects being carried out at the University of Colorado.A word-form concordance can be of considerable utility. Particularly for older states of language of which our knowledge is often less than perfect, this "primary" concordance form seems best for initial circulation, but such a concordance is insensitive to variants and ambiguities. It is often as suggestive of what might have been done as it is directly useful.With the increasing availability of microcomputers and various kinds of remote terminals, it is now possible to remove many of the difficulties of text-editing so that a "secondary" concordance edited toward particular applications can be produced more readily. At the University of Colorado, at which the majority of humanists who use computers wish to make maximum use of the available technology without becoming computing scientists, I have found it practical to suggest a particular synthesis of batch and interactive computing. This involves the use of a retrieval, concordance-generating, and editing system so modular in design that editorial intervention is practical at many points. This editing makes use of device-dependent text editors of sufficient sophistication that the user perceives little of the technical operation beyond requesting his programs and his text; otherwise he has the freedom of a typewriter coupled to the benefits of a screen for displaying modifications to his text as they are made, whether directly by him or by a variety of programmed functions. Stations built around "smart" terminals as well as "dumb" terminals with microcomputer and floppy disks are operational.Thus it is now more practical to produce second-generation concordances which more nearly reflect the perceived needs of a scholarly community: words may be (manually) disambiguated by meaning and function, contexts may be edited either to omit extraneous material or insert explanatory matter, and words may be clustered by dictionary or thesaurus. The result is concordances of far greater utility in specific areas and more meaningful statistics.The development of better equipment and new techniques has made it possible to interact more thoroughly with one's text. There is no need for premature data reduction, but rather the encouragement of what I call the "infinite loop of literary scholarship": one works with one's texts to produce results which suggesst work to produce more results which suggest still more work .... The newer technology seems to fit the humanist far better than did the old.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {59},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810935,
author = {Preston, Michael},
title = {The Text's the Thing: Concordances to Literary Texts (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810935},
doi = {10.1145/1015528.810935},
abstract = {The history of computer-generated concordances is already one-third of a century long. Thousands of concordances have been generated; many have been published. Most of these are useful, but there are limitations to all of them. In this presentation I discuss a number of variations on concordance-making based on specific projects being carried out at the University of Colorado.A word-form concordance can be of considerable utility. Particularly for older states of language of which our knowledge is often less than perfect, this "primary" concordance form seems best for initial circulation, but such a concordance is insensitive to variants and ambiguities. It is often as suggestive of what might have been done as it is directly useful.With the increasing availability of microcomputers and various kinds of remote terminals, it is now possible to remove many of the difficulties of text-editing so that a "secondary" concordance edited toward particular applications can be produced more readily. At the University of Colorado, at which the majority of humanists who use computers wish to make maximum use of the available technology without becoming computing scientists, I have found it practical to suggest a particular synthesis of batch and interactive computing. This involves the use of a retrieval, concordance-generating, and editing system so modular in design that editorial intervention is practical at many points. This editing makes use of device-dependent text editors of sufficient sophistication that the user perceives little of the technical operation beyond requesting his programs and his text; otherwise he has the freedom of a typewriter coupled to the benefits of a screen for displaying modifications to his text as they are made, whether directly by him or by a variety of programmed functions. Stations built around "smart" terminals as well as "dumb" terminals with microcomputer and floppy disks are operational.Thus it is now more practical to produce second-generation concordances which more nearly reflect the perceived needs of a scholarly community: words may be (manually) disambiguated by meaning and function, contexts may be edited either to omit extraneous material or insert explanatory matter, and words may be clustered by dictionary or thesaurus. The result is concordances of far greater utility in specific areas and more meaningful statistics.The development of better equipment and new techniques has made it possible to interact more thoroughly with one's text. There is no need for premature data reduction, but rather the encouragement of what I call the "infinite loop of literary scholarship": one works with one's texts to produce results which suggesst work to produce more results which suggest still more work .... The newer technology seems to fit the humanist far better than did the old.},
journal = {SIGSOC Bull.},
month = may,
pages = {59},
numpages = {2}
}

@inproceedings{10.1145/800275.810936,
author = {Reigem, Oystein},
title = {Sift - Searching in Free Text: A Text Retrieval System (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810936},
doi = {10.1145/800275.810936},
abstract = {The SIFT project is aimed at developing an advanced text retrieval system possessing the features of high modularity, high portability, possibilities for integration with word processing systems and a flexible user interface. Possible applications for such a system would be found wherever any sizable collection of information requires efficient retrieval. The SIFT system is mainly designed to solve the problems of searching in free, i.e. unstructured, text but extensive functions for dealing with structured information are also offered.The SIFT project is based on former experience in the use of other retrieval systems, particularly the Norwegian version of the British STATUS system, NOVA*STATUS, a system which has found application in various public agencies and at all Norwegian universities.The SIFT project was initiated on January 1, 1980, and a prototype version of the system will be implemented on a NORD computer towards the end of 1981. The final product will be made available free of charge.This presentation will treat the structure, characteristics and applications of the SIFT system.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {59},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810936,
author = {Reigem, Oystein},
title = {Sift - Searching in Free Text: A Text Retrieval System (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810936},
doi = {10.1145/1015528.810936},
abstract = {The SIFT project is aimed at developing an advanced text retrieval system possessing the features of high modularity, high portability, possibilities for integration with word processing systems and a flexible user interface. Possible applications for such a system would be found wherever any sizable collection of information requires efficient retrieval. The SIFT system is mainly designed to solve the problems of searching in free, i.e. unstructured, text but extensive functions for dealing with structured information are also offered.The SIFT project is based on former experience in the use of other retrieval systems, particularly the Norwegian version of the British STATUS system, NOVA*STATUS, a system which has found application in various public agencies and at all Norwegian universities.The SIFT project was initiated on January 1, 1980, and a prototype version of the system will be implemented on a NORD computer towards the end of 1981. The final product will be made available free of charge.This presentation will treat the structure, characteristics and applications of the SIFT system.},
journal = {SIGSOC Bull.},
month = may,
pages = {59},
numpages = {2}
}

@inproceedings{10.1145/800275.810938,
author = {Fik, Eleanor E.},
title = {Thesaurus on American Works of Art (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810938},
doi = {10.1145/800275.810938},
abstract = {FOLK is an online analysis and retrieval system developed for the 1801 census of Norway, which is machine readable in a full-text and a coded version, each with approximately 1,000,000 records on individuals. The main advantage of the system is speed. FOLK consists of several parts:- A fast program for statistical analysis of the simple kind. Cross tabulations can be done in 1/16 of the time used by SPSS.- An interface to statistical and graphical packages for more complicated analysis.- A retrieval system for finding subsets of the data base. The subset can be anything from a single person to a region. Information from the coded and the full text version can be used for subtracting individuals.- A recoding system for recoding the coded version using a simple semantic analysis of the full-text version.- A fully computerized record linkage system. Information from other sources can be automatically added to the records on the individuals in the census.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {60},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810938,
author = {Fik, Eleanor E.},
title = {Thesaurus on American Works of Art (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810938},
doi = {10.1145/1015528.810938},
abstract = {FOLK is an online analysis and retrieval system developed for the 1801 census of Norway, which is machine readable in a full-text and a coded version, each with approximately 1,000,000 records on individuals. The main advantage of the system is speed. FOLK consists of several parts:- A fast program for statistical analysis of the simple kind. Cross tabulations can be done in 1/16 of the time used by SPSS.- An interface to statistical and graphical packages for more complicated analysis.- A retrieval system for finding subsets of the data base. The subset can be anything from a single person to a region. Information from the coded and the full text version can be used for subtracting individuals.- A recoding system for recoding the coded version using a simple semantic analysis of the full-text version.- A fully computerized record linkage system. Information from other sources can be automatically added to the records on the individuals in the census.},
journal = {SIGSOC Bull.},
month = may,
pages = {60},
numpages = {2}
}

@inproceedings{10.1145/800275.810937,
author = {Davies, Paul Beynon},
title = {The Role of the Computer in Ethnographic Analysis (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810937},
doi = {10.1145/800275.810937},
abstract = {Art and architecture literature presents indexing difficulties due to the absence of a recognized controlled vocabulary. A recent investigation showed a number of independent partial efforts targeted to local needs. The Art and Architecture Thesaurus (AAT) group is building on the experience of others to create a unified, hierarchical thesaurus for these fields. Although the thesaurus itself will be in machine "readable form, the real value of automation will be the ability to search hierarchically the literature indexed with the AAT.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {60},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810937,
author = {Davies, Paul Beynon},
title = {The Role of the Computer in Ethnographic Analysis (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810937},
doi = {10.1145/1015528.810937},
abstract = {Art and architecture literature presents indexing difficulties due to the absence of a recognized controlled vocabulary. A recent investigation showed a number of independent partial efforts targeted to local needs. The Art and Architecture Thesaurus (AAT) group is building on the experience of others to create a unified, hierarchical thesaurus for these fields. Although the thesaurus itself will be in machine "readable form, the real value of automation will be the ability to search hierarchically the literature indexed with the AAT.},
journal = {SIGSOC Bull.},
month = may,
pages = {60},
numpages = {2}
}

@inproceedings{10.1145/800275.810939,
author = {Mohlot, Pat},
title = {Art and Architecture Thesaurus (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810939},
doi = {10.1145/800275.810939},
abstract = {Designing and providing subject access to works of art has traditionally been very subjective. This is due to the fact that there are no standardized word lists which can be expected to meet the scope of all art collections. Whereas tailoring subject terms to the scope of a given collection is the most practical approach for the curator in charge of the collection, the researcher, who may not be an art specialist, often is frustrated when the listing of subject terms does not include the terms relevant for his/her purposes. This presentation will explore how the development of a thesaurus resolves the conflict of subject vocabulary. Specific examples will be drawn from four computer projects at the National Museum of American Art. Each of the projects varies in scope, yet a single subject classification guide has been developed for purposes of providing subject access to the contents of each project.Because a separate subject word list was not originally designed for each project, a thesaurus is now being developed which will allow for a listing of terms not used for indexing but which are relevant to both the scope of each project and anticipated researcher needs.From a practical viewpoint, the presentation will demonstrate how the computer can be used to generate terminology to be included in the thesaurus.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {60},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810939,
author = {Mohlot, Pat},
title = {Art and Architecture Thesaurus (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810939},
doi = {10.1145/1015528.810939},
abstract = {Designing and providing subject access to works of art has traditionally been very subjective. This is due to the fact that there are no standardized word lists which can be expected to meet the scope of all art collections. Whereas tailoring subject terms to the scope of a given collection is the most practical approach for the curator in charge of the collection, the researcher, who may not be an art specialist, often is frustrated when the listing of subject terms does not include the terms relevant for his/her purposes. This presentation will explore how the development of a thesaurus resolves the conflict of subject vocabulary. Specific examples will be drawn from four computer projects at the National Museum of American Art. Each of the projects varies in scope, yet a single subject classification guide has been developed for purposes of providing subject access to the contents of each project.Because a separate subject word list was not originally designed for each project, a thesaurus is now being developed which will allow for a listing of terms not used for indexing but which are relevant to both the scope of each project and anticipated researcher needs.From a practical viewpoint, the presentation will demonstrate how the computer can be used to generate terminology to be included in the thesaurus.},
journal = {SIGSOC Bull.},
month = may,
pages = {60},
numpages = {2}
}

@inproceedings{10.1145/800275.810940,
author = {Oldervoll, Jan},
title = {Folk (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810940},
doi = {10.1145/800275.810940},
abstract = {Ethnography is a methodology which emphasises a "soft" interpretative approach to social reality. It is often portrayed as being at the opposite pole to quantitative approaches as exemplified in the classic Merton-Lazarsfeld paradigm (Structural-Functionalism wedded to the survey method).Ethnography is a method in which the researcher actively engages in and records the life of a social group. This record of experience is essentially qualitative. It is primarily constructed in the form of textual description: an ongoing account of a person's observations, thoughts and feelings while in the "field". This text is usually given the generic title of field-notes.The Ethnographic researcher is therefore normally confronted with a vast amount of textual data. To get some understanding of, and control over this data, the Ethnographer must in some way split up this record of raw experience. He must in some way "chunk" up his data into easily manageable units or categories. It is this classificatory activity which forms the basis of Ethnographic analysis.In greater detail, Ethnographic data analysis may be generally portrayed as consisting of three analytically distinct, but empirically indistinct activities: represents a1. The reading of field-notes, accompanied by the recording of themes and hypotheses;2. The coding of important topics observed within the field-notes under different category headings;3. The disassembling of field-notes by coded category; the purpose being the creative filing and retrieving of one's data.The prime concern of this presentation will be to discuss means by which such analysis may be accomplished.It is the author's belief that the schema shown below possible evolutionary trend in Ethnographic data analysis: items lower down the schema give the Ethnographer greater power and flexibility in the way he handles text. Reference will be made to presently ongoing research at Cardiff as evidence of this claim.1. The Traditional Filing Cabinet.a. Simple chronological filing of text.b. Multiple filing: the actual disassembling of text into files.2. The Filing cabinet and Separate Indices.a. Chronological filing: card indices.b. Chronological filing: specialised indices.c. Chronological filing: computer indices.3. The Full Computer Approach.a. Indices and fieldnotes stored on UNIX.b. A System of Personalised Interactive Computing for Ethnographers. SPICE: a term purely invented to emphasise the "spice" of Ethnographic research.Finally, this presentation will also discuss the implications that this research has for textual management in general. The projected computer arrangement will, I believe, prove of advantage not only to the Ethnographer, but to any researcher who employs continuous text as his/her primary resource.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {60},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810940,
author = {Oldervoll, Jan},
title = {Folk (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810940},
doi = {10.1145/1015528.810940},
abstract = {Ethnography is a methodology which emphasises a "soft" interpretative approach to social reality. It is often portrayed as being at the opposite pole to quantitative approaches as exemplified in the classic Merton-Lazarsfeld paradigm (Structural-Functionalism wedded to the survey method).Ethnography is a method in which the researcher actively engages in and records the life of a social group. This record of experience is essentially qualitative. It is primarily constructed in the form of textual description: an ongoing account of a person's observations, thoughts and feelings while in the "field". This text is usually given the generic title of field-notes.The Ethnographic researcher is therefore normally confronted with a vast amount of textual data. To get some understanding of, and control over this data, the Ethnographer must in some way split up this record of raw experience. He must in some way "chunk" up his data into easily manageable units or categories. It is this classificatory activity which forms the basis of Ethnographic analysis.In greater detail, Ethnographic data analysis may be generally portrayed as consisting of three analytically distinct, but empirically indistinct activities: represents a1. The reading of field-notes, accompanied by the recording of themes and hypotheses;2. The coding of important topics observed within the field-notes under different category headings;3. The disassembling of field-notes by coded category; the purpose being the creative filing and retrieving of one's data.The prime concern of this presentation will be to discuss means by which such analysis may be accomplished.It is the author's belief that the schema shown below possible evolutionary trend in Ethnographic data analysis: items lower down the schema give the Ethnographer greater power and flexibility in the way he handles text. Reference will be made to presently ongoing research at Cardiff as evidence of this claim.1. The Traditional Filing Cabinet.a. Simple chronological filing of text.b. Multiple filing: the actual disassembling of text into files.2. The Filing cabinet and Separate Indices.a. Chronological filing: card indices.b. Chronological filing: specialised indices.c. Chronological filing: computer indices.3. The Full Computer Approach.a. Indices and fieldnotes stored on UNIX.b. A System of Personalised Interactive Computing for Ethnographers. SPICE: a term purely invented to emphasise the "spice" of Ethnographic research.Finally, this presentation will also discuss the implications that this research has for textual management in general. The projected computer arrangement will, I believe, prove of advantage not only to the Ethnographer, but to any researcher who employs continuous text as his/her primary resource.},
journal = {SIGSOC Bull.},
month = may,
pages = {60},
numpages = {2}
}

@inproceedings{10.1145/800275.810942,
author = {Dixon, Rebecca D. and Meyers, Edmund D.},
title = {Initial Experiences with an On-Line Catalog (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810942},
doi = {10.1145/800275.810942},
abstract = {P-STAT began as a collection of integrated commands which read rectangular files sequentially. A simple modification language for recoding and case selection was added in the late 1960's. Commands were added throughout the 1970's. Recently, however, a major effort has gone into language enhancement and file structure improvements.P-RADE, a random access data enhancement to P-STAT, is an example. This type of file structure supports up to 10 indexing keys, allowing a case or a group of cases to be accessed very rapidly. In addition, any P-STAT command can read a P-RADE file sequentially in any key order.In some ways, this approach blends aspects of database technology into statistical software. Examples of its use will be given.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {61},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810942,
author = {Dixon, Rebecca D. and Meyers, Edmund D.},
title = {Initial Experiences with an On-Line Catalog (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810942},
doi = {10.1145/1015528.810942},
abstract = {P-STAT began as a collection of integrated commands which read rectangular files sequentially. A simple modification language for recoding and case selection was added in the late 1960's. Commands were added throughout the 1970's. Recently, however, a major effort has gone into language enhancement and file structure improvements.P-RADE, a random access data enhancement to P-STAT, is an example. This type of file structure supports up to 10 indexing keys, allowing a case or a group of cases to be accessed very rapidly. In addition, any P-STAT command can read a P-RADE file sequentially in any key order.In some ways, this approach blends aspects of database technology into statistical software. Examples of its use will be given.},
journal = {SIGSOC Bull.},
month = may,
pages = {61},
numpages = {1}
}

@inproceedings{10.1145/800275.810941,
author = {Buhler, Roald},
title = {New File Management in P-STAT (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810941},
doi = {10.1145/800275.810941},
abstract = {In January of 1981, the Center for the Study of Youth Development initiated an on-line catalog of the holdings of its specialized library consisting of 10,000 monographs, journals, vertical file materials, etc. The present paper discusses the reactions of the end-user or patron population to the resource. The background of the library automation project -- including issues of cost-effectiveness, increased power, and user utility--is discussed in order to establish the initial goals of this activity. Then, attention is given to how the project was implemented; this includes a comparison of preliminary goals with what ultimately was delivered. The transition from a COM catalog to the on-line catalog required training of patrons (some of whom had little or no experience with a computer terminal), and only half of the Center staff participated in the initial training sessions. Preliminary patron behavior is reviewed, and an attempt to informally analyze both positive and negative experiences is offered. The initial experiences are summarized in a discussion of the problems and prospects of the user interface of the "query" portion of the on-line catalog software.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {61},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810941,
author = {Buhler, Roald},
title = {New File Management in P-STAT (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810941},
doi = {10.1145/1015528.810941},
abstract = {In January of 1981, the Center for the Study of Youth Development initiated an on-line catalog of the holdings of its specialized library consisting of 10,000 monographs, journals, vertical file materials, etc. The present paper discusses the reactions of the end-user or patron population to the resource. The background of the library automation project -- including issues of cost-effectiveness, increased power, and user utility--is discussed in order to establish the initial goals of this activity. Then, attention is given to how the project was implemented; this includes a comparison of preliminary goals with what ultimately was delivered. The transition from a COM catalog to the on-line catalog required training of patrons (some of whom had little or no experience with a computer terminal), and only half of the Center staff participated in the initial training sessions. Preliminary patron behavior is reviewed, and an attempt to informally analyze both positive and negative experiences is offered. The initial experiences are summarized in a discussion of the problems and prospects of the user interface of the "query" portion of the on-line catalog software.},
journal = {SIGSOC Bull.},
month = may,
pages = {61},
numpages = {1}
}

@inproceedings{10.1145/800275.810943,
author = {Benenson, Harold and Just, Steven},
title = {Urben Research in Ethnic, Demographic and Household-Economic Structures with Small Area, Micro-Databases (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810943},
doi = {10.1145/800275.810943},
abstract = {Computerized U.S. Census data has been most widely used for (1)
employment, fertility, demographic and stratification research
involving Public Use Sample (PUS) microdata on the national level,
and (2) applied research (for planning, administration, marketing,
and other applications) with summary (aggregated) data for
localized (i.e., block, tract, community, etc.) geographic units. A
third, highly productive avenue of research, involving Census PUS
micro-data for localized urban units (i.e., SMSAs, counties and
especially selected large-city neighborhoods), has not received the
attention it merits, either among sophisticated public data users
or among novice users.Three forms of current or future small area,
Census microdata constitute resources for urban research. First,
conventional 1970 Census PUS data sets are available for counties
and/or SMSAs (with minimum populations of 250,000). Second, special
tabulations for the two largest U.S. cities permit analysis of 1970
household and person records group by (sub-county) urban
neighborhoods (27 in New York City; 12 in Chicago). Third, 1980
Census microdata, by allowing identification of geographic areas of
smaller population size (100,000 population), will vastly expand
the applications of localized research with the conventional PUS or
special tabulations. In addition, the 1980 PUS microdata will, for
the first time, allow comparative time-series analyses of county
(or SMSA) area populations, over the 1970-1980 decade.In contrast
to national PUS microdata research, local level analyses have the
advantages of (1) smaller data set size and processing costs, (2)
more immediate integration of computerized research hypotheses with
additional sources of (qualitative) information and questions
(stemming from direct knowledge of the communities studied), and
(3) increased ability to zero in on specialized ethnic,
occupational-industrial, migrant, age, etc. urban population groups
which are disproportionately represented in particular local
environments. Our own research projects (at various stages of
development) which attempt to exploit these advantages include
computerized analysis of:1. Patterns of household composition, and
source and structure of family income, among Upper East Side and
Upper West Side Manhattan residents with family incomes of $
50,000. or more (as reported in the 1970 Census)2. Employment
patterns of married women of Cuban immigrant background, in
relation to family class position and period of immigration, for
Hudson County, New Jersey3. Contrasts in the occupational positions
and household patterns of first-generation and second-generation
husbands and wives of Italian background in a New York City working
class community (Astoria-Long Island City, Queens)4. Wives'
employment patterns in relation to ethnic background and husbands'
occupations and income levels in a working class community located
in a manufacturing center (South Side, Chicago)5. Change in the
social and demographic characteristics of succeeding groups of
migrants to an expanding "sunbelt" metropolitan area (Albuquerque,
New Mexico)6. Contrasts in local housing markets and housing
availability, involving analysis of the number and characteristics
of vacant housing units for New Jersey counties.These, as well as
other projects we have assisted, have been undertaken with varied
software resources, including packages (such as CENTS-AID) with
unique hierarchical file processing capabilities, as well as more
versatile (non-hierarchical), general purpose packages (such as
SPSS). The advantages and research applications of small area,
micro-databases can be realized with a range of software techniques
and user-formulated research strategies.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {62},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810943,
author = {Benenson, Harold and Just, Steven},
title = {Urben Research in Ethnic, Demographic and Household-Economic Structures with Small Area, Micro-Databases (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810943},
doi = {10.1145/1015528.810943},
abstract = {Computerized U.S. Census data has been most widely used for (1)
employment, fertility, demographic and stratification research
involving Public Use Sample (PUS) microdata on the national level,
and (2) applied research (for planning, administration, marketing,
and other applications) with summary (aggregated) data for
localized (i.e., block, tract, community, etc.) geographic units. A
third, highly productive avenue of research, involving Census PUS
micro-data for localized urban units (i.e., SMSAs, counties and
especially selected large-city neighborhoods), has not received the
attention it merits, either among sophisticated public data users
or among novice users.Three forms of current or future small area,
Census microdata constitute resources for urban research. First,
conventional 1970 Census PUS data sets are available for counties
and/or SMSAs (with minimum populations of 250,000). Second, special
tabulations for the two largest U.S. cities permit analysis of 1970
household and person records group by (sub-county) urban
neighborhoods (27 in New York City; 12 in Chicago). Third, 1980
Census microdata, by allowing identification of geographic areas of
smaller population size (100,000 population), will vastly expand
the applications of localized research with the conventional PUS or
special tabulations. In addition, the 1980 PUS microdata will, for
the first time, allow comparative time-series analyses of county
(or SMSA) area populations, over the 1970-1980 decade.In contrast
to national PUS microdata research, local level analyses have the
advantages of (1) smaller data set size and processing costs, (2)
more immediate integration of computerized research hypotheses with
additional sources of (qualitative) information and questions
(stemming from direct knowledge of the communities studied), and
(3) increased ability to zero in on specialized ethnic,
occupational-industrial, migrant, age, etc. urban population groups
which are disproportionately represented in particular local
environments. Our own research projects (at various stages of
development) which attempt to exploit these advantages include
computerized analysis of:1. Patterns of household composition, and
source and structure of family income, among Upper East Side and
Upper West Side Manhattan residents with family incomes of $
50,000. or more (as reported in the 1970 Census)2. Employment
patterns of married women of Cuban immigrant background, in
relation to family class position and period of immigration, for
Hudson County, New Jersey3. Contrasts in the occupational positions
and household patterns of first-generation and second-generation
husbands and wives of Italian background in a New York City working
class community (Astoria-Long Island City, Queens)4. Wives'
employment patterns in relation to ethnic background and husbands'
occupations and income levels in a working class community located
in a manufacturing center (South Side, Chicago)5. Change in the
social and demographic characteristics of succeeding groups of
migrants to an expanding "sunbelt" metropolitan area (Albuquerque,
New Mexico)6. Contrasts in local housing markets and housing
availability, involving analysis of the number and characteristics
of vacant housing units for New Jersey counties.These, as well as
other projects we have assisted, have been undertaken with varied
software resources, including packages (such as CENTS-AID) with
unique hierarchical file processing capabilities, as well as more
versatile (non-hierarchical), general purpose packages (such as
SPSS). The advantages and research applications of small area,
micro-databases can be realized with a range of software techniques
and user-formulated research strategies.},
journal = {SIGSOC Bull.},
month = may,
pages = {62},
numpages = {1}
}

@inproceedings{10.1145/800275.810944,
author = {Whinston, Andrew B.},
title = {Beyond Cataloging Functions for Art Museum Data Banks (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810944},
doi = {10.1145/800275.810944},
abstract = {A data management system for art museums is presented. In addition
to providing conventional cataloging functions such as searching,
sorting and indexing, the system is shown to be able to model
complex relationships between entities relevant to the application.
The importance of this capability with regard to representing
higher levels of information (beyond pure physical characteristics)
is pointed out. Alternative representations of such relationships
are discussed and some directions of further work in the area of
automation of a museum's catalog is cited.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {62},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810944,
author = {Whinston, Andrew B.},
title = {Beyond Cataloging Functions for Art Museum Data Banks (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810944},
doi = {10.1145/1015528.810944},
abstract = {A data management system for art museums is presented. In addition
to providing conventional cataloging functions such as searching,
sorting and indexing, the system is shown to be able to model
complex relationships between entities relevant to the application.
The importance of this capability with regard to representing
higher levels of information (beyond pure physical characteristics)
is pointed out. Alternative representations of such relationships
are discussed and some directions of further work in the area of
automation of a museum's catalog is cited.},
journal = {SIGSOC Bull.},
month = may,
pages = {62},
numpages = {1}
}

@inproceedings{10.1145/800275.810945,
author = {Blombach, Ann K.},
title = {Making Computer Capabilities Accessible to Musicians (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810945},
doi = {10.1145/800275.810945},
abstract = {During the past ten years at The Ohio State University we have developed a large library of music-related computer programs, encompassing many aspects of music scholarship. These programs include procedures for performing basic music analysis functions as well as programs for information retrieval. We have also prepared a considerable library of encoded musical data and bibliographic information. All our programs and data are stored on disks on OSU's Amdahl 470, making them immediately accessible to any computer programmer who has been initiated into the mysteries of manipulating disk-stored data sets. We wanted, however, to make everything equally accessible to those musicians who are not particularly interested in mastering the complexities of computers, but who would nevertheless like to make use of computer-produced results. We have taken two significant steps toward solving this difficult problem:1. SLAM (Simple Language for Analyzing Music), a "super-high-level" language written in SPITBOL by Thomas G. Whitney (formerly on the staff of OSU's Instruction and Research Computer Center). With SLAM, the musician specifies which music analysis procedures and which musical data he would like to use, communicating with the computer in normal English using traditional music analysis terminology. Though he must satisfy certain syntactical requirements and must include certain key words, such constraints are minimal. For example, "Please count the intervals in the alto voice of Bach's chorale 308." and "Count intervals alto 308." are both legal SLAM commands which would produce the same results. SLAM translates the user's request into the appropriate job control language statements which call the programs and data necessary to perform the task.2. IRRS (Information Retrieval Request System), written in SPITBOL, currently under development. This system provides access to different types of bibliographic, textual, and descriptive data stored on the computer. The user makes his request in the appropriate format, and the computer executes the steps necessary to produce the requested information. For example, in order to retrieve a bibliography of books and articles written between 1960 and 1970, dealing with the perception of music intervals, the user enters "keyterm: perception, music intervals" and "year: 1960-1970" from a computer terminal. The terminal then prints a list of books and articles meeting these criteria.SLAM has been very succesful. Not only have the non-computer-programmers found SLAM invaluable, but even our musician-programmers have found it much easier to access existing programs through SLAM. The information retrieval programs are well under way, and we expect them to be equally useful in providing access to research materials. In short, we are achieving our goals: 1) to make available a variety of computer procedures and data to computer-shy musicians and 2) to eliminate the fears, disappointments, and general confusion too often associated with musicians' attempts to use computers.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {63},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810945,
author = {Blombach, Ann K.},
title = {Making Computer Capabilities Accessible to Musicians (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810945},
doi = {10.1145/1015528.810945},
abstract = {During the past ten years at The Ohio State University we have developed a large library of music-related computer programs, encompassing many aspects of music scholarship. These programs include procedures for performing basic music analysis functions as well as programs for information retrieval. We have also prepared a considerable library of encoded musical data and bibliographic information. All our programs and data are stored on disks on OSU's Amdahl 470, making them immediately accessible to any computer programmer who has been initiated into the mysteries of manipulating disk-stored data sets. We wanted, however, to make everything equally accessible to those musicians who are not particularly interested in mastering the complexities of computers, but who would nevertheless like to make use of computer-produced results. We have taken two significant steps toward solving this difficult problem:1. SLAM (Simple Language for Analyzing Music), a "super-high-level" language written in SPITBOL by Thomas G. Whitney (formerly on the staff of OSU's Instruction and Research Computer Center). With SLAM, the musician specifies which music analysis procedures and which musical data he would like to use, communicating with the computer in normal English using traditional music analysis terminology. Though he must satisfy certain syntactical requirements and must include certain key words, such constraints are minimal. For example, "Please count the intervals in the alto voice of Bach's chorale 308." and "Count intervals alto 308." are both legal SLAM commands which would produce the same results. SLAM translates the user's request into the appropriate job control language statements which call the programs and data necessary to perform the task.2. IRRS (Information Retrieval Request System), written in SPITBOL, currently under development. This system provides access to different types of bibliographic, textual, and descriptive data stored on the computer. The user makes his request in the appropriate format, and the computer executes the steps necessary to produce the requested information. For example, in order to retrieve a bibliography of books and articles written between 1960 and 1970, dealing with the perception of music intervals, the user enters "keyterm: perception, music intervals" and "year: 1960-1970" from a computer terminal. The terminal then prints a list of books and articles meeting these criteria.SLAM has been very succesful. Not only have the non-computer-programmers found SLAM invaluable, but even our musician-programmers have found it much easier to access existing programs through SLAM. The information retrieval programs are well under way, and we expect them to be equally useful in providing access to research materials. In short, we are achieving our goals: 1) to make available a variety of computer procedures and data to computer-shy musicians and 2) to eliminate the fears, disappointments, and general confusion too often associated with musicians' attempts to use computers.},
journal = {SIGSOC Bull.},
month = may,
pages = {63},
numpages = {1}
}

@inproceedings{10.1145/800275.810946,
author = {Henize, John},
title = {Requirements for Improving the Use of Computers to Support the Development of Policy Decisions (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810946},
doi = {10.1145/800275.810946},
abstract = {Computer based information systems have been developed and used successfully for production and engineering and for lower level management tasks but they have yet to be widely applied to aiding management decision making at the higher policy making levels. Despite many attempts, the failures have been many and the successes few. This has resulted in large part from the fact that the technicians who have been engaged to design such systems have not correctly understood the nature of the problem environment with which they are dealing. Because they themselves have had no experience at the policy making levels, they have had a poor conception of the problems to be solved, and thus have made mistakes which they would not have made, had they been designing an information system for lower level tasks. In designing an inventory or process control system, for instance, the technicians have carefully studied the nature of the problems to be dealt with, and have decided which information is important and which not. They have not, in these cases, delivered reams of superfluous information to every point in the system. But when designing an information system to aid higher level decision making, they have tended to do the exact opposite. They have attempted to put every conceivable piece of information that could possibly be of the most remote interest at the fingertips of each and every policy maker -- each of whom is already suffering from a severe information overload. The decision maker could never possibly begin to digest all of this information, even if he found it useful, which, in general, he does not. This paper proposes methods for dealing with this crucial inhibiting problem.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {63},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810946,
author = {Henize, John},
title = {Requirements for Improving the Use of Computers to Support the Development of Policy Decisions (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810946},
doi = {10.1145/1015528.810946},
abstract = {Computer based information systems have been developed and used successfully for production and engineering and for lower level management tasks but they have yet to be widely applied to aiding management decision making at the higher policy making levels. Despite many attempts, the failures have been many and the successes few. This has resulted in large part from the fact that the technicians who have been engaged to design such systems have not correctly understood the nature of the problem environment with which they are dealing. Because they themselves have had no experience at the policy making levels, they have had a poor conception of the problems to be solved, and thus have made mistakes which they would not have made, had they been designing an information system for lower level tasks. In designing an inventory or process control system, for instance, the technicians have carefully studied the nature of the problems to be dealt with, and have decided which information is important and which not. They have not, in these cases, delivered reams of superfluous information to every point in the system. But when designing an information system to aid higher level decision making, they have tended to do the exact opposite. They have attempted to put every conceivable piece of information that could possibly be of the most remote interest at the fingertips of each and every policy maker -- each of whom is already suffering from a severe information overload. The decision maker could never possibly begin to digest all of this information, even if he found it useful, which, in general, he does not. This paper proposes methods for dealing with this crucial inhibiting problem.},
journal = {SIGSOC Bull.},
month = may,
pages = {63},
numpages = {1}
}

@inproceedings{10.1145/800275.810947,
author = {Sim, Francis M. and Kreider, Glen D. and Miller, Roscoe T.},
title = {On the Need for Human Rationing (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810947},
doi = {10.1145/800275.810947},
abstract = {The occasion for this discussion is our recent experience with a severe shortfall in computational capacity at The Pennsylvania State University. Although the details of this affliction may not be reproduced elsewhere, it is our opinion that the events we experienced stem from essential, underlying phenomena which do have wide currency. These are, first, that overall demand for computational facilities and services is increasing "exponentially" and shows no sign of slowdown, and, second, that resources (most especially including funds) to provide increases in the relevant supply of computing capacities are not keeping pace and can not be expected to do so.It is possible that technical advances can treat this disorder, but in the nature of the political/bureaucratic systems which are the vehicles for the delivery of such "fixes", acquiring them will not be painless. Concretely, it seems unlikely that faculty and students in colleges and universities can expect relief from recurrent boom-and-bust in computational resources, whether the duration of such cycles is measured in decades or days. It behooves us to ask whether the attendant pains must be endured, and whether they are conducive to easier and more productive use of computing systems. Our answers are, first, that such pain does not ennoble, and, second, that it often is counterproductive. Consequently, we must try to identify the proximate sources of the disrupting effects of these cyclic shortfalls and attempt to curb them, within our means.We propose that the appropriate guidelines for allocating scarce computing resources may be characterized as prescriptions for humane rationing. In the most general terms, these prescriptions are 1) that qualified users should be ensured a fair share of the available resource without unnecessary expenses of effort in competition for them and in queuing, and 2) that use of computing resources should be so governed as to insure that all user sessions are as free as possible of delays, encumbrances, and constraints induced by management practices rather than by inherent limits of hard and software.While rationing is unnecessary during the occasional boom in academic computing resources, we should have on the shelf the management tools which can make fair and effective allocation possible during the recurrent busts we may anticipate in the 1980's.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {63–64},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810947,
author = {Sim, Francis M. and Kreider, Glen D. and Miller, Roscoe T.},
title = {On the Need for Human Rationing (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810947},
doi = {10.1145/1015528.810947},
abstract = {The occasion for this discussion is our recent experience with a severe shortfall in computational capacity at The Pennsylvania State University. Although the details of this affliction may not be reproduced elsewhere, it is our opinion that the events we experienced stem from essential, underlying phenomena which do have wide currency. These are, first, that overall demand for computational facilities and services is increasing "exponentially" and shows no sign of slowdown, and, second, that resources (most especially including funds) to provide increases in the relevant supply of computing capacities are not keeping pace and can not be expected to do so.It is possible that technical advances can treat this disorder, but in the nature of the political/bureaucratic systems which are the vehicles for the delivery of such "fixes", acquiring them will not be painless. Concretely, it seems unlikely that faculty and students in colleges and universities can expect relief from recurrent boom-and-bust in computational resources, whether the duration of such cycles is measured in decades or days. It behooves us to ask whether the attendant pains must be endured, and whether they are conducive to easier and more productive use of computing systems. Our answers are, first, that such pain does not ennoble, and, second, that it often is counterproductive. Consequently, we must try to identify the proximate sources of the disrupting effects of these cyclic shortfalls and attempt to curb them, within our means.We propose that the appropriate guidelines for allocating scarce computing resources may be characterized as prescriptions for humane rationing. In the most general terms, these prescriptions are 1) that qualified users should be ensured a fair share of the available resource without unnecessary expenses of effort in competition for them and in queuing, and 2) that use of computing resources should be so governed as to insure that all user sessions are as free as possible of delays, encumbrances, and constraints induced by management practices rather than by inherent limits of hard and software.While rationing is unnecessary during the occasional boom in academic computing resources, we should have on the shelf the management tools which can make fair and effective allocation possible during the recurrent busts we may anticipate in the 1980's.},
journal = {SIGSOC Bull.},
month = may,
pages = {63},
numpages = {1}
}

@inproceedings{10.1145/800275.810948,
author = {Cohen, Malcolm S.},
title = {Relational Data Base Management Systems: A Tale of Two Systems (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810948},
doi = {10.1145/800275.810948},
abstract = {This project management system is designed to provide research administrators and department executives the capability to handle the financial accounting needs for 30 or more separately budgeted projects/departments on a computer with 64K of RAM and two floppy disks. The system generates a wide variety of flexible reports including: a Cost Accounting Summary showing previous period expenses, current period expenses, total expenses, budgeted amount, encumbrances, and remaining balance by line item; a Budgetary Summary which shows for the current period and year-to-date the actual expenses, the budgeted amount, the variance between actual and budget, and the percent of budget by line item expended; an Income Statement showing revenue and expenses; an Expense Report; and a Transaction Register; and more.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {64},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810948,
author = {Cohen, Malcolm S.},
title = {Relational Data Base Management Systems: A Tale of Two Systems (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810948},
doi = {10.1145/1015528.810948},
abstract = {This project management system is designed to provide research administrators and department executives the capability to handle the financial accounting needs for 30 or more separately budgeted projects/departments on a computer with 64K of RAM and two floppy disks. The system generates a wide variety of flexible reports including: a Cost Accounting Summary showing previous period expenses, current period expenses, total expenses, budgeted amount, encumbrances, and remaining balance by line item; a Budgetary Summary which shows for the current period and year-to-date the actual expenses, the budgeted amount, the variance between actual and budget, and the percent of budget by line item expended; an Income Statement showing revenue and expenses; an Expense Report; and a Transaction Register; and more.},
journal = {SIGSOC Bull.},
month = may,
pages = {64},
numpages = {1}
}

@inproceedings{10.1145/800275.810949,
author = {Dahms, William R.},
title = {A Micro-Based Project Management System (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810949},
doi = {10.1145/800275.810949},
abstract = {A comparison is made of systems the author has designed for a main frame and a microcomputer. The limitations and advantages of microcomputers for data base management are discussed. Example applications are presented. Advantages of the set theoretic approach are discussed. Applications most suitable for the relational model are described, and contrasted on both the large and small system.The systems discussed include a commercial system Condor Series 20 DBMS which runs on CP/M, on Z-80 microcomputers and MICRO, a system which runs on MTS on large virtual memory main frame computers. CP/M is the operating system of Digital Research. MTS is the operating system at the University of Michigan.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {64},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810949,
author = {Dahms, William R.},
title = {A Micro-Based Project Management System (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810949},
doi = {10.1145/1015528.810949},
abstract = {A comparison is made of systems the author has designed for a main frame and a microcomputer. The limitations and advantages of microcomputers for data base management are discussed. Example applications are presented. Advantages of the set theoretic approach are discussed. Applications most suitable for the relational model are described, and contrasted on both the large and small system.The systems discussed include a commercial system Condor Series 20 DBMS which runs on CP/M, on Z-80 microcomputers and MICRO, a system which runs on MTS on large virtual memory main frame computers. CP/M is the operating system of Digital Research. MTS is the operating system at the University of Michigan.},
journal = {SIGSOC Bull.},
month = may,
pages = {64},
numpages = {1}
}

@inproceedings{10.1145/800275.810950,
author = {Rusinkiewizc, Marek},
title = {Combining Database Management and Statistical Subroutinesunto a User-Oriented Data Analysis Facility (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810950},
doi = {10.1145/800275.810950},
abstract = {Most database management systems (DBMS) offer convenient and flexible data structures and very good data maintenance facilities. At the same time their data manipulation languages are usually limited and most data analysis applications require extensive programming in a host language. On the other hand, the packages of statistical subroutines (PSS) usually have very good data manipulation and analysis facilities, while at the same time they lack the well known advantages of DBMS. An attempt was made to combine the data definition and maintenance facilities of DBMS and the data manipulation and analysis facilities of PSS into a single user-oriented system. The additional software developed for this purpose performs the following functions:1. Allows the user to define his own analysis and (optionally) store it in a library for further reference.2. Allows the user to define the data on which analysis is to be performed.3. Allows the user to execute the (predefined) analysis in the following way:a. the user's description of analysis is translated into a sequence of data analysis and/or data manipulation subroutines of PSS.b. the required data are retrieved from a database under the control of DBMS and put into a temporary file, whose structure is determined by the analysis input requirements.c. the analysis is performed under the control of the executing program PSS.The outlined system is now being implemented, as an interdepartmental effort, in The Institute for Organization of the Medicine Industry, Warsaw, Poland. It utilizes IMS/VS as a database management system and the OSIRIS III package for statistical data processing. Although the system is still under development it is used not only by research workers but also by administrators and management for relatively simple analysis which are not routinely performed, standard reports. The main advantages of the outlined approach can be summarized as follows:a. reduction of time and cost of preparing the analysis,b. increasing the reliabilityc. presenting the tool for the actual decision makers to perform their own data analysis without the interference with programmers.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {64–65},
numpages = {2},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810950,
author = {Rusinkiewizc, Marek},
title = {Combining Database Management and Statistical Subroutinesunto a User-Oriented Data Analysis Facility (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810950},
doi = {10.1145/1015528.810950},
abstract = {Most database management systems (DBMS) offer convenient and flexible data structures and very good data maintenance facilities. At the same time their data manipulation languages are usually limited and most data analysis applications require extensive programming in a host language. On the other hand, the packages of statistical subroutines (PSS) usually have very good data manipulation and analysis facilities, while at the same time they lack the well known advantages of DBMS. An attempt was made to combine the data definition and maintenance facilities of DBMS and the data manipulation and analysis facilities of PSS into a single user-oriented system. The additional software developed for this purpose performs the following functions:1. Allows the user to define his own analysis and (optionally) store it in a library for further reference.2. Allows the user to define the data on which analysis is to be performed.3. Allows the user to execute the (predefined) analysis in the following way:a. the user's description of analysis is translated into a sequence of data analysis and/or data manipulation subroutines of PSS.b. the required data are retrieved from a database under the control of DBMS and put into a temporary file, whose structure is determined by the analysis input requirements.c. the analysis is performed under the control of the executing program PSS.The outlined system is now being implemented, as an interdepartmental effort, in The Institute for Organization of the Medicine Industry, Warsaw, Poland. It utilizes IMS/VS as a database management system and the OSIRIS III package for statistical data processing. Although the system is still under development it is used not only by research workers but also by administrators and management for relatively simple analysis which are not routinely performed, standard reports. The main advantages of the outlined approach can be summarized as follows:a. reduction of time and cost of preparing the analysis,b. increasing the reliabilityc. presenting the tool for the actual decision makers to perform their own data analysis without the interference with programmers.},
journal = {SIGSOC Bull.},
month = may,
pages = {64},
numpages = {1}
}

@inproceedings{10.1145/800275.1016647,
author = {Ellison, David L.},
title = {Introductory Sociology with the General Social Survey (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.1016647},
doi = {10.1145/800275.1016647},
abstract = {The purpose of this presentation is to describe an alternative
sociology course that links student computer skills with available
social survey data. Students are given access to a file of SPSS
programs which they can easily modify to fit their own purposes.
Using the General Social Survey they can test hypotheses on current
data reflecting their interests. The broad range of data allows
beginning students with little or no previous computer experience
to investigate a wide variety of topics.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {65},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@inproceedings{10.1145/800275.810951,
author = {Bezdek, William},
title = {Operating Systems, Editors and Application Packages: Conceptual and Terminological Problems Facing New Users of BMDP and SPSS (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810951},
doi = {10.1145/800275.810951},
abstract = {The purpose of this presentation is to describe an alternative sociology course that links student computer skills with available social survey data. Students are given access to a file of SPSS programs which they can easily modify to fit their own purposes. Using the General Social Survey they can test hypotheses on current data reflecting their interests. The broad range of data allows beginning students with little or no previous computer experience to investigate a wide variety of topics.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {65},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810951,
author = {Bezdek, William},
title = {Operating Systems, Editors and Application Packages: Conceptual and Terminological Problems Facing New Users of BMDP and SPSS (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810951},
doi = {10.1145/1015528.810951},
abstract = {The purpose of this presentation is to describe an alternative sociology course that links student computer skills with available social survey data. Students are given access to a file of SPSS programs which they can easily modify to fit their own purposes. Using the General Social Survey they can test hypotheses on current data reflecting their interests. The broad range of data allows beginning students with little or no previous computer experience to investigate a wide variety of topics.},
journal = {SIGSOC Bull.},
month = may,
pages = {65},
numpages = {1}
}

@inproceedings{10.1145/800275.810952,
author = {Renfro, Charles G.},
title = {Public Use of an Economic Data Base System. (Abstract Only)},
year = {1981},
isbn = {0897910567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800275.810952},
doi = {10.1145/800275.810952},
abstract = {The subject of this paper is the design of an economic data base system for public use, taking as a case study the Kentucky Economic Information System. This system offers its users facilities ranging from simple data retrieval and display to the capability to construct, maintain, and use econometric models online. It was designed originally to be user friendly to the trained econometrician, offering a semi-natural, verbal-mathematical free-format command language as the basic communication mechanism. However, with the development of the KEIS into a data base system that is widely used by government officials, academics, and others throughout Kentucky, the need has developed to provide a facility that is user friendly to any possible user. This paper considers the issue of user friendliness as a variable, depending upon the category of user. But it also considers tbe role played by the computer network and its operating conventions as a determinant of the user friendly features that are required. For example, in order to make the KEIS useable by reference librarians in universities, it was necessary to design a special interface; this necessity relates to the operating policies of the Kentucky Educational Computing Network, one of the computer networks the KEIS is resident upon. In addition, this paper considers the issue of user friendliness as it arises due to the specific characteristics of data and the operations performed: an economic data base system is inherently more difficult to operate than, say, a bibliographic data base system. Various other aspects of the KEIS have been considered in articles and papers appearing in such journals as the Journal of the American Society for Information Science and the Review of Public Data Use and in the proceedings of such conferences as the 1981 National Online Conference (March 1981) and the 8th European Urban Data Management Symposium, Oslo, Norway (June, 1981). A paper on the econometric modeling language (MODLER) that is available as part of the KEIS will be given at the 1981 Economic Control and Dynamics Conference, Copenhagen, Denmark (June 1981). This paper complements these other articles and papers.},
booktitle = {Proceedings of the Joint Conference on Easier and More Productive Use of Computer Systems. (Part - I): Information Processing in the Social Sciences and Humanities - Volume 1981},
pages = {66},
numpages = {1},
location = {Ann Arbor, MI},
series = {CHI '81}
}

@article{10.1145/1015528.810952,
author = {Renfro, Charles G.},
title = {Public Use of an Economic Data Base System. (Abstract Only)},
year = {1981},
issue_date = {Aug. 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12–13},
number = {4–1},
issn = {0163-5794},
url = {https://doi.org/10.1145/1015528.810952},
doi = {10.1145/1015528.810952},
abstract = {The subject of this paper is the design of an economic data base system for public use, taking as a case study the Kentucky Economic Information System. This system offers its users facilities ranging from simple data retrieval and display to the capability to construct, maintain, and use econometric models online. It was designed originally to be user friendly to the trained econometrician, offering a semi-natural, verbal-mathematical free-format command language as the basic communication mechanism. However, with the development of the KEIS into a data base system that is widely used by government officials, academics, and others throughout Kentucky, the need has developed to provide a facility that is user friendly to any possible user. This paper considers the issue of user friendliness as a variable, depending upon the category of user. But it also considers tbe role played by the computer network and its operating conventions as a determinant of the user friendly features that are required. For example, in order to make the KEIS useable by reference librarians in universities, it was necessary to design a special interface; this necessity relates to the operating policies of the Kentucky Educational Computing Network, one of the computer networks the KEIS is resident upon. In addition, this paper considers the issue of user friendliness as it arises due to the specific characteristics of data and the operations performed: an economic data base system is inherently more difficult to operate than, say, a bibliographic data base system. Various other aspects of the KEIS have been considered in articles and papers appearing in such journals as the Journal of the American Society for Information Science and the Review of Public Data Use and in the proceedings of such conferences as the 1981 National Online Conference (March 1981) and the 8th European Urban Data Management Symposium, Oslo, Norway (June, 1981). A paper on the econometric modeling language (MODLER) that is available as part of the KEIS will be given at the 1981 Economic Control and Dynamics Conference, Copenhagen, Denmark (June 1981). This paper complements these other articles and papers.},
journal = {SIGSOC Bull.},
month = may,
pages = {66},
numpages = {1}
}

