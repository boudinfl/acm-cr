@inproceedings{10.1145/2046396.2046398,
author = {Zhao, Yuhang and Xue, Chao and Cao, Xiang and Shi, Yuanchun},
title = {PicoPet: "Real World" Digital Pet on a Handheld Projector},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046398},
doi = {10.1145/2046396.2046398},
abstract = {We created PicoPet, a digital pet game based on mobile handheld projectors. The player can project the pet into physical environments, and the pet behaves and evolves differently according to the physical surroundings. PicoPet creates a new form of gaming experience that is directly blended into the physical world, thus could become incorporated into the player's daily life as well as reflecting their lifestyle. Multiple pets projected by multiple players can also interact with each other, potentially triggering social interactions between players. In this paper, we present the design and implementation of PicoPet, as well as directions for future explorations.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {1–2},
numpages = {2},
keywords = {game, handheld projector, digital pet},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046399,
author = {Boulanger (Vaucelle), Cati N. and Dietz, Paul and Bathiche, Steven},
title = {Scopemate: A Tracking Inspection Microscope},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046399},
doi = {10.1145/2046396.2046399},
abstract = {We propose a new interaction mechanism for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with ap-propriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {3–4},
numpages = {2},
keywords = {microscope, virtual window, hand free interface, head tracker},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046400,
author = {LiKamWa, Robert and Zhong, Lin},
title = {SUAVE: Sensor-Based User-Aware Viewing Enhancement for Mobile Device Displays},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046400},
doi = {10.1145/2046396.2046400},
abstract = {As mobile devices are used in various environments, ambient light and wide viewing direction impair a display's perceived display quality. To combat these effects, we introduce SUAVE, our Sensor-based User-Aware Viewing Enhancement system. SUAVE senses the ambient light and viewing direction and applies corresponding image enhancements to the display content, increasing its usability. SUAVE employs a parameter calibration process to help users select suitable image enhancements for particular viewing contexts. We report implementations of SUAVE on a Motorola Xoom Tablet and an Apple iPhone 4.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {5–6},
numpages = {2},
keywords = {context-aware, mobile display, context-awareness},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046401,
author = {Amberg, Michel and Giraud, Fr\'{e}d\'{e}ric and Semail, Betty and Olivo, Paolo and Casiez, G\'{e}ry and Roussel, Nicolas},
title = {STIMTAC: A Tactile Input Device with Programmable Friction},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046401},
doi = {10.1145/2046396.2046401},
abstract = {We present the STIMTAC, a touchpad device that supports friction reduction. Contrary to traditional vibrotactile approaches, the STIMTAC provides information passively, acting as a texture display. It does not transfer energy to the user but modifies how energy is dissipated within the contact area by a user-initiated friction process. We report on the iterative process that led to the current hardware design and briefly describe the software framework that we are developing to illustrate its potential.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {7–8},
numpages = {2},
keywords = {tactile feedback, programmable friction, tactile input, squeeze film effect},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046402,
author = {Murase, Taichi and Moteki, Atsunori and Ozawa, Noriaki and Hara, Nobuyuki and Nakai, Takehiro and Fujimoto, Katsuhito},
title = {Gesture Keyboard Requiring Only One Camera},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046402},
doi = {10.1145/2046396.2046402},
abstract = {In this paper, we propose a novel gesture-based virtual keyboard (Gesture Keyboard) of QWERTY key layout requiring only one camera. Gesture Keyboard tracks the user's fingers and recognizes gestures as the input, and each virtual key of it follows a corresponding finger. Therefore, it is possible to input characters at the user's preferred hand position even if displacing hands during inputting. Because Gesture Keyboard requires only one camera to obtain sensor information, keyboard-less devices can feature it easily.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {9–10},
numpages = {2},
keywords = {user interfaces, gesture, keyboard},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046404,
author = {Sugihara, Kenji and Otsuki, Mai and Kimura, Asako and Shibata, Fumihisa and Tamura, Hideyuki},
title = {MAI Painting Brush++: Augmenting the Feeling of Painting with New Visual and Tactile Feedback Mechanisms},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046404},
doi = {10.1145/2046396.2046404},
abstract = {We have developed a mixed-reality (MR) painting system named the MR-based Artistic Interactive (MAI) Painting Expert and MAI Painting Brush which simulates the painting of physical objects in the real world. In this paper, we describe how the MAI Painting Brush was upgraded to the "MAI Painting Brush++," enabling virtual painting on virtual objects. The improved system has a visual and tactile feedback mechanism that simulates the effect of touch when used on a virtual painting target. This is achieved using deformation of the brush tip and reaction force on the hand.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {13–14},
numpages = {2},
keywords = {input device, mixed reality, paintbrush, painting system, visual and tactile feedback},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046405,
author = {Choi, Sangwon and Han, Jaehyun and Kim, Sunjun and Heo, Seongkook and Lee, Geehyuk},
title = {ThickPad: A Hover-Tracking Touchpad for a Laptop},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046405},
doi = {10.1145/2046396.2046405},
abstract = {We explored the use of a hover tracking touchpad in a laptop environment. In order to study the new experience, we implemented a prototype touchpad consisting of infrared LEDs and photo-transistors, which can track fingers as far as 10mm over the surface. We demonstrate here three major interaction techniques that would become possible when a hover-tracking touchpad meets a laptop},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {15–16},
numpages = {2},
keywords = {hover tracking touchpad, thickpad, laptop touchpad},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046406,
author = {Holman, David and Vertegaal, Roel},
title = {TactileTape: Low-Cost Touch Sensing on Curved Surfaces},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046406},
doi = {10.1145/2046396.2046406},
abstract = {TactileTape is a one-dimensional touch sensor that looks and behaves like regular tape. It can be constructed from everyday materials (a pencil, tin foil, and shelf liner) and senses single-touch input on curved and deformable surfaces. It is used as a roll of touch sensitive material from which designers cut pieces to quickly add touch sensitive strips to physical prototypes. TactileTape is low-cost, easy to interface, and, unlike current non-planar touch solutions [2,7,11], it is better adapted for the rapid exploration and iteration in the early design stage.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {17–18},
numpages = {2},
keywords = {flexible interfaces, curved touch input},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046407,
author = {Paisios, Nektarios and Rubinsteyn, Alex and Vyas, Vrutti and Subramanian, Lakshminarayanan},
title = {Recognizing Currency Bills Using a Mobile Phone: An Assistive Aid for the Visually Impaired},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046407},
doi = {10.1145/2046396.2046407},
abstract = {Despite the rapidly increasing use of credit cards and other electronic forms of payment, cash is still widely used for everyday transactions due to its convenience, perceived security and anonymity. However, the visually impaired might have a hard time telling each paper bill apart, since, for example, all dollar bills have the exact same size and, in general, currency bills around the world are not distinguishable by any tactile markings. We propose the use of a broadly available tool, the camera of a smart-phone, and an adaptation of the SIFT algorithm to recognize partial and even distorted images of paper bills. Our algorithm improves memory efficiency and the speed of SIFT key-point classification by using a k-means clustering approach. Our results show that our system can be used in real-world scenarios to recognize unknown bills with a high accuracy.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {19–20},
numpages = {2},
keywords = {camera phone, visually impaired, currency identification},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046408,
author = {Bhardwaj, Anant P. and Luciano, Dave and Klemmer, Scott R.},
title = {Redprint: Integrating API Specific "Instant Example" and "Instant Documentation" Display Interface in IDEs},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046408},
doi = {10.1145/2046396.2046408},
abstract = {Software libraries for most of the modern programming languages are numerous, large and complex. Remembering the syntax and usage of APIs is a difficult task for not just novices but also expert programmers. IDEs (Integrated Development Environment) provide capabilities like autocomplete and intellisense to assist programmers; however, programmers still need to visit search engines like Google to find API (Application Program Interface) documentation and samples. This paper evaluates Redprint - a browser based development environment for PHP that integrates API specific "Instant Example" and "Instant Documentation" display interfaces. A comparative laboratory study shows that integrating API specific "Instant Example" and "Instant Documentation" display interfaces into a development environment significantly reduces the cost of searching and thus significantly reduces the time to develop software.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {21–22},
numpages = {2},
keywords = {instant example display interface, example centric programming, redprint, instant documentation display interface},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046410,
author = {Goldman, Max},
title = {Role-Based Interfaces for Collaborative Software Development},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046410},
doi = {10.1145/2046396.2046410},
abstract = {Real-time collaboration between multiple simultaneous contributors to a shared document is full of both opportunities and pitfalls, as evidenced by decades of research and industry work in computer-supported cooperative work. In the domain of software engineering, collaboration is still generally achieved either via shared use of a single computer (e.g. pair programming) or with version control (and manual pushing and pulling of changes). By examining and designing for the different roles collaborating programmers play when working synchronously together, we can build real-time collaborative programming systems that make their collaboration more effective. And beyond simple shared editing, we can provide asymmetric, role-specific interfaces on their shared task. Collabode is a web-based IDE for collaborative programming with simultaneous editors that, along with several novel models for closely-collaborative software development, explores the potential of real-time cooperative programming.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {23–26},
numpages = {4},
keywords = {collaboration, software development, crowdsourcing, pair programming},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046411,
author = {Chang, Tsung-Hsiang},
title = {Using Graphical Representation of User Interfaces as Visual References},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046411},
doi = {10.1145/2046396.2046411},
abstract = {Many user interfaces use indirect references to identify specific objects and devices. My thesis investigates using graphical representations of user interfaces (i.e. screenshots) as direct visual references to support various kinds of applications. Sikuli Script enables users to programmatically control GUIs without the support from the underlying applications. Sikuli Test lets GUI developers and testers create test scripts without coding. Deep Shot introduces a framework and interaction techniques to migrate work states across heterogeneous devices in one action, taking a picture. In addition to these pure pixel-based systems, PAX associates the pixel representation with the internal structures and metadata of the user interface. Based on these building blocks, we propose to develop a visual history system that enables users to search and browse what they have seen on their computer screens. We outline some interesting use cases and discuss the challenges in this ongoing work.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {27–30},
numpages = {4},
keywords = {GUI automation, GUI testing, accessibility API, information reuse, task migration},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046412,
author = {Flatla, David R.},
title = {Accessibility for Individuals with Color Vision Deficiency},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046412},
doi = {10.1145/2046396.2046412},
abstract = {Individuals with Color Vision Deficiency (CVD) are often unable to distinguish between colors that individuals without CVD can distinguish. Recoloring tools exist that modify the colors in an image so they are more easily distinguishable for those with CVD. These tools use models of color differentiation that rely on many assumptions about the environment and user. However, these assumptions rarely hold in real-world use cases, leading to incorrect color modification by recoloring tools. In this doctoral symposium, I will present Situation-Specific Models (SSMs) as a solution to this problem. SSMs are color differentiation models created in-situ via a calibration procedure. This calibration procedure captures the exact color differentiation abilities of the user, allowing a color differentiation model to be created that fits the user and his/her environmental situation. An SSM-based recoloring tool will be able to provide recolored images that most accurately reflect the color differentiation abilities of a particular individual in a particular environment.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {31–34},
numpages = {4},
keywords = {color differentiation, situation-specific model (SSM), calibration, color blindness, color vision deficiency (CVD)},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046413,
author = {Hoarau, Rapha\"{e}l},
title = {Augmenting the SCOPE of Interactions with Implicit and Explicit Graphical Structures},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046413},
doi = {10.1145/2046396.2046413},
abstract = {When using interactive graphical tools, users often have to manage a structure, i.e. the arrangement of and relations between the parts or elements of the content. However, the interaction with structures may be complex, and not well integrated with the interaction with the content. Based on contextual inquiries and past work, we have identified a number of concepts and requirements about interaction with structure. We have explored a number of interactive tools and we present one of them in this paper: a new kind of property sheet that relies on the implicit structure of graphics. The interactions with the tool augment the scope of interactions to multiple objects.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {35–38},
numpages = {4},
keywords = {instrumental interaction, exploratory design, graphical interaction design},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046414,
author = {Cauchard, Jessica R.},
title = {Mobile Multi-Display Environments},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046414},
doi = {10.1145/2046396.2046414},
abstract = {Mobile devices are increasingly being fitted with more than one display, presenting a new breed of Mobile Multi-Display Environments (MMDEs). It is however still unclear how the extra display fits within the mobile devices' ecology in terms of visualisation and interaction. My research explores the alignment between multiple displays in a mobile environment and how different alignments affect usability and the choice of a suitable interaction technique. In order to investigate those properties and adapt them to various use cases, I will build a steerable projection system to study different alignments, then analyse visual separation effects in MMDEs and finally explore the possibilities offered when the displays are overlapping.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {39–42},
numpages = {4},
keywords = {pico-projector, mobile multi-display environment},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046415,
author = {L\"{o}chtefeld, Markus},
title = {Advanced Interaction with Mobile Projection Interfaces},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046415},
doi = {10.1145/2046396.2046415},
abstract = {Through the increasing miniaturization of projection units the integration of such units in everyday-life objects is now possible. Even though these so called pico-projectors are already getting integrated into mobile devices like phones or digital cameras, comparably little research has been conducted to empower these devices to their full capabilities. I outline my previous and current work towards an interface design and a privacy framework that will facilitate mobile projection devices to be part in people's everyday-life. In particular my work is divided into two directions, on the one hand the development of a single-user scenario interface and on the other hand a framework to cope with privacy issues. This will allow the deeper exploitation of the capabilities of mobile projection units for a variety of everyday tasks.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {43–46},
numpages = {4},
keywords = {augmented reality, proximic interaction, mobile projection},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046416,
author = {Amershi, Saleema},
title = {Designing for Effective End-User Interaction with Machine Learning},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046416},
doi = {10.1145/2046396.2046416},
abstract = {End-user interactive machine learning is a promising tool for enhancing human capabilities with large data. Recent work has shown that we can create end-user interactive machine learning systems for specific applications. However, we still lack a generalized understanding of how to design effective end-user interaction with interactive machine learning systems. My dissertation work aims to advance our understanding of this question by investigating new techniques that move beyond na\"{\i}ve or ad-hoc approaches and balance the needs of both end-users and machine learning algorithms. Although these explorations are grounded in specific applications, we endeavored to design strategies independent of application or domain specific features. As a result, our findings can inform future end-user interaction with machine learning systems.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {47–50},
numpages = {4},
keywords = {end-user interactive machine learning},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046418,
author = {Qin, Qian and Rohs, Michael and Kratz, Sven},
title = {Dynamic Ambient Lighting for Mobile Devices},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046418},
doi = {10.1145/2046396.2046418},
abstract = {The information a small mobile device can show via its display has been always limited by its size. In large information spaces, relevant information, such as important locations on a map can get clipped when a user starts zooming and panning. Dynamic ambient lighting allows mobile devices to visualize off-screen objects by illuminating the background without compromising valuable display space. The lighted spots can be used to show the direction and distance of such objects by varying the spot's position and intensity. Dynamic ambient lighting also provides a new way of displaying the state of a mobile device. Illumination is provided by a prototype rear of device shell which contains LEDs and requires the device to be placed on a surface, such as a table or desk.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {51–52},
numpages = {2},
keywords = {visualization, ambient light, interaction, android, novel hardware, mobile devices, off-screen locations},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046419,
author = {Takemura, Kentaro and Ito, Akihiro and Takamatsu, Jun and Ogasawara, Tsukasa},
title = {Active Bone-Conducted Sound Sensing for Wearable Interfaces},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046419},
doi = {10.1145/2046396.2046419},
abstract = {In this paper, we propose a wearable sensor system that measures an angle of an elbow and position tapped by finger using bone-conducted sound. Our system consists of two microphones and a speaker, and they are attached on forearm. A novelty of this paper is to use active sensing for measuring an angle of an elbow. In this paper, active sensing means to emit sounds to a bone, and a microphone receives the sounds reflected at the elbow. The reflection of sound depends on the angle of elbow. Since frequencies of bone-conducted sound by tapping and from the speaker are different, these proposed techniques can be used simultaneously. We confirmed the feasibility of proposed system through experiments.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {53–54},
numpages = {2},
keywords = {acoustic sensing, wearable interfaces},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046420,
author = {You, Tun-Hao and Wu, Yi-Jui and Yeh, Yi-Jen},
title = {TOPS: Television Object Promoting System},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046420},
doi = {10.1145/2046396.2046420},
abstract = {In this short paper, we propose the Television Object Promoting system (TOPS), a What You See Is What You Get (WYSIWYG) user interface and user experience designed for users to interact with objects in TV programs. Using TOPS while watching TV, consumers can acquire informa-tion about objects appearing in TV programs, such as mer-chandise, people, and scenic spots. Moreover, consumers can purchase merchandise directly, or can obtain services or items related to those objects. Besides, vendors are able to provide detail and selling information about the objects. The Television Object Promoting system (TOPS) offers not only convenience to consumers, but also new marketing methods to vendors. The paper also discusses the features, design, and implementation of TOPS.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {55–56},
numpages = {2},
keywords = {image recognition, interaction technology, content aware, internet TV},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046421,
author = {Weld, Daniel S. and Mausam and Dai, Peng},
title = {Execution Control for Crowdsourcing},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046421},
doi = {10.1145/2046396.2046421},
abstract = {Crowdsourcing marketplaces enable a wide range of applications, but constructing any new application is challenging - usually requiring a complex, self-managing workflow in order to guarantee quality results. We report on the CLOWDER project, which uses machine learning to continually refine models of worker performance and task difficulty. We present decision-theoretic optimization techniques that can select the best parameters for a range of workflows. Initial experiments show our optimized workflows are significantly more economical than with manually set parameters.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {57–58},
numpages = {2},
keywords = {pomdp, decision theory, crowdsourcing, human computation, execution control},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046422,
author = {Lee, Hosub and Choi, Young Sang},
title = {Fit Your Hand: Personalized User Interface Considering Physical Attributes of Mobile Device Users},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046422},
doi = {10.1145/2046396.2046422},
abstract = {We present a mobile user interface which dynamically reformulates the layout based on the touch input pattern of users. By analyzing the touch input, it infers users' physical characteristics such as handedness, finger length, or usage habits, thereby calculates the optimal touch area for the user. The user interface is gradually adapted to each user by automatically rearranging graphic objects such as application icons to the most easy-to-touch positions. To compute the optimal touch area, we designed software architecture and implemented an Android application which analyzes touch input and determines the touch frequency in specific screen areas, the handedness and hand size of users. As proof of concept, this research prototype shows acceptable performance and accuracy. To decide which items should be placed in the optimal touch area, we plan to integrate our machine learning algorithm which prioritizes applications according to the context of users into the proposed system.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {59–60},
numpages = {2},
keywords = {adaptive user interface, intelligent interaction, physical attribute, touch input pattern, personalization},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046423,
author = {Park, Young-Woo and Hwang, Sungjae and Nam, Tek-Jin},
title = {Poke: Emotional Touch Delivery through an Inflatable Surface over Interpersonal Mobile Communications},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046423},
doi = {10.1145/2046396.2046423},
abstract = {In this paper we present Poke - a soft and human-like re-mote touch technique through an inflatable surface. We aimed to design it for delivering more emotional and plea-sant touches over interpersonal mobile communications. Poke enables to touch human's skin with an inflatable sur-face according to the other user's finger pressures and hand gestures during a phone call. It delivers different kinds of pokes and other affective touches with its inflating patterns (strengths and repetitions) and vibrations from the top of the inflatable surface. The paper also suggests affective touches such as weak/hard poke, poke and then shake, poke back and pat which can be exchanged during typical phone calls.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {61–62},
numpages = {2},
keywords = {inflatable surface, remote touch interaction, phone call},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046424,
author = {Boulanger, Cati and Dietz, Paul and Bathiche, Steven},
title = {Scopemate: A Robotic Microscope},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046424},
doi = {10.1145/2046396.2046424},
abstract = {Scopemate is a robotic microscope that tracks the user for inspection microscopy. The novel input device combines an optically augmented web-cam with a head tracker. A head tracker controls the inspection angle of a webcam fitted with appropriate microscope optics. This allows an operator the full use of their hands while intuitively looking at the work area from different perspectives.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {63–64},
numpages = {2},
keywords = {head tracker, virtual window, microscope, hand free interface},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046425,
author = {Roberts, Charles and Hollerer, Tobias},
title = {Composition for Conductor and Audience: New Uses for Mobile Devices in the Concert Hall},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046425},
doi = {10.1145/2046396.2046425},
abstract = {Composition for Conductor and Audience is an audience interaction piece first performed for an audience of over seventy-five people in June of 2011. The audience becomes the orchestra in this composition as they control different musical variables using the touchscreen surfaces on their personal mobile devices. To the authors' knowledge this is the first concert piece for bi-directional networked interactivity on audience-owned mobile devices to ever be performed. Audience members participated using the iOS / Android application 'Control', a generic solution for creating touchscreen interfaces written by the first author. Over twenty members of the audience participated in the performance, using their personal devices to match gestures made by the conductor with corresponding gestures on their mobile devices.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {65–66},
numpages = {2},
keywords = {music, mobile, network, interaction},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046426,
author = {Krebs, Dave and Conrad, Alexander and Hauskrecht, Milos and Wang, Jingtao},
title = {MARBLS: A Visual Environment for Building Clinical Alert Rules},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046426},
doi = {10.1145/2046396.2046426},
abstract = {Physicians and nurses usually rely on hospital information systems (HIS) for detecting a variety of adverse clinical conditions and reminding repetitive treatments. However, the acquisition of alert rules expected by HIS from experts remains a challenging, error-prone, and time-consuming process. In this work, we present MARBLS (Medical Alert Rule BuiLding System) - a visual environment to facilitate the design and definition of clinical alert rules. MARBLS enables a two-way, synchronized visual rule workspace and visual query explorer. Monitoring rules can be built by manipulating block components in the rule workspace, by querying and generalizing region of interests in the visual query explorer via direct manipulations, or a combination of both. Informal testing with doctors has shown positive feedback.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {67–68},
numpages = {2},
keywords = {visual language, health care, end user programming, clinical monitoring rules},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046427,
author = {Pham, Hubert and Mazzola Paluska, Justin and Miller, Robert C. and Ward, Steve},
title = {Cloudtop: A Workspace for the Cloud},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046427},
doi = {10.1145/2046396.2046427},
abstract = {Even as users rely more on the web for their computing needs, they continue to depend on a desktop-like area for quick access to in-use resources. The traditional desktop is file-centric and prone to clutter, making it suboptimal for use in a web-dominated world. This paper introduces Cloudtop, a browser plugin that offers a lightweight workplace for temporary items, optimized around the idea that its contents originate from and will ultimately return to the web. Cloudtop improves upon the desktop by 1) implementing a simple, time-based notebook metaphor for managing clutter, 2) capturing and bundling extensible metadata for web resources, and 3) providing a platform for greater interface uniformity across sites.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {69–70},
numpages = {2},
keywords = {desktop, cloud, clutter},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046428,
author = {Wu, Leslie and Cirimele, Jesse and Card, Stuart and Klemmer, Scott and Chu, Larry and Harrison, Kyle},
title = {Maintaining Shared Mental Models in Anesthesia Crisis Care with Nurse Tablet Input and Large-Screen Displays},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046428},
doi = {10.1145/2046396.2046428},
abstract = {In an effort to reduce medical errors, doctors are beginning to embrace cognitive aids, such as paper-based checklists. We describe the early stage design process of an interactive cognitive aid for crisis care teams. This process included collaboration with anesthesia professors in the school of medicine and observation of medical students practicing in simulated scenarios. Based on these insights, we identify opportunities to employ large-screen displays and coordinated tablets to support team performance. We also propose a system design for interactive cognitive aids intended to encourage a shared mental model amongst crisis care staff.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {71–72},
numpages = {2},
keywords = {checklists, cognitive aids, medicine},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046429,
author = {Hachisu, Taku and Sato, Michi and Fukushima, Shogo and Kajimoto, Hiroyuki},
title = {HaCHIStick: Simulating Haptic Sensation on Tablet Pc for Musical Instruments Application},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046429},
doi = {10.1145/2046396.2046429},
abstract = {In this paper, we propose a novel stick-type interface, the "HaCHIStick," for musical performance on a tablet PC. The HaCHIStick is composed of a stick with an embedded vibrotactile actuator, a visual display, and an elastic sheet on the display. By combining the kinesthetic sensation induced by striking the elastic sheet with vibrotactile sensation, the system provides natural haptic cues that enable the user to feel what they strike with the stick, such as steel or wood. This haptic interaction would enrich the user's experience when playing the instruments. The interface is regarded as a type of haptic augmented reality (AR) system, with a relatively simple setup.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {73–74},
numpages = {2},
keywords = {haptic augmented reality, vibrotactile sensation., stick-type interface, musical instrument, tablet PC},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046430,
author = {Gu, Jiseong and Lee, Geehyuk},
title = {TouchString: A Flexible Linear Multi-Touch Sensor for Prototyping a Freeform Multi-Touch Surface},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046430},
doi = {10.1145/2046396.2046430},
abstract = {We propose the concept of prototyping a multi-touch surface of an arbitrary form using a flexible linear multi-touch sensor that we call TouchString. We defined the conceptual structure of a TouchString, and implemented an example prototype of a TouchString. We verified the feasibility of the concept by demonstrating a few basic application scenarios using the prototype.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {75–76},
numpages = {2},
keywords = {prototyping, freeform multi-touch surface, linear multi-touch sensor},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046431,
author = {Karnik, Abhijit and Mayol-Cuevas, Walterio and Subramanian, Sriram},
title = {MUST-D: Multi-User See through Display},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046431},
doi = {10.1145/2046396.2046431},
abstract = {In this paper we present MUST-D, a multi-user see-through display that allows users to inspect objects behind a glass panel while projecting view-dependent information on the glass to the user. MUST-D uses liquid crystal panels to implement a multi-view see-through display space in front of physical objects.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {77–78},
numpages = {2},
keywords = {random hole displays, multi-view, see-through},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046433,
author = {Hiyama, Atsushi and Doyama, Yusuke and Miyashita, Mariko and Ebuchi, Eikan and Seki, Masazumi and Hirose, Michitaka},
title = {Artisanship Training Using Wearable Egocentric Display},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046433},
doi = {10.1145/2046396.2046433},
abstract = {In recent years, most of traditional artisanship is declining because of aging skilled artisan and fewer successors. Therefore, methods for digital archiving of such traditional artisanship are needed. We have constructed a wearable skill-training interface that displays egocentric visual and audio information and muscle activities of an artisan. We used acceleration data of an instrument associated with the usage of the tools for evaluating the effect of proposed wearable display system. This paper introduces the concept and development of wearable egocentric display. Then briefly reports the application results in Kamisuki, Japanese traditional papermaking.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {81–82},
numpages = {2},
keywords = {training interface, wearable computer, multimodal display},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046434,
author = {Paisios, Nektarios and Rubinsteyn, Alex and Subramanian, Lakshminarayanan and Tierney, Matt and Vyas, Vrutti},
title = {Tracking Indoor Location and Motion for Navigational Assistance},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046434},
doi = {10.1145/2046396.2046434},
abstract = {Visually impaired people have a harder time remembering their way around complex unfamiliar buildings, whilst obtaining the help of a sighted guide is not always possible or desirable. By sensing the users location and motion, however, mobile phone software can provide navigational assistance in such situations, obviating the need of human guides. We present a simple to operate and highly usable mobile navigational guide that uses Wi-Fi and accelerometer sensors to help the user repeat paths that were already walked once.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {83–84},
numpages = {2},
keywords = {Wi-Fi localization, visually impaired., Bayesian filter, navigational system, mobile phone},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046435,
author = {Bauer, Jared S. and Jansen, Alex and Cirimele, Jesse},
title = {MoodMusic: A Method for Cooperative, Generative Music Playlist Creation},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046435},
doi = {10.1145/2046396.2046435},
abstract = {Music is a major element of social gatherings. However, creating playlists that suit everyone's tastes and the mood of the group can require a large amount of manual effort. In this paper, we present MoodMusic, a method to dynamically generate contextually appropriate music playlists for groups of people. MoodMusic uses speaker pitch and intensity in the conversation to determine the current 'mood'. MoodMusic then queries the online music libraries of the speakers to choose songs appropriate for that mood. This allows groups to listen to music appropriate for their current mood without managing playlists. This work contributes a novel method for dynamically creating music playlists for groups based on their music preferences and current mood.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {85–86},
numpages = {2},
keywords = {affective computing, collaborative filtering, audio interfaces, mood detection, conversation analysis, music},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046436,
author = {Patel, Neil and Klemmer, Scott R. and Parikh, Tapan S.},
title = {An Asymmetric Communications Platform for Knowledge Sharing with Low-End Mobile Phones},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046436},
doi = {10.1145/2046396.2046436},
abstract = {We present Awaaz.De ("give voice"), a social platform for communities to access and share knowledge using low-end mobile phones. Awaaz.De features a configurable mobile voice application organized into asynchronous voice mes-sage boards. For poor, remote and marginal communities, the voice-touchtone interface addresses the constraints of low literacy, language diversity, and affordability of only basic mobile devices. Voice content also presents a low barrier to content authoring, encouraging otherwise disconnected communities to actively participate in knowledge exchange. Awaaz.De includes a web-based administration interface for Internet-connected community managers to moderate, annotate, categorize, route, and narrow-cast voice messages. In this paper we describe the platform's design, implementation, and future directions.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {87–88},
numpages = {2},
keywords = {HCI4D, IVR, voice UI, ICT4D},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046437,
author = {Lafreniere, Benjamin and Bunt, Andrea and Lount, Matthew and Krynicki, Filip and Terry, Michael A.},
title = {AdaptableGIMP: Designing a Socially-Adaptable Interface},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046437},
doi = {10.1145/2046396.2046437},
abstract = {We introduce the concept of a socially-adaptable interface, an interface that provides instant access to task-specific interface customizations created, edited, and documented by the application's user community. We demonstrate this concept in AdaptableGIMP, a modified version of the GIMP image editor that we have developed.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {89–90},
numpages = {2},
keywords = {adaptable interfaces, crowdsourcing, search-based interfaces, wikis},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

@inproceedings{10.1145/2046396.2046438,
author = {Simpson, James and Terry, Michael},
title = {Embedding Interface Sketches in Code},
year = {2011},
isbn = {9781450310147},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046396.2046438},
doi = {10.1145/2046396.2046438},
abstract = {This paper presents a user interface (UI) design tool, GUIIO, which uses ASCII text as its medium for rendering interface components. Like other UI design tools, GUIIO allows individuals to create and manipulate UI components as first-class objects. However, GUIIO has the advantage that its UI designs can be embedded directly within the program code itself. We implemented GUIIO as an extension to an existing development environment. As a result, developers can fluidly transition from editing code to editing the UI mock-up, with the text editor automatically switching its mode from code editing to UI editing as a function of the location of the cursor. By rendering UIs as ASCII art, GUIIO fills an important gap in the design, implementation, and revision of UIs by providing a highly portable and immediately accessible visual representation of the UI that embeds with the code itself.},
booktitle = {Proceedings of the 24th Annual ACM Symposium Adjunct on User Interface Software and Technology},
pages = {91–92},
numpages = {2},
keywords = {documentation, development, design, user interface, plug-in, ASCII art},
location = {Santa Barbara, California, USA},
series = {UIST '11 Adjunct}
}

