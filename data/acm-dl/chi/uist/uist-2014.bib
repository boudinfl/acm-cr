@inproceedings{10.1145/2642918.2642919,
author = {Bolas, Mark},
title = {Designing the User in User Interfaces},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2642919},
doi = {10.1145/2642918.2642919},
abstract = {In the good old days, the human was here, the computer there, and a good living was to be made by designing ways to interface between the two. Now we find ourselves unthinkingly pinching to zoom in on a picture in a paper magazine. User interfaces are changing instinctual human behavior and instinctual human behavior is changing user interfaces. We point or look left in the "virtual" world just as we point or look left in the physical.It is clear that nothing is clear anymore: the need for "interface" vanishes when the boundaries between the physical and the virtual disappear. We are at a watershed moment when to experience being human means to experience being machine. When there is not a user interface - it is just what you do. When instinct supplants mice and menus and the interface insinuates itself into the human psyche.We are redefining and creating what it means to be human in this new physical/virtual integrated reality - we are not just designing user interfaces, we are designing users.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {1},
numpages = {1},
keywords = {mixed reality, transhumanism, virtual reality, posthuman factors, user interace},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647374,
author = {Savage, Valkyrie and Schmidt, Ryan and Grossman, Tovi and Fitzmaurice, George and Hartmann, Bj\"{o}rn},
title = {A Series of Tubes: Adding Interactivity to 3D Prints Using Internal Pipes},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647374},
doi = {10.1145/2642918.2647374},
abstract = {3D printers offer extraordinary flexibility for prototyping the shape and mechanical function of objects. We investigate how 3D models can be modified to facilitate the creation of interactive objects that offer dynamic input and output. We introduce a general technique for supporting the rapid prototyping of interactivity by removing interior material from 3D models to form internal pipes. We describe this new design space of pipes for interaction design, where variables include openings, path constraints, topologies, and inserted media. We then present PipeDream, a tool for routing such pipes through the interior of 3D models, integrated within a 3D modeling program. We use two distinct routing algorithms. The first has users define pipes' terminals, and uses path routing and physics-based simulation to minimize pipe bending energy, allowing easy insertion of media post-print. The second allows users to supply a desired internal shape to which we fit a pipe route: for this we describe a graph-routing algorithm. We present several prototypes created using our tool to show its flexibility and potential.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–12},
numpages = {10},
keywords = {fabrication, interactive objects, 3D printing, design tools},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647388,
author = {McCrae, James and Umetani, Nobuyuki and Singh, Karan},
title = {FlatFitFab: Interactive Modeling with Planar Sections},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647388},
doi = {10.1145/2642918.2647388},
abstract = {We present a comprehensive system to author planar section structures, common in art and engineering. A study on how planar section assemblies are imagined and drawn guide our design principles: planar sections are best drawn in-situ, with little foreshortening, orthogonal to intersecting planar sections, exhibiting regularities between planes and contours. We capture these principles with a novel drawing workflow where a single fluid user stroke specifies a 3D plane and its contour in relation to existing planar sections. Regularity is supported by defining a vocabulary of procedural operations for intersecting planar sections. We exploit planar structure properties to provide real-time visual feedback on physically simulated stresses, and geometric verification that the structure is stable, connected and can be assembled. This feedback is validated by real-world fabrication and testing. As evaluation, we report on over 50 subjects who all used our system with minimal instruction to create unique models.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–22},
numpages = {10},
keywords = {interaction, fabrication, planar sections, shape modeling},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647416,
author = {Paczkowski, Patrick and Dorsey, Julie and Rushmeier, Holly and Kim, Min H.},
title = {Paper3D: Bringing Casual 3D Modeling to a Multi-Touch Interface},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647416},
doi = {10.1145/2642918.2647416},
abstract = {A 3D modeling system that provides all-inclusive functionality is generally too demanding for a casual 3D modeler to learn. In recent years, there has been a shift towards developing more approachable systems, with easy-to-learn, intuitive interfaces. However, most modeling systems still employ mouse and keyboard interfaces, despite the ubiquity of tablet devices, and the benefits of multi-touch interfaces applied to 3D modeling. In this paper, we introduce an alternative 3D modeling paradigm for creating developable surfaces, inspired by traditional papercrafting, and implemented as a system designed from the start for a multi-touch tablet. We demonstrate the process of assembling complex 3D scenes from a collection of simpler models, in turn shaped through operations applied to sheets of virtual paper. The modeling and assembling operations mimic familiar, real-world operations performed on paper, allowing users to quickly learn our system with very little guidance. We outline key design decisions made throughout the development process, based on feedback obtained through collaboration with target users. Finally, we include a range of models created in our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–32},
numpages = {10},
keywords = {3D modeling, folding, multi-touch interface, papercraft, developable surfaces},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647418,
author = {Boyko, Aleksey and Funkhouser, Thomas},
title = {Cheaper by the Dozen: Group Annotation of 3D Data},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647418},
doi = {10.1145/2642918.2647418},
abstract = {This paper proposes a group annotation approach to interactive semantic labeling of data and demonstrates the idea in a system for labeling objects in 3D LiDAR scans of a city. In this approach, the system selects a group of objects, predicts a semantic label for it, and highlights it in an interactive display. In response, the user either confirms the predicted label, provides a different label, or indicates that no single label can be assigned to all objects in the group. This sequence of interactions repeats until a label has been confirmed for every object in the data set. The main advantage of this approach is that it provides faster interactive labeling rates than alternative approaches, especially in cases where all labels must be explicitly confirmed by a person. The main challenge is to provide an algorithm that selects groups with many objects all of the same label type arranged in patterns that are quick to recognize, which requires models for predicting object labels and for estimating times for people to recognize objects in groups. We address these challenges by defining an objective function that models the estimated time required to process all unlabeled objects and approximation algorithms to minimize it. Results of user studies suggest that group annotation can be used to label objects in LiDAR scans of cities significantly faster than one-by-one annotation with active learning.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–42},
numpages = {10},
keywords = {object recognition, active learning, semantic annotation},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647349,
author = {LaToza, Thomas D. and Towne, W. Ben and Adriano, Christian M. and van der Hoek, Andr\'{e}},
title = {Microtask Programming: Building Software with a Crowd},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647349},
doi = {10.1145/2642918.2647349},
abstract = {Microtask crowdsourcing organizes complex work into workflows, decomposing large tasks into small, relatively independent microtasks. Applied to software development, this model might increase participation in open source software development by lowering the barriers to contribu-tion and dramatically decrease time to market by increasing the parallelism in development work. To explore this idea, we have developed an approach to decomposing programming work into microtasks. Work is coordinated through tracking changes to a graph of artifacts, generating appropriate microtasks and propagating change notifications to artifacts with dependencies. We have implemented our approach in CrowdCode, a cloud IDE for crowd development. To evaluate the feasibility of microtask programming, we performed a small study and found that a small crowd of 12 workers was able to successfully write 480 lines of code and 61 unit tests in 14.25 person-hours of time.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–54},
numpages = {12},
keywords = {programming tools, crowdsourcing, development environment},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647362,
author = {Hosio, Simo and Goncalves, Jorge and Lehdonvirta, Vili and Ferreira, Denzil and Kostakos, Vassilis},
title = {Situated Crowdsourcing Using a Market Model},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647362},
doi = {10.1145/2642918.2647362},
abstract = {Research is increasingly highlighting the potential for situated crowdsourcing to overcome some crucial limitations of online crowdsourcing. However, it remains unclear whether a situated crowdsourcing market can be sustained, and whether worker supply responds to price-setting in such a market. Our work is the first to systematically investigate workers' behaviour and response to economic incentives in a situated crowdsourcing market. We show that the market-based model is a sustainable approach to recruiting workers and obtaining situated crowdsourcing contributions. We also show that the price mechanism is a very effective tool for adjusting the supply of labour in a situated crowdsourcing market. Our work advances the body of work investigating situated crowdsourcing.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–64},
numpages = {10},
keywords = {crowdsourcing, situated technologies, virtual currency, market},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647386,
author = {Koyama, Yuki and Sakamoto, Daisuke and Igarashi, Takeo},
title = {Crowd-Powered Parameter Analysis for Visual Design Exploration},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647386},
doi = {10.1145/2642918.2647386},
abstract = {Parameter tweaking is one of the fundamental tasks in the editing of visual digital contents, such as correcting photo color or executing blendshape facial expression control. A problem with parameter tweaking is that it often requires much time and effort to explore a high-dimensional parameter space. We present a new technique to analyze such high-dimensional parameter space to obtain a distribution of human preference. Our method uses crowdsourcing to gather pairwise comparisons between various parameter sets. As a result of analysis, the user obtains a goodness function that computes the goodness value of a given parameter set. This goodness function enables two interfaces for exploration: Smart Suggestion, which provides suggestions of preferable parameter sets, and VisOpt Slider, which interactively visualizes the distribution of goodness values on sliders and gently optimizes slider values while the user is editing. We created four applications with different design parameter spaces. As a result, the system could facilitate the user's design exploration.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {65–74},
numpages = {10},
keywords = {design exploration, crowd-powered parameter analysis, human computation, parameter tweaking},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647409,
author = {Retelny, Daniela and Robaszkiewicz, S\'{e}bastien and To, Alexandra and Lasecki, Walter S. and Patel, Jay and Rahmati, Negar and Doshi, Tulsee and Valentine, Melissa and Bernstein, Michael S.},
title = {Expert Crowdsourcing with Flash Teams},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647409},
doi = {10.1145/2642918.2647409},
abstract = {We introduce flash teams, a framework for dynamically assembling and managing paid experts from the crowd. Flash teams advance a vision of expert crowd work that accomplishes complex, interdependent goals such as engineering and design. These teams consist of sequences of linked modular tasks and handoffs that can be computationally managed. Interactive systems reason about and manipulate these teams' structures: for example, flash teams can be recombined to form larger organizations and authored automatically in response to a user's request. Flash teams can also hire more people elastically in reaction to task needs, and pipeline intermediate output to accelerate completion times. To enable flash teams, we present Foundry, an end-user authoring platform and runtime manager. Foundry allows users to author modular tasks, then manages teams through handoffs of intermediate work. We demonstrate that Foundry and flash teams enable crowdsourcing of a broad class of goals including design prototyping, course development, and film animation, in half the work time of traditional self-managed teams.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–85},
numpages = {11},
keywords = {expert crowd work, flash teams, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647371,
author = {Chang, Kerry Shih-Ping and Myers, Brad A.},
title = {Creating Interactive Web Data Applications with Spreadsheets},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647371},
doi = {10.1145/2642918.2647371},
abstract = {While more and more data are available through web services, it remains difficult for end-users to create web applications that make use of these data without having to write complex code. We present Gneiss, a live programming environment that extends the spreadsheet metaphor to support creating interactive web applications that dynamically use local or web data from multiple sources. Gneiss closely integrates a spreadsheet editor with a web interface builder to let users demonstrate bindings between properties of web GUI elements and cells in the spreadsheet while working with real web service data. The spreadsheet editor provides two-way connections to web services, to both visualize and retrieve different data based on the user input in the web interface. Gneiss achieves rich interactivity without the need for event-based programming by extending the 'pull model' of formulas that is familiar to the spreadsheet users. We use a series of examples to demonstrate Gneiss's ability to create a variety of interactive web data applications.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {87–96},
numpages = {10},
keywords = {live programming, spreadsheets, end-user programming, mashups, web services, web applications},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647387,
author = {Benson, Edward and Zhang, Amy X. and Karger, David R.},
title = {Spreadsheet Driven Web Applications},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647387},
doi = {10.1145/2642918.2647387},
abstract = {Creating and publishing read-write-compute web applications requires programming skills beyond what most end users possess. But many end users know how to make spreadsheets that act as simple information management applications, some even with computation. We present a system for creating basic web applications using such spreadsheets in place of a server and using HTML to describe the client UI. Authors connect the two by placing spreadsheet references inside HTML attributes. Data computation is provided by spreadsheet formulas. The result is a reactive read-write-compute web page without a single line of Javascript code. Nearly all of the fifteen HTML novices we studied were able to connect HTML to spreadsheets using our method with minimal instruction. We draw conclusions from their experience and discuss future extensions to this programming model.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {97–106},
numpages = {10},
keywords = {web design, spreadsheets, information architecture, end-user programming},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647419,
author = {Hailpern, Joshua and Asur, Sitaram and Rector, Kyle},
title = {AttachMate: Highlight Extraction from Email Attachments},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647419},
doi = {10.1145/2642918.2647419},
abstract = {While email is a major conduit for information sharing in enterprise, there has been little work on exploring the files sent along with these messages -- attachments. These accompanying documents can be large (multiple megabytes), lengthy (multiple pages), and not optimized for the smaller screen sizes, limited reading time, and expensive bandwidth of mobile users. Thus, attachments can increase data storage costs (for both end users and email servers), drain users' time when irrelevant, cause important information to be missed when ignored, and pose a serious access issue for mobile users. To address these problems we created AttachMate, a novel email attachment summarization system. AttachMate can summarize the content of email attachments and automatically insert the summary into the text of the email. AttachMate also stores all files in the cloud, reducing file storage costs and bandwidth consumption. In this paper, the primary contribution is the AttachMate client/server architecture. To ground, support and validate the AttachMate system we present two upfront studies (813 participants) to understand the state and limitations of attachments, a novel algorithm to extract representative concept sentences (tested through two validation studies), and a user study of AttachMate within an enterprise.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {107–116},
numpages = {10},
keywords = {attachment summaries, summaries, email, attachment},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647357,
author = {Bigham, Jeffrey P.},
title = {Making the Web Easier to See with Opportunistic Accessibility Improvement},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647357},
doi = {10.1145/2642918.2647357},
abstract = {Many people would find the Web easier to use if content was a little bigger, even those who already find the Web possible to use now. This paper introduces the idea of opportunistic accessibility improvement in which improvements intended to make a web page easier to access, such as magnification, are automatically applied to the extent that they can be without causing negative side effects. We explore this idea with oppaccess.js, an easily-deployed system for magnifying web pages that iteratively increases magnification until it notices negative side effects, such as horizontal scrolling or overlapping text. We validate this approach by magnifying existing web pages 1.6x on average without introducing negative side effects. We believe this concept applies generally across a wide range of accessibility improvements designed to help people with diverse abilities.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {117–122},
numpages = {6},
keywords = {zoom, magnification, low vision, accessibility},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647401,
author = {Kim, Juho and Zhang, Amy X. and Kim, Jihee and Miller, Robert C. and Gajos, Krzysztof Z.},
title = {Content-Aware Kinetic Scrolling for Supporting Web Page Navigation},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647401},
doi = {10.1145/2642918.2647401},
abstract = {Long documents are abundant on the web today, and are accessed in increasing numbers from touchscreen devices such as mobile phones and tablets. Navigating long documents with small screens can be challenging both physically and cognitively because they compel the user to scroll a great deal and to mentally filter for important content. To support navigation of long documents on touchscreen devices, we introduce content-aware kinetic scrolling, a novel scrolling technique that dynamically applies pseudo-haptic feedback in the form of friction around points of high interest within the page. This allows users to quickly find interesting content while exploring without further cluttering the limited visual space. To model degrees of interest (DOI) for a variety of existing web pages, we introduce social wear, a method for capturing DOI based on social signals that indicate collective user interest. Our preliminary evaluation shows that users pay attention to items with kinetic scrolling feedback during search, recognition, and skimming tasks.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {123–127},
numpages = {5},
keywords = {read wear, pseudo-haptic feedback, kinetic scrolling, social wear},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647405,
author = {Rendl, Christian and Kim, David and Fanello, Sean and Parzer, Patrick and Rhemann, Christoph and Taylor, Jonathan and Zirkl, Martin and Scheipl, Gregor and Rothl\"{a}nder, Thomas and Haller, Michael and Izadi, Shahram},
title = {FlexSense: A Transparent Self-Sensing Deformable Surface},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647405},
doi = {10.1145/2642918.2647405},
abstract = {We present FlexSense, a new thin-film, transparent sensing surface based on printed piezoelectric sensors, which can reconstruct complex deformations without the need for any external sensing, such as cameras. FlexSense provides a fully self-contained setup which improves mobility and is not affected from occlusions. Using only a sparse set of sensors, printed on the periphery of the surface substrate, we devise two new algorithms to fully reconstruct the complex deformations of the sheet, using only these sparse sensor measurements. An evaluation shows that both proposed algorithms are capable of reconstructing complex deformations accurately. We demonstrate how FlexSense can be used for a variety of 2.5D interactions, including as a transparent cover for tablets where bending can be performed alongside touch to enable magic lens style effects, layered input, and mode switching, as well as the ability to use our device as a high degree-of-freedom input controller for gaming and beyond.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {129–138},
numpages = {10},
keywords = {self-contained, flexible transparent sensor, shape and deformation reconstruction},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647385,
author = {Seifert, Julian and Boring, Sebastian and Winkler, Christian and Schaub, Florian and Schwab, Fabian and Herrdum, Steffen and Maier, Fabian and Mayer, Daniel and Rukzio, Enrico},
title = {Hover Pad: Interacting with Autonomous and Self-Actuated Displays in Space},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647385},
doi = {10.1145/2642918.2647385},
abstract = {Handheld displays enable flexible spatial exploration of information spaces -- users can physically navigate through three-dimensional space to access information at specific locations. Having users constantly hold the display, however, has several limitations: (1) inaccuracies due to natural hand tremors; (2) fatigue over time; and (3) limited exploration within arm's reach. We investigate autonomous, self-actuated displays that can freely move and hold their position and orientation in space without users having to hold them at all times. We illustrate various stages of such a display's autonomy ranging from manual to fully autonomous, which -- depending on the tasks -- facilitate the interaction. Further, we discuss possible motion control mechanisms for these displays and present several interaction techniques enabled by such displays. Our Hover Pad toolkit enables exploring five degrees of freedom of self-actuated and autonomous displays and the developed control and interaction techniques. We illustrate the utility of our toolkit with five prototype applications, such as a volumetric medical data explorer.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {139–147},
numpages = {9},
keywords = {self-actuated displays, volumetric data sets, interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647370,
author = {Sugiura, Yuta and Toda, Koki and Hoshi, Takayuki and Kamiyama, Youichi and Igarashi, Takeo and Inami, Masahiko},
title = {Graffiti Fur: Turning Your Carpet into a Computer Display},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647370},
doi = {10.1145/2642918.2647370},
abstract = {We devised a display technology that utilizes the phenomenon whereby the shading properties of fur change as the fibers are raised or flattened. One can erase drawings by first flattening the fibers by sweeping the surface by hand in the fiber's growth direction, and then draw lines by raising the fibers by moving the finger in the opposite direction. These material properties can be found in various items such as carpets in our living environments. We have developed three different devices to draw patterns on a "fur display" utilizing this phenomenon: a roller device, a pen device and pressure projection device. Our technology can turn ordinary objects in our environment into rewritable displays without requiring or creating any non-reversible modifications to them. In addition, it can be used to present large-scale image without glare, and the images it creates require no running costs to maintain.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {149–156},
numpages = {8},
keywords = {living environment, fur display, BRDF},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647376,
author = {Kienzle, Wolf and Hinckley, Ken},
title = {LightRing: Always-Available 2D Input on Any Surface},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647376},
doi = {10.1145/2642918.2647376},
abstract = {We present LightRing, a wearable sensor in a ring form factor that senses the 2d location of a fingertip on any surface, independent of orientation or material. The device consists of an infrared proximity sensor for measuring finger flexion and a 1-axis gyroscope for measuring finger rotation. Notably, LightRing tracks subtle fingertip movements from the finger base without requiring instrumentation of other body parts or the environment. This keeps the normal hand function intact and allows for a socially acceptable appearance. We evaluate LightRing in a 2d pointing experiment in two scenarios: on a desk while sitting down, and on the leg while standing. Our results indicate that the device has potential to enable a variety of rich mobile input scenarios.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {157–160},
numpages = {4},
keywords = {sensors, finger, ring, mobile, wearable, input},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647396,
author = {Dementyev, Artem and Paradiso, Joseph A.},
title = {WristFlex: Low-Power Gesture Input with Wrist-Worn Pressure Sensors},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647396},
doi = {10.1145/2642918.2647396},
abstract = {In this paper we present WristFlex, an always-available on-body gestural interface. Using an array of force sensitive resistors (FSRs) worn around the wrist, the interface can distinguish subtle finger pinch gestures with high accuracy (&gt;80 %) and speed. The system is trained to classify gestures from subtle tendon movements on the wrist. We demonstrate that WristFlex is a complete system that works wirelessly in real-time. The system is simple and light-weight in terms of power consumption and computational overhead. WristFlex's sensor power consumption is 60.7 uW, allowing the prototype to potentially last more then a week on a small lithium polymer battery. Also, WristFlex is small and non-obtrusive, and can be integrated into a wristwatch or a bracelet. We perform user studies to evaluate the accuracy, speed, and repeatability. We demonstrate that the number of gestures can be extended with orientation data from an accelerometer. We conclude by showing example applications.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {161–166},
numpages = {6},
keywords = {wrist interface, machine learning, gesture recognition, FSR},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647395,
author = {Adar, Eytan and Dontcheva, Mira and Laput, Gierad},
title = {CommandSpace: Modeling the Relationships between Tasks, Descriptions and Features},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647395},
doi = {10.1145/2642918.2647395},
abstract = {Users often describe what they want to accomplish with an application in a language that is very different from the application's domain language. To address this gap between system and human language, we propose modeling an application's domain language by mining a large corpus of Web documents about the application using deep learning techniques. A high dimensional vector space representation can model the relationships between user tasks, system commands, and natural language descriptions and supports mapping operations, such as identifying likely system commands given natural language queries and identifying user tasks given a trace of user operations. We demonstrate the feasibility of this approach with a system, CommandSpace, for the popular photo editing application Adobe Photoshop. We build and evaluate several applications enabled by our model showing the power and flexibility of this approach.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {167–176},
numpages = {10},
keywords = {application language domain, deep-learning, natural language interfaces},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647408,
author = {Green, Spence and Chuang, Jason and Heer, Jeffrey and Manning, Christopher D.},
title = {Predictive Translation Memory: A Mixed-Initiative System for Human Language Translation},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647408},
doi = {10.1145/2642918.2647408},
abstract = {The standard approach to computer-aided language translation is post-editing: a machine generates a single translation that a human translator corrects. Recent studies have shown this simple technique to be surprisingly effective, yet it underutilizes the complementary strengths of precision-oriented humans and recall-oriented machines. We present Predictive Translation Memory, an interactive, mixed-initiative system for human language translation. Translators build translations incrementally by considering machine suggestions that update according to the user's current partial translation. In a large-scale study, we find that professional translators are slightly slower in the interactive mode yet produce slightly higher quality translations despite significant prior experience with the baseline post-editing condition. Our analysis identifies significant predictors of time and quality, and also characterizes interactive aid usage. Subjects entered over 99% of characters via interactive aids, a significantly higher fraction than that shown in previous work.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {177–187},
numpages = {11},
keywords = {interface design, language translation, mixed-initiative, empirical study},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647403,
author = {Hara, Kotaro and Sun, Jin and Moore, Robert and Jacobs, David and Froehlich, Jon},
title = {Tohme: Detecting Curb Ramps in Google Street View Using Crowdsourcing, Computer Vision, and Machine Learning},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647403},
doi = {10.1145/2642918.2647403},
abstract = {Building on recent prior work that combines Google Street View (GSV) and crowdsourcing to remotely collect information on physical world accessibility, we present the first 'smart' system, Tohme, that combines machine learning, computer vision (CV), and custom crowd interfaces to find curb ramps remotely in GSV scenes. Tohme consists of two workflows, a human labeling pipeline and a CV pipeline with human verification, which are scheduled dynamically based on predicted performance. Using 1,086 GSV scenes (street intersections) from four North American cities and data from 403 crowd workers, we show that Tohme performs similarly in detecting curb ramps compared to a manual labeling approach alone (F- measure: 84% vs. 86% baseline) but at a 13% reduction in time cost. Our work contributes the first CV-based curb ramp detection system, a custom machine-learning based workflow controller, a validation of GSV as a viable curb ramp data source, and a detailed examination of why curb ramp detection is a hard problem along with steps forward.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {189–204},
numpages = {16},
keywords = {crowdsourcing accessibility, computer vision, amazon mechanical turk, Google street view},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647348,
author = {Xia, Haijun and Jota, Ricardo and McCanny, Benjamin and Yu, Zhe and Forlines, Clifton and Singh, Karan and Wigdor, Daniel},
title = {Zero-Latency Tapping: Using Hover Information to Predict Touch Locations and Eliminate Touchdown Latency},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647348},
doi = {10.1145/2642918.2647348},
abstract = {A method of reducing the perceived latency of touch input by employing a model to predict touch events before the finger reaches the touch surface is proposed. A corpus of 3D finger movement data was collected, and used to develop a model capable of three granularities at different phases of movement: initial direction, final touch location, time of touchdown. The model is validated for target distances &gt;= 25.5cm, and demonstrated to have a mean accuracy of 1.05cm 128ms before the user touches the screen. Preference study of different levels of latency reveals a strong preference for unperceived latency touchdown feedback. A form of 'soft' feedback, as well as other uses for this prediction to improve performance, is proposed.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {205–214},
numpages = {10},
keywords = {prediction, pre-touch, model, latency},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647363,
author = {McGrath, William and Li, Yang},
title = {Detecting Tapping Motion on the Side of Mobile Devices by Probabilistically Combining Hand Postures},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647363},
doi = {10.1145/2642918.2647363},
abstract = {We contribute a novel method for detecting finger taps on the different sides of a smartphone, using the built-in motion sensors of the device. In particular, we discuss new features and algorithms that infer side taps by probabilistically combining estimates of tap location and the hand pose--the hand holding the device. Based on a dataset collected from 9 participants, our method achieved 97.3% precision and 98.4% recall on tap event detection against ambient motion. For detecting single-tap locations, our method outperformed an approach that uses inferred hand postures deterministically by 3% and an approach that does not use hand posture inference by 17%. For inferring the location of two consecutive side taps from the same direction, our method outperformed the two baseline approaches by 6% and 17% respectively. We discuss our insights into designing the detection algorithm and the implication on side tap-based interaction behaviors.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {215–219},
numpages = {5},
keywords = {motion gestures, mobile interaction, machine learning},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647412,
author = {Dixon, Morgan and Nied, Alexander and Fogarty, James},
title = {Prefab Layers and Prefab Annotations: Extensible Pixel-Based Interpretation of Graphical Interfaces},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647412},
doi = {10.1145/2642918.2647412},
abstract = {Pixel-based methods have the potential to fundamentally change how we build graphical interfaces, but remain difficult to implement. We introduce a new toolkit for pixel based enhancements, focused on two areas of support. Prefab Layers helps developers write interpretation logic that can be composed, reused, and shared to manage the multi-faceted nature of pixel-based interpretation. Prefab Annotations supports robustly annotating interface elements with metadata needed to enable runtime enhancements. Together, these help developers overcome subtle but critical dependencies between code and data. We validate our toolkit with (1) demonstrative applications and (2) a lab study that compares how developers build an enhancement using our toolkit versus state of the art methods. Our toolkit addresses core challenges faced by developers when building pixel based enhancements, potentially opening up pixel based systems to broader adoption.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {221–230},
numpages = {10},
keywords = {prefab, pixel-based reverse engineering, layers, annotations},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647378,
author = {Hottelier, Thibaud and Bodik, Ras and Ryokai, Kimiko},
title = {Programming by Manipulation for Layout},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647378},
doi = {10.1145/2642918.2647378},
abstract = {We present Programming by Manipulation, a new programming methodology for specifying the layout of data visualizations, targeted at non-programmers. We address the two central sources of bugs that arise when programming with constraints: ambiguities and conflicts (inconsistencies). We rule out conflicts by design and exploit ambiguity to explore possible layout designs. Our users design layouts by highlighting undesirable aspects of a current design, effectively breaking spurious constraints and introducing ambiguity by giving some elements freedom to move or resize. Subsequently, the tool indicates how the ambiguity can be removed, by computing how the free elements can be fixed with available constraints. To support this workflow, our tool computes the ambiguity and summarizes it visually. We evaluate our work with two user-studies demonstrating that both non-programmers and programmers can effectively use our prototype. Our results suggest that our tool is 5-times more productive than direct programming with constraints.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {231–241},
numpages = {11},
keywords = {programming by demonstration, layout editing, constraint-based layout},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647398,
author = {Xu, Pengfei and Fu, Hongbo and Igarashi, Takeo and Tai, Chiew-Lan},
title = {Global Beautification of Layouts with Interactive Ambiguity Resolution},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647398},
doi = {10.1145/2642918.2647398},
abstract = {Automatic global beautification methods have been proposed for sketch-based interfaces, but they can lead to undesired results due to ambiguity in the user's input. To facilitate ambiguity resolution in layout beautification, we present a novel user interface for visualizing and editing inferred relationships. First, our interface provides a preview of the beautified layout with inferred constraints, without directly modifying the input layout. In this way, the user can easily keep refining beautification results by interactively repositioning and/or resizing elements in the input layout. Second, we present a gestural interface for editing automatically inferred constraints by directly interacting with the visualized constraints via simple gestures. Our efficient implementation of the beautification system provides the user instant feedback. Our user studies validate that our tool is capable of creating, editing and refining layouts of graphic elements and is significantly faster than the standard snap-dragging and command-based alignment tools.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {243–252},
numpages = {10},
keywords = {global beautification, gestural interface, snapping, ambiguity resolution, layout editing, alignment},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647411,
author = {Harper, Jonathan and Agrawala, Maneesh},
title = {Deconstructing and Restyling D3 Visualizations},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647411},
doi = {10.1145/2642918.2647411},
abstract = {The D3 JavaScript library has become a ubiquitous tool for developing visualizations on the Web. Yet, once a D3 visualization is published online its visual style is difficult to change. We present a pair of tools for deconstructing and restyling existing D3 visualizations. Our deconstruction tool analyzes a D3 visualization to extract the data, the marks and the mappings between them. Our restyling tool lets users modify the visual attributes of the marks as well as the mappings from the data to these attributes. Together our tools allow users to easily modify D3 visualizations without examining the underlying code and we show how they can be used to deconstruct and restyle a variety of D3 visualizations.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {253–262},
numpages = {10},
keywords = {information extraction, visualization, chart understanding, restyling, D3, redesign},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647358,
author = {Oney, Stephen and Myers, Brad and Brandt, Joel},
title = {InterState: A Language and Environment for Expressing Interface Behavior},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647358},
doi = {10.1145/2642918.2647358},
abstract = {InterState is a new programming language and environment that addresses the challenges of writing and reusing user interface code. InterState represents interactive behaviors clearly and concisely using a combination of novel forms of state machines and constraints. It also introduces new language features that allow programmers to easily modularize and reuse behaviors. InterState uses a new visual notation that allows programmers to better understand and navigate their code. InterState also includes a live editor that immediately updates the running application in response to changes in the editor and vice versa to help programmers understand the state of their program. Finally, InterState can interface with code and widgets written in other languages, for example to create a user interface in InterState that communicates with a database. We evaluated the understandability of InterState's programming primitives in a comparative laboratory study. We found that participants were twice as fast at understanding and modifying GUI components when they were implemented with InterState than when they were implemented in a conventional textual event-callback style. We evaluated InterState's scalability with a series of benchmarks and example applications and found that it can scale to implement complex behaviors involving thousands of objects and constraints.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {263–272},
numpages = {10},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647359,
author = {Mueller, Stefanie and Im, Sangha and Gurevich, Serafima and Teibrich, Alexander and Pfisterer, Lisa and Guimbreti\`{e}re, Fran\c{c}ois and Baudisch, Patrick},
title = {WirePrint: 3D Printed Previews for Fast Prototyping},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647359},
doi = {10.1145/2642918.2647359},
abstract = {Even though considered a rapid prototyping tool, 3D printing is so slow that a reasonably sized object requires printing overnight. This slows designers down to a single iteration per day. In this paper, we propose to instead print low-fidelity wireframe previews in the early stages of the design process. Wireframe previews are 3D prints in which surfaces have been replaced with a wireframe mesh. Since wireframe previews are to scale and represent the overall shape of the 3D object, they allow users to quickly verify key aspects of their 3D design, such as the ergonomic fit. To maximize the speed-up, we instruct 3D printers to extrude filament not layer-by-layer, but directly in 3D-space, allowing them to create the edges of the wireframe model directly one stroke at a time. This allows us to achieve speed-ups of up to a factor of 10 compared to traditional layer-based printing. We demonstrate how to achieve wireframe previews on standard FDM 3D printers, such as the PrintrBot or the Kossel mini. Users only need to install the WirePrint software, making our approach applicable to many 3D printers already in use today. Finally, wireframe previews use only a fraction of material required for a regular print, making it even more affordable to iterate.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {273–280},
numpages = {8},
keywords = {rapid prototyping, 3D printing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647413,
author = {Olberding, Simon and Wessely, Michael and Steimle, J\"{u}rgen},
title = {PrintScreen: Fabricating Highly Customizable Thin-Film Touch-Displays},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647413},
doi = {10.1145/2642918.2647413},
abstract = {PrintScreen is an enabling technology for digital fabrication of customized flexible displays using thin-film electroluminescence (TFEL). It enables inexpensive and rapid fabrication of highly customized displays in low volume, in a simple lab environment, print shop or even at home. We show how to print ultra-thin (120 µm) segmented and passive matrix displays in greyscale or multi-color on a variety of deformable and rigid substrate materials, including PET film, office paper, leather, metal, stone, and wood. The displays can have custom, unconventional 2D shapes and can be bent, rolled and folded to create 3D shapes. We contribute a systematic overview of graphical display primitives for customized displays and show how to integrate them with static print and printed electronics. Furthermore, we contribute a sensing framework, which leverages the display itself for touch sensing. To demonstrate the wide applicability of PrintScreen, we present application examples from ubiquitous, mobile and wearable computing.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {281–290},
numpages = {10},
keywords = {tfel, printed electronics, digital fabrication, electroluminescence, touch input, rapid prototyping, thin-film display, flexible display, ubiquitous computing.},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647421,
author = {Lo, Joanne and Paulos, Eric},
title = {ShrinkyCircuits: Sketching, Shrinking, and Formgiving for Electronic Circuits},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647421},
doi = {10.1145/2642918.2647421},
abstract = {In this paper we describe the development of ShrinkyCircuits, a novel electronic prototyping technique that captures the flexibility of sketching and leverages properties of a common everyday plastic polymer to enable low-cost, miniature, planar, and curved, multi-layer circuit designs in minutes. ShrinkyCircuits take advantage of inexpensive prestressed polymer film that shrinks to its original size when exposed to heat. This enables improved electrical characteristics though sintering of the conductive electrical layer, partial self-assembly of the circuit and components, and mechanically robust custom shapes Including curves and non-planar form factors. We demonstrate the range and adaptability of ShrinkyCircuits designs from simple hand drawn circuits with through-hole components to complex multilayer, printed circuit boards (PCB), with curved and irregular shaped electronic layouts and surface mount components. Our approach enables users to create extremely customized circuit boards with dense circuit layouts while avoiding messy chemical etching, expensive board milling machines, or time consuming delays in using outside PCB production houses.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {291–299},
numpages = {9},
keywords = {electronics, sustainability design, education, DIY, sketching, prototyping, fabrication, creativity support tools},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647368,
author = {Frey, J\'{e}r\'{e}my and Gervais, Renaud and Fleck, St\'{e}phanie and Lotte, Fabien and Hachet, Martin},
title = {Teegi: Tangible EEG Interface},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647368},
doi = {10.1145/2642918.2647368},
abstract = {We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that enables novice users to get to know more about something as complex as brain signals, in an easy, engaging and informative way. To this end, we have designed a new system based on a unique combination of spatial augmented reality, tangible interaction and real-time neurotechnologies. With Teegi, a user can visualize and analyze his or her own brain activity in real-time, on a tangible character that can be easily manipulated, and with which it is possible to interact. An exploration study has shown that interacting with Teegi seems to be easy, motivating, reliable and informative. Overall, this suggests that Teegi is a promising and relevant training and mediation tool for the general public.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {301–308},
numpages = {8},
keywords = {tangible interaction, learning, spatial augmented reality, EEG},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647394,
author = {Rebenitsch, Lisa and Owen, Charles},
title = {Individual Variation in Susceptibility to Cybersickness},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647394},
doi = {10.1145/2642918.2647394},
abstract = {We examined background characteristics of virtual reality participants in order to determine correlations to cybersickness. As 3D media and new VR display technologies from companies such as Occulus and Sony become more popular, the incidence of cybersickness is likely to increase. Understanding the impact of individual backgrounds on susceptibility can help shed light on which individuals are more likely to be impacted. Past history of motion sickness and video game play have the best predictive power of cybersickness of the factors studied. A model to estimate the likelihood of cybersickness using background characteristics is posed.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {309–317},
numpages = {9},
keywords = {virtual reality, cybersickness, individual variation, habituation},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647373,
author = {Song, Jie and S\"{o}r\"{o}s, G\'{a}bor and Pece, Fabrizio and Fanello, Sean Ryan and Izadi, Shahram and Keskin, Cem and Hilliges, Otmar},
title = {In-Air Gestures around Unmodified Mobile Devices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647373},
doi = {10.1145/2642918.2647373},
abstract = {We present a novel machine learning based algorithm extending the interaction space around mobile devices. The technique uses only the RGB camera now commonplace on off-the-shelf mobile devices. Our algorithm robustly recognizes a wide range of in-air gestures, supporting user variation, and varying lighting conditions. We demonstrate that our algorithm runs in real-time on unmodified mobile devices, including resource-constrained smartphones and smartwatches. Our goal is not to replace the touchscreen as primary input device, but rather to augment and enrich the existing interaction vocabulary using gestures. While touch input works well for many scenarios, we demonstrate numerous interaction tasks such as mode switches, application and task management, menu selection and certain types of navigation, where such input can be either complemented or better served by in-air gestures. This removes screen real-estate issues on small touchscreens, and allows input to be expanded to the 3D space around the device. We present results for recognition accuracy (93% test and 98% train), impact of memory footprint and other model parameters. Finally, we report results from preliminary user evaluations, discuss advantages and limitations and conclude with directions for future work.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {319–329},
numpages = {11},
keywords = {HCI, mobile gestures, gesture recognition, random forests, mobile computing, mobile interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647417,
author = {Sukan, Mengu and Elvezio, Carmine and Oda, Ohan and Feiner, Steven and Tversky, Barbara},
title = {ParaFrustum: Visualization Techniques for Guiding a User to a Constrained Set of Viewing Positions and Orientations},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647417},
doi = {10.1145/2642918.2647417},
abstract = {Many tasks in real or virtual environments require users to view a target object or location from one of a set of strategic viewpoints to see it in context, avoid occlusions, or view it at an appropriate angle or distance. We introduce ParaFrustum, a geometric construct that represents this set of strategic viewpoints and viewing directions. ParaFrustum is inspired by the look-from and look-at points of a computer graphics camera specification, which precisely delineate a location for the camera and a direction in which it looks. We generalize this approach by defining a ParaFrustum in terms of a look-from volume and a look-at volume, which establish constraints on a range of acceptable locations for the user's eyes and a range of acceptable angles in which the user's head can be oriented. Providing tolerance in the allowable viewing positions and directions avoids burdening the user with the need to assume a tightly constrained 6DoF pose when it is not required by the task. We describe two visualization techniques for virtual or augmented reality that guide a user to assume one of the poses defined by a ParaFrustum, and present the results of a user study measuring the performance of these techniques. The study shows that the constraints of a tightly constrained ParaFrustum (e.g., approximating a conventional camera frustum) require significantly more time to satisfy than those of a loosely constrained one. The study also reveals interesting differences in participant trajectories in response to the two techniques.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {331–340},
numpages = {10},
keywords = {viewing specification, view frustum, virtual reality, augmented reality},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647351,
author = {Martinez Plasencia, Diego and Berthaut, Florent and Karnik, Abhijit and Subramanian, Sriram},
title = {Through the Combining Glass},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647351},
doi = {10.1145/2642918.2647351},
abstract = {Reflective optical combiners like beam splitters and two way mirrors are used in AR to overlap digital contents on the users' hands or bodies. Augmentations are usually unidirectional, either reflecting virtual contents on the user's body (Situated Augmented Reality) or augmenting user's reflections with digital contents (AR mirrors). But many other novel possibilities remain unexplored. For example, users' hands, reflected inside a museum AR cabinet, can allow visitors to interact with the artifacts exhibited. Projecting on the user's hands as their reflection cuts through the objects can be used to reveal objects' internals. Augmentations from both sides are blended by the combiner, so they are consistently seen by any number of users, independently of their location or, even, the side of the combiner through which they are looking. This paper explores the potential of optical combiners to merge the space in front and behind them. We present this design space, identify novel augmentations/interaction opportunities and explore the design space using three prototypes.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {341–350},
numpages = {10},
keywords = {multi-user, augmented reality, optical combiners},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647361,
author = {Lyons, Kent and Kim, Seung Wook and Seko, Shigeyuki and Nguyen, David and Desjardins, Audrey and Vidal, M\'{e}lodie and Dobbelstein, David and Rubin, Jeremy},
title = {Loupe: A Handheld near-Eye Display},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647361},
doi = {10.1145/2642918.2647361},
abstract = {Loupe is a novel interactive device with a near-eye virtual display similar to head-up display glasses that retains a handheld form factor. We present our hardware implementation and discuss our user interface that leverages Loupe's unique combination of properties. In particular, we present our input capabilities, spatial metaphor, opportunities for using the round aspect of Loupe, and our use of focal depth. We demonstrate how those capabilities come together in an example application designed to allow quick access to information feeds.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {351–354},
numpages = {4},
keywords = {handheld, virtual displays, near-eye display},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647353,
author = {Leigh, Darren and Forlines, Clifton and Jota, Ricardo and Sanders, Steven and Wigdor, Daniel},
title = {High Rate, Low-Latency Multi-Touch Sensing with Simultaneous Orthogonal Multiplexing},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647353},
doi = {10.1145/2642918.2647353},
abstract = {We present "Fast Multi-Touch" (FMT), an extremely high frame rate and low-latency multi-touch sensor based on a novel projected capacitive architecture that employs simultaneous orthogonal signals. The sensor has a frame rate of 4000 Hz and a touch-to-data output latency of only 40 microseconds, providing unprecedented responsiveness. FMT is demonstrated with a high-speed DLP projector yielding a touch-to-light latency of 110 microseconds.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {355–364},
numpages = {10},
keywords = {capacitive sensing, parallelism, user interface, low-latency, multi-touch},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647384,
author = {Liang, Rong-Hao and Kuo, Han-Chih and Chan, Liwei and Yang, De-Nian and Chen, Bing-Yu},
title = {GaussStones: Shielded Magnetic Tangibles for Multi-Token Interactions on Portable Displays},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647384},
doi = {10.1145/2642918.2647384},
abstract = {This work presents GaussStones, a system of shielded magnetic tangibles design for supporting multi-token interactions on portable displays. Unlike prior works in sensing magnetic tangibles on portable displays, the proposed tangible design applies magnetic shielding by using an inexpensive galvanized steel case, which eliminates interference between magnetic tangibles. An analog Hall-sensor grid can recognize the identity of each shielded magnetic unit since each unit generates a magnetic field with a specific intensity distribution and/or polarization. Combining multiple units as a knob further allows for resolving additional identities and their orientations. Enabling these features improves support for applications involving multiple tokens. Thus, using prevalent portable displays provides generic platforms for tangible interaction design.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {365–372},
numpages = {8},
keywords = {tangible interactions, portable display, magnetism, gausssense, analog hall-sensor grid},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647404,
author = {Manabe, Hiroyuki and Yamada, Wataru and Inamura, Hiroshi},
title = {Tag System with Low-Powered Tag and Depth Sensing Camera},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647404},
doi = {10.1145/2642918.2647404},
abstract = {A tag system is proposed that offers a practical approach to ubiquitous computing. It provides small and low-power tags that are easy to distribute; does not need a special device to read the tags (in the future), thus enabling their use anytime, anywhere; and has a wide reading range in angle and distance that extends the design space of tag-based applications. The tag consists of a kind of liquid crystal (LC) and a retroreflector, and it sends its ID by switching the LC. A depth sensing camera that emits infrared (IR) is used as the tag reader; we assume that it will be part of the user's everyday devices, such as a smartphone. Experiments were conducted to confirm its potential, and a regular IR camera was also tested for comparison. The results show that the tag system has a wide readable range in terms of both distance (up to 8m) and viewing angle offset. Several applications were also developed to explore the design space. Finally, limitations of the current setup and possible improvements are discussed.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {373–382},
numpages = {10},
keywords = {liquid crystal, AR, tag, PDLC, depth sensing camera},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647381,
author = {Han, Jaehyun and Gu, Jiseong and Lee, Geehyuk},
title = {Trampoline: A Double-Sided Elastic Touch Device for Creating Reliefs},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647381},
doi = {10.1145/2642918.2647381},
abstract = {Although reliefs are frequently used to add patterns to product surfaces, there is a lack of interaction techniques to model reliefs on the surface of virtual objects. We adopted the repouss\'{e} and chasing artwork techniques in an alternative interaction technique to model relief on virtual surfaces. To support this interaction technique, we developed the double-sided touchpad Trampoline that can detect the position and force of a finger touch on both sides. Additionally, Trampoline provides users with elastic feedback, as its surface consists of a stretchable fabric. We implemented a relief application with this device and the developed interaction technique. An informal user study showed that the proposed system can be a promising solution to create reliefs.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {383–388},
numpages = {6},
keywords = {repouss\'{e} and chasing, trampoline, 3D modeling, elastic touchpad, double-sided touchpad},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647356,
author = {Laput, Gierad and Xiao, Robert and Chen, Xiang 'Anthony' and Hudson, Scott E. and Harrison, Chris},
title = {Skin Buttons: Cheap, Small, Low-Powered and Clickable Fixed-Icon Laser Projectors},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647356},
doi = {10.1145/2642918.2647356},
abstract = {Smartwatches are a promising new interactive platform, but their small size makes even basic actions cumbersome. Hence, there is a great need for approaches that expand the interactive envelope around smartwatches, allowing human input to escape the small physical confines of the device. We propose using tiny projectors integrated into the smartwatch to render icons on the user's skin. These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments, we show that these 'skin buttons' can have high touch accuracy and recognizability, while being low cost and power-efficient.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {389–394},
numpages = {6},
keywords = {around device interaction, ADI, wearable devices, touch input, mobile computing, interaction techniques, smartwatch, on-body computing, sensors},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647375,
author = {Kazi, Rubaiat Habib and Chevalier, Fanny and Grossman, Tovi and Fitzmaurice, George},
title = {Kitty: Sketching Dynamic and Interactive Illustrations},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647375},
doi = {10.1145/2642918.2647375},
abstract = {We present Kitty, a sketch-based tool for authoring dynamic and interactive illustrations. Artists can sketch animated drawings and textures to convey the living phenomena, and specify the functional relationship between its entities to characterize the dynamic behavior of systems and environments. An underlying graph model, customizable through sketching, captures the functional relationships between the visual, spatial, temporal or quantitative parameters of its entities. As the viewer interacts with the resulting dynamic interactive illustration, the parameters of the drawing change accordingly, depicting the dynamics and chain of causal effects within a scene. The generality of this framework makes our tool applicable for a variety of purposes, including technical illustrations, scientific explanation, infographics, medical illustrations, children's e-books, cartoon strips and beyond. A user study demonstrates the ease of usage, variety of applications, artistic expressiveness and creative possibilities of our tool.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {395–405},
numpages = {11},
keywords = {sketching, interactive illustrations, causal animation},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647399,
author = {Xie, Jun and Hertzmann, Aaron and Li, Wilmot and Winnem\"{o}ller, Holger},
title = {PortraitSketch: Face Sketching Assistance for Novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647399},
doi = {10.1145/2642918.2647399},
abstract = {We present PortraitSketch, an interactive drawing system that helps novices create pleasing, recognizable face sketches without requiring prior artistic training. As the user traces over a source portrait photograph, PortraitSketch automatically adjusts the geometry and stroke parameters (thickness, opacity, etc.) to improve the aesthetic quality of the sketch. We present algorithms for adjusting both outlines and shading strokes based on important features of the underlying source image. In contrast to automatic stylization systems, PortraitSketch is designed to encourage a sense of ownership and accomplishment in the user. To this end, all adjustments are performed in real-time, and the user ends up directly drawing all strokes on the canvas. The findings from our user study suggest that users prefer drawing with some automatic assistance, thereby producing better drawings, and that assistance does not decrease the perceived level of involvement in the creative process.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {407–417},
numpages = {11},
keywords = {creativity support tool, sketching, portraits, novices},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647415,
author = {Benedetti, Luca and Winnem\"{o}ller, Holger and Corsini, Massimiliano and Scopigno, Roberto},
title = {Painting with Bob: Assisted Creativity for Novices},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647415},
doi = {10.1145/2642918.2647415},
abstract = {Current digital painting tools are primarily targeted at professionals and are often overwhelmingly complex for use by novices. At the same time, simpler tools may not invoke the user creatively, or are limited to plain styles that lack visual sophistication. There are many people who are not art professionals, yet would like to partake in digital creative expression. Challenges and rewards for novices differ greatly from those for professionals. In this paper, we leverage existing works in Creativity and Creativity Support Tools (CST) to formulate design goals specifically for digital art creation tools for novices. We implemented these goals within a digital painting system, called Painting with Bob. We evaluate the efficacy of the design and our prototype with a user study, and we find that users are highly satisfied with the user experience, as well as the paintings created with our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {419–428},
numpages = {10},
keywords = {painting, creativity, novices},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647420,
author = {Fourney, Adam and Lafreniere, Ben and Chilana, Parmit and Terry, Michael},
title = {InterTwine: Creating Interapplication Information Scent to Support Coordinated Use of Software},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647420},
doi = {10.1145/2642918.2647420},
abstract = {Users often make continued and sustained use of online resources to complement use of a desktop application. For example, users may reference online tutorials to recall how to perform a particular task. While often used in a coordinated fashion, the browser and desktop application provide separate, independent mechanisms for helping users find and re-find task-relevant information. In this paper, we describe InterTwine, a system that links information in the web browser with relevant elements in the desktop application to create interapplication information scent. This explicit link produces a shared interapplication history to assist in re-finding information in both applications. As an example, InterTwine marks all menu items in the desktop application that are currently mentioned in the front-most web page. This paper introduces the notion of interapplication information scent, demonstrates the concept in InterTwine, and describes results from a formative study suggesting the utility of the concept.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {429–438},
numpages = {10},
keywords = {interapplication information scent, finding &amp; re-finding},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647406,
author = {Rubin, Steve and Agrawala, Maneesh},
title = {Generating Emotionally Relevant Musical Scores for Audio Stories},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647406},
doi = {10.1145/2642918.2647406},
abstract = {Highly-produced audio stories often include musical scores that reflect the emotions of the speech. Yet, creating effective musical scores requires deep expertise in sound production and is time-consuming even for experts. We present a system and algorithm for re-sequencing music tracks to generate emotionally relevant music scores for audio stories. The user provides a speech track and music tracks and our system gathers emotion labels on the speech through hand-labeling, crowdsourcing, and automatic methods. We develop a constraint-based dynamic programming algorithm that uses these emotion labels to generate emotionally relevant musical scores. We demonstrate the effectiveness of our algorithm by generating 20 musical scores for audio stories and showing that crowd workers rank their overall quality significantly higher than stories without music.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {439–448},
numpages = {10},
keywords = {storytelling, audio stories, musical scores, music retargeting},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647372,
author = {Gauglitz, Steffen and Nuernberger, Benjamin and Turk, Matthew and H\"{o}llerer, Tobias},
title = {World-Stabilized Annotations and Virtual Scene Navigation for Remote Collaboration},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647372},
doi = {10.1145/2642918.2647372},
abstract = {We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {449–459},
numpages = {11},
keywords = {telepresence, augmented reality, video-mediated communication, CSCW},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647377,
author = {Leithinger, Daniel and Follmer, Sean and Olwal, Alex and Ishii, Hiroshi},
title = {Physical Telepresence: Shape Capture and Display for Embodied, Computer-Mediated Remote Collaboration},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647377},
doi = {10.1145/2642918.2647377},
abstract = {We propose a new approach to Physical Telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects, and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {461–470},
numpages = {10},
keywords = {shape-changing user interfaces, physical telepresence, teleoperation, shape displays, actuated tangible interfaces},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647393,
author = {Zillner, Jakob and Rhemann, Christoph and Izadi, Shahram and Haller, Michael},
title = {3D-Board: A Whole-Body Remote Collaborative Whiteboard},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647393},
doi = {10.1145/2642918.2647393},
abstract = {This paper presents 3D-Board, a digital whiteboard capable of capturing life-sized virtual embodiments of geographically distributed users. When using large-scale screens for remote collaboration, awareness for the distributed users' gestures and actions is of particular importance. Our work adds to the literature on remote collaborative workspaces, it facilitates intuitive remote collaboration on large scale interactive whiteboards by preserving awareness of the full-body pose and gestures of the remote collaborator. By blending the front-facing 3D embodiment of a remote collaborator with the shared workspace, an illusion is created as if the observer was looking through the transparent whiteboard into the remote user's room. The system was tested and verified in a usability assessment, showing that 3D-Board significantly improves the effectiveness of remote collaboration on a large interactive surface.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {471–479},
numpages = {9},
keywords = {interactive whiteboard, reconstruction, remote collaboration, shared workspace, teleconferencing},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647390,
author = {Yoon, Dongwook and Chen, Nicholas and Guimbreti\`{e}re, Fran\c{c}ois and Sellen, Abigail},
title = {RichReview: Blending Ink, Speech, and Gesture to Support Collaborative Document Review},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647390},
doi = {10.1145/2642918.2647390},
abstract = {This paper introduces a novel document annotation system that aims to enable the kinds of rich communication that usually only occur in face-to-face meetings. Our system, RichReview, lets users create annotations on top of digital documents using three main modalities: freeform inking, voice for narration, and deictic gestures in support of voice. RichReview uses novel visual representations and time-synchronization between modalities to simplify annotation access and navigation. Moreover, RichReview's versatile support for multi-modal annotations enables users to mix and interweave different modalities in threaded conversations. A formative evaluation demonstrates early promise for the system finding support for voice, pointing, and the combination of both to be especially valuable. In addition, initial findings point to the ways in which both content and social context affect modality choice.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {481–490},
numpages = {10},
keywords = {collaborative authoring, pen interaction, multi-modal input, pointing gesture, speech, annotation, voice, asynchronous communication},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647410,
author = {Paulos, Eric and Myers, Chris and Tian, Rundong and Paulos, Paxton},
title = {Sensory Triptych: Here, near, out There},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647410},
doi = {10.1145/2642918.2647410},
abstract = {Sensory Triptych is a set of exploratory, interactive sensors designed for children that invite "new ways of seeing" our world from the perspective of the here (the earth, air, and water around us), near (things just out of sight), and out there (orbiting satellites and space junk) using familiar and novel interfaces, affordances, and narratives. We present a series of novel physical design prototypes that reframe sensing technologies for children that foster an early adoption of technology usage for exploring, understanding, communicating, sharing, and changing our world. Finally, we discuss how such designs expand the potential opportunities and landscapes for our future interactive systems and experiences within the UIST community.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {491–496},
numpages = {6},
keywords = {toys, critical design, sensors, children, education, citizen science, citizen exploration},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647364,
author = {Zeagler, Clint and Gilliland, Scott and Freil, Larry and Starner, Thad and Jackson, Melody},
title = {Going to the Dogs: Towards an Interactive Touchscreen Interface for Working Dogs},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647364},
doi = {10.1145/2642918.2647364},
abstract = {Computer-mediated interaction for working dogs is an important new domain for interaction research. In domestic settings, touchscreens could provide a way for dogs to communicate critical information to humans. In this paper we explore how a dog might interact with a touchscreen interface. We observe dogs' touchscreen interactions and record difficulties against what is expected of humans' touchscreen interactions. We also solve hardware issues through screen adaptations and projection styles to make a touchscreen usable for a canine's nose touch interactions. We also compare our canine touch data to humans' touch data on the same system. Our goal is to understand the affordances needed to make touchscreen interfaces usable for canines and help the future design of touchscreen interfaces for assistive dogs in the home.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {497–507},
numpages = {11},
keywords = {touchscreen interactions, animal computer interaction, assistance dog interface design, Fitts' law},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647397,
author = {Pfeuffer, Ken and Alexander, Jason and Chong, Ming Ki and Gellersen, Hans},
title = {Gaze-Touch: Combining Gaze with Multi-Touch for Interaction on the Same Surface},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647397},
doi = {10.1145/2642918.2647397},
abstract = {Gaze has the potential to complement multi-touch for interaction on the same surface. We present gaze-touch, a technique that combines the two modalities based on the principle of 'gaze selects, touch manipulates'. Gaze is used to select a target, and coupled with multi-touch gestures that the user can perform anywhere on the surface. Gaze-touch enables users to manipulate any target from the same touch position, for whole-surface reachability and rapid context switching. Conversely, gaze-touch enables manipulation of the same target from any touch position on the surface, for example to avoid occlusion. Gaze-touch is designed to complement direct-touch as the default interaction on multi-touch surfaces. We provide a design space analysis of the properties of gaze-touch versus direct-touch, and present four applications that explore how gaze-touch can be used alongside direct-touch. The applications demonstrate use cases for interchangeable, complementary and alternative use of the two modes of interaction, and introduce novel techniques arising from the combination of gaze-touch and conventional multi-touch.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {509–518},
numpages = {10},
keywords = {interactive surface, gaze input, multimodal ui, multi-touch},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647392,
author = {Chen, Xiang 'Anthony' and Schwarz, Julia and Harrison, Chris and Mankoff, Jennifer and Hudson, Scott E.},
title = {Air+touch: Interweaving Touch &amp; in-Air Gestures},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647392},
doi = {10.1145/2642918.2647392},
abstract = {We present Air+Touch, a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events. For example, a user can draw a circle in the air and tap to trigger a context menu, do a finger 'high jump' between two touches to select a region of text, or drag and in-air 'pigtail' to copy text to the clipboard. Through an observational study, we devised a basic taxonomy of Air+Touch interactions, based on whether the in-air component occurs before, between or after touches. To illustrate the potential of our approach, we built four applications that showcase seven exemplar Air+Touch interactions we created.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {519–525},
numpages = {7},
keywords = {free space gestures, input sensing, interaction techniques, touch input, around device interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647380,
author = {Zhao, Chen and Chen, Ke-Yu and Aumi, Md Tanvir Islam and Patel, Shwetak and Reynolds, Matthew S.},
title = {SideSwipe: Detecting in-Air Gestures around Mobile Devices Using Actual GSM Signal},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647380},
doi = {10.1145/2642918.2647380},
abstract = {Current smartphone inputs are limited to physical buttons, touchscreens, cameras or built-in sensors. These approaches either require a dedicated surface or line-of-sight for interaction. We introduce SideSwipe, a novel system that enables in-air gestures both above and around a mobile device. Our system leverages the actual GSM signal to detect hand gestures around the device. We developed an algorithm to convert the discrete and bursty GSM pulses to a continuous wave that can be used for gesture recognition. Specifically, when a user waves their hand near the phone, the hand movement disturbs the signal propagation between the phone's transmitter and added receiving antennas. Our system captures this variation and uses it for gesture recognition. To evaluate our system, we conduct a study with 10 participants and present robust gesture recognition with an average accuracy of 87.2% across 14 hand gestures.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {527–534},
numpages = {8},
keywords = {in-air hand gesture, antenna, GSM},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647365,
author = {Yamanaka, Shota and Miyashita, Homei},
title = {Vibkinesis: Notification by Direct Tap and 'dying Message' Using Vibronic Movement Controllable Smartphones},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647365},
doi = {10.1145/2642918.2647365},
abstract = {We propose Vibkinesis, a smartphone that can control its angle and directions of movement and rotation. By separately controlling the vibration motors attached to it, the smartphone can move on a table in the direction it chooses. Vibkinesis can inform a user of a message received when the user is away from the smartphone by changing its orientation, e.g., the smartphone has rotated 90° to the left before the user returns to the smartphone. With this capability, Vibkinesis can notify the user of a message even if the battery is discharged. We also extend the sensing area of Vibkinesis by using an omni-directional lens so that the smartphone tracks the surrounding objects. This allows Vibkinesis to tap the user's hand. These novel interactions expand the mobile device's movement area, notification channels, and notification time span.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {535–540},
numpages = {6},
keywords = {mobile sensing, vibration, notification, smartphone, mobile device},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647366,
author = {Matejka, Justin and Grossman, Tovi and Fitzmaurice, George},
title = {Video Lens: Rapid Playback and Exploration of Large Video Collections and Associated Metadata},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647366},
doi = {10.1145/2642918.2647366},
abstract = {We present Video Lens, a framework which allows users to visualize and interactively explore large collections of videos and associated metadata. The primary goal of the framework is to let users quickly find relevant sections within the videos and play them back in rapid succession. The individual UI elements are linked and highly interactive, supporting a faceted search paradigm and encouraging exploration of the data set. We demonstrate the capabilities and specific scenarios of Video Lens within the domain of professional baseball videos. A user study with 12 participants indicates that Video Lens efficiently supports a diverse range of powerful yet desirable video query tasks, while a series of interviews with professionals in the field demonstrates the framework's benefits and future potential.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {541–550},
numpages = {10},
keywords = {interface, visualization, video},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647367,
author = {Lasecki, Walter S. and Gordon, Mitchell and Koutra, Danai and Jung, Malte F. and Dow, Steven P. and Bigham, Jeffrey P.},
title = {Glance: Rapidly Coding Behavioral Video with the Crowd},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647367},
doi = {10.1145/2642918.2647367},
abstract = {Behavioral researchers spend considerable amount of time coding video data to systematically extract meaning from subtle human actions and emotions. In this paper, we present Glance, a tool that allows researchers to rapidly query, sample, and analyze large video datasets for behavioral events that are hard to detect automatically. Glance takes advantage of the parallelism available in paid online crowds to interpret natural language queries and then aggregates responses in a summary view of the video data. Glance provides analysts with rapid responses when initially exploring a dataset, and reliable codings when refining an analysis. Our experiments show that Glance can code nearly 50 minutes of video in 5 minutes by recruiting over 60 workers simultaneously, and can get initial feedback to analysts in under 10 seconds for most clips. We present and compare new methods for accurately aggregating the input of multiple workers marking the spans of events in video data, and for measuring the quality of their coding in real-time before a baseline is established by measuring the variance between workers. Glance's rapid responses to natural language queries, feedback regarding question ambiguity and anomalies in the data, and ability to build on prior context in followup queries allow users to have a conversation-like interaction with their data - opening up new possibilities for naturally exploring video data.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {551–562},
numpages = {12},
keywords = {subjective coding, crowdsourcing, video, data analysis},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647389,
author = {Kim, Juho and Guo, Philip J. and Cai, Carrie J. and Li, Shang-Wen (Daniel) and Gajos, Krzysztof Z. and Miller, Robert C.},
title = {Data-Driven Interaction Techniques for Improving Navigation of Educational Videos},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647389},
doi = {10.1145/2642918.2647389},
abstract = {With an unprecedented scale of learners watching educational videos on online platforms such as MOOCs and YouTube, there is an opportunity to incorporate data generated from their interactions into the design of novel video interaction techniques. Interaction data has the potential to help not only instructors to improve their videos, but also to enrich the learning experience of educational video watchers. This paper explores the design space of data-driven interaction techniques for educational video navigation. We introduce a set of techniques that augment existing video interface widgets, including: a 2D video timeline with an embedded visualization of collective navigation traces; dynamic and non-linear timeline scrubbing; data-enhanced transcript search and keyword summary; automatic display of relevant still frames next to the video; and a visual summary representing points with high learner activity. To evaluate the feasibility of the techniques, we ran a laboratory user study with simulated learning tasks. Participants rated watching lecture videos with interaction data to be efficient and useful in completing the tasks. However, no significant differences were found in task performance, suggesting that interaction data may not always align with moment-by-moment information needs during the tasks.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {563–572},
numpages = {10},
keywords = {video summarization, multimedia learning, interaction peaks, video learning, MOOCs, video content analysis},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647400,
author = {Pavel, Amy and Reed, Colorado and Hartmann, Bj\"{o}rn and Agrawala, Maneesh},
title = {Video Digests: A Browsable, Skimmable Format for Informational Lecture Videos},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647400},
doi = {10.1145/2642918.2647400},
abstract = {Increasingly, authors are publishing long informational talks, lectures, and distance-learning videos online. However, it is difficult to browse and skim the content of such videos using current timeline-based video players. Video digests are a new format for informational videos that afford browsing and skimming by segmenting videos into a chapter/section structure and providing short text summaries and thumbnails for each section. Viewers can navigate by reading the summaries and clicking on sections to access the corresponding point in the video. We present a set of tools to help authors create such digests using transcript-based interactions. With our tools, authors can manually create a video digest from scratch, or they can automatically generate a digest by applying a combination of algorithmic and crowdsourcing techniques and then manually refine it as needed. Feedback from first-time users suggests that our transcript-based authoring tools and automated techniques greatly facilitate video digest creation. In an evaluative crowdsourced study we find that given a short viewing time, video digests support browsing and skimming better than timeline-based or transcript-based video players.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {573–582},
numpages = {10},
keywords = {video presentation interfaces, video digests, education},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647414,
author = {Afergan, Daniel and Shibata, Tomoki and Hincks, Samuel W. and Peck, Evan M. and Yuksel, Beste F. and Chang, Remco and Jacob, Robert J.K.},
title = {Brain-Based Target Expansion},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647414},
doi = {10.1145/2642918.2647414},
abstract = {The bubble cursor is a promising cursor expansion technique, improving a user's movement time and accuracy in pointing tasks. We introduce a brain-based target expansion system, which improves the efficacy of bubble cursor by increasing the expansion of high importance targets at the optimal time based on brain measurements correlated to a particular type of multitasking. We demonstrate through controlled experiments that brain-based target expansion can deliver a graded and continuous level of assistance to a user according to their cognitive state, thereby improving task and speed-accuracy metrics, even without explicit visual changes to the system. Such an adaptation is ideal for use in complex systems to steer users toward higher priority goals during times of increased demand.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {583–593},
numpages = {11},
keywords = {fNIRS, BCI, brain-computer interface, bubble cursor., adaptive interface},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647352,
author = {Avery, Jeff and Choi, Mark and Vogel, Daniel and Lank, Edward},
title = {Pinch-to-Zoom-plus: An Enhanced Pinch-to-Zoom That Reduces Clutching and Panning},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647352},
doi = {10.1145/2642918.2647352},
abstract = {Despite its popularity, the classic pinch-to-zoom gesture used in modern multi-touch interfaces has drawbacks: specifically, the need to support an extended range of scales and the need to keep content within the view window on the display can result in the need to clutch and pan. In two formative studies of unimanual and bimanual pinch-to-zoom, we found patterns: zooming actions follows a predictable ballistic velocity curve, and users tend to pan the point-of-interest towards the center of the screen. We apply these results to design an enhanced zooming technique called Pinch-to-Zoom-Plus (PZP) that reduces clutching and panning operations compared to standard pinch-to-zoom behaviour.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {595–604},
numpages = {10},
keywords = {interaction, mobile, multi-touch},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647379,
author = {Hinckley, Ken and Pahud, Michel and Benko, Hrvoje and Irani, Pourang and Guimbreti\`{e}re, Fran\c{c}ois and Gavriliu, Marcel and Chen, Xiang 'Anthony' and Matulic, Fabrice and Buxton, William and Wilson, Andrew},
title = {Sensing Techniques for Tablet+stylus Interaction},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647379},
doi = {10.1145/2642918.2647379},
abstract = {We explore grip and motion sensing to afford new techniques that leverage how users naturally manipulate tablet and stylus devices during pen + touch interaction. We can detect whether the user holds the pen in a writing grip or tucked between his fingers. We can distinguish bare-handed inputs, such as drag and pinch gestures produced by the nonpreferred hand, from touch gestures produced by the hand holding the pen, which necessarily impart a detectable motion signal to the stylus. We can sense which hand grips the tablet, and determine the screen's relative orientation to the pen. By selectively combining these signals and using them to complement one another, we can tailor interaction to the context, such as by ignoring unintentional touch inputs while writing, or supporting contextually-appropriate tools such as a magnifier for detailed stroke work that appears when the user pinches with the pen tucked between his fingers. These and other techniques can be used to impart new, previously unanticipated subtleties to pen + touch interaction on tablets.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {605–614},
numpages = {10},
keywords = {tablet, sensing, motion, pen+touch, grip, bimanual input},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647354,
author = {Chen, Xiang 'Anthony' and Grossman, Tovi and Fitzmaurice, George},
title = {Swipeboard: A Text Entry Technique for Ultra-Small Interfaces That Supports Novice to Expert Transitions},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647354},
doi = {10.1145/2642918.2647354},
abstract = {Ultra-small smart devices, such as smart watches, have become increasingly popular in recent years. Most of these devices rely on touch as the primary input modality, which makes tasks such as text entry increasingly difficult as the devices continue to shrink. In the sole pursuit of entry speed, the ultimate solution is a shorthand technique (e.g., Morse code) that sequences tokens of input (e.g., key, tap, swipe) into unique representations of each character. However, learning such techniques is hard, as it often resorts to rote memory. Our technique, Swipeboard, leverages our spatial memory of a QWERTY keyboard to learn, and eventually master a shorthand, eyes-free text entry method designed for ultra-small interfaces. Characters are entered with two swipes; the first swipe specifies the region where the character is located, and the second swipe specifies the character within that region. Our study showed that with less than two hours' training, Tested on a reduced word set, Swipeboard users achieved 19.58 words per minute (WPM), 15% faster than an existing baseline technique.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {615–620},
numpages = {6},
keywords = {mobile device, text entry, input technique, swipeboard},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647382,
author = {Karrenbauer, Andreas and Oulasvirta, Antti},
title = {Improvements to Keyboard Optimization with Integer Programming},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647382},
doi = {10.1145/2642918.2647382},
abstract = {Keyboard optimization is concerned with the design of keyboards for different terminals, languages, user groups, and tasks. Previous work in HCI has used random search based methods, such as simulated annealing. These "black box" approaches are convenient, because good solutions are found quickly and no assumption must be made about the objective function. This paper contributes by developing integer programming (IP) as a complementary approach. To this end, we present IP formulations for the letter assignment problem and solve them by branch-and-bound. Although computationally expensive, we show that IP offers two strong benefits. First, its structured non-random search approach improves the out- comes. Second, it guarantees bounds, which increases the designer's confidence over the quality of results. We report improvements to three keyboard optimization cases.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {621–626},
numpages = {6},
keywords = {user interface optimization, branch-and-bound, keyboard layouts, random search methods, integer programming},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647369,
author = {Gandy, Maribeth and MacIntyre, Blair},
title = {Designer's Augmented Reality Toolkit, Ten Years Later: Implications for New Media Authoring Tools},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647369},
doi = {10.1145/2642918.2647369},
abstract = {The Designer's Augmented Reality Toolkit (DART) was an augmented (AR) and mixed reality (MR) authoring tool targeted at new media designers. It was released in 2003 and was heavily used by a diverse population of creators for the next several years [28]. Ten years later, we approached a group of users to collect reflections on their use of DART, the artifacts they produced, their subsequent AR/MR authoring, their thoughts on the challenges of AR/MR authoring in general, and the state of modern tools. In this paper we present the findings from in-depth interviews with these DART developers and other AR experts. Their reflections provide insights on how to successfully engage non-technologists with new media and the challenges they face during authoring, the unique requirements of new media authoring, and how modern tools are still not meeting the needs of this type of author, highlighting where additional research is needed.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {627–636},
numpages = {10},
keywords = {mixed reality, practitioners, authoring tools, evaluation, augmented reality},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647383,
author = {Jones, Brett and Sodhi, Rajinder and Murdock, Michael and Mehra, Ravish and Benko, Hrvoje and Wilson, Andrew and Ofek, Eyal and MacIntyre, Blair and Raghuvanshi, Nikunj and Shapira, Lior},
title = {RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-Camera Units},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647383},
doi = {10.1145/2642918.2647383},
abstract = {RoomAlive is a proof-of-concept prototype that transforms any room into an immersive, augmented entertainment experience. Our system enables new interactive projection mapping experiences that dynamically adapts content to any room. Users can touch, shoot, stomp, dodge and steer projected content that seamlessly co-exists with their existing physical environment. The basic building blocks of RoomAlive are projector-depth camera units, which can be combined through a scalable, distributed framework. The projector-depth camera units are individually auto-calibrating, self-localizing, and create a unified model of the room with no user intervention. We investigate the design space of gaming experiences that are possible with RoomAlive and explore methods for dynamically mapping content based on room layout and user position. Finally we showcase four experience prototypes that demonstrate the novel interactive experiences that are possible with RoomAlive and discuss the design challenges of adapting any game to any room.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {637–644},
numpages = {8},
keywords = {projection mapping, projector-camera system, spatial augmented reality},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647402,
author = {Benko, Hrvoje and Wilson, Andrew D. and Zannier, Federico},
title = {Dyadic Projected Spatial Augmented Reality},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647402},
doi = {10.1145/2642918.2647402},
abstract = {Mano-a-Mano is a unique spatial augmented reality system that combines dynamic projection mapping, multiple perspective views and device-less interaction to support face to face, or dyadic, interaction with 3D virtual objects. Its main advantage over more traditional AR approaches, such as handheld devices with composited graphics or see-through head worn displays, is that users are able to interact with 3D virtual objects and each other without cumbersome devices that obstruct face to face interaction. We detail our prototype system and a number of interactive experiences. We present an initial user experiment that shows that participants are able to deduce the size and distance of a virtual projected object. A second experiment shows that participants are able to infer which of a number of targets the other user indicates by pointing.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {645–655},
numpages = {11},
keywords = {projector camera system, augmented reality, depth cameras},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647350,
author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\"{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\"{u}ller, J\"{o}rg},
title = {Tracs: Transparency-Control for See-through Displays},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647350},
doi = {10.1145/2642918.2647350},
abstract = {We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {657–661},
numpages = {5},
keywords = {transparency-controlled display, transparent display},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647407,
author = {Monnai, Yasuaki and Hasegawa, Keisuke and Fujiwara, Masahiro and Yoshino, Kazuma and Inoue, Seki and Shinoda, Hiroyuki},
title = {HaptoMime: Mid-Air Haptic Interaction with a Floating Virtual Screen},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647407},
doi = {10.1145/2642918.2647407},
abstract = {We present HaptoMime, a mid-air interaction system that allows users to touch a floating virtual screen with hands-free tactile feedback. Floating images formed by tailored light beams are inherently lacking in tactile feedback. Here we propose a method to superpose hands-free tactile feedback on such a floating image using ultrasound. By tracking a fingertip with an electronically steerable ultrasonic beam, the fingertip encounters a mechanical force consistent with the floating image. We demonstrate and characterize the proposed transmission scheme and discuss promising applications with an emphasis that it helps us 'pantomime' in mid-air.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {663–667},
numpages = {5},
keywords = {tactile feedback, aerial imaging, mid-air interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647360,
author = {Satyanarayan, Arvind and Wongsuphasawat, Kanit and Heer, Jeffrey},
title = {Declarative Interaction Design for Data Visualization},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647360},
doi = {10.1145/2642918.2647360},
abstract = {Declarative visualization grammars can accelerate development, facilitate retargeting across platforms, and allow language-level optimizations. However, existing declarative visualization languages are primarily concerned with visual encoding, and rely on imperative event handlers for interactive behaviors. In response, we introduce a model of declarative interaction design for data visualizations. Adopting methods from reactive programming, we model low-level events as composable data streams from which we form higher-level semantic signals. Signals feed predicates and scale inversions, which allow us to generalize interactive selections at the level of item geometry (pixels) into interactive queries over the data domain. Production rules then use these queries to manipulate the visualization's appearance. To facilitate reuse and sharing, these constructs can be encapsulated as named interactors: standalone, purely declarative specifications of interaction techniques. We assess our model's feasibility and expressivity by instantiating it with extensions to the Vega visualization grammar. Through a diverse range of examples, we demonstrate coverage over an established taxonomy of visualization interaction techniques.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {669–678},
numpages = {10},
keywords = {declarative design, visualization, toolkits, interaction design},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647391,
author = {Jeuris, Steven and Houben, Steven and Bardram, Jakob},
title = {Laevo: A Temporal Desktop Interface for Integrated Knowledge Work},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647391},
doi = {10.1145/2642918.2647391},
abstract = {Prior studies show that knowledge work is characterized by highly interlinked practices, including task, file and window management. However, existing personal information management tools primarily focus on a limited subset of knowledge work, forcing users to perform additional manual configuration work to integrate the different tools they use. In order to understand tool usage, we review literature on how users' activities are created and evolve over time as part of knowledge worker practices. From this we derive the activity life cycle, a conceptual framework describing the different states and transitions of an activity. The life cycle is used to inform the design of Laevo, a temporal activity-centric desktop interface for personal knowledge work. Laevo allows users to structure work within dedicated workspaces, managed on a timeline. Through a centralized notification system which doubles as a to-do list, incoming interruptions can be handled. Our field study indicates how highlighting the temporal nature of activities results in lightweight scalable activity management, while making users more aware about their ongoing and planned work.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {679–688},
numpages = {10},
keywords = {tool integration, activity-centric computing, knowledge work, personal information management},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2647355,
author = {Li, Yang},
title = {Reflection: Enabling Event Prediction as an on-Device Service for Mobile Interaction},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2647355},
doi = {10.1145/2642918.2647355},
abstract = {By knowing which upcoming action a user might perform, a mobile application can optimize its user interface for accomplishing the task. However, it is technically challenging for developers to implement event prediction in their own application. We created Reflection, an on-device service that answers queries from a mobile application regarding which actions the user is likely to perform at a given time. Any application can register itself and communicate with Reflection via a simple API. Reflection continuously learns a prediction model for each application based on its evolving event history. It employs a novel method for prediction by 1) combining multiple well-designed predictors with an online learning method, and 2) capturing event patterns not only within but also across registered applications--only possible as an infrastructure solution. We evaluated Reflection with two sets of large-scale, in situ mobile event logs, which showed our infrastructure approach is feasible.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {689–698},
numpages = {10},
keywords = {infrastructure, passive aggressive, mobile computing, API, event prediction, online learning, probabilistic model},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

@inproceedings{10.1145/2642918.2642920,
author = {Victor, Bret},
title = {Humane Representation of Thought: A Trail Map for the 21st Century},
year = {2014},
isbn = {9781450330695},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642918.2642920},
doi = {10.1145/2642918.2642920},
abstract = {New representations of thought -- written language, mathematical notation, information graphics, etc -- have been responsible for some of the most significant leaps in the progress of civilization, by expanding humanity's collectively-thinkable territory. But at debilitating cost. These representations, having been invented for static media such as paper, tap into a small subset of human capabilities and neglect the rest. Knowledge work means sitting at a desk, interpreting and manipulating symbols. The human body is reduced to an eye staring at tiny rectangles and fingers on a pen or keyboard. Like any severely unbalanced way of living, this is crippling to mind and body. But less obviously, and more importantly, it is enormously wasteful of the vast human potential. Human beings naturally have many powerful modes of thinking and understanding. Most are incompatible with static media. In a culture that has contorted itself around the limitations of marks on paper, these modes are undeveloped, unrecognized, or scorned.We are now seeing the start of a dynamic medium. To a large extent, people today are using this medium merely to emulate and extend static representations from the era of paper, and to further constrain the ways in which the human body can interact with external representations of thought.But the dynamic medium offers the opportunity to deliberately invent a humane and empowering form of knowledge work. We can design dynamic representations which draw on the entire range of human capabilities -- all senses, all forms of movement, all forms of understanding -- instead of straining a few and atrophying the rest.This talk suggests how each of the human activities in which thought is externalized (conversing, presenting, reading, writing, etc) can be redesigned around such representations.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {699},
numpages = {1},
keywords = {dynamic representations},
location = {Honolulu, Hawaii, USA},
series = {UIST '14}
}

