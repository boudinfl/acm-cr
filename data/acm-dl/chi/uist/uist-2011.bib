@inproceedings{10.1145/3244832,
author = {schraefel, mc},
title = {Session Details: Crowdsourcing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244832},
doi = {10.1145/3244832},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047198,
author = {Noronha, Jon and Hysen, Eric and Zhang, Haoqi and Gajos, Krzysztof Z.},
title = {Platemate: Crowdsourcing Nutritional Analysis from Food Photographs},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047198},
doi = {10.1145/2047196.2047198},
abstract = {We introduce PlateMate, a system that allows users to take photos of their meals and receive estimates of food intake and composition. Accurate awareness of this information can help people monitor their progress towards dieting goals, but current methods for food logging via self-reporting, expert observation, or algorithmic analysis are time-consuming, expensive, or inaccurate. PlateMate crowdsources nutritional analysis from photographs using Amazon Mechanical Turk, automatically coordinating untrained workers to estimate a meal's calories, fat, carbohydrates, and protein. We present the Management framework for crowdsourcing complex tasks, which supports PlateMate's nutrition analysis workflow. Results of our evaluations show that PlateMate is nearly as accurate as a trained dietitian and easier to use for most users than traditional self-reporting.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–12},
numpages = {12},
keywords = {nutrition, remote food photography, crowdsourcing, human computation, amazon mechanical turk},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047199,
author = {Rzeszotarski, Jeffrey M. and Kittur, Aniket},
title = {Instrumenting the Crowd: Using Implicit Behavioral Measures to Predict Task Performance},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047199},
doi = {10.1145/2047196.2047199},
abstract = {Detecting and correcting low quality submissions in crowdsourcing tasks is an important challenge. Prior work has primarily focused on worker outcomes or reputation, using approaches such as agreement across workers or with a gold standard to evaluate quality. We propose an alternative and complementary technique that focuses on the way workers work rather than the products they produce. Our technique captures behavioral traces from online crowd workers and uses them to predict outcome measures such quality, errors, and the likelihood of cheating. We evaluate the effectiveness of the approach across three contexts including classification, generation, and comprehension tasks. The results indicate that we can build predictive models of task performance based on behavioral traces alone, and that these models generalize to related tasks. Finally, we discuss limitations and extensions of the approach.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–22},
numpages = {10},
keywords = {event logging, user behavior, crowdsourcing, performance, user logging, mechanical turk},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047200,
author = {Lasecki, Walter S. and Murray, Kyle I. and White, Samuel and Miller, Robert C. and Bigham, Jeffrey P.},
title = {Real-Time Crowd Control of Existing Interfaces},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047200},
doi = {10.1145/2047196.2047200},
abstract = {Crowdsourcing has been shown to be an effective approach for solving difficult problems, but current crowdsourcing systems suffer two main limitations: (i) tasks must be repackaged for proper display to crowd workers, which generally requires substantial one-off programming effort and support infrastructure, and (ii) crowd workers generally lack a tight feedback loop with their task. In this paper, we introduce Legion, a system that allows end users to easily capture existing GUIs and outsource them for collaborative, real-time control by the crowd. We present mediation strategies for integrating the input of multiple crowd workers in real-time, evaluate these mediation strategies across several applications, and further validate Legion by exploring the space of novel applications that it enables.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–32},
numpages = {10},
keywords = {real-time human computation, real-time crowd control, remote control, crowdsourcing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047201,
author = {Bernstein, Michael S. and Brandt, Joel and Miller, Robert C. and Karger, David R.},
title = {Crowds in Two Seconds: Enabling Realtime Crowd-Powered Interfaces},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047201},
doi = {10.1145/2047196.2047201},
abstract = {Interactive systems must respond to user input within seconds. Therefore, to create realtime crowd-powered interfaces, we need to dramatically lower crowd latency. In this paper, we introduce the use of synchronous crowds for on-demand, realtime crowdsourcing. With synchronous crowds, systems can dynamically adapt tasks by leveraging the fact that workers are present at the same time. We develop techniques that recruit synchronous crowds in two seconds and use them to execute complex search tasks in ten seconds. The first technique, the retainer model, pays workers a small wage to wait and respond quickly when asked. We offer empirically derived guidelines for a retainer system that is low-cost and produces on-demand crowds in two seconds. Our second technique, rapid refinement, observes early signs of agreement in synchronous crowds and dynamically narrows the search space to focus on promising directions. This approach produces results that, on average, are of more reliable quality and arrive faster than the fastest crowd member working alone. To explore benefits and limitations of these techniques for interaction, we present three applications: Adrenaline, a crowd-powered camera where workers quickly filter a short video down to the best single moment for a photo; and Puppeteer and A|B, which examine creative generation tasks, communication with workers, and low-latency voting.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–42},
numpages = {10},
keywords = {human computation, crowdsourcing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047202,
author = {Kittur, Aniket and Smus, Boris and Khamkar, Susheel and Kraut, Robert E.},
title = {CrowdForge: Crowdsourcing Complex Work},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047202},
doi = {10.1145/2047196.2047202},
abstract = {Micro-task markets such as Amazon's Mechanical Turk represent a new paradigm for accomplishing work, in which employers can tap into a large population of workers around the globe to accomplish tasks in a fraction of the time and money of more traditional methods. However, such markets have been primarily used for simple, independent tasks, such as labeling an image or judging the relevance of a search result. Here we present a general purpose framework for accomplishing complex and interdependent tasks using micro-task markets. We describe our framework, a web-based prototype, and case studies on article writing, decision making, and science journalism that demonstrate the benefits and limitations of the approach.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–52},
numpages = {10},
keywords = {mapreduce, coordination, distributed processing, mechanical turk, crowdsourcing, human computation},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047203,
author = {Ahmad, Salman and Battle, Alexis and Malkani, Zahan and Kamvar, Sepander},
title = {The Jabberwocky Programming Environment for Structured Social Computing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047203},
doi = {10.1145/2047196.2047203},
abstract = {We present Jabberwocky, a social computing stack that consists of three components: a human and machine resource management system called Dormouse, a parallel programming framework for human and machine computation called ManReduce, and a high-level programming language on top of ManReduce called Dog. Dormouse is designed to enable cross-platform programming languages for social computation, so, for example, programs written for Mechanical Turk can also run on other crowdsourcing platforms. Dormouse also enables a programmer to easily combine crowdsourcing platforms or create new ones. Further, machines and people are both first-class citizens in Dormouse, allowing for natural parallelization and control flows for a broad range of data-intensive applications. And finally and importantly, Dormouse includes notions of real identity, heterogeneity, and social structure. We show that the unique properties of Dormouse enable elegant programming models for complex and useful problems, and we propose two such frameworks. ManReduce is a framework for combining human and machine computation into an intuitive parallel data flow that goes beyond existing frameworks in several important ways, such as enabling functions on arbitrary communication graphs between human and machine clusters. And Dog is a high-level procedural language written on top of ManReduce that focuses on expressivity and reuse. We explore two applications written in Dog: bootstrapping product recommendations without purchase data, and expert labeling of medical images.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {53–64},
numpages = {12},
keywords = {crowdsourcing, social computing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244833,
author = {Bernstein, Michael},
title = {Session Details: Social Information},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244833},
doi = {10.1145/3244833},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047205,
author = {Guo, Philip J. and Kandel, Sean and Hellerstein, Joseph M. and Heer, Jeffrey},
title = {Proactive Wrangling: Mixed-Initiative End-User Programming of Data Transformation Scripts},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047205},
doi = {10.1145/2047196.2047205},
abstract = {Analysts regularly wrangle data into a form suitable for computational tools through a tedious process that delays more substantive analysis. While interactive tools can assist data transformation, analysts must still conceptualize the desired output state, formulate a transformation strategy, and specify complex transforms. We present a model to proactively suggest data transforms which map input data to a relational format expected by analysis tools. To guide search through the space of transforms, we propose a metric that scores tables according to type homogeneity, sparsity and the presence of delimiters. When compared to "ideal" hand-crafted transformations, our model suggests over half of the needed steps; in these cases the top-ranked suggestion is preferred 77% of the time. User study results indicate that suggestions produced by our model can assist analysts' transformation tasks, but that users do not always value proactive assistance, instead preferring to maintain the initiative. We discuss some implications of these results for mixed-initiative interfaces.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {65–74},
numpages = {10},
keywords = {data analysis, data transformation, data cleaning, mixed-initiative interfaces, end-user programming},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047206,
author = {Hangal, Sudheendra and Lam, Monica S. and Heer, Jeffrey},
title = {MUSE: Reviving Memories Using Email Archives},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047206},
doi = {10.1145/2047196.2047206},
abstract = {Email archives silently record our actions and thoughts over the years, forming a passively acquired and detailed life-log that contains rich material for reminiscing on our lives. However, exploratory browsing of archives containing thousands of messages is tedious without effective ways to guide the user towards interesting events and messages. We present Muse (Memories USing Email), a system that combines data mining techniques and an interactive interface to help users browse a long-term email archive. Muse analyzes the contents of the archive and generates a set of cues that help to spark users' memories: communication activity with inferred social groups, a summary of recurring named entities, occurrence of sentimental words, and image attachments. These cues serve as salient entry points into a browsing interface that enables faceted navigation and rapid skimming of email messages. In our user studies, we found that users generally enjoyed browsing their archives with Muse, and extracted a range of benefits, from summarizing work progress to renewing friendships and making serendipitous discoveries.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–84},
numpages = {10},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047207,
author = {Hincapi\'{e}-Ramos, Juan David and Voida, Stephen and Mark, Gloria},
title = {A Design Space Analysis of Availability-Sharing Systems},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047207},
doi = {10.1145/2047196.2047207},
abstract = {Workplace collaboration often requires interruptions, which can happen at inopportune times. Designing a successful availability-sharing system requires finding the right balance to optimize the benefits and reduce costs for both the interrupter and interruptee. In this paper, we examine the design space of availability-sharing systems and identify six relevant design dimensions: abstraction, presentation, information delivery, symmetry, obtrusiveness and temporal gradient. We describe these dimensions in terms of the tensions between interrupters and interruptees revealed in previous studies of workplace collaboration and deployments of awareness systems. As a demonstration of the utility of our design space, we introduce InterruptMe, a novel availability-sharing system that represents a previously unexplored point in the design space and that balances the tensions between interrupters and interruptees. InterruptMe differs from previous systems in that it displays availability information only when needed by monitoring implicit inputs from the system's users, implements a traceable asymmetry structure, and introduces the notion of per-communications channel availability.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {85–96},
numpages = {12},
keywords = {availability, interruptibility, workplace awareness},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047208,
author = {Takahashi, Yuki and Kojima, Hiroaki and Okada, Ken-ichi},
title = {Injured Person Information Management during Second Triage},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047208},
doi = {10.1145/2047196.2047208},
abstract = {In a large-scale disaster in which many persons are injured at the same time, triage has been introduced. Triage is a method that temporarily delays the treatment of people with mild to moderate injuries and symptoms and gives priority to those in a critical condition. In the process of multiple triage, more specific information is needed in the second triage compared to the first to accurately prioritize the persons injuries and state. To solve this problem we proposed and constructed a touch-based interface for managing information inserted during second triage. A touch-based tablet interface is introduced to specify wound areas and gestures for wound types. The information is shared wirelessly with all emergency personnel, giving medics shared, data-centric, visibility of overall triage status for the first time. The evaluation experiment shows that this proposed system enables to reduce input errors, speed up injured person care, and facilitate information sharing between medics efficiently. As a result, we believe that many more injured persons can and will be saved.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {97–106},
numpages = {10},
keywords = {touch panel, interface, electronic tag, triage, biological information},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047209,
author = {Paepcke, Andreas and Soto, Bianca and Takayama, Leila and Koenig, Frank and Gassend, Blaise},
title = {Yelling in the Hall: Using Sidetone to Address a Problem with Mobile Remote Presence Systems},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047209},
doi = {10.1145/2047196.2047209},
abstract = {In our field deployments of mobile remote presence (MRP) systems in offices, we observed that remote operators of MRPs often unintentionally spoke too loudly. This disrupted their local co-workers, who happened to be within earshot of the MRP system. To address this issue, we prototyped and empirically evaluated the effect of sidetone to help operators self regulate their speaking loudness. Sidetone is the intentional, attenuated feedback of speakers' voices to their ears while they are using a telecommunication device. In a 3-level (no sidetone vs. low sidetone vs. high sidetone) within- participants pair of experiments, people interacted with a confederate through an MRP system. The first experiment involved MRP operators using headsets with boom microphones (N=20). The second experiment involved MRP operators using loudspeakers and desktop microphones (N=14). While we detected the effects of the sidetone manipulation in our audio-visual context, the effect was attenuated in comparison to earlier audio-only studies. We hypothesize that the strong visual component of our MRP system interferes with the sidetone effect. We also found that engaging in more social tasks (e.g., a getting-to-know-you activity) and more intellectually demanding tasks (e.g., a creativity exercise) influenced how loudly people spoke. This suggests that testing such sidetone effects in the typical read-aloud setting is insufficient for generalizing to more interactive, communication tasks. We conclude that MRP application support must reach beyond the time honored audio-only technologies to solve the problem of excessive speaker loudness.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {107–116},
numpages = {10},
keywords = {computer mediated communication, experiment, sidetone, loudness regulation, telepresence},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047210,
author = {Slyper, Ronit and Lehman, Jill and Forlizzi, Jodi and Hodgins, Jessica},
title = {A Tongue Input Device for Creating Conversations},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047210},
doi = {10.1145/2047196.2047210},
abstract = {We present a new tongue input device, the tongue joystick, for use by an actor inside an articulated-head character costume. Using our device, the actor can maneuver through a dialogue tree, selecting clips of prerecorded audio to hold a conversation in the voice of the character. The device is constructed of silicone sewn with conductive thread, a unique method for creating rugged, soft, low-actuation force devices. This method has application for entertainment and assistive technology. We compare our device against other portable mouth input devices, showing it to be the fastest and most accurate in tasks mimicking our target application. Finally, we show early results of an actor inside an articulated-head costume using the tongue joystick to interact with a child.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {117–126},
numpages = {10},
keywords = {turn-taking, dialogue tree, mouth, interface},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244834,
author = {Kamvar, Sep},
title = {Session Details: Social Learning},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244834},
doi = {10.1145/3244834},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047212,
author = {Ramesh, Vidya and Hsu, Charlie and Agrawala, Maneesh and Hartmann, Bj\"{o}rn},
title = {ShowMeHow: Translating User Interface Instructions between Applications},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047212},
doi = {10.1145/2047196.2047212},
abstract = {Many people learn how to use complex authoring applications through tutorials. However, user interfaces for authoring tools differ between versions, platforms, and competing products, limiting the utility of tutorials. Our goal is to make tutorials more useful by enabling users to repurpose tutorials between similar applications. We introduce UI translation interfaces which enable users to locate commands in one application using the interface language of another application. Our end-user tool, ShowMeHow, demonstrates two interaction techniques to accomplish translations: 1) direct manipulation of interface facades and 2) text search for commands using the vocabulary of another application. We discuss tools needed to construct the translation maps that enable these techniques. An initial study (n=12) shows that users can locate unfamiliar commands twice as fast with interface facades. A second study showed that users can work through tutorials written for one application in another application.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {127–134},
numpages = {8},
keywords = {instructions, mapping, tutorials, translation},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047213,
author = {Pongnumkul, Suporn and Dontcheva, Mira and Li, Wilmot and Wang, Jue and Bourdev, Lubomir and Avidan, Shai and Cohen, Michael F.},
title = {Pause-and-Play: Automatically Linking Screencast Video Tutorials with Applications},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047213},
doi = {10.1145/2047196.2047213},
abstract = {Video tutorials provide a convenient means for novices to learn new software applications. Unfortunately, staying in sync with a video while trying to use the target application at the same time requires users to repeatedly switch from the application to the video to pause or scrub backwards to replay missed steps. We present Pause-and-Play, a system that helps users work along with existing video tutorials. Pause-and-Play detects important events in the video and links them with corresponding events in the target application as the user tries to replicate the depicted procedure. This linking allows our system to automatically pause and play the video to stay in sync with the user. Pause-and-Play also supports convenient video navigation controls that are accessible from within the target application and allow the user to easily replay portions of the video without switching focus out of the application. Finally, since our system uses computer vision to detect events in existing videos and leverages application scripting APIs to obtain real time usage traces, our approach is largely independent of the specific target application and does not require access or modifications to application source code. We have implemented Pause-and-Play for two target applications, Google SketchUp and Adobe Photoshop, and we report on a user study that shows our system improves the user experience of working with video tutorials.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {135–144},
numpages = {10},
keywords = {instructions, screencast, video tutorial},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047214,
author = {Yeh, Tom and Chang, Tsung-Hsiang and Xie, Bo and Walsh, Greg and Watkins, Ivan and Wongsuphasawat, Krist and Huang, Man and Davis, Larry S. and Bederson, Benjamin B.},
title = {Creating Contextual Help for GUIs Using Screenshots},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047214},
doi = {10.1145/2047196.2047214},
abstract = {Contextual help is effective for learning how to use GUIs by showing instructions and highlights on the actual interface rather than in a separate viewer. However, end-users and third-party tech support typically cannot create contextual help to assist other users because it requires programming skill and source code access. We present a creation tool for contextual help that allows users to apply common computer skills-taking screenshots and writing simple scripts. We perform pixel analysis on screenshots to make this tool applicable to a wide range of applications and platforms without source code access. We evaluated the tool's usability with three groups of participants: developers, in-structors, and tech support. We further validated the applicability of our tool with 60 real tasks supported by the tech support of a university campus.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {145–154},
numpages = {10},
keywords = {help, pixel analysis, contextual help},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047215,
author = {Goldman, Max and Little, Greg and Miller, Robert C.},
title = {Real-Time Collaborative Coding in a Web IDE},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047215},
doi = {10.1145/2047196.2047215},
abstract = {This paper describes Collabode, a web-based Java integrated development environment designed to support close, synchronous collaboration between programmers. We examine the problem of collaborative coding in the face of program compilation errors introduced by other users which make collaboration more difficult, and describe an algorithm for error-mediated integration of program code. Concurrent editors see the text of changes made by collaborators, but the errors reported in their view are based only on their own changes. Editors may run the program at any time, using only error-free edits supplied so far, and ignoring incomplete or otherwise error-generating changes. We evaluate this algorithm and interface on recorded data from previous pilot experiments with Collabode, and via a user study with student and professional programmers. We conclude that it offers appreciable benefits over naive continuous synchronization without regard to errors and over manual version control.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {155–164},
numpages = {10},
keywords = {pair programming, collaboration, collaborative editing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047216,
author = {Ritchie, Daniel and Kejriwal, Ankita Arvind and Klemmer, Scott R.},
title = {D.Tour: Style-Based Exploration of Design Example Galleries},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047216},
doi = {10.1145/2047196.2047216},
abstract = {In design, people often seek examples for inspiration. However, current example-finding practices suffer many drawbacks: templates present designs without a usage context; search engines can only examine the text on a page. This paper introduces exploratory techniques for finding relevant and inspiring design examples. These novel techniques include searching by stylistic similarity to a known example design and searching by stylistic keyword. These interactions are manifest in d.tour, a style-based design exploration tool. d.tour presents a curated database of Web pages as an explorable design gallery. It extracts and analyzes design features of these pages, allowing it to process style-based queries and recommend designs to the user. d.tour's gallery interface decreases the gulfs of execution and evaluation for design example-finding.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {165–174},
numpages = {10},
keywords = {examples, design, search},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244835,
author = {Brandt, Joel},
title = {Session Details: With a Little Help},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244835},
doi = {10.1145/3244835},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047218,
author = {Matejka, Justin and Grossman, Tovi and Fitzmaurice, George},
title = {IP-QAT: In-Product Questions, Answers, &amp; Tips},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047218},
doi = {10.1145/2047196.2047218},
abstract = {We present IP-QAT, a new community-based question and answer system for software users. Unlike most community forums, IP-QAT is integrated into the actual software application, allowing users to easily post questions, answers and tips without having to leave the application. Our in-product implementation is context-aware and shows relevant posts based on a user's recent activity. It is also designed with minimal transaction costs to encourage users to easily post, include annotated images and file attachments, as well as tag their posts with relevant UI components. We describe a robust cloud-based system implementation, which allowed us to release IP-QAT to 37 users for a 2 week field study. Our study showed that IP-QAT increased user contributions, and subjectively, users found our system more useful and easier to use, in comparison to the existing commercial discussion board.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {175–184},
numpages = {10},
keywords = {community, in-product learning, Q&amp;A, help},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047219,
author = {Li, Wei and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
title = {TwitApp: In-Product Micro-Blogging for Design Sharing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047219},
doi = {10.1145/2047196.2047219},
abstract = {We describe TwitApp, an enhanced micro-blogging system integrated within AutoCAD for design sharing. TwitApp integrates rich content and still keeps the sharing transaction cost low. In TwitApp, tweets are organized by their project, and users can follow or unfollow each individual project. We introduce the concept of automatic tweet drafting and other novel features such as enhanced real-time search and integrated live video streaming. The TwitApp system leverages the existing Twitter micro-blogging system. We also contribute a study which provides insights on these concepts and associated designs, and demonstrates potential user excitement of such tools.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {185–194},
numpages = {10},
keywords = {micro-blogging, twitter, design, sharing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047220,
author = {Ekstrand, Michael and Li, Wei and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
title = {Searching for Software Learning Resources Using Application Context},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047220},
doi = {10.1145/2047196.2047220},
abstract = {Users of complex software applications frequently need to consult documentation, tutorials, and support resources to learn how to use the software and further their understand-ing of its capabilities. Existing online help systems provide limited context awareness through "what's this?" and simi-lar techniques. We examine the possibility of making more use of the user's current context in a particular application to provide useful help resources. We provide an analysis and taxonomy of various aspects of application context and how they may be used in retrieving software help artifacts with web browsers, present the design of a context-aware augmented web search system, and describe a prototype implementation and initial user study of this system. We conclude with a discussion of open issues and an agenda for further research.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {195–204},
numpages = {10},
keywords = {software learning, context-based search, help search},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047222,
author = {Wang, Ge},
title = {Breaking Barriers with Sound},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047222},
doi = {10.1145/2047196.2047222},
abstract = {The computer, in its many shapes and sizes, is evolving rapidly and pervading our everyday lives like never before. Mobile computing devices have become much more than simply "mobile", increasingly serving as personal and "natural" extensions of us. Therein lies immense potential to reshape the way we think and interact, and especially in how we engage one another creatively, expressively, and socially. This talk explores interaction and social design for music through the computer, told through laptop orchestras, mobile phone orchestras, an audio programming language, designing the iPhone's Ocarina, ecosystems for crowd-sourcing musical creation, and an emerging social dimension where computer, music, and people interact.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {205–206},
numpages = {2},
keywords = {interaction design, social, ocarina, music, mobile, laptop orchestra, computer},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244836,
author = {Terry, Michael},
title = {Session Details: Development},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244836},
doi = {10.1145/3244836},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047224,
author = {Fourney, Adam and Mann, Richard and Terry, Michael},
title = {Query-Feature Graphs: Bridging User Vocabulary and System Functionality},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047224},
doi = {10.1145/2047196.2047224},
abstract = {This paper introduces query-feature graphs, or QF-graphs. QF-graphs encode associations between high-level descriptions of user goals (articulated as natural language search queries) and the specific features of an interactive system relevant to achieving those goals. For example, a QF-graph for the GIMP graphics manipulation software links the query "GIMP black and white" to the commands "desaturate" and "grayscale." We demonstrate how QF-graphs can be constructed using search query logs, search engine results, web page content, and localization data from interactive systems. An analysis of QF-graphs shows that the associations produced by our approach exhibit levels of accuracy that make them eminently usable in a range of real-world applications. Finally, we present three hypothetical user interface mechanisms that illustrate the potential of QF-graphs: search-driven interaction, dynamic tooltips, and app-to-app analogy search.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {207–216},
numpages = {10},
keywords = {dynamic tooltips, query-feature graph, search-driven interaction, analogy search},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047225,
author = {Karrer, Thorsten and Kr\"{a}mer, Jan-Peter and Diehl, Jonathan and Hartmann, Bj\"{o}rn and Borchers, Jan},
title = {Stacksplorer: Call Graph Navigation Helps Increasing Code Maintenance Efficiency},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047225},
doi = {10.1145/2047196.2047225},
abstract = {We present Stacksplorer, a new tool to support source code navigation and comprehension. Stacksplorer computes the call graph of a given piece of code, visualizes relevant parts of it, and allows developers to interactively traverse it. This augments the traditional code editor by offering an additional layer of navigation. Stacksplorer is particularly useful to understand and edit unknown source code because branches of the call graph can be explored and backtracked easily. Visualizing the callers of a method reduces the risk of introducing unintended side effects. In a quantitative study, programmers using Stacksplorer performed three of four software maintenance tasks significantly faster and with higher success rates, and Stacksplorer received a System Usability Scale rating of 85.4 from participants.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {217–224},
numpages = {8},
keywords = {visualization, development tools / toolkits / programming environments},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047226,
author = {Eagan, James R. and Beaudouin-Lafon, Michel and Mackay, Wendy E.},
title = {Cracking the Cocoa Nut: User Interface Programming at Runtime},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047226},
doi = {10.1145/2047196.2047226},
abstract = {This article introduces runtime toolkit overloading, a novel approach to help third-party developers modify the interaction and behavior of existing software applications without access to their underlying source code. We describe the abstractions provided by this approach as well as the mechanisms for implementing them in existing environments. We describe Scotty, a prototype implementation for Mac OS X Cocoa that enables developers to modify existing applications at runtime, and we demonstrate a collection of interaction and functional transformations on existing off-the-shelf applications. We show how Scotty helps a developer make sense of unfamiliar software, even without access to its source code. We further discuss what features of future environments would facilitate this kind of runtime software development.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {225–234},
numpages = {10},
keywords = {runtime toolkit overloading, runtime software development, user interfaces, meta-toolkits},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047227,
author = {Schwarz, Julia and Mankoff, Jennifer and Hudson, Scott},
title = {Monte Carlo Methods for Managing Interactive State, Action and Feedback under Uncertainty},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047227},
doi = {10.1145/2047196.2047227},
abstract = {Current input handling systems provide effective techniques for modeling, tracking, interpreting, and acting on user input. However, new interaction technologies violate the standard assumption that input is certain. Touch, speech recognition, gestural input, and sensors for context often produce uncertain estimates of user inputs. Current systems tend to remove uncertainty early on. However, information available in the user interface and application can help to resolve uncertainty more appropriately for the end user. This paper presents a set of techniques for tracking the state of interactive objects in the presence of uncertain inputs. These techniques use a Monte Carlo approach to maintain a probabilistically accurate description of the user interface that can be used to make informed choices about actions. Samples are used to approximate the distribution of possible inputs, possible interactor states that result from inputs, and possible actions (callbacks and feedback) interactors may execute. Because each sample is certain, the developer can specify most of the behavior of interactors in a familiar, non-probabilistic fashion. This approach retains all the advantages of maintaining information about uncertainty while minimizing the need for the developer to work in probabilistic terms. We present a working implementation of our framework and illustrate the power of these techniques within a paint program that includes three different kinds of uncertain input.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {235–244},
numpages = {10},
keywords = {uncertain input, dialog specification, finite state machines, probabilistic modeling},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047228,
author = {Chang, Tsung-Hsiang and Yeh, Tom and Miller, Rob},
title = {Associating the Visual Representation of User Interfaces with Their Internal Structures and Metadata},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047228},
doi = {10.1145/2047196.2047228},
abstract = {Pixel-based methods are emerging as a new and promising way to develop new interaction techniques on top of existing user interfaces. However, in order to maintain platform independence, other available low-level information about GUI widgets, such as accessibility metadata, was neglected intentionally. In this paper, we present a hybrid framework, PAX, which associates the visual representation of user interfaces (i.e. the pixels) and their internal hierarchical metadata (i.e. the content, role, and value). We identify challenges to building such a framework. We also develop and evaluate two new algorithms for detecting text at arbitrary places on the screen, and for segmenting a text image into individual word blobs. Finally, we validate our framework in implementations of three applications. We enhance an existing pixel-based system, Sikuli Script, and preserve the readability of its script code at the same time. Further, we create two novel applications, Screen Search and Screen Copy, to demonstrate how PAX can be applied to development of desktop-level interactive systems.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {245–256},
numpages = {12},
keywords = {text detection, pixel, text segmentation, graphical user interfaces, accessibility api},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047229,
author = {Dragicevic, Pierre and Huot, St\'{e}phane and Chevalier, Fanny},
title = {Gliimpse: Animating from Markup Code to Rendered Documents and Vice Versa},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047229},
doi = {10.1145/2047196.2047229},
abstract = {We present a quick preview technique that smoothly transitions between document markup code and its visual rendering. This technique allows users to regularly check the code they are editing in-place, without leaving the text editor. This method can complement classical preview windows by offering rapid overviews of code-to-document mappings and leaving more screen real-estate. We discuss the design and implementation of our technique.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {257–262},
numpages = {6},
keywords = {animation, markup code, document editing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244837,
author = {Bigham, Jeff},
title = {Session Details: Tactile/Blind},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244837},
doi = {10.1145/3244837},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047231,
author = {Lin, Felix Xiaozhu and Ashbrook, Daniel and White, Sean},
title = {RhythmLink: Securely Pairing I/O-Constrained Devices by Tapping},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047231},
doi = {10.1145/2047196.2047231},
abstract = {We present RhythmLink, a system that improves the wireless pairing user experience. Users can link devices such as phones and headsets together by tapping a known rhythm on each device. In contrast to current solutions, RhythmLink does not require user interaction with the host device during the pairing process; and it only requires binary input on the peripheral, making it appropriate for small devices with minimal physical affordances. We describe the challenges in enabling this user experience and our solution, an algorithm that allows two devices to compare imprecisely-entered tap sequences while maintaining the secrecy of those sequences. We also discuss our prototype implementation of RhythmLink and review the results of initial user tests.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {263–272},
numpages = {10},
keywords = {security, rhythm, input, taps, mobile devices, device pairing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047232,
author = {Kane, Shaun K. and Morris, Meredith Ringel and Perkins, Annuska Z. and Wigdor, Daniel and Ladner, Richard E. and Wobbrock, Jacob O.},
title = {Access Overlays: Improving Non-Visual Access to Large Touch Screens for Blind Users},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047232},
doi = {10.1145/2047196.2047232},
abstract = {Many touch screens remain inaccessible to blind users, and those approaches to providing access that do exist offer minimal support for interacting with large touch screens or spatial data. In this paper, we introduce a set of three software-based access overlays intended to improve the accessibility of large touch screen interfaces, specifically interactive tabletops. Our access overlays are called edge projection, neighborhood browsing, and touch-and-speak. In a user study, 14 blind users compared access overlays to an implementation of Apple's VoiceOver screen reader. Our results show that two of our techniques were faster than VoiceOver, that participants correctly answered more questions about the screen's layout using our techniques, and that participants overwhelmingly preferred our techniques. We developed several applications demonstrating the use of access overlays, including an accessible map kiosk and an accessible board game.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {273–282},
numpages = {10},
keywords = {blindness, visual impairments, touch screens, accessibility},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047233,
author = {Gustafson, Sean and Holz, Christian and Baudisch, Patrick},
title = {Imaginary Phone: Learning Imaginary Interfaces by Transferring Spatial Memory from a Familiar Device},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047233},
doi = {10.1145/2047196.2047233},
abstract = {We propose a method for learning how to use an imaginary interface (i.e., a spatial non-visual interface) that we call "transfer learning". By using a physical device (e.g. an iPhone) a user inadvertently learns the interface and can then transfer that knowledge to an imaginary interface. We illustrate this concept with our Imaginary Phone prototype. With it users interact by mimicking the use of a physical iPhone by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing on the hand is tracked using a depth camera and touch events are sent wirelessly to an actual iPhone, where they invoke the corresponding actions. Our prototype allows the user to perform everyday task such as picking up a phone call or launching the timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that frees users from the necessity of retrieving the actual physical device. We present two user studies that validate the three assumptions underlying the transfer learning method. (1) Users build up spatial memory automatically while using a physical device: participants knew the correct location of 68% of their own iPhone home screen apps by heart. (2) Spatial memory transfers from a physical to an imaginary inter-face: participants recalled 61% of their home screen apps when recalling app location on the palm of their hand. (3) Palm interaction is precise enough to operate a typical mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their palm-sufficiently large to operate any iPhone standard widget.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {283–292},
numpages = {10},
keywords = {non-visual, spatial memory, memory, mobile, screen-less, imaginary interface, touch, wearable},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047234,
author = {R\"{u}melin, Sonja and Rukzio, Enrico and Hardy, Robert},
title = {NaviRadar: A Novel Tactile Information Display for Pedestrian Navigation},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047234},
doi = {10.1145/2047196.2047234},
abstract = {We introduce NaviRadar: an interaction technique for mobile phones that uses a radar metaphor in order to communicate the user's correct direction for crossings along a desired route. A radar sweep rotates clockwise and tactile feedback is provided where each sweep distinctly conveys the user's current direction and the direction in which the user must travel. In a first study, we evaluated the overall concept and tested five different tactile patterns to communicate the two different directions via a single tactor. The results show that people are able to easily understand the NaviRadar concept and can identify the correct direction with a mean deviation of 37° out of the full 360° provided. A second study shows that NaviRadar achieves similar results in terms of perceived usability and navigation performance when compared with spoken instructions. By using only tactile feedback, NaviRadar provides distinct advantages over current systems. In particular, no visual attention is required to navigate; thus, it can be spent on providing greater awareness of one's surroundings. Moreover, the lack of audio attention enables it to be used in noisy environments or this attention can be better spent on talking with others during navigation.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {293–302},
numpages = {10},
keywords = {pedestrian navigation, tactile feedback, radar},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047235,
author = {Saponas, T. Scott and Harrison, Chris and Benko, Hrvoje},
title = {PocketTouch: Through-Fabric Capacitive Touch Input},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047235},
doi = {10.1145/2047196.2047235},
abstract = {PocketTouch is a capacitive sensing prototype that enables eyes-free multitouch input on a handheld device without having to remove the device from the pocket of one's pants, shirt, bag, or purse. PocketTouch enables a rich set of gesture interactions, ranging from simple touch strokes to full alphanumeric text entry. Our prototype device consists of a custom multitouch capacitive sensor mounted on the back of a smartphone. Similar capabilities could be enabled on most existing capacitive touchscreens through low-level access to the capacitive sensor. We demonstrate how touch strokes can be used to initialize the device for interaction and how strokes can be processed to enable text recognition of characters written over the same physical area. We also contribute a comparative study that empirically measures how different fabrics attenuate touch inputs, providing insight for future investigations. Our results suggest that PocketTouch will work reliably with a wide variety of fabrics used in today's garments, and is a viable input method for quick eyes-free operation of devices in pockets.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {303–308},
numpages = {6},
keywords = {touch, always-available input, touch-stroke text entry, eyes-free, wearable computing, gestures},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047236,
author = {Manabe, Hiroyuki and Fukumoto, Masaaki},
title = {Tap Control for Headphones without Sensors},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047236},
doi = {10.1145/2047196.2047236},
abstract = {A tap control technique for headphones is proposed. A simple circuit is used to detect tapping of the headphone shell by using the speaker unit in the headphone as a tap sensor. No additional devices are required in the headphone shell and cable, so the user can use their favorite headphones as a controller while listening music. A prototype is implemented with several calibration processes to compensate the differences in headphones and users' tapping actions. Tests confirm that the user can control a music player by tapping regular headphones.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {309–314},
numpages = {6},
keywords = {tap, input device, wearable, headphones},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244838,
author = {Beaudouin-Lafon, Michel},
title = {Session Details: Tangible},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244838},
doi = {10.1145/3244838},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047238,
author = {Marquardt, Nicolai and Diaz-Marino, Robert and Boring, Sebastian and Greenberg, Saul},
title = {The Proximity Toolkit: Prototyping Proxemic Interactions in Ubiquitous Computing Ecologies},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047238},
doi = {10.1145/2047196.2047238},
abstract = {People naturally understand and use proxemic relationships (e.g., their distance and orientation towards others) in everyday situations. However, only few ubiquitous computing (ubicomp) systems interpret such proxemic relationships to mediate interaction (proxemic interaction). A technical problem is that developers find it challenging and tedious to access proxemic information from sensors. Our Proximity Toolkit solves this problem. It simplifies the exploration of interaction techniques by supplying fine-grained proxemic information between people, portable devices, large interactive surfaces, and other non-digital objects in a room-sized environment. The toolkit offers three key features. 1) It facilitates rapid prototyping of proxemic-aware systems by supplying developers with the orientation, distance, motion, identity, and location information between entities. 2) It includes various tools, such as a visual monitoring tool, that allows developers to visually observe, record and explore proxemic relationships in 3D space. (3) Its flexible architecture separates sensing hardware from the proxemic data model derived from these sensors, which means that a variety of sensing technologies can be substituted or combined to derive proxemic information. We illustrate the versatility of the toolkit with proxemic-aware systems built by students.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {315–326},
numpages = {12},
keywords = {proxemics, proximity, development, toolkit, prototyping, proxemic interactions, ubiquitous computing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047239,
author = {Lee, Jinha and Post, Rehmi and Ishii, Hiroshi},
title = {ZeroN: Mid-Air Tangible Interaction Enabled by Computer Controlled Magnetic Levitation},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047239},
doi = {10.1145/2047196.2047239},
abstract = {This paper presents ZeroN, a new tangible interface element that can be levitated and moved freely by computer in a three dimensional space. ZeroN serves as a tangible rep-resentation of a 3D coordinate of the virtual world through which users can see, feel, and control computation. To ac-complish this, we developed a magnetic control system that can levitate and actuate a permanent magnet in a pre-defined 3D volume. This is combined with an optical tracking and display system that projects images on the levitating object. We present applications that explore this new interaction modality. Users are invited to place or move the ZeroN object just as they can place objects on surfaces. For example, users can place the sun above physical objects to cast digital shadows, or place a planet that will start revolving based on simulated physical conditions. We describe the technology and interaction scenarios, discuss initial observations, and outline future development.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {327–336},
numpages = {10},
keywords = {gesture interface, tangible interface, 3D interface, magnetic levitation},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047240,
author = {Annett, Michelle and Grossman, Tovi and Wigdor, Daniel and Fitzmaurice, George},
title = {Medusa: A Proximity-Aware Multi-Touch Tabletop},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047240},
doi = {10.1145/2047196.2047240},
abstract = {We present Medusa, a proximity-aware multi-touch tabletop. Medusa uses 138 inexpensive proximity sensors to: detect a user's presence and location, determine body and arm locations, distinguish between the right and left arms, and map touch point to specific users and specific hands. Our tracking algorithms and hardware designs are described. Exploring this unique design, we develop and report on a collection of interactions enabled by Medusa in support of multi-user collaborative design, specifically within the context of Proxi-Sketch, a multi-user UI prototyping tool. We discuss design issues, system implementation, limitations, and generalizable concepts throughout the paper.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {337–346},
numpages = {10},
keywords = {proximity, context-aware, tabletop, hover, bi-manual, proxemics, gestures, touch, multi-touch},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047241,
author = {Avrahami, Daniel and Wobbrock, Jacob O. and Izadi, Shahram},
title = {Portico: Tangible Interaction on and around a Tablet},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047241},
doi = {10.1145/2047196.2047241},
abstract = {We present Portico, a portable system for enabling tangible interaction on and around tablet computers. Two cameras on small foldable arms are positioned above the display to recognize a variety of physical objects placed on or around the tablet. These cameras have a larger field-of-view than the screen, allowing Portico to extend interaction significantly beyond the tablet itself. Our prototype, which uses a 12" tablet, delivers an interaction space six times the size of the tablet screen. Portico thus allows tablets to extend both their sensing capabilities and interaction space without sacrificing portability. We describe the design of our system and present a number of applications that demonstrate Portico's unique capability to track objects. We focus on a number of fun applications that demonstrate how such a device can be used as a low-cost way to create personal surface computing experiences. Finally, we discuss the challenges in supporting tangible interaction beyond the screen and describe possible mechanisms for overcoming them.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {347–356},
numpages = {10},
keywords = {TUI, tablet, surface, tangible, portable},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047242,
author = {Vogel, Daniel and Casiez, G\'{e}ry},
title = {Cont\'{e}: Multimodal Input Inspired by an Artist's Crayon},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047242},
doi = {10.1145/2047196.2047242},
abstract = {Cont\'{e} is a small input device inspired by the way artists manipulate a real Cont\'{e} crayon. By changing which corner, edge, end, or side is contacting the display, the operator can switch interaction modes using a single hand. Cont\'{e}'s rectangular prism shape enables both precise pen-like input and tangible handle interaction. Cont\'{e} also has a natural compatibility with multi-touch input: it can be tucked in the palm to interleave same-hand touch input, or used to expand the vocabulary of bimanual touch. Inspired by informal interviews with artists, we catalogue Cont\'{e}'s characteristics, and use these to outline a design space. We describe a prototype device using common materials and simple electronics. With this device, we demonstrate interaction techniques in a test-bed drawing application. Finally, we discuss alternate hardware designs and future human factors research to study this new class of input.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {357–366},
numpages = {10},
keywords = {gestures, multimodal, pen, tabletop, touch},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047243,
author = {Yu, Neng-Hao and Tsai, Sung-Sheng and Hsiao, I-Chun and Tsai, Dian-Je and Lee, Meng-Han and Chen, Mike Y. and Hung, Yi-Ping},
title = {Clip-on Gadgets: Expanding Multi-Touch Interaction Area with Unpowered Tactile Controls},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047243},
doi = {10.1145/2047196.2047243},
abstract = {Virtual keyboards and controls, commonly used on mobile multi-touch devices, occlude content of interest and do not provide tactile feedback. Clip-on Gadgets solve these issues by extending the interaction area of multi-touch devices with physical controllers. Clip-on Gadgets use only conductive materials to map user input on the controllers to touch points on the edges of screens; therefore, they are battery-free, lightweight, and low-cost. In addition, they can be used in combination with multi-touch gestures. We present several hardware designs and a software toolkit, which enable users to simply attach Clip-on Gadgets to an edge of a device and start interacting with it.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {367–372},
numpages = {6},
keywords = {tangible, tactile input, physical controllers, multi-touch, toolkit, mobile devices},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244839,
author = {Hartmann, Bjoern},
title = {Session Details: Sensing Form and Rhythm},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244839},
doi = {10.1145/3244839},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047245,
author = {Fernquist, Jennifer and Grossman, Tovi and Fitzmaurice, George},
title = {Sketch-Sketch Revolution: An Engaging Tutorial System for Guided Sketching and Application Learning},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047245},
doi = {10.1145/2047196.2047245},
abstract = {We describe Sketch-Sketch Revolution, a new tutorial system that allows any user to experience the success of drawing content previously created by an expert artist. Sketch-Sketch Revolution not only guides users through the application user interface, it also provides assistance with the actual sketching. In addition, the system offers an authoring tool that enables artists to create content and then automatically generates a tutorial from their recorded workflow history. Sketch-Sketch Revolution is a unique hybrid tutorial system that combines in-product, content-centric and reactive tutorial methods to provide an engaging learning experience. A qualitative user study showed that our system successfully taught users how to interact with a drawing application user interface, gave users confidence they could recreate expert content, and was uniformly considered useful and easy to use.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {373–382},
numpages = {10},
keywords = {learning, engagement, sketching, tutorials},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047246,
author = {Thiel, Yannick and Singh, Karan and Balakrishnan, Ravin},
title = {Elasticurves: Exploiting Stroke Dynamics and Inertia for the Real-Time Neatening of Sketched 2D Curves},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047246},
doi = {10.1145/2047196.2047246},
abstract = {Elasticurves present a novel approach to neaten sketches in real-time, resulting in curves that combine smoothness with user-intended detail. Inspired by natural variations in stroke speed when drawing quickly or with precision, we exploit stroke dynamics to distinguish intentional fine detail from stroke noise. Combining inertia and stroke dynamics, elasticurves can be imagined as the trace of a pen attached to the user by an oscillation-free elastic band. Sketched quickly, the elasticurve spatially lags behind the stroke, smoothing over stroke detail, but catches up and matches the input stroke at slower speeds. Connectors, such as lines or circular-arcs link the evolving elasticurve to the next input point, growing the curve by a responsiveness fraction along the connector. Responsiveness is calibrated, to reflect drawing skill or device noise. Elasticurves are theoretically sound and robust to variations in stroke sampling. Practically, they neaten digital strokes in real-time while retaining the modeless and visceral feel of pen on paper.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {383–392},
numpages = {10},
keywords = {stroke-based interfaces, fair curve design, sketching},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047247,
author = {Savva, Manolis and Kong, Nicholas and Chhajta, Arti and Fei-Fei, Li and Agrawala, Maneesh and Heer, Jeffrey},
title = {ReVision: Automated Classification, Analysis and Redesign of Chart Images},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047247},
doi = {10.1145/2047196.2047247},
abstract = {Poorly designed charts are prevalent in reports, magazines, books and on the Web. Most of these charts are only available as bitmap images; without access to the underlying data it is prohibitively difficult for viewers to create more effective visual representations. In response we present ReVision, a system that automatically redesigns visualizations to improve graphical perception. Given a bitmap image of a chart as input, ReVision applies computer vision and machine learning techniques to identify the chart type (e.g., pie chart, bar chart, scatterplot, etc.). It then extracts the graphical marks and infers the underlying data. Using a corpus of images drawn from the web, ReVision achieves image classification accuracy of 96% across ten chart categories. It also accurately extracts marks from 79% of bar charts and 62% of pie charts, and from these charts it successfully extracts data from 71% of bar charts and 64% of pie charts. ReVision then applies perceptually-based design principles to populate an interactive gallery of redesigned charts. With this interface, users can view alternative chart designs and retarget content to different visual styles.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {393–402},
numpages = {10},
keywords = {visualization, redesign, chart understanding, information extraction, computer vision},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047248,
author = {Flatla, David R. and Gutwin, Carl and Nacke, Lennart E. and Bateman, Scott and Mandryk, Regan L.},
title = {Calibration Games: Making Calibration Tasks Enjoyable by Adding Motivating Game Elements},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047248},
doi = {10.1145/2047196.2047248},
abstract = {Interactive systems often require calibration to ensure that input and output are optimally configured. Without calibration, user performance can degrade (e.g., if an input device is not adjusted for the user's abilities), errors can increase (e.g., if color spaces are not matched), and some interactions may not be possible (e.g., use of an eye tracker). The value of calibration is often lost, however, because many calibration processes are tedious and unenjoyable, and many users avoid them altogether. To address this problem, we propose calibration games that gather calibration data in an engaging and entertaining manner. To facilitate the creation of calibration games, we present design guidelines that map common types of calibration to core tasks, and then to well-known game mechanics. To evaluate the approach, we developed three calibration games and compared them to standard procedures. Users found the game versions significantly more enjoyable than regular calibration procedures, without compromising the quality of the data. Calibration games are a novel way to motivate users to carry out calibrations, thereby improving the performance and accuracy of many human-computer systems.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {403–412},
numpages = {10},
keywords = {modeling, computer games, calibration},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047249,
author = {Yamamoto, Yusuke and Uchiyama, Hideaki and Kakehi, Yasuaki},
title = {OnNote: Playing Printed Music Scores as a Musical Instrument},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047249},
doi = {10.1145/2047196.2047249},
abstract = {This paper presents a novel musical performance system named onNote that directly utilizes printed music scores as a musical instrument. This system can make users believe that sound is indeed embedded on the music notes in the scores. The users can play music simply by placing, moving and touching the scores under a desk lamp equipped with a camera and a small projector. By varying the movement, the users can control the playing sound and the tempo of the music. To develop this system, we propose an image processing based framework for retrieving music from a music database by capturing printed music scores. From a captured image, we identify the scores by matching them with the reference music scores, and compute the position and pose of the scores with respect to the camera. By using this framework, we can develop novel types of musical interactions.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {413–422},
numpages = {10},
keywords = {image retrieval, tangible interface, musical instrument},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047250,
author = {Moraveji, Neema and Olson, Ben and Nguyen, Truc and Saadat, Mahmoud and Khalighi, Yaser and Pea, Roy and Heer, Jeffrey},
title = {Peripheral Paced Respiration: Influencing User Physiology during Information Work},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047250},
doi = {10.1145/2047196.2047250},
abstract = {We present the design and evaluation of a technique for influencing user respiration by integrating respiration-pacing methods into the desktop operating system in a peripheral manner. Peripheral paced respiration differs from prior techniques in that it does not require the user's full attention. We conducted a within-subjects study to evaluate the efficacy of peripheral paced respiration, as compared to no feedback, in an ecologically valid environment. Participant respiration decreased significantly in the pacing condition. Upon further analysis, we attribute this difference to a significant decrease in breath rate while the intermittent pacing feedback is active, rather than a persistent change in respiratory pattern. The results have implications for researchers in physiological computing, biofeedback designers, and human-computer interaction researchers concerned with user stress and affect.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {423–428},
numpages = {6},
keywords = {respiration, peripheral, biofeedback, stress},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047252,
author = {Jurafsky, Dan},
title = {Sex, Food, and Words: The Hidden Meanings behind Everyday Language},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047252},
doi = {10.1145/2047196.2047252},
abstract = {Language is a subtle and powerful tool for communication. But the words we use also provide a rich mine of information for the social scientist. The history of words like "ketchup", "ceviche", or "dessert" tells us about the relationships between the superpowers who dominated the globe 500 or 1000 years ago. The words on the back of potato chip packages can demonstrate popular attitudes toward social class. And the names we give ice cream flavors may be an evolutionary reflex of the attempt by early mammals to appear larger than their competitors. The language of dating is just as informative as the language of food. In experiments with speed dating, work in our lab shows that we can detect flirtation or other stances in men and women on dates, just by looking at linguistic features like their pitch, their use of negative words like "can't" or "don't", or how often they use hedges like "sort of" or "kind of". The language of these two popular topics of conversation, food and dating, can teach us a lot about history, culture, and psychology.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {429–430},
numpages = {2},
keywords = {food, dating, language},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244840,
author = {Ballagas, Rafael Tico},
title = {Session Details: Mobile},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244840},
doi = {10.1145/3244840},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047254,
author = {Willis, Karl D.D. and Poupyrev, Ivan and Hudson, Scott E. and Mahler, Moshe},
title = {SideBySide: Ad-Hoc Multi-User Interaction with Handheld Projectors},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047254},
doi = {10.1145/2047196.2047254},
abstract = {We introduce SideBySide, a system designed for ad-hoc multi-user interaction with handheld projectors. SideBySide uses device-mounted cameras and hybrid visible/infrared light projectors to track multiple independent projected images in relation to one another. This is accomplished by projecting invisible fiducial markers in the near-infrared spectrum. Our system is completely self-contained and can be deployed as a handheld device without instrumentation of the environment. We present the design and implementation of our system including a hybrid handheld projector to project visible and infrared light, and techniques for tracking projected fiducial markers that move and overlap. We introduce a range of example applications that demonstrate the applicability of our system to real-world scenarios such as mobile content exchange, gaming, and education.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {431–440},
numpages = {10},
keywords = {ad hoc interaction, handheld projector, interaction techniques, multi-user, pico projector, games},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047255,
author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
title = {OmniTouch: Wearable Multitouch Interaction Everywhere},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047255},
doi = {10.1145/2047196.2047255},
abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {441–450},
numpages = {10},
keywords = {on-demand interfaces, appropriated surfaces, object classification, finger tracking, on-body computing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047256,
author = {Cauchard, Jessica R. and L\"{o}chtefeld, Markus and Irani, Pourang and Schoening, Johannes and Kr\"{u}ger, Antonio and Fraser, Mike and Subramanian, Sriram},
title = {Visual Separation in Mobile Multi-Display Environments},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047256},
doi = {10.1145/2047196.2047256},
abstract = {Projector phones, handheld game consoles and many other mobile devices increasingly include more than one display, and therefore present a new breed of mobile Multi-Display Environments (MDEs) to users. Existing studies illustrate the effects of visual separation between displays in MDEs and suggest interaction techniques that mitigate these effects. Currently, mobile devices with heterogeneous displays such as projector phones are often designed without reference to visual separation issues; therefore it is critical to establish whether concerns and opportunities raised in the existing MDE literature apply to the emerging category of Mobile MDEs (MMDEs). This paper investigates the effects of visual separation in the context of MMDEs and contrasts these with fixed MDE results, and explores design factors for Mobile MDEs. Our study uses a novel eye-tracking methodology for measuring switches in visual context between displays and identifies that MMDEs offer increased design flexibility over traditional MDEs in terms of visual separation. We discuss these results and identify several design implications.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {451–460},
numpages = {10},
keywords = {mobile multi-display environment, visual search, eye tracking, mobile projection, visual separation, handheld devices},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047257,
author = {Li, Frank Chun Yat and Guy, Richard T. and Yatani, Koji and Truong, Khai N.},
title = {The 1line Keyboard: A QWERTY Layout in a Single Line},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047257},
doi = {10.1145/2047196.2047257},
abstract = {Current soft QWERTY keyboards often consume a large portion of the screen space on portable touchscreens. This space consumption can diminish the overall user experi-ence on these devices. In this paper, we present the 1Line keyboard, a soft QWERTY keyboard that is 140 pixels tall (in landscape mode) and 40% of the height of the native iPad QWERTY keyboard. Our keyboard condenses the three rows of keys in the normal QWERTY layout into a single line with eight keys. The sizing of the eight keys is based on users' mental layout of a QWERTY keyboard on an iPad. The system disambiguates the word the user types based on the sequence of keys pressed. The user can use flick gestures to perform backspace and enter, and tap on the bezel below the keyboard to input a space. Through an evaluation, we show that participants are able to quickly learn how to use the 1Line keyboard and type at a rate of over 30 WPM after just five 20-minute typing sessions. Using a keystroke level model, we predict the peak expert text entry rate with the 1Line keyboard to be 66--68 WPM.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {461–470},
numpages = {10},
keywords = {soft keyboard, keystroke level model, reduced keyboard, text entry, word disambiguation},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047258,
author = {MacKenzie, I. Scott and Soukoreff, R. William and Helga, Joanna},
title = {1 Thumb, 4 Buttons, 20 Words per Minute: Design and Evaluation of H4-Writer},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047258},
doi = {10.1145/2047196.2047258},
abstract = {We present what we believe is the most efficient and quickest four-key text entry method available. H4-Writer uses Huffman coding to assign minimized key sequences to letters, with full access to error correction, punctuation, digits, modes, etc. The key sequences are learned quickly, and support eyes-free entry. With KSPC = 2.321, the effort to enter text is comparable to multitap on a mobile phone keypad; yet multitap requires nine keys. In a longitudinal study with six participants, an average text entry speed of 20.4 wpm was observed in the 10th session. Error rates were under 1%. To improve external validity, an extended session was included that required input of punctuation and other symbols. Entry speed dropped only by about 3 wpm, suggesting participants quickly leveraged their acquired skill with H4-Writer to access advanced features.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {471–480},
numpages = {10},
keywords = {text entry, Huffman coding, mobile text entry, small devices},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047259,
author = {Lin, Shu-Yang and Su, Chao-Huai and Cheng, Kai-Yin and Liang, Rong-Hao and Kuo, Tzu-Hao and Chen, Bing-Yu},
title = {Pub - Point upon Body: Exploring Eyes-Free Interaction and Methods on an Arm},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047259},
doi = {10.1145/2047196.2047259},
abstract = {This paper presents a novel interaction system, PUB (Point Upon Body), to explore eyes-free interaction in a personal space by allowing users tapping on their own arms to be provided with haptic feedback from their skin. Two user studies determine how users can interact precisely with their forearms and how users behave when operating in their arm space. According to those results, normal users can divide their arm space at most into 6 points between their wrists and elbows with iterative practice. Experimental results also indicate that the divided pattern of each user is unique from that of other ones. Based on the design principles from the observations, an interaction system, PUB, is designed to demonstrate how interaction design benefits from those findings. Two scenarios, remote display control and mobile device control, are demonstrated through the UltraSonic device attached on the users' wrists to detect their tapped positions.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {481–488},
numpages = {8},
keywords = {kinetic user interface, natural haptic feedback., eyes-free interaction},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244841,
author = {Wilson, Andy},
title = {Session Details: Sensing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244841},
doi = {10.1145/3244841},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047261,
author = {Zizka, Jan and Olwal, Alex and Raskar, Ramesh},
title = {SpeckleSense: Fast, Precise, Low-Cost and Compact Motion Sensing Using Laser Speckle},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047261},
doi = {10.1145/2047196.2047261},
abstract = {Motion sensing is of fundamental importance for user interfaces and input devices. In applications, where optical sensing is preferred, traditional camera-based approaches can be prohibitive due to limited resolution, low frame rates and the required computational power for image processing. We introduce a novel set of motion-sensing configurations based on laser speckle sensing that are particularly suitable for human-computer interaction. The underlying principles allow these configurations to be fast, precise, extremely compact and low cost. We provide an overview and design guidelines for laser speckle sensing for user interaction and introduce four general speckle projector/sensor configurations. We describe a set of prototypes and applications that demonstrate the versatility of our laser speckle sensing techniques.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {489–498},
numpages = {10},
keywords = {laser speckle, tracking, mouse, input devices},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047262,
author = {Chigira, Hiroshi and Maeda, Atsuhiko and Kobayashi, Minoru},
title = {Area-Based Photo-Plethysmographic Sensing Method for the Surfaces of Handheld Devices},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047262},
doi = {10.1145/2047196.2047262},
abstract = {Capturing the user's vital signs is an urgent goal in the HCI community. Photo-plethysmography (PPG) is one approach; it can collect data from the finger tips that indicate the user's autonomic nervous system (ANS) and offers new potentials such as mental stress measurement and drowsy state detection. Our goal is to set PPG sensors on the surfaces of ordinary devices such as mice, smartphones, and steering wheels. This will offer smart monitoring without the burden of additional wearable sensors. Unfortunately, current PPG sensors are very narrow, and even if the sensor is attached to the surface of a device, the user is forced to align and hold the finger to the sensor point, which degrades device usability. To solve this problem, we propose an area-based sensing method that relaxes the alignment requirement. The proposed method uses two thin acrylic plates, a diffuser plate and a detection plate, as an IR waveguide. The proposed method can yield very thin sensing surfaces and gentle curvatures are possible. An experiment compares the proposed method to the conventional point-sensor in terms of LF/HF discrimination performance with the participant in the resting state, and the proposed method is shown to offer comparable sensing performance with superior usability.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {499–508},
numpages = {10},
keywords = {photo-plethysmography, wave guide, light guide, heart rate variability, lf/hf, mental stress, vital sensing, autonomic nervous system, drowsiness},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047263,
author = {Sugiura, Yuta and Kakehi, Gota and Withana, Anusha and Lee, Calista and Sakamoto, Daisuke and Sugimoto, Maki and Inami, Masahiko and Igarashi, Takeo},
title = {Detecting Shape Deformation of Soft Objects Using Directional Photoreflectivity Measurement},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047263},
doi = {10.1145/2047196.2047263},
abstract = {We present the FuwaFuwa sensor module, a round, hand-size, wireless device for measuring the shape deformations of soft objects such as cushions and plush toys. It can be embedded in typical soft objects in the household without complex installation procedures and without spoiling the softness of the object because it requires no physical connection. Six LEDs in the module emit IR light in six orthogonal directions, and six corresponding photosensors measure the reflected light energy. One can easily convert almost any soft object into a touch-input device that can detect both touch position and surface displacement by embedding multiple FuwaFuwa sensor modules in the object. A variety of example applications illustrate the utility of the FuwaFuwa sensor module. An evaluation of the proposed deformation measurement technique confirms its effectiveness.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {509–516},
numpages = {8},
keywords = {multiple sensors, soft user interface, density measurement, photoreflectivity, tangible user interface},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047264,
author = {Wimmer, Raphael and Baudisch, Patrick},
title = {Modular and Deformable Touch-Sensitive Surfaces Based on Time Domain Reflectometry},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047264},
doi = {10.1145/2047196.2047264},
abstract = {Time domain reflectometry, a technique originally used in diagnosing cable faults, can also locate where a cable is being touched. In this paper, we explore how to extend time domain reflectometry in order to touch-enable thin, modular, and deformable surfaces and devices. We demonstrate how to use this approach to make smart clothing and to rapid prototype touch-sensitive objects of arbitrary shape. To accomplish this, we extend time domain reflectometry in three ways: (1) Thin: We demonstrate how to run time domain reflectometry on a single wire. This allows us to touch-enable thin metal objects, such as guitar strings. (2) Modularity: We present a two-pin connector system that allows users to daisy chain touch-sensitive segments. We illustrate these enhancements with 13 prototypes and a series of performance measurements. (3) Deformability: We create deformable touch devices by mounting stretch-able wire patterns onto elastic tape and meshes. We present selected performance measurements.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {517–526},
numpages = {10},
keywords = {input, wearable, TDR, touch sensing, deformable, time domain reflectometry, capacitive sensing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047265,
author = {Follmer, Sean and Johnson, Micah and Adelson, Edward and Ishii, Hiroshi},
title = {DeForm: An Interactive Malleable Surface for Capturing 2.5D Arbitrary Objects, Tools and Touch},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047265},
doi = {10.1145/2047196.2047265},
abstract = {We introduce a novel input device, deForm, that supports 2.5D touch gestures, tangible tools, and arbitrary objects through real-time structured light scanning of a malleable surface of interaction. DeForm captures high-resolution surface deformations and 2D grey-scale textures of a gel surface through a three-phase structured light 3D scanner. This technique can be combined with IR projection to allow for invisible capture, providing the opportunity for co-located visual feedback on the deformable surface. We describe methods for tracking fingers, whole hand gestures, and arbitrary tangible tools. We outline a method for physically encoding fiducial marker information in the height map of tangible tools. In addition, we describe a novel method for distinguishing between human touch and tangible tools, through capacitive sensing on top of the input surface. Finally we motivate our device through a number of sample applications.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {527–536},
numpages = {10},
keywords = {sculpting interfaces, malleable surface, 2.5D input device, deformable interfaces},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047266,
author = {Harrison, Chris and Hudson, Scott E.},
title = {A New Angle on Cheap LCDs: Making Positive Use of Optical Distortion},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047266},
doi = {10.1145/2047196.2047266},
abstract = {Most LCD screens exhibit color distortions when viewed at oblique angles. Engineers have invested significant time and resources to alleviate this effect. However, the massive manufacturing base, as well as millions of in-the-wild monitors, means this effect will be common for many years to come. We take an opposite stance, embracing these optical peculiarities, and consider how they can be used in productive ways. This paper discusses how a special palette of colors can yield visual elements that are invisible when viewed straight-on, but visible at oblique angles. In essence, this allows conventional, unmodified LCD screens to output two images simultaneously - a feature normally only available in far more complex setups. We enumerate several applications that could take advantage of this ability.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {537–540},
numpages = {4},
keywords = {dual output, multiuser, privacy, displays},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244842,
author = {Feiner, Steve},
title = {Session Details: 3D},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244842},
doi = {10.1145/3244842},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047268,
author = {Leithinger, Daniel and Lakatos, David and DeVincenzi, Anthony and Blackshaw, Matthew and Ishii, Hiroshi},
title = {Direct and Gestural Interaction with Relief: A 2.5D Shape Display},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047268},
doi = {10.1145/2047196.2047268},
abstract = {Actuated shape output provides novel opportunities for experiencing, creating and manipulating 3D content in the physical world. While various shape displays have been proposed, a common approach utilizes an array of linear actuators to form 2.5D surfaces. Through identifying a set of common interactions for viewing and manipulating content on shape displays, we argue why input modalities beyond direct touch are required. The combination of freehand gestures and direct touch provides additional degrees of freedom and resolves input ambiguities, while keeping the locus of interaction on the shape output. To demonstrate the proposed combination of input modalities and explore applications for 2.5D shape displays, two example scenarios are implemented on a prototype system.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {541–548},
numpages = {8},
keywords = {direct manipulation, tangible input, shape display, actuated surface, gestural input},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047269,
author = {Wang, Robert and Paris, Sylvain and Popovi\'{c}, Jovan},
title = {6D Hands: Markerless Hand-Tracking for Computer Aided Design},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047269},
doi = {10.1145/2047196.2047269},
abstract = {Computer Aided Design (CAD) typically involves tasks such as adjusting the camera perspective and assembling pieces in free space that require specifying 6 degrees of freedom (DOF). The standard approach is to factor these DOFs into 2D subspaces that are mapped to the x and y axes of a mouse. This metaphor is inherently modal because one needs to switch between subspaces, and disconnects the input space from the modeling space. In this paper, we propose a bimanual hand tracking system that provides physically-motivated 6-DOF control for 3D assembly. First, we discuss a set of principles that guide the design of our precise, easy-to-use, and comfortable-to-use system. Based on these guidelines, we describe a 3D input metaphor that supports constraint specification classically used in CAD software, is based on only a few simple gestures, lets users rest their elbows on their desk, and works alongside the keyboard and mouse. Our approach uses two consumer-grade webcams to observe the user's hands. We solve the pose estimation problem with efficient queries of a precomputed database that relates hand silhouettes to their 3D configuration. We demonstrate efficient 3D mechanical assembly of several CAD models using our hand-tracking system.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {549–558},
numpages = {10},
keywords = {3D object manipulation, hand tracking, computer aided design},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047270,
author = {Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and Fitzgibbon, Andrew},
title = {KinectFusion: Real-Time 3D Reconstruction and Interaction Using a Moving Depth Camera},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047270},
doi = {10.1145/2047196.2047270},
abstract = {KinectFusion enables a user holding and moving a standard Kinect camera to rapidly create detailed 3D reconstructions of an indoor scene. Only the depth data from Kinect is used to track the 3D pose of the sensor and reconstruct, geometrically precise, 3D models of the physical scene in real-time. The capabilities of KinectFusion, as well as the novel GPU-based pipeline are described in full. Uses of the core system for low-cost handheld scanning, and geometry-aware augmented reality and physics-based interactions are shown. Novel extensions to the core GPU pipeline demonstrate object segmentation and user interaction directly in front of the sensor, without degrading camera tracking or reconstruction. These extensions are used to enable real-time multi-touch interactions anywhere, allowing any planar or non-planar reconstructed physical surface to be appropriated for touch.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {559–568},
numpages = {10},
keywords = {surface reconstruction, physics, 3D, GPU, AR, tracking, depth cameras, geometry-aware interactions},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047271,
author = {Butler, Alex and Hilliges, Otmar and Izadi, Shahram and Hodges, Steve and Molyneaux, David and Kim, David and Kong, Danny},
title = {Vermeer: Direct Interaction with a 360° Viewable 3D Display},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047271},
doi = {10.1145/2047196.2047271},
abstract = {We present Vermeer, a novel interactive 360° viewable 3D display. Like prior systems in this area, Vermeer provides viewpoint-corrected, stereoscopic 3D graphics to simultaneous users, 360° around the display, without the need for eyewear or other user instrumentation. Our goal is to over-come an issue inherent in these prior systems which - typically due to moving parts - restrict interactions to outside the display volume. Our system leverages a known optical illusion to demonstrate, for the first time, how users can reach into and directly touch 3D objects inside the display volume. Vermeer is intended to be a new enabling technology for interaction, and we therefore describe our hardware implementation in full, focusing on the challenges of combining this optical configuration with an existing approach for creating a 360° viewable 3D display. Initially we demonstrate direct involume interaction by sensing user input with a Kinect camera placed above the display. However, by exploiting the properties of the optical configuration, we also demonstrate novel prototypes for fully integrated input sensing alongside simultaneous display. We conclude by discussing limitations, implications for interaction, and ideas for future work.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {569–576},
numpages = {8},
keywords = {optical illusion, tabletop, input sensing, in-volume interaction, 360? viewable 3D display},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047272,
author = {Heo, Seongkook and Han, Jaehyun and Choi, Sangwon and Lee, Seunghwan and Lee, Geehyuk and Lee, Hyong-Euk and Kim, SangHyun and Bang, Won-Chul and Kim, DoKyoon and Kim, ChangYeong},
title = {IrCube Tracker: An Optical 6-DOF Tracker Based on LED Directivity},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047272},
doi = {10.1145/2047196.2047272},
abstract = {Six-degrees-of-freedom (6-DOF) trackers, which were mainly for professional computer applications, are now in demand by everyday consumer applications. With the requirements of consumer electronics in mind, we designed an optical 6-DOF tracker where a few photo-sensors can track the position and orientation of an LED cluster. The operating principle of the tracker is basically source localization by solving an inverse problem. We implemented a prototype system for a TV viewing environment, verified the feasibility of the operating principle, and evaluated the basic performance of the prototype system in terms of accuracy and speed. We also examined its application possibility to different environments, such as a tabletop computer, a tablet computer, and a mobile spatial interaction environment.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {577–586},
numpages = {10},
keywords = {optical 6-DOF tracker, LED directivity, source localization, ircube},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047273,
author = {Hachet, Martin and Bossavit, Benoit and Coh\'{e}, Aur\'{e}lie and de la Rivi\`{e}re, Jean-Baptiste},
title = {Toucheo: Multitouch and Stereo Combined in a Seamless Workspace},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047273},
doi = {10.1145/2047196.2047273},
abstract = {We propose a new system that efficiently combines direct multitouch interaction with co-located 3D stereoscopic visualization. In our approach, users benefit from well-known 2D metaphors and widgets displayed on a monoscopic touchscreen, while visualizing occlusion-free 3D objects floating above the surface at an optically correct distance. Technically, a horizontal semi-transparent mirror is used to reflect 3D images produced by a stereoscopic screen, while the user's hand as well as a multitouch screen located below this mirror remain visible. By registering the 3D virtual space and the physical space, we produce a rich and unified workspace where users benefit simultaneously from the advantages of both direct and indirect interaction, and from 2D and 3D visualizations. A pilot usability study shows that this combination of technology provides a good user experience.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {587–592},
numpages = {6},
keywords = {mutitouch, 3D user interfaces, stereoscopy display},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/3244843,
author = {Wobbrock, Jacob},
title = {Session Details: Pointing},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3244843},
doi = {10.1145/3244843},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047275,
author = {Leitner, Jakob and Haller, Michael},
title = {Harpoon Selection: Efficient Selections for Ungrouped Content on Large Pen-Based Surfaces},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047275},
doi = {10.1145/2047196.2047275},
abstract = {In this paper, we present the Harpoon selection tool, a novel selection technique specifically designed for interactive whiteboards. The tool combines area cursors and crossing to perform complex selections amongst a large number of unsorted, ungrouped items. It is optimized for large-scale pen-based surfaces and works well in both dense and sparse surroundings. We describe a list of key features relevant to the design of the tool and provide a detailed description of both the mechanics as well as the feedback of the tool. The results of a user study are described and analyzed to confirm our design. The study shows that the Harpoon tool performs significantly faster than Tapping and Lassoing.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {593–602},
numpages = {10},
keywords = {crossing, lasso, selection tool, pen input},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047276,
author = {Casiez, G\'{e}ry and Roussel, Nicolas},
title = {No More Bricolage! Methods and Tools to Characterize, Replicate and Compare Pointing Transfer Functions},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047276},
doi = {10.1145/2047196.2047276},
abstract = {Transfer functions are the only pointing facilitation technique actually used in modern graphical interfaces involving the indirect control of an on-screen cursor. But despite their general use, very little is known about them. We present EchoMouse, a device we created to characterize the transfer functions of any system, and libpointing, a toolkit that we developed to replicate and compare the ones used by Windows, OS X and Xorg. We describe these functions and report on an experiment that compared the default one of the three systems. Our results show that these default functions improve performance up to 24% compared to a unitless constant CD gain. We also found significant differences between them, with the one from OS X improving performance for small target widths but reducing its performance up to 9% for larger ones compared to Windows and Xorg. These results notably suggest replacing the constant CD gain function commonly used by HCI researchers by the default function of the considered systems.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {603–614},
numpages = {12},
keywords = {toolkit, pointer acceleration, transfer functions, pointing, CD gain, control-display gain functions},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047277,
author = {Weiss, Malte and Wacharamanotham, Chat and Voelker, Simon and Borchers, Jan},
title = {FingerFlux: Near-Surface Haptic Feedback on Tabletops},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047277},
doi = {10.1145/2047196.2047277},
abstract = {We introduce FingerFlux, an output technique to generate near-surface haptic feedback on interactive tabletops. Our system combines electromagnetic actuation with permanent magnets attached to the user's hand. FingerFlux lets users feel the interface before touching, and can create both attracting and repelling forces. This enables applications such as reducing drifting, adding physical constraints to virtual controls, and guiding the user without visual output. We show that users can feel vibration patterns up to 35 mm above our table, and that FingerFlux can significantly reduce drifting when operating on-screen buttons without looking.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {615–620},
numpages = {6},
keywords = {haptic feedback, actuation, magnets, interactive tabletops},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047278,
author = {Heo, Seongkook and Lee, Geehyuk},
title = {Force Gestures: Augmenting Touch Screen Gestures with Normal and Tangential Forces},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047278},
doi = {10.1145/2047196.2047278},
abstract = {Force gestures are touch screen gestures augmented by the normal and tangential forces on the screen. In order to study the feasibility of the force gestures on a mobile touch screen, we implemented a prototype touch screen device that can sense the normal and tangential forces of a touch gesture on the screen. We also designed two example applications, a web browser and an e-book reader, that utilize the force gestures for their primary actions. We conducted a user study with the prototype and the applications to study the characteristics of the force gestures and the effectiveness of their mapping to the primary actions. In the user study we could also discover interesting usability issues and collect useful user feedback about the force gestures and their mapping to GUI actions.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {621–626},
numpages = {6},
keywords = {finger properties, touch input, force input},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{10.1145/2047196.2047279,
author = {Harrison, Chris and Schwarz, Julia and Hudson, Scott E.},
title = {TapSense: Enhancing Finger Interaction on Touch Surfaces},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047279},
doi = {10.1145/2047196.2047279},
abstract = {We present TapSense, an enhancement to touch interaction that allows conventional surfaces to identify the type of object being used for input. This is achieved by segmenting and classifying sounds resulting from an object's impact. For example, the diverse anatomy of a human finger allows different parts to be recognized including the tip, pad, nail and knuckle - without having to instrument the user. This opens several new and powerful interaction opportunities for touch input, especially in mobile devices, where input is extremely constrained. Our system can also identify different sets of passive tools. We conclude with a comprehensive investigation of classification accuracy and training implications. Results show our proof-of-concept system can support sets with four input types at around 95% accuracy. Small, but useful input sets of two (e.g., pen and finger discrimination) can operate in excess of 99% accuracy.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {627–636},
numpages = {10},
keywords = {collaborative, pens, tools, touchscreen, acoustic classification, input, multi-user, tangibles, finger, interactive surfaces, stylus, tabletop computing},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

