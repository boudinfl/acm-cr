@inproceedings{10.1145/2658779.2661163,
author = {Hara, Kotaro},
title = {Scalable Methods to Collect and Visualize Sidewalk Accessibility Data for People with Mobility Impairments},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661163},
doi = {10.1145/2658779.2661163},
abstract = {Poorly maintained sidewalks pose considerable accessibility challenges for mobility impaired persons; however, there are currently few, if any, mechanisms to determine accessible areas of a city a priori. In this paper, I introduce four threads of research that I will conduct for my Ph.D. thesis aimed at creating new methods and tools to provide unprecedented levels of information on the accessibility of streets and sidewalk. Namely, I will (i) conduct a formative study to better understand accessibility problems, (ii) develop and evaluate scalable map-based data collection methods, (iii) integrate computer vision algorithms to increase the scalability of the methods, and (iv) develop accessible-aware map-based tools that demonstrate the utility of our data (Figure 1 and 6).},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–4},
numpages = {4},
keywords = {computer vision, Google street view, crowdsourcing accessibility, accessible urban navigation},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661164,
author = {Orlosky, Jason},
title = {Depth Based Interaction and Field of View Manipulation for Augmented Reality},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661164},
doi = {10.1145/2658779.2661164},
abstract = {In recent years, the market for portable devices has seen a large increase in the development of head mounted displays. While these displays provide many benefits to users, safety is still a concern. In particular, ensuring that content does not interfere with everyday activities and that users have adequate peripheral vision is very important for situational awareness. In this paper, I address these issues through the use of two novel display prototypes. The first is an optical see-through multi-focal plane display combined with an eye tracking interface. Through eye tracking and knowledge of the focal plane distances, I can calculate whether a user is looking at the environment or at a focal plane in the display. Any distracting text can then be quickly removed so that he or she has a clear view of the environment. The second prototype is a video see-through display which expands a user's environmental view through the use of 238° ultra wide field of view fisheye lenses. Based on the results of several initial evaluations, these new interfaces have the potential help users improve environmental awareness.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {5–8},
numpages = {4},
keywords = {eye tracking, multi-focal plane, augmented reality, fisheye vision., spatial interaction, wide field of view},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661165,
author = {Liu, Can},
title = {Leveraging Physical Human Actions in Large Interaction Spaces},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661165},
doi = {10.1145/2658779.2661165},
abstract = {Large interaction spaces such as wall-size displays allow users to interact not only with their hands, like traditional desktop environment, but also with their whole body by, e.g. walking or moving their head orientation. While this is particularly suitable for tasks where users need to navigate large amounts of data and manipulate them at the same time, we still lack a deep understanding of the advantages of large displays for such tasks. My dissertation begins with a set of studies to understand the benefits and drawbacks of a high-resolution wall-size display vs. a desktop environments. The results show strong benefits of the former due to the flexibility of "physical navigation" involving the whole body when compared with mouse input. From whole-body interaction to human-to-human interaction, my current work seeks to leverage natural human actions to collaborative contexts and to design interaction techniques that detects gestural interactions between users to support collaborative data exchange.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {9–12},
numpages = {4},
keywords = {data manipulation, wall-sized displays, multiuser interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661166,
author = {Afergan, Daniel},
title = {Using Brain-Computer Interfaces for Implicit Input},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661166},
doi = {10.1145/2658779.2661166},
abstract = {Passive brain-computer interfaces, in which implicit input is derived from a user's changing brain activity without conscious effort from the user, may be one of the most promising applications of brain-computer interfaces because they can improve user performance without additional effort on the user's part. I seek to use physiological signals that correlate to particular brain states in order to adapt an interface while the user behaves normally. My research aims to develop strategies to adapt the interface to the user and the user's cognitive state using functional near-infrared spectroscopy (fNIRS), a non-invasive, lightweight brain-sensing technique. While passive brain-computer interfaces are currently being developed and researchers have shown their utility, there has been little effort to develop a framework or hierarchy for adaptation strategies.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–16},
numpages = {4},
keywords = {fNIRS, adaptations, passive brain-computer interfaces, BCI},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661167,
author = {Glassman, Elena L.},
title = {Interacting with Massive Numbers of Student Solutions},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661167},
doi = {10.1145/2658779.2661167},
abstract = {When teaching programming or hardware design, it is pedagogically valuable for students to generate examples of functions, circuits, or system designs. Teachers can be overwhelmed by these types of student submissions when running large residential or recently released massive online courses. The underlying distribution of student solutions submitted in response to a particular assignment may be complex, but the newly available volume of student solutions represents a denser sampling of that distribution. Working with large datasets of students' solutions, I am building systems with user interfaces that allow teachers to explore the variety of their students' correct and incorrect solutions. Forum posts, grading rubrics, and automatic graders can be based on student solution data, and turn massive engineering and computer science classrooms into useful insight and feedback for teachers. In the development process, I hope to describe essential design principles for such systems.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {17–20},
numpages = {4},
keywords = {data mining, programming exercises, MOOCs},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661168,
author = {Lasecki, Walter S.},
title = {Powering Interactive Intelligent Systems with the Crowd},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661168},
doi = {10.1145/2658779.2661168},
abstract = {Creating intelligent systems that are able to recognize a user's behavior, understand unrestricted spoken natural language, complete complex tasks, and respond fluently could change the way computers are used in daily life. But fully-automated intelligent systems are a far-off goal -- currently, machines struggle in many real-world settings because problems can be almost entirely unconstrained and can vary greatly between instances. Human computation has been shown to be effective in many of these settings, but is traditionally applied in an offline, batch-processing fashion. My work focuses on a new model of continuous, real-time crowdsourcing that enables interactive crowd-powered systems.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {21–24},
numpages = {4},
keywords = {intelligent systems, human computation, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661169,
author = {Kulkarni, Chinmay},
title = {Making Distance Matter: Leveraging Scale and Diversity in Massive Online Classes},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661169},
doi = {10.1145/2658779.2661169},
abstract = {The large scale of online classes and the diversity of the students that participate in them can enable new educational systems. This massive scale and diversity can enable always-available systems that help students share diverse ideas, and inspire and learn from each other. We introduce systems for two core educational processes at scale: discussion and assessment. To date, several thousand students in a dozen online classes have used our discussion system. Controlled experiments suggest that participants in more diverse discussions perform better on tests and that discussion improves engagement. Similarly, more than 100,000 students have reviewed peer work for both summative assessment and feedback. Through these systems, we argue that to create new educational experiences at scale, pedagogical strategies and software that leverage scale and diversity must be co-developed. More broadly, we suggest the key to creating new educational experiences online lies in leveraging massive networks of peers.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {25–28},
numpages = {4},
keywords = {MOOC, social computing, discussion, online education, peer assessment},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2661170,
author = {Yao, Lining},
title = {Matter Matters: Offloading Machine Computation to Material Computation for Shape Changing Interfaces},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2661170},
doi = {10.1145/2658779.2661170},
abstract = {This paper introduces material computation to offload computing from machine to material, in the process of creating shape-changing output. It contains the explanation on the mechanism of transformation, the concept of material computation, the summary and analysis of literature research within and beyond the HCI field, the interaction loop integrating material computation, and my own practice in material computation technics and applications.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {29–32},
numpages = {4},
keywords = {material computation, machine computation, hybrid material, shape change output, shape changing interface},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659100,
author = {Goguey, Alix and Casiez, G\'{e}ry and Vogel, Daniel and Chevalier, Fanny and Pietrzak, Thomas and Roussel, Nicolas},
title = {A Three-Step Interaction Pattern for Improving Discoverability in Finger Identification Techniques},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659100},
doi = {10.1145/2658779.2659100},
abstract = {Identifying which fingers are in contact with a multi-touch surface provides a very large input space that can be leveraged for command selection. However, the numerous possibilities enabled by such vast space come at the cost of discoverability. To alleviate this problem, we introduce a three-step interaction pattern inspired by hotkeys that also supports feed-forward. We illustrate this interaction with three applications allowing us to explore and adapt it in different contexts.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–34},
numpages = {2},
keywords = {multi-touch, parameter control, shortcuts, command selection, direct manipulation, finger identification},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659101,
author = {Ono, Makoto and Shizuki, Buntarou and Tanaka, Jiro},
title = {A Rapid Prototyping Toolkit for Touch Sensitive Objects Using Active Acoustic Sensing},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659101},
doi = {10.1145/2658779.2659101},
abstract = {We present a prototyping toolkit for creating touch sensitive prototypes from everyday objects without needing special skills such as code writing or designing circuits. This toolkit consists of an acoustic based touch sensor module that captures the resonant properties of objects, software modules including one that recognizes how an object is touched by using machine learning, and plugins for visual programming environments such as Scratch and Max/MSP. As a result, our toolkit enables users to easily configure the response of touches using a wide variety of visual or audio responses. We believe that our toolkit expands the creativity of a non-specialist, such as children and media artists.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {35–36},
numpages = {2},
keywords = {opensound control, sensors, visual programming, machine learning, tangibles, support vector machine, prototyping, acoustic classification, piezo-electric sensor},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659102,
author = {Denoue, Laurent and Carter, Scott and Cooper, Matthew},
title = {Video Text Retouch: Retouching Text in Videos with Direct Manipulation},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659102},
doi = {10.1145/2658779.2659102},
abstract = {Video Text Retouch is a technique for retouching textual content found in many online videos such as screencasts, recorded presentations and many online e-learning videos. Viewed through our special, HTML5-based player, users can edit in real-time the textual content of the video frames, such as correcting typos or inserting new words between existing characters. Edits are overlaid and tracked at the desired position for as long as the original video content remains similar. We describe the interaction techniques, image processing algorithms and give implementation details of the system.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {37–38},
numpages = {2},
keywords = {web-based systems, direct manipulation, video editing, text editing, html5 video processing, video retouch},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659103,
author = {Tsujii, Takahiro and Koizumi, Naoya and Naemura, Takeshi},
title = {Inkantatory Paper: Dynamically Color-Changing Prints with Multiple Functional Inks},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659103},
doi = {10.1145/2658779.2659103},
abstract = {We propose an effective combination of multiple functional inks, including conductive silver ink, thermo-chromic ink, and regular inkjet ink, for a novel paper-based interface called Inkantatory Paper that can dynamically change the color of its printed pattern. Constructed with off-the-shelf inkjet printing using silver conductive ink, our system enables users to fabricate thin, flat, flexible, and low-cost interactive paper. We evaluated the characteristics of the conductive silver ink as a heating system for the thermo-chromic ink and created applications demonstrating the usability of the system.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–40},
numpages = {2},
keywords = {flexible printed electronics, conductive ink, paper computing, inkjet printing, digital fabrication, prototyping},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659104,
author = {Ando, Masahiro and Itoh, Yuichi and Hosoi, Toshiki and Takashima, Kazuki and Nakajima, Kosuke and Kitamura, Yoshifumi},
title = {StackBlock: Block-Shaped Interface for Flexible Stacking},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659104},
doi = {10.1145/2658779.2659104},
abstract = {We propose a novel building-block interface called StackBlock that allows users to precisely construct 3D shapes by stacking blocks at arbitrary positions and angles. Infrared LEDs and phototransistors are laid in a matrix on each surface of a block to detect the areas contacted by other blocks. Contact-area information is transmitted to the bottom block by the relay of infrared communication between the stacked blocks, and then the bottom block sends all information to the host computer for recognizing the 3D shape. We implemented a prototype of StackBlock with several blocks and evaluated the accuracy and latency of 3D shape recognition. As a result, StackBlock could sufficiently perform 3D shape recognition for users' flexible stacking.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {41–42},
numpages = {2},
keywords = {IR communication, building blocks, tangible},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659105,
author = {Yamaoka, Junichi and Kakehi, Yasuaki},
title = {A Pen-Based Device for Sketching with Multi-Directional Traction Forces},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659105},
doi = {10.1145/2658779.2659105},
abstract = {This paper presents a pen-grip-shaped device that assists in sketching using multi-directional traction forces. By using an asymmetric acceleration of the vibration actuator that drive in a linear direction, the system can create a virtual traction force with the proper direction. We augment users' drawing skills with the device that arranged 4 vibration actuators that provides a traction force and a rotary sensation. Therefore the device is portable and does not have any limitation of needing to be in a particular location, this device can be used to guide the direction and assist the user who is sketching on a large piece of paper. Moreover, users can attach it to any writing utensil such as brushes, crayons. In this paper, we describe the details of the design of device, evaluation experiments, and applications.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–44},
numpages = {2},
keywords = {pen and tactile input, pen-based uis, creativity supporting tools, tactile and haptic uis},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659106,
author = {Jacobson, Alec and Panozzo, Daniele and Glauser, Oliver and Pradalier, Cedric and Hilliges, Otmar and Sorkine-Hornung, Olga},
title = {Tangible and Modular Input Device for Character Articulation},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659106},
doi = {10.1145/2658779.2659106},
abstract = {We present a modular, novel mechanical device for animation authoring. The pose of the device is sensed at interactive rates, enabling quick posing of characters rigged with a skeleton of arbitrary topology. The mapping between the physical device and virtual skeleton is computed semi-automatically guided by sparse user correspondences. Our demonstration allows visitors to experiment with our device and software, choosing from a variety of characters to control.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {45–46},
numpages = {2},
keywords = {character articulation, novel input devices},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659107,
author = {Ranasinghe, Nimesha and Suthokumar, Gajan and Lee, Kuan Yi and Do, Ellen Yi-Luen},
title = {Digital Flavor Interface},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659107},
doi = {10.1145/2658779.2659107},
abstract = {This demo presents a unique technology to enable digital simulation of flavors. The Digital Flavor Interface, a digital control system, is developed to stimulate taste (using electrical and thermal stimulation methodologies on the human tongue) and smell (using a controlled scent emitting mechanism) senses simultaneously, thus simulating different virtual flavors. A preliminary user experiment was conducted to investigate the effectiveness of this approach with five distinct flavor stimuli. The experimental results suggested that the users' were effectively able to identify different flavors such as minty, spicy, and lemon flavor. In summary, our work demonstrates a novel controllable digital flavor instrument, which may be utilized in interactive computer systems for rendering virtual flavors.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {47–48},
numpages = {2},
keywords = {smell, flavor, virtual reality, virtual flavor, taste},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659108,
author = {Hurter, Christophe and Taylor, A. Russel and Carpendale, Sheelagh and Telea, Alexandru},
title = {Interactive Exploration and Selection in Volumetric Datasets with Color Tunneling},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659108},
doi = {10.1145/2658779.2659108},
abstract = {Interactive data exploration and manipulation are often hindered by dataset sizes. For 3D data, this is aggravated by occlusion, important adjacencies, and entangled patterns. Such challenges make visual interaction via common filtering techniques hard. We describe a set of real-time multi-dimensional data deformation techniques that aim to help users to easily select, analyze, and eliminate spatial and data patterns. Our techniques allow animation between view configurations, semantic filtering and view deformation. Any data subset can be selected at any step along the animation. Data can be filtered and deformed to reduce occlusion and ease complex data selections. Our techniques are simple to learn and implement, flexible, and real-time interactive with datasets of tens of millions of data points. We demonstrate our techniques on three domain areas: 2D image segmentation and manipulation, 3D medical volume exploration, and astrophysical exploration.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {49–50},
numpages = {2},
keywords = {GPU techniques, data cube visualization, data exploration},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659109,
author = {Zhao, Siyan and Schneider, Oliver and Klatzky, Roberta and Lehman, Jill and Israr, Ali},
title = {FeelCraft: Crafting Tactile Experiences for Media Using a Feel Effect Library},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659109},
doi = {10.1145/2658779.2659109},
abstract = {FeelCraft is a media plugin that monitors events and states in the media and associates them with expressive tactile content using a library of feel effects (FEs). A feel effect (FE) is a user-defined haptic pattern that, by virtue of its connection to a meaningful event, generates dynamic and expressive effects on the user's body. We compiled a library of more than fifty FEs associated with common events in games, movies, storybooks, etc., and used them in a sandbox-type gaming platform. The FeelCraft plugin allows a game designer to quickly generate haptic effects, associate them to events in the game, play them back for testing, save them and/or broadcast them to other users to feel the same haptic experience. Our demonstration shows an interactive procedure for authoring haptic media content using the FE library, playing it back during interactions in the game, and broadcasting it to a group of guests.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {51–52},
numpages = {2},
keywords = {feel effect, haptic authoring tool, haptic gaming experience, haptic vocabulary},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659110,
author = {Kim, Jeeeun and Kasper, Mike and Yeh, Tom and Correll, Nikolaus},
title = {SikuliBot: Automating Physical Interface Using Images},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659110},
doi = {10.1145/2658779.2659110},
abstract = {We present SikuliBot, an image-based approach to automating user interface. SikuliBot extends the visual programming concept of Sikuli Script[2] from the graphical UIs to the real world of physical UIs, such as mobile devices' touch-screens and hardware buttons. The key to our approach is using a physical robot to see an interface, identify a target, and perform an action on the target using the robot's actuators. We demonstrate working examples on MakerBot 3D printer that could move a stylus to perform multi-touch gestures on touchscreen to automate tasks such as swipe-to unlock, playing a virtual piano, and playing the Angry Bird game. A wide range of automation possibilities are made viable using a simple scripting language based on images of UI components. The benefits of our approach are: generalizability, instrumentation-free, and high-level programming abstraction.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {53–54},
numpages = {2},
keywords = {visual programming, automation, robotics, tangible user interfaces},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659111,
author = {Leigh, Sang-won and Schoessler, Philipp and Heibeck, Felix and Maes, Pattie and Ishii, Hiroshi},
title = {THAW: Tangible Interaction with See-through Augmentation for Smartphones on Computer Screens},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659111},
doi = {10.1145/2658779.2659111},
abstract = {In this paper, we present a novel interaction system that allows a collocated large display and small handheld devices to seamlessly work together. The smartphone acts both as a physical interface and as an additional graphics layer for near-surface interaction on a computer screen. Our system enables accurate position tracking of a smartphone placed on or over any screen by displaying a 2D color pattern that is captured using the smartphone's back-facing camera. The proposed technique can be implemented on existing devices without the need for additional hardware.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–56},
numpages = {2},
keywords = {tangible magic lens, multi-device interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659113,
author = {Akiyama, Yoh and Miyashita, Homei},
title = {Projectron Mapping: The Exercise and Extension of Augmented Workspaces for Learning Electronic Modeling through Projection Mapping},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659113},
doi = {10.1145/2658779.2659113},
abstract = {There has been research using software simulations to support the learning of electronic modeling by beginners. There have also been systems to extend workspaces and support electronic modeling on tabletop interfaces. However, in the case of software-based circuit operation, as it is not possible to operate the actual elements, the feeling of actually moving the elements is lacking. For this reason, we are proposing a system that extends the sense of reality in software simulators through the use of projection mapping. This will make it possible to actually give the impression of moving the elements by using a software simulator, and to achieve both high speed and a sense of reality through trial and error.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {57–58},
numpages = {2},
keywords = {tangible user interface, pseudo lighting, understanding electricity, projection mapping, electronics},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2668032,
author = {Kato, Kunihiro and Miyashita, Homei},
title = {Extension Sticker: A Method for Transferring External Touch Input Using a Striped Pattern Sticker},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2668032},
doi = {10.1145/2658779.2668032},
abstract = {A method for transferring external touch input is proposed by partially attaching a sticker to a touch-panel display. The touch input area can be extended by printing striped patterns using a conductive ink and attaching them to overlap with a portion of a touch-panel display. Even if the user does not touch the touch panel directly, a touch event can be generated by touching the stripes at an arbitrary point corresponding to the touched area. Thus, continuous touch input can be generated, such as a scrolling operation without interruption. This method can be applied to a variety of devices including PCs, smartphones, and wearable devices. In this paper, we present several different examples of applications, including a method for extending control areas outside of the touch panel, such as the side or back of a smartphone.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {59–60},
numpages = {2},
keywords = {striped pattern sticker, capacitive touch panel, continuous touch input, conductive ink},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659114,
author = {Kasahara, Shunichi and Nagai, Shohei and Rekimoto, Jun},
title = {LiveSphere: Immersive Experience Sharing with 360 Degrees Head-Mounted Cameras},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659114},
doi = {10.1145/2658779.2659114},
abstract = {Sharing full immersive experience in real-time has been the one of ultimate goals of telecommunication. Possible application can include various applications such as entertainment, sports viewing, education, social network and professional assistance. Recent head-worn wearable camera enables to shoot the first person video, however, view of angle is limited with the head direction of the person who is wearing, and also captured video is shaky that makes us dizzy. We propose LiveSphere, immersive experience sharing system with wearable camera headgear that provide 360 degrees spherical images of the user's surrounding environment. LiveSphere system performs spherical video stabilization and transmits it to other users, so that they are enable to view shared video comfortably and also look around at the scene from a different view angle independently from the first person. In this note, we explain the overview of the LiveSphere system implementation, stabilization and viewing experience.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {61–62},
numpages = {2},
keywords = {wearable computer, 360 degrees spherical image, first person view streaming},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659115,
author = {Goyal, Pragun and Paradiso, Joseph and Maes, Pattie},
title = {Nishanchi: CAD for Hand-Fabrication},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659115},
doi = {10.1145/2658779.2659115},
abstract = {We present Nishanchi, a position and orientation aware handheld inkjet printer which can be used to transfer the reference marks from CAD to the workpiece for use in manual fabrication workflows. Nishanchi also has a digitizing tip that can be used to input features about the workpiece to a computer model. By allowing for this two-way exchange of information from CAD to a nonconcormal workpiece, we believe that Nishanchi might help make inclusion of CAD in manual fabrication workflows more seamless.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {63–64},
numpages = {2},
keywords = {printing, digital fabrication, computer-aided design (CAD)},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659116,
author = {Ziat, Mounia and Rolison, Taylor and Shirtz, Andrew and Wilbern, Daniel and Balcer, Carrie Anne},
title = {Enhancing Virtual Immersion through Tactile Feedback},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659116},
doi = {10.1145/2658779.2659116},
abstract = {The lack of tangibility while interacting with virtual objects can be compensated by adding haptic and/or tactile devices or actuators to enhance the user experience. In this demonstration, we present two scenarios that consist of perceiving moving objects on the human body (insects) and feeling physical sensations of virtual thermal objects.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {65–66},
numpages = {2},
keywords = {oculus rift, tactile feedback, temperature perception},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659117,
author = {Ishiguro, Yoshio and Brockmeyer, Eric and Rothera, Alex and Israr, Ali},
title = {Ubisonus: Spatial Freeform Interactive Speakers},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659117},
doi = {10.1145/2658779.2659117},
abstract = {We present freeform interactive speakers for creating spatial sound experiences from a variety of surfaces. Traditional surround sound systems are widely used and consist of multiple electromagnetic speakers that create point sound sources within a space. Our proposed system creates directional sound and can be easily embedded into architecture, furniture and many everyday objects. We use electrostatic loudspeaker technology made from thin, flexible, lightweight and low cost materials and can be of different size and shape. In this demonstration we will show various configurations such as single speaker, speaker array and tangible speakers for playful and exciting interactions with spatial sounds. This is an example of new possibilities for the design of various interactive surfaces.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {67–68},
numpages = {2},
keywords = {interactions, furniture design, electrostatic loudspeakers, interactive architecture, freeform speakers},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658780,
author = {Goh, Taedong and Kim, Sang Woo},
title = {Eyes-Free Text Entry Interface Based on Contact Area for People with Visual Impairment},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658780},
doi = {10.1145/2658779.2658780},
abstract = {We developed an eyes-free text entry interface using contact area to determine pressed state for mobile device with touchscreen. The interface gives audio feedback for a touched character similar to VoiceOver of iPhone, but audio feedbacks of two simultaneous touches are considered. A desired character is entered by pressing once. Independent entry of two fingers can reduce movement distance for searching a character. Whole interaction occurs in touched states, additional tactile feedback can be augmented.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {69–70},
numpages = {2},
keywords = {pseudo-pressure detection, touchscreen interface, text-to-speech},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658781,
author = {Heun, Valentin and Friedman, Kenneth and Mendez, Andrew and Reynolds, Benjamin and Wong, Kevin and Maes, Pattie},
title = {Third Surface: An Augmented World Wide Web for the Physical World},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658781},
doi = {10.1145/2658779.2658781},
abstract = {The ubiquitous use of Augmented Reality (AR) applications is dependent on an easy way of authoring and using content. Present systems depend on specific authoring tools or content delivery systems that provide a limited amount of freedom and content ownership to the author compared to the possibilities of the World Wide Web (WWW). Third Surface is a system that allows the user to publish and use WWW content saved on personal HTTP servers for augmented reality applications in the physical environment. The contribution of this work is a system that allows a web developer to post location-based augmented reality content and AR marker on one's own HTTP server. A Global Location Service (GLS) provides a browsing application with location-based URLs that link the browsing application to content, AR markers, and data for the right positioning of content in the augmented reality interface. The Third Surface has three advantages compared to other concepts. It is globally scalable able to millions of users. The interactive possibilities for developers and users are the same as for the WWW. The developers are in charge of their own content distribution.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {71–72},
numpages = {2},
keywords = {augmented reality, web, user interface},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658782,
author = {Ogawa, Daichi and Ikeno, Sakiko and Okazaki, Ryuta and Hachisu, Taku and Kajimoto, Hiroyuki},
title = {Tactile Cue Presentation for Vocabulary Learning with Keyboard},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658782},
doi = {10.1145/2658779.2658782},
abstract = {This paper presents the results of a pilot experiment observ-ing the effect of tactile cues on vocabulary learning. Con-sidering that we generally memorize words by associating them with various cues, we designed a tactile cue presentation device that aids vocabulary learning by applying vibra-tions to the finger that is associated with the next key to press when typing on a keyboard. Experiments comparing tactile and visual cues indicated that tactile cues can signifi-cantly improve long-term retention of vocabulary after one week.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {73–74},
numpages = {2},
keywords = {vocabulary learning, haptic, wearable device, tactile cue},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658783,
author = {Wang, Guanyun and Tao, Ye and Yu, Dian and Cao, Chuan and Chen, Hongyu and Yao, Cheng},
title = {Trainer: A Motion-Based Interactive Game for Balance Rehabilitation Training},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658783},
doi = {10.1145/2658779.2658783},
abstract = {In physiotherapy, the traditional approach of using fixed aids to train patients to keep their balance is often ineffective, due to the tendency of people to lose interest in the training or to lose confidence in their ability to finish the training. A Trainer system is proposed on traditional physiotherapy treatment methods to allow patients to play qualified and immersive games with a mobile aid. Using RF localization and self-balancing technology, the system allows patients to control a vehicle with their sense of balance. This platform provides a series of game feedback interface which involves part-body motion in sitting manipulation therapy to make the rehabilitation more flexible and more effective. This paper reports the designing and the control of the Trainer, the experimental evaluations of the performance of system, as well as an exploration of the future work in detail. Our work is intended to improve the patient experience of the physiotherapy rehabilitation using games with instinctive ways of controlling mobile instruments.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–76},
numpages = {2},
keywords = {video game, game-based rehabilitation, habitual actions, balance training},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658784,
author = {Hernandez, Javier and Picard, Rosalind W.},
title = {SenseGlass: Using Google Glass to Sense Daily Emotions},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658784},
doi = {10.1145/2658779.2658784},
abstract = {For over a century, scientists have studied human emotions in laboratory settings. However, these emotions have been largely contrived -- elicited by movies or fake "lab" stimuli, which tend not to matter to the participants in the studies, at least not compared with events in their real life. This work explores the utility of Google Glass, a head-mounted wearable device, to enable fundamental advances in the creation of affect-based user interfaces in natural settings.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {77–78},
numpages = {2},
keywords = {wearable devices, emotions, real-life settings, affective computing},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658785,
author = {Cho, Hyeonjoong and Kim, Miso and Seo, Kyeongeun},
title = {A Text Entry Technique for Wrist-Worn Watches with Tiny Touchscreens},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658785},
doi = {10.1145/2658779.2658785},
abstract = {We consider a text entry technique for wrist-worn watches with inch-scale touchscreens. Most of the watches which are commercially available, for example, Galaxy Gear, Omate, etc., have around 1.5-inch touchscreens that is too small for the shrinked Qwerty keyboard. Moreover, the virtual button-based techniques determine input-letters by distinguishing touched locations on touchscreens which continuously demands a user to carefully touch certain locations. Thus, they are not suitable to tiny-touchscreen devices in mobile environment. Instead, the proposed text entry technique allows a user to touch almost anywhere on the touchscreen for text entry by determining input-letters based on drag direction regardless of touched location. We implemented the proposed method on a commercial watch with 1.54-inch touchscreen for validating its feasibility.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {79–80},
numpages = {2},
keywords = {wrist-worn watch, smart watch, text entry, touchscreen},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658786,
author = {Mueller, Florian and Muirhead, Matthew},
title = {Understanding the Design of a Flying Jogging Companion},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658786},
doi = {10.1145/2658779.2658786},
abstract = {Jogging can offer many health benefits, and mobile phone apps have recently emerged that aim to support the jogging experience. We believe that jogging is an embodied experience, and therefore present a contrasting approach to these existing systems by arguing that any supporting technology should also take on an embodied approach. In order to exemplify this approach, we detail the technical specifications of a flying quadcopter that has successfully been used with joggers in order to explore the design of embodied systems to support physical exertion activities. Based on interviews with five joggers running with our system, we present preliminary insights about the experience of jogging with a flying robot. With our work, we hope to inspire and guide designers who are interested in developing embodied systems to support exertion activities.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {81–82},
numpages = {2},
keywords = {whole-body interaction, robot, exertion, exergames, sports, quadcopter, running, multirotor, drone, jogging},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658787,
author = {Seo, Kyeongeun and Cho, Hyeonjoong},
title = {AirPincher: A Handheld Device for Recognizing Delicate Mid-Air Hand Gestures},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658787},
doi = {10.1145/2658779.2658787},
abstract = {We propose AirPincher, a handheld device for recognizing delicate mid-air hand gestures. AirPincher is designed to overcome disadvantages of the two kinds of existing hand gesture-aware techniques such as wearable sensor-based and external vision-based. The wearable sensor-based techniques cause cumbersomeness of wearing sensors every time and the external vision-based techniques incur performance dependence on distance between a user and a remote display. AirPincher allows a user to hold the device in one hand and to generate several delicate mid-air finger gestures. The gestures are captured by several sensors proximately embedded into AirPincher. These features help AirPincher avoid the aforementioned disadvantages of the existing techniques. It allows several delicate finger gestures, for example, rubbing a thumb against a middle finger, swiping with a thumb on an index finger, pinching with a thumb and an index finger, etc. Due to the inherent haptic feedback of these gestures, AirPincher eventually supports the eyes-free interaction. To validate AirPincher's feasibility, we implemented two use cases, i.e., controlling a pointing cursor and moving a virtual 3D object on the remote screen.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {83–84},
numpages = {2},
keywords = {hand gesture recognition, hand-held device},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658788,
author = {Rhee, Taik Heon and Byeon, Kwangmin and Shin, Hochul},
title = {Contelli: A User-Controllable Intelligent Keyboard for Watch-Sized Small Touchscreens},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658788},
doi = {10.1145/2658779.2658788},
abstract = {Intelligent keyboards aid fast text entry by correcting user's erroneous input, but there is a big problem that a user always has to watch and judge of their suggestion results. Contelli, a user-controllable intelligent keyboard, monitors the duration of each key-tapping, and analyzes the possibility of mis-typing only for short-tapped letters. A long-tapped letter is regarded as a precise input and excluded in the process of candidate generation from a lexicon. Using Contelli, a user may actively "control" the intelligent keyboards. S/he may type ordinary words quickly on watch-sized small touchscreens. Also, s/he may input a word as typed without switching off the automatic replacement or performing additional actions for the replaced result. In addition, long-tapping a part of a string reduces the number of replacement candidates, which contributes the more precise word replacement for highly erroneous input typed on small touchscreens.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {85–86},
numpages = {2},
keywords = {auto-correction, intelligent keyboards, user-controllable intelligent keyboard, automatic word replacement, contelli},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658789,
author = {Sim, Jungu and Kim, Chang-Min and Nam, Seung-Woo and Nam, Tek-Jin},
title = {G-Raffe: An Elevating Tangible Block Supporting 2.5D Interaction in a Tabletop Computing Environment},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658789},
doi = {10.1145/2658779.2658789},
abstract = {We present an elevating tangible block, G-raffe, supporting 2.5-dimensional (2.5-D) interaction in a tabletop computing environment. There is a lack of specialized interface devices for tabletop computing environments. G-raffe overcomes the limitation of conventional 2-D interactions inherited from the vertical desktop computing setting. We adopted a rollable metal tape structure to create up and down movements in a small volume of the block. This also becomes a connecting device for a mobile display to be used with the tabletop computer. We report on our design rationale as well as the results of a preliminary user study.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {87–88},
numpages = {2},
keywords = {interface devices, tangible interface, spatial interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658790,
author = {Shibata, Tomoki and Peck, Evan M. and Afergan, Daniel and Hincks, Samuel W. and Yuksel, Beste F. and Jacob, Robert J.K.},
title = {Building Implicit Interfaces for Wearable Computers with Physiological Inputs: Zero Shutter Camera and Phylter},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658790},
doi = {10.1145/2658779.2658790},
abstract = {We propose implicit interfaces that use passive physiological input as additional communication channels between wearable devices and wearers. A defining characteristic of physiological input is that it is implicit and continuous, distinguishing it from conventional event-driven action on a keyboard, for example, which is explicit and discrete. By considering the fundamental differences between the two types of inputs, we introduce a core framework to support building implicit interface, such that the framework follows the three key principles: Subscription, Accumulation, and Interpretation of implicit inputs. Unlike a conventional event driven system, our framework subscribes to continuous streams of input data, accumulates the data in a buffer, and subsequently attempts to recognize patterns in the accumulated data -- upon request from the application, rather than directly in response to the input events. Finally, in order to embody the impacts of implicit interfaces in the real world, we introduce two prototype applications for Google Glass, Zero Shutter Camera triggering a camera snapshot and Phylter filtering notifications the both leverage the wearer's physiological state information.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {89–90},
numpages = {2},
keywords = {Google glass, BCI, wearable computing, implicit interface, fnirs, brain-computer interface},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658791,
author = {Yokoyama, Masanori and Matsuda, Masafumi and Muto, Shinyo and Kanamaru, Naoyoshi},
title = {PoliTel: Mobile Remote Presence System That Autonomously Adjusts the Interpersonal Distance},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658791},
doi = {10.1145/2658779.2658791},
abstract = {Mobile Remote Presence (MRP) system that uses a smart device such as smartphone and tablet pc as video conferencing equipment is getting popular. There are varieties of smart devices, and the appearance of a smart device varies from one to another. We assumed that the appropriate interpersonal distance for an MRP system varies depending on the appearance of the smart device. To confirm our assumption, we conducted a preliminary experiment. The result of the experiment suggested that the value of the proper interpersonal distance increases as the video size increases. It is known that the task load of the remote operator of the MRP system increases if the operator is forced to manually control the MRP system to keep the interpersonal distance to the appropriate level, which adversely affects the quality of the communication through MRP. To resolve the problem, we propose PoliTel, a novel MRP system which autonomously adjusts the interpersonal distance according to the appearance of the smart device by controlling the position or video size of MRP, and allows the operator to concentrate more on the conversation with the person facing to the MRP system.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {91–92},
numpages = {2},
keywords = {semi-autonomy, human robot interaction, proxemics, interpersonal distance, telepresence},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658792,
author = {Feng, Jinbo and Wartell, Zachary},
title = {Riding the Plane: Bimanual, Desktop 3D Manipulation},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658792},
doi = {10.1145/2658779.2658792},
abstract = {A bimanual 7 Degree of Freedom (DOF) manipulation technique based on a hybrid 3D cursor driven by the combination of mouse and trackball is presented. This technique allows the user to move the cursor to the target location in 3D scene by following a conceived straight or curved path. In the pilot study, participants could learn the technique in a short time and perform the docking task steadily without physical fatigue.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–94},
numpages = {2},
keywords = {stereoscopic virtual environment, 2D input device, 3D manipulation, docking test, 7 degree of freedom},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658793,
author = {Umezu, Shuhei and Ohkubo, Masaru and Ooide, Yoshiharu and Nojima, Takuya},
title = {Hairlytop Interface: A Basic Tool for Active Interfacing},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658793},
doi = {10.1145/2658779.2658793},
abstract = {The Hairlytop Interface is a high scalability interface composed of hair-like units called smart hairs. The original version of the smart hair comprised a shape-memory alloy, drive circuits, and a light sensor. Simply placing the smart hair above a light display device enabled each smart hair to be bent and controlled by modulating the intensity of light from the display. Various prototypes of the Hairlytop Interface have been created to show its high flexibility in configuration. This flexibility should help users to develop their own moving interfaces.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {95–96},
numpages = {2},
keywords = {haptic, smart material interface, soft actuator, hairlytop interface, shape-memory alloy, surface display, smart hair},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658794,
author = {Kao, Chen-Tai and Liu, Yen-Ting and Hsu, Alexander},
title = {Speeda: Adaptive Speed-up for Lecture Videos},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658794},
doi = {10.1145/2658779.2658794},
abstract = {Increasing the playback speed of lecture videos is a common technique to shorten watching time. This creates challenges when part of the lecture becomes too fast to be discernible, even if the overall playback speed is acceptable. In this paper, we present a speed-up system that preserves lecture clearness in high playback rate. A user test was conducted to evaluate the system. The result indicates that our system significantly improves user's comprehension level.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {97–98},
numpages = {2},
keywords = {MOOC, speed, playback},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658795,
author = {Embiricos, Alex and Rahmati, Negar and Zhu, Nicole and Bernstein, Michael S.},
title = {Structured Handoffs in Expert Crowdsourcing Improve Communication and Work Output},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658795},
doi = {10.1145/2658779.2658795},
abstract = {Expert crowdsourcing allows specialized, remote teams to complete projects, often large and involving multiple stages. Its execution is complicated due to communication difficulties between remote workers. This paper investigates whether structured handoff methods, from one worker to the next, improve final product quality by helping the workers understand the input of their tasks and reduce overall integration cost. We investigate this question through 1) a "live" handoff method where the next worker shadows the former via screen sharing technology and 2) a "recorded" handoff, where workers summarize work done for the next, via a screen capture and narration. We confirm the need for a handoff process. We conclude that structured handoffs result in higher quality work, improved satisfaction (especially for workers with creative tasks), improved communication of non-obvious instructions, and increased adherence to the original intent of the project.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–100},
numpages = {2},
keywords = {expert crowdsourcing, CSCW, crowdsourcing},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658796,
author = {Cochran, Zane and Tomlinson, Brianna and Chen, Dar-Wei and Patel, Kunal},
title = {LightWeight: Wearable Resistance Visualizer for Rehabilitation},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658796},
doi = {10.1145/2658779.2658796},
abstract = {People recovering from arm injuries are often prescribed limits to the amount of strain they can place on their muscles at a given point during the recovery process. However, it is sometimes difficult for them to know when a given activity creates strain in excess of these limits. To inform this process, we have developed a prototype, the LightWeight, and describe it here. The aim of the LightWeight is to inform users of the strain on targeted muscles as the activity occurs, and to display the relationship of that strain to the aforementioned limits. LightWeight is embedded within a compression sleeve that measures muscle strain through conductive fabric and EMG while displaying that information through an intuitive circular LED display.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {101–102},
numpages = {2},
keywords = {wearable technology, human factors, rehabilitation, user interface},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658797,
author = {Han, Jaehyun and Ahn, Sunggeun and Lee, Geehyuk},
title = {Push-Push: A Two-Point Touchscreen Operation Utilizing the Pressed State and the Hover State},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658797},
doi = {10.1145/2658779.2658797},
abstract = {A drag operation is used for many two-point functions in mouse-based graphical user interfaces (GUIs), but its usage in touchscreen GUIs is limited because it is mainly used for scrolling. We propose Push-Push as a second two-point touchscreen operation that is not in conflict with a drag operation. We implemented three application scenarios and showed how Push-Push can be used effectively for other two-point functions while overlapping drag operations are used for scrolling.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {103–104},
numpages = {2},
keywords = {hover, two point operation, pressure, touch interface},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658798,
author = {Youn, Eunhye and Lee, Geehyuk},
title = {Slack-Scroll: Sharing Sliding Operations among Scrolling and Other GUI Functions},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658798},
doi = {10.1145/2658779.2658798},
abstract = {Sliding is one of the basic touchscreen operations, but is mainly used for scrolling in mobile touchscreen GUIs. As a way to share sliding operations among scrolling and other GUI functions, we propose Slack-Scroll. We implemented two application scenarios of Slack-Scroll, and asserted their feasibility in a user study. All participants could accept and adapt well to the new techniques enabled by Slack-Scroll.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {105–106},
numpages = {2},
keywords = {touchscreen interaction, dragging, scrolling, slack-scroll},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658799,
author = {Tung, Ying-Chao and Cheng, Ta-Yang and Yu, Neng-Hao and Chen, Mike Y.},
title = {FlickBoard: Enabling Trackpad Interaction with Automatic Mode Switching on a Capacitive-Sensing Keyboard},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658799},
doi = {10.1145/2658779.2658799},
abstract = {We present FlickBoard, which combines a trackpad and a keyboard into the same interaction area to reduce hand movement between separate keyboards and trackpads. It supports automatic input mode detection and switching (ie. trackpad vs keyboard mode) without explicit user input. We developed a prototype by embedding a 58x20 capacitive sensing grid into a soft keyboard cover, and uses machine learning to distinguish between moving a cursor (trackpad mode) and entering text (keyboard mode). Our prototype has a thin profile and can be placed over existing keyboards.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {107–108},
numpages = {2},
keywords = {touchpad, keyboard, co-located input devices},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658800,
author = {Tavakolizadeh, Farshid and Gu, Jiawei and Saket, Bahador},
title = {Traceband: Locating Missing Items by Visual Remembrance},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658800},
doi = {10.1145/2658779.2658800},
abstract = {Finding missing items has always been troublesome. To tackle the hassle, several systems have been suggested; yet they are inflexible due to excessive setup time, operational cost, and effectiveness. We present Traceband; a lightweight and portable bracelet, which keeps track of every targeted commonly used object that a user interacts with. Users can find the location of missing items via a web-based software portal.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {109–110},
numpages = {2},
keywords = {finding missing items, bracelet, image matching, life logging, traceband},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658801,
author = {Nakai, Yuriko and kudo, Shinya and Okazaki, Ryuta and Kajimoto, Hiroyuki},
title = {Tangential Force Input for Touch Panels Using Bezel-Aligned Elastic Pillars and a Transparent Sheet},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658801},
doi = {10.1145/2658779.2658801},
abstract = {This research aims to enable tangential force input for touch panels by measuring the tangential force. The system is composed of a plastic sheet on a touch panel, urethane pillars on the panel that are aligned at the four corners of the bezel, and a case on top of the pillars. When the sheet moves with a finger, the pillars deform so that a tangential force can be obtained by measuring the movement of the finger. We evaluated the method and found that the system showed realistic force sensing accuracy in any direction. This input method will enable development of new applications for touch panels such as using any part of the touch panel surface as joysticks, or modeling virtual objects by deforming them with the fingers.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {111–112},
numpages = {2},
keywords = {bezel, shear deformation, tangential force, elastic pillar, touch panel},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658802,
author = {Speiginer, Gheric and MacIntyre, Blair},
title = {Ethereal: A Toolkit for Spatially Adaptive Augmented Reality Content},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658802},
doi = {10.1145/2658779.2658802},
abstract = {In this poster, we describe a framework and toolkit (Ethereal) for creating spatially adaptive content based on complex spatial and visual metrics in augmented reality, and demonstrate our approach with an illustrative example.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–114},
numpages = {2},
keywords = {UI toolkits, web-based augmented reality, augmented reality, adaptive information presentation},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658803,
author = {Chang, Youli and L'Yi, Sehi and Seo, Jinwook},
title = {Reaching Targets on Discomfort Region Using Tilting Gesture},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658803},
doi = {10.1145/2658779.2658803},
abstract = {We present three novel methods to facilitate one hand targeting at discomfort regions on a mobile touch screen using tilting gestures; TiltSlide, TiltReduction, and TiltCursor. We conducted a controlled user study to evaluate them in terms of their performance and user preferences by comparing them with other related methods, i.e. ThumbSpace, Edge Triggered with Extendible Cursor (ETEC), and Direct Touch (directly touching with a thumb). All three methods showed better performance than ThumbSpace in terms of speed and accuracy. Moreover, TiltReduction led users to require less thumb/grip movement than Direct Touch while showing comparable performance in speed and accuracy.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {115–116},
numpages = {2},
keywords = {touch screen, one-handed interaction, tilting gesture},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658804,
author = {Yao, Lining and Ou, Jifei and Tauber, Daniel and Ishii, Hiroshi},
title = {Integrating Optical Waveguides for Display and Sensing on Pneumatic Soft Shape Changing Interfaces},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658804},
doi = {10.1145/2658779.2658804},
abstract = {We introduce the design and fabrication process of integrating optical fiber into pneumatically driven soft composite shape changing interfaces. Embedded optical waveguides can provide both sensing and illumination, and add one more building block to the design of designing soft pneumatic shape changing interfaces.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {117–118},
numpages = {2},
keywords = {optical fiber, soft mechanics, display, shape changing, optical sensing},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658805,
author = {Bernstein, Gilbert Louis and Klemmer, Scott},
title = {Towards Responsive Retargeting of Existing Websites},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658805},
doi = {10.1145/2658779.2658805},
abstract = {Websites need to be displayed on a panoply of different devices today, but most websites are designed with fixed widths only appropriate to browsers on workstation computers. We propose to programmatically rewrite websites into responsive formats capable of adapting to different device display sizes. To accomplish this goal, we cast retargeting as a cross-compilation problem. We decompose existing HTML pages into boxes (lexing), infer hierarchical structure between these boxes (parsing) and finally generate parameterized layouts from the hierarchical structure (code generation). This document describes preliminary work on ReMorph, a prototype 'retargeting as cross-compilation' system.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {119–120},
numpages = {2},
keywords = {document layout, webpages, retargeting, responsive design},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658806,
author = {Ou, Jifei and Yao, Lining and Della Silva, Clark and Wang, Wen and Ishii, Hiroshi},
title = {BioPrint: An Automatic Deposition System for Bacteria Spore Actuators},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658806},
doi = {10.1145/2658779.2658806},
abstract = {We propose an automatic deposition method of bacteria spores, which deform thin soft materials under environmental humidity change. We describe the process of two-dimensional printing the spore solution as well as a design application. This research intends to contribute to the understanding of the control and pre-programming the transformation of future interfaces.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {121–122},
numpages = {2},
keywords = {bio-actuators, bio-hybrid actuator, programmable material, shape changing interfaces, living organism, hygromorph},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658807,
author = {Pels, Trevor and Kao, Christina and Goel, Saguna},
title = {FatBelt: Motivating Behavior Change through Isomorphic Feedback},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658807},
doi = {10.1145/2658779.2658807},
abstract = {The ultimate problem of systems facilitating long-term health and fitness goals is the disconnect between an action and its eventual consequence. As the long-term effects of behavior change are not immediately apparent, it can be hard to motivate the desired behavior over a long period of time. As such, we introduce a system that uses physical feedback through a wearable device that inflates around the stomach as a response to calorie overconsumption, simulating the long-term weight-gain associated with over-eating. We tested a version of this system with 12 users over a period of 2 days, and found a significant decrease in consumption over a baseline period of the same length, suggesting that through physical response, FatBelt moved calorie intake drastically closer to participants' goals. Interviews with participants indicate that isomorphism to the long-term consequences was a large factor in the system's efficacy. In addition, the wearable, physical feedback was perceived as an extension of the user's body, an effect with great emotional consequences.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {123–124},
numpages = {2},
keywords = {physical feedback, health and fitness, wearable computing},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658808,
author = {Kim, Ju-Whan and Nam, Tek-Jin},
title = {M-Gesture: Geometric Gesture Authoring Framework for Multi-Device Gestures Using Wearable Devices},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658808},
doi = {10.1145/2658779.2658808},
abstract = {Wearable devices and mobile devices have great potential to detect various body motions as they are attached to different body parts. We present M-Gesture, a geometric gesture authoring framework using multiple wearable devices. We implemented physical metaphor, geometric gesture language, and continuity in spatial layout for easy and clear gesture authoring. M-Gesture demonstrates the use of geometric notation as an intuitive gesture language.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {125–126},
numpages = {2},
keywords = {geometric notation, multi device gesture authoring},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2659765,
author = {Grote, Casey and Segreto, Evan and Okerlund, Johanna and Kincaid, Robert and Shaer, Orit},
title = {Eugenie: Gestural and Tangible Interaction with Active Tokens for Bio-Design},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2659765},
doi = {10.1145/2658779.2659765},
abstract = {We present a case study of a tangible user interface that implements novel interaction techniques for the construction of complex queries in large data sets. Our interface, Eugenie, utilizes gestural interaction with active physical tokens and a multi-touch interactive surface to aid in the collaborative design process of synthetic biological circuits. We developed new interaction techniques for navigating large hierarchical data sets and for exploring a combinatorial design space. The goal of this research is to study the effect of gestural and tangible interaction with active tokens on sense-making throughout the bio-design process.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {127–128},
numpages = {2},
keywords = {tangible tokens, multi-display environments, tabletop, gestures, cross-device interaction},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

@inproceedings{10.1145/2658779.2658809,
author = {Glassman, Elena L. and Scott, Jeremy and Singh, Rishabh and Guo, Philip and Miller, Robert},
title = {OverCode: Visualizing Variation in Student Solutions to Programming Problems at Scale},
year = {2014},
isbn = {9781450330688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658779.2658809},
doi = {10.1145/2658779.2658809},
abstract = {In MOOCs, a single programming exercise may produce thousands of solutions from learners. Understanding solution variation is important for providing appropriate feedback to students at scale. The wide variation among these solutions can be a source of pedagogically valuable examples, and can be used to refine the autograder for the exercise by exposing corner cases. We present OverCode, a system for visualizing and exploring thousands of programming solutions. OverCode uses both static and dynamic analysis to cluster similar solutions, and lets instructors further filter and cluster solutions based on different criteria. We evaluated OverCode against a non-clustering baseline in a within-subjects study with 24 teaching assistants, and found that the OverCode interface allows teachers to more quickly develop a high-level view of students' understanding and misconceptions, and to provide feedback that is relevant to more students.},
booktitle = {Proceedings of the Adjunct Publication of the 27th Annual ACM Symposium on User Interface Software and Technology},
pages = {129–130},
numpages = {2},
keywords = {programming exercises, MOOC, data mining},
location = {Honolulu, Hawaii, USA},
series = {UIST'14 Adjunct}
}

