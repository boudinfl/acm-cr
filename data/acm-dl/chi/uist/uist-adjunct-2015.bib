@inproceedings{10.1145/2815585.2815586,
author = {Greenwald, Scott W.},
title = {Responsive Facilitation of Experiential Learning Through Access to Attentional State},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815586},
doi = {10.1145/2815585.2815586},
abstract = {The planned thesis presents a vision of the future of learning, where learners explore environments, physical and virtual, in a curiosity-driven or intrinsically motivated way, and receive contextual information from a companion facilitator or teacher. Learners are instrumented with sensors that convey their cognitive and attentional state to the companion, who can then accurately judge what is interesting or relevant, and when is a good moment to jump in. I provide a broad definition of the possible types of sensor input as well as the modalities of intervention, and then present a specific proof-of-concept system that uses gaze behavior as a means of communication between the learner and a human companion.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {1–4},
numpages = {4},
keywords = {experiential learning, contextual information, remote collaboration, eye gaze interaction},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815587,
author = {Ramakers, Raf},
title = {Reconfiguring and Fabricating Special-Purpose Tangible Controls},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815587},
doi = {10.1145/2815585.2815587},
abstract = {Unlike regular interfaces on touch screens or desktop computers, tangible user interfaces allow for more physically rich interactions that better uses the capacity of our motor system. On the flipside, the physicality of tangibles comes with rigidity. This makes it hard to (1) use tangibles on systems that require a variety of controls and interaction styles, and (2) make changes to physical interfaces once manufactured. In my research, I explore techniques that allow users to reconfigure and fabricate tangible interfaces in order to mitigate these issues.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {5–8},
numpages = {4},
keywords = {tangible interfaces, deformable devices, end-user fabrication, actuator mechanisms},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815588,
author = {Siangliulue, Pao},
title = {Supporting Collaborative Innovation at Scale},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815588},
doi = {10.1145/2815585.2815588},
abstract = {Emerging online innovation platforms have enabled large groups of people to collaborate and generate ideas together in ways that were not possible before. However, these platforms also introduce new challenges in finding inspiration from a large number of ideas, and coordinating the collective effort. In my dissertation, I address the challenges of large scale idea generation platforms by developing methods and systems for helping people make effective use of each other's ideas, and for orchestrating collective effort to reduce redundancy and increase the quality and breadth of generated ideas.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {9–12},
numpages = {4},
keywords = {creativity support system, collective intelligence, ideation},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815589,
author = {Cai, Carrie J.},
title = {Wait-Learning: Leveraging Wait Time for Education},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815589},
doi = {10.1145/2815585.2815589},
abstract = {Competing priorities in daily life make it difficult for those with a casual interest in learning to set aside time for regular practice. Yet, learning often requires significant time and effort, with repeated exposures to learning material on a recurring basis. Despite the struggle to find time for learning, there are numerous times in a day that are wasted due to micro-waiting. In my research, I develop systems for wait-learning, leveraging wait time for education. Combining wait time with productive work opens up a new class of software systems that overcomes the problem of limited time while addressing the frustration often associated with waiting. My research tackles several challenges in learning and task management, such as identifying which waiting moments to leverage; how to encourage learning unobtrusively; how to integrate learning across a diversity of waiting moments; and how to extend wait-learning to more complex domains. In the development process, I hope to understand how to manage these waiting moments, and describe essential design principles for wait-learning systems.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {13–16},
numpages = {4},
keywords = {micro-learning, wait-learning, attention management},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815590,
author = {Oh, Hyunjoo},
title = {From Papercraft to Paper Mechatronics: Exploring a New Medium and Developing a Computational Design Tool},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815590},
doi = {10.1145/2815585.2815590},
abstract = {Paper Mechatronics is a novel interdisciplinary design medium, enabled by recent advances in craft technologies: the term refers to a reappraisal of traditional papercraft in combination with accessible mechanical, electronic, and computational elements. I am investigating the design space of paper mechatronics as a new hands-on medium by developing a series of examples and building a computational tool, FoldMecha, to support non-experts to design and construct their own paper mechatronics models. This paper describes how I used the tool to create two kinds of paper mechatronics models: walkers and flowers and discuss next steps.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {17–20},
numpages = {4},
keywords = {craft technology., paper mechatronics, papercraft, computational design tool},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815591,
author = {Yoon, Dongwook},
title = {Enriching Online Classroom Communication with Collaborative Multi-Modal Annotations},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815591},
doi = {10.1145/2815585.2815591},
abstract = {In massive open online courses, peer discussion is a scalable solution for offering interactive and engaging learning experiences to a large number of students. On the other hand, the quality of communication mediated through online discussion tools, such as discussion forums, is far less expressive than that of face-to-face communication. As a solution, I present RichReview, a multi-modal annotation system through which distant students can exchange ideas using versatile combinations of voice, text, and pointing gestures. A series of lab and deployment studies of RichReview promised that the expressive multimedia mixture and lightweight audio browsing feature help students better understand commentators? intention. For the large-scale deployment, I redesigned RichReview as a web applet in edX?s courseware framework. By deploying the system at scale, I will investigate (1) the optimal group assignment scheme that maximizes overall diversities of group members, (2) educational data mining applications based on user-generated rich discussion data, and (3) the impact of the rich discussion to students? retention of knowledge. Throughout these studies, I will argue that a multi-modal anchored digital document annotation system enables rich online peer discussion at scale.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {21–24},
numpages = {4},
keywords = {peer discussion, multi-modal annotation, online education, massive open online courses., instructor feedback},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815592,
author = {von Zadow, Ulrich},
title = {Using Personal Devices to Facilitate Multi-User Interaction with Large Display Walls},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815592},
doi = {10.1145/2815585.2815592},
abstract = {Large display walls and personal devices such as Smartphones have complementary characteristics. While large displays are well-suited to multi-user interaction (potentially with complex data), they are inherently public and generally cannot present an interface adapted to the individual user. However, effective multi-user interaction in many cases depends on the ability to tailor the interface, to interact without interfering with others, and to access and possibly share private data. The combination with personal devices facilitates exactly this. Multi-device interaction concepts enable data transfer and include moving parts of UIs to the personal device. In addition, hand-held devices can be used to present personal views to the user. Our work will focus on using personal devices for true multi-user interaction with interactive display walls. It will cover appropriate interaction techniques as well as the technical foundation and will be validated with corresponding application cases.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {25–28},
numpages = {4},
keywords = {mobile phones, cross-device interaction, wearable display, collaboration, display wall, multi-user, data transfer},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815593,
author = {Carter, Nancy J.},
title = {Graphical Passwords for Older Computer Users},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815593},
doi = {10.1145/2815585.2815593},
abstract = {Computers and the internet have been challenging for many computer users over the age of 60. We conducted a survey of older users which revealed that the creation, management and recall of strong text passwords were some of the challenging aspects of modern technology. In practice, this user group based passwords on familiar facts such as family member names, pets, phone numbers and important personal dates. Graphical passwords formed from abstract graphical symbols or anonymous facial images are feasible, but harder for older computers users to grasp and recall. In this paper we describe initial results for our graphical password system based on recognition of culturally-familiar facial images that are age-relevant to the life experiences of older users. Our goals are to design an easy-to-memorize, graphical password system intended specifically for older users, and achieve a level of password entropy comparable to traditional PINs and text passwords. We are also conducting a user study to demonstrate our technique and capture performance and recall metrics for comparison with traditional password systems.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {29–32},
numpages = {4},
keywords = {older adults, human factors, face recognition, authentication, human cognition, graphical passwords},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@dataset{10.1145/review-2815585.2815593_R52082,
author = {Kurfess, Franz J},
title = {Review ID:R52082 for DOI: 10.1145/2815585.2815593},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2815585.2815593_R52082}
}

@inproceedings{10.1145/2815585.2817775,
author = {Huang, Yu-Hsuan and Yu, Tzu-Chieh and Tsai, Pei-Hsuan and Wang, Yu-Xiang and Yang, Wan-ling and Ouhyoung, Ming},
title = {Scope+: A Stereoscopic Video See-Through Augmented Reality Microscope},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817775},
doi = {10.1145/2815585.2817775},
abstract = {During the process of using conventional stereo microscope, users need to move their head away from the eyepieces repeatedly to access more information, such as anatomy structures from atlas. It happens during microsurgery if surgeons want to check patient?s data again. You might lose your target and your concentration after this kind of disruption. To solve this critical problem and to improve the user experience of stereo microscope, we present Scope+, a stereoscopic video see-through augmented reality system. Scope+ is designed for biological procedures, education and surgical training. While performing biological procedures, for example, dissection of a frog, anatomical atlas will show up inside the head mounted display (HMD) overlaid onto the magnified images. For education purpose, the specimens will no longer be silent under Scope+. When their body parts are pointed by a marked stick, related animation or transparent background video will merge with the real object and interact with observers. If surgeons want to improve their techniques of microsurgery, they can practice with Scope+ which provides complete foot pedal control functions identical to standard surgical microscope. Moreover, cooperating with special designed phantom models, this augmented reality system will guide you to perform some key steps of operation, such as Continuous Curvilinear Capsulorhexis in cataract surgery. Video see-through rather than optical see-through technology is adopt by Scope+ system, therefore remote observation via another Scope+ or web applications can be achieved. This feature can not only assist teachers during experiment classes, but also help researchers keep their eyes on the observables after work. Array mode is powered by the motor-driven stage plate which allows users to load multiple samples at the same time. Quick comparison between samples is possible when switching them by the foot pedal.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {33–34},
numpages = {2},
keywords = {3d interaction and graphics, tutorial and help systems, virtualreality, augmented reality},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817776,
author = {Kato, Kunihiro and Miyashita, Homei},
title = {Creating a Mobile Head-Mounted Display with Proprietary Controllers for Interactive Virtual Reality Content},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817776},
doi = {10.1145/2815585.2817776},
abstract = {A method to create a mobile head-mounted display (HMD) a proprietary controller for interactive virtual reality (VR) content is proposed. The proposed method uses an interface cartridge printed with a conductive pattern. This allows the user to operate a smartphone by touching on the face of the mobile HMD. In addition, the user can easily create a mobile HMD and interface cartridge using a laser cutter and inkjet printer. Changing the form of the conductive pattern allows the user to create a variety of controllers. The proposed method can realize an environment that can deliver a variety of interactions with VR content.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {35–36},
numpages = {2},
keywords = {interface cartridge, conductive ink, mobile hmd},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817777,
author = {Lee, Byungjoo and Oulasvirta, Antti},
title = {Spotlights: Facilitating Skim Reading with Attention-Optimized Highlights},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817777},
doi = {10.1145/2815585.2817777},
abstract = {This demo presents Spotlights, a technique to facilitate skim reading, or the activity of rapidly comprehending long documents such as webpages or PDFs. Users mainly use continuous rate-based scrolling to skim. However, visual attention fails when scrolling rapidly due to excessive number of objects and brief exposure per object. Spotlights supports continuous scrolling at high speeds. It selects a small number of objects and raises them to transparent overlays (spotlights) in the viewer. Spotlights stay static for a prolonged time and then fade away. The technical contribution is novel method for ?brokering? user?s attentional resources in a way that guarantees sufficient attentional resources for some objects, even at very high scrolling rates. It facilitates visual attention by (1) decreasing the number of objects competing for divided attention and (2) by ensuring sufficient processing time per object.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {37–38},
numpages = {2},
keywords = {skim reading, attentional blink., attention brokering, visual attention, comprehension, scrolling techniques},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817782,
author = {Nebeling, Michael and Guo, Anhong and To, Alexandra and Dow, Steven and Teevan, Jaime and Bigham, Jeffrey},
title = {WearWrite: Orchestrating the Crowd to Complete Complex Tasks from Wearables},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817782},
doi = {10.1145/2815585.2817782},
abstract = {Smartwatches are becoming increasingly powerful, but limited input makes completing complex tasks impractical. Our WearWrite system introduces a new paradigm for enabling a watch user to contribute to complex tasks, not through new hardware or input methods, but by directing a crowd to work on their behalf from their wearable device. WearWrite lets authors give writing instructions and provide bits of expertise and big picture directions from their smartwatch, while crowd workers actually write the document on more powerful devices. We used this approach to write three academic papers, and found it was effective at producing reasonable drafts.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {39–40},
numpages = {2},
keywords = {writing, wearables, smartwatch interaction, crowdsourcing},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817786,
author = {Sato, Munehiko and Puri, Rohan S. and Olwal, Alex and Chandra, Deepak and Poupyrev, Ivan and Raskar, Ramesh},
title = {Zensei: Augmenting Objects with Effortless User Recognition Capabilities through Bioimpedance Sensing},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817786},
doi = {10.1145/2815585.2817786},
abstract = {As interactions with everyday handheld devices and objects become increasingly common, a more seamless and effortless identification and personalization technique will be essential to an uninterrupted user experience. In this paper, we present Zensei, a user identification and customization system using human body bioimpedance sensing through multiple electrodes embedded into everyday objects. Zensei provides for an uninterrupted user-device personalization experience that is difficult to forge because it uses both the unique physiological and behavioral characteristics of the user. We demonstrate our measurement system in three exemplary device configurations that showcase different levels of constraint via environment-based, whole-body-based, and handheld-based identification scenarios. We evaluated Zensei's classification accuracy among 12 subjects on each configuration over 22 days of collected data and report our promising results.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {41–42},
numpages = {2},
keywords = {bioimpedance sensing, customization, user identification},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817797,
author = {Kato, Jun and Goto, Masataka},
title = {Form Follows Function(): An IDE to Create Laser-Cut Interfaces and Microcontroller Programs from Single Code Base},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817797},
doi = {10.1145/2815585.2817797},
abstract = {During the development of physical computing devices, physical object models and programs for microcontrollers are usually created with separate tools with distinct files. As a result, it is difficult to track the changes in hardware and software without discrepancy. Moreover, the software cannot directly access hardware metrics. Designing hardware interface cannot benefit from the source code information either. This demonstration proposes a browser-based IDE named f3.js that enables development of both as a single JavaScript code base. The demonstration allows audiences to play with the f3.js IDE and showcases example applications such as laser-cut interfaces generated from the same code but with different parameters. Programmers can experience the full feature and designers can interact with preset projects with a mouse or touch to customize laser-cut interfaces. More information is available at http://f3js.org.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {43–44},
numpages = {2},
keywords = {integrated development environment, laser-cut interface, microcontroller, personal fabrication},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817801,
author = {Bedri, Hisham and Gupta, Otkrist and Temme, Andrew and Feigin, Micha and Charvat, Gregory and Raskar, Ramesh},
title = {RFlow: User Interaction Beyond Walls},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817801},
doi = {10.1145/2815585.2817801},
abstract = {Current user-interaction with optical gesture tracking technologies suffer from occlusions, limiting the functionality to direct line-of-sight. We introduce RFlow, a compact, medium-range interface based on Radio Frequency (RF) that enables camera-free tracking of the position of a moving hand through drywall and other occluders. Our system uses Time of Flight (TOF) RF sensors and speed-based segmentation to localize the hand of a single user with 5cm accuracy (as measured to the closest ground-truth point), enabling an interface which is not restricted to a training set.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {45–46},
numpages = {2},
keywords = {hand-tracking, radio-frequency, 3d, through-wall},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817802,
author = {Sra, Misha and Schmandt, Chris},
title = {MetaSpace: Full-Body Tracking for Immersive Multiperson Virtual Reality},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817802},
doi = {10.1145/2815585.2817802},
abstract = {Most current virtual reality (VR) interactions are mediated by hand-held input devices or hand gestures and they usually display only a partial representation of the user in the synthetic environment. We believe, representing the user as a full avatar that is controlled by natural movements of the person in the real world will lead to a greater sense of presence in VR. Possible applications exist in various domains such as entertainment, therapy, travel, real estate, education, social interaction and professional assistance. In this demo, we present MetaSpace, a virtual reality system that allows co-located users to explore a VR world together by walking around in physical space. Each user's body is represented by an avatar that is dynamically controlled by their body movements. We achieve this by tracking each user's body with a Kinect device such that their physical movements are mirrored in the virtual world. Users can see their own avatar and the other person's avatar allowing them to perceive and act intuitively in the virtual environment.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {47–48},
numpages = {2},
keywords = {multi-person virtual reality, full-body immersion, locomotion, physical to virtual mapping, first person view},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2835511,
author = {Liang, Rong-Hao and Kuo, Han-Chih and Chen, Bing-Yu},
title = {GaussStarter: Prototyping Analog Hall-Sensor Grids with Breadboards},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2835511},
doi = {10.1145/2815585.2835511},
abstract = {This work presents GaussStarter, a pluggable and tileable analog Hall-sensor grid module for easy and scalable bread- board prototyping. In terms of ease-of-use, the graspable units allow users to easily plug them on or remove them from a breadboard. In terms of scalability, tiling the units on the breadboard can easily expand the sensing area. A software development kit is also provided for designing applications based on this hardware module.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {49–50},
numpages = {2},
keywords = {breadboard, gausssense, analog hall-sensor grid},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817807,
author = {Yamada, Wataru and Manabe, Hiroyuki and Inamura, Hiroshi},
title = {Enhanced Motion Robustness from ToF-Based Depth Sensing Cameras},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817807},
doi = {10.1145/2815585.2817807},
abstract = {Depth sensing cameras that can acquire RGB and depth information are being widely used. They can expand and enhance various camera-based applications and are cheap but strong tools for computer human interaction. RGB and depth sensing cameras have quite different key parameters, such as exposure time. We focus on the differences in their motion robustness; the RGB camera has relatively long exposure times while those of ToF (Time-of-flight) based depth sensing camera are relatively short. An experiment on visual tag reading, one typical application, shows that depth sensing cameras can robustly decode moving tags. The proposed technique will yield robust tag reading, indoor localization, and color image stabilization while walking and jogging or even glancing momentarily without requiring any special additional devices.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {51–52},
numpages = {2},
keywords = {depth sensing camera, exposure time, visual tag},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817808,
author = {Cortes Torres, Carlos C. and Sampei, Kota and Sato, Munehiko and Raskar, Ramesh and Miki, Norihisa},
title = {Workload Assessment with Eye Movement Monitoring Aided by Non-Invasive and Unobtrusive Micro-Fabricated Optical Sensors},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817808},
doi = {10.1145/2815585.2817808},
abstract = {Mental state or workload of a person are very relevant when the person is executing delicate tasks such as piloting an aircraft, operating a crane because the high level of workload could prevent accomplishing the task and lead to disastrous results. Some frameworks have been developed to assess the workload and determine whether the person is capable of executing a new task. However, such methodologies are applied when the operator finished the task. Another feature that these methodologies share is that are based on paper and pencil tests. Therefore, human-friendly devices that could assess the workload in real time are in high demand. In this paper, we report a wearable device that can correlate physical eye behavior with the mental state for the workload assessment.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {53–54},
numpages = {2},
keywords = {nasa tlx, optical interface, dye-sensitized photovoltaic devices, transparent sensors, line-of-sight, wearable, workload assessment},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817809,
author = {Yoon, Dongwook and Mitros, Piotr},
title = {Multi-Modal Peer Discussion with RichReview on EdX},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817809},
doi = {10.1145/2815585.2817809},
abstract = {In this demo, we present RichReview, a multi-modal peer discussion system, implemented as an XBlock in the edX courseware platform. The system brings richness similar to face-to-face communication into online learning at scale. With this demonstration, we discuss the system?s scalable back-end architecture, semantic voice editing user interface, and a future research plan for the profile based group-assignment scheme.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {55–56},
numpages = {2},
keywords = {multi-modal annotation, massive open online courses, voice user interface, peer discussion, peer group assignment.},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817810,
author = {Rubens, Calvin and Braley, Sean and Gomes, Antonio and Goc, Daniel and Zhang, Xujing and Carrascal, Juan Pablo and Vertegaal, Roel},
title = {BitDrones: Towards Levitating Programmable Matter Using Interactive 3D Quadcopter Displays},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817810},
doi = {10.1145/2815585.2817810},
abstract = {In this paper, we present BitDrones, a platform for the construction of interactive 3D displays that utilize nano quadcopters as self-levitating tangible building blocks. Our prototype is a first step towards supporting interactive mid-air, tangible experiences with physical interaction techniques through multiple building blocks capable of physically representing interactive 3D data.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {57–58},
numpages = {2},
keywords = {claytronics, organic user interfaces, radical atoms, tangible user interfaces},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817812,
author = {Ou, Jifei and Cheng, Chin-Yi and Zhou, Liang and Dublon, Gershon and Ishii, Hiroshi},
title = {Methods of 3D Printing Micro-Pillar Structures on Surfaces},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817812},
doi = {10.1145/2815585.2817812},
abstract = {This work presents a method of 3D printing hair-like structures on both flat and curved surfaces. It allows a user to design and fabricate hair geometry that is smaller than 100 micron. We built a software platform to let one quickly define a hair's angle, thickness, density, and height. The ability to fabricate customized hair-like structures expands the library of 3D-printable shape. We then present several applications to show how the 3D-printed hair can be used for designing toy objects.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {59–60},
numpages = {2},
keywords = {3d printing, surface texture},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2817815,
author = {Momeni, Ali and Rispoli, Zachary},
title = {Dranimate: Rapid Real-Time Gestural Rigging and Control of Animation},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2817815},
doi = {10.1145/2815585.2817815},
abstract = {Dranimate is an interactive animation system that allows users to rapidly and intuitively rig and control animations based on a still image or drawing, using hand gestures. Dranimate combines two complementary methods of shape manipulation: bone-joint-based physics simulation, and the as-rigid-as-possible deformation algorithm. Dranimate also introduces a number of designed interactions that focus the users attention on the animated content, as opposed to computer keyboard or mouse.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {61–62},
numpages = {2},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815726,
author = {Lee, Jinha and Baek, Seungcheon},
title = {Elastic Cursor and Elastic Edge: Applying Simulated Resistance to Interface Elements for Seamless Edge-Scroll},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815726},
doi = {10.1145/2815585.2815726},
abstract = {We present elastic cursor and elastic edge, new interaction techniques for seamless edge-scroll. Through the use of light-weight physical simulations of elastic behavior on interface elements, we can improve precision, usability, and cueing on the use of edge-scroll in scrollable windows or screens, and make experiences more playful and easier to learn.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {63–64},
numpages = {2},
keywords = {physics simulation, cursor, scroll, pseudo-haptic simulation},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815721,
author = {Tartz, Robert and Gooding, Ted},
title = {Hand Biometrics Using Capacitive Touchscreens},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815721},
doi = {10.1145/2815585.2815721},
abstract = {Biometric methods for authentication on mobile devices are becoming popular. Some methods such as face and voice biometrics are problematic in noisy mobile environments, while others such as fingerprint require specialized hardware to operate. We present a novel biometric authentication method that uses raw touch capacitance data captured from the hand touching a display. Performance results using a moderate sample size (N = 40) yielded an equal error rate (EER) of 2.5%, while a 1-month longitudinal study using a smaller sample (N = 10) yielded an EER = 2.3%. Overall, our results provide evidence for biometric uniqueness, permanence and user acceptance.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {67–68},
numpages = {2},
keywords = {touch interaction, touchscreens, biometrics, security},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815722,
author = {Park, Chanho and Ogawa, Takefumi},
title = {A Study on Grasp Recognition Independent of Users' Situations Using Built-in Sensors of Smartphones},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815722},
doi = {10.1145/2815585.2815722},
abstract = {There are many hand postures of smartphone according to the users? situations. In order to support appropriate inter-face, it is important to know user?s hand posture. To recognize grasp postures, which is not depend on users? situations, we consider using smartphone?s touchscreen and their built-in gyroscope and accelerometer and use support vector machine (SVM). In order to evaluate our system, we described the result of the experiments when users are using the devices in the room and on the train. We knew that our system could be feasible for personal use only system by improving the information from the accelerometer. We also collected users? data when users are sitting in the room. Results showed that grasp recognition accuracy under 5 and 4 hand postures were 87.7%, 92.4% respectively when training and testing on 6 users.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {69–70},
numpages = {2},
keywords = {gyroscope, hand posture, support vector machine, touchscreen, smartphone, grasp recognition, accelerometer},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815723,
author = {Yoon, Sang Ho and Huo, Ke and Ramani, Karthik},
title = {TMotion: Embedded 3D Mobile Input Using Magnetic Sensing Technique},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815723},
doi = {10.1145/2815585.2815723},
abstract = {We present TMotion, a self-contained 3D input that enables spatial interactions around mobile using a magnetic sensing technique. Using a single magnetometer from the mobile device, we can track the 3D position of the permanent magnet embedded in the prototype along with an inertial measurement unit. By numerically solving non-linear magnetic field equations with known orientation from inertial measurement unit (IMU), we attain a tracking rate greater than 30Hz based solely on the mobile device computation. We describe the working principle of TMotion and example applications illustrating its capability.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {71–72},
numpages = {2},
keywords = {embedded interaction, mobile interaction, 3d input, magnetic sensing, sensor fusion},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815724,
author = {Yang, Yoonsik and Chae, Seungho and Shim, Jinwook and Han, Tack-Don},
title = {EMG Sensor-Based Two-Hand Smart Watch Interaction},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815724},
doi = {10.1145/2815585.2815724},
abstract = {These days, smart watches have drawn more attention of users, and many smart watch products have been launched (Samsung Gear series, apple watch and etc.). Since a smart watch is put on the wrist, the device should be small and unobtrusive. Because of these features, display of the smart watch is small and there is a limitation to interaction. To overcome the limitation, many studies are conducted. In this paper, we propose a two-hand interaction technique that obtains posture information of a hand using electromyography (EMG) sensor attached to the arm and to make input interaction to a smart watch different depending on each posture. EMG sensors recognize information about a user's hand posture, and the non-dominant hand is used for smart watch inputs. In this way, different function is executed depending on postures. As a result, a smart watch that has limited input methods is given a variety of interaction functions with users.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {73–74},
numpages = {2},
keywords = {electromyography, smart watch interaction, arm-placed devices},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815725,
author = {Shankar Mysore, Alok and Yaligar, Vikas S. and Arrieta Ibarra, Imanol and Simoiu, Camelia and Goel, Sharad and Arvind, Ramesh and Sumanth, Chiraag and Srikantan, Arvind and HS, Bhargav and Pahadia, Mayank and Dobha, Tushar and Ahmed, Atif and Shankar, Mani and Agarwal, Himani and Agarwal, Rajat and Anirudh-Kondaveeti, Sai and Arun-Gokhale, Shashank and Attri, Aayush and Chandra, Arpita and Chilukur, Yogitha and Dharmaji, Sharath and Garg, Deepak and Gupta, Naman and Gupta, Paras and Jacob, Glincy Mary and Jain, Siddharth and Joshi, Shashank and Khajuria, Tarun and Khillan, Sameeksha and Konam, Sandeep and Kumar-Kolla, Praveen and Loomba, Sahil and Madan, Rachit and Maharaja, Akshansh and Mathur, Vidit and Munshi, Bharat and Nawazish, Mohammed and Neehar-Kurukunda, Venkata and Nirmal-Gavarraju, Venkat and Parashar, Sonali and Parikh, Harsh and Paritala, Avinash and Patil, Amit and Phatak, Rahul and Pradhan, Mandar and Ravichander, Abhilasha and Sangeeth, Krishna and Sankaranarayanan, Sreecharan and Sehgal, Vibhor and Sheshan, Ashrith and Shibiraj, Suprajha and Singh, Aditya and Singh, Anjali and Sinha, Prashant and Soni, Pushkin and Thomas, Bipin and Varma-Dattada, Kasyap and Venkataraman, Sukanya and Verma, Pulkit and Yelurwar, Ishan},
title = {Investigating the "Wisdom of Crowds" at Scale},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815725},
doi = {10.1145/2815585.2815725},
abstract = {In a variety of problem domains, it has been observed that the aggregate opinions of groups are often more accurate than those of the constituent individuals, a phenomenon that has been termed the "wisdom of the crowd." Yet, perhaps surprisingly, there is still little consensus on how generally the phenomenon holds, how best to aggregate crowd judgements, and how social influence affects estimates. We investigate these questions by taking a meta wisdom of crowds approach. With a distributed team of over 100 student researchers across 17 institutions in the United States and India, we develop a large-scale online experiment to systematically study the wisdom of crowds effect for 1,000 different tasks in 50 subject domains. These tasks involve various types of knowledge (e.g., explicit knowledge, tacit knowledge, and prediction), question formats (e.g., multiple choice and point estimation), and inputs (e.g., text, audio, and video). To examine the effect of social influence, participants are randomly assigned to one of three different experiment conditions in which they see varying degrees of information on the responses of others. In this ongoing project, we are now preparing to recruit participants via Amazon?s Mechanical Turk.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {75–76},
numpages = {2},
keywords = {crowd consensus, crowdsourcing, online experiment},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815727,
author = {Arnold, Kenneth C. and Gajos, Krzysztof Z.},
title = {Effective Interactions for Personalizing Spatial Visualizations of Collections},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815727},
doi = {10.1145/2815585.2815727},
abstract = {Interactive spatial visualizations powered by machine learning will help us explore and understand large collections in meaningful ways, but little is yet known about the design space of interactions. We ran a pilot user study to compare two different interaction techniques: a "grouping?" interaction adapted from interactive clustering, and an existing "positioning?" interaction. We identified three important dimensions of the interaction design space that inform future design of more intuitive and expressive interactions.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {77–78},
numpages = {2},
keywords = {interactive spatial visualization, mixed-initiative interfaces},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815728,
author = {Suzuki, Kenji and Okabe, Kazumasa and Sakamoto, Ryuuki and Sakamoto, Daisuke},
title = {Fix and Slide: Caret Navigation with Movable Background},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815728},
doi = {10.1145/2815585.2815728},
abstract = {We present a ?Fix and Slide? technique, which is a concept to use a movable background to place a caret insertion point and to select text on a mobile device. Standard approach to select text on the mobile devices is touching to the text where a user wants to select, and sometimes pop-up menu is displayed and s/he choose ?select? mode and then start to specify an area to be selected. A big problem is that the user?s finger hides the area to select; this is called a "fat finger problem." We use the movable background to navigate a caret. First a user places a caret by tapping on a screen and then moves the background by touching and dragging on a screen. In this situation, the caret is fixed on the screen so that the user can move the background to navigate the caret where the user wants to move the caret. We implement the Fix and Slide technique on iOS device (iPhone) to demonstrate the impact of this text selection technique on small mobile devices.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {79–80},
numpages = {2},
keywords = {movable background, mobile device., caret, pointing, touch interaction, text selection},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815729,
author = {Gordon, Mitchell and Bigham, Jeffrey P. and Lasecki, Walter S.},
title = {LegionTools: A Toolkit + UI for Recruiting and Routing Crowds to Synchronous Real-Time Tasks},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815729},
doi = {10.1145/2815585.2815729},
abstract = {We introduce LegionTools, a toolkit and interface for managing large, synchronous crowds of online workers for experiments. This poster contributes the design and implementation of a state-of-the-art crowd management tool, along with a publicly-available, open-source toolkit that future system builders can use to coordinate synchronous crowds of online workers for their systems and studies.We describe the toolkit itself, along with the underlying design rationale, in order to make it clear to the community of system builders at UIST when and how this tool may be beneficial to their project. We also describe initial deployments of the system in which workers were synchronously recruited to support real-time crowdsourcing systems, including the largest synchronous recruitment and routing of workers from Mechanical Turk that we are aware of. While the version of LegionTools discussed here focuses on Amazon's Mechanical Turk platform, it can be easily extended to other platforms as APIs become available.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {81–82},
numpages = {2},
keywords = {human computation, crowdsourcing, tools, real-time},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815730,
author = {Benavides, Xavier and Zhu Jin, Chang Long and Maes, Pattie and Paradiso, Joseph},
title = {KickSoul: A Wearable System for Feet Interactions with Digital Devices},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815730},
doi = {10.1145/2815585.2815730},
abstract = {In this paper we present a wearable device that maps natural feet movements into inputs for digital devices. KickSoul consists of an insole with sensors embedded that tracks movements and triggers actions in devices that surround us. We present a novel approach to use our feet as input devices in mobile situations when our hands are busy. We analyze natural feet?s movements and their meaning before activating an action. This paper discusses different applications for this technology as well as the implementation of our prototype.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {83–84},
numpages = {2},
keywords = {wearable electronics., diy, gesture recognition},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815731,
author = {Yoshida, Arika and Shizuki, Buntarou and Tanaka, Jiro},
title = {Capacitive Blocks: A Block System That Connects the Physical with the Virtual Using Changes of Capacitance},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815731},
doi = {10.1145/2815585.2815731},
abstract = {We propose a block-stacking system based on capacitance. The system, called Capacitive Blocks, allows users to build 3D models in a virtual space by stacking physical blocks. The construction of the block-stacking system is simple, and fundamental components including physical blocks can be made with a 3D printer. The block is a capacitor that consists of two layers made of conductive plastic filament and between them a layer made of non-conductive plastic filament. In this paper, we present a prototype of this block-stacking system and the mechanism that detects the height of blocks (i.e., the number of stacked blocks) by measuring the capacitance of the stacked blocks, which changes in accordance with the number of stacked blocks.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {85–86},
numpages = {2},
keywords = {building block, stacking, interactive devices, capacitive block, tangible, 3d printing, computational crafts},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815732,
author = {Okawa, Yuya and Takemura, Kentaro},
title = {Haptic-Enabled Active Bone-Conducted Sound Sensing},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815732},
doi = {10.1145/2815585.2815732},
abstract = {In this study, we propose active bone-conducted sound sens- ing for estimating a joint angle of a finger and simultaneous use as a haptic interface. For estimating the joint angle, an unnoticeable vibration is input to the finger, and a perceptible vibration is additionally input to the finger for providing hap- tic feedback. The joint angle is estimated by switching the estimation model depending on the haptic feedback and the average error of the estimation is within about seven degree.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {87–88},
numpages = {2},
keywords = {active sensing, vibration, finger joint angle, haptic interface},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815733,
author = {Palleis, Henri and Wagner, Julie and Hussmann, Heinrich},
title = {Perspective-Dependent Indirect Touch Input for 3D Polygon Extrusion},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815733},
doi = {10.1145/2815585.2815733},
abstract = {We present a two-handed indirect touch interaction technique for the extrusion of polygons within a 3D modeling tool that we have built for a horizontal/vertical dual touch screen setup. In particular, we introduce perspective-dependent touch gestures: using several graphical input areas on the horizontal display, the non-dominant hand navigates the virtual camera and thus continuously updates the spatial frame of reference within which the dominant hand performs extrusions with dragging gestures.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {89–90},
numpages = {2},
keywords = {extrusion, perspective-dependent gestures, bimanual interaction, 3d polygon modeling},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815734,
author = {Oh, Hyunjoo and Gross, Mark D. and Eisenberg, Michael},
title = {FoldMecha: Design for Linkage-Based Paper Toys},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815734},
doi = {10.1145/2815585.2815734},
abstract = {We present FoldMecha, a computational tool to help non-experts design and build paper mechanical toys. By customizing templates a user can experiment with basic mechanisms, design their own model, print and cut out a folding net to construct the toy. We used the tool to build two kinds of paper automata models: walkers and flowers.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {91–92},
numpages = {2},
keywords = {papercraft, computational design tool, paper automata},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815735,
author = {Knibbe, Jarrod and Benko, Hrvoje and Wilson, Andrew D.},
title = {Juggling the Effects of Latency: Software Approaches to Minimizing Latency in Dynamic Projector-Camera Systems},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815735},
doi = {10.1145/2815585.2815735},
abstract = {Projector-camera (pro-cam) systems afford a wide range of interactive possibilities, combining both natural and mixed-reality 3D interaction. However, the latency inherent within these systems can cause the projection to ?slip? from any moving target, so pro-cam systems have typically shied away from truly dynamic scenarios. We explore software-only techniques to reduce latency; considering the best achievable results with widely adopted commodity devices (e.g. 30Hz depth cameras and 60Hz projectors). We achieve 50% projection alignment on objects in free flight (a 34% improvement) and 69% alignment on dynamic human movement (a 40% improvement).},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {93–94},
numpages = {2},
keywords = {prediction, projection lag, latency, projector-camera systems},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815736,
author = {Son, KyoungHee and Oh, Seo Young and Kim, Yongkwan and Choi, Hayan and Bae, Seok-Hyung and Hwang, Ganguk},
title = {Color Sommelier: Interactive Color Recommendation System Based on Community-Generated Color Palettes},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815736},
doi = {10.1145/2815585.2815736},
abstract = {We present Color Sommelier, an interactive color recommendation system based on community-generated color palettes that helps users to choose harmonious colors on the fly. We used an item-based collaborative filtering technique with Adobe Color CC palettes in order to take advantage of their ratings that reflect the general public?s color harmony preferences. Every time a user chooses a color(s), Color Sommelier calculates how harmonious each of the remaining colors is with the chosen color(s). This interactive recommendation enables users to choose colors iteratively until they are satisfied. To illustrate the usefulness of the algorithm, we implemented a coloring application with a specially designed color chooser. With the chooser, users can intuitively recognize the harmony score of each color based on its bubble size and use the recommendations at their discretion. The Color Sommelier algorithm is flexible enough to be applicable to any color chooser in any software package and is easy to implement.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {95–96},
numpages = {2},
keywords = {interactive color recommendation, collaborative filtering},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815737,
author = {Shima, Keigo and Takada, Ryosuke and Onishi, Kazusa and Adachi, Takuya and Shizuki, Buntarou and Tanaka, Jiro},
title = {AirFlip-Undo: Quick Undo Using a Double Crossing In-Air Gesture in Hover Zone},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815737},
doi = {10.1145/2815585.2815737},
abstract = {In this work, we use AirFlip to undo text input on mobile touchscreen devices. AirFlip involves a quick double crossing in-air gesture in the boundary surfaces of hover zone of devices that have hover sensing capability. To evaluate the effectiveness of undoing text input with AirFlip, we implemented two QWERTY soft keyboards (AirFlip keyboard and Typical keyboard). With these keyboards, we conducted a user study to investigate the users? workload and to collect subjective opinions. The results show that there is no significant difference in workload between keyboards.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {97–98},
numpages = {2},
keywords = {one handed, double crossing gesture, in-air gesture, touch panel, hover gesture},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815738,
author = {Benavides, Xavier and Amores, Judith and Maes, Pattie},
title = {Remot-IO: A System for Reaching into the Environment of a Remote Collaborator},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815738},
doi = {10.1145/2815585.2815738},
abstract = {In this paper we present Remot-IO, a system for mobile collaboration and remote assistance around Internet connected devices. The system uses two Head Mounted Displays, cameras and depth sensors to enable a remote expert to be immersed in a local user's point of view and control devices in that user?s environment. The remote expert can provide guidance through the use of hand gestures that appear in real-time in the local user?s field of view as superimposed 3D hands. In addition, the remote expert is able to operate devices in the novice?s environment and bring about physical changes by using the same hand gestures the novice would use. We describe a smart radio where the knobs of the radio can be controlled by local and remote user alike. Moreover, the user can visualize, interact and modify properties of sound waves in real time by using intuitive hand gestures.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {99–100},
numpages = {2},
keywords = {shared experiences, hands free interaction., remote collaboration, telepresence, 3d interaction, augmented reality},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815739,
author = {Gaikwad, Snehal (Neil) and Morina, Durim and Nistala, Rohit and Agarwal, Megha and Cossette, Alison and Bhanu, Radhika and Savage, Saiph and Narwal, Vishwajeet and Rajpal, Karan and Regino, Jeff and Mithal, Aditi and Ginzberg, Adam and Nath, Aditi and Ziulkoski, Karolina R. and Cossette, Trygve and Gamage, Dilrukshi and Richmond-Fuller, Angela and Suzuki, Ryo and Herrej\'{o}n, Jeerel and Le, Kevin and Flores-Saviaga, Claudia and Thilakarathne, Haritha and Gupta, Kajal and Dai, William and Sastry, Ankita and Goyal, Shirish and Rajapakshe, Thejan and Abolhassani, Niki and Xie, Angela and Reyes, Abigail and Ingle, Surabhi and Jaramillo, Ver\'{o}nica and God\'{\i}nez, Martin and \'{A}ngel, Walter and Toxtli, Carlos and Flores, Juan and Gupta, Asmita and Sethia, Vineet and Padilla, Diana and Milland, Kristy and Setyadi, Kristiono and Wajirasena, Nuwan and Batagoda, Muthitha and Cruz, Rolando and Damon, James and Nekkanti, Divya and Sarma, Tejas and Saleh, Mohamed and Gongora-Svartzman, Gabriela and Bateni, Soroosh and Toledo Barrera, Gema and Pe\~{n}a, Alex and Compton, Ryan and Aariff, Deen and Palacios, Luis and Ritter, Manuela Paula and K.K., Nisha and Kay, Alan and Uhrmeister, Jana and Nistala, Srivalli and Esfahani, Milad and Bakiu, Elsa and Diemert, Christopher and Matsumoto, Luca and Singh, Manik and Patel, Krupa and Krishna, Ranjay and Kovacs, Geza and Vaish, Rajan and Bernstein, Michael},
title = {Daemo: A Self-Governed Crowdsourcing Marketplace},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815739},
doi = {10.1145/2815585.2815739},
abstract = {Crowdsourcing marketplaces provide opportunities for autonomous and collaborative professional work as well as social engagement. However, in these marketplaces, workers feel disrespected due to unreasonable rejections and low payments, whereas requesters do not trust the results they receive. The lack of trust and uneven distribution of power among workers and requesters have raised serious concerns about sustainability of these marketplaces. To address the challenges of trust and power, this paper introduces Daemo, a self-governed crowdsourcing marketplace. We propose a prototype task to improve the work quality and open-governance model to achieve equitable representation. We envisage Daemo will enable workers to build sustainable careers and provide requesters with timely, quality labor for their businesses.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {101–102},
numpages = {2},
keywords = {crowdsourcing, crowd work., crowd research},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815740,
author = {Xu, Ding and Momeni, Ali and Brockmeyer, Eric},
title = {MagPad: A Near Surface Augmented Reading System for Physical Paper and Smartphone Coupling},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815740},
doi = {10.1145/2815585.2815740},
abstract = {In this paper, we present a novel near surface augmented reading system that brings digital content to physical papers. Our system allows a collocated mobile phone to provide augmented content based on its position on top of paper. Our system utilizes built-in magnetometer of a smartphone together with six constantly spinning magnets that generate designed patterns of magnetic flux, to detect 2D location of phone and render dynamic interactive content on the smartphone screen. The proposed technique could be implemented on most of mobile platforms without external sensing hardware.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {103–104},
numpages = {2},
keywords = {augmented reading, smart lens, active magnetic sensing},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

@inproceedings{10.1145/2815585.2815741,
author = {Gaylord, Weston and Hare, Vivian and Ngu, Ashley},
title = {Adding Body Motion and Intonation to Instant Messaging with Animation},
year = {2015},
isbn = {9781450337809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815585.2815741},
doi = {10.1145/2815585.2815741},
abstract = {Digital text communication (DTC) has transformed the way people communicate. Static typographical cues like emoticons, punctuation, letter case, and word lengthening (ie. Hellooo?) are regularly employed to convey intonation and affect. However, DTC platforms like instant messaging still suffer from a lack of nonverbal communication cues. This paper introduces an Animated Text Instant Messenger (ATIM), which uses text animations to add another distinct layer of cues to existing plaintext. ATIM builds upon previous research by using kinetic typography in communication. This paper describes the design principles and features of ATIM and discusses how animated text can add more nuanced communication cues of intonation and body motion.},
booktitle = {Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
pages = {105–106},
numpages = {2},
keywords = {instant messaging, body motion, animation, kinetic typography, intonation},
location = {Daegu, Kyungpook, Republic of Korea},
series = {UIST '15 Adjunct}
}

