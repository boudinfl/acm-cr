@inproceedings{10.1145/3251554,
author = {Miller, Rob},
title = {Session Details: Keynote Address},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251554},
doi = {10.1145/3251554},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380118,
author = {Livingstone, Margaret},
title = {What Art Can Tell Us about the Brain},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380118},
doi = {10.1145/2380116.2380118},
abstract = {Artists have been doing experiments on vision longer than neurobiologists. Some major works of art have provided insights as to how we see; some of these insights are so fundamental that they can be understood in terms of the underlying neurobiology. For example, artists have long realized that color and luminance can play independent roles in visual perception. Picasso said, "Colors are only symbols. Reality is to be found in luminance alone." This observation has a parallel in the functional subdivision of our visual systems, where color and luminance are processed by the newer, primate-specific What system, and the older, colorblind, Where (or How) system. Many techniques developed over the centuries by artists can be understood in terms of the parallel organization of our visual systems. I will explore how the segregation of color and luminance processing are the basis for why some Impressionist paintings seem to shimmer, why some op art paintings seem to move, some principles of Matisse's use of color, and how the Impressionists painted "air". Central and peripheral vision are distinct, and I will show how the differences in resolution across our visual field make the Mona Lisa's smile elusive, and produce a dynamic illusion in Pointillist paintings, Chuck Close paintings, and photomosaics. I will explore how artists have intuited important features about how our brains extract relevant information about faces and objects, and I will discuss why learning disabilities may be associated with artistic talent.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–2},
numpages = {2},
keywords = {keynote talk, art, brain},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251555,
author = {Morris, Meredith Ringel},
title = {Session Details: Groups &amp; Crowds},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251555},
doi = {10.1145/3251555},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380120,
author = {Huang, Jeff and Etzioni, Oren and Zettlemoyer, Luke and Clark, Kevin and Lee, Christian},
title = {RevMiner: An Extractive Interface for Navigating Reviews on a Smartphone},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380120},
doi = {10.1145/2380116.2380120},
abstract = {Smartphones are convenient, but their small screens make searching, clicking, and reading awkward. Thus, perusing product reviews on a smartphone is difficult. In response, we introduce RevMiner - a novel smartphone interface that utilizes Natural Language Processing techniques to analyze and navigate reviews. RevMiner was run over 300K Yelp restaurant reviews extracting attribute-value pairs, where attributes represent restaurant attributes such as sushi and service, and values represent opinions about the attributes such as fresh or fast. These pairs were aggregated and used to: 1) answer queries such as "cheap Indian food", 2) concisely present information about each restaurant, and 3) identify similar restaurants. Our user studies demonstrate that on a smartphone, participants preferred RevMiner's interface to tag clouds and color bars, and that they preferred RevMiner's results to Yelp's, particularly for conjunctive queries (e.g., "great food and huge portions"). Demonstrations of RevMiner are available at revminer.com.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–12},
numpages = {10},
keywords = {information extraction, opinion mining, information retrieval, user reviews},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380121,
author = {Marquardt, Nicolai and Hinckley, Ken and Greenberg, Saul},
title = {Cross-Device Interaction via Micro-Mobility and f-Formations},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380121},
doi = {10.1145/2380116.2380121},
abstract = {GroupTogether is a system that explores cross-device interaction using two sociological constructs. First, F-formations concern the distance and relative body orientation among multiple users, which indicate when and how people position themselves as a group. Second, micro-mobility describes how people orient and tilt devices towards one another to promote fine-grained sharing during co-present collaboration. We sense these constructs using: (a) a pair of overhead Kinect depth cameras to sense small groups of people, (b) low-power 8GHz band radio modules to establish the identity, presence, and coarse-grained relative locations of devices, and (c) accelerometers to detect tilting of slate devices. The resulting system supports fluid, minimally disruptive techniques for co-located collaboration by leveraging the proxemics of people as well as the proxemics of devices.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–22},
numpages = {10},
keywords = {proxemics, micro-mobility, sensors, tablets, f-formations},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380122,
author = {Lasecki, Walter and Miller, Christopher and Sadilek, Adam and Abumoussa, Andrew and Borrello, Donato and Kushalnagar, Raja and Bigham, Jeffrey},
title = {Real-Time Captioning by Groups of Non-Experts},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380122},
doi = {10.1145/2380116.2380122},
abstract = {Real-time captioning provides deaf and hard of hearing people immediate access to spoken language and enables participation in dialogue with others. Low latency is critical because it allows speech to be paired with relevant visual cues. Currently, the only reliable source of real-time captions are expensive stenographers who must be recruited in advance and who are trained to use specialized keyboards. Automatic speech recognition (ASR) is less expensive and available on-demand, but its low accuracy, high noise sensitivity, and need for training beforehand render it unusable in real-world situations. In this paper, we introduce a new approach in which groups of non-expert captionists (people who can hear and type) collectively caption speech in real-time on-demand. We present Legion:Scribe, an end-to-end system that allows deaf people to request captions at any time. We introduce an algorithm for merging partial captions into a single output stream in real-time, and a captioning interface designed to encourage coverage of the entire audio stream. Evaluation with 20 local participants and 18 crowd workers shows that non-experts can provide an effective solution for captioning, accurately covering an average of 93.2% of an audio stream with only 10 workers and an average per-word latency of 2.9 seconds. More generally, our model in which multiple workers contribute partial inputs that are automatically merged in real-time may be extended to allow dynamic groups to surpass constituent individuals (even experts) on a variety of human performance tasks.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–34},
numpages = {12},
keywords = {transcription, crowdsourcing, hard of hearing, text alignment, captioning, real-time, deaf},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380123,
author = {Ramakers, Raf and Vanacken, Davy and Luyten, Kris and Coninx, Karin and Sch\"{o}ning, Johannes},
title = {Carpus: A Non-Intrusive User Identification Technique for Interactive Surfaces},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380123},
doi = {10.1145/2380116.2380123},
abstract = {Interactive surfaces have great potential for co-located collaboration because of their ability to track multiple inputs simultaneously. However, the multi-user experience on these devices could be enriched significantly if touch points could be associated with a particular user. Existing approaches to user identification are intrusive, require users to stay in a fixed position, or suffer from poor accuracy. We present a non-intrusive, high-accuracy technique for mapping touches to their corresponding user in a collaborative environment. By mounting a high-resolution camera above the interactive surface, we are able to identify touches reliably without any extra instrumentation, and users are able to move around the surface at will. Our technique, which leverages the back of users' hands as identifiers, supports walk-up-and-use situations in which multiple people interact on a shared surface.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {35–44},
numpages = {10},
keywords = {multi-touch interaction, user identification, multi-user applications, surface computing, interactive tabletops},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380124,
author = {Cross, Andrew and Cutrell, Edward and Thies, William},
title = {Low-Cost Audience Polling Using Computer Vision},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380124},
doi = {10.1145/2380116.2380124},
abstract = {Electronic response systems known as "clickers" have demonstrated educational benefits in well-resourced classrooms, but remain out-of-reach for most schools due to their prohibitive cost. We propose a new, low-cost technique that utilizes computer vision for real-time polling of a classroom. Our approach allows teachers to ask a multiple-choice question. Students respond by holding up a qCard: a sheet of paper that contains a printed code, similar to a QR code, encoding their student IDs. Students indicate their answers (A, B, C or D) by holding the card in one of four orientations. Using a laptop and an off-the-shelf webcam, our software automatically recognizes and aggregates the students' responses and displays them to the teacher. We built this system and performed initial trials in secondary schools in Bangalore, India. In a 25-student classroom, our system offers 99.8% recognition accuracy, captures 97% of responses within 10 seconds, and costs 15 times less than existing electronic solutions.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {45–54},
numpages = {10},
keywords = {electronic response system, education, ict4d, audience polling, clickers, low-cost},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380125,
author = {Rzeszotarski, Jeffrey and Kittur, Aniket},
title = {CrowdScape: Interactively Visualizing User Behavior and Output},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380125},
doi = {10.1145/2380116.2380125},
abstract = {Crowdsourcing has become a powerful paradigm for accomplishing work quickly and at scale, but involves significant challenges in quality control. Researchers have developed algorithmic quality control approaches based on either worker outputs (such as gold standards or worker agreement) or worker behavior (such as task fingerprinting), but each approach has serious limitations, especially for complex or creative work. Human evaluation addresses these limitations but does not scale well with increasing numbers of workers. We present CrowdScape, a system that supports the human evaluation of complex crowd work through interactive visualization and mixed initiative machine learning. The system combines information about worker behavior with worker outputs, helping users to better understand and harness the crowd. We describe the system and discuss its utility through grounded case studies. We explore other contexts where CrowdScape's visualizations might be useful, such as in user studies.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–62},
numpages = {8},
keywords = {crowdsourcing, visualization, user behavior, interfaces, event logging, performance, quality control},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251556,
author = {Brandt, Joel},
title = {Session Details: Tutorials &amp; Learning},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251556},
doi = {10.1145/3251556},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380127,
author = {Talton, Jerry and Yang, Lingfeng and Kumar, Ranjitha and Lim, Maxine and Goodman, Noah and M\v{e}ch, Radom\'{\i}r},
title = {Learning Design Patterns with Bayesian Grammar Induction},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380127},
doi = {10.1145/2380116.2380127},
abstract = {Design patterns have proven useful in many creative fields, providing content creators with archetypal, reusable guidelines to leverage in projects. Creating such patterns, however, is a time-consuming, manual process, typically relegated to a few experts in any given domain. In this paper, we describe an algorithmic method for learning design patterns directly from data using techniques from natural language processing and structured concept learning. Given a set of labeled, hierarchical designs as input, we induce a probabilistic formal grammar over these exemplars. Once learned, this grammar encodes a set of generative rules for the class of designs, which can be sampled to synthesize novel artifacts. We demonstrate the method on geometric models and Web pages, and discuss how the learned patterns can drive new interaction mechanisms for content creators.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {63–74},
numpages = {12},
keywords = {grammar induction, design patterns},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380128,
author = {Wu, Min and Bhowmick, Arin and Goldberg, Joseph},
title = {Adding Structured Data in Unstructured Web Chat Conversation},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380128},
doi = {10.1145/2380116.2380128},
abstract = {Web chat is becoming the primary customer contact channel in customer relationship management (CRM), and Question/Answer/Lookup (QAL) is the dominant communication pattern in CRM agent-to-customer chat. Text-based web chat for QAL has two main usability problems. Chat transcripts between agents and customers are not tightly integrated into agent-side applications, requiring customer service agents to re-enter customer typed data. Also, sensitive information posted in chat sessions in plain text raises security concerns. The addition of web form widgets to web chat not only solves both of these problems but also adds new usability benefits to QAL. Forms can be defined beforehand or, more flexibly, dynamically composed. Two preliminary user studies were conducted. An agent-side study showed that adding inline forms to web chat decreased overall QAL completion time by 47 percent and increased QAL accuracy by removing all potential human errors. A customer-side study showed that web chat with inline forms is intuitive to customers.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–82},
numpages = {8},
keywords = {crm, human performance, web chat},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380129,
author = {Banovic, Nikola and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
title = {Waken: Reverse Engineering Usage Information and Interface Structure from Software Videos},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380129},
doi = {10.1145/2380116.2380129},
abstract = {We present Waken, an application-independent system that recognizes UI components and activities from screen captured videos, without any prior knowledge of that application. Waken can identify the cursors, icons, menus, and tooltips that an application contains, and when those items are used. Waken uses frame differencing to identify occurrences of behaviors that are common across graphical user interfaces. Candidate templates are built, and then other occurrences of those templates are identified using a multi-phase algorithm. An evaluation demonstrates that the system can successfully reconstruct many aspects of a UI without any prior application-dependant knowledge. To showcase the design opportunities that are introduced by having this additional meta-data, we present the Waken Video Player, which allows users to directly interact with UI components that are displayed in the video.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {83–92},
numpages = {10},
keywords = {tutorials, pixel-based reverse engineering, videos},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380130,
author = {Chi, Pei-Yu and Ahn, Sally and Ren, Amanda and Dontcheva, Mira and Li, Wilmot and Hartmann, Bj\"{o}rn},
title = {MixT: Automatic Generation of Step-by-Step Mixed Media Tutorials},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380130},
doi = {10.1145/2380116.2380130},
abstract = {Users of complex software applications often learn concepts and skills through step-by-step tutorials. Today, these tutorials are published in two dominant forms: static tutorials composed of images and text that are easy to scan, but cannot effectively describe dynamic interactions; and video tutorials that show all manipulations in detail, but are hard to navigate. We hypothesize that a mixed tutorial with static instructions and per-step videos can combine the benefits of both formats. We describe a comparative study of static, video, and mixed image manipulation tutorials with 12 participants and distill design guidelines for mixed tutorials. We present MixT, a system that automatically generates step-by-step mixed media tutorials from user demonstrations. MixT segments screencapture video into steps using logs of application commands and input events, applies video compositing techniques to focus on salient infor-mation, and highlights interactions through mouse trails. Informal evaluation suggests that automatically generated mixed media tutorials were as effective in helping users complete tasks as tutorials that were created manually.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–102},
numpages = {10},
keywords = {video, instructions, software tutorials},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380131,
author = {Li, Wei and Grossman, Tovi and Fitzmaurice, George},
title = {GamiCAD: A Gamified Tutorial System for First Time Autocad Users},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380131},
doi = {10.1145/2380116.2380131},
abstract = {We present GamiCAD, a gamified in-product, interactive tutorial system for first time AutoCAD users. We introduce a software event driven finite state machine to model a user's progress through a tutorial, which allows the system to provide real-time feedback and recognize success and failures. GamiCAD provides extensive real-time visual and audio feedback that has not been explored before in the context of software tutorials. We perform an empirical evaluation of GamiCAD, comparing it to an equivalent in-product tutorial system without the gamified components. In an evaluation, users using the gamified system reported higher subjective engagement levels and performed a set of testing tasks faster with a higher completion ratio.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {103–112},
numpages = {10},
keywords = {learning, tutorial, game},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380132,
author = {Laput, Gierad and Adar, Eytan and Dontcheva, Mira and Li, Wilmot},
title = {Tutorial-Based Interfaces for Cloud-Enabled Applications},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380132},
doi = {10.1145/2380116.2380132},
abstract = {Powerful image editing software like Adobe Photoshop and GIMP have complex interfaces that can be hard to master. To help users perform image editing tasks, we introduce tutorial-based applications (tapps) that retain the step-by-step structure and descriptive text of tutorials but can also automatically apply tutorial steps to new images. Thus, tapps can be used to batch process many images automatically, similar to traditional macros. Tapps also support interactive exploration of parameters, automatic variations, and direct manipulation (e.g., selection, brushing). Another key feature of tapps is that they execute on remote instances of Photoshop, which allows users to edit their images on any Web-enabled device. We demonstrate a working prototype system called TappCloud for creating, managing and using tapps. Initial user feedback indicates support for both the interactive features of tapps and their ability to automate image editing. We conclude with a discussion of approaches and challenges of pushing monolithic direct-manipulation GUIs to the cloud.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–122},
numpages = {10},
keywords = {cloud computing, macros, tutorials},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251557,
author = {Harrison, Chris},
title = {Session Details: Hands &amp; Fingers},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251557},
doi = {10.1145/3251557},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380134,
author = {Lyons, Kent and Nguyen, David and Ashbrook, Daniel and White, Sean},
title = {Facet: A Multi-Segment Wrist Worn System},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380134},
doi = {10.1145/2380116.2380134},
abstract = {We present Facet, a multi-display wrist worn system consisting of multiple independent touch-sensitive segments joined into a bracelet. Facet automatically determines the pose of the system as a whole and of each segment individually. It further supports multi-segment touch, yielding a rich set of touch input techniques. Our work builds on these two primitives to allow the user to control how applications use segments alone and in coordination. Applications can expand to use more segments, collapses to encompass fewer, and be swapped with other segments. We also explore how the concepts from Facet could apply to other devices in this design space.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {123–130},
numpages = {8},
keywords = {wearable, watch, multi-segment touch, multi-display, bracelet},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380135,
author = {Ogata, Masa and Sugiura, Yuta and Osawa, Hirotaka and Imai, Michita},
title = {IRing: Intelligent Ring Using Infrared Reflection},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380135},
doi = {10.1145/2380116.2380135},
abstract = {We present the iRing, an intelligent input ring device developed for measuring finger gestures and external input. iRing recognizes rotation, finger bending, and external force via an infrared (IR) reflection sensor that leverages skin characteristics such as reflectance and softness. Furthermore, iRing allows using a push and stroke input method, which is popular in touch displays. The ring design has potential to be used as a wearable controller because its accessory shape is socially acceptable, easy to install, and safe, and iRing does not require extra devices. We present examples of iRing applications and discuss its validity as an inexpensive wearable interface and as a human sensing device.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {131–136},
numpages = {6},
keywords = {wearable computing, photo reflectivity, finger gesture, skin sensing, multiple sensors},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380136,
author = {Bi, Xiaojun and Chelba, Ciprian and Ouyang, Tom and Partridge, Kurt and Zhai, Shumin},
title = {Bimanual Gesture Keyboard},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380136},
doi = {10.1145/2380116.2380136},
abstract = {Gesture keyboards represent an increasingly popular way to input text on mobile devices today. However, current gesture keyboards are exclusively unimanual. To take advantage of the capability of modern multi-touch screens, we created a novel bimanual gesture text entry system, extending the gesture keyboard paradigm from one finger to multiple fingers. To address the complexity of recognizing bimanual gesture, we designed and implemented two related interaction methods, finger-release and space-required, both based on a new multi-stroke gesture recognition algorithm. A formal experiment showed that bimanual gesture behaviors were easy to learn. They improved comfort and reduced the physical demand relative to unimanual gestures on tablets. The results indicated that these new gesture keyboards were valuable complements to unimanual gesture and regular typing keyboards.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {137–146},
numpages = {10},
keywords = {text entry, touch screen},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380137,
author = {Yang, Xing-Dong and Grossman, Tovi and Wigdor, Daniel and Fitzmaurice, George},
title = {Magic Finger: Always-Available Input through Finger Instrumentation},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380137},
doi = {10.1145/2380116.2380137},
abstract = {We present Magic Finger, a small device worn on the fingertip, which supports always-available input. Magic Finger inverts the typical relationship between the finger and an interactive surface: with Magic Finger, we instrument the user's finger itself, rather than the surface it is touching. Magic Finger senses touch through an optical mouse sensor, enabling any surface to act as a touch screen. Magic Finger also senses texture through a micro RGB camera, allowing contextual actions to be carried out based on the particular surface being touched. A technical evaluation shows that Magic Finger can accurately sense 22 textures with an accuracy of 98.9%. We explore the interaction design space enabled by Magic Finger, and implement a number of novel interaction techniques that leverage its unique capabilities.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {147–156},
numpages = {10},
keywords = {gesture input, touch input, contextual action, always-available input, texture reorganization},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380138,
author = {Gooch, Daniel and Watts, Leon},
title = {YourGloves, Hothands and Hotmits: Devices to Hold Hands at a Distance},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380138},
doi = {10.1145/2380116.2380138},
abstract = {There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of hand-holding prototypes. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that the combined evocative power of unique co-created physical representations of the absent other can be used by separated lovers to generate powerful and positive experiences, in turn sustaining romantic connections at a distance.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {157–166},
numpages = {10},
keywords = {design, intimate communication, communication systems, hand holding},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380139,
author = {Kim, David and Hilliges, Otmar and Izadi, Shahram and Butler, Alex D. and Chen, Jiawen and Oikonomidis, Iason and Olivier, Patrick},
title = {Digits: Freehand 3D Interactions Anywhere Using a Wrist-Worn Gloveless Sensor},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380139},
doi = {10.1145/2380116.2380139},
abstract = {Digits is a wrist-worn sensor that recovers the full 3D pose of the user's hand. This enables a variety of freehand interactions on the move. The system targets mobile settings, and is specifically designed to be low-power and easily reproducible using only off-the-shelf hardware. The electronics are self-contained on the user's wrist, but optically image the entirety of the user's hand. This data is processed using a new pipeline that robustly samples key parts of the hand, such as the tips and lower regions of each finger. These sparse samples are fed into new kinematic models that leverage the biomechanical constraints of the hand to recover the 3D pose of the user's hand. The proposed system works without the need for full instrumentation of the hand (for example using data gloves), additional sensors in the environment, or depth cameras which are currently prohibitive for mobile scenarios due to power and form-factor considerations. We demonstrate the utility of Digits for a variety of application scenarios, including 3D spatial interaction with mobile devices, eyes-free interaction on-the-move, and gaming. We conclude with a quantitative and qualitative evaluation of our system, and discussion of strengths, limitations and future work.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {167–176},
numpages = {10},
keywords = {wearables, 3d interaction, hand tracking, mobile},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251558,
author = {Gajos, Krzysztof},
title = {Session Details: Toolkits},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251558},
doi = {10.1145/3251558},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380141,
author = {Pham, Hubert and Mazzola Paluska, Justin and Miller, Rob and Ward, Steve},
title = {Clui: A Platform for Handles to Rich Objects},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380141},
doi = {10.1145/2380116.2380141},
abstract = {On the desktop, users are accustomed to having visible handles to objects that they want to organize, share, or manipulate. Web applications today feature many classes of such objects, like flight itineraries, products for sale, people, recipes, and businesses, but there are no interoperable handles for high-level semantic objects that users can grab. This paper proposes Clui, a platform for exploring a new data type, called a Webit, that provides uniform handles to rich objects. Clui uses plugins to 1) create Webits on existing pages by extracting semantic data from those pages, and 2) augmenting existing sites with drag and drop targets that accept and interpret Webits. Users drag and drop Webits between sites to transfer data, auto-fill search forms, map associated locations, or share Webits with others. Clui enables experimentation with handles to semantic objects and the standards that underlie them.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {177–188},
numpages = {12},
keywords = {handles, cloud, semantic web},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380142,
author = {Kato, Jun and McDirmid, Sean and Cao, Xiang},
title = {DejaVu: Integrated Support for Developing Interactive Camera-Based Programs},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380142},
doi = {10.1145/2380116.2380142},
abstract = {The increasing popularity of interactive camera-based programs highlights the inadequacies of conventional IDEs in developing these programs given their distinctive attributes and workflows. We present DejaVu, an IDE enhancement that eases the development of these programs by enabling programmers to visually and continuously monitor program data in consistency with the frame-based pipeline of computer-vision programs; and to easily record, review, and reprocess temporal data to iteratively improve the processing of non-reproducible camera input. DejaVu was positively received by three experienced programmers of interactive camera-based programs in our preliminary user trial.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {189–196},
numpages = {8},
keywords = {development environment, computer vision},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380143,
author = {Mortier, Richard and Rodden, Tom and Tolmie, Peter and Lodge, Tom and Spencer, Robert and crabtree, Andy and Sventek, Joe and Koliousis, Alexandros},
title = {Homework: Putting Interaction into the Infrastructure},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380143},
doi = {10.1145/2380116.2380143},
abstract = {This paper presents a user driven redesign of the domestic network infrastructure that draws upon a series of ethnographic studies of home networks. We present an infrastructure based around a purpose built access point that has modified the handling of protocols and services to reflect the interactive needs of the home. The developed infrastructure offers a novel measurement framework that allows a broad range of infrastructure information to be easily captured and made available to interactive applications. This is complemented by a diverse set of novel interactive control mechanisms and interfaces for the underlying infrastructure. We also briefly reflect on the technical and user issues arising from deployments.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {197–206},
numpages = {10},
keywords = {home network, infrastructure, interaction},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380144,
author = {Abouzied, Azza and Hellerstein, Joseph and Silberschatz, Avi},
title = {DataPlay: Interactive Tweaking and Example-Driven Correction of Graphical Database Queries},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380144},
doi = {10.1145/2380116.2380144},
abstract = {Writing complex queries in SQL is a challenge for users. Prior work has developed several techniques to ease query specification but none of these techniques are applicable to a particularly difficult class of queries: quantified queries. Our hypothesis is that users prefer to specify quantified queries interactively by trial-and-error. We identify two impediments to this form of interactive trial-and-error query specification in SQL: (i) changing quantifiers often requires global syntactical query restructuring, and (ii) the absence of non-answers from SQL's results makes verifying query correctness difficult. We remedy these issues with DataPlay, a query tool with an underlying graphical query language, a unique data model and a graphical interface. DataPlay provides two interaction features that support trial-and-error query specification. First, DataPlay allows users to directly manipulate a graphical query by changing quantifiers and modifying dependencies between constraints. Users receive real-time feedback in the form of updated answers and non-answers. Second, DataPlay can auto-correct a user's query, based on user feedback about which tuples to keep or drop from the answers and non-answers. We evaluated the effectiveness of each interaction feature with a user study and we found that direct query manipulation is more effective than auto-correction for simple queries but auto-correction is more effective than direct query manipulation for more complex queries.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {207–218},
numpages = {12},
keywords = {query specification, query correction, quantification, semantic fine-tuning},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380145,
author = {Wightman, Doug and Ye, Zi and Brandt, Joel and Vertegaal, Roel},
title = {SnipMatch: Using Source Code Context to Enhance Snippet Retrieval and Parameterization},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380145},
doi = {10.1145/2380116.2380145},
abstract = {Programmers routinely use source code snippets to increase their productivity. However, locating and adapting code snippets to the current context still takes time: for example, variables must be renamed, and dependencies included. We believe that when programmers decide to invest time in creating a new code snippet from scratch, they would also be willing to spend additional effort to make that code snippet configurable and easy to integrate. To explore this insight, we built SnipMatch, a plug-in for the Eclipse IDE. SnipMatch introduces a simple markup that allows snippet authors to specify search patterns and integration instructions. SnipMatch leverages this information, in conjunction with current code context, to improve snippet search and parameterization. For example, when a search query includes local variables, SnipMatch suggests compatible snippets, and automatically adapts them by substituting in these variables. In the lab, we observed that participants integrated snippets faster when using SnipMatch than when using standard Eclipse. Findings from a public deployment to 93 programmers suggest that SnipMatch has become integrated into the work practices of real users.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {219–228},
numpages = {10},
keywords = {prototyping, example-centric development, natural language processing},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380146,
author = {Oney, Stephen and Myers, Brad and Brandt, Joel},
title = {ConstraintJS: Programming Interactive Behaviors for the Web by Integrating Constraints and States},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380146},
doi = {10.1145/2380116.2380146},
abstract = {Interactive behaviors in GUIs are often described in terms of states, transitions, and constraints, where the constraints only hold in certain states. These constraints maintain relationships among objects, control the graphical layout, and link the user interface to an underlying data model. However, no existing Web implementation technology provides direct support for all of these, so the code for maintaining constraints and tracking state may end up spread across multiple languages and libraries. In this paper we describe ConstraintJS, a system that integrates constraints and finite-state machines (FSMs) with Web languages. A key role for the FSMs is to enable and disable constraints based on the interface's current mode, making it possible to write constraints that sometimes hold. We illustrate that constraints combined with FSMs can be a clearer way of defining many interactive behaviors with a series of examples.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {229–238},
numpages = {10},
keywords = {bindings, web development, user interface technology, constraints, finite-state machines},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380147,
author = {Roesner, Franziska and Fogarty, James and Kohno, Tadayoshi},
title = {User Interface Toolkit Mechanisms for Securing Interface Elements},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380147},
doi = {10.1145/2380116.2380147},
abstract = {User interface toolkit research has traditionally assumed that developers have full control of an interface. This assumption is challenged by the mashup nature of many modern interfaces, in which different portions of a single interface are implemented by multiple, potentially mutually distrusting developers (e.g., an Android application embedding a third-party advertisement). We propose considering security as a primary goal for user interface toolkits. We motivate the need for security at this level by examining today's mashup scenarios, in which security and interface flexibility are not simultaneously achieved. We describe a security-aware user interface toolkit architecture that secures interface elements while providing developers with the flexibility and expressivity traditionally desired in a user interface toolkit. By challenging trust assumptions inherent in existing approaches, this architecture effectively addresses important interface-level security concerns.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {239–250},
numpages = {12},
keywords = {user interface toolkits, security},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251559,
author = {Izadi, Shahram},
title = {Session Details: Interactions 1},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251559},
doi = {10.1145/3251559},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380149,
author = {Joshi, Neel and Mehta, Sisil and Drucker, Steven and Stollnitz, Eric and Hoppe, Hugues and Uyttendaele, Matt and Cohen, Michael},
title = {Cliplets: Juxtaposing Still and Dynamic Imagery},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380149},
doi = {10.1145/2380116.2380149},
abstract = {We explore creating ""cliplets"", a form of visual media that juxtaposes still image and video segments, both spatially and temporally, to expressively abstract a moment. Much as in ""cinemagraphs"", the tension between static and dynamic elements in a cliplet reinforces both aspects, strongly focusing the viewer's attention. Creating this type of imagery is challenging without professional tools and training. We develop a set of idioms, essentially spatiotemporal mappings, that characterize cliplet elements, and use these idioms in an interactive system to quickly compose a cliplet from ordinary handheld video. One difficulty is to avoid artifacts in the cliplet composition without resorting to extensive manual input. We address this with automatic alignment, looping optimization and feathering, simultaneous matting and compositing, and Laplacian blending. A key user-interface challenge is to provide affordances to define the parameters of the mappings from input time to output time while maintaining a focus on the cliplet being created. We demonstrate the creation of a variety of cliplet types. We also report on informal feedback as well as a more structured survey of users.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {251–260},
numpages = {10},
keywords = {video textures, cinemagraphs, image and video compositing, image and video editing},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380150,
author = {Pindat, Cyprien and Pietriga, Emmanuel and Chapuis, Olivier and Puech, Claude},
title = {JellyLens: Content-Aware Adaptive Lenses},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380150},
doi = {10.1145/2380116.2380150},
abstract = {Focus+context lens-based techniques smoothly integrate two levels of detail using spatial distortion to connect the magnified region and the context. Distortion guarantees visual continuity, but causes problems of interpretation and focus targeting, partly due to the fact that most techniques are based on statically-defined, regular lens shapes, that result in far-from-optimal magnification and distortion. JellyLenses dynamically adapt to the shape of the objects of interest, providing detail-in-context visualizations of higher relevance by optimizing what regions fall into the focus, context and spatially-distorted transition regions. This both improves the visibility of content in the focus region and preserves a larger part of the context region. We describe the approach and its implementation, and report on a controlled experiment that evaluates the usability of JellyLenses compared to regular fisheye lenses, showing clear performance improvements with the new technique for a multi-scale visual search task.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {261–270},
numpages = {10},
keywords = {multi-scale interfaces, lenses, focus+context},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380151,
author = {Karnik, Abhijit and Martinez Plasencia, Diego and Mayol-Cuevas, Walterio and Subramanian, Sriram},
title = {PiVOT: Personalized View-Overlays for Tabletops},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380151},
doi = {10.1145/2380116.2380151},
abstract = {We present PiVOT, a tabletop system aimed at supporting mixed-focus collaborative tasks. Through two view-zones, PiVOT provides personalized views to individual users while presenting an unaffected and unobstructed shared view to all users. The system supports multiple personalized views which can be present at the same spatial location and yet be only visible to the users it belongs to. The system also allows the creation of personal views that can be either 2D or (auto-stereoscopic) 3D images. We first discuss the motivation and the different implementation principles required for realizing such a system, before exploring different designs able to address the seemingly opposing challenges of shared and personalized views. We then implement and evaluate a sample prototype to validate our design ideas and present a set of sample applications to demonstrate the utility of the system.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {271–280},
numpages = {10},
keywords = {multi-user, tabletop, multi-view, lumisty},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380152,
author = {Chevalier, Fanny and Dragicevic, Pierre and Hurter, Christophe},
title = {Histomages: Fully Synchronized Views for Image Editing},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380152},
doi = {10.1145/2380116.2380152},
abstract = {We present Histomages, a new interaction model for image editing that considers color histograms as spatial rearrangements of image pixels. Users can select pixels on image histograms as they would select image regions and directly manipulate them to adjust their colors. Histomages are also affected by other image tools such as paintbrushes. We explore some possibilities offered by this interaction model, and discuss the four key principles behind it as well as their implications for the design of feature-rich software in general.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {281–286},
numpages = {6},
keywords = {synchronized views, histogram, image editing},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251560,
author = {Vogel, Daniel},
title = {Session Details: Pen},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251560},
doi = {10.1145/3251560},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380154,
author = {Ribeiro, Andre and Igarashi, Takeo},
title = {Sketch-Editing Games: Human-Machine Communication, Game Theory and Applications},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380154},
doi = {10.1145/2380116.2380154},
abstract = {We study uncertainty in graphical-based interaction (with special attention to sketches). We argue that a comprehensive model for the problem must include the interaction participants (and their current beliefs), their possible actions and their past sketches. It's yet unclear how to frame and solve the former problem, considering all the latter elements. We suggest framing the problem as a game and solving it with a game-theoretical solution, which leads to a framework for the design of new two-way, sketch-based user interfaces. In special, we use the framework to design a game that can progressively learn visual models of objects from user sketches, and use the models in real-world interactions. Instead of an abstract visual criterion, players in this game learn models to optimize interaction (the game's duration). This two-way sketching game addresses problems essential in emerging interfaces (such as learning and how to deal with interpretation errors). We review possible applications in robotic sketch-to-command, hand gesture recognition, media authoring and visual search, and evaluate two. Evaluations demonstrate how players improve performance with repeated play, and the influence of interaction aspects on learning.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {287–298},
numpages = {12},
keywords = {machine learning, communication, cooperation, sketch recognition, active learning, game theory},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380155,
author = {Tsandilas, Theophanis},
title = {Interpreting Strokes on Paper with a Mobile Assistant},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380155},
doi = {10.1145/2380116.2380155},
abstract = {Digital pen technology has allowed for the easy transfer of pen data from paper to the computer. However, linking handwritten content with the digital world remains a hard problem as it requires the translation of unstructured and highly personal vocabularies into structured ones that computers can easily understand and process. Automatic recognition can help to this direction, but as it is not always reliable, solutions require the active cooperation between users and recognition algorithms. This work examines the use of portable touch-screen devices in connection with pen and paper to help users direct and refine the interpretation of their strokes on paper. We explore four techniques of bi-manual interaction that combine touch and pen-writing, where user attention is divided between the original strokes on paper and their interpretation by the electronic device. We demonstrate the techniques through a mobile interface for writing music that complements the automatic recognition with interactive user-driven interpretation. An experiment evaluates the four techniques and provides insights about their strengths and limitations.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {299–308},
numpages = {10},
keywords = {interactive recognition, mobile, music interfaces., pen + touch, bimanual interaction, paper interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380156,
author = {Hamilton, William and Kerne, Andruid and Robbins, Tom},
title = {High-Performance Pen + Touch Modality Interactions: A Real-Time Strategy Game ESports Context},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380156},
doi = {10.1145/2380116.2380156},
abstract = {We used the situated context of real-time strategy (RTS) games to address the design and evaluation of new pen + touch interaction techniques. RTS play is a popular genre of Electronic Sports (eSports), games played and spectated at an extremely high level. Interaction techniques are critical for eSports players, because they so directly impact performance.Through this process, new techniques and implications for pen + touch and bi-manual interaction emerged. We enhance non-dominant hand (NDH) interaction with edge-constrained affordances, anchored to physical features of interactive sur- faces, effectively increasing target width. We develop bi-manual overloading, an approach to reduce the total number of occurrences of NDH retargeting. The novel isosceles lasso select technique facilitates selection of complex object subsets. Pen-in-hand interaction, dominant hand touch interaction performed with the pen stowed in the palm, also emerged as an efficient and expressive interaction paradigm.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {309–318},
numpages = {10},
keywords = {esports, bi-manual interaction, game design},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380157,
author = {Liang, Rong-Hao and Cheng, Kai-Yin and Su, Chao-Huai and Weng, Chien-Ting and Chen, Bing-Yu and Yang, De-Nian},
title = {GaussSense: Attachable Stylus Sensing Using Magnetic Sensor Grid},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380157},
doi = {10.1145/2380116.2380157},
abstract = {This work presents GaussSense, which is a back-of-device sensing technique for enabling input on an arbitrary surface using stylus by exploiting magnetism. A 2mm-thick Hall sensor grid is developed to sense magnets that are embedded in the stylus. Our system can sense the magnetic field that is emitted from the stylus when it is within 2cm of any non-ferromagnetic surface. Attaching the sensor behind an arbitrary thin surface enables the stylus input to be recognized by analyzing the distribution of the applied magnetic field. Attaching the sensor grid to the back of a touchscreen device and incorporating magnets into the corresponding stylus enable the system 1) to distinguish touch events that are caused by a finger from those caused by the stylus, 2) to sense the tilt angle of the stylus and the pressure with which it is applied, and 3) to detect where the stylus hovers over the screen. A pilot study reveals that people were satisfied with the novel sketching experiences based on this system.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {319–326},
numpages = {8},
keywords = {sensor, magnetism, touchscreen, stylus},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380158,
author = {Liu, Shenwei and Guimbreti\`{e}re, Fran\c{c}ois},
title = {FlexAura: A Flexible near-Surface Range Sensor},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380158},
doi = {10.1145/2380116.2380158},
abstract = {The availability of flexible capacitive sensors that can be fitted around mice, smartphones, and pens carries great potential in leveraging grasp as a new interaction modality. Unfortunately, most capacitive sensors only track interaction directly on the surface, making it harder to differentiate among grips and constraining user movements. We present a new optical range sensor design based on high power infrared LEDs and photo-transistors, which can be fabricat-ed on a flexible PCB and wrapped around a wide variety of graspable objects including pens, mice, smartphones, and slates. Our sensor offers a native resolution of 10 dpi with a sensing range of up to 30mm (1.2"") and sampling speed of 50Hz. Based on our prototype wrapped around the barrel of a pen, we present a summary of the characteristics of the sensor and describe the sensor output in several typical pen grips. Our design is versatile enough to apply not only to pens but to a wide variety of graspable objects including smartphones and slates.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {327–330},
numpages = {4},
keywords = {pen ui, ir range sensing, touch sensors, graspable ui},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380159,
author = {Lee, David and Son, KyoungHee and Lee, Joon Hyub and Bae, Seok-Hyung},
title = {PhantomPen: Virtualization of Pen Head for Digital Drawing Free from Pen Occlusion &amp; Visual Parallax},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380159},
doi = {10.1145/2380116.2380159},
abstract = {We present PhantomPen, a direct pen input device whose pen head is virtualized onto the tablet display surface and visually connected to a graspable pen barrel in order to achieve digital drawing free from pen occlusion and visual parallax. As the pen barrel approaches the display, the virtual pen head smoothly appears as if the rendered virtual pen head and the physical pen barrel are in unity. The virtual pen head provides visual feedback by changing its virtual form according to pen type, color, and thickness while the physical pen tip, hidden in the user's sight, provides tactile feedback. Three experiments were carefully designed based on an analysis of drawings by design professionals and observations of design drawing classes. With these experiments that simulate natural drawing we proved significant performance advantages of PhantomPen. PhantomPen was at least as usable as the normal stylus in basic line drawing, and was 17 % faster in focus region drawing (26% faster in extreme focus region drawing). PhantomPen also reduced error rate by 40 % in a typical drawing setup where users have to manage a complex combination of pen and stroke properties.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {331–340},
numpages = {10},
keywords = {visual feedback, pen, parallax, occlusion},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251561,
author = {Olsen, Dan},
title = {Session Details: Interactions II},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251561},
doi = {10.1145/3251561},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380161,
author = {Quinn, Philip and Cockburn, Andy and Casiez, G\'{e}ry and Roussel, Nicolas and Gutwin, Carl},
title = {Exposing and Understanding Scrolling Transfer Functions},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380161},
doi = {10.1145/2380116.2380161},
abstract = {Scrolling is controlled through many forms of input devices, such as mouse wheels, trackpad gestures, arrow keys, and joysticks. Performance with these devices can be adjusted by introducing variable transfer functions to alter the range of expressible speed, precision, and sensitivity. However, existing transfer functions are typically "black boxes" bundled into proprietary operating systems and drivers. This presents three problems for researchers: (1) a lack of knowledge about the current state of the field; (2) a difficulty in replicating research that uses scrolling devices; and (3) a potential experimental confound when evaluating scrolling devices and techniques. These three problems are caused by gaps in researchers' knowledge about what device and movement factors are important for scrolling transfer functions, and about how existing devices and drivers use these factors. We fill these knowledge gaps with a framework of transfer function factors for scrolling, and a method for analysing proprietary transfer functions---demonstrating how state of the art commercial devices accommodate some of the human control phenomena observed in prior studies.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {341–350},
numpages = {10},
keywords = {control-display gain, scroll acceleration, transfer functions, scrolling},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380162,
author = {Roussel, Nicolas and Casiez, G\'{e}ry and Aceituno, Jonathan and Vogel, Daniel},
title = {Giving a Hand to the Eyes: Leveraging Input Accuracy for Subpixel Interaction},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380162},
doi = {10.1145/2380116.2380162},
abstract = {We argue that the current practice of using integer positions for pointing events artificially constrains human precision capabilities. The high sensitivity of current input devices can be harnessed to enable precise direct manipulation ""in between"" pixels, called subpixel interaction. We provide detailed analysis of subpixel theory and implementation, including the critical component of revised control-display gain transfer functions. A prototype implementation is described with several illustrative examples. Guidelines for subpixel domain applicability are provided and an overview of required changes to operating systems and graphical user interface frameworks are discussed.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {351–358},
numpages = {8},
keywords = {display density, direct manipulation, indirect pointing, subpixel interaction, input device sensitivity, device's human resolution},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380163,
author = {Rubin, Steve and Berthouzoz, Floraine and Mysore, Gautham and Li, Wilmot and Agrawala, Maneesh},
title = {UnderScore: Musical Underlays for Audio Stories},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380163},
doi = {10.1145/2380116.2380163},
abstract = {Audio producers often use musical underlays to emphasize key moments in spoken content and give listeners time to reflect on what was said. Yet, creating such underlays is time-consuming as producers must carefully (1) mark an emphasis point in the speech (2) select music with the appropriate style, (3) align the music with the emphasis point, and (4) adjust dynamics to produce a harmonious composition. We present UnderScore, a set of semi-automated tools designed to facilitate the creation of such underlays. The producer simply marks an emphasis point in the speech and selects a music track. UnderScore automatically refines, aligns and adjusts the speech and music to generate a high-quality underlay. UnderScore allows producers to focus on the high-level design of the underlay; they can quickly try out a variety of music and test different points of emphasis in the story. Amateur producers, who may lack the time or skills necessary to author underlays, can quickly add music to their stories. An informal evaluation of UnderScore suggests that it can produce high-quality underlays for a variety of examples while significantly reducing the time and effort required of radio producers.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {359–366},
numpages = {8},
keywords = {radio, music, storytelling, audio editing},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380164,
author = {Ahmed, Faisal and Borodin, Yevgen and Soviak, Andrii and Islam, Muhammad and Ramakrishnan, I.V. and Hedgpeth, Terri},
title = {Accessible Skimming: Faster Screen Reading of Web Pages},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380164},
doi = {10.1145/2380116.2380164},
abstract = {In our information-driven web-based society, we are all gradually falling ""victims"" to information overload [5].However, while sighted people are finding ways to sift through information faster, Internet users who are blind are experiencing an even greater information overload. These people access computers and Internet using screen-reader software, which reads the information on a computer screen sequentially using computer-generated speech. While sighted people can learn how to quickly glance over the headlines and news articles online to get the gist of information, people who are blind have to use keyboard shortcuts to listen through the content narrated by a serial audio interface. This interface does not give them an opportunity to know what content to skip and what to listen to. So, they either listen to all of the content or listen to the first part of each sentence or paragraph before they skip to the next one. In this paper, we propose an automated approach to facilitate non-visual skimming of web pages. We describe the underlying algorithm, outline a non-visual skimming interface, and report on the results of automated experiments, as well as on our user study with 23 screen-reader users. The results of the experiments suggest that we have been moderately successful in designing a viable algorithm for automatic summarization that could be used for non-visual skimming. In our user studies, we confirmed that people who are blind could read and search through online articles faster and were able to understand and remember most of what they have read with our skimming system. Finally, all 23 participants expressed genuine interest in using non-visual skimming in the future.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {367–378},
numpages = {12},
keywords = {screen reader, assistive technology, skimming, blind, speed reading, accessibility, summarization},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251562,
author = {Avrahami, Daniel},
title = {Session Details: Augmented Reality},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251562},
doi = {10.1145/3251562},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380166,
author = {Schmidt, Dominik and Molyneaux, David and Cao, Xiang},
title = {PICOntrol: Using a Handheld Projector for Direct Control of Physical Devices through Visible Light},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380166},
doi = {10.1145/2380116.2380166},
abstract = {Today's environments are populated with a growing number of electric devices which come in diverse form factors and provide a plethora of functions. However, rich interaction with these devices can become challenging if they need be controlled from a distance, or are too small to accommodate user interfaces on their own. In this work, we explore PICOntrol, a new approach using an off-the-shelf handheld pico projector for direct control of physical devices through visible light. The projected image serves a dual purpose by simultaneously presenting a visible interface to the user, and transmitting embedded control information to inexpensive sensor units integrated with the devices. To use PICOntrol, the user points the handheld projector at a target device, overlays a projected user interface on its sensor unit, and performs various GUI-style or gestural interactions. PICOntrol enables direct, visible, and rich interactions with various physical devices without requiring central infrastructure. We present our prototype implementation as well as explorations of its interaction space through various application examples.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {379–388},
numpages = {10},
keywords = {physical devices, visible light communication, handheld projector},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380167,
author = {Gupta, Ankit and Fox, Dieter and Curless, Brian and Cohen, Michael},
title = {DuploTrack: A Real-Time System for Authoring and Guiding Duplo Block Assembly},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380167},
doi = {10.1145/2380116.2380167},
abstract = {We demonstrate a realtime system which infers and tracks the assembly process of a snap-together block model using a Kinect® sensor. The inference enables us to build a virtual replica of the model at every step. Tracking enables us to provide context specific visual feedback on a screen by augmenting the rendered virtual model aligned with the physical model. The system allows users to author a new model and uses the inferred assembly process to guide its recreation by others. We propose a novel way of assembly guidance where the next block to be added is rendered in blinking mode with the tracked virtual model on screen. The system is also able to detect any mistakes made and helps correct them by providing appropriate feedback. We focus on assemblies of Duplo® blocks.We discuss the shortcomings of existing methods of guidance - static figures or recorded videos - and demonstrate how our method avoids those shortcomings. We also report on a user study to compare our system with standard figure-based guidance methods found in user manuals. The results of the user study suggest that our method is able to aid users' structural perception of the model better, leads to fewer assembly errors, and reduces model construction time.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {389–402},
numpages = {14},
keywords = {augmented reality, depth camera, tracking, virtual reality, assembly tasks, active visual feedback},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380168,
author = {Sankar, Aditya and Seitz, Steven},
title = {Capturing Indoor Scenes with Smartphones},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380168},
doi = {10.1145/2380116.2380168},
abstract = {In this paper, we present a novel smartphone application designed to easily capture, visualize and reconstruct homes, offices and other indoor scenes. Our application leverages data from smartphone sensors such as the camera, accelerometer, gyroscope and magnetometer to help model the indoor scene. The output of the system is two-fold; first, an interactive visual tour of the scene is generated in real time that allows the user to explore each room and transition between connected rooms. Second, with some basic interactive photogrammetric modeling the system generates a 2D floor plan and accompanying 3D model of the scene, under a Manhattan-world assumption. The approach does not require any specialized equipment or training and is able to produce accurate floor plans.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {403–412},
numpages = {10},
keywords = {visualization, augmented reality, handheld devices and mobile computing, virtual reality},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380169,
author = {Wilson, Andrew and Benko, Hrvoje and Izadi, Shahram and Hilliges, Otmar},
title = {Steerable Augmented Reality with the Beamatron},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380169},
doi = {10.1145/2380116.2380169},
abstract = {Steerable displays use a motorized platform to orient a projector to display graphics at any point in the room. Often a camera is included to recognize markers and other objects, as well as user gestures in the display volume. Such systems can be used to superimpose graphics onto the real world, and so are useful in a number of augmented reality and ubiquitous computing scenarios. We contribute the Beamatron, which advances steerable displays by drawing on recent progress in depth camera-based interactions. The Beamatron consists of a computer-controlled pan and tilt platform on which is mounted a projector and Microsoft Kinect sensor. While much previous work with steerable displays deals primarily with projecting corrected graphics onto a discrete set of static planes, we describe computational techniques that enable reasoning in 3D using live depth data. We show two example applications that are enabled by the unique capabilities of the Beamatron: an augmented reality game in which a player can drive a virtual toy car around a room, and a ubiquitous computing demo that uses speech and gesture to move projected graphics throughout the room.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {413–422},
numpages = {10},
keywords = {ubiquitous computing, steerable displays, depth cameras, augmented reality},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380170,
author = {Held, Robert and Gupta, Ankit and Curless, Brian and Agrawala, Maneesh},
title = {3D Puppetry: A Kinect-Based Interface for 3D Animation},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380170},
doi = {10.1145/2380116.2380170},
abstract = {We present a system for producing 3D animations using physical objects (i.e., puppets) as input. Puppeteers can load 3D models of familiar rigid objects, including toys, into our system and use them as puppets for an animation. During a performance, the puppeteer physically manipulates these puppets in front of a Kinect depth sensor. Our system uses a combination of image-feature matching and 3D shape matching to identify and track the physical puppets. It then renders the corresponding 3D models into a virtual set. Our system operates in real time so that the puppeteer can immediately see the resulting animation and make adjustments on the fly. It also provides 6D virtual camera rev{and lighting} controls, which the puppeteer can adjust before, during, or after a performance. Finally our system supports layered animations to help puppeteers produce animations in which several characters move at the same time. We demonstrate the accessibility of our system with a variety of animations created by puppeteers with no prior animation experience.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {423–434},
numpages = {12},
keywords = {tangible user interface, animation, object tracking},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380171,
author = {Chen, Jiawen and Izadi, Shahram and Fitzgibbon, Andrew},
title = {Kin\^{E}Tre: Animating the World with the Human Body},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380171},
doi = {10.1145/2380116.2380171},
abstract = {Kin\^{E}tre allows novice users to scan arbitrary physical objects and bring them to life in seconds. The fully interactive system allows diverse static meshes to be animated using the entire human body. Traditionally, the process of mesh animation is laborious and requires domain expertise, with rigging specified manually by an artist when designing the character. Kin\^{E}tre makes creating animations a more playful activity, conducted by novice users interactively "at runtime". This paper describes the Kin\^{E}tre system in full, highlighting key technical contributions and demonstrating many examples of users animating meshes of varying shapes and sizes. These include non-humanoid meshes and incomplete surfaces produced by 3D scanning - two challenging scenarios for existing mesh animation systems. Rather than targeting professional CG animators, Kin\^{E}tre is intended to bring mesh animation to a new audience of novice users. We demonstrate potential uses of our system for interactive storytelling and new forms of physical gaming.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {435–444},
numpages = {10},
keywords = {real-time, 3d interfaces, mesh animation, depth cameras},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251563,
author = {Baudisch, Patrick},
title = {Session Details: Multi-Touch},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251563},
doi = {10.1145/3251563},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380173,
author = {Lehtinen, Ville and Oulasvirta, Antti and Salovaara, Antti and Nurmi, Petteri},
title = {Dynamic Tactile Guidance for Visual Search Tasks},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380173},
doi = {10.1145/2380116.2380173},
abstract = {Visual search in large real-world scenes is both time consuming and frustrating, because the search becomes serial when items are visually similar. Tactile guidance techniques can facilitate search by allowing visual attention to focus on a subregion of the scene. We present a technique for dynamic tactile cueing that couples hand position with a scene position and uses tactile feedback to guide the hand actively toward the target. We demonstrate substantial improvements in task performance over a baseline of visual search only, when the scene's complexity increases. Analyzing task performance, we demonstrate that the effect of visual complexity can be practically eliminated through improved spatial precision of the guidance.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {445–452},
numpages = {8},
keywords = {target selection, multimodal interaction, user performance models, tactile guidance, visual search},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380174,
author = {Ng, Albert and Lepinski, Julian and Wigdor, Daniel and Sanders, Steven and Dietz, Paul},
title = {Designing for Low-Latency Direct-Touch Input},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380174},
doi = {10.1145/2380116.2380174},
abstract = {Software designed for direct-touch interfaces often utilize a metaphor of direct physical manipulation of pseudo "real-world" objects. However, current touch systems typically take 50-200ms to update the display in response to a physi-cal touch action. Utilizing a high performance touch de-monstrator, subjects were able to experience touch latencies ranging from current levels down to about 1ms. Our tests show that users greatly prefer lower latencies, and noticea-ble improvement continued well below 10ms. This level of performance is difficult to achieve in commercial compu-ting systems using current technologies. As an alternative, we propose a hybrid system that provides low-fidelity visu-al feedback immediately, followed by high-fidelity visuals at standard levels of latency.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {453–464},
numpages = {12},
keywords = {latency, touch input, multi-touch, user input},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380175,
author = {Weir, Daryl and Rogers, Simon and Murray-Smith, Roderick and L\"{o}chtefeld, Markus},
title = {A User-Specific Machine Learning Approach for Improving Touch Accuracy on Mobile Devices},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380175},
doi = {10.1145/2380116.2380175},
abstract = {We present a flexible Machine Learning approach for learning user-specific touch input models to increase touch accuracy on mobile devices. The model is based on flexible, non-parametric Gaussian Process regression and is learned using recorded touch inputs. We demonstrate that significant touch accuracy improvements can be obtained when either raw sensor data is used as an input or when the device's reported touch location is used as an input, with the latter marginally outperforming the former. We show that learned offset functions are highly nonlinear and user-specific and that user-specific models outperform models trained on data pooled from several users. Crucially, significant performance improvements can be obtained with a small (≈200) number of training examples, easily obtained for a particular user through a calibration game or from keyboard entry data.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {465–476},
numpages = {12},
keywords = {touch, regression, gaussian processes, machine learning, probabilistic modelling},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380176,
author = {Kin, Kenrick and Hartmann, Bj\"{o}rn and DeRose, Tony and Agrawala, Maneesh},
title = {Proton++: A Customizable Declarative Multitouch Framework},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380176},
doi = {10.1145/2380116.2380176},
abstract = {Proton++ is a declarative multitouch framework that allows developers to describe multitouch gestures as regular expressions of touch event symbols. It builds on the Proton framework by allowing developers to incorporate custom touch attributes directly into the gesture description. These custom attributes increase the expressivity of the gestures, while preserving the benefits of Proton: automatic gesture matching, static analysis of conflict detection, and graphical gesture creation. We demonstrate Proton++'s flexibility with several examples: a direction attribute for describing trajectory, a pinch attribute for detecting when touches move towards one another, a touch area attribute for simulating pressure, an orientation attribute for selecting menu items, and a screen location attribute for simulating hand ID. We also use screen location to simulate user ID and enable simultaneous recognition of gestures by multiple users. In addition, we show how to incorporate timing into Proton++ gestures by reporting touch events at a regular time interval. Finally, we present a user study that suggests that users are roughly four times faster at interpreting gestures written using Proton++ than those written in procedural event-handling code commonly used today.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {477–486},
numpages = {10},
keywords = {regular expressions, proton++, gesture tablature, touch events symbols, custom attributes},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380177,
author = {Murugappan, Sundar and Vinayak and Elmqvist, Niklas and Ramani, Karthik},
title = {Extended Multitouch: Recovering Touch Posture and Differentiating Users Using a Depth Camera},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380177},
doi = {10.1145/2380116.2380177},
abstract = {Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user's actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of extended multitouch interaction as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user's exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {487–496},
numpages = {10},
keywords = {tabletop, pen + touch, multitouch, depth camera, evaluation},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380178,
author = {Block, Florian and Wigdor, Daniel and Phillips, Brenda Caldwell and Horn, Michael S. and Shen, Chia},
title = {FlowBlocks: A Multi-Touch Ui for Crowd Interaction},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380178},
doi = {10.1145/2380116.2380178},
abstract = {Multi-touch technology lends itself to collaborative crowd interaction (CI). However, common tap-operated widgets are impractical for CI, since they are susceptible to accidental touches and interference from other users. We present a novel multi-touch interface called FlowBlocks in which every UI action is invoked through a small sequence of user actions: dragging parametric UI-Blocks, and dropping them over operational UI-Docks. The FlowBlocks approach is advantageous for CI because it a) makes accidental touches inconsequential; and b) introduces design parameters for mutual awareness, concurrent input, and conflict management. FlowBlocks was successfully used on the floor of a busy natural history museum. We present the complete design space and describe a year-long iterative design and evaluation process which employed the Rapid Iterative Test and Evaluation (RITE) method in a museum setting.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {497–508},
numpages = {12},
keywords = {multi-touch ui, drag &amp; drop, crowd interaction},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251564,
author = {Wilson, Andy},
title = {Session Details: Tactile &amp; Grip},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251564},
doi = {10.1145/3251564},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380180,
author = {Rendl, Christian and Greindl, Patrick and Haller, Michael and Zirkl, Martin and Stadlober, Barbara and Hartmann, Paul},
title = {PyzoFlex: Printed Piezoelectric Pressure Sensing Foil},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380180},
doi = {10.1145/2380116.2380180},
abstract = {Ferroelectric material supports both pyro- and piezoelectric effects that can be used for sensing pressures on large, bended surfaces. We present PyzoFlex, a pressure-sensing input device that is based on a ferroelectric material. It is constructed with a sandwich structure of four layers that can be printed easily on any material. We use this material in combination with a high-resolution Anoto-sensing foil to support both hand and pen input tracking. The foil is bendable, energy-efficient, and it can be produced in a printing process. Even a hovering mode is feasible due to its pyroelectric effect. In this paper, we introduce this novel input technology and discuss its benefits and limitations.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {509–518},
numpages = {10},
keywords = {piezoelectric, pressure sensing, bendable, input device, pyroelectric},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380181,
author = {Follmer, Sean and Leithinger, Daniel and Olwal, Alex and Cheng, Nadia and Ishii, Hiroshi},
title = {Jamming User Interfaces: Programmable Particle Stiffness and Sensing for Malleable and Shape-Changing Devices},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380181},
doi = {10.1145/2380116.2380181},
abstract = {Malleable and organic user interfaces have the potential to enable radically new forms of interactions and expressiveness through flexible, free-form and computationally controlled shapes and displays. This work, specifically focuses on particle jamming as a simple, effective method for flexible, shape-changing user interfaces where programmatic control of material stiffness enables haptic feedback, deformation, tunable affordances and control gain. We introduce a compact, low-power pneumatic jamming system suitable for mobile devices, and a new hydraulic-based technique with fast, silent actuation and optical shape sensing. We enable jamming structures to sense input and function as interaction devices through two contributed methods for high-resolution shape sensing using: 1) index-matched particles and fluids, and 2) capacitive and electric field sensing. We explore the design space of malleable and organic user interfaces enabled by jamming through four motivational prototypes that highlight jamming's potential in HCI, including applications for tabletops, tablets and for portable shape-changing mobile devices.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {519–528},
numpages = {10},
keywords = {malleable input, jamming, haptic feedback, variable stiffness, organic user interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380182,
author = {Sugiura, Yuta and Inami, Masahiko and Igarashi, Takeo},
title = {A Thin Stretchable Interface for Tangential Force Measurement},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380182},
doi = {10.1145/2380116.2380182},
abstract = {We have developed a simple skin-like user interface that can be easily attached to curved as well as flat surfaces and used to measure tangential force generated by pinching and dragging interactions. The interface consists of several photoreflectors that consist of an IR LED and a phototransistor and elastic fabric such as stocking and rubber membrane. The sensing method used is based on our observation that photoreflectors can be used to measure the ratio of expansion and contraction of a stocking using the changes in transmissivity of IR light passing through the stocking. Since a stocking is thin, stretchable, and nearly transparent, it can be easily attached to various types of objects such as mobile devices, robots, and different parts of the body as well as to various types of conventional pressure sensors without altering the original shape of the object. It can also present natural haptic feedback in accordance with the amount of force exerted. A system using several such sensors can determine the direction of a two-dimensional force. A variety of example applications illustrated the utility of this sensing system.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {529–536},
numpages = {8},
keywords = {transmissivity measurement, soft user interface, tangential force input, multiple sensors},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380183,
author = {Harrison, Chris and Sato, Munehiko and Poupyrev, Ivan},
title = {Capacitive Fingerprinting: Exploring User Differentiation by Sensing Electrical Properties of the Human Body},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380183},
doi = {10.1145/2380116.2380183},
abstract = {At present, touchscreens can differentiate multiple points of contact, but not who is touching the device. In this work, we consider how the electrical properties of humans and their attire can be used to support user differentiation on touchscreens. We propose a novel sensing approach based on Swept Frequency Capacitive Sensing, which measures the impedance of a user to the environment (i.e., ground) across a range of AC frequencies. Different people have different bone densities and muscle mass, wear different footwear, and so on. This, in turn, yields different impedance profiles, which allows for touch events, including multitouch gestures, to be attributed to a particular user. This has many interesting implications for interactive design. We describe and evaluate our sensing approach, demonstrating that the technique has considerable promise. We also discuss limitations, how these might be overcome, and next steps.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {537–544},
numpages = {8},
keywords = {gestures, swept frequency capacitive sensing, user identification, login, collaborative multi-user interaction, touch?, id, touchscreens, finger input, sfcs},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380184,
author = {Goel, Mayank and Wobbrock, Jacob and Patel, Shwetak},
title = {GripSense: Using Built-in Sensors to Detect Hand Posture and Pressure on Commodity Mobile Phones},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380184},
doi = {10.1145/2380116.2380184},
abstract = {We introduce GripSense, a system that leverages mobile device touchscreens and their built-in inertial sensors and vibration motor to infer hand postures including one- or two-handed interaction, use of thumb or index finger, or use on a table. GripSense also senses the amount of pres-sure a user exerts on the touchscreen despite a lack of direct pressure sensors by inferring from gyroscope readings when the vibration motor is "pulsed." In a controlled study with 10 participants, GripSense accurately differentiated device usage on a table vs. in hand with 99.67% accuracy and when in hand, it inferred hand postures with 84.26% accuracy. In addition, GripSense distinguished three levels of pressure with 95.1% accuracy. A usability analysis of GripSense was conducted in three custom applications and showed that pressure input and hand-posture sensing can be useful in a number of scenarios.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {545–554},
numpages = {10},
keywords = {gyroscope, inertial sensors, hand posture, situational impairments, posture, mobile, touchscreen},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380185,
author = {Hoggan, Eve and Stewart, Craig and Haverinen, Laura and Jacucci, Giulio and Lantz, Vuokko},
title = {Pressages: Augmenting Phone Calls with Non-Verbal Messages},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380185},
doi = {10.1145/2380116.2380185},
abstract = {ForcePhone is a mobile synchronous haptic communication system. During phone calls, users can squeeze the side of the device and the pressure level is mapped to vibrations on the recipient's device. The pressure/vibrotactile messages supported by ForcePhone are called pressages. Using a lab-based study and a small field study, this paper addresses the following questions: how can haptic interpersonal communication be integrated into a standard mobile device? What is the most appropriate feedback design for pressages? What types of non-verbal cues can be represented by pressages? Do users make use of pressages during their conversations? The results of this research indicate that such a system has value as a communication channel in real-world settings with users expressing greetings, presence and emotions through pressages.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {555–562},
numpages = {8},
keywords = {pressure input, haptic feedback, squeeze interaction, mobile interpersonal communication},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/3251565,
author = {Cao, Xiang},
title = {Session Details: Fabrication &amp; Hardware},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251565},
doi = {10.1145/3251565},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
numpages = {1},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380187,
author = {Harrison, Chris and Xiao, Robert and Hudson, Scott},
title = {Acoustic Barcodes: Passive, Durable and Inexpensive Notched Identification Tags},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380187},
doi = {10.1145/2380116.2380187},
abstract = {We present acoustic barcodes, structured patterns of physical notches that, when swiped with e.g., a fingernail, produce a complex sound that can be resolved to a binary ID. A single, inexpensive contact microphone attached to a surface or object is used to capture the waveform. We present our method for decoding sounds into IDs, which handles variations in swipe velocity and other factors. Acoustic barcodes could be used for information retrieval or to triggering interactive functions. They are passive, durable and inexpensive to produce. Further, they can be applied to a wide range of materials and objects, including plastic, wood, glass and stone. We conclude with several example applications that highlight the utility of our approach, and a user study that explores its feasibility.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {563–568},
numpages = {6},
keywords = {ubiquitous and pervasive computing, microphones, markers, classification, identification, tags, id, sound, location, vibration, interaction techniques},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380188,
author = {Fourney, Adam and Terry, Michael},
title = {PICL: Portable in-Circuit Learner},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380188},
doi = {10.1145/2380116.2380188},
abstract = {This paper introduces the PICL, the portable in-circuit learner. The PICL explores the possibility of providing standalone, low-cost, programming-by-demonstration machine learning capabilities to circuit prototyping. To train the PICL, users attach a sensor to the PICL, demonstrate example input, then specify the desired output (expressed as a voltage) for the given input. The current version of the PICL provides two learning modes, binary classification and linear regression. To streamline training and also make it possible to train on highly transient signals (such as those produced by a camera flash or a hand clap), the PICL includes a number of input inferencing techniques. These techniques make it possible for the PICL to learn with as few as one example. The PICL's behavioural repertoire can be expanded by means of various output adapters, which serve to transform the output in useful ways when prototyping. Collectively, the PICL's capabilities allow users of systems such as the Arduino or littleBits electronics kit to quickly add basic sensor-based behaviour, with little or no programming required.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {569–578},
numpages = {10},
keywords = {circuits, machine learning, tangible user interface, toolkits},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380189,
author = {Savage, Valkyrie and Zhang, Xiaohan and Hartmann, Bj\"{o}rn},
title = {Midas: Fabricating Custom Capacitive Touch Sensors to Prototype Interactive Objects},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380189},
doi = {10.1145/2380116.2380189},
abstract = {An increasing number of consumer products include user interfaces that rely on touch input. While digital fabrication techniques such as 3D printing make it easier to prototype the shape of custom devices, adding interactivity to such prototypes remains a challenge for many designers. We introduce Midas, a software and hardware toolkit to support the design, fabrication, and programming of flexible capacitive touch sensors for interactive objects. With Midas, designers first define the desired shape, layout, and type of touch sensitive areas, as well as routing obstacles, in a sensor editor. From this high-level specification, Midas automatically generates layout files with appropriate sensor pads and routed connections. These files are then used to fabricate sensors using digital fabrication processes, e.g., vinyl cutters and conductive ink printers. Using step-by-step assembly instructions generated by Midas, designers connect these sensors to the Midas microcontroller, which detects touch events. Once the prototype is assembled, designers can define interactivity for their sensors: Midas supports both record-and-replay actions for controlling existing local applications and WebSocket-based event output for controlling novel or remote applications. In a first-use study with three participants, users successfully prototyped media players. We also demonstrate how Midas can be used to create a number of touch-sensitive interfaces.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {579–588},
numpages = {10},
keywords = {prototyping, design tools, fabrication, capacitive touch sensing},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380190,
author = {Willis, Karl and Brockmeyer, Eric and Hudson, Scott and Poupyrev, Ivan},
title = {Printed Optics: 3D Printing of Embedded Optical Elements for Interactive Devices},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380190},
doi = {10.1145/2380116.2380190},
abstract = {We present an approach to 3D printing custom optical elements for interactive devices labelled Printed Optics. Printed Optics enable sensing, display, and illumination elements to be directly embedded in the casing or mechanical structure of an interactive device. Using these elements, unique display surfaces, novel illumination techniques, custom optical sensors, and embedded optoelectronic components can be digitally fabricated for rapid, high fidelity, highly customized interactive devices. Printed Optics is part of our long term vision for interactive devices that are 3D printed in their entirety. In this paper we explore the possibilities for this vision afforded by fabrication of custom optical elements using today's 3D printing technology.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {589–598},
numpages = {10},
keywords = {3d printing, projection, additive manufacturing, rapid prototyping, display, optics, sensing, light},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

@inproceedings{10.1145/2380116.2380191,
author = {Mueller, Stefanie and Lopes, Pedro and Baudisch, Patrick},
title = {Interactive Construction: Interactive Fabrication of Functional Mechanical Devices},
year = {2012},
isbn = {9781450315807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380116.2380191},
doi = {10.1145/2380116.2380191},
abstract = {Personal fabrication tools, such as laser cutters and 3D printers allow users to create precise objects quickly. However, working through a CAD system removes users from the workpiece. Recent interactive fabrication tools reintroduce this directness, but at the expense of precision.In this paper, we introduce constructable, an interactive drafting table that produces precise physical output in every step. Users interact by drafting directly on the workpiece using a hand-held laser pointer. The system tracks the pointer, beautifies its path, and implements its effect by cutting the workpiece using a fast high-powered laser cutter.Constructable achieves precision through tool-specific constraints, user-defined sketch lines, and by using the laser cutter itself for all visual feedback, rather than using a screen or projection. We demonstrate how constructable allows creating simple but functional devices, including a simple gearbox, that cannot be created with traditional interactive fabrication tools.},
booktitle = {Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {599–606},
numpages = {8},
keywords = {mechanics, sketching, construction, interactive fabrication, laser cutting, rapid prototyping},
location = {Cambridge, Massachusetts, USA},
series = {UIST '12}
}

