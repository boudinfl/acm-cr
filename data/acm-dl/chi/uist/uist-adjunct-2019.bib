@inproceedings{10.1145/3332167.3357095,
author = {Sasaki, Masato and Nagamatsu, Takashi and Takemura, Kentaro},
title = {Cross-Ratio Based Gaze Estimation for Multiple Displays Using a Polarization Camera},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357095},
doi = {10.1145/3332167.3357095},
abstract = {While eye tracking has been typically used for achieving intuitive user interfaces, it is not sufficient when it comes to dealing with multiple-display environments. In such environments, which have become popular recently, the point-of-gaze should be estimated on multiple screens. Therefore, we propose a cross-ratio based gaze estimation using a polarization camera for multiple screens. The point-of-gaze can be estimated on each monitor by identifying the screen reflected on the corneal surface at a polarization angle. Near-infrared light emitting diodes (NIR-LEDs) attached to the display are not required. This means that standard displays can be used with high general versatility as an advantage.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {1–3},
numpages = {3},
keywords = {gaze estimation, multiple displays, polarization},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357096,
author = {Gonzalez, Eric J. and Abtahi, Parastoo and Follmer, Sean},
title = {Evaluating the Minimum Jerk Motion Model for Redirected Reach in Virtual Reality},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357096},
doi = {10.1145/3332167.3357096},
abstract = {Reach redirection in virtual reality uses spatial distortion to augment interaction with passive props as well as active haptic devices. For such dynamic physical systems, motion modeling is needed to update the interface based on users' predicted targets &amp; trajectories. However, it remains unclear how well standard predictive models hold under redirection. In this work we evaluate one such commonly used model, the Minimum-Jerk (MJ) model, during redirected reach at various lateral offsets up to 16cm. Results show that larger redirection significantly worsens MJ model fit, suggesting that models should be adjusted for reaches with considerable redirection. Predicted arrival times, based on fitting an MJ model on the first half of reach data, led to an average error of -0.29s for redirected reach, compared to -0.03s for normal reach.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {4–6},
numpages = {3},
keywords = {perception, retargeting, virtual reality, haptics, illusion},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357097,
author = {Coe, Patrick and Evreinov, Grigori and Raisamo, Roope},
title = {Gel-Based Haptic Mediator for High-Definition Tactile Communication},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357097},
doi = {10.1145/3332167.3357097},
abstract = {In an effort to find a way to more efficiently and accurately display localized high definition tactile information to the skin we studied the vibrotactile constructive wave interference properties of silicone gel. By placing two actuators at a given distance from each other and controlling the delay when each actuator is activated in relation to the other, we can achieve a point of constructive wave interference. The time when the interference occurs along with any loses due to attenuation depends on the material the vibration travels through. The goal is to find a compliant material that can be used as a reference to human tissue when designing feedback for assistive robots.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {7–9},
numpages = {3},
keywords = {tangible interfaces, multimodal interaction, user interfaces},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357098,
author = {Kim, Won and Kim, SeungJun},
title = {A New Approach to Studying Sleep in Autonomous Vehicles: Simulating the Waking Situation},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357098},
doi = {10.1145/3332167.3357098},
abstract = {In this paper, we present a novel methodology for simulating the physical and cognitive demands that individuals experience when waking from sleep. Better understanding this scenario has significant implications for research in Autonomous Vehicles (AV), where prior research has shown that many drivers would like to sleep while the vehicle is in operation. Our experiment setup replicates the waking situation in two ways: (1) Subjects wear a sleep shade (physical demand) for 3 sessions (5min, 8min, and 11min) in randomly assigned order, after which (2) they view a screen (cognitive demand) that fades from blurry to clear over a 10s-timeframe. We compared subjects' experiences in-study to the physical and cognitive conditions they experience when waking in real life. Our experiment setup was highly rated in effectiveness and appropriateness for alternating sleeping situation. Findings will be utilized as scenario design in future AV studies and can be adopted in other fields, as well.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {10–12},
numpages = {3},
keywords = {sleep, autonomous vehicle, driving simulation, methodology, situation design},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357099,
author = {Zhelezniakov, Dmytro and Zaytsev, Viktor and Radyvonenko, Olga and Yakishyn, Yevhenii},
title = {InteractivePaper: Minimalism in Document Editing UI Through the Handwriting Prism},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357099},
doi = {10.1145/3332167.3357099},
abstract = {The use of the minimalistic design in the document editing UI based on novel handwriting recognition technologies for mobile devices with a touch screen is very limited in real-world applications, mainly due to the propagation of errors made by recognition engines for different types of input. It is caused by the iterative nature of recognition for diverse input. Currently, one of the common ways is to pre-configure the input type manually for different types of documents' content that leads to an increase in UI complexity. Bearing this in mind, we revisit UI design issues regarding the unification of recognition flow by introducing the document layout analyzer, to avoid switching between modes during input of handwritten documents. The system for the iterative input of diverse document by using handwritten strokes recognition techniques was presented. Using an evaluation study, we have shown that the proposed approach provides a more interactive experience for the user and increases the input speed by 1.5--2 times compared to the classical approaches.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {13–15},
numpages = {3},
keywords = {mobile computing, handwriting recognition, interactive input, handwritten document input, document layout analysis},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357100,
author = {Chung, John Joon Young and Xiao, Fuhu and Banovic, Nikola and Lasecki, Walter S.},
title = {Towards Instantaneous Recovery from Autonomous System Failures via Predictive Crowdsourcing},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357100},
doi = {10.1145/3332167.3357100},
abstract = {Autonomous systems (e.g., long-distance driverless trucks) aim to reduce the need for people to complete tedious tasks. In many domains, automation is challenging because systems may fail to recognize or comprehend all relevant aspects of its current state. When an unknown or uncertain state is encountered in a mission-critical setting, recovery often requires human intervention or hand-off. However, human intervention is associated with decision (and communication, if remote) delays that prevent recovery in low-latency settings. Instantaneous crowdsourcing approaches that leverage predictive techniques reduce this latency by preparing human responses for possible near future states before they occur. Unfortunately, the number of possible future states can be vast and considering all of them is intractable in all but the simplest of settings. Instead, to reduce the number of states that must later be explored, we propose the approach that uses the crowd to first predict the most relevant or likely future states. We examine the latency and accuracy of crowd workers in a simple future state prediction task, and find that more than half of crowd workers were able to provide accurate answers within one second. Our results show that crowd predictions can filter out critical future states in tasks where decisions are required in less than three seconds.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {16–18},
numpages = {3},
keywords = {real-time crowdsourcing, prediction, human computation},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357101,
author = {Asai, Kentaro and Fukusato, Tsukasa and Igarashi, Takeo},
title = {Plotshop: An Interactive System for Designing a 2D Data Distribution on a Scatter Plot},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357101},
doi = {10.1145/3332167.3357101},
abstract = {We introduce an interactive system to design 2D data sets on a scatter plot. 2D data sets are often used to obtain an intuition about the behavior of machine learning techniques or statistical modeling techniques . A traditional method for creating a data set is to use textural commands, such as generating a normal distribution with specific parameters. However, it is difficult to create an arbitrary data distribution from scratch or to edit an existing one using these methods. We therefore developed an interactive system for creating and editing a 2D data distribution on a scatter plot by using a graphics user interface. It also allows the user to observe the response of a data analysis method while the user is editing a distribution. We show that the proposed interface is effective for understanding characteristics of data analysis methods.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {19–20},
numpages = {2},
keywords = {data analysis, interactive data design and edit},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357102,
author = {Tamura, Yuto and Takemura, Kentaro},
title = {Estimating Focused Object Using Smooth Pursuit Eye Movements and Interest Points in the Real World},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357102},
doi = {10.1145/3332167.3357102},
abstract = {User calibration is a significant problem in eye-based interaction. To overcome this, several solutions, such as the calibration-free method and implicit user calibration, have been proposed. Pursuits-based interaction is another such solution that has been studied for public screens and virtual reality. It has been applied to select graphical user interfaces (GUIs) because the movements in a GUI can be designed in advance. Smooth pursuit eye movements (smooth pursuits) occur when a user looks at objects in the physical space as well and thus, we propose a method to identify the focused object by using smooth pursuits in the real world. We attempted to determine the focused objects without prior information under several conditions by using the pursuits-based approach and confirmed the feasibility and limitations of the proposed method through experimental evaluations.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {21–23},
numpages = {3},
keywords = {smooth pursuit eye movement, focused object, eye tracking, real world},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357103,
author = {Goetsu, Shiyoh and Sakai, Tetsuya},
title = {Voice Input Interface Failures and Frustration: Developer and User Perspectives},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357103},
doi = {10.1145/3332167.3357103},
abstract = {We identify different types of failures in a voice user interface application, from both developer and user perspectives. Our preliminary experiment suggests that user-perceived Pattern Match Failure may have a strong negative effect on user frustration; based on this result, we conduct power analysis to obtain more conclusive results in a future experiment.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {24–26},
numpages = {3},
keywords = {user study, dialogue systems, voice input},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357104,
author = {Kim, Taeyong and Kim, Sanghong and Choi, Joonhee and Lee, Youngsun and Lee, Bowon},
title = {Say and Find It: A Multimodal Wearable Interface for People with Visual Impairment},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357104},
doi = {10.1145/3332167.3357104},
abstract = {Recent advances in computer vision and natural language processing using deep neural networks (DNNs) have enabled rich and intuitive multimodal interfaces. However, research on intelligent assistance systems for persons with visual impairment has not been well explored. In this work, we present an interactive object recognition and guidance interface based on multimodal interaction for blind and partially sighted people using an embedded mobile device. We demonstrate that the proposed solution using DNNs can effectively assist visually impaired people. We believe that this work will provide new and helpful insights for designing intelligent assistance systems in the future.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {27–29},
numpages = {3},
keywords = {assistive system, multimodal wearable interface, mobile interface, visual impairment},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357105,
author = {Suzuki, Ryo and Nakayama, Ryosuke and Liu, Dan and Kakehi, Yasuaki and Gross, Mark D. and Leithinger, Daniel},
title = {LiftTiles: Modular and Reconfigurable Room-Scale Shape Displays through Retractable Inflatable Actuators},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357105},
doi = {10.1145/3332167.3357105},
abstract = {This paper introduces LiftTiles, a modular and reconfigurable room-scale shape display. LiftTiles consist of an array of retractable and inflatable actuator that is compact (e.g., 15cm tall) and light (e.g., 1.8kg), while extending up to 1.5m to allow for large-scale shape transformation. Inflatable actuation also provides a robust structure that can support heavy objects (e.g., 10 kg weight). This paper describes the design and implementation of LiftTiles and explores the application space for reconfigurable room-scale shape displays.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {30–32},
numpages = {3},
keywords = {inflatables, pneumatic actuation, shape displays},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357106,
author = {Arakawa, Riku and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
title = {TransVoice: Real-Time Voice Conversion for Augmenting Near-Field Speech Communication},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357106},
doi = {10.1145/3332167.3357106},
abstract = {Despite promising initial studies, a speaker's original voice can cause problems when it comes to the application of real-time voice conversion (data-driven speaker conversion) technology in our daily lives, specifically in our near-field communication, because the overlapping speech degrades the sense of immersion to the converted speech. We present TransVoice, a real-time voice conversion system that physically confines original speech with a mask-shaped device. Our preliminary study shows the proposed device can reduce the volume of original speech significantly, while it ameliorates the deteriorated conversion quality of the deep neural network (DNN) thanks to an integrated filter that weakens the low frequency range. We discuss novel applications using TransVoice that can augment our communication.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {33–35},
numpages = {3},
keywords = {voice conversion, deep neural network, speech communication},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357107,
author = {Hu, Sheng-Pei and Hou, June-Hao},
title = {Pneu-Multi-Tools: Auto-Folding and Multi-Shapes Interface by Pneumatics in Virtual Reality},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357107},
doi = {10.1145/3332167.3357107},
abstract = {Pneu-Multi-Tools is a novel passive haptic feedback (PHF) wearable device that let users grip different shapes of virtual props in virtual reality(VR) with sensing the shape changes of foldable airbags driven by pneumatics. The solution to the limitation of primitive shapes in haptic interfaces for VR in the past is proposed in this research. TPU films can be manufactured into 4 kinds of folding shapes(clip, rectangle, cylinder and cone shape) owing to the numbers and orientation of folding hinges on a single airbag. Therefore, Pneu-Multi-Tools, which is stacked with different folding shapes of airbags and capable of automatically folding, enables users to use multi-props intuitively in VR games. There are 3 interaction scenarios are provided by this interface: "Pick-Up", "Order" and "Hot Key" in multi-props games to make it possible for users to switch the props more efficiently.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {36–38},
numpages = {3},
keywords = {virtual reality, auto-folding, haptics, multi-shapes},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357108,
author = {Peetz, Andreas and Klamka, Konstantin and Dachselt, Raimund},
title = {BodyHub: A Reconfigurable Wearable System for Clothing},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357108},
doi = {10.1145/3332167.3357108},
abstract = {While mobile technologies are moving closer to our body and novel wearable gadgets and smart textile interfaces emerge, current approaches are often expensive individual solutions for specific applications and lack reconfiguration possibilities. With this work, we introduce BodyHub, a modular wearable approach that allows users to realize their own smart garment applications by arranging and configuring exchangeable functional modules. To address individual user requirements and preferences, we developed a comprehensive repertoire of input and output modules that can be placed freely onto slide-in sockets which are imprinted in the textile by using 3D printing. Further, we developed a smartphone companion app that facilitates the creation of user-defined system functions without any programming skills. BodyHub thereby allows the creation of personalized wearable solutions by the users themselves and also supports ad-hoc assemblies for interface design explorations in research labs. To demonstrate the range of possible applications, we describe real-world use-cases from the areas of work life, shopping, mobility, and gaming.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {39–41},
numpages = {3},
keywords = {3d printing, smart fabric, rapid prototyping, modular system, e-textiles, wearable construction kits, wearable},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357109,
author = {Li, Xiangdong and Chen, Wenqian and Wu, Yue},
title = {Distance-Driven User Interface for Collaborative Exhibit Viewing in Augmented Reality Museum},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357109},
doi = {10.1145/3332167.3357109},
abstract = {The rapid advancement of augmented reality technology brings museum visitors enhanced interaction and immersion experience. However, most existing augmented reality museums adopt individual-based user interfaces that hinder joint interaction across multiple users. We present the distance-driven user interface (DUI) to enable collaborative exhibit viewing in augmented reality museum. We classify the users in four groups according to the user-exhibit distance and assign each group specific interaction privileges - put simply, the DUI elicits the users standing far from an exhibit to explore more of the desired exhibit by approaching closer to the exhibit. We describe the DUI architecture and preliminarily evaluate users' acceptance and effectiveness of the DUI and find that, the DUI is interpretable, improves users' awareness of collaboration, and increases user interests to the exhibit with considerably improved willingness of approaching the exhibit.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {42–43},
numpages = {2},
keywords = {augmented reality, distance-driven user interface, collaborative interaction, museum experience, exhibit viewing},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357110,
author = {Hu, Donghan and Lee, Sang Won},
title = {ScreenTrack: Using Visual History for Self-Tracking Computer Activities and Retrieving Working Context},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357110},
doi = {10.1145/3332167.3357110},
abstract = {Reconstructing a previous working context (files, websites and software) when resuming a task is inevitable for computer users after switching software or being interrupted. The process of retrieving relevant applications, files, and web pages can be time-consuming; while users may be able to use a "recent documents" feature to resume work in some applications, it is challenging to find relevant information from a text-based history --- file names, web page titles, or URLs may not be informative enough. To address this problem, we tested the idea of using a visual history of a computer screen through the development of ScreenTrack. ScreenTrack is software that captures an image of a computer screen at regular intervals. It lets a user watch and navigate a time-lapse video made of computer screenshots to retrieve a previous working context from a screenshot. Through a controlled user study, it was found that participants were able to retrieve requested information more quickly with ScreenTrack than the control condition; this difference was statistically significant. Additionally, participants gave positive feedback on possible future uses for this software. We present the software, our preliminary user study result, and a plan for further validation.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {44–46},
numpages = {3},
keywords = {visual history, self-tracking, mental context reconstruction, human computer interaction},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357111,
author = {Ko, Donghyeon and Lee, Yujin and Yim, Jee Bin and Lee, Woohun},
title = {HeatMat: Designing Internal Structures for Supporting Hands-on Design Activity with Heated 3D Printed Objects},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357111},
doi = {10.1145/3332167.3357111},
abstract = {Recently, adopting a hands-on approach to conventional 3D fabrication has been attracting attention due to its advantages in design activity. In this context, we aim to support hands-on design activity in digital fabrication by designing internal structures for alleviating issues of external heating for shape deformation. As a first step, we simulate four simple structures with Computational Fluid Dynamic (CFD) simulation to investigate effective structural parameters such as cavity's ratio, its geometry, exposure to the heat source for influencing thermal properties, and deformation in a malleable state. Through the pilot experiment, we figured out that the simulation results of the basic structures are valid, the structure is stable in a malleable state, and the parameters are effective. In the future, we will design functional structures based on the explored parameters and embed them on various topologies.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {47–49},
numpages = {3},
keywords = {3d fabrication, physical manipulation, metamaterial, malleability},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357112,
author = {Zhu, Qian and Ma, Shuai and Ma, Cuixia},
title = {Pre-Screen: Assisting Material Screening in Early-Stage of Video Editing},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357112},
doi = {10.1145/3332167.3357112},
abstract = {Video editing is a difficult task for both professionals and amateur editors. One of the biggest reasons is that screening useful clips from raw material in the early stage of editing is too much time-consuming and laborious. To better understand current difficulties faced by users in editing task, we first conduct a pilot study involving a survey and an interview among 20 participants. Based on the results, we then design Pre-screen, a novel tool to provide users with both global-view and detailed-view video analysis as well as material screening features based on intelligent video processing, analysis and visualization methods. User study shows that Pre-screen can not only effectively help users screen and arrange raw video material to save much more time than a widely used video editing tool in video editing tasks, but also inspire and satisfy users.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {50–52},
numpages = {3},
keywords = {video content analysis, video editing, material screening},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357113,
author = {Zhu, Qian and Ma, Shuai},
title = {What Did I Miss?},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357113},
doi = {10.1145/3332167.3357113},
abstract = {In Massive Open Online Courses (MOOCs), learners face a lot of distractions which will cause divided attention (DA). However, it is not easy for learners to realize that they are distracted and to find out which part of the course they have missed. In this paper, we present Reminder, a system for detecting divided attention and reminding learners what they just missed on both PC and mobile devices with a camera capturing their status. To get learners' attention level, we build a regression model to predict attention score from an integrated feature vector. Meanwhile, we design an interactively updating method to make the model adaptive to a specific user. We also propose a visualization method to help learners review missed content easily. User study shows that Reminder detects learners' divided attention and assists them to review missed course contents effectively.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {53–55},
numpages = {3},
keywords = {attention monitoring, mooc learning, content reviewing.},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357114,
author = {Saini, Aryan and Mathur, Kartik and Thukral, Abhinav and Singhal, Nishtha and Parnami, Aman},
title = {Aesop: Authoring Engaging Digital Storytelling Experiences},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357114},
doi = {10.1145/3332167.3357114},
abstract = {The traditional storytelling experiences are often one dimensional, wherein they only contain a single channel of communication with the audience through narration. With the advancements in technology, storytelling experiences have been augmented with the help of digital media to be more engaging and immersive. Authoring these scenarios, however, is complicated as it requires technical knowledge to interface the means of engagement. In this work, we talk about Aesop, a system which assists the narrator to author engaging storytelling experiences. Aesop provides a block-based interface like Scratch and manifests words of a story, Cues, and Visualization (Outputs) as blocks that enable the user to create captivating stories. Our system also leverages physical actions performed by the user as Cues. These cues can trigger visualizations like robot actions, animations, environments simulation using sound and lighting effects.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {56–59},
numpages = {4},
keywords = {digital storytelling, story, authoring tool, narration},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357115,
author = {Verma, Pramod},
title = {Stick User Interface},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357115},
doi = {10.1145/3332167.3357115},
abstract = {This paper describes a novel device called the "Stick User Interface", comprised of a robotic projector-camera system, an onboard digital computer connected to the Internet, sensors, and a hardware interface capable of sticking to any surface such as walls, ceilings, etc. The computer further consists of other subsystems, devices, and sensors such as an accelerometer, compass, gyroscope, flashlight, etc. The device unfolds (expands) on activation, detects a surface, and attaches itself to it. After a successful attachment, the device then projects or augments images, information, and user interfaces on nearby surfaces and walls. The user interface may contain applications, information about the object being augmented, and information from the Internet. Users can interact with the user-interface using commands and gestures such as hand, body, feet, voice, etc.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {60–62},
numpages = {3},
keywords = {artificial intelligence, display device, robotic assistant, user-interface, autonomous robots},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357116,
author = {Shekhar, Sumit and Maheshwari, Paridhi and J, Monisha and Singhal, Amrit and Singh, Kush Kumar and Krishna, Kundan},
title = {ARComposer: Authoring Augmented Reality Experiences through Text},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357116},
doi = {10.1145/3332167.3357116},
abstract = {Augmented Reality (AR) is rapidly gaining popularity, enhancing human perception of the real world by augmenting digital experiences. Existing tools for authoring AR scenes are either template based or require domain knowledge from experts, and are therefore restrictive. ARComposer is a novel interface that enables easy authoring of AR experiences from free-form text describing the scene. Our proposed interface allows creators to compose varied scenes comprising of multiple objects with diverse relationships to each other as well as human models with animations, starting merely with a textual description. A qualitative evaluation shows that ARComposer provides a good flow experience to its users. Furthermore, a crowd-sourced experiment evaluating various aspects of the rendered AR scenes indicates the viability of the proposed approach.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {63–65},
numpages = {3},
keywords = {ar rendering, augmented reality, natural language, 3d models},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357117,
author = {Wang, Xiyue and Ikematsu, Kaori and Fujita, Kazuyuki and Takashima, Kazuki and Kitamura, Yoshifumi},
title = {An Investigation of Electrode Design for Physical Touch Extensions on a Capacitive Touch Surface},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357117},
doi = {10.1145/3332167.3357117},
abstract = {A simple way to prototype touch interaction is to extend electrodes from a capacitive touch screen to off-screen areas. With that we aim to develop a toolkit that transforms a user-designed layout into a layout of screen-extension electrodes that realizes touch for rapid prototyping. Nevertheless, this kind of extension cannot detect touch if the physical properties of the electrodes become large. In this work, we decompose the physical design properties of the extension electrode into two factors, the target area, and line bridge, and investigate the limitation of each separately. While revealing some factors, such as area, is extremely limited in term of designing freely, we look into the causes by measuring the capacitive charge on-screen and on-extensions.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {66–68},
numpages = {3},
keywords = {rapid prototyping, capacitive touch sensing, fabrication},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357118,
author = {Fortin, Pascal E. and Blum, Jeffrey R. and Cooperstock, Jeremy R.},
title = {Towards Consistent Haptic Coupling with HaptiStrap: Doing Better than "Tight yet Comfortable"},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357118},
doi = {10.1145/3332167.3357118},
abstract = {How firmly a haptic device, such as a smartwatch, is coupled to the body can change how its haptic effects are perceived. However, hapticians often rely on vague subjective coupling characteristics such as "strapped snugly" or "tight yet comfortable". Achieving consistent strap tightness across body sites and between participants can be challenging, since even if strap tension is consistent, differences in limb circumference alter the resulting normal force under the haptic actuator in potentially unintuitive ways. Furthermore, when participants must attach the devices on their own, e.g., during a longitudinal in-the-wild study, they may not use the same tightness each day without guidance. We present HaptiStrap, a low-cost, easily fabricated tool, as a contribution towards a standard method for ensuring that wearable haptic studies do better than vague and subjective "tight yet comfortable" guidelines.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {69–71},
numpages = {3},
keywords = {wearable systems, wearable device attachment, haptic coupling, mobile haptics, in-situ studies},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357119,
author = {Viatchaninov, Oleksandr and Dziubliuk, Valerii and Radyvonenko, Olga and Yakishyn, Yevhenii and Zlotnyk, Mykhailo},
title = {CalliScan: On-Device Privacy-Preserving Image-Based Handwritten Text Recognition with Visual Hints},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357119},
doi = {10.1145/3332167.3357119},
abstract = {In this work, a solution for handwriting text extraction from images with visual user assistance is proposed. Use of end-to-end systems that pipe together text detection and recognition is often awkward because the user cannot influence the detection stage. On the other hand, glossing over the word's regions to help system with text localization requires a manual job and can be unacceptable. This paper proposes a solution that gives visual cues to the user during a detection stage. These hints differ from traditional bounding boxes in two ways. Firstly, the found text is surrounded with polygonal bounding reflecting a possible complex nature of text blocks. Secondly, TextRadar scanning effect provides a non-overloaded camera view, helping the user to capture the most relevant part of the text on image on-the-fly. CalliScan works on-device and keeps the user's privacy. The evaluation study has shown that users need such a solution, but it is necessary to carefully handle the text layout complexity.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {72–74},
numpages = {3},
keywords = {on-device processing, recurrent networks, htr, handwriting recognition},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357120,
author = {Heck, Melanie and Edinger, Janick and Becker, Christian},
title = {Gaze-Based Product Filtering: A System for Creating Adaptive User Interfaces to Personalize Stateless Point-of-Sale Machines},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357120},
doi = {10.1145/3332167.3357120},
abstract = {User interfaces in self-order terminals aim to satisfy the need for information of a broad audience and thus get easily clut-tered. Online shops present personalized product recommen-dations based on previously gathered user data to channel the user's attention. In contrast, stateless point-of-sales machines generally have no access to the user's personal information nor previous purchase behavior. User preferences must therefore be determined during the interaction. We thus propose using gaze data to determine preferences in real-time. In this paper we present a system for dynamic gaze-based fltering.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {75–77},
numpages = {3},
keywords = {gaze-contingent systems, implicit user feedback, interactive public displays, adaptive user interfaces, eye tracking},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357121,
author = {Ghosal, Radhika and Rana, Bhavika and Kapur, Ishan and Parnami, Aman},
title = {Rapid Prototyping of Pneumatically Actuated Inflatable Structures},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357121},
doi = {10.1145/3332167.3357121},
abstract = {Fabricating and actuating inflatables for shape-changing interfaces and soft robotics is challenging and time-consuming, requiring knowledge in diverse domains such as pneumatics, manufacturing processes for elastomers, and embedded systems. We propose in this poster a scheme for rapid prototyping and pneumatically actuating piecewise multi-chambered inflatables, using balloons as our building blocks. We provide a construction kit containing pneumatic control boards, pneumatic components, and balloons for constructing simple actuated balloon models. We also provide various primitives of actuation and locomotion to help the user put together their desired actuator, along with an Android app and software API for controlling it via Bluetooth. Finally, we demonstrate the construction and actuation of these inflatable structures using three sample applications.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {78–80},
numpages = {3},
keywords = {soft robotics, soft actuator, pneumatic system, shape changing interfaces, inflatables},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357122,
author = {Vernier, Andrew M. and Song, Jean Y. and Sun, Edward and Kench, Allison and Lasecki, Walter S.},
title = {Towards Universal Evaluation of Image Annotation Interfaces},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357122},
doi = {10.1145/3332167.3357122},
abstract = {To guide the design of interactive image annotation systems that generalize to new domains and applications, we need ways to evaluate the capabilities of new annotation tools across a range of different types of image, content, and task domains. In this work, we introduce Corsica, a test harness for image an- notation tools that uses calibration images to evaluate a tool's capabilities on general image properties and task requirements. Corsica is comprised of sets of three key components: 1) synthesized images with visual elements that are not domain- specific, 2) target microtasks that connects the visual elements and tools for evaluation, and 3) ground truth data for each mi- crotask and visual element pair. By introducing a specification for calibration images and microtasks, we aim to create an evolving repository that allows the community to propose new evaluation challenges. Our work aims to help facilitate the robust verification of image annotation tools and techniques.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {81–83},
numpages = {3},
keywords = {evaluation, image annotation, tools, crowdsourcing},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357123,
author = {Arora, Jatin and Gupta, Manan and Parnami, Aman},
title = {Choose a Lift and Walk into It: Manifesting Choice Blindness in Real-Life Scenarios Using Immersive Virtual Reality},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357123},
doi = {10.1145/3332167.3357123},
abstract = {In this paper, we propose the novel concept of manifesting the Choice Blindness paradigm in real-life scenarios through immersive Virtual Reality (VR). We designed a VR application wherein the participants encountered two-alternative choices regarding implicit racial bias. Our observations show that 92% of subjects failed to notice a mismatch in their choices, while 75% exhibited choice blindness, hence indicating a healthy scope for exploring choice blindness through virtual reality.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {84–86},
numpages = {3},
keywords = {preference changes, choice blindness, virtual reality},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357124,
author = {Sakuma, Ryota and Fujita, Yuki and Zempo, Keiichi},
title = {Audio Substituting Haptic System to Aware the Sound from Backwards},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357124},
doi = {10.1145/3332167.3357124},
abstract = {A person with hearing impairments is difficult to notice greets and calling attention from their back due to low ability of listening, and it leads the decline of the ability of sensing danger and the wrong guess at beginning communication. Therefore, we propose the system conveying the information of sound from backwards through haptic feedback. Through the experiment verifying recognition rate of DoA estimation, it is confirmed that the system can substitute the ability of rear sound image localization. Through the experiment verifying recognition rate of type of sounds, it is also confirmed that the system can aware the sound from backwards.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {87–89},
numpages = {3},
keywords = {multimodal accessibility, haptics},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357125,
author = {Miyakawa, Haruna and Kuratomo, Noko and Salih, Hisham Elser Bilal and Zempo, Keiichi},
title = {Auditory Uta-KARUTA: Sonificated Card Game towards Inclusive Design},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357125},
doi = {10.1145/3332167.3357125},
abstract = {We propose an auditory card game system towards playing equally with others, regardless of whether they have a visual impairment or not. To develop an audible card game system, we convert a Japanese traditional card game: Karuta into tablet devices which emit identificated sound signals, respectively. The proposed method makes it possible to boost the sense of victory and rivalry even without visually information.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {90–92},
numpages = {3},
keywords = {auralization, leisure accessibility, assistive technology},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3357126,
author = {Nishino, Hiroki},
title = {Ardestan: A Visual Programming Language for Arduino},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3357126},
doi = {10.1145/3332167.3357126},
abstract = {This paper describes Ardestan, a visual programming language (VPL) for Arduino currently under development. The language is designed with art and design students in mind as novice programmers. Although multitasking and event scheduling are essential for interactive prototypes, novices often face difficulty in implementing these features. By borrowing the language design from Pure Data, a VPL for interactive music, Ardestan facilitates the implementation of multitasking and event scheduling, while generating C++ code for a standalone Arduino system. Such features would be beneficial to support prototyping activity by art and design students in undergraduate interaction design courses.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {93–95},
numpages = {3},
keywords = {stem education, novice programming, arduino, rapid prototyping, visual programming, physical computing},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356884,
author = {Fraser, C. Ailie and Markel, Julia M. and Basa, N. James and Dontcheva, Mira and Klemmer, Scott},
title = {ReMap: Multimodal Help-Seeking},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356884},
doi = {10.1145/3332167.3356884},
abstract = {ReMap is a multimodal interface that enables searching for learning videos using speech and in-task pointing. ReMap extends multimodal interaction to help-seeking for complex tasks. Users can speak search queries, adding app-specific terms deictically. Users can navigate ReMap's search results via speech or mouse. These features allow people to stay focused on their task while simultaneously searching for and using help resources. Future work should explore how to implement more robust deictic resolution and more modalities.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {96–98},
numpages = {3},
keywords = {multimodal interaction, contextual search, speech, deixis},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356885,
author = {Segawa, Norihisa and Kato, Kunihiro and Manabe, Hiroyuki},
title = {Rapid Prototyping of Paper Electronics Using a Metal Leaf and Laser Printer},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356885},
doi = {10.1145/3332167.3356885},
abstract = {We introduce a novel prototyping method using a metal leaf and laser printer. Proposed metal leaf circuit is capable of producing circuits on normal paper that can be printed by a laser printer. The metal leaf is adhered to paper using a toner used in a laser printer. When the metal leaf is placed on the printed circuit pattern and pressed with an iron, the metal leaf adheres to the circuit pattern. Removal of the excess metal leaf completes the metal leaf circuit diagram on the paper. In addition, by using the change of temperature of toner, we can easily cut and repair the circuit. We made a metal leaf circuit on the cloth with the masu sake cup, and evaluated it.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {99–101},
numpages = {3},
keywords = {metal leaf, prototyping, wearable device, paper electronic},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356886,
author = {Katsumata, Yuki and Yamada, Wataru and Manabe, Hiroyuki},
title = {Optical See-Through Head-Mounted Display with Deep Depth of Field Using Pinhole Polarizing Plates},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356886},
doi = {10.1145/3332167.3356886},
abstract = {Optical See-Through Head-Mounted Displays (OST-HMDs) have attracted much attention as augmented reality (AR) devices. Depth of field (DoF) is a key parameter for OST-HMDs because a shallow DoF often yields focal point gaps between computer-generated graphics (CGs) and real scenes forcing users to re-adjust their focus. However, it is difficult to achieve a deep DoF for CGs over real scenes in compact and low-cost devices because laser projectors or complicated optics are needed. In this paper, we propose an OST-HMD that uses polarizing plates with a pinhole to achieve a deep DoF. The optics of the proposed device comprises two polarizing plates and a beam splitter. The polarization planes of the plates are orthogonal and one of the plates has a pinhole. The key idea of the proposal is the pinhole effect using polarizing plates to achieve a deep DoF without any influence to the field of view (FoV) for real scenes. This method can be implemented at significantly low cost and ease in compact thanks to its simple optics and is applicable to OST-HMDs that employ smartphones. In addition, we confirm that CGs are clearly seen whatever the focal point by constructing a prototype.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {102–104},
numpages = {3},
keywords = {optical see-through hmd, polarizing plate, pinhole effect, augmented reality},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356887,
author = {Sueyoshi, Tomoki and Morimoto, Yuki},
title = {Tangible Projection Mapping onto Deformable Moving Thin Plants via Markerless Tracking},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356887},
doi = {10.1145/3332167.3356887},
abstract = {We propose a system that automatically generates a projection mapping onto natural objects such as leaves and flowers, for which it is currently difficult to apply tracking markers. Our PM allows user interactions such as contact and covering. We describe our results with some animated effects on various moving leaves and flowers.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {105–106},
numpages = {2},
keywords = {visual effects, interactive projection mapping, image processing},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356888,
author = {Zhou, Zhongyi and Tsubouchi, Yuki and Yatani, Koji},
title = {Visualizing Out-of-Synchronization in Group Dancing},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356888},
doi = {10.1145/3332167.3356888},
abstract = {Prior work on choreographic learning support systems lacks supports of group dancers. In particular, existing systems cannot quantify the degree of synchronization, which is a critical factor for successful group dancing. In this paper, we create a system to support multi-person choreographic learning. The system visualizes body parts of dancers which are out of synchronization. Our interface aims to enable dancers to quickly identify moments where they need additional practice. This paper presents our current prototype interface and demonstration.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {107–109},
numpages = {3},
keywords = {visualization, multi-person choreographics, human motion analysis., dance practice support},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356889,
author = {Langerak, Thomas and Zarate, Juan and Vechev, Velko and Panozzo, Daniele and Hilliges, Otmar},
title = {A Demonstration on Dynamic Drawing Guidance via Electromagnetic Haptic Feedback},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356889},
doi = {10.1145/3332167.3356889},
abstract = {We demonstrate a system to deliver dynamic guidance in drawing, sketching and handwriting tasks via an electromagnet moving underneath a high refresh rate pressure sensitive tablet presented in citelangerak2019dynamic. The system allows the user to move the pen at their own pace and style and does not take away control. Using a closed-loop time-free approach allows for error-correcting behavior. The user will experience to be smoothly and natural pulled back to the desired trajectory rather than pushing or pulling the pen to a continuously advancing setpoint. The optimization of the setpoint with regard to the user is unique in our approach.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {110–112},
numpages = {3},
keywords = {hardware, magnetism, stylus-based interaction, haptics},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356890,
author = {Fujinawa, Eisuke and Goto, Kenji and Irie, Atsushi and Wu, Songtao and Xu, Kuanhong},
title = {Occlusion-Aware Hand Posture Based Interaction on Tabletop Projector},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356890},
doi = {10.1145/3332167.3356890},
abstract = {Conventional camera-based hand interaction technique suffered from self-occlusion among fingers, which lowers the detection accuracy of fingertip positions, leading to uncomfortable UI controls. Based on observations, self-occlusion depends on hand postures. We design an interaction framework in which interaction is decided in response to a recognized hand posture. Using a tabletop projection system that has a projector and a depth sensor, we implement the framework by integrating five touch and in-air interactions that balance its stability and usability.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {113–115},
numpages = {3},
keywords = {augmented reality, hand gesture, projection mapping, occlusion, interaction techniques, depth sensing, touch},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356891,
author = {Narazani, Marla and Eghtebas, Chloe and Klinker, Gudrun and Jenney, Sarah L. and M\"{u}hlhaus, Michael and Petzold, Frank},
title = {Extending AR Interaction through 3D Printed Tangible Interfaces in an Urban Planning Context},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356891},
doi = {10.1145/3332167.3356891},
abstract = {Embedding conductive material into 3D printed objects enables non-interactive objects to become tangible without the need to attach additional components. We present a novel use for such touch-sensitive objects in an augmented reality (AR) setting and explore the use of gestures for enabling different types of interaction with digital and physical content. In our demonstration, the setting is an urban planning scenario. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. Attendees can either interact with the physical model or with the mobile AR interface for selecting, adding or deleting buildings.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {116–118},
numpages = {3},
keywords = {capacitive sensing, gestures, augmented reality, tangible user interface, 3d printing},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356892,
author = {Kitade, Takuya and Yamada, Wataru},
title = {Prismodule: Modular UI for Smartphones Using Internal Reflection},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356892},
doi = {10.1145/3332167.3356892},
abstract = {The multi-touch interface is essential for smartphone user experience; however many issues such as a lack of physical feedback, the occlusion problem, and the reachability problem still remain. The back-of-device (BoD) approach is a way to address these issues; nevertheless, there are problems such as that the user finger or external landscape affects the view of the camera and that simultaneous input is difficult in the technique using the acceleration sensor. We propose a thin and low-cost method using internal reflection to actualize an additional physical UI on the back of the smartphone. No special devices such as mirrors are needed only acrylic panels and plastic parts. This method extends the smartphone input UI to include a physical button, dial, pointing stick, and also a slider. In addition, the method supports simultaneous input and overcome the effect of the user finger and external landscape.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {119–121},
numpages = {3},
keywords = {physical input, rear-camera, total internal reflection (tir), back-of-device, smartphone},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356893,
author = {Salemi Parizi, Farshid and Whitmire, Eric and Cao, Alvin and Li, Tianke and Patel, Shwetak},
title = {Demo of AuraRing: Precise Electromagnetic Finger Tracking},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356893},
doi = {10.1145/3332167.3356893},
abstract = {We present AuraRing, a wearable electromagnetic tracking system for fine-grained finger movement. The hardware consists of a ring with an embedded electromagnetic transmitter coil and a wristband with multiple sensor coils. By measuring the magnetic fields at different points around the wrist, AuraRing estimates the five degree-of-freedom pose of the finger. AuraRing is trained only on simulated data and requires no runtime supervised training, ensuring user and session independence. AuraRing has a resolution of 0.1 mm and a dynamic accuracy of 4.4 mm, as measured through a user evaluation with optical ground truth. The ring is completely self-contained and consumes just 2.3 mW of power.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {122–124},
numpages = {3},
keywords = {electromagnetic tracking, smart ring, wearables},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356894,
author = {Sugiyama, Kei and Tsukada, Koji},
title = {Proposal of Lens Shaping Method Using UV Printer},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356894},
doi = {10.1145/3332167.3356894},
abstract = {We proposed a lens forming method that does not require post-output processing using an ultraviolet (UV) printer. The transparent ink is first laminated to form a shape. A UV printer then smooths its surface by filling the layer roughness with a gloss. We implemented a tool to design the lens shape by inputting the lens diameter, focal length, etc.; with this tool, multiple lenses can be arranged and output as data to be printed by a UV printer.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {125–127},
numpages = {3},
keywords = {uv printer, lens, digital fabrication},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356895,
author = {Imai, Yuhei and Kato, Kunihiro and Segawa, Norihisa and Manabe, Hiroyuki},
title = {Hot Stamping of Electric Circuits by 3D Printer},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356895},
doi = {10.1145/3332167.3356895},
abstract = {Making modern electric circuits involves a lot of processes, skills, and effort. One challenge in fabricating prototypes is the difficulty of making electric circuits. We propose a fabrication technique that allows users to easily build electric circuits. It is based on a hot stamping technique but uses a single extruder 3D printer instead of dies. The circuits are made of metal foil and can be deposited on various surfaces including paper and acrylic board. Tests confirm that the technique can fabricate circuits on which electronic devices can be implemented by soldering.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {128–130},
numpages = {3},
keywords = {hot stamping, 3d printer, gold leaf, transfer foil, electric circuit},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356896,
author = {Yamada, Wataru and Manabe, Hiroyuki and Ikeda, Daizo and Rekimoto, Jun},
title = {VARiable HMD: Optical See-Through HMD for AR and VR},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356896},
doi = {10.1145/3332167.3356896},
abstract = {Virtual reality (VR) and augmented reality (AR) with head-mounted displays (HMDs) are rapidly becoming popular. Unfortunately, optical see-through HMDs for AR are not compatible with VR and users must prepare the other type of HMD to experience VR. We present a new method for optical see-through HMDs that enables switching between VR and AR by changing the degree of transparency. The proposed method controls liquid crystals to change the transparency in an enclosure constructed of polarizing plates to achieve the switch. The proposed method does not require the use of expensive modules and is applicable to various HMDs including those that are smartphone based and retinal projection based. We construct a prototype implementing the proposed method and evaluate its performance and unique characteristics.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {131–133},
numpages = {3},
keywords = {mixed reality, optical see-though, head-mounted display},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356897,
author = {Tran O'Leary, Jasper and Peek, Nadya},
title = {Machine-o-Matic: A Programming Environment for Prototyping Digital Fabrication Workflows},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356897},
doi = {10.1145/3332167.3356897},
abstract = {We propose a programming environment for prototyping workflows that consists of custom digital fabrication machines and user-defined interactions. At its core, Machine-o-Matic comprises a domain-specific programming language for defining custom CNC machines as aconfiguration of tools and moving stages connected together. Given a software defined machine configuration, the language compiles to firmware code that allows a user to control and test a physical machine immediately. The language includes constructs for users to define custom actions with the tool and to interface with input from sensors or a camera feed. To aid users in writing Machine-o-Matic programs, we include a drag and drop GUI for assembling, simulating, and experimenting with potential machine configurations before physically fabricating them. We present three proofs of concept to showcase the potential of our programming environment.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {134–136},
numpages = {3},
keywords = {custom interaction, digital fabrication, programming languages},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356898,
author = {Lee, Jaeyeon and Sinclair, Mike and Gonzalez-Franco, Mar and Ofek, Eyal and Holz, Christian},
title = {Demonstration of TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356898},
doi = {10.1145/3332167.3356898},
abstract = {Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. We demonstrate the TORC interaction scenarios for a virtual object in hand.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {137–139},
numpages = {3},
keywords = {haptic texture, vr object manipulation, haptics, haptic compliance},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356899,
author = {Hornstein, Alex N. and Moore, Evan and Chang, Kai-han},
title = {A Match Made in Heaven: Streaming Real-Time Imagery from a Lightfield Camera to a Lightfield Display},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356899},
doi = {10.1145/3332167.3356899},
abstract = {We are demonstrating a novel design of a live lightfield camera capturing a scene and showing the captured lightfield in realtime in a lightfield display. The simple, distributed design of the camera allows for low-cost construction of an array of 2D cameras that captures high quality, artifact-free imagery of the most challenging of subjects. This camera takes advantage of the natural duality of outside-in lightfield cameras with inside-out lightfield displays, letting us render complex lightfield imagery with a minimum of processing power.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {140–141},
numpages = {2},
keywords = {streaming lightfield video, lightfield camera, realtime communication, interactive lightfield display, lightfield display},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356874,
author = {Chang, Minsuk},
title = {Data Structures for Designing Interactions with Contextual Task Support},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356874},
doi = {10.1145/3332167.3356874},
abstract = {The diversity and the scale of available online instructions introduce opportunities but also user challenges in currently used software interfaces; Users have limited computational resources, and thus often make strategic decisions when browsing, navigating, and understanding instructions to accomplish a task. These strategic user interactions possess nuanced semantics such as users' interpretations, intents, and contexts in which the task is carried out. My dissertation research introduces techniques in constructing data structures that capture the diverse strategies users employ in which the collective nuanced semantics across multiple strategies are preserved. These computational representations are then used as building blocks for designing novel interactions that allow users to effectively browse and navigate instructions, and provide contextual task guidance. Specifically, I investigate 1) structure of instructions for task analysis at scale, 2) structure of collective user task demonstrations, and 3) structure of object uses in how-to videos to support tracking, guiding and searching task states. My research demonstrates that the user-centered organization of information extracted from interaction traces enables novel interfaces with contextual task support.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {142–145},
numpages = {4},
keywords = {computational representations, computational interaction},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356875,
author = {Siu, Alexa F.},
title = {Advancing Accessible 3D Design for the Blind and Visually-Impaired via Tactile Shape Displays},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356875},
doi = {10.1145/3332167.3356875},
abstract = {Affordable rapid 3D printing technologies have become key enablers of the Maker Movement by giving individuals the ability to create physical finished products. However, existing computer-aided design (CAD) tools that allow authoring and editing of 3D models are mostly visually reliant and limit access to people with blindness and visual impairment (BVI). In this paper I address three areas of research that I will conduct as part of my PhD dissertation towards bridging a gap between blind and sighted makers. My dissertation aims to create an accessible 3D design and printing workflow for BVI people through the use of 2.5D tactile displays, and to rigorously understand how BVI people use the workflow in the context of perception, interaction, and learning.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {146–149},
numpages = {4},
keywords = {2.5d shape displays, tactile displays, tactile graphics, accessible 3d printing, accessible authoring tools},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356876,
author = {Strasnick, Evan},
title = {Circuit Design Tools for Exploratory Understanding},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356876},
doi = {10.1145/3332167.3356876},
abstract = {Effective circuit design and debugging require developing an intimate understanding of the behaviors of a complex system. In my work, I've distilled barriers to such understanding to three fundamental challenges of circuit design: transparency, malleability, and modelability. In turn, my research contributes tools that address these challenges through novel changes to the circuit design workflow: Pinpoint improves transparency and malleability in the debugging of printed circuit boards (PCBs) by augmenting board connections with automatic instrumentation and reconfigurable connectivity. Scanalog similarly improves transparency and malleability in prototyping by providing an interactively reprogrammable platform on which to design and tune fully instrumented mixed-signal circuits. My ongoing work addresses issues in modelability through tools that generate empirically-derived fault models and highlight causal relationships between components in a circuit. By evaluating these interactions, my research examines the role of exploratory understanding in circuit design, asking, "How can tools promote understanding of a circuit by facilitating exploration and reflection?"},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {150–153},
numpages = {4},
keywords = {pcb, circuit, prototyping, design tools, physical computing, debugging},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356877,
author = {Suzuki, Ryo},
title = {Collective Shape-Changing Interfaces},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356877},
doi = {10.1145/3332167.3356877},
abstract = {In this paper, I introduce collective shape-changing interfaces, a class of shape-changing interfaces that consist of a set of discrete collective elements. Through massively parallel transformation, locomotion, and connection of individual building blocks, the overall physical structure can be dynamically changed. Given this parallel change of individual elements, I propose three approaches for user interaction: dynamic, improvised, and actuated collective shape transformation. I exemplify each approach through my own work and possible future work.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {154–157},
numpages = {4},
keywords = {collective shape-changing interfaces, swarm robots, programmable matter, modular robots, tangible bits},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356878,
author = {Subramonyam, Hariharan},
title = {Designing Interactive Intelligent Systems for Human Learning, Creativity, and Sensemaking},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356878},
doi = {10.1145/3332167.3356878},
abstract = {The focus of intelligent systems is on "making things easy'' through automation. However, for many cognitive tasks---such as learning, creativity, or sensemaking---there is such a thing as too easy or too automated. Current human-AI design principles, as well as general usability guidelines, prioritize automation, and efficient task execution over human effort. However, this type of advice may not be suitable for designing systems that need to balance automation with other cognitive goals. In these cases, designers lack the necessary tools that will allow them to consider the trade-offs between automation, AI assistance, and human-effort. My dissertation looks at using models from cognitive psychology to inform the design of intelligent systems. The first system, Florum, looks at automation after human-effort as a strategy to facilitate learning from science text. The second system, TakeToons, explores automation as a complementary strategy to human-effort to support creative animation tasks. A third set, SmartCues and Affinity Lens use AI as a last-mile optimization strategy for human sensemaking tasks. Based on these systems, I am looking to develop a design framework that (1) classifies threats across different levels of design including automation, user interface, expectations from AI, and cognition and (2) offers ways to validate design decisions.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {158–161},
numpages = {4},
keywords = {cognitive models, human-ai, interaction design},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356879,
author = {Lu, Zhicong},
title = {Improving Viewer Engagement and Communication Efficiency within Non-Entertainment Live Streaming},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356879},
doi = {10.1145/3332167.3356879},
abstract = {Live streaming has recently gained worldwide popularity, due to the affordable digital video devices, high-speed Internet access, and social media. While video games and other entertainment content attract a broad audience to live streaming, it has become an important channel for sharing a variety of non-entertainment content, such as civil content, knowledge sharing, and even promoting traditional cultural practices. However, little research has explored the practices and challenges of the vibrant communities of these streamers who share knowledge or showcase cultural practices through live streams, and few tools have been designed and developed to support their needs for engaging viewers and communicate with viewers more efficiently. The goal of my research is to better understand the practices of these streamers and their communities, and to design tools to better support knowledge sharing and cultural heritage preservation through live streaming.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {162–165},
numpages = {4},
keywords = {knowledge sharing, live streaming, social media, collaborative activities, intangible cultural heritage},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356880,
author = {Aneja, Deepali},
title = {Performance-Based Expressive Character Animation},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356880},
doi = {10.1145/3332167.3356880},
abstract = {For decades, animation has been a popular storytelling technique. Traditional tools for creating animations are labor-intensive requiring animators to painstakingly draw frames and motion curves by hand. An alternative workflow is to equip animators with direct real-time control over digital characters via performance, which offers a more immediate and efficient way to create animation. Even when using these existing expression transfer and lip sync methods, producing convincing facial animation in real-time is a challenging task. In this position paper, I describe my past and proposed future research in developing interactive systems for perceptually-valid expression retargeting from humans to stylized characters, real-time lip sync for 2D animation, and building an expressive style aligned embodied conversational agent.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {166–169},
numpages = {4},
keywords = {expression retargeting, lip sync, embodied conversational agents, multi-modality, deep learning},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3332167.3356881,
author = {Tian, Rundong},
title = {Lucid Fabrication},
year = {2019},
isbn = {9781450368179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332167.3356881},
doi = {10.1145/3332167.3356881},
abstract = {Advances in digital fabrication have created new capabilities and simultaneously reinforced outdated workflows. In my thesis work, I primarily explore alternative workflows for digital fabrication that introduce new capabilities and interactions. Methodologically, I build fabrication systems spanning mechanical design, electronics, and software in order to examine these ideas in specific detail. In this paper, I introduce related work and frame it within the historical context of digital fabrication, and discuss my previous and ongoing work.},
booktitle = {The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {170–173},
numpages = {4},
keywords = {augmented tools, fabrication workflows, embodied interactions, digital fabrication},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

