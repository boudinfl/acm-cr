@inproceedings{10.1145/2984751.2984781,
author = {Koyama, Yuki},
title = {Computational Design Driven by Aesthetic Preference},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984781},
doi = {10.1145/2984751.2984781},
abstract = {Tweaking design parameters is one of the most fundamental tasks in many design domains. In this paper, we describe three computational design methods for parameter tweaking tasks in which aesthetic preference---how aesthetically preferable the design looks---is used as a criterion to be maximized. The first method estimates a preference distribution in the target parameter space using crowdsourced human computation. The estimated preference distribution is then used in a design interface to facilitate interactive design exploration. The second method also estimates a preference distribution and uses it in an interface, but the distribution is estimated using the editing history of the target user. In contrast to these two methods, the third method automatically finds the best parameter that maximizes aesthetic preference, without requiring the user of this method to manually tweak parameters. This is enabled by implementing optimization algorithms using crowdsourced human computation. We validated these methods mainly in the scenario of photo color enhancement where parameters, such as brightness and contrast, need to be tweaked.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {1–4},
numpages = {4},
keywords = {aesthetic preference, computational design, design exploration},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984782,
author = {Yoon, Sang Ho},
title = {Promoting Natural Interactions Through Embedded Input Using Novel Sensing Techniques},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984782},
doi = {10.1145/2984751.2984782},
abstract = {From mobile devices to interactive objects, various input methods are provided using built-in motion and capacitive touch sensors. These inputs are offered in effective and efficient manner where users can operate interface quickly and easily. However, they do not fully explore the input space supported by human's natural motion behavior. As a solution, my work focuses on promoting natural interaction through hand-driven embedded input powered by multimodal and magnetic sensing techniques. In my previous works, embedded inputs were implemented in the form of smart textile, stylus, and ring supporting from mobile devices to everyday objects. Throughout the paper, I will briefly go over implemented systems along with evaluated results and potential applications. Future research direction is highlighted at the end of the paper.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {5–8},
numpages = {4},
keywords = {wearables, input device, mobile interaction, sensing technique, embedded interaction, magnetism},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984783,
author = {Gugenheimer, Jan},
title = {Nomadic Virtual Reality: Exploring New Interaction Concepts for Mobile Virtual Reality Head-Mounted Displays},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984783},
doi = {10.1145/2984751.2984783},
abstract = {Technical progress and miniaturization enables virtual reality (VR) head-mounted displays (HMDs) now to be solely operated using a smartphone as a display, processing unit and sensor unit. These mobile VR HMDs (e.g. Samsung GearVR) allow for a whole new interaction scenario, where users can bring their HMD with them wherever they want and immerse themselves anytime at any place (nomadic VR). However, most of the early research on interaction with VR HMDs focused around stationary setups. My research revolves around enabling new forms of interaction for these nomadic VR scenarios. In my research I choose a user-centered design approach where I build research prototypes to solve potential problems of nomadic VR and evaluate those prototypes in user studies. I am going to present three prototypes revolving around current challenges of nomadic VR (input and feedback).},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {9–12},
numpages = {4},
keywords = {mobile vr, nomadic vr, virtual reality, vr interaction},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984784,
author = {Xia, Haijun},
title = {Object-Oriented Interaction: Enabling Direct Physical Manipulation of Abstract Content via Objectification},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984784},
doi = {10.1145/2984751.2984784},
abstract = {Touch input promises intuitive interactions with digital content as it employs our experience of manipulating physical objects: digital content can be rotated, scaled, and translated using direct manipulation gestures. However, the reliance on analog also confines the scope of direct physical manipulation: the physical world provides no mechanism to interact with digital abstract content. As such, applications on touchscreen devices either only include limited functionalities or fallback on the traditional form-filling paradigm, which is tedious, slow, and error prone for touch input. My research focuses on designing a new UI framework to enable complex functionalities on touch screen devices by expanding direct physical manipulation to abstract content via objectification. I present two research projects, objectification of attributes and selection, which demonstrate considerable promises.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {13–16},
numpages = {4},
keywords = {object-oriented interaction, collection object, pen and touch, attribute object},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984785,
author = {Chen, Xiang 'Anthony'},
title = {Making Fabrication Real},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984785},
doi = {10.1145/2984751.2984785},
abstract = {Low-cost, easy-to-use 3D printers have promised to empower everyday users with the ability to fabricate physical objects of their own design. While these printers specialize in building objects from scratch, they are innately oblivious to the real world in which the printed objects will be situated and in use. In my thesis research, I develop fabrication techniques with tool integration to enable users to expressively specify how a design can be attached to, augment, adapt, support, or otherwise function with existing real world objects. In this paper, I describe projects to date as well as ongoing work that explores this space of research.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {17–20},
numpages = {4},
keywords = {design tool, real world objects, augmentation, fabrication, adaptation, 3d printing},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984786,
author = {Deka, Biplab},
title = {Data-Driven Mobile App Design},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984786},
doi = {10.1145/2984751.2984786},
abstract = {Design is becoming a key differentiating factor for successful apps in today's crowded app marketplaces. This thesis describes how data-driven approaches can enable useful tools for mobile app design. It presents interaction mining -- capturing both static (UI layouts, visual details) and dynamic (user flows, motion details) components of an app's design. It develops two approaches for interaction mining existing Android apps and presents three applications enabled by the resultant data: a search engine for user flows, lightweight usability testing at scale, and automated generation of mobile UIs.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {21–24},
numpages = {4},
keywords = {mobile apps, data-driven design, interaction mining},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984787,
author = {Pai, Yun Suen},
title = {Physiological Signal-Driven Virtual Reality in Social Spaces},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984787},
doi = {10.1145/2984751.2984787},
abstract = {Virtual and augmented reality are becoming the new medium that transcend the way we interact with virtual content, paving the way for many immersive and interactive forms of applications. The main purpose of my research is to create a seamless combination of physiological sensing with virtual reality to provide users with a new layer of input modality or as a form of implicit feedback. To achieve this, my research focuses in novel augmented reality (AR) and virtual reality (VR) based application for a multi-user, multi-view, multi-modal system augmented by physiological sensing methods towards an increased public and social acceptance.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {25–28},
numpages = {4},
keywords = {physiological sensing, virtual reality, augmented reality},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984788,
author = {Sra, Misha},
title = {Asymmetric Design Approach and Collision Avoidance Techniques For Room-Scale Multiplayer Virtual Reality},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984788},
doi = {10.1145/2984751.2984788},
abstract = {Recent advances in consumer virtual reality (VR) technology have made it easy to accurately capture users' motions over room-sized areas allowing natural locomotion for navigation in VR. While this helps create a stronger match between proprioceptive information from human body movements for enhancing immersion and reducing motion sickness, it introduces a few challenges. Walking is only possible within virtual environments (VEs) that fit inside the boundaries of the tracked physical space which for most users is quite small. Within this space the potential for colliding with physical objects around the play area is high. Additionally, only limited haptic feedback is available. In this paper, I focus on the problem of variations in the size and shape of each user's tracked physical space for multiplayer interactions. As part of the constrained physical space problem, I also present an automated system for steering the user away from play area boundaries using Galvanic Vestibular Stimulation (GVS). In my thesis, I will build techniques to enable the system to intelligently apply redirection and GVS-based steering as users explore virtual environments of arbitrary sizes.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {29–32},
numpages = {4},
keywords = {tracking, galvanic vestibular stimulation, natural locomotion, asymmetric design, 3d mapping, obstacle avoidance, games, virtual reality, head-mounted displays},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985695,
author = {Nakayasu, Akira},
title = {Luminescent Tentacles: A Scalable SMA Motion Display},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985695},
doi = {10.1145/2984751.2985695},
abstract = {The Luminescent Tentacles system is a scalable kinetic surface system for kinetic art, ambient display, and animatronics. The 256 shape-memory alloy actuators react to hand movement by fluid dynamics and Kinect. These actuators behave like waving tentacles of sea anemones under the sea, and the top of the actuator softly glows like a bioluminescent organism. To precisely control a large number of actuators simultaneously, the system utilizes one microcontroller per actuator for distributed processing. In addition, it provides a scalable platform, which can be easily built into various forms.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {33–34},
numpages = {2},
keywords = {kinetic surface, smart material interface, shape-memory alloy, soft actuator, actuated surface},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985696,
author = {Glauser, Oliver and Vartok, Benedek and Ma, Wan-Chun and Panozzo, Daniele and Jacobson, Alec and Hilliges, Otmar and Sorkine-Hornung, Olga},
title = {Rig Animation with a Tangible and Modular Input Device},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985696},
doi = {10.1145/2984751.2985696},
abstract = {We propose a novel approach to digital character animation, combining the benefits of modular and tangible input devices and sophisticated rig animation algorithms. With a symbiotic software and hardware approach, we overcome limitations inherent to all previous tangible devices. It allows users to directly control complex rigs with 5-10 physical controls only. These compact input device configurations - optimized for a specific rig and a set of sample poses - are automatically generated by our algorithm. This avoids oversimplification of the pose space and excessively bulky devices.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {35–36},
numpages = {2},
keywords = {novel input devices, character articulation},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985697,
author = {Kubo, Yuki and Shizuki, Buntarou and Takahashi, Shin},
title = {Watch Commander: A Gesture-Based Invocation System for Rectangular Smartwatches Using B2B-Swipe},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985697},
doi = {10.1145/2984751.2985697},
abstract = {We present Watch Commander, a gesture-based invocation system for rectangular smartwatches. Watch Commander allows the user to invoke functions easily and quickly by using Bezel to Bezel-Swipe (B2B-Swipe). This is because B2B-Swipe does not conflict with other swipe gestures such as flick and bezel swipe and can be performed in an eyes-free manner. Moreover, by providing GUIs that display functions assigned with B2B-Swipe, Watch Commander helps the user memorize those functions.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {37–39},
numpages = {3},
keywords = {ultra-small device, watch applications, wrist-worn device, watch gui., shortcut, eyes-free, bezel swipe},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985698,
author = {Kim, Hyung-Sik and Gim, Seong-Young and Kim, Woo-Ram and Choi, Mi-Hyun and Choi, Seungmoon and Chung, Soon-Cheol},
title = {MagTacS: Delivering Tactile Sensation over an Object},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985698},
doi = {10.1145/2984751.2985698},
abstract = {A system that can deliver tactile sensation despite an object existing between an actuator and human was developed. This system composed of a control part, power part, output part, and coil. The control part controls the overall system using a microcontroller. The power part generates electric current to create a magnetic field. The output part delivers high energies to the coil. The coil generates a time-varying magnetic field to induce current flow within the body. Through the tactile sensation recognition test, delivery of tactile sensation was confirmed in the air even an object existed between actuator and human skin.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {41–42},
numpages = {2},
keywords = {magnetic stimulation, time-varying magnetic field, tactile sensation},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985699,
author = {Yui, Toshiya and Hashida, Tomoko},
title = {Floatio: Floating Tangible User Interface Based on Animacy Perception},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985699},
doi = {10.1145/2984751.2985699},
abstract = {In this study, we propose floatio: a floating tangible user interface that makes it easy to create a perception of animacy (lifelike movement). It has been pointed out that there are three requirements that make animacy more likely to be perceived: interactivity, irregularities, and automatic movement resisting the force of gravity. Based on these requirements, floatio provides a tangible user interface where a polystyrene ball resembling a pixel is suspended in a stream of air where it can be positioned passively by the user, or autonomously by the system itself. To implement floatio, we developed three mechanisms: a floating field mechanism, a pointer input/output mechanism and a hand-over mechanism. We also measured the precision of the pointer input/output and hand-over mechanisms.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {43–45},
numpages = {3},
keywords = {3d interaction, tangible user interface, animacy perception},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985700,
author = {Kato, Kunihiro and Miyashita, Homei},
title = {3D Printed Physical Interfaces That Can Extend Touch Devices},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985700},
doi = {10.1145/2984751.2985700},
abstract = {We propose a method to create a physical interface that allows touch input to be transferred from an external surface attached to a touch panel. Our technique prints a grid having multiple conductive points using an FDM-based 3D printer. When the user touches the conductive points, touch input is generated. This allows the user to control the touch input at arbitrary locations on an XY plane. By arranging the structure of the conductive wiring inside a physical object, a variety of interfaces can be realized.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {47–49},
numpages = {3},
keywords = {physical interface, 3d printer, capacitive touch panel},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985701,
author = {Takahashi, Haruki and Miyashita, Homei},
title = {Thickness Control Technique for Printing Tactile Sheets with Fused Deposition Modeling},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985701},
doi = {10.1145/2984751.2985701},
abstract = {We present a printing technique that controls the thickness of objects by increasing and decreasing the amount of material extruded during printing. Using this technique, printers can dynamically control thickness and output thicker objects without a staircase effect. This technique allows users to print aesthetic pattern sheets and objects that are tactile without requiring any new hardware. This extends the capabilities of fused deposition modeling (FDM) 3D printers in a simple way. We describe a method of generating and calculating a movement path for printing tactile sheets, and demonstrate the usage and processing of example objects.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {51–53},
numpages = {3},
keywords = {3d printing, digital fabrication, haptics, texture, thickness control},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985702,
author = {Spelmezan, Daniel and Sahoo, Deepak Ranjan and Subramanian, Sriram},
title = {Sparkle: Towards Haptic Hover-Feedback with Electric Arcs},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985702},
doi = {10.1145/2984751.2985702},
abstract = {We demonstrate a method for stimulating the fingertip with touchable electric arcs above a hover sensing input device. We built a hardware platform using a high-voltage resonant transformer for which we control the electric discharge to create in-air haptic feedback up to 4 mm in height, and combined this technology with infrared proximity sensing. Our method is a first step towards supporting novel in-air haptic experiences for hover input that does not require the user to wear haptic feedback stimulators.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {55–57},
numpages = {3},
keywords = {in-air haptic feedback, electric discharge, hover sensing, infrared proximity sensor., high-voltage transformer},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985703,
author = {Griggio, Carla F. and Giang, Nam and Leiva, Germ\'{a}n and Mackay, Wendy E.},
title = {The UIST Video Browser: Creating Shareable Playlists of Video Previews},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985703},
doi = {10.1145/2984751.2985703},
abstract = {We introduce the UIST Video Browser which provides a rapid overview of the UIST 30-second video previews, based on the conference schedule. Attendees can see an overview of upcoming talks, search by topic, and create personalized, shareable video playlists that capture the most interesting or relevant papers.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {59–60},
numpages = {2},
keywords = {personalized video playlists, video previews},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985704,
author = {Zhang, Xinlei and Miyaki, Takashi and Rekimoto, Jun},
title = {WithYou: An Interactive Shadowing Coach with Speech Recognition},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985704},
doi = {10.1145/2984751.2985704},
abstract = {Speech shadowing, in which the subject listens to native narration sound and tries to repeat it immediately while listening, is a proven way of practicing speaking skills when learning foreign languages. However, since the narration is independent of user's speech, the playback cannot make an adjustment when the learner fails to catch up, and this makes shadowing difficult. We propose WithYou, a system based on Automated Speech Recognition (ASR) that is able to adjust narration playback during a live shadowing speech. WithYou compares the student's live speech with the narration playback to detect shadowing mistakes. In addition, WithYou is able to handle pauses and recognize repetitive phrases in shadowing practice. A user study shows that practicing shadowing with WithYou is easier and more effective compared with conventional methods.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {61–63},
numpages = {3},
keywords = {shadowing, speech recognition, voice interface, computer-assisted language learning, speaking support},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985705,
author = {Otsuki, Mai and Kawano, Taiki and Maruyama, Keita and Kuzuoka, Hideaki and Suzuki, Yusuke},
title = {Representing Gaze Direction in Video Communication Using Eye-Shaped Display},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985705},
doi = {10.1145/2984751.2985705},
abstract = {A long-standing challenge of video-mediated communication systems is to correctly represent a remote participant's gaze direction in local environments. To address this problem, we developed a video communication system using an "eye-shaped display." This display is made of an artificial ulexite (TV rock) that is cut into a hemispherical shape, enabling the light from the bottom surface to be projected onto the curved surface. By displaying a simulated iris onto the eye-shaped display, we theorize that our system can represent the gaze direction as accurately as a real human eye.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {65–67},
numpages = {3},
keywords = {perception of gaze direction, telecommunication},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985706,
author = {Suzuki, Ippei and Yoshimitsu, Shuntarou and Kawahara, Keisuke and Ito, Nobutaka and Shinoda, Atushi and Ishii, Akira and Yoshida, Takatoshi and Ochiai, Yoichi},
title = {Gushed Diffusers: Fast-Moving, Floating, and Lightweight Midair Display},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985706},
doi = {10.1145/2984751.2985706},
abstract = {We present a novel method for fast-moving aerial imaging using aerosol-based fog screens. Conventional systems of aerial imaging cannot move fast because they need large and heavy setup. In this study, we propose to add new tradeoffs between limited display time and payloads. This system employ aerosol distribution from off-the-shelf spray as a fog screen that can resist the wind, and have high portability. As application examples, we present wearable application and aerial imaging on objects with high speed movements such as a drone, a radio-controlled model car, and performers. We believe that our study contribute to the exploration of new application areas for fog displays and expand expressions of entertainments and interactivity.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {69–70},
numpages = {2},
keywords = {display, multicopter, fog screen, communication., entertainment},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985707,
author = {Chen, Yang-Sheng and Han, Ping-Hsuan and Hsiao, Jui-Chun and Lee, Kong-Chang and Hsieh, Chiao-En and Lu, Kuan-Yin and Chou, Chien-Hsing and Hung, Yi-Ping},
title = {SoEs: Attachable Augmented Haptic on Gaming Controller for Immersive Interaction},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985707},
doi = {10.1145/2984751.2985707},
abstract = {We present SoEs (Sword of Elements), an attachable augmented haptic device for enhancing gaming controller in the immersive first-person game. Generally, Player can easily receive visual and auditory feedback through head-mounted displays (HMD) and headphones from first-person perspective in virtual world. However, the tactile feedback is less than those feedbacks in immersive environment. Although gaming controller, i.e. VIVE or Oculus controller, can provide tactile feedback by some vibration sensors, the haptic feedback is more complicated and various, it includes kinesthesia and cutaneous feedback. Our key idea is to provide a low-cost approach to simulate the haptic feedback of player manipulation in the immersive environment such as striking while the iron is hot which the player could feel the heat and reaction force. Eventually, the game makers could utilize the attachable device into their games for providing haptic feedback.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {71–72},
numpages = {2},
keywords = {haptic feedback, game controller, augmented haptic, immersive game, virtual reality},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985708,
author = {Lee, Hojin and Cha, Hojun and Park, Junsuk and Choi, Seungmoon and Kim, Hyung-Sik and Chung, Soon-Cheol},
title = {LaserStroke: Mid-Air Tactile Experiences on Contours Using Indirect Laser Radiation},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985708},
doi = {10.1145/2984751.2985708},
abstract = {This demonstration presents a novel form of mid-air tactile display, LaserStroke, that makes use of a laser irradiated on the elastic medium attached to the skin. LaserStroke extends a laser device with an orientation control platform and a magnetic tracker so that it can elicit tapping and stroking sensations to a user's palm from a distance. LaserStroke offers unique tactile experiences while a user freely moves his/her hand in midair.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {73–74},
numpages = {2},
keywords = {tactile display, haptics, laser, mid-air, touch interaction},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985709,
author = {Kim, Han-Jong and Jeong, Yunwoo and Kim, Ju-Whan and Nam, Tek-Jin},
title = {M.Sketch: Prototyping Tool for Linkage-Based Mechanism Design},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985709},
doi = {10.1145/2984751.2985709},
abstract = {We present M.Sketch, a prototyping tool to support non-experts to design and build linkage-based mechanism prototype. It enables users to draw and simulate arbitrary mechanisms as well as to make physical prototype for testing actual movement. Mix of bottom-up and top-down sketching approaches, real-time movement visualization, and functions for digital fabrication can make the users to design the desired mechanism easily and effectively. M.Sketch can be used to design customized products with kinetic movement, such as interactive robot, toys, and sculptures.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {75–77},
numpages = {3},
keywords = {mechanism design, prototyping tool, digital fabrication, linkage-based mechanism, computational design},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985710,
author = {Shingu, Jun and Chiu, Patrick and Kratz, Sven and Vaughan, Jim and Kimber, Don},
title = {Depth Based Shadow Pointing Interface for Public Displays},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985710},
doi = {10.1145/2984751.2985710},
abstract = {We propose a robust pointing detection with virtual shadow representation for interacting with a public display. Using a depth camera, our shadow is generated by a model with an angled virtual sun light and detects the nearest point as a pointer. The position of the shadow becomes higher when user walks closer, which conveys the notion of correct distance to control the pointer and offers accessibility to the higher area of the display.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {79–80},
numpages = {2},
keywords = {large display, depth based interaction, pointing interface},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985711,
author = {Ueno, Saraha and Kato, Kunihiro and Miyashita, Homei},
title = {A Tangible Interface to Realize Touch Operations on the Face of a Physical Object},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985711},
doi = {10.1145/2984751.2985711},
abstract = {In this paper, we describe a tangible interface that can realize touch operations on a physical object. We printed physical objects that have conductive striped patterns using a multi-material 3D printer. The ExtensionSticker technique allows the user to operate capacitive touch-panel devices by tapping, scrolling, and swiping the physical object. By shaping the structure of conductive wiring inside a physical object, a variety of interfaces can be realized. We examined the conditions for using our proposed method on touch-panel devices.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {81–83},
numpages = {3},
keywords = {capacitive touch panel., physical interface, 3d printer},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985712,
author = {Ranasinghe, Nimesha and Jain, Pravar and Tolley, David and Karwita, Shienny and Yilei, Shi and Do, Ellen Yi-Luen},
title = {AmbioTherm: Simulating Ambient Temperatures and Wind Conditions in VR Environments},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985712},
doi = {10.1145/2984751.2985712},
abstract = {As Virtual Reality (VR) experiences become increasingly popular, simulating sensory perceptions of environmental conditions is essential for providing an immersive user experience. In this paper, we present Ambiotherm, a wearable accessory for existing Head Mounted Displays (HMD), which simulates real-world environmental conditions such as ambient temperatures and wind conditions. The system consists of a wearable accessory for the HMD and a mobile application, which generates interactive VR environments and controls the thermal and wind stimuli. The thermal stimulation module is attached to the user's neck while two fans are focused on the user's face to simulate wind conditions. We demonstrate the Ambiotherm system with two VR environments, a desert and a snowy mountain, to showcase the different types of ambient temperatures and wind conditions that can be simulated. Results from initial user experiments show that the participants perceive VR environments to be more immersive when external thermal and wind stimuli are presented as part of the VR experience.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {85–86},
numpages = {2},
keywords = {ambient temperature, multimodal interaction, virtual reality, virtual wind},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985713,
author = {Miyata, Natsuki and Honoki, Takehiro and Maeda, Yusuke and Endo, Yui and Tada, Mitsunori and Sugiura, Yuta},
title = {Wrap &amp; Sense: Grasp Capture by a Band Sensor},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985713},
doi = {10.1145/2984751.2985713},
abstract = {This paper proposes a bare hand grasp observation system named Wrap &amp; Sense. We built a band type sensing equipment composed of infrared distance sensors placed in an array. The sensor band is attached to a target object with all sensors directed along the object surface and detects the hand side edge with respect to the object. Assuming type of grasp as 'power grasp', the whole hand posture can be determined according to the 3D shape of the object. Three types of application are shown as proof-of-concept.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {87–89},
numpages = {3},
keywords = {a band type sensor, grasping posture, digital hand model},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985714,
author = {Suzuki, Katsuhiro and Nakamura, Fumihiko and Otsuka, Jiu and Masai, Katsutoshi and Itoh, Yuta and Sugiura, Yuta and Sugimoto, Maki},
title = {Facial Expression Mapping inside Head Mounted Display by Embedded Optical Sensors},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985714},
doi = {10.1145/2984751.2985714},
abstract = {Head Mounted Display (HMD) provides an immersive ex-perience in virtual environments for various purposes such as for games and communication. However, it is difficult to capture facial expression in a HMD-based virtual environ-ment because the upper half of user's face is covered up by the HMD. In this paper, we propose a facial expression mapping technology between user and a virtual avatar using embedded optical sensors and machine learning. The distance between each sensor and surface of the face is measured by the optical sensors that are attached inside the HMD. Our system learns the sensor values of each facial expression by neural network and creates a classifier to estimate the current facial expression.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {91–92},
numpages = {2},
keywords = {facial expression recognition, sensing, virtual reality, neural network, emotion, photo reflectivity},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985715,
author = {Ikematsu, Kaori and Siio, Itiro},
title = {An Input Switching Interface Using Carbon Copy Metaphor},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985715},
doi = {10.1145/2984751.2985715},
abstract = {This paper proposes a novel input technique that aims to switch between relative and absolute coordinates input methods seamlessly based on the "carbon copy" metaphor. We display a small workspace (``carbon copy area'') on a computer screen that corresponds one-to-one with the handy trackpad. The user can input hand-written characters or images using absolute coordinates input on this virtual carbon copy paper and move it anywhere using relative coordinates. Our technique allows a user to call both absolute and relative coordinates input methods and use them appropriately with arbitrary timing. Several advantages can be obtained by combining these methods. We developed a desktop application software to utilize this technique in a real GUI environment based on a user's evaluation.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {93–95},
numpages = {3},
keywords = {touch/haptic/pointing/gesture, input techniques},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985716,
author = {Shibasaki, Mina and Kamiyama, Youichi and Minamizawa, Kouta},
title = {Designing a Haptic Feedback System for Hearing-Impaired to Experience Tap Dance},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985716},
doi = {10.1145/2984751.2985716},
abstract = {In this study, we have designed a system to enable hearing-impaired to enjoy the performance of tap dancers. This system transfers the haptic sensation of tap dancing from the stage to the audience and helps hearing-impaired people enjoy the vibration of the taps even if they cannot hear the sound. We organized an event to verify the effectiveness of the system. To do this, we collaborated with a tap dance unit and science museum. We found that our system succeeded in helping the tap dancers share the fun and enjoyment of dance with the audience comprising people with hearing disabilities.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {97–99},
numpages = {3},
keywords = {hearing-impaired, haptics, tap dance, techtile, embodied interaction},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985717,
author = {Miyafuji, Shio and Sugasaki, Masato and Koike, Hideki},
title = {Ballumiere: Real-Time Tracking and Projection for High-Speed Moving Balls},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985717},
doi = {10.1145/2984751.2985717},
abstract = {Projection onto moving objects has a serious slipping problem due to delay between tracking and projection. We propose a new method to overcome the delay problem, and we succeed in increasing the accuracy of projection. We present Ballumiere as a demo for projection to volleyballs and juggling balls.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {101–102},
numpages = {2},
keywords = {tracking, prediction, projection mapping},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985718,
author = {Matsuda, Akira and Rekimoto, Jun},
title = {ScalableBody: A Telepresence Robot Supporting Socially Acceptable Interactions and Human Augmentation through Vertical Actuation},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985718},
doi = {10.1145/2984751.2985718},
abstract = {Most telepresence robots have a fixed-size body, and are unable to change the camera or display position. Therefore, although making eye contact is important in human expression, current fixed-size telepresence robots fail to achieve this.We propose a novel telepresence robot called ScalableBody, which enables users to make eye contact during conversations by changing its height. ScalableBody extends its body to modify the position of its camera or display. This approach provides eye contacts in remote conversations, thus creating almost same situation when the remote and local users make conversation like a real meeting. As for the remote users, this approach also enables them to experience having a conversation from different heights, such as being a giant or a dwarf. This technique extends the possibilities of remote communication by telepresence robots.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {103–105},
numpages = {3},
keywords = {telepresence robot, eye-contact, remote communication, surrogate robot, human augmentation},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985719,
author = {Sato, Toshiki and Koike, Hideki},
title = {MlioLight: Multi-Layered Image Overlay Using Multiple Flashlight Devices},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985719},
doi = {10.1145/2984751.2985719},
abstract = {We propose a technique that overlays natural images on the real world using the information from multiple flashlight devices. We focus on finding areas of overlapping lights in a multiple light-source scenario and overlaying multi-layered information on a real world object in these areas.In order to mix multiple images, we developed a light identification and overlapping area detection technique using rapid synchronization between high-speed cameras and multiple light devices.In this paper, we describe the concept of our system and a prototype implementation.We also describe two different applications.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {107–108},
numpages = {2},
keywords = {multiple flashlights, magic lens, projection mapping},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985720,
author = {Li, Xinxiao and Kuroda, Akira and Matsuzaki, Hidenori},
title = {Polyspector™: An Interactive Visualization Platform Optimized for Visual Analysis of Big Data},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985720},
doi = {10.1145/2984751.2985720},
abstract = {With the advent of the 'big data' era, there are unprecedented opportunities and challenges to explore complex and large datasets. In the paper, we introduce Polyspector, a web-based interactive visualization platform optimized for interactive visual analysis with two distinguishing features. Firstly, a visualization-specific database engine based on pixel-aware aggregation is implemented to generate views of hundreds of millions of data items within a second even with an off-the-shelf PC. Secondly, a novel deep-linking mechanism, combined with the pixel-aware aggregation, is exploited to realize interactive visual analysis interfaces such as zooming, overview + detail, context + focus etc.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {109–111},
numpages = {3},
keywords = {visualization, interactive visual analysis},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985721,
author = {Yokota, Tomohiro and Hashida, Tomoko},
title = {Hand Gesture and On-Body Touch Recognition by Active Acoustic Sensing throughout the Human Body},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985721},
doi = {10.1145/2984751.2985721},
abstract = {In this paper, we present a novel acoustic sensing technique that recognizes two convenient input actions: hand gestures and on-body touch. We achieved them by observing the frequency spectrum of the wave propagated in the body, around the periphery of the wrist. Our approach can recognize hand gestures and on-body touch concurrently in real-time and is expected to obtain rich input variations by combining them. We conducted a user study that showed classification accuracy of 97%, 96%, and 97% for hand gestures, touches on the forearm, and touches on the back of the hand.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {113–115},
numpages = {3},
keywords = {combined input, hand gestures, on-body touch, machine learning, acoustic sensing},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985725,
author = {Choi, Inrak and Follmer, Sean},
title = {Wolverine: A Wearable Haptic Interface for Grasping in VR},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985725},
doi = {10.1145/2984751.2985725},
abstract = {The Wolverine is a mobile, wearable haptic device designed for simulating the grasping of rigid objects in virtual environment. In contrast to prior work on force feedback gloves, we focus on creating a low cost, lightweight, and wireless device that renders a force directly between the thumb and three fingers to simulate objects held in pad opposition type grasps. Leveraging low-power brake-based locking sliders, the system can withstand over 100N of force between each finger and the thumb, and only consumes 2.78 Wh(10 mJ) for each braking interaction. Integrated sensors are used both for feedback control and user input: time-of-flight sensors provide the position of each finger and an IMU provides overall orientation tracking. This design enables us to use the device for roughly 6 hours with 5500 full fingered grasping events. The total weight is 55g including a 350 mAh battery.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {117–119},
numpages = {3},
keywords = {force feedback, wearable haptic interfaces, virtual reality},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985727,
author = {Hiraki, Takefumi and Narumi, Koya and Yatani, Koji and Kawahara, Yoshihiro},
title = {Phones on Wheels: Exploring Interaction for Smartphones with Kinetic Capabilities},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985727},
doi = {10.1145/2984751.2985727},
abstract = {This paper introduces novel interaction and applications using smartphones with kinetic capabilities. We develop an accessory module with robot wheels for a smartphone. With this module, the smartphone can move in a linear direction or rotate with sufficient power. The module also includes rotary encoders, allowing us to use the wheels as an input modality. We demonstrate a series of novel mobile interaction for mobile devices with kinetic capabilities through three applications.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {121–122},
numpages = {2},
keywords = {kinetic interaction, mobile interfaces},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985728,
author = {Strasnick, Evan and Follmer, Sean},
title = {Applications of Switchable Permanent Magnetic Actuators in Shape Change and Tactile Display},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985728},
doi = {10.1145/2984751.2985728},
abstract = {Systems realizing shape change and tactile display remain hindered by the power, cost, and size limitations of current actuation technology. We describe and evaluate a novel use of switchable permanent magnets as a bistable actuator for haptic feedback which draws power only when switching states. Because of their efficiency, low cost, and small size, these actuators show promise in realizing tactile display within mobile, wearable, and embedded systems. We present several applications demonstrating potential uses in the mobile, automotive, and desktop computing domains, and perform a technical evaluation of the actuators used in these systems.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {123–125},
numpages = {3},
keywords = {low-power, low-cost, tactile, actuator, haptic, mobile, low coercivity, shape change, magnet, display, magnetic},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985729,
author = {Ranasinghe, Nimesha and Do, Ellen Yi-Luen},
title = {Virtual Sweet: Simulating Sweet Sensation Using Thermal Stimulation on the Tip of the Tongue},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985729},
doi = {10.1145/2984751.2985729},
abstract = {Being a pleasurable sensation, sweetness is recognized as the most preferred sensation among the five primary taste sensations. In this paper, we present a novel method to virtually simulate the sensation of sweetness by applying thermal stimulation to the tip of the human tongue. To digitally simulate the sensation of sweetness, the system delivers rapid heating and cooling stimuli to the tongue via a 2x2 grid of Peltier elements. To achieve distinct, controlled, and synchronized temperature variations in the stimuli, a control module is used to regulate each of the Peltier elements. Results from our preliminary experiments suggest that the participants were able to perceive mild sweetness on the tip of their tongue while using the proposed system.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {127–128},
numpages = {2},
keywords = {thermal taste, multimodal interaction, virtual reality, sweet, virtual sweet},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985730,
author = {Akiyama, Yoh and Miyashita, Homei},
title = {Fitter: A System for Easily Printing Objects That Fit Real Objects},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985730},
doi = {10.1145/2984751.2985730},
abstract = {When printing both self-making and existing 3D models, users often create models to fit to a real object within it. Fitting models to the size of a real object is a delicate problem. To address it, we present a concept to capture the size of a real object, create or modify a model that conforms to the captured image, and print the model on the spot. We create a 3D printer to realize this concept by installing a touch panel display in the build plate system. In this paper, we focus on creating containers that fit accessories. We create containers for a pair of scissors, a smart watch, a drone, a pair of glasses, and a pen holder.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {129–131},
numpages = {3},
keywords = {fabrication, fit real objects, what you see is what you get, fused deposition modeling, 3d printer},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985731,
author = {Wei, Shi-Yao and Wang, Chen-Yu and Chiu, Ting-Wei and Lo, Yi-Ping and Yang, Zhi-Wei and Wang, Hsing-Man and Hung, Yi-ping},
title = {RunPlay: Action Recognition Using Wearable Device Apply on Parkour Game},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985731},
doi = {10.1145/2984751.2985731},
abstract = {In this paper, we present an action recognition system which consists of pressure insoles, with 16 pressure sensors, and an inertial measurement unit. By analysing the data measured from these sensors, we are able to recognised several human activities. In this circumstance, we focus on the detection of jumping, squatting, moving left and right. We also designed a parkour game on a mobile device to demonstrate the in-game control of an avatar by human action.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {133–135},
numpages = {3},
keywords = {action recognition, inertial measurement unit (imu), internet of things., activity identification, pressure insole},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985733,
author = {Kikuchi, Junki and Yanagi, Hidekatsu and Mima, Yoshiaki},
title = {Music Composition with Recommendation},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985733},
doi = {10.1145/2984751.2985733},
abstract = {Creating a piece of music requires deep knowledge of composition, and is time-consuming even for experts. Algorithmic composition systems can generate pieces in an existing style. However, these systems are not interactive. Therefore, it is difficult for them to express the user's intention. We propose a system that recommends a continuation melody in accordance with a melody expressed by the user. Recommendation uses the style of the piece of the composer, thus users give the system a piece of the style in which they want to compose. With this system, users can compose pieces tailored to their needs, and composers can get assistance with composition.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {137–138},
numpages = {2},
keywords = {recommendation, music, music composition, interaction},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985734,
author = {Withana, Anusha and Ransiri, Shanaka and Kaluarachchi, Tharindu and Singhabahu, Chanaka and Shi, Yilei and Elvitigala, Samitha and Nanayakkara, Suranga},
title = {WaveSense: Ultra Low Power Gesture Sensing Based on Selective Volumetric Illumination},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985734},
doi = {10.1145/2984751.2985734},
abstract = {We present waveSense, a low power hand gestures recogni- tion system suitable for mobile and wearable devices. A novel Selective Volumetric Illumination (SVI) approach using off-the-shelf infrared (IR) emitters and non-focused IR sensors were introduced to achieve the power efficiency. Our current implementation consumes 8.65mW while sensing hand gestures within 60cm radius from the sensors. In this demo, we introduce the concept and the theoretical background of waveSense, details of the prototype implementation, and application possibilities.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {139–140},
numpages = {2},
keywords = {hand gesture recognition, smart wearables, selective volumetric illumination, compressive sensing, interacting with virtual reality devices},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985735,
author = {Yamada, Wataru and Manabe, Hiroyuki},
title = {Expanding the Field-of-View of Head-Mounted Displays with Peripheral Blurred Images},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985735},
doi = {10.1145/2984751.2985735},
abstract = {Head-mounted displays are rapidly becoming popular. Field-of-view is one of the key parameters of head-mounted displays, because a wider field-of-view gives higher presence and immersion in the virtual environment. However, wider field-of-view often increase device cost and weight because it needs complicated optics or expensive modules such as multi high-resolution displays or complex lenses. This paper proposes a method that expands the field-of-view by using two kinds of lenses with different levels of magnification. The principle of the proposed method is that Fresnel lenses with high magnification surround convex lenses to fill the peripheral vision with a blurred image. The proposed method doesn't need complicated optics, and is advantageous in terms of device cost and weight, because only two additional Fresnel lenses are necessary. We implement a prototype and confirm that the Fresnel lenses fill the peripheral with a blurred image, and effectively expand the field-of-view.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {141–142},
numpages = {2},
keywords = {field-of-view, virtual reality, head-mounted display},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985736,
author = {Tomohiro, Ayuri and Sumi, Yasuyuki},
title = {Reconstruction of Scene from Multiple Sketches},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985736},
doi = {10.1145/2984751.2985736},
abstract = {This paper discusses the feasibility of extension of expressive style with multiple 3D sketches drawn by a sketching tool that enables its users to draw and paint on 3D structured surfaces. Users of our proposed system take a picture of target objects and sketch with reference to the taken picture. They can not only sketch on the pictures but can also change their viewpoint of the sketched environment, since the system captures 3D structure by using a depth sensor as well as RGB data. Trial usage of the system shows that our users can rapidly extract their target objects/space and extend their ideas by taking pictures and drawing/painting on them. This paper presents examples of system usage, and discusses the feasibility of extension of sketches.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {143–144},
numpages = {2},
keywords = {interactive illustrations, sketching},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985737,
author = {Ikematsu, Kaori and Sasagawa, Mana and Siio, Itiro},
title = {2.5 Dimensional Panoramic Viewing Technique Utilizing a Cylindrical Mirror Widget},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985737},
doi = {10.1145/2984751.2985737},
abstract = {We propose a panoramic viewing system, which applies the technique of Anamorphosis, mapping a 2D display onto a cylindrical mirror. In this system, a distorted scene image is shown on a flat panel display or tabletop surface. When a user places the cylindrical mirror on the display, the original image appears on the cylindrical mirror.By simply rotating the cylinder, a user can decide the direction to walk-through in VR world.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {145–146},
numpages = {2},
keywords = {anamorphosis, phicon},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985738,
author = {Asai, Yuki and Ueda, Yuta and Enomoto, Ryuichi and Iwai, Daisuke and Sato, Kosuke},
title = {ExtendedHand on Wheelchair},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985738},
doi = {10.1145/2984751.2985738},
abstract = {In this paper, we present a novel welfare system which utilizes a spatial augmented reality technique. Hand is a crucial component in human-human communication. For example, we can intuitively indicate an object or place by reaching and pointing it to nearby partners. Unfortunately, for wheelchair users, such communication is often limited because their reaching ranges are narrow, and moving their bodies to the target is tiresome. To solve this issue, we propose a novel wheelchair system on which a battery-powered mobile projector is mounted. A user manipulates the projected virtual hand as an extension of the real one using a touch panel equipped on an armrest of the wheelchair. We implement our proposed system and demonstrate the effectiveness.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {147–148},
numpages = {2},
keywords = {communication, virtual hand, tablet, wheelchair},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985739,
author = {Konishi, Yukari and Hanamitsu, Nobuhisa and Outram, Benjamin and Minamizawa, Kouta and Sato, Ayahiko and Mizuguchi, Tetsuya},
title = {Synesthesia Suit},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985739},
doi = {10.1145/2984751.2985739},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {149},
numpages = {1},
keywords = {haptic design, virtual reality, haptic suit, techtile},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985740,
author = {Kawakatsu, Ryosuke and Hirai, Shigeyuki},
title = {Interaction Technique Using Acoustic Sensing for Different Squeak Sounds Caused by Number of Rubbing Fingers},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985740},
doi = {10.1145/2984751.2985740},
abstract = {Various studies have been conducted for developing interaction techniques in a smart house. Some of our previous studies [1, 2] focused on bathrooms and we converted an existing normal bathtub system into a user interface by using embedded sensors. A system called Bathcratch [2] detects squeak sounds by rubbing on a bathtub edge. To generate squeaks, it requires some conditions to cause the Stick-slip phenomenon. A bathtub has a smooth surface, and water can cause the phenomenon with human skins. Bathcratch uses it as an interaction technique to play DJ-scratching. Here, we extended the interaction technique using squeaks to recognize rubbing states, rubbing events including sequence, and the difference between squeaks caused by the number of fingers. This can be used in various wet environments including kitchen, washbowls in a restroom, swimming pool, and spa. This paper describes the method and its performance for identifying the number of rubbing fingers by using frequency analysis. In addition, we illustrate some smart home applications by using the proposed technique.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {151–153},
numpages = {3},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985741,
author = {LIU, Xin and Vega, Katia and Qian, Jing and Paradiso, Joseph and Maes, Pattie},
title = {Fluxa: Body Movements as a Social Display},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985741},
doi = {10.1145/2984751.2985741},
abstract = {This paper presents Fluxa, a compact wearable device that exploits body movements, as well as the visual effects of persistence of vision (POV), to generate mid-air displays on and around the body. When the user moves his/her limb, Fluxa displays a pattern that, due to retinal afterimage, can be perceived by the surrounding people. We envision Fluxa as a wearable display to foster social interactions. It can be used to enhance existing social gestures such as hand-waving to get attention, as a communicative tool that displays the speed and distance covered by joggers, and as a self-expression device that generates images while dancing. We discuss the advantages of Fluxa: a display size that could be much larger than the device itself, a semi-transparent display that allows users and others to see though it and promotes social interaction.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {155–157},
numpages = {3},
keywords = {display, spatial-temporal interfaces, wearables, persistence of vision, social display, body movement},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985742,
author = {Gomes, Antonio and Priyadarshana, Lahiru and Carrascal, Juan Pablo and Vertegaal, Roel},
title = {WhammyPhone: Exploring Tangible Audio Manipulation Using Bend Input on a Flexible Smartphone},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985742},
doi = {10.1145/2984751.2985742},
abstract = {We present WhammyPhone, a novel audio interface that supports physical manipulation of digital audio through bend gestures. WhammyPhone combines a high-resolution flexible display, bend sensors, and a set of intuitive interaction techniques that enable novice users to manipulate sound in a tangible fashion. With WhammyPhone, bend gestures can control both discrete (e.g. triggering a note) and continuous parameters (e.g. pitch bend). We showcase application scenarios that leverage the unique input modalities of WhammyPhone and discuss its potential for digital audio manipulation.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {159–161},
numpages = {3},
keywords = {music interfaces., tangible interaction, organic user interfaces, bend input, sound mixing, flexible displays},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2985743,
author = {Tamaki, Emi and Chan, Terence and Iwasaki, Ken},
title = {UnlimitedHand: Input and Output Hand Gestures with Less Calibration Time},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2985743},
doi = {10.1145/2984751.2985743},
abstract = {Numerous devices that either track hand gestures or provide haptic feedback have been developed with the aim of manipulating objects within Virtual Reality(VR) and Augmented Reality(AR) environments. However, these devices implement lengthy calibration processes to ease out individual differences. In this research, a wearable device that simultaneously recognizes hand gestures and outputs haptic feedback: UnlimitedHand is suggested. Photo-reflectors are placed over specific muscle groups on the forearm to read in hand gestures. For output, electrodes are placed over the same muscles to control the user's hand movements. Both sensors and electrodes target main muscle groups responsible for moving the hand. Since the positions of these muscle groups are common between humans, UnlimitedHand is able to reduce the time spent on performing calibration.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {163–165},
numpages = {3},
keywords = {ems(electric muscle stimulation), electric stimulation, ar(augmented reality), vr(virtual reality), photo-reflector array, haptic sensation, hand gesture, fes(functional electric stimulation)},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984752,
author = {Youn, Kyungmin},
title = {Orchestrated Informal Care Coordination: Designing a Connected Network of Tools in Support of Collective Care Activities for Informal Caregivers},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984752},
doi = {10.1145/2984751.2984752},
abstract = {Often, family caregivers experience difficulties in coordinating older adults' health care because it requires not only a lot of time but also a diverse set of responsibilities to coordinate care for their loved ones. While many can reduce their individual burden by sharing care tasks with other family members, there are still many challenges to overcome in maintaining the quality of care when they work together. As they increase their informal care network, it becomes more difficult for them to stay informed and coordinated. Coordination breakdowns caused by having multiple caregivers who are cooperating to care for the same care recipient result in reduced quality of care. I explored opportunities for "Internet of Things (IoT)" technologies to help informal caregivers better coordinate and communicate care with each other for their loved ones. Based on identified design opportunities, I propose the concept of CareBot, a smart home platform consisting of interactive tools in support of collective care activities of family caregivers. },
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {167–168},
numpages = {2},
keywords = {qualitative methods, caregiving, internet of things, caregiver, connected home, field study},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984753,
author = {Yokomizo, Yukiko and Kotegawa, Tomoya and Haimes, Paul and Baba, Tetsuaki},
title = {Switch++: An Output Device of the Switches by the Finger Gestures},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984753},
doi = {10.1145/2984751.2984753},
abstract = {Regarding human-machine-interfaces, switches have not changed significantly despite the machines themselves evolving constantly. In this paper, we propose a new method of operability for devices by providing multiple switches dynamically, and users choose the switch that has the functionality that they want to use. Switch++ senses the mental model of the operating sensation of switches against the user's finger gestures and changes the shape of the switch and its affordances accordingly. We design the interface based on the raw data.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {169–170},
numpages = {2},
keywords = {interfaces, gestures., switch, affordances},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984754,
author = {Pai, Yun Suen and Outram, Benjamin and Vontin, Noriyasu and Kunze, Kai},
title = {Transparent Reality: Using Eye Gaze Focus Depth as Interaction Modality},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984754},
doi = {10.1145/2984751.2984754},
abstract = {We present a novel, eye gaze based interaction technique, using focus depth as an input modality for virtual reality (VR) applications. We also show custom hardware prototype implementation. Comparing the focus depth based interaction to a scroll wheel interface, we find no statistically significant difference in performance (the focus depth works slightly better) and a subjective preference of the users in a user study with 10 participants playing a simple VR game. This indicates that it is a suitable interface modality that should be further explored. Finally, we give some application scenarios and guidelines for using focus depth interactions in VR applications.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {171–172},
numpages = {2},
keywords = {eye tracking, focus depth, interaction modality, virtual reality},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984755,
author = {Takashina, Tomomi and Tamura, Tsutomu and Nakazumi, Makoto and Nomura, Tatsushi and Kokumai, Yuji},
title = {Toward a Compact Device to Interact with a Capacitive Touch Screen},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984755},
doi = {10.1145/2984751.2984755},
abstract = {Capacitive touch screens are widely used in various products. Touch screens have an advantage that an input system and output system can be integrated into a single module. We consider this advantage could make it possible to realize a new universal interface for both human-to-machine (H2M) and machine-to-machine (M2M). For a M2M interface, some sort of method to simulate finger touching is needed. Therefore, we propose an alternative method to interact with a touch screen using two electrical approaches. Our proposal is effective in automating touch screen operations, modality conversion device for people with disabilities, and so on. We assembled a prototype to confirm the principle to control a touch screen with the electrical methods. We believe that our proposal will complement the weakness of touch screens and expand their possibility.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {173–174},
numpages = {2},
keywords = {m2m by h2m, supportive user interface, touch screen},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984756,
author = {Ando, Mitsuhito and Murakami, Chisaki and Ito, Takayuki and Jo, Kazuhiro},
title = {Initial Trials of OfxEpilog: From Real Time Operation to Dynamic Focus of Epilog Laser Cutter},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984756},
doi = {10.1145/2984751.2984756},
abstract = {This paper describes ofxEpilog which enable people to control a laser cutter of Epilog in real time. ofxEpilog is an add on of open Frameworks, an open source C++ toolkit for creative coding. With the add on, people could directly send their image object to a laser cutter through Ethernet. By alternating the generation and transmission of the command of cutting, the add on could sequentially control a laser cutter in real time. This paper introduces our initial trials of ofxEpilog with a real time operation (A), dynamic focus (z-axis) control with a given 3D object (B), and a scanned 3D object (C). Technical limitations and our upcoming challenges are also discussed.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {175–176},
numpages = {2},
keywords = {personal fabrication, laser cutter, open source toolkit},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984757,
author = {Ueno, Keiichi and Go, Kentaro and Kinoshita, Yuichiro},
title = {Design and Evaluation of EdgeWrite Alphabets for Round Face Smartwatches},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984757},
doi = {10.1145/2984751.2984757},
abstract = {This study presents a project aimed at designing and evaluating a unistroke gesture set of alphanumeric characters targeting round-face smartwatches. We conducted a user study with 10 participants to generate the basic gesture design for 40 characters. For each character, we measured the preference and agreement scores and uncovered any challenges faced in designing unistroke gestures for round-face smartwatches. We developed a gesture recognizer using machine learning, which used a backpropagation mechanism to evaluate the designed gestures. Using the gesture recognizer, we collected 80,000 gesture data, and evaluated them with 5-fold cross-validation. The obtained mean recognition rate was 92.14%.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {177–178},
numpages = {2},
keywords = {text entry, edges, smartwatches, unistrokes},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984758,
author = {Farooq, Ahmed and Weitz, Philipp and Evreinov, Grigori and Raisamo, Roope and Takahata, Daisuke},
title = {Touchscreen Overlay Augmented with the Stick-Slip Phenomenon to Generate Kinetic Energy},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984758},
doi = {10.1145/2984751.2984758},
abstract = {Kinesthetic feedback requires linkage-based high-powered multi-dimensional manipulators, which are currently not possible to integrate with mobile devices. To overcome this challenge, we developed a novel system that can utilize a wide range of actuation components and apply various techniques to optimize stick-slip motion of a tangible object on a display surface. The current setup demonstrates how it may be possible to generate directional forces on an interactive display in order to move a linkage-free stylus over a touchscreen in a fully controlled and efficient manner. The technology described in this research opens up new possibilities for interacting with displays and tangible surfaces such as continuously supervised learning; active feed-forward systems as well as dynamic gaming environments that predict user behavior and are able modify and physically react to human input at real-time.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {179–180},
numpages = {2},
keywords = {user interfaces, multimodal interaction, tangible interfaces},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984759,
author = {Kim, Soomin and Oh, JongHwan and Lee, Joonhwan},
title = {Histogram: Spatiotemporal Photo-Displaying Interface},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984759},
doi = {10.1145/2984751.2984759},
abstract = {As the smartphone has become more widely available, we easily take photos and upload them online to share with others. Photographs are abundant, but they are not used properly, even though they provide meaningful information about the social scenes of our daily lives. To address this issue, Histogram was created as a new interface for displaying and sharing location-related photographs chronologically to trace the changes in a location. The prototype of this system is mobile-optimized to encourage users to easily upload photos with their smartphones, so that the system can be run through social cooperative work.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {181–182},
numpages = {2},
keywords = {geo-tagged photography, photograph interface, historical geographic information, digital photography, timeline, history tracking},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984760,
author = {Yamashita, Shogo and Zhang, Xinlei and Rekimoto, Jun},
title = {AquaCAVE: Augmented Swimming Environment with Immersive Surround-Screen Virtual Reality},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984760},
doi = {10.1145/2984751.2984760},
abstract = {AquaCAVE is a system for enhancing the swimming experience. Although swimming is considered to be one of the best exercises to maintain our health, swimming in a pool is normally monotonous; thus, maintaining its motivation is sometimes difficult. AquaCAVE is a computer-augmented swimming pool with rear-projection acrylic walls that surround a swimmer, providing a CAVE-like immersive stereoscopic projection environment. The swimmer wears goggles with liquid-crystal display (LCD) shutter glasses, and cameras installed in the pool tracks swimmer's head position. Swimmers can be immersed into synthetic scenes such as coral reefs, outer space, or any other computer generated environments. The system can also provide swimming training with projections such as record lines and swimming forms as 3D virtual characters in the 3D space.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {183–184},
numpages = {2},
keywords = {immersive environment, augmented sports, virtual reality, projection-based systems, swimming, cave},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984761,
author = {Nagatomo, Takehiro and Tachibana, Takahiro and Sato, Keizo and Nakashima, Makoto},
title = {Partial Bookmarking: A Structure-Independent Mechanism of Transclusion for a Portion of Any Web Page},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984761},
doi = {10.1145/2984751.2984761},
abstract = {A novel mechanism of transclusion for collecting and producing information on the Web, named partial bookmarking, is proposed. Partial bookmarking allows a user to collect portions of any web page by making it able to use for a spatial hypertext, like a web document element, without the need to duplicate its contents. Whereas the previous studies involving transclusion required pre-designed linkable objects, such as XML elements or HTML objects, partial bookmarking does not rely on any document structure. To accomplish partial bookmarking, we enhanced a conventional web browser with multiple tabs by introducing the technology of mirroring to display only a portion of a web page appropriately while factoring in potential copyright issues.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {185–186},
numpages = {2},
keywords = {transclusion, partial bookmarking, spatial hypertext},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984762,
author = {Peiris, Roshan Lalintha and Chan, Liwei and Minamizawa, Kouta},
title = {Thermocons: Evaluating the Thermal Haptic Perception of the Forehead},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984762},
doi = {10.1145/2984751.2984762},
abstract = {Thermocons describes our work in progress for evaluating thermal haptic feedback on the forehead as a viable feedback modality for integration with head mounted devices. The purpose was to identify the thermal perception for simultaneous feedback at three locations of the forehead. We provided hot-only, cold-only and hot/cold-mixed thermal stimulations at these location to identify the sensitivity for accurate perception. Our evaluation with 9 participants indicated that perceiving cold-only stimulations were significantly better with an accuracy of 88%. The perception accuracy for hot-only and hot/cold-mixed stimulations were 66% and 65% respectively. },
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {187–188},
numpages = {2},
keywords = {haptics, head, thermal haptics, forehead},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984763,
author = {Yeo, Hui-Shyong and Lee, Juyoung and Bianchi, Andrea and Quigley, Aaron},
title = {Sidetap &amp; Slingshot Gestures on Unmodified Smartwatches},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984763},
doi = {10.1145/2984751.2984763},
abstract = {We present a technique for detecting gestures on the edge of an unmodified smartwatch. We demonstrate two exemplary gestures, i) Sidetap -- tapping on any side and ii) Slingshot -- pressing on the edge and then releasing quickly. Our technique is lightweight, as it relies on measuring the data from the internal Inertial measurement unit (IMU) only. With these two gestures, we expand the input expressiveness of a smartwatch, allowing users to use intuitive gestures with natural tactile feedback, e.g., for the rapid navigation of a long list of items with a tap, or act as shortcut commands to launch applications. It can also allow for eyes-free interaction or subtle interaction where visual attention is not available.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {189–190},
numpages = {2},
keywords = {slingshot, tapping gesture, sidetap, smartwatches},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984764,
author = {Putra, Handityo Aulia and Ren, Xiangshi},
title = {Developing FMRI-Compatible Interaction Systems through Air Pressure},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984764},
doi = {10.1145/2984751.2984764},
abstract = {We leverage the use of air pressure to expand the interaction space within fMRI (functional magnetic resonance imaging). We present three example applications that are not previously possible in conventional fMRI interaction devices: 1) pedal interface that can record continuous pressure value pressed by users, 2) wrist tactile interface that can provide various tactile patterns or stimuli, 3) adjustable resistance joystick that can provide feedback through different resistance levels. Our work shows that the use of air pressure can enable new research opportunities for fMRI researchers.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {191–192},
numpages = {2},
keywords = {air pressure, response device, fmri, tactile feedback},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984765,
author = {Li, Zhengqing and Miyafuji, Shio and Sato, Toshiki and Koike, Hideki},
title = {OmniEyeball: Spherical Display Embedded With Omnidirectional Camera Using Dynamic Spherical Mapping},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984765},
doi = {10.1145/2984751.2984765},
abstract = {Recently, 360-degree panorama and spherical displays have received more and more attention due to their unique panoramic properties. Compared with existing works, we plan to utilize omnidirectional cameras in our spherical display system to enable omnidirectional panoramic image as input and output. In our work, we present a novel movable spherical display embedded with omnidirectional cameras. Our system can use embedded cameras to shoot 360-degree panoramic video and project the live stream from its cameras onto its spherical display in real time.In addition, we implemented an approach to achieve the dynamic spherical projection mapping in order to project to moving spherical devices. We have also been creating applications utilizing system's features by using 360-degree panoramic image as input and output.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {193–194},
numpages = {2},
keywords = {spherical display, projection mapping, omnidirectional camera, 360-degree panorama},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984766,
author = {Funato, Nobuhiro and Takemura, Kentaro},
title = {Estimating Contact Force of Fingertip and Providing Tactile Feedback Simultaneously},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984766},
doi = {10.1145/2984751.2984766},
abstract = {This study proposes a method for estimating the contact force of the fingertip by inputting vibrations actively. The use of active bone-conducted sound sensing has been limited to estimating the joint angle of the elbow and the finger. We applied it to the method for estimating the contact force of the fingertip. Unlike related works, it is not necessary to mount the device on a fingertip, and tactile feedback is enabled using tangible vibrations.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {195–196},
numpages = {2},
keywords = {haptic interface, vibration, active sensing, contact force},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984767,
author = {Funakoshi, Ryohei and Boddeti, Vishnu Naresh and Kitani, Kris and Koike, Hideki},
title = {Activity-Aware Video Stabilization for BallCam},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984767},
doi = {10.1145/2984751.2984767},
abstract = {We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {197–198},
numpages = {2},
keywords = {activity segmentation, ballcam, video stabilization},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984768,
author = {Niijima, Arinobu and Ogawa, Takefumi},
title = {Study on Control Method of Virtual Food Texture by Electrical Muscle Stimulation},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984768},
doi = {10.1145/2984751.2984768},
abstract = {We propose Electric Food Texture System, which can present virtual food texture such as hardness and elasticity by electrical muscle stimulation (EMS) to the masseter muscle. In our previous study, we investigated the feasibility to detect user's bite with a photoreflector and that to construct database of food texture with electromyography sensors. In this paper, we investigated the feasibility to control virtual food texture by EMS. We conducted an experiment to reveal the relationship of the parameters of EMS and those of virtual food texture. The experimental results show that the higher strength of EMS is, the harder virtual food texture is, and the longer duration of EMS is, the more elastic virtual food texture is.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {199–200},
numpages = {2},
keywords = {food texture, virtual reality, electrical muscle stimulation},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984769,
author = {Karani, Salem and Varanasi, Chaitanya},
title = {Uniformity Based Haptic Alert Network},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984769},
doi = {10.1145/2984751.2984769},
abstract = {We experience haptic feedback on a wide variety of devices in the modern day, including cellphones, tablets, and smartwatches. However haptic alerts can quickly become disruptive rather than helpful to a user when multiple devices are providing feedback simultaneously or consecutively. Thus in this paper, we propose an intercommunicating, turn-based local network between a user's devices. This will allow a guaranteed minimal time span between device alerts. Additionally, when multiple devices provide a notification-based haptic alert, devices often produce different feedback due to the varying materials they are placed on. To address this, our framework allows devices to self-regulate their levels of haptic responses based on the material density of the surface they are placed on. This allows the framework to enforce a uniform level of haptic feedback across all the surface-device combinations. Finally, we will also utilize this common network to eliminate redundant alerts across devices.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {201–202},
numpages = {2},
keywords = {haptic feedback, distributed network, material haptic response, intercommunicating, haptic alert},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984770,
author = {Verma, Pramod},
title = {Flying User Interface},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984770},
doi = {10.1145/2984751.2984770},
abstract = {This paper describes a special type of drone called "Flying User Interface", comprised of a robotic projector-camera system, an onboard digital computer connected with the Internet, sensors, and a hardware interface capable of sticking to any surface such as wall, ceilings, etc. Computer further consists of other subsystems, devices, and sensors such as accelerometer, compass, gyroscope, flashlight, etc. Drone flies from one place to another, detects a surface, and attaches itself to it. After a successful attachment, the device stops all its rotators; it then projects or augments images, information, and user interfaces on nearby surfaces and walls. User interface may contain applications, information about object being augmented and information from Internet. User can interact with user-interface using commands and gestures such as hand, body, feet, voice, etc.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {203–204},
numpages = {2},
keywords = {user interface, augmented reality., display device, drone},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984771,
author = {Chae, Han Joo and Hwang, Jeong-in and Choi, Yuri and Kim, Yieun and Koh, Kyle and Seo, Jinwook},
title = {Prevention of Unintentional Input While Using Wrist Rotation for Device Configuration},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984771},
doi = {10.1145/2984751.2984771},
abstract = {We describe the design of the safeguard interface that helps users avoid unintentional input while using wrist rotation. When configuring the parameters of various devices, our interface helps reduce the chance of making accidental changes by delaying the result of input and allowing the users to make deliberate attempt to change the parameters to their desired value. We evaluated our methods with a set of user experience and found that our methods were more preferred when the end-results of configurational changes of the devices become more critical and can cause irreversible damage.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {205–206},
numpages = {2},
keywords = {internet of things, wrist rotation, radial meter},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984772,
author = {Lee, Jooyeon and Cheon, Manri and Moon, Seong-Eun and Lee, Jong-Seok},
title = {Peripersonal Space in Virtual Reality: Navigating 3D Space with Different Perspectives},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984772},
doi = {10.1145/2984751.2984772},
abstract = {We introduce the concept of "peripersonal space" of an avatar in 3D virtual reality and discuss how it plays an important role on 3D navigation with different perspectives. By analyzing the eye-gaze data of avatar-based navigation with first-person perspective and third-person perspective, we examine the effects of an avatar's peripersonal space on the users' perceptual scopes within 3D virtual environments. We propose that manipulating peripersonal space of an avatar with various perspectives has the immediate effects on the users' scopes of perception as well as the patterns of attentional capture. This study provides a helpful guideline for designing more effective navigation system with an avatar in 3D virtual environment.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {207–208},
numpages = {2},
keywords = {perspective, gaze analysis, virtual space, human perception and cognition, navigation, peripersonal space, eye-tracking},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984773,
author = {Chiu, Yu-Kai and Chang, Hao-Yu and Yang, Wan-ling and Huang, Yu-Hsuan and Ming, Ouhyoung},
title = {A Novel Real Time Monitor System of 3D Printing Layers for Better Slicing Parameter Setting},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984773},
doi = {10.1145/2984751.2984773},
abstract = {We proposed a novel real time monitor system of 3D printer with dual cameras, which capture and reconstruct the printed result layer by layer. With the reconstructed image, we can apply computer vision technique to evaluate the difference with the ideal path generate by G-code. The difference gives us clues to classify which might be the possible factor of the result. Hence we can produce advice to user for better slicing parameter settings. We believe that this system can give helps to beginner or users of 3D printer that struggle in parameter settings in the future.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {209–210},
numpages = {2},
keywords = {"3d printer, dual camera, slicing parameters, machine vision, monitor system},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984774,
author = {Frier, William and Seo, Kyoungwon and Subramanian, Sriram},
title = {Hilbert Curves: A Tool for Resolution Independent Haptic Texture},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984774},
doi = {10.1145/2984751.2984774},
abstract = {Haptic systems usually stimulate the kinesthetic aspects of the sense of touch, i.e. force feedback systems. But more and more devices aim to stimulate the cutaneous part of the sense of touch to reproduce more complex tactile sensations. To do so, they stimulate one's fingertip in different locations, usually in the fashion of a matrix pattern. In this paper we investigate the new possibilities that are offered by such a framework and present an ongoing project that investigates the benefits of Hilbert curves to display resolution independent mid-air haptic textures in comparison with other implementation approaches.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {211–212},
numpages = {2},
keywords = {resolution, mid-air haptic, hilbert curves, rendering, texture},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984775,
author = {Nunez, Eleuda and Visentin, Francesco and Suzuki, Kenji},
title = {Friend*Chip: A Bracelet with Digital Pet for Socially Inclusive Games for Children},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984775},
doi = {10.1145/2984751.2984775},
abstract = {Learning in groups have different potential benefits for children. They have the opportunity to solve problems together, to share experiences and to develop social skills. However, from teachers point of view, creating a safe and inclusive positive environment for children is not an simple task since each child has differences that represent a challenge for implementing effectively group dynamics. The focus of this work is the design of a system that motivates children to approach to others and create opportunities of social interaction. The system creates a fun and enjoyable situation that is always supervised by the teacher, who can monitor and change the group dynamics at any moment during the activity.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {213–214},
numpages = {2},
keywords = {wearable device, group dynamics, persuasive technologies},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984776,
author = {Rosello, Oscar and Exposito, Marc and Maes, Pattie},
title = {NeverMind: Using Augmented Reality for Memorization},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984776},
doi = {10.1145/2984751.2984776},
abstract = {NeverMind is an interface and application designed to support human memory. We combine the memory palace memorization method with augmented reality technology to create a tool to help anyone memorize more effectively. Preliminary experiments show that content memorized with NeverMind remains longer in memory compared to general memorization techniques. With this project, we hope to make the memory palace method accessible to novices and demonstrate one way augmented reality can support learning.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {215–216},
numpages = {2},
keywords = {method of loci, memory palace, memory augmentation, educational interfaces, augmented reality},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984777,
author = {Lee, Jihye and Lee, Sangwon},
title = {Analysis of Sequential Tasks in Use Context of Mobile Apps},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984777},
doi = {10.1145/2984751.2984777},
abstract = {Most of the work on context-aware systems has focused on the context of time, location, and activity. Previous studies on the context flow have been primarily conducted on a qualitative basis. This paper proposes a new approach from a quantitative perspective. We gathered the data from automated task service, "If This Then That (IFTTT)", and analyzed the sequential tasks in terms of event occurrence in smart devices through association rule mining. We found out three consecutive tasks in cross-applications. The results of analysis have potential to find hidden use patterns as telling what kinds of services and channels are associated with each other. The findings provide some insights on the development of design guidelines for context-aware services.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {217–218},
numpages = {2},
keywords = {sequential task, association rule mining, task automation, context-aware services},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984778,
author = {van der Meulen, Hidde and Varsanyi, Petra and Westendorf, Lauren and Kun, Andrew L. and Shaer, Orit},
title = {Towards Understanding Collaboration around Interactive Surfaces: Exploring Joint Visual Attention},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984778},
doi = {10.1145/2984751.2984778},
abstract = {In this abstract, we present a novel method for exploring the visual behavior of multiple users engaged in a collaborative task around an interactive surface. The proposed method synchronizes input from multiple eye trackers, describes the visual behavior of individual users over time, and identifies joint attention across multiple users. We applied this method to analyze the visual behavior of four users collaborating using a large-scale multi-touch tabletop.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {219–220},
numpages = {2},
keywords = {collaboration, visual attention, eye tracking},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984779,
author = {Sra, Misha and Jain, Dhruv and Caetano, Arthur Pitzer and Calvo, Andres and Hilton, Erwin and Schmandt, Chris},
title = {Resolving Spatial Variation And Allowing Spectator Participation In Multiplayer VR},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984779},
doi = {10.1145/2984751.2984779},
abstract = {Multiplayer virtual reality (VR) games introduce the problem of variations in the physical size and shape of each user's space for mapping into a shared virtual space. We propose an asymmetric approach to solve the spatial variation problem, by allowing people to choose roles based on the size of their space. We demonstrate this concept through the implementation of a virtual snowball fight where players can choose from multiple roles, namely the shooter, the target, or an onlooker depending on whether the game is played remotely or together in one large space. In the co-located version, the target stands behind an actuated cardboard fort that responds to events in VR, providing non-VR spectators a way to participate in the experience. During preliminary deployment, users showed extremely positive reactions and the spectators were thrilled.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {221–222},
numpages = {2},
keywords = {haptics, actuated., mobile, virtual reality, asymmetrical vr, input devices, multiplayer},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

@inproceedings{10.1145/2984751.2984780,
author = {Lim, Hyunchul and Chung, Jungmin and Oh, Changhoon and Park, SoHyun and Suh, Bongwon},
title = {OctaRing: Examining Pressure-Sensitive Multi-Touch Input on a Finger Ring Device},
year = {2016},
isbn = {9781450345316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984751.2984780},
doi = {10.1145/2984751.2984780},
abstract = {In this paper, we introduce OctaRing, an octagon-shaped finger ring device that facilitates pressure-sensitive multi- touch gestures. To explore the feasibility of its prototype, we conducted an experiment and investigated users' sensorimotor skills in exerting different levels of pressure on the ring with more than one finger. The results of the experiment indicate that users are comfortable with the two-finger touch configuration with two levels of pressure. Based on this result, future work will explore novel gestures involving a finger ring device.},
booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
pages = {223–224},
numpages = {2},
keywords = {ring, pressure-sensitive, wearable, multi-touch},
location = {Tokyo, Japan},
series = {UIST '16 Adjunct}
}

