@inproceedings{10.1145/2508468.2514926,
author = {Voelker, Simon and Nakajima, Kosuke and Thoresen, Christian and Itoh, Yuichi and \O{}verg\r{a}rd, Kjell Ivar and Borchers, Jan},
title = {PUCs: Detecting Transparent, Passive Untouched Capacitive Widgets on Unmodified Multi-Touch Displays},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514926},
doi = {10.1145/2508468.2514926},
abstract = {Capacitive multi-touch displays are not typically designed to detect passive objects placed on them. In fact, these systems usually contain filters to actively reject such input data. We present a technical analysis of this problem and introduce Passive Untouched Capacitive Widgets (PUCs). Unlike previous approaches, PUCs do not require power, they can be made entirely transparent, and they do not require internal electrical or software modifications. Most importantly they are detected reliably even when no user is touching them.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–2},
numpages = {2},
keywords = {capacitive multi-touch, transparent widgets, tangible user interfaces, passive widgets, tabletop interaction},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514927,
author = {Yamanaka, Shota and Miyashita, Homei},
title = {The Nudging Technique: Input Method without Fine-Grained Pointing by Pushing a Segment},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514927},
doi = {10.1145/2508468.2514927},
abstract = {The Nudging Technique is a new manipulation paradigm for GUIs. With traditional techniques, the user sometimes has to perform a fine-grained operation (e.g., pointing at the edge of a window to resize). When the user makes a mistake in the pointing, problems may arise such as an accidental switching of the foreground window. The nudging technique relieves the user from the fine pointing before dragging; the user just moves the cursor to a target then pushes it. Visual and acoustic feedbacks also help the user's operation. We describe two application examples: window resizing and spreadsheet cell resizing systems.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–4},
numpages = {2},
keywords = {graphical user interfaces (guis), nudging technique, drag-and-drop, mouse cursor operation., pointing technique},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514928,
author = {Zhao, Yuhang and Qin, Yongqiang and Liu, Yang and Liu, Siqi and Shi, Yuanchun},
title = {QOOK: A New Physical-Virtual Coupling Experience for Active Reading},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514928},
doi = {10.1145/2508468.2514928},
abstract = {We present QOOK, an interactive reading system that incorporates the benefits of both physical and digital books to facilitate active reading. QOOK uses a top-projector to create digital contents on a blank paper book. By detecting markers attached to each page, QOOK allows users to flip pages just like they would with a real book. Electronic functions such as keyword searching, highlighting and bookmarking are included to provide users with additional digital assistance. With a Kinect sensor that recognizes touch gestures, QOOK enables people to use these electronic functions directly with their fingers. The combination of the electronic functions of the virtual interface and free-form interaction with the physical book creates a natural reading experience, providing an opportunity for faster navigation between pages and better understanding of the book contents.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {5–6},
numpages = {2},
keywords = {tangible user interface, physical-virtual, active reading},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514929,
author = {Mullenbach, Joe and Shultz, Craig and Piper, Anne Marie and Peshkin, Michael and Colgate, J. Edward},
title = {Surface Haptic Interactions with a TPad Tablet},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514929},
doi = {10.1145/2508468.2514929},
abstract = {A TPad Tablet is a tablet computer with a variable friction touchscreen. It can create the perception of force, shape, and texture on a fingertip, enabling unique and novel haptic interactions on a flat touchscreen surface. We have created an affordable and easy to use variable friction device and have made it available through the open-hardware TPad Tablet Project. We present this device as a potential research platform as well as demonstrate two applications: remote touch communication and rapid haptic sketching.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {7–8},
numpages = {2},
keywords = {surface haptics, tablet, variable friction, touchscreen},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514930,
author = {Scott, Jeremy and Davis, Randall},
title = {Physink: Sketching Physical Behavior},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514930},
doi = {10.1145/2508468.2514930},
abstract = {Describing device behavior is a common task that is currently not well supported by general animation or CAD software. We present PhysInk, a system that enables users to demonstrate 2D behavior by sketching and directly manipulating objects on a physics-enabled stage. Unlike previous tools that simply capture the user's animation, PhysInk captures an understanding of the behavior in a timeline. This enables useful capabilities such as causality-aware editing and finding physically-correct equivalent behavior. We envision PhysInk being used as a physics teacher's sketchpad or a WYSIWYG tool for game designers.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {9–10},
numpages = {2},
keywords = {sketch understanding, direct manipulation, natural user interfaces},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514931,
author = {Kovacs, Geza and Miller, Robert C.},
title = {Foreign Manga Reader: Learn Grammar and Pronunciation While Reading Comics},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514931},
doi = {10.1145/2508468.2514931},
abstract = {Foreign-language comics are potentially an enjoyable way to learn foreign languages. However, the difficulty of reading authentic material makes them inaccessible to novice learners. We present the Foreign Manga Reader, a system that helps readers comprehend foreign-language written materials and learn multiple aspects of the language. Specifically, it generates a sentence-structure visualization to help learners understand the grammar, pronounces dialogs to improve listening comprehension and pronunciation, and translates dialogs, phrases, and words to teach vocabulary. Learners can use the system to match their experience level, giving novices access to dialog-level translations and pronunciations, and more advanced learners with access to information at the level of phrases and individual words. The annotations are automatically generated, and can be used with arbitrary written materials in several languages. A preliminary study suggests that learners find our system useful for understanding and learning from authentic foreign-language material.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {11–12},
numpages = {2},
keywords = {translation, foreign language learning, multimedia},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514932,
author = {Gong, Nan-Wei and Zoran, Amit and Paradiso, Joseph A.},
title = {Inkjet-Printed Conductive Patterns for Physical Manipulation of Audio Signals},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514932},
doi = {10.1145/2508468.2514932},
abstract = {In this demo paper, we present the realization of a completely aesthetically driven conductive image as a multi-modal music controller. Combining two emerging technologies - rapid prototyping with an off-the-shelf inkjet printer using conductive ink and parametric graphic design, we are able to create an interactive surface that is thin, flat, and flexible. This sensate surface can be conformally wrapped around a simple curved surface, and unlike touch screens, can accommodate complex structures and shapes such as holes on a surface. We present the design and manufacturing flow and discuss the technology behind this multi-modal sensing design. Our work seeks to offer a new dimension of designing sonic interaction with graphic tools, playing and learning music from a visual perspective and performing with expressive physical manipulation.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–14},
numpages = {2},
keywords = {flexible printed electronics, electronic skin, sensate surface, music controller, customizable controller surface},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514933,
author = {Manabe, Hiroyuki},
title = {Multi-Touch Gesture Recognition by Single Photoreflector},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514933},
doi = {10.1145/2508468.2514933},
abstract = {A simple technique is proposed that uses a single photoreflector to recognize multi-touch gestures. Touch and multi-finger swipe are robustly discriminated and recognized. Further, swipe direction can be detected by adding a gradient to the sensitivity.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {15–16},
numpages = {2},
keywords = {photoreflector., multi-touch gesture, swipe},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514934,
author = {Holman, David and Burstyn, Jesse and Brotman, Ryan and Younkin, Audrey and Vertegaal, Roel},
title = {Flexkit: A Rapid Prototyping Platform for Flexible Displays},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514934},
doi = {10.1145/2508468.2514934},
abstract = {Commercially available development platforms for flexible displays are not designed for rapid prototyping. To create a deformable interface, one that uses a functional flexible display, designers must be familiar with embedded hardware systems and corresponding programming. We introduce Flexkit, a platform that allows designers to rapidly prototype deformable applications. With Flexkit, designers can rapidly prototype using a thin-film electrophoretic display, one that is "Plug and Play". To demonstrate Flexkit's ease-of-use, we present its application in PaperTab's design iteration as a case study. We further discuss how dithering can be used to increase the frame rate of electrophoretic displays from 1fps to 5fps.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {17–18},
numpages = {2},
keywords = {electrophoretic display, flexible display, deformable display, organic user interfaces, e ink.},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514936,
author = {Goyal, Pragun and Agrawal, Harshit and Paradiso, Joseph A. and Maes, Pattie},
title = {BoardLab: PCB as an Interface to EDA Software},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514936},
doi = {10.1145/2508468.2514936},
abstract = {The tools used to work with Printed Circuit Boards (PCBs), for example soldering iron, multi-meter and oscilloscope involve working directly with the board and the board components. However, the Electronic Design Automation (EDA) software used to query a PCB's design data requires using a keyboard and a mouse. These different interfaces make it difficult to connect both kinds of operations in a workflow. Further, the measurements made by tools like a multi-meter have to be understood in the context of the schematics of the board manually. We propose a solution to reduce the cognitive load of this disconnect by introducing a handheld probe that allows for direct interactions with the PCB for just-in-time information on board schematics, component datasheets and source code. The probe also doubles up as a voltmeter and annotates the schematics of the board with voltage measurements.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {19–20},
numpages = {2},
keywords = {electronic design automation},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514937,
author = {Sharma, Anirudh and Liu, Lirong and Maes, Pattie},
title = {Glassified: An Augmented Ruler Based on a Transparent Display for Real-Time Interactions with Paper},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514937},
doi = {10.1145/2508468.2514937},
abstract = {We introduce Glassified, a modified ruler with a transparent display to supplement physical strokes made on paper with virtual graphics. Because the display is transparent, both the physical strokes and the virtual graphics are visible in the same plane. A digitizer captures the pen strokes in order to update the graphical overlay, fusing the traditional function of a ruler with the added advantages of a digital, display-based system. We describe use-cases of Glassified in the areas of math and physics and discuss its advantages over traditional systems.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {21–22},
numpages = {2},
keywords = {product design, toy design, transparent displays, augmented paper, augmented reality},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514935,
author = {Liu, Xin and Xia, Haijun and Gu, Jiawei},
title = {FlexStroke: A Jamming Brush Tip Simulating Multiple Painting Tools on Digital Platform},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514935},
doi = {10.1145/2508468.2514935},
abstract = {We propose a new system to enable the real painting experience on digital platform and extend it to multi-strokes for different painting needs. In this paper, we describe how the FlexStroke is used as Chinese brush, oil brush and crayon with changes of its jamming tip. This tip has different levels of stiffness based on its jamming structure. Visual simulations on PixelSense jointly enhance the intuitive painting process with highly realistic display results.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–24},
numpages = {2},
keywords = {jamming structure, painting system, brush stroke, input device},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2516909,
author = {Sodhi, Rajinder and Glisson, Matthew and Poupyrev, Ivan},
title = {AIRREAL: Tactile Interactive Experiences in Free Air},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2516909},
doi = {10.1145/2508468.2516909},
abstract = {AIREAL is a novel haptic technology that delivers effective and expressive tactile sensations in free air, without requiring the user to wear a physical device. Combined with interactive computers graphics, AIREAL enables users to feel virtual 3D objects, experience free air textures and receive haptic feedback on gestures performed in free space. AIREAL relies on air vortex generation directed by an actuated flexible nozzle to provide effective tactile feedback with a 75 degrees field of view, and within an 8.5cm resolution at 1 meter. AIREAL is a scalable, inexpensive and practical free air haptic technology that can be used in a broad range of applications, including gaming, mobile applications, and gesture interaction among many others. This paper reports the details of the AIREAL design and control, experimental evaluations of the device's performance, as well as an exploration of the application space of free air haptic displays. Although we used vortices, we believe that the results reported are generalizable and will inform the design of haptic displays based on alternative principles of free air tactile actuation.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {25–26},
numpages = {2},
keywords = {augmented reality, haptic displays, output devices},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2516910,
author = {Rhee, Taik Heon and Jung, Minkyu and Baek, Sungwook and Kim, Hyun-Jin and Kuk, Sungbin and Kang, Seonghoon and Kim, Hark-Joon},
title = {Ambient Surface: Enhancing Interface Capabilities of Mobile Objects Aided by Ambient Environment},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2516910},
doi = {10.1145/2508468.2516910},
abstract = {We introduce Ambient Surface, an interactive surrounded equipment for enhancing interface capabilities of mobile devices placed on an ordinary surface. Object information and a user's interaction are captured by 2D/3D cameras, and appropriate feedback images are projected on the surface. By the help of the ambient system, we may not only provide a wider screen for mobile devices with a limited screen size, but also allow analog objects to dynamically interact with users. We believe that this demo will help interaction designers to draw new inspiration of utilizing mobile objects with ambient environment.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {27–28},
numpages = {2},
keywords = {camera, analog objects, ambient interaction, projector, mobile objects},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508469,
author = {Dixon, Morgan},
title = {Pixel-Based Reverse Engineering of Graphical Interfaces},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508469},
doi = {10.1145/2508468.2508469},
abstract = {My dissertation proposes a vision in which anybody can modify any interface of any application. Realizing this vision is difficult because of the rigidity and fragmentation of current interfaces. Specifically, rigidity makes it difficult or impossible for a designer to modify or customize existing interfaces. Fragmentation results from the fact that people generally use many different applications built with a variety of toolkits. Each is implemented differently, so it is difficult to consistently add new functionality. As a result, researchers are often limited to demonstrating new ideas in small testbeds, and practitioners often find it difficult to adopt and deploy ideas from the literature. In my dissertation, I propose transcending the rigidity and fragmentation of modern interfaces by building upon their single largest commonality: that they ultimately consist of pixels painted to a display. Building from this universal representation, I propose pixel-based interpretation to enable modification of interfaces without their source code and independent of their underlying toolkit implementation.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {29–32},
numpages = {4},
keywords = {user interface toolkits, pixel-based reverse engineering},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508470,
author = {Liang, Rong-Hao},
title = {Augmenting the Input Space of Portable Displays Using Add-on Hall-Sensor Grid},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508470},
doi = {10.1145/2508468.2508470},
abstract = {Since handheld and wearable displays are highly mobile, various applications are enabled to enrich our daily life. In addition to displaying high-fidelity information, these devices also support natural and effective user interactions by exploiting the capability of various embedded sensors. Nonetheless, the set of built-in sensors has limitations. Add-on sensor technologies, therefore, are needed. This work chooses to exploit magnetism as an additional channel of user input. The author first explains the reasons of developing the add-on magnetic field sensing technology based on neodymium magnets and the analog Hall-sensor grid. Then, the augmented input space is showcased through two instances. 1) For handheld displays, the sensor extends the object tracking capability to the near-surface 3D space by simply attaching it to the back of devices. 2) For wearable displays, the sensor enables private and rich-haptic 2D input by wearing it on user's fingernails. Limitations and possible research directions of this approach are highlighted in the end of paper.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–36},
numpages = {4},
keywords = {magnetism, subtle interaction, hall-sensor grid, portable displays, tangible interaction, add-on sensing},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508471,
author = {Turner, Jayson},
title = {Cross-Device Eye-Based Interaction},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508471},
doi = {10.1145/2508468.2508471},
abstract = {Eye-tracking technology is envisaged to become part of our daily life, as its development progresses it becomes more wearable. Additionally there is a wealth of digital content around us, either close to us, on our personal devices or out-of-reach on public displays. The scope of this work aims to combine gaze with mobile input modalities to enable the transfer of content between public and close proximity personal displays. The work contributes enabling technologies, novel interaction techniques, and poses bigger questions that move toward a formalisation of this design space to develop guidelines for the development of future cross-device eye-based interaction methods.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {37–40},
numpages = {4},
keywords = {pervasive displays, cross-device interaction, multimodal interaction techniques, eye-based interaction},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508472,
author = {Wiese, Jason},
title = {Enabling an Ecosystem of Personal Behavioral Data},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508472},
doi = {10.1145/2508468.2508472},
abstract = {Almost every computational system a person interacts with keeps a detailed log of that person's behavior. The possibility of this data promises a breadth of new service opportunities for improving people's lives through deep personalization, tools to manage aspects of their personal wellbeing, and services that support identity construction. However, the way that this data is collected and managed today introduces several challenges that severely limit the utility of this rich data.This thesis maps out a computational ecosystem for personal behavioral data through the design, implementation, and evaluation of Phenom, a web service that factors out common activities in making inferences from personal behavioral data. The primary benefits of Phenom include: a structured process for aggregating and representing user data; support for developing models based on personal behavioral data; and a unified API for accessing inferences made by models within Phenom. To evaluate Phenom for ease of use and versatility, an external set of developers will create example applications with it.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {41–44},
numpages = {4},
keywords = {personal behavioral data, contextual inferences},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508473,
author = {Mohd Noor, Mohammad Faizuddin},
title = {Exploring Back-of-Device Interaction},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508473},
doi = {10.1145/2508468.2508473},
abstract = {Back of device interaction is gaining popularity as an alternative input modality in mobile devices, however it is still unclear how the back of device is related to other interactions. My research explores the relationship between hand grip from the back of the device and other interactions. In order to investigate this relationship, I will use touch target application to study hand grip patterns, then analyse the correlation that exists between touch target and hand grip. Finally I will explore the possibilities offered when the relationship between the touch target and hand grip is established in a quantifiable way.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {45–48},
numpages = {4},
keywords = {handheld, capacitive, grip, back-of-device, machine-learning, grasp},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508474,
author = {Cola\c{c}o, Andrea},
title = {Sensor Design and Interaction Techniques for Gestural Input to Smart Glasses and Mobile Devices},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508474},
doi = {10.1145/2508468.2508474},
abstract = {Touchscreen interfaces for small display devices have several limitations: the act of touching the screen occludes the display, interface elements like keyboards consume precious display real estate, and even simple tasks like document navigation - which the user performs effortlessly using a mouse and keyboard - require repeated actions like pinch-and-zoom with touch input. More recently, smart glasses with limited or no touch input are starting to emerge commercially. However, the primary input to these systems has been voice.In this paper, we explore the space around the device as a means of touchless gestural input to devices with small or no displays. Capturing gestural input in the surrounding volume requires sensing the human hand. To achieve gestural input we have built Mime [3] -- a compact, low-power 3D sensor for short-range gestural control of small display devices. Our sensor is based on a novel signal processing pipeline and is built using standard off-the-shelf components. Using Mime we demonstrated a variety of application scenarios including 3D spatial input using close-range gestures, gaming, on-the-move interaction, and operation in cluttered environments and in broad daylight conditions. In my thesis, I will continue to extend sensor capabilities to support new interaction styles.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {49–52},
numpages = {4},
keywords = {3d sensing, gesture sensing, wearables., mobile, time-of-flight imaging, hand tracking, head mounted displays},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2508475,
author = {Apaolaza, Aitor},
title = {Identifying Emergent Behaviours from Longitudinal Web Use},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508475},
doi = {10.1145/2508468.2508475},
abstract = {Laboratory studies present difficulties in the understanding of how usage evolves over time. Employed observations are obtrusive and not naturalistic. Our system employs a remote capture tool that provides longitudinal low-level interaction data. It is easily deployable into any Web site allowing deployments in-the-wild and is completely unobtrusive. Web application interfaces are designed assuming users' goals. Requirement specifications contain well defined use cases and scenarios that drive design and subsequent optimisations. Users' interaction patterns outside the expected ones are not considered. This results in an optimisation for a stylised user rather than a real one. A bottom-up analysis from low-level interaction data makes possible the emergence of users' tasks. Similarities among users can be found and solutions that are effective for real users can be designed. Factors such as learnability and how interface changes affect users are difficult to observe in laboratory studies. Our solution makes it possible, adding a longitudinal point of view to traditional laboratory studies. The capture tool is deployed in real world Web applications capturing in-situ data from users. These data serve to explore analysis and visualisation possibilities. We present an example of the exploration results with one Web application.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {53–56},
numpages = {4},
keywords = {usage mining, accessibility, user behaviour analysis, user issues, ergonomics, longitudinal observation, web},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inbook{10.1145/2508468.2508476,
author = {Kato, Jun},
title = {Integrated Visual Representations for Programming with Real-World Input and Output},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2508476},
abstract = {As computers become more pervasive, more programs deal with real-world input and output (real-world I/O) such as processing camera images and controlling robots. The real-world I/O usually contains complex data hardly represented by text or symbols, while most of the current integrated development environments (IDEs) are equipped with text-based editors and debuggers. My thesis investigates how visual representations of the real world can be integrated within the text-based development environment to enhance the programming experience. In particular, we have designed and implemented IDEs for three scenarios, all of which make use of photos and videos representing the real world. Based on these experiences, we discuss "programming with example data," a technique where the programmer demonstrates examples to the IDE and writes text-based code with support of the examples.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {57–60},
numpages = {4}
}

@inproceedings{10.1145/2508468.2514710,
author = {Cheng, Dawei and Li, Danqiong and Fang, Liang},
title = {A Cluster Information Navigate Method by Gaze Tracking},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514710},
doi = {10.1145/2508468.2514710},
abstract = {According to the rapid growth of data volume, it's increasingly complicated to present and navigate large amount of data in a convenient method on mobile devices with a small screen. To address this challenge, we present a new method which displays cluster information in a hierarchy pattern and interact with them by eyes' movement captured by the front camera of mobile devices. The key of this system is providing users a new interacting method to navigate and select data quickly by eyes without any additional equipment.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {61–62},
numpages = {2},
keywords = {information navigation, gaze tracking, cluster analysis},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514711,
author = {Hwang, Sungjae and Kim, Dongchul and Leigh, Sang-won and Wohn, Kwang-yun},
title = {NailSense: Fingertip Force as a New Input Modality},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514711},
doi = {10.1145/2508468.2514711},
abstract = {In this paper, we propose a new interaction technique, called NailSense, which allows users to control a mobile device by hovering and slightly bending/extending fingers behind the device. NailSense provides basic interactions equivalent to that of touchscreen interactions; 2-D locations and binary states (i.e., touch or released) are tracked and used for input, but without any need of touching on the screen. The proposed technique tracks the user's fingertip in real-time and triggers event on color change in the fingernail area. It works with conventional smartphone cameras, which means no additional hardware is needed for its utilization. This novel technique allows users to use mobile devices without occlusion which was a crucial problem in touchscreens, also promising extended interaction space in the air, on desktop, or in everywhere. This new interaction technique is tested with example applications: a drawing app and a web browser.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {63–64},
numpages = {2},
keywords = {around-device interaction(adi), mobile device, computer vision, fingernail, natural user interface (nui)},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514712,
author = {Khademi, Maryam and Fan, Mingming and Mousavi Hondori, Hossein and Lopes, Cristina Videira},
title = {Multi-Perspective Multi-Layer Interaction on Mobile Device},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514712},
doi = {10.1145/2508468.2514712},
abstract = {We propose a novel multi-perspective multi-layer interaction using a mobile device, which provides an immersive experience of 3D navigation through an object. The mobile device serves as a window, through which the user can observe the object in detail from various perspectives by orienting the device differently. Various layers of the object can also be shown while users move the device away and toward themselves. Our approach is real-time, completely mobile (running on Android) and does not depend on external sensor/displays (e.g., camera and projector).},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {65–66},
numpages = {2},
keywords = {multi-perspective, mobile, optical flow, spatially aware displays, multi-layer interaction},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514713,
author = {Wojtczuk, Piotr and Binnie, David and Armitage, Alistair and Chamberlain, Tim and Giebeler, Carsten},
title = {A Touchless Passive Infrared Gesture Sensor},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514713},
doi = {10.1145/2508468.2514713},
abstract = {A sensing device for a touchless, hand gesture, user interface based on an inexpensive passive infrared pyroelectric detector array is presented. The 2 x 2 element sensor responds to changing infrared radiation generated by hand movement over the array. The sensing range is from a few millimetres to tens of centimetres. The low power consumption (&lt; 50 μW) enables the sensor's use in mobile devices and in low energy applications. Detection rates of 77% have been demonstrated using a prototype system that differentiates the four main hand motion trajectories -- up, down, left and right. This device allows greater non-contact control capability without an increase in size, cost or power consumption over existing on/off devices.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {67–68},
numpages = {2},
keywords = {gesture recognition, pyroelectric infrared, dynamic hand gesture, hand motion trajectories, touchless interfaces, infrared motion sensors, passive infrared sensors},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514714,
author = {Kurihara, Tatsuya and Okabe, Makoto and Onai, Rikio},
title = {DDMixer2.5D: Drag and Drop to Mix 2.5D Video Objects},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514714},
doi = {10.1145/2508468.2514714},
abstract = {We propose a 2.5D video editing system called DDMixer2.5D. 2.5D video contains not only color channels but also a depth channel, which can be recorded easily using recently available depth sensors, such as Microsoft Kinect. Our system employs this depth channel to allow a user to quickly and easily edit video objects by using simple drag-and-drop gestures. For example, a user can copy a video object of a dancing figure from video to video simply by dragging and dropping using finger on the touch screen of a mobile phone handset. In addition, the user can drag to adjust the 3D position in the new video so that contact between foot and floor is preserved and the size of the body is automatically adjusted according to the depth. DDMixer2.5D has other useful functions required for practical use, including object removal, editing 3D camera path, creating of anaglyph 3D video, as well as a timeline interface.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {69–70},
numpages = {2},
keywords = {video editing, 2.5d},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514715,
author = {Kobayashi, Kazuki and Yamada, Seiji},
title = {Shape Changing Device for Notification},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514715},
doi = {10.1145/2508468.2514715},
abstract = {In this paper, we describe a notification method with peripheral cognition technology that uses a human cognitive characteristic. The method achieves notification without interrupting users' primary tasks. We developed a shape changing device that change its shape to notify the arrival of information. Such behavior enables a user to easily find and accept notifications without interruption when their attention on the primary task decreases. The result of an experiment showed that the successful notification rate was 45.5%.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {71–72},
numpages = {2},
keywords = {notification., peripheral cognition technologies},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514716,
author = {Lyons, Kent and Nguyen, David H. and Seki, Shigeyuki and White, Sean and Ashbrook, Daniel and Profita, Halley},
title = {BitWear: A Platform for Small, Connected, Interactive Devices},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514716},
doi = {10.1145/2508468.2514716},
abstract = {We describe BitWear, a platform for prototyping small, wireless, interactive devices. BitWear incorporates hardware, wireless connectivity, and a cloud component to enable collections of connected devices. We are using this platform to create, explore, and experiment with a multitude of wearable and deployable physical forms and interactions.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {73–74},
numpages = {2},
keywords = {sensors, iotb, web, interaction, ui toolkit},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514717,
author = {Hong, Yujie and Shi, Lei and Ying, Fangtian},
title = {Hanzi Lamp: An Intelligent Guide Interface for Chinese Character Learning},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514717},
doi = {10.1145/2508468.2514717},
abstract = {In recent years, an increasing number of people want to understand Chinese culture and Hanzi (Chinese characters) is a key to that. Learning Chinese characters as a second language can be quite challenging. What confuses learners is not only the meaning of Hanzi, but also the complicated writing rules since they are very different from the alphabetic ways English uses. Although many mobile applications and online learning systems provide Hanzi teaching interfaces, they are restricted to the two-dimensional screens and thus they offer little flexibility for practicing while learning. In this paper, we propose Hanzi Lamp, an intelligent guide interface which allows users to practice writing under real-time and adaptive projected guidance. Information captured by sensors has been utilized to perceive learners' behaviors and make appropriate response. We explore how we can enhance Chinese characters learning by improving the system's understanding of physical learning environment.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–76},
numpages = {2},
keywords = {chinese character learning, writing rules, instructions, learning environment., guide interface},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514718,
author = {Asai, Hiroki and Yamana, Hayato},
title = {Detecting Student Frustration Based on Handwriting Behavior},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514718},
doi = {10.1145/2508468.2514718},
abstract = {Detecting states of frustration among students engaged in learning activities is critical to the success of teaching assistance tools. We examine the relationship between a student's pen activity and his/her state of frustration while solving handwritten problems. Based on a user study involving mathematics problems, we found that our detection method was able to detect student frustration with a precision of 87% and a recall of 90%. We also identified several particularly discriminative features, including writing stroke number, erased stroke number, pen activity time, and air stroke speed.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {77–78},
numpages = {2},
keywords = {learner tracking, digital ink},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514719,
author = {Leigh, Sang-won},
title = {EyeCan: Affordable and Versatile Gaze Interaction},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514719},
doi = {10.1145/2508468.2514719},
abstract = {We present eyeCan, a software system that promises rich, sophisticated, and still usable gaze interactions with low-cost gaze tracking setups. The creation of this practical system was to drastically lower the hurdle of gaze interaction by presenting easy-to-use gaze gestures, and by reducing the cost-of-entry with the utilization of low precision gaze trackers. Our system effectively compensates for the noise from tracking sensors and involuntary eye movements, boosting both the precision and speed in cursor control. Also the possible variety of gaze gestures was explored and defined. By combining eyelid actions and gaze direction cues, our system provides rich set of gaze events and therefore enables the use of sophisticated applications e.g. playing video games or navigating street view.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {79–80},
numpages = {2},
keywords = {gaze tracking, gaze interaction, accessibility},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514720,
author = {Nicolau, Hugo and Montague, Kyle and Guerreiro, Jo\~{a}o and Marques, Diogo and Guerreiro, Tiago and Stewart, Craig and Hanson, Vicki},
title = {Augmenting Braille Input through Multitouch Feedback},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514720},
doi = {10.1145/2508468.2514720},
abstract = {Current touch interfaces lack the rich tactile feedback that allows blind users to detect and correct errors. This is especially relevant for multitouch interactions, such as Braille input. We propose HoliBraille, a system that combines touch input and multi-point vibrotactile output on mobile devices. We believe this technology can offer several benefits to blind users; namely, convey feedback for complex multitouch gestures, improve input performance, and support inconspicuous interactions. In this paper, we present the design of our unique prototype, which allows users to receive multitouch localized vibrotactile feedback. Preliminary results on perceptual discrimination show an average of 100% and 82% accuracy for single-point and chord discrimination, respectively. Finally, we discuss a text-entry application with rich tactile feedback.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {81–82},
numpages = {2},
keywords = {feedback, multitouch, vibrotactile, braille, input, output},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514721,
author = {Danieau, Fabien and Bernon, J\'{e}r\'{e}mie and Fleureau, Julien and Guillotel, Philippe and Mollet, Nicolas and Christie, Marc and L\'{e}cuyer, Anatole},
title = {H-Studio: An Authoring Tool for Adding Haptic and Motion Effects to Audiovisual Content},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514721},
doi = {10.1145/2508468.2514721},
abstract = {Haptic and motion effects have been widely used for virtual reality applications in order to provide a physical feedback from the virtual world. Such feedback was recently studied to improve the user experience in audiovisual entertainment applications. But the creation of haptic and motion effects is a main issue and requires dedicated editing tool. This paper describes a user-friendly authoring tool to create and synchronize such effects with audiovisual content. More precisely we focus on the edition of motion effects. Authoring is simplified thanks to a dedicated graphical user interface, allowing either to import external data or to synthesize effects thanks to a force-feedback device. Another key feature of this editor is the playback function which enables to preview the motion effect. Hence this new tool allows non expert users to create immersive haptic-audiovisual experiences.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {83–84},
numpages = {2},
keywords = {sensation of motion, audiovisual content, authoring tool, haptic},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514722,
author = {Gadde, Prathik and Bolchini, Davide},
title = {WebNexter: Dynamic Guided Tours for Screen Readers},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514722},
doi = {10.1145/2508468.2514722},
abstract = {Recent research has shown that screen-reader users can find information on a website almost twice as fast if they bypass indexes and just navigate the content pages of a collection linearly (in a guided-tour fashion). Yet manually building a guided tour for each existing index requires significant resources from web developers, especially for very large web applications. To address this problem, we introduce WebNexter, a web browser extension that automatically generates guided tours from the indexes present in the page a screen-reader user is currently visiting. WebNexter is manifest in a Google Chrome extension that implements screen-reader accessible, dynamic construction of guided tours from a very large, eCommerce website prototype. Our goal is to develop WebNexter extensions for multiple browsers that will work on any website; this will relieve developers from the burden of designing guided tours while greatly accelerating the screen-reader navigation experience during fact-finding.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {85–86},
numpages = {2},
keywords = {screen-reader users, web navigation, guided-tours},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514723,
author = {Ooide, Yoshiharu and Kawaguchi, Hiroki and Nojima, Takuya},
title = {An Assembly of Soft Actuators for an Organic User Interface},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514723},
doi = {10.1145/2508468.2514723},
abstract = {An organic user interface (OUI) is a kind of interface that is based on natural human-human and human-physical object interaction models. In such situations, hair and fur play important roles in establishing smooth and natural communication. Animals and birds use their hair, fur and feathers to express their emotions, and groom each other when forming closer relationships. Therefore, hair and fur are potential materials for development of the ideal OUI. In this research, we propose the hairlytop interface, which is a collection of hair-like units composed of shape memory alloys, for use as an OUI. The proposed interface is capable of improving its spatial resolution and can be used to develop a hair surface on any electrical device shape.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {87–88},
numpages = {2},
keywords = {emotion and affective user interface, robots, interaction design, entertainment},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514724,
author = {Andolina, Salvatore and Lee, Daniel and Dow, Steven},
title = {Crowdboard: An Augmented Whiteboard to Support Large-Scale Co-Design},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514724},
doi = {10.1145/2508468.2514724},
abstract = {Co-design efforts attempt to account for many diverse viewpoints. However, design teams lack support for meaningful real-time interaction with a large community of potential stakeholders. We present Crowdboard, a novel whiteboard system that enables many potential stakeholders to provide real-time input during early-stage design activities, such as concept mapping. Local design teams develop ideas on a standard whiteboard, which is augmented with annotations and comments from online participants. The system makes it possible for design teams to solicit real-time opinions and ideas from a community of people intrinsically motivated to shape the product/service.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {89–90},
numpages = {2},
keywords = {crowdsourcing, creativity support, real-time collaboration.},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514725,
author = {Heo, Seongkook and Lee, Geehyuk},
title = {Ta-Tap: Consecutive Distant Tap Operations for One-Handed Touch Screen Use},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514725},
doi = {10.1145/2508468.2514725},
abstract = {Tapping on the same point twice is a common operation known as double tap, but tapping on distant points in sequence is underutilized. In this poster we explore the potential uses of consecutive distant tap operations, which we call Ta-Tap. As a single-touch operation, it is expected to be particularly useful for single-handed touch screen use. We examined three possible uses of Ta-Tap: simulating multi-touch operations, invoking a virtual scroll wheel, and invoking a pie-menu. We verified the feasibility of Ta-Tap through the experiment.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {91–92},
numpages = {2},
keywords = {one-handed interaction, tap gesature, touch screen},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514726,
author = {Wang, Borui and Chen, Jingshu},
title = {Visimu: A Game for Music Color Label Collection},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514726},
doi = {10.1145/2508468.2514726},
abstract = {Based on previous studies of the associations between color and music, we introduce a scalable way of using colors to label songs and a visualization of music archives that facilitates music exploration. We present Visimu, an online game that attracted users to generate 926 color labels for 102 songs, with over 75% of the songs having color labels reaching high consensus in the Lab color space. We implemented a music archive visualization using the color labels generated by Visimu, and conducted an experiment to show that labeling music by color is more effective than text tags when the user is looking for songs of a particular mood or use scenario. Our results showed that Visimu is effective to produce meaningful color labels for music mood classification, and such approach enables a wide range of applications for music visualization and discovery.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–94},
numpages = {2},
keywords = {game with a purpose, color perception, music visualization, music information retrieval},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514727,
author = {Tsimeris, Jessica and Stevenson, Duncan and Adcock, Matt and Gedeon, Tom and Broughton, Michael},
title = {User Created Tangible Controls Using ForceForm: A Dynamically Deformable Interactive Surface},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514727},
doi = {10.1145/2508468.2514727},
abstract = {Touch surfaces are common devices but they are often uniformly flat and provide little flexibility beyond changing the visual information communicated to the user via software. Furthermore, controls for interaction are not tangible and are usually specified and placed by the user interface designer. Using ForceForm, a dynamically deformable interactive surface, the user is able to directly sculpt the surface to create tangible controls with force feedback properties. These controls can be made according to the user's specifications, and can then be relinquished when no longer needed. We describe this method of interaction, provide an implementation of a slider, and ideas for further controls.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {95–96},
numpages = {2},
keywords = {tactile feedback, interaction techniques, tangible interaction},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514728,
author = {Kihm, Jaeyeon and Guimbreti\`{e}re, Fran\c{c}ois},
title = {Asymmetric Cores for Low Power User Interface Systems},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514728},
doi = {10.1145/2508468.2514728},
abstract = {In recent years, advances in hardware design have lead to significant improvements in the battery life of everyday information appliances. In particular, application processors increasingly include low power "helper" cores dedicated to simpler tasks. Using a custom board design, Guimbreti\`{e}re et al. [2], demonstrated that such helper cores can also be used to execute simple user interface tasks. We revisit their approach by implementing a similar system on an off-the-shelf application processor (TI OMAP4), and demonstrate that, in many cases, the gains reported by Guimbreti\`{e}re et al. [2], can be achieved by simply having the helper core dispatch input events. This new approach can be implemented by merely changing the toolkit infrastructure, thus greatly simplifying deployment},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {97–98},
numpages = {2},
keywords = {low-power user interface system, asymmetric architecture},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514729,
author = {Wang, Borui and Zhang, Ningxia and Hu, Jianfeng and Shen, Zheng},
title = {Visualizing Web Browsing History with Barcode Chart},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514729},
doi = {10.1145/2508468.2514729},
abstract = {Inspired by the DNA art, we introduce a data visualization technique called barcode chart, which uses color-illuminated stripes that resemble barcodes to visualize temporal data. Barcode chart excels at demonstrating high-level patterns in highly segmented temporal data, while retaining details in the data through interaction. We demonstrate Yogurt, a browser extension that implements barcode chart to visualize online browsing history. We conducted a user study and analyzed the effectiveness of using barcode chart for Yogurt in comparison with other applications. We conclude that barcode chart satisfies the need of visualizing the high-density and high-fragmentation nature of temporal data in Yogurt, and helps reveal online distraction and other web browsing patterns.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–100},
numpages = {2},
keywords = {barcode chart, visualization, web history visualization, temporal data visualization},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514730,
author = {Stewart, Craig and Traitor, Penny and Hanson, Vicki L.},
title = {Wheels in Motion: Inertia Sensing in Roller Derby},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514730},
doi = {10.1145/2508468.2514730},
abstract = {The recent resurgence of Roller Derby has seen the game progress to an elite level with leagues becoming increasingly competitive and taking a more structured and athletic approach to training. Leagues that the authors are involved in have expressed a desire for an objective measure of basic skills and a way to monitor improvements in performance especially amongst junior skaters. This paper details the construction of an inertia-sensing platform designed to be safe to wear by skaters. We have identified a skating manoeuvre, the "crossover" that can be automatically detected using a simple filtering and thresholding procedure. We also report on some initial results in automatically detecting when a crossover occurs and provide details of our future work.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {101–102},
numpages = {2},
keywords = {feedback, inertia sensors, roller derby},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514731,
author = {Pardomuan, Jefferson and Sato, Toshiki and Koike, Hideki},
title = {LivingClay: Particle Actuation to Control Display Volume and Stiffness},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514731},
doi = {10.1145/2508468.2514731},
abstract = {We present a new type of display actuation that is able to control both display geometry and stiffness properties using a filler material and air flow control technique. The display consists of a flat, flexible layer of cells on the surface and chamber filled with particles under it. Display geometries can be changed by transporting an amount of particles between display cells and the particle chamber using pressured air and vacuum to control the air flows. This system also allow for variable stiffness using vacuum technique to harden the particles inside chamber. In this paper, we present the design and control technique of this new type actuator and also possible interaction on a single actuator display. We also propose a low-cost, effective way to control an array of actuators where the air flow line and particle line are arranged in a multiplexed grid configuration.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {103–104},
numpages = {2},
keywords = {tabletop, haptic display, interactive surface},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514732,
author = {Elder, Andrew Nicholas and Zhou, Elaine},
title = {Brainstorm, Define, Prototype: Timing Constraints to Balance Appropriate and Novel Design},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514732},
doi = {10.1145/2508468.2514732},
abstract = {We present the results of a human creativity experiment that examined the effect of varying the timing of narrowed constraints. Participants were asked to create a static web ad for Stanford University guided under a timed design process and were introduced to a narrowed constraint either at the beginning, middle, or end of the prototyping process. The narrow constraint addressed goal and task constraints by specifying the target audience and ad size. We find that groups introduced to narrow constraints prior to the brainstorm yielded more appropriate results, while those introduced prior to the final production yielded more novel results. Our results suggest that effective timing of design constraints may further optimize ideation and design methodologies.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {105–106},
numpages = {2},
keywords = {creative problem solving, design process, constraints},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514733,
author = {Son, Jeongmin and Lee, Geehyuk},
title = {FingerSkate: Making Multi-Touch Operations Less Constrained and More Continuous},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514733},
doi = {10.1145/2508468.2514733},
abstract = {Multi-touch operations are sometimes difficult to perform due to musculoskeletal constraints. We propose FingerSkate, a variation to the current multi-touch operations to make them less constrained and more continuous. With FingerSkate, once one starts a multi-touch operation, one can continue the operation without having to maintain both fingers on the screen. In a pilot study, we observe that participants could learn to FingerSkate easily and were utilizing the new technique actively.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {107–108},
numpages = {2},
keywords = {musculoskeletal constraints, touchscreen interface, fingerskate, multi-touch operations},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514734,
author = {Dand, Dhairya and Hemsley, Robert},
title = {Obake: Interactions on a 2.5D Elastic Display},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514734},
doi = {10.1145/2508468.2514734},
abstract = {In this poster we present an interaction language for the manipulation of an elastic deformable 2.5D display. We discuss a range of gestures to interact and directly deform the surface. To demonstrate these affordances and the associated interactions, we present a scenario of a topographic data viewer using this prototype system.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {109–110},
numpages = {2},
keywords = {interaction techniques for stretchable display, tangible input., shape display},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514735,
author = {Zhang, Cheng and Parnami, Aman and Southern, Caleb and Thomaz, Edison and Reyes, Gabriel and Arriaga, Rosa and Abowd, Gregory D.},
title = {BackTap: Robust Four-Point Tapping on the Back of an off-the-Shelf Smartphone},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514735},
doi = {10.1145/2508468.2514735},
abstract = {We present BackTap, an interaction technique that extends the input modality of a smartphone to add four distinct tap locations on the back case of a smartphone. The BackTap interaction can be used eyes-free with the phone in a user's pocket, purse, or armband while walking, or while holding the phone with two hands so as not to occlude the screen with the fingers. We employ three common built-in sensors on the smartphone (microphone, gyroscope, and accelerometer) and feature a lightweight heuristic implementation. In an evaluation with eleven participants and three usage conditions, users were able to tap four distinct points with 92% to 96% accuracy.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {111–112},
numpages = {2},
keywords = {always-available input, mobile, lightweight interaction, inertial sensors, heuristics, touch, acoustic classification},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

@inproceedings{10.1145/2508468.2514736,
author = {Valkov, Dimitar and Mantler, Andreas and Hinrichs, Klaus},
title = {Haptic Props: Semi-Actuated Tangible Props for Haptic Interaction on the Surface},
year = {2013},
isbn = {9781450324069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508468.2514736},
doi = {10.1145/2508468.2514736},
abstract = {While multiple methods to extend the expressiveness of tangible interaction have been proposed, e. g., self-motion, stacking and transparency, providing haptic feedback to the tangible prop itself has rarely been considered. In this poster we present a semi-actuated, nano-powered, tangible prop, which is able to provide programmable friction for interaction with a tabletop setup. We have conducted a preliminary user study evaluating the users' acceptance for the device and their ability to detect changes in the programmed level of friction and received some promising results.},
booktitle = {Proceedings of the Adjunct Publication of the 26th Annual ACM Symposium on User Interface Software and Technology},
pages = {113–114},
numpages = {2},
keywords = {nano-powered devices, tangible interaction, tabletop},
location = {St. Andrews, Scotland, United Kingdom},
series = {UIST '13 Adjunct}
}

