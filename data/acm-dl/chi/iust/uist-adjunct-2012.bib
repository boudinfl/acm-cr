@inproceedings{10.1145/2380296.2380298,
author = {Nakagaki, Ken and Kakehi, Yasuaki},
title = {Needle User Interface: A Sewing Interface Using Layered Conductive Fabrics},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380298},
doi = {10.1145/2380296.2380298},
abstract = {Embroidery is a creative manual activity practiced by many people for a living. Such a craft demands skill and knowledge, and as it is sometimes complicated and delicate, it can be difficult for beginners to learn. We propose a system, named the Needle User Interface, which enables sewers to record and share their needlework, and receive feedback. In particular, this system can detect the position and orientation of a needle being inserted into and removed from a textile. Moreover, this system can give visual, auditory, and haptic feedback to users in real time for directing their ac-tions appropriately. In this paper, we describe the system design, the input system, and the feedback delivery mechanism.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {1–2},
numpages = {2},
keywords = {input system., sewing interface, embroidery supporting system},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380299,
author = {Zoran, Amit and Paradiso, Joseph},
title = {The FreeD: A Handheld Digital Milling Device for Craft and Fabrication},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380299},
doi = {10.1145/2380296.2380299},
abstract = {We present an approach to combine digital fabrication and craft that is focused on a new fabrication experience. The FreeD is a hand-held, digitally controlled, milling device. It is guided and monitored by a computer while still preserving gestural freedom. The computer intervenes only when the milling bit approaches the 3D model, which was designed beforehand, either by slowing down the spindle's speed or by drawing back the shaft. The rest of the time it allows complete freedom, allowing the user to manipulate and shape the work in any creative way. We believe The FreeD will enable a designer to move in between the straight boundaries of established CAD systems and the free expression of handcraft.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–4},
numpages = {2},
keywords = {milling, computer-aided design (cad)},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380300,
author = {Kaye, Joseph 'Jofish'},
title = {Sawtooth Planar Waves for Haptic Feedback},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380300},
doi = {10.1145/2380296.2380300},
abstract = {Current touchscreen technology does not provide adequate haptic feedback to the user. Mostly haptic feedback solutions for touchscreens involve either a) deforming the surface layers screen itself or b) placing actuators under the screen to vibrate it. This means that we have only limited control over where on the screen the feedback feels like it is coming from, and that we are limited to feedback that feels like movement up and down, orthogonal to the screen. In this work I demonstrate a novel technique for haptic feedback: sawtooth planar waves. In a series of paper Canny &amp; Reznick showed that sawtooth planar waves could be used for object manipulation. Here that technique is applied to haptic feedback. By varying the input waves, from 1 one to 4 actuators, it is possible to provide feelings of motion in any planar direction to a finger at one point on the screen while providing a different sensation, or none at all, to fingers placed at several other points on the screen.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {5–6},
numpages = {2},
keywords = {haptics, sawtooth planar waves},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380301,
author = {Manabe, Hiroyuki and Fukumoto, Masaaki},
title = {Touch Sensing by Partial Shadowing of PV Module},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380301},
doi = {10.1145/2380296.2380301},
abstract = {A novel touch sensing technique is proposed. By utilizing partial shadowing of a photovoltaic (PV) module, touch events are accurately detected. Since the PV module also works as a power source, a battery-less touch sensing device is easily realized. We develop a wireless touch commander consisting of 6 PV modules so the user can input by using both touch and swipe actions.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {7–8},
numpages = {2},
keywords = {touch, photovoltaic, partial shadowing},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380302,
author = {Laviole, Jeremy and Hachet, Martin},
title = {Spatial Augmented Reality for Physical Drawing},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380302},
doi = {10.1145/2380296.2380302},
abstract = {Spatial augmented reality (SAR) makes possible the projection of virtual environments into the real world. In this demo, we propose to demonstrate our SAR tools dedicated to the creation of physical drawings. From the most simple tools: the projection on virtual guidelines enabling to trace lines and curves to more advanced techniques enabling stereoscopic drawing through the projection of a 3D scene. This demo presents how we can use computer graphics tools to ease the drawing, and how it will enable new kinds of physical drawings.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {9–10},
numpages = {2},
keywords = {user interfaces, spatial augmented reality, interactive projection, arts},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380303,
author = {Mi, Haipeng and Ishii, Kentaro and Ma, Lei and Laokulrat, Natsuda and Inami, Masahiko and Igarashi, Takeo},
title = {Pebbles: An Interactive Configuration Tool for Indoor Robot Navigation},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380303},
doi = {10.1145/2380296.2380303},
abstract = {This study presents an interactive configuration tool that assists non-expert users to design specific navigation route for mobile robot in an indoor environment. The user places small active markers, called pebbles, on the floor along the desired route in order to guide the robot to the destination. The active markers establish a navigation network by communicating each other with IR beacon and the robot follows the markers to reach the designated goal. During the installation, a user can get effective feedback from LED indicators and voice prompts, so that the user can immediately understand if the navigation route is appropriately configured as expected. With this tool a novice user may easily customize a mobile robot for various indoor tasks.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {11–12},
numpages = {2},
keywords = {indoor navigation, tangible user interface, non-expert user, interactive configuration},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380304,
author = {Takeuchi, Yuta and Katakura, Hirotaka and Kamuro, Sho and Minamizawa, Kouta and Tachi, Susumu},
title = {TouchCast: An on-Line Platform for Creation and Sharing of Tactile Content Based on Tactile Copy &amp; Paste},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380304},
doi = {10.1145/2380296.2380304},
abstract = {We propose TouchCast, which is an on-line platform for the creating and sharing of tactile content based on Tactile Copy &amp; Paste. User-Generated Tactile Content refers to tactile content that is created, shared and appreciated by general Internet users. TouchCast enables users to create tactile content by applying tactile textures to existing on- line content (e.g., illustrations) and to share the created content over the network. Applied textures are scanned from real objects as audio signals and we call this technique Tactile Copy &amp; Paste. In this study, we implement the system as a web browser add-on and to create User Generated Tactile Content.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–14},
numpages = {2},
keywords = {tactile transmission, user-generated content, tactile interaction, web-based interaction},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380305,
author = {Cheng, Lung-Pan and Hsiao, Fang-I and Liu, Yen-Ting and Chen, Mike Y.},
title = {IRotate Grasp: Automatic Screen Rotation Based on Grasp of Mobile Devices},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380305},
doi = {10.1145/2380296.2380305},
abstract = {Automatic screen rotation improves viewing experience and usability of mobile devices, but current gravity-based approaches do not support postures such as lying on one side, and manual rotation switches require explicit user input. iRotate Grasp automatically rotates screens of mobile devices to match users' viewing orientations based on how users are grasping the devices. Our insight is that users' grasps are consistent for each orientation, but significantly differ between different orientations. Our prototype embeds a total of 32 light sensors along the four sides and the back of an iPod Touch, and uses support vector machine (SVM) to recognize grasps at 25Hz. We collected 6-users' usage under 54 different conditions: 1) grasping the device using left, right, and both hands, 2) scrolling, zooming and typing, 3) in portrait, landscape-left, and landscape-right orientations, and while 4) sitting and lying down on one side. Results show that our grasp-based approach is promising, and our iRotate Grasp prototype could correctly rotate the screen 90.5% of the time when training and testing on different users.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {15–16},
numpages = {2},
keywords = {auto rotation, grasp recognition, device orientation},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380306,
author = {Arisandi, Ryan and Takami, Yusuke and Otsuki, Mai and Kimura, Asako and Shibata, Fumihisa and Tamura, Hideyuki},
title = {Enjoying Virtual Handcrafting with ToolDevice},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380306},
doi = {10.1145/2380296.2380306},
abstract = {ToolDevice is a set of devices developed to help users in spatial work such as layout design and three-dimensional (3D) modeling. It consists of three components: TweezersDevice, Knife/HammerDevice, and BrushDevice, which use hand tool metaphors to help users recognize each device's unique functions. We have developed a mixed reality (MR) 3D modeling system that imitates real-life woodworking using the TweezersDevice and the Knife/HammerDevice. In the system, users can pick up and move virtual objects with the TweezersDevice. Users can also cut and join virtual objects using the Knife/HammerDevice. By repeating these operations, users can build virtual wood models.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {17–18},
numpages = {2},
keywords = {mixed reality, tooldevice, woodworking},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380307,
author = {Takashima, Kazuki and Fujita, Kazuyuki and Itoh, Yuichi and Kitamura, Yoshifumi},
title = {Elastic Scroll for Multi-Focus Interactions},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380307},
doi = {10.1145/2380296.2380307},
abstract = {This paper proposes a novel and efficient multi-focus scroll interface that consists of a two-step operation using a con-tents distortion technique. The displayed content can be handled just like an elastic material that can be shrunk and stretched by a user's fingers. In the first operation, the us-er's dragging temporarily shows the results of the viewport transition of the scroll by elastically distorting the content. This operation allows the user to see both the newly obtained and the original focus on the viewport. Then, three types of simple gestures can be used to perform the second operation such as scrolling, restoring and zooming out to get the demanded focus (or foci).},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {19–20},
numpages = {2},
keywords = {map navigation, content distortion},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380308,
author = {Dai, Xiaowei and Gu, Jiawei and Cao, Xiang and Colgate, J. Edward and Tan, Hong},
title = {SlickFeel: Sliding and Clicking Haptic Feedback on a Touchscreen},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380308},
doi = {10.1145/2380296.2380308},
abstract = {We present SlickFeel, a single haptic display setup that can deliver two distinct types of feedback to a finger on a touchscreen during typical operations of sliding and clicking. Sliding feedback enables the sliding finger to feel interactive objects on a touchscreen through variations in friction. Clicking feedback provides a key-click sensation for confirming a key or button click. Two scenarios have been developed to demonstrate the utility of the two haptic effects. In the first, simple button-click scenario, a user feels the positions of four buttons on a touchscreen by sliding a finger over them and feels a simulated key-click signal by pressing on any of the buttons. In the second scenario, the advantage of haptic feedback is demonstrated in a haptically-enhanced thumb-typing scenario. A user enters text on a touchscreen with two thumbs without having to monitor the thumbs' locations on the screen. By integrating SlickFeel with a Kindle Fire tablet, we show that it can be used with existing mobile touchscreen devices.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {21–22},
numpages = {2},
keywords = {clicking feedback, touchscreen, haptic display, sliding feedback},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380309,
author = {Sra, Misha and Lee, Austin and Pao, Sheng-Ying and Jiang, Gonglue and Ishii, Hiroshii},
title = {Point and Share: From Paper to Whiteboard},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380309},
doi = {10.1145/2380296.2380309},
abstract = {Traditional writing instruments have the potential to enable new forms of interactions and collaboration though digital enhancement. This work specifically enables the user to utilize pen and paper as input mechanisms for content to be displayed on a shared interactive whiteboard. We introduce a pen cap with an infrared led, an actuator and a switch. Pointing the pen cap at the whiteboard allows users to select and position a "canvas" on the whiteboard to display handwritten text while the actuator enables resizing the canvas and the text. It is conceivable that anything one can write on paper anywhere, could be displayed on an interactive whiteboard.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–24},
numpages = {2},
keywords = {collaboration, multiuser, tangible interaction, interactive whiteboard, pen and paper},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380310,
author = {Lasecki, Walter and Wesley, Rachel and Kulkarni, Anand and Bigham, Jeffrey},
title = {Speaking with the Crowd},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380310},
doi = {10.1145/2380296.2380310},
abstract = {Automated systems are not yet able to engage in a robust dialogue with users due the complexity and ambiguity of natural language. However, humans can easily converse with one another and maintain a shared history of past interactions. In this paper, we introduce Chorus, a system that enables real-time, two-way natural language conversation between an end user and a crowd acting as a single agent. Chorus is capable of maintaining a consistent, on-topic conversation with end users across multiple sessions, despite constituent individuals perpetually joining and leaving the crowd. This is enabled by using a curated shared dialogue history.Even though crowd members are constantly providing input, we present users with a stream of dialogue that appears to be from a single conversational partner. Experiments demonstrate that dialogue with Chorus displays elements of conversational memory and interaction consistency. Workers were able to answer 84.6% of user queries correctly, demonstrating that crowd-powered communication interfaces can serve as a robust means of interacting with software systems.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {25–26},
numpages = {2},
keywords = {conversational interfaces, crowdsourcing},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380312,
author = {Gupta, Ankit},
title = {Closing the Loop between Intentions and Actions},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380312},
doi = {10.1145/2380296.2380312},
abstract = {In this document, I propose systems that aim to minimize the gap between intentions and the corresponding actions under different scenarios. The gap exists because of many reasons like subjective mapping between the two, lack of resources to implement the action, or inherent noise in the physical processes. The proposed system observes the action and infers the intention behind it. The system then generates a refined action using the inference. The inferred intention and the refined action are then provided as feedback to the user who can then perform corrective actions or choose the refined action as it is as the desired result. I demonstrate the design and implementation of such systems through five projects - Image Deblurring, Tracking Block Model Assembly, Animating with Physical Proxies, What Affects Handwriting and Spying on the Writer.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {27–30},
numpages = {4},
keywords = {tracking, active feedback, augmented reality, virtual reality, task assistance},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380313,
author = {Weir, Daryl},
title = {Machine Learning Models for Uncertain Interaction},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380313},
doi = {10.1145/2380296.2380313},
abstract = {As interaction methods beyond the static mouse and keyboard setup of the desktop era - such as touch, gesture sensing, and visual tracking - become more common, existing interaction paradigms are no longer good enough. These new modalities have high uncertainty, and conventional interfaces are not designed to reflect this. Research has shown that modelling uncertainty can improve the quality of interaction with these systems. Machine learning offers a rich set of tools to make probabilistic inferences in uncertain systems - this is the focus of my thesis work. In particular, I'm interested in making inferences at the sensor level and propagating uncertainty forward appropriately to applications. In this paper I describe a probabilistic model for touch interaction, and discuss how I intend to use the uncertainty in this model to improve typing accuracy on a soft keyboard. The model described here lays the groundwork for a rich framework for interaction in the presence of uncertainty, incorporating data from multiple sensors to make more accurate inferences about the goals of users, and allowing systems to adapt smoothly and appropriately to their context of use.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {31–34},
numpages = {4},
keywords = {touch, machine learning, uncertain interaction, probabilistic modelling, regression, gaussian processes},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380314,
author = {Matulic, Fabrice},
title = {Towards Document Engineering on Pen and Touch-Operated Interactive Tabletops},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380314},
doi = {10.1145/2380296.2380314},
abstract = {Touch interfaces have now become mainstream thanks to modern smartphones and tablets. However, there are still very few "productivity" applications, i.e. tools that support mundane but essential work, especially for large interactive surfaces such as digital tabletops. This work aims to partly fill the relative void in the area of document engineering by investigating what kind of intuitive and efficient tools can be provided to support the manipulation of documents on a digital workdesk, in particular the creation and editing of documents. The fundamental interaction model relies on bimanual pen and multitouch input, which was recently introduced to tabletops and enables richer interaction possibilities. The goal is ultimately to provide useful and highly accessible UIs for document-centric applications, whose design principles will hopefully pave the way from DTP towards DTTP (Digital Tabletop Publishing).},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {35–38},
numpages = {4},
keywords = {tabletops, pen and touch interaction, document engineering},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380315,
author = {Montague, Kyle},
title = {Interactions Speak Louder than Words: Shared User Models and Adaptive Interfaces},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380315},
doi = {10.1145/2380296.2380315},
abstract = {Touch-screens are becoming increasingly ubiquitous. They have great appeal due to their capabilities to support new forms of human interaction, including their abilities to interpret rich gestural inputs, render flexible user interfaces and enable multi-user interactions. However, the technology creates new challenges and barriers for users with limited levels of vision and motor abilities. The PhD work described in this paper proposes a technique combining Shared User Models (SUM) and adaptive interfaces to improve the accessibility of touch-screen devices for people with low levels of vision and motor ability. SUM, built from an individual's interaction data across multiple applications and devices, is used to infer new knowledge of their abilities and characteristics, without the need for continuous calibration exercises or user configurations. This approach has been realized through the development of an open source software framework to support the creation of applications that make use of SUM to adapt interfaces that match the needs of individual users.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–42},
numpages = {4},
keywords = {shared user modelling, mobile touch screens, adaptive interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380316,
author = {Laviole, Jeremy and Hachet, Martin},
title = {Spatial Augmented Reality to Enhance Physical Artistic Creation},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380316},
doi = {10.1145/2380296.2380316},
abstract = {Spatial augmented reality (SAR) promises the integration of digital information in the real (physical) world through projection. In this doctoral symposium paper, I propose different tools to improve speed or ease the drawing by projecting photos, virtual construction lines and interactive 3D scenes. After describing the tools, I explain some future challenges to explore such as the creation of tools which helps to create drawings that are "difficult" to achieve for a human being, but easy to do by a computer. Furthermore, I propose some insights for the creation of digital games and programs which can take full advantages of physical drawings.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–46},
numpages = {4},
keywords = {arts, user interfaces, spatial augmented reality, interactive projection},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380317,
author = {Wu, Leslie},
title = {Medical Operating Documents: Dynamic Checklists Improve Crisis Attention},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380317},
doi = {10.1145/2380296.2380317},
abstract = {The attentional aspects of crisis computing - supporting highly trained teams as they respond to real-life emergencies - have been underexplored in the user interface community. My research investigates the development of interactive software systems that support crisis teams, with an eye towards intelligently managing attention. In this paper, I briefly describe MDOCS, a Medical operating DOCuments System built for time-critical interaction. MDOCS is a multi-user, multi-surface software system that implements dynamic checklists and interactive cognitive aids written to support medical crisis teams. I present the results of a study that evaluates the deployment of MDOCS in a realistic, mannequin-based medical simulator used by anesthesiologists. I propose controlled laboratory experiments that evaluate the feasibility and effectiveness of our design principles and attentional interaction techniques.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {47–50},
numpages = {4},
keywords = {medicine, checklists, cognitive aids},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380318,
author = {Kumar, Ranjitha},
title = {Data-Driven Interactions for Web Design},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380318},
doi = {10.1145/2380296.2380318},
abstract = {This thesis describes how data-driven approaches to Web design problems can enable useful interactions for designers. It presents three machine learning applications which enable new interaction mechanisms for Web design: rapid retargeting between page designs, scalable design search, and generative probabilistic model induction to support design interactions cast as probabilistic inference. It also presents a scalable architecture for efficient data-mining on Web designs, which supports these three applications.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {51–54},
numpages = {4},
keywords = {machine learning, web design},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380319,
author = {Puzis, Yury},
title = {An Interface Agent for Non-Visual, Accessible Web Automation},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380319},
doi = {10.1145/2380296.2380319},
abstract = {The Web is far less usable and accessible for the users with visual impairments than it is for the sighted people. Web automation has the potential to bridge the divide between the ways visually impaired people and sighted people access the Web, and enable visually impaired users to breeze through Web browsing tasks that beforehand were slow, hard, or even impossible to achieve. Typical automation interfaces require that the user record a macro, a useful sequence of browsing steps, so that these steps can be re-played in the future. In this paper, I present a high-level overview of an approach that enables users to find quickly relevant information on the webpage, and automate browsing without recording macros. This approach is potentially useful both for visually impaired, and sighted users.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {55–58},
numpages = {4},
keywords = {interface agent, web accessibility, information filtering, screen reader, audio interface, blind, web browser, macro recorder, visually impaired, macro player, predictive model, non-visual, web automation},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380321,
author = {Savage, Saiph and Forbes, Angus and Savage, Rodrigo and H\"{o}llerer, Tobias and Ch\'{a}vez, Norma Elva},
title = {Directed Social Queries with Transparent User Models},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380321},
doi = {10.1145/2380296.2380321},
abstract = {The friend list of many social network users can be very large. This creates challenges when users seek to direct their social interactions to friends that share a particular interest. We present a self-organizing online tool that by incorporating ideas from user modeling and data visualization allows a person to quickly identify which friends best match a social query, enabling precise and efficient directed social interactions. To cover the different modalities in which our tool might be used, we introduce two different interactive visualizations. One view enables a human-in-the-loop approach for result analysis and verification, and, in a second view, location, social affiliations and "personality" data is incorporated, allowing the user to quickly consider different social and spatial factors when directing social queries. We report on a qualitative analysis, which indicates that transparency leads to an increased effectiveness of the system. This work contributes a novel method for exploring online friends.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {59–60},
numpages = {2},
keywords = {social search, access control lists, q&amp;a, social transparency, social network group creation, interactive verification, friendsourcing, social media, user recommendation},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380322,
author = {Gooch, Daniel and Watts, Leon},
title = {SleepyWhispers: Sharing Goodnights within Distant Relationships},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380322},
doi = {10.1145/2380296.2380322},
abstract = {There is a growing body of work in HCI on the design of communication technologies to help support lovers in long distance relationships. We build upon this work by presenting an exploratory study of a prototype device intended to allow distant lovers to share goodnight messages. Our work distinguishes itself by basing distance communication metaphors on elements of familiar, simple co-located behaviours. We argue that voice remains an under-utilised media when designing interactive technologies for long-distant couples. Through exploring the results of a 2-month case study we present some of the unique challenges that using voice entails.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {61–62},
numpages = {2},
keywords = {intimate communication, communication systems},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380323,
author = {Hamidi, Foad and Baljko, Melanie and Moakler, Alexander and Gadot, Assaf},
title = {Synchrum: A Tangible Interface for Rhythmic Collaboration},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380323},
doi = {10.1145/2380296.2380323},
abstract = {Synchrum is a tangible interface, inspired by the Tibetan prayer wheel, for audience participation and collaboration during digital performance. It engages audience members in effortful interaction, where they have to rotate the device in accord with a given rotation speed. We used synchrum in a video installation and report our observations.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {63–64},
numpages = {2},
keywords = {digital performance, tangible interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380324,
author = {Han, Seungju and Kim, Jung-Bae and Kim, James D.K.},
title = {Follow-Me! Conducting a Virtual Concert},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380324},
doi = {10.1145/2380296.2380324},
abstract = {In this paper, we present a real-time continuous gesture recognition system for conducting a virtual concert. Our systems allow the user control over beat, by conducting four different beat-pattern gestures; tempo, by making faster or slower gestures; volume, by making larger or smaller gestures; and instrument emphasis, by directing the gestures towards specific areas of the orchestra on a large display. A recognition accuracy of up to 95% could be achieved for the conducting gestures (beat, tempo, and volume).},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {65–66},
numpages = {2},
keywords = {conducting, gesture recognition},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380325,
author = {Cabral, Diogo and Correia, Nuno},
title = {Videoink: A Pen-Based Approach for Video Editing},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380325},
doi = {10.1145/2380296.2380325},
abstract = {Due the growth of video sharing, its manipulation is important, however still a hard task. In order to improve it, this work proposes a pen-based approach, called VideoInk. The concept exploits the painting metaphor, replacing digital ink with video frames. The method allows the user to paint video content in a canvas, which works as a two dimensional timeline. This approach includes transition effects and zoom features based on pen pressure. A Tablet PC prototype implementing the concept was also developed.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {67–68},
numpages = {2},
keywords = {video editing, pen-based video interaction, video manipulation, pen-based interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380326,
author = {Wongsuphasawat, Kanit and Gamburg, Alex and Moraveji, Neema},
title = {You Can't Force Calm: Designing and Evaluating Respiratory Regulating Interfaces for Calming Technology},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380326},
doi = {10.1145/2380296.2380326},
abstract = {Interactive systems are increasingly being used to explicitly support change in the user's psychophysiological state and behavior. One trend in this vein is systems that support calm breathing habits. We designed and evaluated techniques to support respiratory regulation to reduce stress and increase parasympathetic tone. Our study revealed that auditory guidance was more effective than visual at creating self-reported calm. We attribute this to the users' ability to effectively map sound to respiration, thereby reducing cognitive load and mental exertion. Interestingly, we found that visual guidance led to more respiratory change but less subjective calm. Thus, motivating users to exert physical or mental efforts may counter the calming effects of slow breathing. Designers of calming technologies must acknowledge the discrepancy between mechanical slow breathing and experiential calm in designing future systems.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {69–70},
numpages = {2},
keywords = {respiratory regulation, stress, biofeedback, calming technology},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380327,
author = {Wang, Danli and Zhang, Yang and Gu, Tianyuan and He, Liang and Wang, Hongan},
title = {E-Block: A Tangible Programming Tool for Children},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380327},
doi = {10.1145/2380296.2380327},
abstract = {E-Block is a tangible programming tool for children aged 5 to 9 which gives children a preliminary understanding of programming. Children can write programs to play a maze game by placing the programming blocks in E-Block. The two stages in a general programming process: programming and running are all embodied in E-Block. We realized E-Block by wireless and infrared technology and gave it feedbacks on both screen and programming blocks. The result of a preliminary user study proved that E-Block is attractive to children and easy to learn and use.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {71–72},
numpages = {2},
keywords = {sensors, tangible user interface, children, tangible programming, programming languages, education, maze},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380328,
author = {Kim, Sunjun and Lee, Geehyuk},
title = {Restorable Backspace},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380328},
doi = {10.1145/2380296.2380328},
abstract = {This paper presents Restorable Backspace, an input helper for mistyping correction. It stores characters deleted by backspace keystrokes, and restores them in the retyping phase. We developed Restoration algorithm that compares deleted characters and retyped characters, and makes a suggestion while retyping. In a pilot study we could observe the algorithm work as expected for most of the cases. All participants in the pilot study showed satisfaction about the concept of Restorable Backspace.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {73–74},
numpages = {2},
keywords = {typing errors, error correction, word suggestion, restorable backspace},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380329,
author = {Miyauchi, Masato and Kimura, Takashi and Nojima, Takuya},
title = {Development of a Non-Contact Tongue-Motion Acquisition System},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380329},
doi = {10.1145/2380296.2380329},
abstract = {We present a new tongue detection system called SITA, which comprises only a Kinect device and conventional laptop computer. In contrast with other tongue-based devices, the SITA system does not require the subject to wear a device. This avoids the issue of oral hygiene and removes the risk of swallowing a device inserted in the mouth. In this paper, we introduce the SITA system and an application. To evaluate the system, a user test was conducted. The results indicate that the system could detect the tongue position in real time. Moreover, there are possibilities of training the tongue with this system.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {75–76},
numpages = {2},
keywords = {tongue-machine interface, optical method, device not worn},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380330,
author = {Solovey, Erin and Jackson, Kim and Cummings, Mary},
title = {Collision Avoidance Interface for Safe Piloting of Unmanned Vehicles Using a Mobile Device},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380330},
doi = {10.1145/2380296.2380330},
abstract = {Autonomous robots and vehicles can perform tasks that are unsafe or undesirable for humans to do themselves, such as investigate safety in nuclear reactors or assess structural damage to a building or bridge after an earthquake. In addition, improvements in autonomous modes of such vehicles are making it easier for minimally-trained individuals to operate the vehicles. As the autonomous capabilities advance, the user's role shifts from a direct teleoperator to a supervisory control role. Since the human operator is often better suited to make decisions in uncertain situations, it is important for the human operator to have awareness of the environment in which the vehicle is operating in order to prevent collisions and damage to the vehicle as well as the structures and people in the vicinity. In this paper, we present the Collision and Obstacle Detection and Alerting (CODA) display, a novel interface to enable safe piloting of a Micro Aerial Vehicle with a mobile device in real-world settings.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {77–78},
numpages = {2},
keywords = {uav, helicopters, mobile, touchscreen},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380331,
author = {Bernays, Ryan and Mone, Jeremy and Yau, Patty and Murcia, Michael and Gonzalez-Sanchez, Javier and Chavez-Echeagaray, Maria Elena and Christopherson, Robert and Atkinson, Robert},
title = {Lost in the Dark: Emotion Adaption},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380331},
doi = {10.1145/2380296.2380331},
abstract = {Having environments that are able to adjust accordingly with the user has been sought in the last years particularly in the area of Human Computer Interfaces. Environments able to recognize the user emotions and react in consequence have been of interest on the area of Affective Computing. This work presents a project -- an adaptable 3D video game, Lost in the Dark: Emotion Adaption, which uses user's emotions as input to alter and adjust the gaming environment. To achieve this, an interface that is capable of reading brain waves, facial expressions, and head motion was used, an Emotiv® EPOC headset. For our purposes we read emotions such as meditation, excitement, and engagement into the game, altering the lighting, music, gates, colors, and other elements that would appeal to the user emotional state. With this, we achieve closing the loop of using the emotions as inputs, adjusting a system accordingly as a result, and elicit emotions.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {79–80},
numpages = {2},
keywords = {eeg, emotion recognition, 3d videogames, affective states},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380332,
author = {Popov, Igor and schraefel, m.c. and Hall, Wendy and Shadbolt, Nigel},
title = {Mashpoint: Browsing the Web along Structured Lines},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380332},
doi = {10.1145/2380296.2380332},
abstract = {Large numbers of Web sites support rich data-centric features to explore and interact with data. In this paper we present mashpoint, a framework that allows distributed data-powered Web applications to linked based on similarities of the entities in their data. By linking applications in this way we allow browsing with selections of data from one application to another application. This sort of browsing allows complex queries and exploration of data to be done by average Web users using multiple applications. We additionally use this concept to surface structured information to users in Web pages. In this paper we present this concept and our initial prototype.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {81–82},
numpages = {2},
keywords = {www, user interfaces, data mashups},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380333,
author = {Huang, Shih-Wen and Tu, Pei-Fen and Amanzadeh, Mohammad and Fu, Wai-Tat},
title = {Review Explorer: An Innovative Interface for Displaying and Collecting Categorized Review Information},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380333},
doi = {10.1145/2380296.2380333},
abstract = {Review Explorer is an interface that utilizes categorized information to help users to explore a huge amount of online reviews more easily. It allows users to sort entities (e.g. restaurants, products) based on their ratings of different aspects (e.g. food for restaurants) and highlight sentences that are related to the selected aspect. Existing interfaces that summarize the aspect information in reviews suffer from the erroneous predictions made by the systems. To solve this problem, Review Explorer performs a real-time aspect sentiment analysis when a reviewer is composing a review and provides an interface for the reviewer to easily correct the errors. This novel design motivates reviewers to provide corrected aspect sentiment labels, which enables our system to provide more accurate information than existing interfaces.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {83–84},
numpages = {2},
keywords = {human computation, sentiment analysis},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380334,
author = {Dahl, Luke and Robaszkiewicz, S\'{e}bastien},
title = {For Novices Playing Music Together, Adding Structural Constraints Leads to Better Music and May Improve User Experience},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380334},
doi = {10.1145/2380296.2380334},
abstract = {We investigate the effects of adding structure to musical interactions for novices. A simple instrument allows control of three musical parameters: pitch, timbre, and note density. Two users can play at once, and their actions are visible on a public display. We asked pairs of users to perform duets under two interaction conditions: unstructured, where users are free to play what they like, and structured, where users are directed to different areas of the musical parameter space by time-varying constraints indicated on the display. A control group played two duets without structure, while an experimental group played one duet with structure and a second without. By crowd-sourcing the ranking of recorded duets we find that structure leads to musically better results. A post experiment survey showed that the experimental group had a better experience during the second unstructured duet than during the structured.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {85–86},
numpages = {2},
keywords = {constraints, collaboration, music, interaction},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380335,
author = {Kitani, Kris and Horita, Kodai and Koike, Hideki},
title = {BallCam! Dynamic View Synthesis from Spinning Cameras},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380335},
doi = {10.1145/2380296.2380335},
abstract = {We are interested in generating novel video sequences from a ball's point of view for sports domains. Despite the challenge of extreme camera motion, we show that we can leverage the periodicity of spinning cameras to generate a stabilized ball point-of-view video. We present preliminary results of image stabilization and view synthesis from a single camera being hurled in the air at 600 RPM.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {87–88},
numpages = {2},
keywords = {ballcam, image stitching, view synthesis},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380336,
author = {Yamakawa, Shumpei and Nojima, Takuya},
title = {A Proposal for a MMG-Based Hand Gesture Recognition Method},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380336},
doi = {10.1145/2380296.2380336},
abstract = {We propose a novel hand-gesture recognition method based on mechanomyograms (MMGs). Skeletal muscles generate sounds specific to their activity. By recording and analyzing these sounds, MMGs provide means to evaluate the activity. Previous research revealed that specific motions produce specific sounds enabling human motion to be classified based on MMGs. In that research, microphones and accelerometers are often used to record muscle sounds. However, environmental conditions such as noise and human motion itself easily overwhelm such sensors. In this paper, we propose to use piezoelectric-based sensing of MMGs to improve robustness from environmental conditions. The preliminary evaluation shows this method is capable of classifying several hand gestures correctly with high accuracy under certain situations.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {89–90},
numpages = {2},
keywords = {gesture recognition, user interface, mechanomyogram},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380337,
author = {Aihara, Noriyuki and Sato, Toshiki and Koike, Hideki},
title = {Highly Deformable Interactive 3D Surface Display},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380337},
doi = {10.1145/2380296.2380337},
abstract = {In this research, we focused on the flexibility limitation of a display material as one of the main causes for height con-straints in deformable surfaces. We propose a method that does not only utilize the material flexibility but also allows for increased variations of shapes and their corresponding interaction possibilities. Using this method, our proposed display design can then support additional expansion via protrusion of an air-pressure-controlled moldable display surface using a residual cloth-excess method and a fixed airbag mount.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {91–92},
numpages = {2},
keywords = {tabletop, interactive surface, shape deformable display},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380338,
author = {Fleer, David and Leichsenring, Christian},
title = {MISO: A Context-Sensitive Multimodal Interface for Smart Objects Based on Hand Gestures and Finger Snaps},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380338},
doi = {10.1145/2380296.2380338},
abstract = {We present an unobtrusive multimodal interface for smart objects (MISO) in an everyday indoor environment. MISO uses pointing for object selection and context-sensitive arm gestures for object control. Finger snaps are used to confirm object selections and to aid with gesture segmentation. Audio feedback is provided during the interaction. The use of a Kinect depth camera allows for a compact system and robustness in varying environments and lighting conditions at low cost.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–94},
numpages = {2},
keywords = {home automation, ambient intelligence, gestures, finger snaps, multimodal interfaces},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380339,
author = {Seko, Keiichi and Fukuchi, Kentaro},
title = {A Guidance Technique for Motion Tracking with a Handheld Camera Using Auditory Feedback},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380339},
doi = {10.1145/2380296.2380339},
abstract = {We introduce a novel guidance technique based on auditory feedback for a handheld video camera. Tracking a moving object with a handheld camera is a difficult task, especially when the camera operator follows the target, because it is difficult to see through the viewfinder at the same time as following the target. The proposed technique provides auditory feedback via a headphone, which assists the operator to keep the target in sight. Two feedback sounds are introduced: three-dimensional (3D) audio and amplitude modulation (AM)-based sonification.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {95–96},
numpages = {2},
keywords = {auditory feedback, object tracking, camera},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380340,
author = {Lee, Joon Hyub and Bae, Seok-Hyung and Jung, Jinyung and Choi, Hayan},
title = {Transparent Display Interaction without Binocular Parallax},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380340},
doi = {10.1145/2380296.2380340},
abstract = {Binocular parallax is a problem for any interaction system that has a transparent display and objects behind it. A proposed quantitative measure called Binocular Selectability Discriminant (BSD) allows UI designers to predict the ability of the user to perform selection task in their transparent display systems, in spite of binocular parallax. A proposed technique called Single-Distance Pseudo Transparency (SDPT) aims to eliminate binocular parallax for on-screen interactions that require precision. A mock-up study shows potentials and directions for future investigation.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {97–98},
numpages = {2},
keywords = {transparent display, binocular parallax},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380341,
author = {Lasecki, Walter and Lau, Tessa and He, Grant and Bigham, Jeffrey},
title = {Crowd-Based Recognition of Web Interaction Patterns},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380341},
doi = {10.1145/2380296.2380341},
abstract = {Web automation often involves users describing complex tasks to a system, with directives generally limited to low-level constituent actions like "click the search button." This level of description is unnatural and makes it difficult to generalize the task across websites. In this paper, we propose a system for automatically recognizing higher-level interaction patterns from user's completion of tasks, such as "searching for cat videos" or "replying to a post". We present PatFinder, a system that identifies these patterns using the input of crowd workers. We validate the system by generating data for 10 tasks, having 62 crowd workers label them, and automatically extracting 14 interaction patterns. Our results show that the number of patterns grows sublinearly with the number of tasks, suggesting that a small finite set of patterns may suffice to describe the vast majority of tasks on the web.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–100},
numpages = {2},
keywords = {crowdsourcing, web automation, interaction patterns},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

@inproceedings{10.1145/2380296.2380342,
author = {Lieber, Thomas and Miller, Rob},
title = {Programming with Everybody: Tightening the Copy-Modify-Publish Feedback Loop},
year = {2012},
isbn = {9781450315821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380296.2380342},
doi = {10.1145/2380296.2380342},
abstract = {People write more code than they ever share online. They also copy and tweak code more often than they contribute their modifications back to the public. These situations can lead to widespread duplication of effort. However, the copy-modify-publish feedback loop which could solve the problem is inhibited by the effort required to publish code online. In this paper we present our preliminary, ongoing effort to create Ditty, a programming environment that attacks the problem by sharing changes immediately, making all code public by default. Ditty tracks the changes users make to code they find and exposes the modified versions alongside the original so that commonly-used derivatives can eventually become canonical. Our work will examine mechanical and social methods to consolidate global effort on common code snippets, and the effects of designing a programming interface that inspires a feeling of the whole world programming together.},
booktitle = {Adjunct Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology},
pages = {101–102},
numpages = {2},
keywords = {awareness, collaboration, open source software development, programming environments, social computing},
location = {Cambridge, Massachusetts, USA},
series = {UIST Adjunct Proceedings '12}
}

