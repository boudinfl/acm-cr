@inproceedings{10.1145/1095034.1095037,
author = {Ko, Andrew J. and Myers, Brad A.},
title = {Citrus: A Language and Toolkit for Simplifying the Creation of Structured Editors for Code and Data},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095037},
doi = {10.1145/1095034.1095037},
abstract = {Direct-manipulation editors for structured data are increasingly common. While such editors can greatly simplify the creation of structured data, there are few tools to simplify the creation of the editors themselves. This paper presents Citrus, a new programming language and user interface toolkit designed for this purpose. Citrus offers language-level support for constraints, restrictions and change notifications on primitive and aggregate data, mechanisms for automatically creating, removing, and reusing views as data changes, a library of widgets, layouts and behaviors for defining interactive views, and two comprehensive interactive editors as an interface to the language and toolkit itself. Together, these features support the creation of editors for a large class of data and code.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–12},
numpages = {10},
keywords = {toolkit, structured editing, interface builder},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095038,
author = {Chapuis, Olivier and Roussel, Nicolas},
title = {Metisse is Not a 3D Desktop!},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095038},
doi = {10.1145/1095034.1095038},
abstract = {Twenty years after the general adoption of overlapping windows and the desktop metaphor, modern window systems differ mainly in minor details such as window decorations or mouse and keyboard bindings. While a number of innovative window management techniques have been proposed, few of them have been evaluated and fewer have made their way into real systems. We believe that one reason for this is that most of the proposed techniques have been designed using a low fidelity approach and were never made properly available. In this paper, we present Metisse, a fully functional window system specifically created to facilitate the design, the implementation and the evaluation of innovative window management techniques. We describe the architecture of the system, some of its implementation details and present several examples that illustrate its potential.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–22},
numpages = {10},
keywords = {window system, window management},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095039,
author = {Berry, Lior and Bartram, Lyn and Booth, Kellogg S.},
title = {Role-Based Control of Shared Application Views},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095039},
doi = {10.1145/1095034.1095039},
abstract = {Collaboration often relies on all group members having a shared view of a single-user application. A common situation is a single active presenter sharing a live view of her workstation screen with a passive audience, using simple hardware-based video signal projection onto a large screen or simple bitmap-based sharing protocols. This offers simplicity and some advantages over more sophisticated software-based replication solutions, but everyone has the exact same view of the application. This conflicts with the presenter's need to keep some information and interaction details private. It also fails to recognize the needs of the passive audience, who may struggle to follow the presentation because of verbosity, display clutter or insufficient familiarity with the application.Views that cater to the different roles of the presenter and the audience can be provided by custom solutions, but these tend to be bound to a particular application. In this paper we describe a general technique and implementation details of a prototype system that allows standardized role-specific views of existing single-user applications and permits additional customization that is application-specific with no change to the application source code. Role-based policies control manipulation and display of shared windows and image buffers produced by the application, providing semi-automated privacy protection and relaxed verbosity to meet both presenter and audience needs.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–32},
numpages = {10},
keywords = {application, view, policy, bitmap, role, sharing, CSCW, verbosity, privacy},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095041,
author = {Vogel, Daniel and Balakrishnan, Ravin},
title = {Distant Freehand Pointing and Clicking on Very Large, High Resolution Displays},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095041},
doi = {10.1145/1095034.1095041},
abstract = {We explore the design space of freehand pointing and clicking interaction with very large high resolution displays from a distance. Three techniques for gestural pointing and two for clicking are developed and evaluated. In addition, we present subtle auditory and visual feedback techniques to compensate for the lack of kinesthetic feedback in freehand interaction, and to promote learning and use of appropriate postures.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–42},
numpages = {10},
keywords = {freehand gestures, whole hand interaction, very large displays, pointing},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095042,
author = {Malik, Shahzad and Ranjan, Abhishek and Balakrishnan, Ravin},
title = {Interacting with Large Displays from a Distance with Vision-Tracked Multi-Finger Gestural Input},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095042},
doi = {10.1145/1095034.1095042},
abstract = {We explore the idea of using vision-based hand tracking over a constrained tabletop surface area to perform multi-finger and whole-hand gestural interactions with large displays from a distance. We develop bimanual techniques to support a variety of asymmetric and symmetric interactions, including fast targeting and navigation to all parts of a large display from the comfort of a desk and chair, as well as techniques that exploit the ability of the vision-based hand tracking system to provide multi-finger identification and full 2D hand segmentation. We also posit a design that allows for handling multiple concurrent users.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–52},
numpages = {10},
keywords = {visual touchpad, from afar, large wall, touch surface, asymmetric, symmetric, multi-point, gesture, two hands, bimanual, interaction},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095043,
author = {Smith, John D. and Vertegaal, Roel and Sohn, Changuk},
title = {ViewPointer: Lightweight Calibration-Free Eye Tracking for Ubiquitous Handsfree Deixis},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095043},
doi = {10.1145/1095034.1095043},
abstract = {We introduce ViewPointer, a wearable eye contact sensor that detects deixis towards ubiquitous computers embedded in real world objects. ViewPointer consists of a small wearable camera no more obtrusive than a common Bluetooth headset. ViewPointer allows any real-world object to be augmented with eye contact sensing capabilities, simply by embedding a small infrared (IR) tag. The headset camera detects when a user is looking at an infrared tag by determining whether the reflection of the tag on the cornea of the user's eye appears sufficiently central to the pupil. ViewPointer not only allows any object to become an eye contact sensing appliance, it also allows identification of users and transmission of data to the user through the object. We present a novel encoding scheme used to uniquely identify ViewPointer tags, as well as a method for transmitting URLs over tags. We present a number of scenarios of application as well as an analysis of design principles. We conclude eye contact sensing input is best utilized to provide context to action.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {53–61},
numpages = {9},
keywords = {eye tracking, attentive user interface},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095045,
author = {Lee, Johnny C. and Hudson, Scott E. and Summet, Jay W. and Dietz, Paul H.},
title = {Moveable Interactive Projected Displays Using Projector Based Tracking},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095045},
doi = {10.1145/1095034.1095045},
abstract = {Video projectors have typically been used to display images on surfaces whose geometric relationship to the projector remains constant, such as walls or pre-calibrated surfaces. In this paper, we present a technique for projecting content onto moveable surfaces that adapts to the motion and location of the surface to simulate an active display. This is accomplished using a projector based location tracking techinque. We use light sensors embedded into the moveable surface and project low-perceptibility Gray-coded patterns to first discover the sensor locations, and then incrementally track them at interactive rates. We describe how to reduce the perceptibility of tracking patterns, achieve interactive tracking rates, use motion modeling to improve tracking performance, and respond to sensor occlusions. A group of tracked sensors can define quadrangles for simulating moveable displays while single sensors can be used as control inputs. By unifying the tracking and display technology into a single mechanism, we can substantially reduce the cost and complexity of implementing applications that combine motion tracking and projected imagery.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {63–72},
numpages = {10},
keywords = {physical interaction, augmented reality, projector based tracking, simulated displays},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095046,
author = {Forlines, Clifton and Balakrishnan, Ravin and Beardsley, Paul and van Baar, Jeroen and Raskar, Ramesh},
title = {Zoom-and-Pick: Facilitating Visual Zooming and Precision Pointing with Interactive Handheld Projectors},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095046},
doi = {10.1145/1095034.1095046},
abstract = {Designing interfaces for interactive handheld projectors is an exiting new area of research that is currently limited by two problems: hand jitter resulting in poor input control, and possible reduction of image resolution due to the needs of image stabilization and warping algorithms. We present the design and evaluation of a new interaction technique, called zoom-and-pick, that addresses both problems by allowing the user to fluidly zoom in on areas of interest and make accurate target selections. Subtle design features of zoom-and-pick enable pixel-accurate pointing, which is not possible in most freehand interaction techniques. Our evaluation results indicate that zoom-and-pick is significantly more accurate than the standard pointing technique described in our previous work.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {73–82},
numpages = {10},
keywords = {zooming, jittery input, selection techniques, interactive handheld projectors},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095047,
author = {Wilson, Andrew D.},
title = {PlayAnywhere: A Compact Interactive Tabletop Projection-Vision System},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095047},
doi = {10.1145/1095034.1095047},
abstract = {We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {83–92},
numpages = {10},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095049,
author = {Kortuem, Gerd and Kray, Christian and Gellersen, Hans},
title = {Sensing and Visualizing Spatial Relations of Mobile Devices},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095049},
doi = {10.1145/1095034.1095049},
abstract = {Location information can be used to enhance interaction with mobile devices. While many location systems require instrumentation of the environment, we present a system that allows devices to measure their spatial relations in a true peer-to-peer fashion. The system is based on custom sensor hardware implemented as USB dongle, and computes spatial relations in real-time. In extension of this system we propose a set of spatialized widgets for incorporation of spatial relations in the user interface. The use of these widgets is illustrated in a number of applications, showing how spatial relations can be employed to support and streamline interaction with mobile devices.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {93–102},
numpages = {10},
keywords = {mobile computing, context-aware computing, spatial relations, spatially-aware interfaces, location systems},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095050,
author = {Dickie, Connor and Vertegaal, Roel and Sohn, Changuk and Cheng, Daniel},
title = {EyeLook: Using Attention to Facilitate Mobile Media Consumption},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095050},
doi = {10.1145/1095034.1095050},
abstract = {One of the problems with mobile media devices is that they may distract users during critical everyday tasks, such as navigating the streets of a busy city. We addressed this issue in the design of eyeLook: a platform for attention sensitive mobile computing. eyeLook appliances use embedded low cost eyeCONTACT sensors (ECS) to detect when the user looks at the display. We discuss two eyeLook applications, seeTV and seeTXT, that facilitate courteous media consumption in mobile contexts by using the ECS to respond to user attention. seeTV is an attentive mobile video player that automatically pauses content when the user is not looking. seeTXT is an attentive speed reading application that flashes words on the display, advancing text only when the user is looking. By making mobile media devices sensitive to actual user attention, eyeLook allows applications to gracefully transition users between consuming media, and managing life.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {103–106},
numpages = {4},
keywords = {eye tracking, context-aware computing, ubiquitous computing, mobile computing, attentive user interfaces},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095051,
author = {Sohn, Byungkon and Lee, Geehyuk},
title = {Circle &amp; Identify: Interactivity-Augmented Object Recognition for Handheld Devices},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095051},
doi = {10.1145/1095034.1095051},
abstract = {The first requirement of a "spatial mouse" is the ability to identify the object that it is aiming at. Among many possible technologies that can be employed for this purpose, possibly the best solution would be object recognition by machine vision. The problem, however, is that object recognition algorithms are not yet reliable enough or light enough for hand-held devices. This paper demonstrates that a simple object recognition algorithm can become a practical solution when augmented by interactivity. The user draw a circle around a target using a spatial mouse, and the mouse captures a series of camera frames. The frames can be easily stitched together to give a target image separated from the background, with which we need only additional steps of feature extraction and object classification. We present here results from two experiments with a few household objects.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {107–110},
numpages = {4},
keywords = {smart environment, spatial mouse, interactivity-augmented object recognition},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095052,
author = {Coelho, Enylton Machado and MacIntyre, Blair and Julier, Simon J.},
title = {Supporting Interaction in Augmented Reality in the Presence of Uncertain Spatial Knowledge},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095052},
doi = {10.1145/1095034.1095052},
abstract = {A significant problem encountered when building Augmented Reality (AR) systems is that all spatial knowledge about the world has uncertainty associated with it. This uncertainty manifests itself as registration errors between the graphics and the physical world, and ambiguity in user interaction. In this paper, we show how estimates of the registration error can be leveraged to support predictable selection in the presence of uncertain 3D knowledge. These ideas are demonstrated in osgAR, an extension to OpenSceneGraph with explicit support for uncertainty in the 3D transformations. The osgAR runtime propagates this uncertainty throughout the scene graph to compute robust estimates of the probable location of all entities in the system from the user's viewpoint, in real-time. We discuss the implementation of selection in osgAR, and the issues that must be addressed when creating interaction techniques in such a system.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {111–114},
numpages = {4},
keywords = {3D user interaction, registration error, augmented reality, uncertainty},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095054,
author = {Han, Jefferson Y.},
title = {Low-Cost Multi-Touch Sensing through Frustrated Total Internal Reflection},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095054},
doi = {10.1145/1095034.1095054},
abstract = {This paper describes a simple, inexpensive, and scalable technique for enabling high-resolution multi-touch sensing on rear-projected interactive surfaces based on frustrated total internal reflection. We review previous applications of this phenomenon to sensing, provide implementation details, discuss results from our initial prototype, and outline future directions.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {115–118},
numpages = {4},
keywords = {touch, frustrated total internal reflection, multi-touch, tactile},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095055,
author = {Forlines, Clifton and Shen, Chia},
title = {DTLens: Multi-User Tabletop Spatial Data Exploration},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095055},
doi = {10.1145/1095034.1095055},
abstract = {Supporting groups of individuals exploring large maps and design diagrams on interactive tabletops is still an open research problem. Today's geospatial, mechanical engineering and CAD design applications are mostly single-user, keyboard and mouse-based desktop applications. In this paper, we present the design of and experience with DTLens, a new zoom-in-context, multi-user, two-handed, multi-lens interaction technique that enables group exploration of spatial data with multiple individual lenses on the same direct-touch interactive tabletop. DTLens provides a set of consistent interactions on lens operations, thus minimizes tool switching by users during spatial data exploration.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {119–122},
numpages = {4},
keywords = {GeoSpatial visualization, multi-user input, Fisheye visualization, tabletop computing},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095057,
author = {Latulipe, Celine and Kaplan, Craig S. and Clarke, Charles L. A.},
title = {Bimanual and Unimanual Image Alignment: An Evaluation of Mouse-Based Techniques},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095057},
doi = {10.1145/1095034.1095057},
abstract = {We present an evaluation of three mouse-based techniques for aligning digital images. We investigate the physical image alignment task and discuss the implications for interacting with virtual images. In a formal evaluation we show that a symmetric bimanual technique outperforms an asymmetric bimanual technique which in turn outperforms a unimanual technique. We show that even after mode switching times are removed, the symmetric technique outperforms the single mouse technique. Subjects also exhibited more parallel interaction using the symmetric technique than when using the asymmetric technique.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {123–131},
numpages = {9},
keywords = {symmetric interaction, image alignment, image registration, two-handed interaction, bimanual input},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095058,
author = {Asano, Takeshi and Sharlin, Ehud and Kitamura, Yoshifumi and Takashima, Kazuki and Kishino, Fumio},
title = {Predictive Interaction Using the Delphian Desktop},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095058},
doi = {10.1145/1095034.1095058},
abstract = {This paper details the design and evaluation of the Delphian Desktop, a mechanism for online spatial prediction of cursor movements in a Windows-Icons-Menus-Pointers (WIMP) environment. Interaction with WIMP-based interfaces often becomes a spatially challenging task when the physical interaction mediators are the common mouse and a high resolution, physically large display screen. These spatial challenges are especially evident in overly crowded Windows desktops. The Delphian Desktop integrates simple yet effective predictive spatial tracking and selection paradigms into ordinary WIMP environments in order to simplify and ease pointing tasks. Predictions are calculated by tracking cursor movements and estimating spatial intentions using a computationally inexpensive online algorithm based on estimating the movement direction and peak velocity. In testing the Delphian Desktop effectively shortened pointing time to faraway icons, and reduced the overall physical distance the mouse (and user hand) had to mechanically traverse.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {133–141},
numpages = {9},
keywords = {graphics user interfaces (GUI), mouse, cursor, Windows icons menus pointer (WIMP), prediction, desktop},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095059,
author = {Ramos, Gonzalo and Balakrishnan, Ravin},
title = {Zliding: Fluid Zooming and Sliding for High Precision Parameter Manipulation},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095059},
doi = {10.1145/1095034.1095059},
abstract = {High precision parameter manipulation tasks typically require adjustment of the scale of manipulation in addition to the parameter itself. This paper introduces the notion of Zoom Sliding, or Zliding, for fluid integrated manipulation of scale (zooming) via pressure input while parameter manipulation within that scale is achieved via x-y cursor movement (sliding). We also present the Zlider (Figure 1), a widget that instantiates the Zliding concept. We experimentally evaluate three different input techniques for use with the Zlider in conjunction with a stylus for x-y cursor positioning, in a high accuracy zoom and select task. Our results marginally favor the stylus with integrated isometric pressure sensing tip over bimanual techniques which separate zooming and sliding controls over the two hands. We discuss the implications of our results and present further designs that make use of Zliding.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {143–152},
numpages = {10},
keywords = {multi-scale navigation, input, pressure widgets, pen-based interfaces},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095061,
author = {Liu, Feng and Gleicher, Michael},
title = {Automatic Image Retargeting with Fisheye-View Warping},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095061},
doi = {10.1145/1095034.1095061},
abstract = {Image retargeting is the problem of adapting images for display on devices different than originally intended. This paper presents a method for adapting large images, such as those taken with a digital camera, for a small display, such as a cellular telephone. The method uses a non-linear fisheye-view warp that emphasizes parts of an image while shrinking others. Like previous methods, fisheye-view warping uses image information, such as low-level salience and high-level object recognition to find important regions of the source image. However, unlike prior approaches, a non-linear image warping function emphasizes the important aspects of the image while retaining the surrounding context. The method has advantages in preserving information content, alerting the viewer to missing information and providing robustness.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {153–162},
numpages = {10},
keywords = {salience map, image warping, fisheye-view warping, image retargeting, focus plus context},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095062,
author = {Bolin, Michael and Webber, Matthew and Rha, Philip and Wilson, Tom and Miller, Robert C.},
title = {Automation and Customization of Rendered Web Pages},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095062},
doi = {10.1145/1095034.1095062},
abstract = {On the desktop, an application can expect to control its user interface down to the last pixel, but on the World Wide Web, a content provider has no control over how the client will view the page, once delivered to the browser. This creates an opportunity for end-users who want to automate and customize their web experiences, but the growing complexity of web pages and standards prevents most users from realizing this opportunity. We describe Chickenfoot, a programming system embedded in the Firefox web browser, which enables end-users to automate, customize, and integrate web applications without examining their source code. One way Chickenfoot addresses this goal is a novel technique for identifying page components by keyword pattern matching. We motivate this technique by studying how users name web page components, and present a heuristic keyword matching algorithm that identifies the desired component from the user's name.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {163–172},
numpages = {10},
keywords = {web automation, web browsers},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095063,
author = {Gajos, Krzysztof and Weld, Daniel S.},
title = {Preference Elicitation for Interface Optimization},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095063},
doi = {10.1145/1095034.1095063},
abstract = {Decision-theoretic optimization is becoming a popular tool in the user interface community, but creating accurate cost (or utility) functions has become a bottleneck --- in most cases the numerous parameters of these functions are chosen manually, which is a tedious and error-prone process. This paper describes ARNAULD, a general interactive tool for eliciting user preferences concerning concrete outcomes and using this feedback to automatically learn a factored cost function. We empirically evaluate our machine learning algorithm and two automatic query generation approaches and report on an informal user study.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {173–182},
numpages = {10},
keywords = {optimization, utility elicitation, active learning},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095065,
author = {Diakopoulos, Nicholas and Essa, Irfan},
title = {Mediating Photo Collage Authoring},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095065},
doi = {10.1145/1095034.1095065},
abstract = {The medium of collage supports the visualization of meaningful event summaries using photographs. It can however be rather tedious to author a collage from a large collection of photographs. In this work we present an approach that supports efficient construction of a collage by assisting the user with an automatic layout procedure that can be controlled at a high level. Our layout method utilizes a pre-designed template which consists of cells for photos and annotations applied to these cells. The layout is then filled by matching the metadata of photos to the annotations in the cells using an optimization algorithm. The user exercises flexibility in the authoring process by (a) maintaining high-level control through the types of constraints applied and (b) leveraging visual emphases supported by the layout algorithm. The user can of course provide fine-grained control of the final collage through direct manipulation. Off-loading the tedium of collage construction to a user controlled yet automated process clears the way for rapidly generating different views of the same album and could also support the increased sharing of digital photos in the form of compact collages.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {183–186},
numpages = {4},
keywords = {painting interface, photo collage},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095066,
author = {Yin, Min and Zhai, Shumin},
title = {Dial and See: Tackling the Voice Menu Navigation Problem with Cross-Device User Experience Integration},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095066},
doi = {10.1145/1095034.1095066},
abstract = {IVR (interactive voice response) menu navigation has long been recognized as a frustrating interaction experience. We propose an IM-based system that sends a coordinated visual IVR menu to the caller's computer screen. The visual menu is updated in real time in response to the caller's actions. With this automatically opened supplementary channel, callers can take advantages of different modalities over different devices and interact with the IVR system with the ease of graphical menu selection. Our approach of utilizing existing network infrastructure to pinpoint the caller's virtual location and coordinating multiple devices and multiple channels based on users' ID registration can also be more generally applied to create integrated user experiences across a group of devices.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {187–190},
numpages = {4},
keywords = {device integration, interacting with a group of devices, voice menu, device aggregation, integrated user experience, telephone, multi-modal interaction, instant messaging},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095067,
author = {Bergman, Lawrence and Castelli, Vittorio and Lau, Tessa and Oblinger, Daniel},
title = {DocWizards: A System for Authoring Follow-Me Documentation Wizards},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095067},
doi = {10.1145/1095034.1095067},
abstract = {Traditional documentation for computer-based procedures is difficult to use: readers have trouble navigating long complex instructions, have trouble mapping from the text to display widgets, and waste time performing repetitive procedures. We propose a new class of improved documentation that we call follow-me documentation wizards. Follow-me documentation wizards step a user through a script representation of a procedure by highlighting portions of the text, as well application UI elements. This paper presents algorithms for automatically capturing follow-me documentation wizards by demonstration, through observing experts performing the procedure. We also present our DocWizards implementation on the Eclipse platform. We evaluate our system with an initial user study that showing that most users have a marked preference for this form of guidance over traditional documentation.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {191–200},
numpages = {10},
keywords = {documentation generation, programming-by-demonstration},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095069,
author = {Dragicevic, Pierre and Chatty, St\'{e}phane and Thevenin, David and Vinot, Jean-Luc},
title = {Artistic Resizing: A Technique for Rich Scale-Sensitive Vector Graphics},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095069},
doi = {10.1145/1095034.1095069},
abstract = {When involved in the visual design of graphical user interfaces, graphic designers can do more than providing static graphics for programmers to incorporate into applications. We describe a technique that allows them to provide examples of graphical objects at various key sizes using their usual drawing tool, then let the system interpolate their resizing behavior. We relate this technique to current practices of graphic designers, provide examples of its use and describe the underlying inference algorithm. We show how the mathematical properties of the algorithm allows the system to be predictable and explain how it can be combined with more traditional layout mechanisms.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {201–210},
numpages = {10},
keywords = {interpolation, constraints, SVG, visual design, vector graphics, GUI tools, resizing, layout},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095070,
author = {Kandogan, Eser and Haber, Eben and Barrett, Rob and Cypher, Allen and Maglio, Paul and Zhao, Haixia},
title = {A1: End-User Programming for Web-Based System Administration},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095070},
doi = {10.1145/1095034.1095070},
abstract = {System administrators work with many different tools to manage and fix complex hardware and software infrastructure in a rapidly paced work environment. Through extensive field studies, we observed that they often build and share custom tools for specific tasks that are not supported by vendor tools. Recent trends toward web-based management consoles offer many advantages but put an extra burden on system administrators, as customization requires web programming, which is beyond the skills of many system administrators. To meet their needs, we developed A1, a spreadsheet-based environment with a task-specific system-administration language for quickly creating small tools or migrating existing scripts to run as web portlets. Using A1, system administrators can build spreadsheets to access remote and heterogeneous systems, gather and integrate status data, and orchestrate control of disparate systems in a uniform way. A preliminary user study showed that in just a few hours, system administrators can learn to use A1 to build relatively complex tools from scratch.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {211–220},
numpages = {10},
keywords = {system management, spreadsheets, end-user programming, web-portal user interfaces},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095071,
author = {Li, Yang and Landay, James A.},
title = {Informal Prototyping of Continuous Graphical Interactions by Demonstration},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095071},
doi = {10.1145/1095034.1095071},
abstract = {Informal prototyping tools have shown great potential in facilitating the early stage design of user interfaces. How-ever, continuous interactions, an important constituent of highly interactive interfaces, have not been well supported by previous tools. These interactions give continuous visual feedback, such as geometric changes of a graphical object, in response to continuous user input, such as the movement of a mouse. We built Monet, a sketch-based tool for proto-typing continuous interactions by demonstration. In Monet, designers can prototype continuous widgets and their states of interest using examples. They can also demonstrate com-pound behaviors involving multiple widgets by direct ma-nipulation. Monet allows continuous interactions to be eas-ily integrated with event-based, discrete interactions. Con-tinuous widgets can be embedded into storyboards and their states can condition or trigger storyboard transitions. Monet achieves these features by employing continuous function approximation and statistical classification techniques, without using any domain specific knowledge or assuming any application semantics. Informal feedback showed that Monet is a promising approach to enabling more complete tool support for early stage UI design.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {221–230},
numpages = {10},
keywords = {sketching, direct manipulation, informal prototyping, pen-based user interfaces, continuous interactions, programming by demonstration},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095073,
author = {Marti, Stefan and Schmandt, Chris},
title = {Physical Embodiments for Mobile Communication Agents},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095073},
doi = {10.1145/1095034.1095073},
abstract = {This paper describes a physically embodied and animated user interface to an interactive call handling agent, consisting of a small wireless animatronic device in the form of a squirrel, bunny, or parrot. A software tool creates movement primitives, composes these primitives into complex behaviors, and triggers these behaviors dynamically at state changes in the conversational agent's finite state machine. Gaze and gestural cues from the animatronics alert both the user and co-located third parties of incoming phone calls, and data suggests that such alerting is less intrusive than conventional telephones.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {231–240},
numpages = {10},
keywords = {robotic user interface, embodiment, conversational agent, human style non-verbal cues, interruptions},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095074,
author = {Liao, Chunyuan and Guimbreti\`{e}re, Fran\c{c}ois and Hinckley, Ken},
title = {PapierCraft: A Command System for Interactive Paper},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095074},
doi = {10.1145/1095034.1095074},
abstract = {Knowledge workers use paper extensively for document reviewing and note-taking due to its versatility and simplicity of use. As users annotate printed documents and gather notes, they create a rich web of annotations and cross references. Unfortunately, as paper is a static media, this web often gets trapped in the physical world. While several digital solutions such as XLibris [15] and Digital Desk [18] have been proposed, they suffer from a small display size or onerous hardware requirements.To address these limitations, we propose PapierCraft, a gesture-based interface that allows users to manipulate digital documents directly using their printouts as proxies. Using a digital pen, users can annotate a printout or draw command gestures to indicate operations such as copying a document area, pasting an area previously copied, or creating a link. Upon pen synchronization, our infrastructure executes these commands and presents the result in a customized viewer. In this paper we describe the design and implementation of the PapierCraft command system, and report on early user feedback.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {241–244},
numpages = {4},
keywords = {pens, paper interfaces, distributed systems},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095075,
author = {Dietz, Paul H. and Harsham, Bret and Forlines, Clifton and Leigh, Darren and Yerazunis, William and Shipman, Sam and Schmidt-Nielsen, Bent and Ryall, Kathy},
title = {DT Controls: Adding Identity to Physical Interfaces},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095075},
doi = {10.1145/1095034.1095075},
abstract = {In this paper, we show how traditional physical interface components such as switches, levers, knobs and touch screens can be easily modified to identify who is activating each control. This allows us to change the function per-formed by the control, and the sensory feedback provided by the control itself, dependent upon the user. An auditing function is also available that logs each user's actions. We describe a number of example usage scenarios for our tech-nique, and present two sample implementations.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {245–252},
numpages = {8},
keywords = {haptics, auditing, DiamondTouch, identity, physical interfaces, multi-user},
location = {Seattle, WA, USA},
series = {UIST '05}
}

@inproceedings{10.1145/1095034.1095076,
author = {Mankoff, Demi and Dey, Anind and Mankoff, Jennifer and Mankoff, Ken},
title = {Supporting Interspecies Social Awareness: Using Peripheral Displays for Distributed Pack Awareness},
year = {2005},
isbn = {1595932712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095034.1095076},
doi = {10.1145/1095034.1095076},
abstract = {In interspecies households, it is common for the non homo sapien members to be isolated and ignored for many hours each day when humans are out of the house or working. For pack animals, such as canines, information about a pack member's extended pack interactions (outside of the nuclear household) could help to mitigate this social isolation. We have developed a Pack Activity Watch System: Allowing Broad Interspecies Love In Telecommunication with Internet-Enabled Sociability (PAWSABILITIES) for helping to support remote awareness of social activities. Our work focuses on canine companions, and includes, pawticipatory design, labradory tests, and canid camera monitoring.},
booktitle = {Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology},
pages = {253–258},
numpages = {6},
keywords = {interspecies interaction, dogs, peripheral displays, awareness},
location = {Seattle, WA, USA},
series = {UIST '05}
}

