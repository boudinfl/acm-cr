@inproceedings{10.1145/1029632.1029635,
author = {Apitz, Georg and Guimbreti\`{e}re, Fran\c{c}ois},
title = {CrossY: A Crossing-Based Drawing Application},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029635},
doi = {10.1145/1029632.1029635},
abstract = {We introduce CrossY, a simple drawing application developed as a benchmark to demonstrate the feasibility of goal crossing as the basis for a graphical user interface. We show that crossing is not only as expressive as the current point-and-click interface, but also offers more flexibility in interaction design. In particular, crossing encourages the fluid composition of commands which supports the development of more fluid interfaces. While crossing was previously identified as a potential substitute for the classic point-and-click interaction, this work is the first to report on the practical aspects of implementing an interface based on goal crossing as the fundamental building block.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {3–12},
numpages = {10},
keywords = {fluid interaction, pen-computing, command composition, crossing based interfaces},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029636,
author = {Kara, Levent Burak and Stahovich, Thomas F.},
title = {Hierarchical Parsing and Recognition of Hand-Sketched Diagrams},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029636},
doi = {10.1145/1029632.1029636},
abstract = {A long standing challenge in pen-based computer interaction is the ability to make sense of informal sketches. A main difficulty lies in reliably extracting and recognizing the intended set of visual objects from a continuous stream of pen strokes. Existing pen-based systems either avoid these issues altogether, thus resulting in the equivalent of a drawing program, or rely on algorithms that place unnatural constraints on the way the user draws. As one step toward alleviating these difficulties, we present an integrated sketch parsing and recognition approach designed to enable natural, fluid, sketch-based computer interaction. The techniques presented in this paper are oriented toward the domain of network diagrams. In the first step of our approach, the stream of pen strokes is examined to identify the arrows in the sketch. The identified arrows then anchor a spatial analysis which groups the uninterpreted strokes into distinct clusters, each representing a single object. Finally, a trainable shape recognizer, which is informed by the spatial analysis, is used to find the best interpretations of the clusters. Based on these concepts, we have built SimuSketch, a sketch-based interface for Matlab's Simulink software package. An evaluation of SimuSketch has indicated that even novice users can effectively utilize our system to solve real engineering problems without having to know much about the underlying recognition techniques.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {13–22},
numpages = {10},
keywords = {Simulink, pnns, visual parsing, symbol recognition, pen computing, sketch understanding},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029637,
author = {Alvarado, Christine and Davis, Randall},
title = {SketchREAD: A Multi-Domain Sketch Recognition Engine},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029637},
doi = {10.1145/1029632.1029637},
abstract = {We present SketchREAD, a multi-domain sketch recognition engine capable of recognizing freely hand-drawn diagrammatic sketches. Current computer sketch recognition systems are difficult to construct, and either are fragile or accomplish robustness by severely limiting the designer's drawing freedom. Our system can be applied to a variety of domains by providing structural descriptions of the shapes in that domain; no training data or programming is necessary. Robustness to the ambiguity and uncertainty inherent in complex, freely-drawn sketches is achieved through the use of context. The system uses context to guide the search for possible interpretations and uses a novel form of dynamically constructed Bayesian networks to evaluate these interpretations. This process allows the system to recover from low-level recognition errors (e.g., a line misclassified as an arc) that would otherwise result in domain level recognition errors. We evaluated Sketch-READ on real sketches in two domains--family trees and circuit diagrams--and found that in both domains the use of context to reclassify low-level shapes significantly reduced recognition error over a baseline system that did not reinterpret low-level classifications. We also discuss the system's potential role in sketch based user interfaces.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {23–32},
numpages = {10},
keywords = {Bayesian networks, intelligent UIs, sketch recognition, input and interaction technology, pen-based UIs},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029639,
author = {Zhao, Shengdong and Balakrishnan, Ravin},
title = {Simple vs. Compound Mark Hierarchical Marking Menus},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029639},
doi = {10.1145/1029632.1029639},
abstract = {We present a variant of hierarchical marking menus where items are selected using a series of inflection-free simple marks, rather than the single "zig-zag" compound mark used in the traditional design. Theoretical analysis indicates that this simple mark approach has the potential to significantly increase the number of items in a marking menu that can be selected efficiently and accurately. A user experiment is presented that compares the simple and compound mark techniques. Results show that the simple mark technique allows for significantly more accurate and faster menu selections overall, but most importantly also in menus with a large number of items where performance of the compound mark technique is particularly poor. The simple mark technique also requires significantly less physical input space to perform the selections, making it particularly suitable for small footprint pen-based input devices. Visual design alternatives are also discussed.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {33–42},
numpages = {10},
keywords = {pie menus, marking menus},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029640,
author = {Kristensson, Per-Ola and Zhai, Shumin},
title = {SHARK<sup>2</sup>: A Large Vocabulary Shorthand Writing System for Pen-Based Computers},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029640},
doi = {10.1145/1029632.1029640},
abstract = {Zhai and Kristensson (2003) presented a method of speed-writing for pen-based computing which utilizes gesturing on a stylus keyboard for familiar words and tapping for others. In SHARK<sup>2</sup>:, we eliminated the necessity to alternate between the two modes of writing, allowing any word in a large vocabulary (e.g. 10,000-20,000 words) to be entered as a shorthand gesture. This new paradigm supports a gradual and seamless transition from visually guided tracing to recall-based gesturing. Based on the use characteristics and human performance observations, we designed and implemented the architecture, algorithms and interfaces of a high-capacity multi-channel pen-gesture recognition system. The system's key components and performance are also reported.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {43–52},
numpages = {10},
keywords = {gesture recognition, shorthand recognition, text input, shorthand, stenography},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029641,
author = {Smith, G. M. and schraefel, m. c.},
title = {The Radial Scroll Tool: Scrolling Support for Stylus- or Touch-Based Document Navigation},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029641},
doi = {10.1145/1029632.1029641},
abstract = {We present radial scroll, an interface widget to support scrolling particularly on either small or large scale touch displays. Instead of dragging a elevator in a scroll bar, or using repetitive key presses to page up or down, users gesture anywhere on the document surface such that clockwise gestures advance the document; counter clockwise gestures reverse the document. We describe our prototype implementation and discuss the results of an initial user study.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {53–56},
numpages = {4},
keywords = {large displays, touch screens, scrolling, stylus input, radial scroll},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029642,
author = {Moscovich, Tomer and Hughes, John F.},
title = {Navigating Documents with the Virtual Scroll Ring},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029642},
doi = {10.1145/1029632.1029642},
abstract = {We present a technique for scrolling through documents that is simple to implement and requires no special hardware. This is accomplished by simulating a hardware scroll ring--a device that maps circular finger motion into vertical scrolling. The technique performs at least as well as a mouse wheel for medium and long distances, and is preferred by users. It can be particularly useful in portable devices where screen-space and space for peripherals is at a premium.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {57–60},
numpages = {4},
keywords = {scrolling, interaction techniques},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029644,
author = {Grossman, Tovi and Wigdor, Daniel and Balakrishnan, Ravin},
title = {Multi-Finger Gestural Interaction with 3d Volumetric Displays},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029644},
doi = {10.1145/1029632.1029644},
abstract = {Volumetric displays provide interesting opportunities and challenges for 3D interaction and visualization, particularly when used in a highly interactive manner. We explore this area through the design and implementation of techniques for interactive direct manipulation of objects with a 3D volumetric display. Motion tracking of the user's fingers provides for direct gestural interaction with the virtual objects, through manipulations on and around the display's hemispheric enclosure. Our techniques leverage the unique features of volumetric displays, including a 360° viewing volume that enables manipulation from any viewpoint around the display, as well as natural and accurate perception of true depth information in the displayed 3D scene. We demonstrate our techniques within a prototype 3D geometric model building application.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {61–70},
numpages = {10},
keywords = {multi-finger and two-handed gestural input, volumetric display, 3d interaction},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029645,
author = {Carpendale, Sheelagh and Light, John and Pattison, Eric},
title = {Achieving Higher Magnification in Context},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029645},
doi = {10.1145/1029632.1029645},
abstract = {The difficulty of accessing information details while preserving context has generated many different focus-in-context techniques. A common limitation of focus-in-context techniques is their ability to work well at high magnification. We present a set of improvements that will make high magnification in context more feasible. We demonstrate new distortion functions that effectively integrate high magnification within its context. Finally, we show how lenses can be used on top of other lenses, effectively multiplying their magnification power in the same manner that a magnifying glass applied on top of another causes multiplicative magnification. The combined effect is to change feasible detail-in-context magnification factors from less than 8 to more than 40.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {71–80},
numpages = {10},
keywords = {distortion viewing, focus-in-context, magnification},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029646,
author = {Bae, Seok-Hyung and Kobayash, Takahiro and Kijima, Ryugo and Kim, Won-Sup},
title = {Tangible NURBS-Curve Manipulation Techniques Using Graspable Handles on a Large Display},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029646},
doi = {10.1145/1029632.1029646},
abstract = {This paper presents tangible interaction techniques for fine-tuning one-to-one scale NURBS curves on a large display for automotive design. We developed a new graspable handle with a transparent groove that allows designers to manipulate virtual curves on a display screen directly. The use of the proposed handle leads naturally to a rich vocabulary of terms describing interaction techniques that reflect existing shape styling methods. A user test raised various issues related to the graspable user interface, two-handed input, and large-display interaction.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {81–90},
numpages = {10},
keywords = {graspable handle, two-handed input, NURBS-curve manipulation, automotive design, large display, graspable user interface},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029647,
author = {Baudisch, Patrick and Xie, Xing and Wang, Chong and Ma, Wei-Ying},
title = {Collapse-to-Zoom: Viewing Web Pages on Small Screen Devices by Interactively Removing Irrelevant Content},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029647},
doi = {10.1145/1029632.1029647},
abstract = {Overview visualizations for small-screen web browsers were designed to provide users with visual context and to allow them to rapidly zoom in on tiles of relevant content. Given that content in the overview is reduced, however, users are often unable to tell which tiles hold the relevant material, which can force them to adopt a time-consuming hunt-and-peck strategy. Collapse-to-zoom addresses this issue by offering an alternative exploration strategy. In addition to allowing users to zoom into relevant areas, collapse-to-zoom allows users to collapse areas deemed irrelevant, such as columns containing menus, archive material, or advertising. Collapsing content causes all remaining content to expand in size causing it to reveal more detail, which increases the user's chance of identifying relevant content. Collapse-to-zoom navigation is based on a hybrid between a marquee selection tool and a marking menu, called marquee menu. It offers four commands for collapsing content areas at different granularities and to switch to a full-size reading view of what is left of the page.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {91–94},
numpages = {4},
keywords = {gesture, marquee menu, collapse-to-zoom, web browsing, overview, pen, PDA, small screen device},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029648,
author = {Singh, Karan and Grimm, Cindy and Sudarsanam, Nisha},
title = {The IBar: A Perspective-Based Camera Widget},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029648},
doi = {10.1145/1029632.1029648},
abstract = {We present a new screen space widget, the IBar, for effective camera control in 3D graphics environments. The IBar provides a compelling interface for controlling scene perspective based on the artistic concept of vanishing points. Various handles on the widget manipulate multiple camera parameters simultaneously to create a single perceived projection change. For example, changing just the perspective distortion is accomplished by simultaneously decreasing the camera's distance to the scene while increasing focal length. We demonstrate that the IBar is easier to learn for novice users and improves their understanding of camera perspective.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {95–98},
numpages = {4},
keywords = {camera control, widgets, perspective},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029650,
author = {Kim, Jiwon and Seitz, Steven M. and Agrawala, Maneesh},
title = {Video-Based Document Tracking: Unifying Your Physical and Electronic Desktops},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029650},
doi = {10.1145/1029632.1029650},
abstract = {This paper presents an approach for tracking paper documents on the desk over time and automatically linking them to the corresponding electronic documents using an overhead video camera. We demonstrate our system in the context of two scenarios, <i>paper tracking</i> and <i>photo sorting</i>. In the paper tracking scenario, the system tracks changes in the stacks of printed documents and books on the desk and builds a complete representation of the spatial structure of the desktop. When users want to find a printed document buried in the stacks, they can query the system based on appearance, keywords, or access time. The system also provides a <i>remote desktop</i> interface for directly browsing the physical desktop from a remote location. In the photo sorting scenario, users sort printed photographs into physical stacks on the desk. The systemautomatically recognizes the photographs and organizes the corresponding digital photographs into separate folders according to the physical arrangement. Our framework provides a way to unify the physical and electronic desktops without the need for a specialized physical infrastructure except for a video camera.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {99–107},
numpages = {9},
keywords = {interactive desktop, document recognition, video analysis, intelligent office},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029651,
author = {Yamada, Toshiya and Shingu, Jun and Churchill, Elizabeth and Nelson, Les and Helfman, Jonathan and Murphy, Paul},
title = {Who Cares? Reflecting Who is Reading What on Distributed Community Bulletin Boards},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029651},
doi = {10.1145/1029632.1029651},
abstract = {In this paper, we describe the YeTi information sharing system that has been designed to foster community building through informal digital content sharing. The YeTi system is a general information parsing, hosting and distribution infrastructure, with interfaces designed for individual and public content reading. In this paper we describe the YeTi public display interface, with a particular focus on tools we have designed to provide lightweight awareness of others' interactions with posted content. Our tools augment content with metadata that reflect people's reading of content - captured video clips of who's reading and interacting with content, tools to allow people to leave explicit freehand annotations about content, and a visualization of the content access history to show when content is interacted with. Results from an initial evaluation are presented and discussed.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {109–118},
numpages = {10},
keywords = {video capturing, situated display, public commentary, distant communities, annotation, information sharing},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029652,
author = {Letessier, Julien and B\'{e}rard, Fran\c{c}ois},
title = {Visual Tracking of Bare Fingers for Interactive Surfaces},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029652},
doi = {10.1145/1029632.1029652},
abstract = {Visual tracking of bare fingers allows more direct manipulation of digital objects, multiple simultaneous users interacting with their two hands, and permits the interaction on large surfaces, using only commodity hardware. After presenting related work, we detail our implementation. Its design is based on our modeling of two classes of algorithms that are key to the tracker: Image Differencing Segmentation (IDS) and Fast Rejection Filters (FRF). We introduce a new chromatic distance for IDS and a FRF that is independent to finger rotation. The system runs at full frame rate (25 Hz) with an average total system latency of 80 ms, independently of the number of tracked fingers. When used in a controlled environment such as a meeting room, its robustness is satisfying for everyday use.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {119–122},
numpages = {4},
keywords = {finger tracking with computer vision, large interactive surface, multi-user multi-hand interaction},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029653,
author = {Lee, Johnny C. and Dietz, Paul H. and Maynes-Aminzade, Dan and Raskar, Ramesh and Hudson, Scott E.},
title = {Automatic Projector Calibration with Embedded Light Sensors},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029653},
doi = {10.1145/1029632.1029653},
abstract = {Projection technology typically places several constraints on the geometric relationship between the projector and the projection surface to obtain an undistorted, properly sized image. In this paper we describe a simple, robust, fast, and low-cost method for automatic projector calibration that eliminates many of these constraints. We embed light sensors in the target surface, project Gray-coded binary patterns to discover the sensor locations, and then prewarp the image to accurately fit the physical features of the projection surface. This technique can be expanded to automatically stitch multiple projectors, calibrate onto non-planar surfaces for object decoration, and provide a method for simple geometry acquisition.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {123–126},
numpages = {4},
keywords = {object decoration, multi-projector stitching, structured light, projector calibration, keystone correction},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029655,
author = {Khan, Azam and Fitzmaurice, George and Almeida, Don and Burtnyk, Nicolas and Kurtenbach, Gordon},
title = {A Remote Control Interface for Large Displays},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029655},
doi = {10.1145/1029632.1029655},
abstract = {We describe a new widget and interaction technique, known as a "Frisbee," for interacting with areas of a large display that are difficult or impossible to access directly. A frisbee is simply a portal to another part of the display. It consists of a local "telescope" and a remote "target". The remote data surrounded by the target is drawn in the telescope and interactions performed within it are applied on the remote data. In this paper we define the behavior of frisbees, show unique affordances of the widget, and discuss design characteristics. We have implemented a test application and report on an experiment that shows the benefit of using the frisbee on a large display. Our results suggest that the frisbee is preferred over walking back and forth to the local and remote spaces at a distance of 4.5 feet.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {127–136},
numpages = {10},
keywords = {drag-and-drop, pen user interfaces, interaction technique, large displays},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029656,
author = {Vogel, Daniel and Balakrishnan, Ravin},
title = {Interactive Public Ambient Displays: Transitioning from Implicit to Explicit, Public to Personal, Interaction with Multiple Users},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029656},
doi = {10.1145/1029632.1029656},
abstract = {We develop design principles and an interaction framework for sharable, interactive public ambient displays that support the transition from implicit to explicit interaction with both public and personal information. A prototype system implementation that embodies these design principles is described. We use novel display and interaction techniques such as simple hand gestures and touch screen input for explicit interaction and contextual body orientation and position cues for implicit interaction. Techniques are presented for subtle notification, self-revealing help, privacy controls, and shared use by multiple people each in their own context. Initial user feedback is also presented, and future directions discussed.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {137–146},
numpages = {10},
keywords = {ambient displays, subtle interaction, interactive public displays, ubicomp},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029657,
author = {Miyaoku, Kento and Higashino, Suguru and Tonomura, Yoshinobu},
title = {C-Blink: A Hue-Difference-Based Light Signal Marker for Large Screen Interaction via Any Mobile Terminal},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029657},
doi = {10.1145/1029632.1029657},
abstract = {To enable common mobile terminals to interact with contents shown on large screens, we propose "C-Blink", a new light signal marker method that uses the color liquid-crystal display of a mobile terminal as a visible light source. We overcome the performance limitations of such displays by developing a hue-difference-blink technique. In combination with a screen-side sensor, we describe a system that detects and receives light signal markers sent by cell phone displays. Evaluations of a prototype system confirm that C-Blink performs well under common indoor lighting. The C-Blink program can be installed in any mobile terminal that has a color display, and the installation costs are small. C-Blink is a very useful way of enabling ubiquitous large screens to become interfaces for mobile terminals.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {147–156},
numpages = {10},
keywords = {screen interaction, hue difference, color display, mobile terminal},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029658,
author = {Patel, Shwetak N. and Pierce, Jeffrey S. and Abowd, Gregory D.},
title = {A Gesture-Based Authentication Scheme for Untrusted Public Terminals},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029658},
doi = {10.1145/1029632.1029658},
abstract = {Powerful mobile devices with minimal I/O capabilities increase the likelihood that we will want to annex these devices to I/O resources we encounter in the local environment. This opportunistic annexing will require authentication. We present a sensor-based authentication mechanism for mobile devices that relies on physical possession instead of knowledge to setup the initial connection to a public terminal. Our solution provides a simple mechanism for shaking a device to authenticate with the public infrastructure, making few assumptions about the surrounding infrastructure while also maintaining a reasonable level of security.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {157–160},
numpages = {4},
keywords = {sensors, mobile phone, interaction with gestures},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029660,
author = {Greenberg, Saul},
title = {Physical User Interfaces: What They Are and How to Build Them},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029660},
doi = {10.1145/1029632.1029660},
abstract = {Physical user interfaces are special purpose devices that can be situated in a real-world setting. Unlike general purpose computers, they are typically designed for particular contexts and uses. In this survey, I present an introductory tour of this new interface genre. First, I will summarize what they are by describing several design niches for these devices: ubiquitous computing, tangible media, foreground and ambient devices, collaborative devices, roomware, and physical controls. Examples will be plentiful, and will range from the playful, to the artistic, and to the serious. Second, I will introduce technologies that are suitable for software professionals who wish to prototype these physical user interfaces. The commercially available Phidgets (www.phidgets.com) are used as a case study of what is available and what can be done with them.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {161},
numpages = {1},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029661,
author = {Kaye, Joseph 'Jofish'},
title = {Olfactory Display},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029661},
doi = {10.1145/1029632.1029661},
abstract = {The last twenty years have seen enormous leaps forward in computers' abilities to generate sound and video. What happens when computers can produce scents on demand? In this talk, I present three approaches to this question. I first look at human olfactory processing: what is our olfactory bandwidth, and what are the limitations of our sense of smell? I then explore the use of scent to accompany other media, from historical examples like Sense-o-Rama and Aromarama, to more recent work including firefighter training systems, augmented gaming, and food and beverage applications. Finally, I look at the possibilities of olfactory output as an ambient display medium. I conclude with an overview of current computer-controlled olfactory output devices: off the shelf solutions for incorporating scent into user interface applications.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {163},
numpages = {1},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029663,
author = {Olsen, Dan R. and Taufer, Trent and Fails, Jerry Alan},
title = {ScreenCrayons: Annotating Anything},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029663},
doi = {10.1145/1029632.1029663},
abstract = {ScreenCrayons is a system for collecting annotations on any type of document or visual information from any application. The basis for the system is a screen capture upon which the user can highlight the relevant portions of the image. The user can define any number of topics for organizing notes. Each topic is associated with a highlighting "crayon." In addition the user can supply annotations in digital ink or text. Algorithms are described that summarize captured images based on the highlight strokes so as to provide overviews of many annotations as well as being able to "zoom in" on particular information about a given note and the context of that note.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {165–174},
numpages = {10},
keywords = {screen capture, annotation, digital ink, image summarization},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029664,
author = {Fujima, Jun and Lunzer, Aran and Hornb\ae{}k, Kasper and Tanaka, Yuzuru},
title = {Clip, Connect, Clone: Combining Application Elements to Build Custom Interfaces for Information Access},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029664},
doi = {10.1145/1029632.1029664},
abstract = {Many applications provide a form-like interface for requesting information: the user fills in some fields, submits the form, and the application presents corresponding results. Such a procedure becomes burdensome if (1) the user must submit many different requests, for example in pursuing a trial-and-error search, (2) results from one application are to be used as inputs for another, requiring the user to transfer them by hand, or (3) the user wants to compare results, but only the results from one request can be seen at a time. We describe how users can reduce this burden by creating custom interfaces using three mechanisms: clipping of input and result elements from existing applications to form cells on a spreadsheet; connecting these cells using formulas, thus enabling result transfer between applications; and cloning cells so that multiple requests can be handled side by side. We demonstrate a prototype of these mechanisms, initially specialised for handling Web applications, and show how it lets users build new interfaces to suit their individual needs.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {175–184},
numpages = {10},
keywords = {parallel exploration, customized information access, end-user programming},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029665,
author = {Stylos, Jeffrey and Myers, Brad A. and Faulring, Andrew},
title = {Citrine: Providing Intelligent Copy-and-Paste},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029665},
doi = {10.1145/1029632.1029665},
abstract = {We present Citrine, a system that extends the widespread copy-and-paste interaction technique with intelligent transformations, making it useful in more situations. Citrine uses text parsing to find the structure in copied text and allows users to paste the structured information, which might have many pieces, in a single paste operation. For example, using Citrine, a user can copy the text of a meeting request and add it to the Outlook calendar with a single paste. In applications such as Excel, users can teach Citrine by example how to copy and paste data by showing it which fields go into which columns, and can use this to copy or paste many items at a time in a user-defined manner. Citrine can be used with a wide variety of applications and types of data and can be easily extended to work with more. It currently includes parsers that recognize contact information, calendar appointments and bibliographic citations. It works with Internet Explorer, Outlook, Excel, Palm Desktop, EndNote and other applications. Citrine is available to download on the internet.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {185–188},
numpages = {4},
keywords = {web-pasting, copy-and-paste, intelligent user interfaces},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029666,
author = {Ishak, Edward W. and Feiner, Steven K.},
title = {Interacting with Hidden Content Using Content-Aware Free-Space Transparency},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029666},
doi = {10.1145/1029632.1029666},
abstract = {We present <i>content-aware free-space transparency</i>, an approach to viewing and manipulating the otherwise hidden content of obscured windows through unimportant regions of overlapping windows. Traditional approaches to interacting with otherwise obscured content in a window system render an entire window uniformly transparent. In contrast, content-aware free-space transparency uses opaque-to-transparent gradients and image-processing filters to minimize the interference from overlapping material, based on properties of that material. By increasing the amount of simultaneously visible content and allowing basic interaction with otherwise obscured content, without modifying window geometry, we believe that free-space transparency has the potential to improve user productivity.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {189–192},
numpages = {4},
keywords = {screen space, pie menu, space management, transparency, content disambiguation, interaction techniques},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029667,
author = {Dragicevic, Pierre},
title = {Combining Crossing-Based and Paper-Based Interaction Paradigms for Dragging and Dropping between Overlapping Windows},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029667},
doi = {10.1145/1029632.1029667},
abstract = {Despite novel interaction techniques proposed for virtual desktops, common yet challenging tasks remain to be investigated. Dragging and dropping between overlapping windows is one of them. The fold-and-drop technique presented here offers a natural and efficient way of performing those tasks. We show how this technique successfully builds upon several interaction paradigms previously described, while shedding new light on them.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {193–196},
numpages = {4},
keywords = {crossing-based interfaces, drag-and-drop, gestural interaction, paper-based metaphors},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029669,
author = {MacIntyre, Blair and Gandy, Maribeth and Dow, Steven and Bolter, Jay David},
title = {DART: A Toolkit for Rapid Design Exploration of Augmented Reality Experiences},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029669},
doi = {10.1145/1029632.1029669},
abstract = {In this paper, we describe The Designer's Augmented Reality Toolkit (DART). DART is built on top of Macromedia Director, a widely used multimedia development environment. We summarize the most significant problems faced by designers working with AR in the real world, and discuss how DART addresses them. Most of DART is implemented in an interpreted scripting language, and can be modified by designers to suit their needs. Our work focuses on supporting early design activities, especially a rapid transition from story-boards to working experience, so that the experiential part of a design can be tested early and often. DART allows designers to specify complex relationships between the physical and virtual worlds, and supports 3D animatic actors (informal, sketch-based content) in addition to more polished content. Designers can capture and replay synchronized video and sensor data, allowing them to work off-site and to test specific parts of their experience more effectively.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {197–206},
numpages = {10},
keywords = {design environments, capture/replay, mixed reality, augmented reality, animatics, storyboards},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029670,
author = {Vander Zanden, Bradley T. and Baker, David and Jin, Jing},
title = {An Explanation-Based, Visual Debugger for One-Way Constraints},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029670},
doi = {10.1145/1029632.1029670},
abstract = {This paper describes a domain-specific debugger for one-way constraint solvers. The debugger makes use of several new techniques. First, the debugger displays only a portion of the dataflow graph, called a <i>constraint slice</i>, that is directly related to an incorrect variable. This technique helps the debugger scale to a system containing thousands of constraints. Second, the debugger presents a visual representation of the solver's data structures and uses color encodings to highlight changes to the data structures. Finally, the debugger allows the user to point to a variable that has an unexpected value and ask the debugger to suggest reasons for the unexpected value. The debugger makes use of information gathered during the constraint satisfaction process to generate plausible suggestions. Informal testing has shown that the explanatory capability and the color coding of the constraint solver's data structures are particularly useful in locating bugs in constraint code.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {207–216},
numpages = {10},
keywords = {software visualization, data structures, visual debugging, one-way constraints, constraint satisfaction},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029671,
author = {Li, Yang and Hong, Jason I. and Landay, James A.},
title = {Topiary: A Tool for Prototyping Location-Enhanced Applications},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029671},
doi = {10.1145/1029632.1029671},
abstract = {Location-enhanced applications use the location of people, places, and things to augment or streamline interaction. Location-enhanced applications are just starting to emerge in several different domains, and many people believe that this type of application will experience tremendous growth in the near future. However, it currently requires a high level of technical expertise to build location-enhanced applications, making it hard to iterate on designs. To address this problem we introduce Topiary, a tool for rapidly prototyping location-enhanced applications. Topiary lets designers create a map that models the location of people, places, and things; use this active map to demonstrate scenarios depicting location contexts; use these scenarios in creating storyboards that describe interaction sequences; and then run these storyboards on mobile devices, with a wizard updating the location of people and things on a separate device. We performed an informal evaluation with seven researchers and interface designers and found that they reacted positively to the concept.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {217–226},
numpages = {10},
keywords = {ubiquitous computing, informal user interface, Wizard of Oz, location-enhanced, context-aware, prototyping},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029673,
author = {Zhou, Michelle X. and Aggarwal, Vikram},
title = {An Optimization-Based Approach to Dynamic Data Content Selection in Intelligent Multimedia Interfaces},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029673},
doi = {10.1145/1029632.1029673},
abstract = {We are building a multimedia conversation system to facilitate information seeking in large and complex data spaces. To provide tailored responses to diverse user queries introduced during a conversation, we automate the generation of a system response. Here we focus on the problem of determining the data content of a response. Specifically, we develop an optimization-based approach to content selection. Compared to existing rule-based or plan-based approaches, our work offers three unique contributions. First, our approach provides a general framework that effectively addresses content selection for various interaction situations by balancing a comprehensive set of constraints (e.g., content quality and quantity constraints). Second, our method is easily extensible, since it uses feature-based metrics to systematically model selection constraints. Third, our method improves selection results by incorporating content organization and media allocation effects, which otherwise are treated separately. Preliminary studies show that our method can handle most of the user situations identified in a Wizard-of-Oz study, and achieves results similar to those produced by human designers.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {227–236},
numpages = {10},
keywords = {intelligent multimedia interfaces, automated generation of multimedia presentations},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029674,
author = {Lyons, Kent and Skeels, Christopher and Starner, Thad and Snoeck, Cornelis M. and Wong, Benjamin A. and Ashbrook, Daniel},
title = {Augmenting Conversations Using Dual-Purpose Speech},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029674},
doi = {10.1145/1029632.1029674},
abstract = {In this paper, we explore the concept of dual-purpose speech: speech that is socially appropriate in the context of a human-to-human conversation which also provides meaningful input to a computer. We motivate the use of dual-purpose speech and explore issues of privacy and technological challenges related to mobile speech recognition. We present three applications that utilize dual-purpose speech to assist a user in conversational tasks: the Calendar Navigator Agent, DialogTabs, and Speech Courier. The Calendar Navigator Agent navigates a user's calendar based on socially appropriate speech used while scheduling appointments. DialogTabs allows a user to postpone cognitive processing of conversational material by proving short-term capture of transient information. Finally, Speech Courier allows asynchronous delivery of relevant conversational information to a third party.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {237–246},
numpages = {10},
keywords = {speech user interfaces, mobile computing, dual-purpose speech},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029676,
author = {Matthews, Tara and Dey, Anind K. and Mankoff, Jennifer and Carter, Scott and Rattenbury, Tye},
title = {A Toolkit for Managing User Attention in Peripheral Displays},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029676},
doi = {10.1145/1029632.1029676},
abstract = {Traditionally, computer interfaces have been confined to conventional displays and focused activities. However, as displays become embedded throughout our environment and daily lives, increasing numbers of them must operate on the periphery of our attention. <i>Peripheral displays</i> can allow a person to be aware of information while she is attending to some other primary task or activity. We present the Peripheral Displays Toolkit (PTK), a toolkit that provides structured support for managing user attention in the development of peripheral displays. Our goal is to enable designers to explore different approaches to managing user attention. The PTK supports three issues specific to conveying information on the periphery of human attention. These issues are <i>abstraction</i> of raw input, rules for assigning <i>notification levels</i> to input, and <i>transitions</i> for updating a display when input arrives. Our contribution is the investigation of issues specific to attention in peripheral display design and a toolkit that encapsulates support for these issues. We describe our toolkit architecture and present five sample peripheral displays demonstrating our toolkit's capabilities.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {247–256},
numpages = {10},
keywords = {toolkits, user attention, peripheral displays},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029677,
author = {Huot, St\'{e}phane and Dumas, C\'{e}dric and Dragicevic, Pierre and Fekete, Jean-Daniel and H\'{e}gron, G\'{e}rard},
title = {The MaggLite Post-WIMP Toolkit: Draw It, Connect It and Run It},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029677},
doi = {10.1145/1029632.1029677},
abstract = {This article presents MaggLite, a toolkit and sketch-based interface builder allowing fast and interactive design of post-WIMP user interfaces. MaggLite improves design of advanced UIs thanks to its novel <i>mixed-graph</i> architecture that dynamically combines scene-graphs with interaction-graphs. <i>Scene-graphs</i> provide mechanisms to describe and produce rich graphical effects, whereas <i>interaction-graphs</i> allow expressive and fine-grained description of advanced interaction techniques and behaviors such as multiple pointers management, toolglasses, bimanual interaction, gesture, and speech recognition. Both graphs can be built interactively by sketching the UI and specifying the interaction using a dataflow visual language. Communication between the two graphs is managed at runtime by components we call <i>Interaction Access Points</i>. While developers can extend the toolkit by refining built-in generic mechanisms, UI designers can quickly and interactively design, prototype and test advanced user interfaces by applying the MaggLite principle: "draw it, connect it and run it".},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {257–266},
numpages = {10},
keywords = {GUI architectures, MaggLite, interaction techniques, ICON, GUI toolkits, interaction design},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029678,
author = {Chatty, St\'{e}phane and Sire, St\'{e}phane and Vinot, Jean-Luc and Lecoanet, Patrick and Lemort, Alexandre and Mertz, Christophe},
title = {Revisiting Visual Interface Programming: Creating GUI Tools for Designers and Programmers},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029678},
doi = {10.1145/1029632.1029678},
abstract = {Involving graphic designers in the large-scale development of user interfaces requires tools that provide more graphical flexibility and support efficient software processes. These requirements were analysed and used in the design of the TkZ-inc graphical library and the IntuiKit interface design environment. More flexibility is obtained through a wider palette of visual techniques and support for iterative construction of images, composition and parametric displays. More efficient processes are obtained with the use of the SVG standard to import graphics, support for linking graphics and behaviour, and a unifying model-driven architecture. We describe the corresponding features of our tools, and show their use in the development of an application for airports. Benefits include a wider access to high quality visual interfaces for specialised applications, and shorter prototyping and development cycles for multidisciplinary teams.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {267–276},
numpages = {10},
keywords = {model-driven architecture, SVG, GUI tools, visual design, vector graphics, software architecture},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029680,
author = {Chi, Ed H. and Song, Jin and Corbin, Greg},
title = {"Killer App" of Wearable Computing: Wireless Force Sensing Body Protectors for Martial Arts},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029680},
doi = {10.1145/1029632.1029680},
abstract = {Ubiquitous and Wearable Computing both have the goal of pushing the computer into the background, supporting all kinds of human activities. Application areas include areas such as everyday environments (e.g. clothing, home, office), promoting new forms of creative learning via physical/virtual objects, and new tools for interactive design. In this paper, we thrust ubiquitous computing into the extremely hostile environment of the sparring ring of a martial art competition. Our system uses piezoelectric force sensors that transmit signals wirelessly to enable the detection of when a significant impact has been delivered to a competitor's body. The objective is to support the judges in scoring the sparring matches accurately, while preserving the goal of merging and blending into the background of the activity. The system therefore must take into account of the rules of the game, be responsive in real-time asynchronously, and often cope with untrained operators of the system. We present a pilot study of the finished prototype and detail our experience.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {277–285},
numpages = {9},
keywords = {wearable computing, taekwondo, wireless computing, usability of wearable wireless systems, extreme sports},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029681,
author = {Hudson, Scott E.},
title = {Using Light Emitting Diode Arrays as Touch-Sensitive Input and Output Devices},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029681},
doi = {10.1145/1029632.1029681},
abstract = {Light Emitting Diodes (LEDs) offer long life, low cost, efficiency, brightness, and a full range of colors. Because of these properties, they are widely used for simple displays in electronic devices. A previously characterized, but little known property of LEDs allows them to be used as photo sensors. In this paper, we show how this capability can be used to turn unmodified, off the shelf, LED arrays into touch sensitive input devices (while still remaining capable of producing output). The technique is simple and requires little or no extra hardware - in some cases operating with the same micro-controller based circuitry normally used to produce output, requiring only software changes. We will describe a simple hybrid input/output device prototype implemented with this technique, and discuss the design opportunities that this type of device opens up.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {287–290},
numpages = {4},
keywords = {touch sensors, input devices, display devices},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

@inproceedings{10.1145/1029632.1029682,
author = {Lee, Johnny C. and Dietz, Paul H. and Leigh, Darren and Yerazunis, William S. and Hudson, Scott E.},
title = {Haptic Pen: A Tactile Feedback Stylus for Touch Screens},
year = {2004},
isbn = {1581139578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1029632.1029682},
doi = {10.1145/1029632.1029682},
abstract = {In this paper we present a system for providing tactile feedback for stylus-based touch-screen displays. The Haptic Pen is a simple low-cost device that provides individualized tactile feedback for multiple simultaneous users and can operate on large touch screens as well as ordinary surfaces. A pressure-sensitive stylus is combined with a small solenoid to generate a wide range of tactile sensations. The physical sensations generated by the Haptic pen can be used to enhance our existing interaction with graphical user interfaces as well as to help make modern computing systems more accessible to those with visual or motor impairments.},
booktitle = {Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology},
pages = {291–294},
numpages = {4},
keywords = {touch screen, stylus, multiuser, tactile feedback, haptic},
location = {Santa Fe, NM, USA},
series = {UIST '04}
}

