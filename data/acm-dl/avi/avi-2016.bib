@inproceedings{10.1145/2909132.2934645,
author = {Baudisch, Patrick},
title = {Personal Fabrication in HCI: Trends and Challenges},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2934645},
doi = {10.1145/2909132.2934645},
abstract = {In this keynote, I will argue that 3D printing and personal fabrication in general are about to bring massive, disruptive change to interactive computing, as well as to computing as a whole. I discuss the six challenges that need to be addressed for this change to take place, and I explain why I think researchers in HCI will play a key role in it.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {1–2},
numpages = {2},
keywords = {3D printing, future, Personal fabrication},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2933286,
author = {Bianchi-Berthouze, Nadia},
title = {The Affective Body Argument in Technology Design},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2933286},
doi = {10.1145/2909132.2933286},
abstract = {In this paper, I argue that the affective body is underused in the design of interactive technology despite what it has to offer. Whilst the literature shows it to be a powerful affective communication channel, it is often ignored in favor of the more commonly studied facial and vocal expression modalities. This is despite it being as informative and in some situations even more reliable than the other affective channels. In addition, due to the proliferation of increasingly cheaper and ubiquitous movement sensing technologies, the regulatory affective functions of the body could open new possibilities in various application areas. In this paper, after presenting a brief summary of the opportunities that the affective body offers to technology designers, I will use the case of physical rehabilitation to discuss how its use could lead to interesting new solutions and more effective therapies.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {3–6},
numpages = {4},
keywords = {touch behavior, embodied affect, automatic recognition, physical rehabilitation, Affective body expressions, muscle activity},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2933287,
author = {Russell, Daniel M.},
title = {Simple is Good: Observations of Visualization Use Amongst the Big Data Digerati},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2933287},
doi = {10.1145/2909132.2933287},
abstract = {While modern information visualization (IV) has been around for several decades, the inventions of IV seem to be peripheral to the everyday work in companies that would seem to be the most likely to use these inventions. In this case study, Google uses very few IV tools, relying mostly on more traditional ways of looking at data and data relationships. What has brought about this state of affairs? An analysis shows that the basic causes of low adoption are (a) difficulty of data wrangling and sharing the work products of analysis, (b) the need to share a common visual language literacy across different parts of the organization, (c) problems in using IV tools to communicate and present complex data analyses. At the same time, IV technology is found to be more useful in the investigation phase of research, rather than for communication and presentation reasons.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {7–12},
numpages = {6},
keywords = {Information Visualization, Evaluation, Field studies},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909259,
author = {Cremonesi, Paolo and Di Rienzo, Antonella and Garzotto, Franca and Oliveto, Luigi and Piazzolla, Pietro},
title = {Smart Lighting for Fashion Store Windows},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909259},
doi = {10.1145/2909132.2909259},
abstract = {Smart light technology offers new dynamic and interactive capabilities that extend the potential of traditional lighting systems of attracting people and affecting their mood, emotions, and behavior. Our research explores smart lights in the context of shop windows. The paper describes an extensive empirical study that has been performed for 5 weeks in a top-level fashion store located in one of the trendiest and crowed shopping areas in Milan (Italy), where we installed a sophisticate smart lighting system and created three different light configurations (static, dynamic, and interactive). The goal of the study is to explore the shopping experience in front of the shop in these different experimental conditions. Users' data were gathered in three ways: i) automatic collection of data from over &gt;300.000 people passing by or stopping in front of the shop windows of the store; ii) live observation of approximately 600 passers-by; iii) questionnaires submitted to 62 persons in front of the shop. Users' data provide empirical evidence that interactive smart lights have the potential to increase the attractiveness of shopping windows, fostering engagement in passers-by, and in general improve the shopping experience in the brick-and-mortar stores.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {13–20},
numpages = {8},
keywords = {Interactive light, Retail, Shop windows, Fashion},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909248,
author = {Koutsabasis, Panayiotis and Domouzis, Chris K.},
title = {Mid-Air Browsing and Selection in Image Collections},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909248},
doi = {10.1145/2909132.2909248},
abstract = {Image collections are a common interaction pattern for 2D interfaces, however mid-air user interaction with collections has received little attention. We present a controlled experiment (within-groups, n=24) comparing three sets of hand gestures for mid-air browsing and selection in image collections, that were identified out of an elicitation study, using MS Kinect. Each set includes cursor-less gestures for browsing (sideways hand extension, wheel and swipe) and for selection/deselection (hand-up/hand-down). Task success was universal with high accuracy and few errors for all gestures. Sideways extension outperforms swipe and perceived effort for this gesture is significantly lower. Both gestures outperform wheel. We suggest that from a usability perspective, sideways hand extension should be preferred for browsing image galleries, if no other contextual factors apply. Also, the results of the elicitation study, in which most users proposed the swipe gesture for browsing, were not confirmed by the controlled usability experiment. This suggests a combined use of elicitation studies with rigorous usability testing, especially when gestures for particular user interface design patterns are sought.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {21–27},
numpages = {7},
keywords = {Usability, Gestures, Image Collections, Mid-Air Interaction, Browsing, Selection, Elicitation, Kinect},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909254,
author = {Hata, Hajime and Koike, Hideki and Sato, Yoichi},
title = {Visual Guidance with Unnoticed Blur Effect},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909254},
doi = {10.1145/2909132.2909254},
abstract = {In information media such as TV programs, digital signage, or web pages, information content providers often want to guide viewers' attention to a particular location of the display. However, "active" methods, such as flashing displays, using animation, or changing colors, often interrupt viewers' concentration and makes viewers feel annoyed. This paper proposes a method for guiding viewers' attention without viewers noticing. By focusing on a characteristic of the human visual system, we propose a dynamic blur control method. Our method gradually blurs the image on the display to the threshold at which viewers are aware of the modulation of the display, while the region where viewers' attention should be guided remains unblurred. Two subjective experiments were conducted to show the effectiveness of our method. In the first, viewers' attention was guided to the unblurred region using blur control. In the second, a threshold was found at which viewers were aware of the modulation, and viewers' gaze is guided below this threshold. This means that the viewers' attention can be guided without them noticing.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {28–35},
numpages = {8},
keywords = {dynamic blur control, gaze direction, visual saliency, Attentive user interfaces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909285,
author = {Palleis, Henri and Niedermeier, Vanessa},
title = {Exploring Two-Handed Indirect Multi-Touch Input for a Docking Task},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909285},
doi = {10.1145/2909132.2909285},
abstract = {To better understand the characteristics of two-handed indirect multi-touch input, we conducted a docking task experiment that asked participants to align two squares using one-finger dragging as well as two-finger pinch-to-zoom and rotation gestures in three conditions: (1) using the dominant hand only, (2) using the non-dominant hand only and (3) using both hands simultaneously. Our experiment was based on a conventional desktop computer setup, extended with a pair of tablet devices that were placed left and right of the keyboard. Most importantly, the results indicate that the two-handed condition yields the fastest results and the participants' actions exhibited Guiard's well-known principles of asymmetric bimanual interaction. Further, we observed that the performance of the non-dominant hand was on par with the dominant hand, suggesting that designers of two-handed multi-touch input techniques can assume a comparable level of dexterity for both hands.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {36–39},
numpages = {4},
keywords = {desktop computing, two-handed input, indirect touch},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909282,
author = {Gentile, Vito and Sorce, Salvatore and Malizia, Alessio and Pirrello, Dario and Gentile, Antonio},
title = {Touchless Interfaces For Public Displays: Can We Deliver Interface Designers From Introducing Artificial Push Button Gestures?},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909282},
doi = {10.1145/2909132.2909282},
abstract = {Public displays have lately become ubiquitous thanks to the decreasing cost of such technology and public policies supporting the development of smart cities. Depending on form factor, those displays might use touchless gestural interfaces that therefore are becoming more often the subject of public and private research. In this paper, we focus on touchless interactions with situated public displays, and introduce a pilot study on comparing two interfaces: an interface based on the Microsoft Human Interface Guidelines (HIG), a de facto standard in the field, and a novel interface, designed by us. Differently from the HIG-based one, our interface displays an avatar, which does not require any activation gestures to trigger actions. Our aim is to study how the two interfaces address the so-called interaction blindness --- the inability of the users to recognize the interactive capabilities of those displays. According to our pilot study, although providing a different approach, both interfaces has proven effective in the proposed scenario: a public display in a hall inside a University campus building.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {40–43},
numpages = {4},
keywords = {public displays, Touchless interfaces, user interface design, gestural input},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909253,
author = {Sun, Maoyuan and Convertino, Gregorio},
title = {Interver: Drilling into Categorical-Numerical Relationships},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909253},
doi = {10.1145/2909132.2909253},
abstract = {Data analytics is increasingly performed by non-expert analysts (e.g., casual business users). In this context, future analytics tools need easy-to-use techniques to reveal relations between columns of data in a spreadsheet or table. For example, a market analyst, may want to find if industry categories and funding amounts are related: i.e., if some industries receive amounts within distinctive intervals. Traditional filtering and script-based querying poorly support non-expert users in such explorations because they require iterative parameter adjusting and query writing until a meaningful result is found. In this paper, we focus on supporting the analysis of relationships between categorical and numerical columns. We present a novel visualization, Interver, which dynamically reveals insights as the user selects an interval within the relationship. With a concrete scenario, specific analysis tasks, and an informal evaluation, we show how Interver can help non-expert analysts self-serve and answer realistic questions.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {44–51},
numpages = {8},
keywords = {categorical and numerical variables, Visual analytics},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909265,
author = {Guillon, Maxime and Leitner, Fran\c{c}ois and Nigay, Laurence},
title = {Target Expansion Lens: It is Not the More Visual Feedback the Better!},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909265},
doi = {10.1145/2909132.2909265},
abstract = {To enhance pointing tasks, target expansion techniques allocate larger activation areas to targets. We distinguish two basic elements of a target expansion technique: the expansion algorithm and the visual aid on the effective expanded targets. We present a systematic analysis of the relevance of the visual aid provided by (1) existing target expansion techniques and (2) Expansion Lens. The latter is a new continuous technique for acquiring targets. Expansion Lens namely, uses a round area centered on the cursor: the lens. The users can see in the lens the target expanded area boundaries that the lens is hovering over. Expansion Lens serves as a magic lens revealing the underlying expansion algorithm. The design rationale of Expansion Lens is based on a systematic analysis of the relevance of the visual aid according to the three goal-oriented phases of a pointing task namely the starting, transfer and validation phases. Expansion Lens optimizes (1) the transfer phase by providing a simple-shaped visual aid centered on the cursor, and (2) the validation phase regarding error rates, by displaying the target expanded area boundaries. The results of our controlled experiment comparing Expansion Lens with four existing target expansion techniques show that Expansion Lens highlights a good trade-off for performance by being the less-error prone technique and the second fastest technique. The experimental data for each phase of the pointing task also confirm our design approach based on the relevance of the visual aid according to the phase of the pointing task.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {52–59},
numpages = {8},
keywords = {Target Expansion, Visual Aid, Pointing},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909280,
author = {Guerra-Gomez, John A. and Boulanger, Cati and Kairam, Sanjay and Shamma, David A.},
title = {Identifying Best Practices for Visualizing Photo Statistics and Galleries Using Treemaps},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909280},
doi = {10.1145/2909132.2909280},
abstract = {Online photo collections are often presented along with numeric data, such as views, likes, or comments. Treemaps are ideal for visualizing such collections, as they present numeric values using rectangular blocks, within which photos can be presented. Despite abundant research showing that photo treemaps and similar space-filling approaches are useful and appealing for users, understanding of the ideal parameters for constructing these representations remains limited. To address this, we contribute a series of experiments targeted at identifying design parameters for building photo treemaps. Our first study explores the number of photos presented at each level in the treemap, finding that using fewer photos makes visualizations more searchable and preferable to users. A second study examines the tradeoff between sizing photos according to the numeric values and presenting them in a more familiar layout with fixed aspect ratios. These results inform a third study, in which a prototype was used to probe the preferences of active Flickr users for using treemaps for navigating photo collections.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {60–63},
numpages = {4},
keywords = {Information Visualization, Photo treemaps, Photo Visulizations, Treemaps},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909286,
author = {Wittenburg, Kent and Turchi, Tommaso},
title = {Treemaps and the Visual Comparison of Hierarchical Multi-Attribute Data},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909286},
doi = {10.1145/2909132.2909286},
abstract = {Treemaps have the desirable property of presenting overviews along with details of data and thus are of interest in visualizations of multi-attribute tabular data with attribute hierarchies. However, the original treemap algorithms and most subsequent variations are hampered in making parallel structures in a hierarchical data structure visually comparable. Structurally parallel elements are not aligned, making it difficult to compare them visually. We propose a method that allows for proportional and non-proportional subdivisions of subtrees while preserving visual alignment of parallel structures. We extend the framework so that other types of data visualizations can be placed within the graphical areas of a treemap to allow for the visual comparison of a broad collection of data types including temporal data.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {64–67},
numpages = {4},
keywords = {Treemaps, information visualization, hierarchical multi-attribute data, multidimensional data, visual comparison},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909269,
author = {Jin, Yucheng and Seipp, Karsten and Duval, Erik and Verbert, Katrien},
title = {Go With the Flow: Effects of Transparency and User Control on Targeted Advertising Using Flow Charts},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909269},
doi = {10.1145/2909132.2909269},
abstract = {Targeted advertising reaches users based on various traits, such as demographics or behaviour. However, users are often reluctant to accept ads. We hypothesise that users are more open to targeted advertising if they can inspect, control and thereby understand the process of ad selection. We conducted a between-subjects study (N=200) to investigate to what extent four key aspects of ads (Quality, Behavioural Intention, Understanding and Attitude) may be affected by transparency and user control using a flow chart. Our results indicate that positive effects of flow charts reported from other domains may also be applicable to advertising: Using flow charts to provide transparency together with user control is found to have more positive effects on domain-specific quality measures than established, text-based approaches and using either of the techniques in isolation. The paper concludes with recommendations for practitioners aiming to improve user response to ads.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {68–75},
numpages = {8},
keywords = {ow chart, transparency, user control, Targeted advertising},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909247,
author = {Le Goc, Mathieu and Dragicevic, Pierre and Huron, Samuel and Boy, Jeremy and Fekete, Jean-Daniel},
title = {A Better Grasp on Pictures Under Glass: Comparing Touch and Tangible Object Manipulation Using Physical Proxies},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909247},
doi = {10.1145/2909132.2909247},
abstract = {We introduce a novel method based on physical proxies for investigating fundamental differences between touch and tangible interfaces. This method uses physical chips to emulate the flat, non-graspable objects that make up touch interfaces, in a way that supports direct comparison with tangible interfaces. We ran an experiment to test the effect of object thickness on participants' behavior, performance and subjective experience in spatial rearrangement tasks. We found that for the tasks tested, thick objects are faster but less accurate to operate, and that their graspability is only used occasionally. We also found that coarse manipulation of multiple thin objects is error-prone, an issue that only thick objects may allow to alleviate.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {76–83},
numpages = {8},
keywords = {touch interfaces, Tangible user interfaces, physical proxies},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909274,
author = {Sadeghi, Javad and Perin, Charles and Flemisch, Tamara and Hancock, Mark and Carpendale, Sheelagh},
title = {Flexible Trees: Sketching Tree Layouts},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909274},
doi = {10.1145/2909132.2909274},
abstract = {We introduce Flexible Trees, a sketch-based layout adjustment technique. Although numerous tree layout algorithms exist, these algorithms are usually bound to fit within standard shapes such as rectangles, circles and triangles. In order to provide the possibility of interactively customizing a tree layout, we offer a free-form sketch-based interaction through which one can re-define the boundary constraints for the tree layouts by combining ray-line intersection and line segment intersection. Flexible Trees offer topology preserving adjustments; can be used with a variety of tree layouts; and offer a simple way of authoring tree layouts for infographic purposes.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {84–87},
numpages = {4},
keywords = {Visualization, interaction, trees, infographics, sketching, authoring},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909272,
author = {Patern\`{o}, F. and Schiavone, A. G. and Pitardi, P.},
title = {Timelines for Mobile Web Usability Evaluation},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909272},
doi = {10.1145/2909132.2909272},
abstract = {In the field of Web usability evaluation, one potential effective approach is the usage of logging tools for remote usability analysis (i.e. tools capable of tracking and recording the users' activities while they interact with a Web site), and then presenting the recorded data to usability experts in such a way to support detection of possible usability problems. In the design of such automated tools, in addition to the problems related to user behavior recording, another important issue is the choice of meaningful visual representations in order to support the usability expert analysis.In this paper we present a timeline-based system for interactive events visualization, which has been exploited in a proxy-based mobile Web usability evaluation tool. We discuss how such visualizations can be exploited in finding usability issues and the type of problems that can be detected through them. We show various ways to compare timelines related to users sessions with ideal timelines representing optimal behavior.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {88–91},
numpages = {4},
keywords = {Tool for mobile Web logging analysis, Remote usability evaluation, Timelines},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909245,
author = {Di Donato, Valentino and Patrignani, Maurizio and Squarcella, Claudio},
title = {NetFork: Mapping Time to Space in Network Visualization},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909245},
doi = {10.1145/2909132.2909245},
abstract = {Dynamic network visualization aims at representing the evolution of relational information in a readable, scalable, and effective way. A natural approach, called 'time-to-time mapping', consists of computing a representation of the network at each time step and animating the transition between subsequent time steps. However, recent literature recommends to represent time-related events by means of static graphic counterparts, realizing the so called 'time-to-space mapping'. This paradigm has been successfully applied to networks where nodes and edges are subject to a restricted set of events: appearances, disappearances, and attribute changes. In this paper we describe NetFork, a system that conveys the timings and the impact of path changes that occur in a routing network by suitable time-to-space metaphors, without relying on the time-to-time mapping adopted by the play-back interfaces of alternative network monitoring tools. A user study and a comparison with the state of the art show that users can leverage on high level static representations to quickly assess the quantity and quality of the path dynamics that took place in the network.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {92–99},
numpages = {8},
keywords = {Dynamic network visualization, time-to-space mapping, interdomain routing visualization},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909252,
author = {Alexander, Eric and Gleicher, Michael},
title = {Assessing Topic Representations for Gist-Forming},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909252},
doi = {10.1145/2909132.2909252},
abstract = {As topic modeling has grown in popularity, tools for visualizing the process have become increasingly common. Though these tools support a variety of different tasks, they generally have a view or module that conveys the contents of an individual topic. These views support the important task of gist-forming: helping the user build a cohesive overall sense of the topic's semantic content that can be generalized outside the specific subset of words that are shown. There are a number of factors that affect these views, including the visual encoding used, the number of topic words included, and the quality of the topics themselves. To our knowledge, there has been no formal evaluation comparing the ways in which these factors might change users' interpretations. In a series of crowdsourced experiments, we sought to compare features of visual topic representations in their suitability for gist-forming. We found that gist-forming ability is remarkably resistant to changes in visual representation, though it deteriorates with topics of lower quality.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {100–107},
numpages = {8},
keywords = {Topic model visualization, word clouds},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909281,
author = {Guerra-Gomez, John Alexis and Wilson, Aaron and Liu, Juan and Davies, Dan and Jarvis, Peter and Bier, Eric},
title = {Network Explorer: Design, Implementation, and Real World Deployment of a Large Network Visualization Tool},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909281},
doi = {10.1145/2909132.2909281},
abstract = {This paper describes the process of design, implementation, and real world deployment of a web-based network exploration tool called Network Explorer. We designed Network Explorer based on the expressed needs of our clients and later deployed it as part of a larger system for fraud detection in health care. Our implementation of Network Explorer provides visual interactive access to large-scale network data. As part of the Network Explorer tool we contribute a dynamic group-in-a-box implementation for laying out clusters, and a node navigator widget that aids in the exploration of large networks. We are also contributing two open source components of the Network Explorer for the community to reuse: an in-browser clustering library, and the dynamic group-in-a-box algorithm. We have evaluated the network explorer tool in multiple real-world environments including the fraud detection setting above.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {108–111},
numpages = {4},
keywords = {Industry Visualization Deployment, Network Visualization of Large Networks},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909267,
author = {Botros, Fadi and Perin, Charles and Aseniero, Bon Adriel and Carpendale, Sheelagh},
title = {Go and Grow: Mapping Personal Data to a Living Plant},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909267},
doi = {10.1145/2909132.2909267},
abstract = {Motivation is a key factor for introducing and maintaining healthy changes in behaviour. However, typical visualization methods (e.g., bar-, pie-, and line charts) hardly motivate individuals. We investigate how a plant---a living visualization---whose health relies on the plant owner's level of activity, can engage people in tracking and self-reflecting on their fitness data. To address this question, we designed, implemented, and studied Go &amp; Grow, a living plant that receives water proportionally to its owner's activity. Our six-week qualitative study with ten participants suggests that living visualizations have qualities that their digital counterparts do not have. This includes people feeling: emotionally connected to their plant; sentiments such as pride and guilt; and responsibility towards their plant. Based on this study, we introduce the Goal Motivation Model, a model considering the diversity of individuals, thus supporting and encouraging a diversity of strategies for accomplishing goals.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {112–119},
numpages = {8},
keywords = {Plant, Personal Visualization, Living Visualization, Fitness},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909270,
author = {Quintal, Filipe and Jorge, Clinton and Nisi, Valentina and Nunes, Nuno},
title = {Watt-I-See: A Tangible Visualization of Energy},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909270},
doi = {10.1145/2909132.2909270},
abstract = {This paper describes a tangible visualization that explores the link between the impact of energy feedback on household consumers and the resource demand impact on energy production. Specifically, it positions a novel perspective attempting to move beyond the known limitations of current eco-feedback systems and contributes to enhance our understanding of how consumers comprehend energy production. The work is informed by a comprehensive study of an installation that displays the ratio of current power generation sources and the percentage of grid renewables. The paper provides design insights for creating novel eco-feedback visualizations that leverage the balance between user lifestyles and the desire to influence consumption behaviors and practices. Evaluation results show an increase in energy literacy and awareness as well as identifies high consumer preferences towards simple, representative interfaces and ubiquitous immediate feedback. Our study shows potential in terms of future scenarios for eco-feedback in distributed energy micro-generation and other inevitable disruptive changes for the energy utility.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {120–127},
numpages = {8},
keywords = {Sustainability, Visualization, Public Installations, User Study, Eco-feedback},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909263,
author = {Katsuragawa, Keiko and Pietroszek, Krzysztof and Wallace, James R. and Lank, Edward},
title = {Watchpoint: Freehand Pointing with a Smartwatch in a Ubiquitous Display Environment},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909263},
doi = {10.1145/2909132.2909263},
abstract = {We describe the design and evaluation of a freehand, smartwatch-based, mid-air pointing and clicking interaction technique, called Watchpoint. Watchpoint enables a user to point at a target on a nearby large display by moving their arm. It also enables target selection through a wrist rotation gesture. We validate the use of Watchpoint by comparing its performance with two existing techniques: Myopoint, which uses a specialized forearm mounted motion sensor, and a camera-based (Vicon) motion capture system. We show that Watchpoint is statistically comparable in speed and error rate to both systems and, in fact, outperforms in terms of error rate for small (high Fitts's ID) targets. Our work demonstrates that a commodity smartwatch can serve as an effective pointing device in ubiquitous display environments.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {128–135},
numpages = {8},
keywords = {wearable, large displays, smartwatch, pointing},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909262,
author = {Alborno, Paolo and Piana, Stefano and Mancini, Maurizio and Niewiadomski, Radoslaw and Volpe, Gualtiero and Camurri, Antonio},
title = {Analysis of Intrapersonal Synchronization in Full-Body Movements Displaying Different Expressive Qualities},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909262},
doi = {10.1145/2909132.2909262},
abstract = {Intrapersonal synchronization of limb movements is a relevant feature for assessing coordination of motoric behavior. In this paper, we show that it can also distinguish between full-body movements performed with different expressive qualities, namely rigidity, fluidity, and impulsivity. For this purpose, we collected a dataset of movements performed by professional dancers, and annotated the perceived movement qualities with the help of a group of experts in expressive movement analysis. We computed intra personal synchronization by applying the Event Synchronization algorithm to the time-series of the speed of arms and hands. Results show that movements performed with different qualities display a significantly different amount of intra personal synchronization: impulsive movements are the most synchronized, the fluid ones show the lowest values of synchronization, and the rigid ones lay in between.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {136–143},
numpages = {8},
keywords = {Dance, Movement, Intrapersonal synchronization, Analysis, Event Synchronization},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909258,
author = {von Zadow, Ulrich and Reipschl\"{a}ger, Patrick and B\"{o}sel, Daniel and Sellent, Anita and Dachselt, Raimund},
title = {YouTouch! Low-Cost User Identification at an Interactive Display Wall},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909258},
doi = {10.1145/2909132.2909258},
abstract = {We present YouTouch!, a system that tracks users in front of an interactive display wall and associates touches with users. With their large size, display walls are inherently suitable for multi-user interaction. However, current touch recognition technology does not distinguish between users, making it hard to provide personalized user interfaces or access to private data. In our system we place a commodity RGB + depth camera in front of the wall, allowing us to track users and correlate them with touch events. While the camera's driver is able to track people, it loses the user's ID whenever she is occluded or leaves the scene. In these cases, we re-identify the person by means of a descriptor comprised of color histograms of body parts and skeleton-based biometric measurements. Additional processing reliably handles short-term occlusion as well as assignment of touches to occluded users. YouTouch! requires no user instrumentation nor custom hardware, and there is no registration nor learning phase. Our system was thoroughly tested with data sets comprising 81 people, demonstrating its ability to re-identify users and correlate them to touches even under adverse conditions.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {144–151},
numpages = {8},
keywords = {multitouch, user identification, multi-user interaction, RGBD sensor, re-identification, interactive surface, Display wall},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909260,
author = {Delamare, William and Janssoone, Thomas and Coutrix, C\'{e}line and Nigay, Laurence},
title = {Designing 3D Gesture Guidance: Visual Feedback and Feedforward Design Options},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909260},
doi = {10.1145/2909132.2909260},
abstract = {Dynamic symbolic in-air hand gestures are an increasingly popular means of interaction with smart environments. However, novices need to know what commands are available and which gesture to execute in order to trigger these commands. We propose to adapt OctoPocus, a 2D gesture guiding system, to the case of 3D. The OctoPocus3D guidance system displays a set of 3D gestures as 3D pipes and allows users to understand how the system processes gesture input. Several feedback and feedforward visual alternatives are proposed in the literature. However, their impact on guidance remains to be evaluated. We report the results of two user experiments that aim at designing OctoPocus3D by exploring these alternatives. The results show that a concurrent feedback, which visually simplifies the 3D scene during the execution of the gesture, increases the recognition rate, but only during the first two repetitions. After the first two repetitions, users achieve the same recognition rate with a terminal feedback (after the execution of the gesture), a concurrent feedback, both or neither. With respect to feedforward, the overall stability of the 3D scene explored through the origin of the pipes during the execution of the gestures does not influence the recognition rate or the execution time. Finally, the results also show that displaying upcoming portions of the gestures allows 8% faster completion times than displaying the complete remaining portions. This indicates that preventing visual clutter of the 3D scene prevails over gesture anticipation.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {152–159},
numpages = {8},
keywords = {3D hand gesture, Feedback, Feedforward, Guidance},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909268,
author = {Fiorentino, Michele and Radkowski, Rafael and Boccaccio, Antonio and Uva, Antonio Emmanuele},
title = {Magic Mirror Interface for Augmented Reality Maintenance: An Automotive Case Study},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909268},
doi = {10.1145/2909132.2909268},
abstract = {We present a novel interaction method for augmented industrial maintenance based on a "magic mirror" interface and virtual motion buttons. The system includes a video camera for object tracking, a videodepth camera for capturing user gestures, a projector for displaying technical instruction to the operator and a LCD monitor providing feedback of the virtual buttons. The operator can trigger maintenance commands by directional swift of the hands in regions sensitive to motion speed and direction. The main advantage of the presented interface is that it can work in realistic industrial conditions: (i) operators wearing gloves, (ii) operators handling tools, (iii) presence of moving machinery and personnel in the background. We measured the performances of the system with a laboratory test and we proved the feasibility with an automotive inspection test case. We calculated an average interaction time below 2 seconds and an error rate lower than 5%. However, we found some performances limitations if the operator is handling tools.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {160–167},
numpages = {8},
keywords = {Computer assisted maintenance, human computer interfaces, augmented reality, motion buttons},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909251,
author = {Simeone, Adalberto L. and Bulling, Andreas and Alexander, Jason and Gellersen, Hans},
title = {Three-Point Interaction: Combining Bi-Manual Direct Touch with Gaze},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909251},
doi = {10.1145/2909132.2909251},
abstract = {The benefits of two-point interaction for tasks that require users to simultaneously manipulate multiple entities or dimensions are widely known. Two-point interaction has become common, e.g., when zooming or pinching using two fingers on a smartphone. We propose a novel interaction technique that implements three-point interaction by augmenting two-finger direct touch with gaze as a third input channel. We evaluate two key characteristics of our technique in two multi-participant user studies. In the first, we used the technique for object selection. In the second, we evaluate it in a 3D matching task that requires simultaneous continuous input from fingers and the eyes. Our results show that in both cases participants learned to interact with three input channels without cognitive or mental overload. Participants' performance tended towards fast selection times in the first study and exhibited parallel interaction in the second. These results are promising and show that there is scope for additional input channels beyond two-point interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {168–175},
numpages = {8},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909261,
author = {Trullemans, Sandra and Sanctorum, Audrey and Signer, Beat},
title = {PimVis: Exploring and Re-Finding Documents in Cross-Media Information Spaces},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909261},
doi = {10.1145/2909132.2909261},
abstract = {Over the last decade, we have witnessed an emergence of Personal Information Management (PIM) solutions. Despite the fact that paper documents still form a significant part of our daily working activities, existing PIM systems usually support the organisation and re-finding of digital documents only. While physical document tracking solutions such as RFID- or computer vision-based systems are recently gaining some attention, they usually focus on the paper document tracking and offer limited support for re-finding activities. We present PimVis, a solution for exploring and re-finding digital and paper documents in so-called cross-media information spaces. The PimVis user interface enables a unified organisation of digital and paper documents through the creation of bidirectional links between the digital and physical information space. The presented personal cross-media information management solution further supports the extension with alternative document tracking techniques as well as augmented reality solutions. A formative PimVis evaluation revealed the high potential of fully integrated cross-media PIM solutions.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {176–183},
numpages = {8},
keywords = {document tracking, paper-digital interface, Personal information management, cross-media information spaces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909264,
author = {Francese, Rita and Risi, Michele and Scanniello, Giuseppe and Tortora, Genoveffa},
title = {LifeBook: A Mobile Personal Information Management System on the Cloud},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909264},
doi = {10.1145/2909132.2909264},
abstract = {In this paper, we present LifeBook, a Personal Information Management (PIM) system that handles information on events captured by all the user's devices. Our PIM retrieves events on the basis of both the user's context and event similarity, which is computed by exploiting an information retrieval technique. We aggregated together the similarity of content, location, time, and event type to relate and surf the events. To this aim, we propose a re-find interface enabling the user to search and visualize information already seen before, of which he remembers some context aspects, such as time and/or place. The events captured on different devices are stored on the cloud without user intervention. A preliminary quantitative and qualitative evaluation has been also conducted to assess the effectiveness of LifeBook. Results in terms of time, effort and relevance of the information provided suggest that LifeBook be a viable means to retrieve personal information. Participants in the empirical investigation also considered the tool appropriate for supporting information re-finding tasks.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {184–191},
numpages = {8},
keywords = {Mobile Application, Event Management, Personal Information Management System, Graphical Search Interface},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909271,
author = {Dix, Alan and Cowgill, Rachel and Bashford, Christina and McVeigh, Simon and Ridgewell, Rupert},
title = {Spreadsheets as User Interfaces},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909271},
doi = {10.1145/2909132.2909271},
abstract = {Spreadsheets are ubiquitous, familiar, often overlooked, and embody vast financial and human investment, not least in their user interface. This paper shows how spreadsheets can be used as an integral part of interactive processes, for activities from simple data entry, to more complex grouping and linking of datasets, both as fully functional prototypes and as part of a final system. They reveal artful digital and physical end-user appropriation; exemplify key design principles including 'appropriate intelligence', ensuring 'smart' technology fits the complete human--computer process; and expose further design issues such as the importance of 'exception' sets.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {192–195},
numpages = {4},
keywords = {appropriation, Spreadsheets, musicology, digital humanities},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909256,
author = {Garzotto, Franca and Gelsomini, Mirko and Clasadonte, Francesco and Montesano, Daniele and Occhiuto, Daniele},
title = {Wearable Immersive Storytelling for Disabled Children},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909256},
doi = {10.1145/2909132.2909256},
abstract = {Our research aims at supporting existing therapies for children with intellectual and developmental disabilities (IDD) and with autism spectrum disorders (ASD). The personal and social autonomy is the desired end state to be achieved to enable a smooth integration in the real world. We developed and tested a framework for storytelling and learning activities that exploits an immersive virtual reality viewer to interact with target users. Our system uses Google Cardboard platform to enhance existing therapies for IDD and ASD children, enabling caregivers to supervise and personalize single therapeutic sessions. This way curative meetings can be adapted for each child's specific need accordingly to the severity of their disabilities. We co-designed our system with experts from the medical sector, identifying features that allow patients to stay focused on the task to perform. Our approach triggers a learning process for a seamless assimilation of common behavioral skills useful in every day's life. This paper highlights the technologic challenges in healthcare and discusses cutting-edge interaction paradigms. Among those challenges, we try to identify the best solution to support advanced visual interfaces for an interactive storytelling experience. Furthermore, this work reports our preliminary experimental results from a still ongoing evaluation with IDD and ASD children and discusses the benefits and flaws of our approach. On the one hand, we explore children reaction to - and acceptance of - the viewer, on the other hand, therapists' ease of use when interacting with our framework. We conclude this paper with few considerations on our approach.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {196–203},
numpages = {8},
keywords = {virtual reality, Children, intellectual and developmental disorders, autism, touchless interaction},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909249,
author = {De Prisco, Roberto and Malandrino, Delfina and Zaccagnino, Gianluca and Zaccagnino, Rocco},
title = {Natural User Interfaces to Support and Enhance Real-Time Music Performance},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909249},
doi = {10.1145/2909132.2909249},
abstract = {Today's technology is redefining the way individuals can work, communicate, share experiences, constructively debate, and actively participate to any aspect of the daily life, ranging from business to education, from political and intellectual to social, and so on. Enabling access to technology by any individual, reducing obstacles, avoiding discrimination, and making the overall experience easier and enjoyable is an important objective of both research and industry.Exploiting natural user interfaces, initially conceived for the game market, it is possible to enhance the traditional modalities of interaction when accessing to technology, build new forms of interactions by transporting users in a virtual dimension, but that fully reflects the reality, and finally, improve the overall perceived experience. The increasing popularity of these innovative interfaces involved their adoption in other fields, including Computer Music.This paper presents MarcoSmiles, a system designed to allow individuals to perform music in a easy, innovative, and personalized way. The idea is to design new interaction modalities during music performances by using hands without the support of a real musical instrument. We exploited Artificial Neural Networks to customize the virtual musical instrument, to provide the information for the mapping of the hands configurations into musical notes and, finally, to train and test these configurations.We studied the behavior of the system and its efficacy in terms of learning capabilities. We also report results about a preliminary evaluation study aimed at analyze general users' perceptions about the system and their overall satisfaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {204–211},
numpages = {8},
keywords = {Natural User Interfaces, Music Performance, Evaluation, Neural Networks},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909287,
author = {Tuveri, Elena and Macis, Luca and Sorrentino, Fabio and Spano, Lucio Davide and Scateni, Riccardo},
title = {Fitmersive Games: Fitness Gamification through Immersive VR},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909287},
doi = {10.1145/2909132.2909287},
abstract = {The decreasing hardware cost makes it affordable to pair Immersive Virtual Environments (IVR) visors with treadmills and exercise bikes. In this paper, we discuss the application of different gamification techniques in IVR for supporting physical exercise. We describe both the hardware setting and the design of Rift-a-bike, a cycling fitmersive game (immersive games for fitness). We evaluate the effectiveness of such techniques through a user study, which provides different insights on their effectiveness in designing such applications.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {212–215},
numpages = {4},
keywords = {Virtual Reality, Fitness, Immersion, Gamification},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909279,
author = {Tudoreanu, M. Eduard},
title = {Interface for Augmenting Spatial Orientation of Pilots via Low Cognitive Load, Peripheral Vision},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909279},
doi = {10.1145/2909132.2909279},
abstract = {Untrained pilots flying in reduced visibility are at a high risk of losing control of a small aircraft when using typical instrumentation. The reduced visibility makes it impossible for the pilot to see outside references, which increases the workload and cognitive stress on the pilot. An advanced visual interface that focuses on the use of non-central vision to allow pilots to feel rather than compute the basic spatial orientation of their small aircraft is presented in this paper. To promote our goal of reducing the pilot's cognitive load, the interface is controlled by a single input button in addition to the routine controls of the aircraft. The visual display aims to provide an immersive experience to the user, while not relying on the pilot's central focus of attention. Thus, the user can fully exploit and focus their attention on any available instrumentation and technology present in the cockpit, such as glass displays, synthetic or enhanced vision, GPS devices, with our interface offering an additional layer of awareness. The visualization provides both instantaneous and trend information to the user. A preliminary user study that revealed additional future improvements to the visual display is also briefly described.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {216–219},
numpages = {4},
keywords = {situational awareness, immersive interfaces, peripheral vision, aircraft cockpit displays},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909273,
author = {Katsuragawa, Keiko and Wallace, James R. and Lank, Edward},
title = {Gestural Text Input Using a Smartwatch},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909273},
doi = {10.1145/2909132.2909273},
abstract = {One challenge with modern smartwatches is text input. In this paper we explore the use of gestural interaction with a smartwatch to support text input. The inertial measurement unit of a smartwatch is used to capture gestural interaction by a user, and an external display is used to provide feedback. We examine two specific variants of gesture keyboards: the swype keyboard common on modern smartphones and the cirrin keyboard, a gestural keyboard that supports character input via directional gestures. We show, first, that freearm gestural input as sensed by a smartwatch exhibits similar efficiency as freearm gestural input sensed by motion capture systems. As well, we show that the smartwatch inertial measurement unit can support text input on ubiquitous computing displays.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {220–223},
numpages = {4},
keywords = {Text Input, Gestures, Ubiquitous Computing, Smartwatch},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909283,
author = {Garzotto, Franca and Gelsomini, Mirko and Pappalardo, Alessandro and Sanna, Claudio and Stella, Erica and Zanella, Michele},
title = {Monitoring and Adaptation in Smart Spaces for Disabled Children},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909283},
doi = {10.1145/2909132.2909283},
abstract = {Our research explores new forms of technology-enhanced intervention for children with Intellectual Disability (ID) that exploits our Magic K-Room, an interactive multisensory smart space designed in cooperation with therapists and special educators at a local rehabilitation center. The Magic K-Room integrates smart objects, smart lights, and (immersive) multimedia contents. It also supports monitoring of brain-signals and automatic ambient adaptation, to dynamically personalize the smart space to the specific needs of each child. These features are enabled by a wearable device that automatically detects and interprets EEG signals.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {224–227},
numpages = {4},
keywords = {Smart spaces, Children, EEG headset, Brain-computer interface, Intellectual Disability},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909257,
author = {Palleis, Henri and Wagner, Julie and Hussmann, Heinrich},
title = {Novel Indirect Touch Input Techniques Applied to Finger-Forming 3D Models},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909257},
doi = {10.1145/2909132.2909257},
abstract = {We address novel two-handed interaction techniques in dual display interactive workspaces combining direct and indirect touch input. In particular, we introduce the notion of a horizontal tool space with task-dependent graphical input areas. These input areas are designed as single purpose control elements for specific functions and allow users to manipulate objects displayed on a vertical screen using simple one- and two-finger touch gestures and both hands. For demonstrating this concept, we use 3D modeling tasks as a specific application area. Initial feedback of six expert users indicates that our techniques are easy to use and stimulate exploration rather than precise modeling. Further, we gathered qualitative feedback during a multi-session observational study with five novices who learned to use our tool and were interviewed several times. Preliminary results indicate that working with our setup is easy to learn and remember. Participants liked the partitioning character of the dual-surface setup and agreed on the benefiting quality of touch input, giving them a 'hands-on feeling'.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {228–235},
numpages = {8},
keywords = {qualitative data, two-handed input, polygon modeling, perspective-dependent gestures, indirect touch},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909277,
author = {Meng, Xiaojun and Zhao, Shengdong and Edge, Darren},
title = {HyNote: Integrated Concept Mapping and Notetaking},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909277},
doi = {10.1145/2909132.2909277},
abstract = {Notes can be taken in a linear or nonlinear way. Previous work suggests that nonlinear note taking is advantageous in terms of sense-making and long-term recall. However, previous studies also reveal that the combination of divided attention and time pressure make realtime notetaking a challenge. In this paper, we propose a new hybrid workflow inheriting advantages from both linear and nonlinear notetaking approaches. Our resulting HyNote (Hybrid Notetaking) system uses statistical parsing of linear raw notes to facilitate concepts mapping, allowing users to smoothly switch between linear and nonlinear approaches with low effort and time costs. Results from our preliminary study of HyNote show that users can easily map concepts in realtime and achieve superior understanding of lecture contents in a video learning task compared with using the traditional linear Notepad application.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {236–239},
numpages = {4},
keywords = {Mobile interaction, Concept Mapping, Notetaking},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909275,
author = {Ginon, Blandine and Stumpf, Simone and Jean-Daubias, St\'{e}phanie},
title = {Towards the Right Assistance at the Right Time for Using Complex Interfaces},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909275},
doi = {10.1145/2909132.2909275},
abstract = {Many users struggle when they have to use complex interfaces to complete everyday computing tasks. Offering intelligent, proactive assistance is becoming commonplace yet determining the right time to provide help is still difficult. We conducted an empirical study that aimed to uncover what user factors influenced following advice. Our results describe a user's background and expectations that appear to play a role in heeding assistance. Our work is a step towards understanding how to provide the right assistance at the right time and build proactive assistance systems that are personalized for individual users.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {240–243},
numpages = {4},
keywords = {activity monitoring, trace-based systems, proactive assistance, User assistance, predictive model},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909284,
author = {Butscher, Simon and Reiterer, Harald},
title = {Applying Guidelines for the Design of Distortions on Focus+Context Interfaces},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909284},
doi = {10.1145/2909132.2909284},
abstract = {Distortion-based visualization techniques allow users to examine focused regions of a multiscale space at high scales but preserve their contextual information. However, the distortion can come at the coast of confusion, disorientation and impairment of the users' spatial memory. Yet, how distortions influence users' ability to build up spatial memory, while taking into account human skills of perception, interpretation and comprehension, remains underexplored. This note reports findings of an experimental comparison between a distortion-based focus+context interface and an undistorted overview+detail interface. The focus+context technique follows guidelines for the design of comprehensible distortions: make use of real-world metaphors, visual clues like shading, smooth transitions and scaled-only focus regions. The results show that the focus+context technique designed following these guidelines help to keep track of the position within the multiscale space and does not impair users' spatial memory.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {244–247},
numpages = {4},
keywords = {multiscale space, overview+detail, focus+context, Spatial memory, distortion, design guidelines},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909278,
author = {\'{E}vain, And\'{e}ol and Argelaguet, Ferran and Strock, Anthony and Roussel, Nicolas and Casiez, G\'{e}ry and L\'{e}cuyer, Anatole},
title = {Influence of Error Rate on Frustration of BCI Users},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909278},
doi = {10.1145/2909132.2909278},
abstract = {Brain-Computer Interfaces (BCIs) are still much less reliable than other input devices. The error rates of BCIs range from 5% up to 60%. In this paper, we assess the subjective frustration, motivation, and fatigue of BCI users, when confronted to different levels of error rate. We conducted a BCI experiment in which the error rate was artificially controlled. Our results first show that a prolonged use of BCI significantly increases the perceived fatigue, and induces a drop in motivation. We also found that user frustration increases with the error rate of the system but this increase does not seem critical for small differences of error rate. Thus, for future BCIs, we would advise to favor user comfort over accuracy when the potential gain of accuracy remains small.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {248–251},
numpages = {4},
keywords = {SSVEP, frustration, motivation, fake feedback, fatigue, BCI},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909276,
author = {Feuerstack, Sebastian and Wortelen, Bertram},
title = {AM-DCT: A Visual Attention Modeling Data Capturing Tool for Investigating Users' Interface Monitoring Behavior},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909276},
doi = {10.1145/2909132.2909276},
abstract = {Methods to get insights about users' monitoring behavior either depend on the expertise of Human Factor experts to model and predict stereotypic monitoring behavior or on performing eye tracking studies in simulated environments, which require subjects to be physically present and usually to be tested successively. AM-DCT is a tool that can be applied by domain experts without expertise in human factors and with limited training in parallel sessions to learn about a population's monitoring behavior. In an experiment 20 car drivers used the AM-DCT independently after watching a 15 minutes video tutorial. 19 subjects were able to model their monitoring behavior for a car overtaking scenario in 36 minutes on average. The identification of areas of interest for areas with clearly defined borders was very consistent among subjects. For those without clear borders an aggregated model of all participants seems surprisingly accurate to represent the real monitoring area.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {252–255},
numpages = {4},
keywords = {Monitoring, Visual Attention, Human Factors},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909266,
author = {Gupta, Shrey and McGuffin, Michael J.},
title = {Multitouch Radial Menu Integrating Command Selection and Control of Arguments with up to 4 Degrees of Freedom},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909266},
doi = {10.1145/2909132.2909266},
abstract = {We design and evaluate a multitouch radial menu for large screens with two desirable properties. First, it allows a single gesture to select a command and then continuously control arguments for that command with unbroken kinesthetic tension. Second, arguments are controlled with 1 or 2 fingers for up to 4 degrees of freedom (DoF). For example, the user may select one command for 4 DoF direct manipulation (translation + scaling + rotation), or another command for 3 DoF camera operations (pan + zoom), using the same two-finger pinch gesture, but with different initial orientations of the gesture to disambiguate. We present a taxonomy to classify previous menuing techniques sharing the first property, and discuss how very few techniques have both of these properties. Our work also extends previous work by Banovic et al. in the following ways: our menu supports submenus and a fast default command, and we experimentally evaluate the effect of varying the number of rings in the menu, the symmetry of the menu, and the use of one hand vs. two hands vs. a stylus and hand.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {256–263},
numpages = {8},
keywords = {direct manipulation, popup menu, pen, stylus, multitouch},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909250,
author = {Albo, Yael and Lanir, Joel and Bak, Peter and Rafaeli, Sheizaf},
title = {Static vs. Dynamic Time Mapping in Radial Composite Indicator Visualization},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909250},
doi = {10.1145/2909132.2909250},
abstract = {Composite Indicators (CIs), are a common measurement and benchmarking tool that are used to reflect and measure multidimensional concepts such as digital divides, individual's well-being and more. Measurement iterations produce a series of time-oriented data, which stakeholders as well as the general public might be interested to interpret. Visualization of a CI is highly recommended in order to ease interpretation, and many CI websites use radial solutions to visualize CIs. Yet it is unclear how to visualize the temporal dynamics in radial diagrams. Static solutions, mapping time to small multiples might be challenging due to screen space issues. Dynamic solutions are appealing, yet, there is no clear empirical evidence on benefits of dynamic time coding in radial diagrams. In this paper, we compare static vs. dynamic time mapping using two radial CI visualization methods. The popular Radar chart technique is compared to the innovative Flower chart as used in the well-known OECD Better Life index. We compare users' performance and preferences empirically under formal task taxonomy, adjusted to CI tasks. Results indicate that in general, static time encoding was more effective than dynamic encoding. Still, an in depth analysis showed that the dynamic approach is a feasible and sometimes even better solution for important CIs tasks, leveraged by the fact that users seem to like and enjoy it.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {264–271},
numpages = {8},
keywords = {evaluation, Animation, information visualization, small multiples, composite indicators},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909246,
author = {Pienta, Robert and Tamersoy, Acar and Endert, Alex and Navathe, Shamkant and Tong, Hanghang and Chau, Duen Horng},
title = {VISAGE: Interactive Visual Graph Querying},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909246},
doi = {10.1145/2909132.2909246},
abstract = {Extracting useful patterns from large network datasets has become a fundamental challenge in many domains. We present Visage, an interactive visual graph querying approach that empowers users to construct expressive queries, without writing complex code (e.g., finding money laundering rings of bankers and business owners). Our contributions are as follows: (1) we introduce graph autocomplete, an interactive approach that guides users to construct and refine queries, preventing over-specification; (2) Visage guides the construction of graph queries using a data-driven approach, enabling users to specify queries with varying levels of specificity, from concrete and detailed (e.g., query by example), to abstract (e.g., with "wildcard" nodes of any types), to purely structural matching; (3) a twelve-participant, within-subject user study demonstrates Visage's ease of use and the ability to construct graph queries significantly faster than using a conventional query language; (4) Visage works on real graphs with over 468K edges, achieving sub-second response times for common queries.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {272–279},
numpages = {8},
keywords = {Graph Querying and Mining, Visualization, Interaction Design},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2909255,
author = {Chevalier, Fanny and Riche, Nathalie Henry and Plaisant, Catherine and Chalbi, Amira and Hurter, Christophe},
title = {Animations 25 Years Later: New Roles and Opportunities},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2909255},
doi = {10.1145/2909132.2909255},
abstract = {Animations are commonplace in today's user interfaces. From bouncing icons that catch attention, to transitions helping with orientation, to tutorials, animations can serve numerous purposes. We revisit Baecker and Small's pioneering work Animation at the Interface, 25 years later. We reviewed academic publications and commercial systems, and interviewed 20 professionals of various backgrounds. Our insights led to an expanded set of roles played by animation in interfaces today for keeping in context, teaching, improving user experience, data encoding and visual discourse. We illustrate each role with examples from practice and research, discuss evaluation methods and point to opportunities for future research. This expanded description of roles aims at inspiring the HCI research community to find novel uses of animation, guide them towards evaluation and spark further research.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {280–287},
numpages = {8},
keywords = {roles, transition, Animation, taxonomy},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926066,
author = {AlTarawneh, Ragaad and Humayoun, Shah Rukh},
title = {Visualizing Software Structures through Enhanced Interactive Sunburst Layout},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926066},
doi = {10.1145/2909132.2926066},
abstract = {Visualizing large software system structure in compact representations would help software architects and analysts in understanding the overall software structure accurately and efficiently. Space-filling techniques (e.g., Sunburst or Tree-map) are nowadays used for producing compact representations of large hierarchical data. In this paper, we use the Sunburst layout with some enhancements to show the overall software system structure and the inside details in a compact visual form, in order to make it more readable for software architects and analysts. We also report some initial findings of the preliminary conducted evaluation study.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {288–289},
numpages = {2},
keywords = {Large Tree Hierarchies, Information Visualization, Sunburst, Software Visualization},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926082,
author = {Ardito, Carmelo and Desolda, Giuseppe and Matera, Maristella and Costabile, Maria F.},
title = {Exploiting Visual Notations for Data Exploration: The EFESTO Platform},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926082},
doi = {10.1145/2909132.2926082},
abstract = {The EFESTO platform allows the creation of interactive workspaces supporting end users in the exploration and seamless composition of heterogeneous data sources. By means of a visual paradigm implemented within a Web composition environment, the end users dynamically create "live" mashups where relevant information, extracted from different types of data sources including the Linked Open Data, and functions that can be performed on it can be flexibly shaped-up at runtime.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {290–291},
numpages = {2},
keywords = {Mashups, Web Composition Environments, Data Integration},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926059,
author = {Barricelli, Barbara Rita and Valtolina, Stefano},
title = {End-User Development for Lifelogging and EWellness},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926059},
doi = {10.1145/2909132.2926059},
abstract = {Thanks to pervasiveness and mobility of wearable Internet of Things tools, we are witnessing the emergence of a new research and development domain: the eWellness. Together with this, new challenges arise, and ask for effective methods and techniques to improve the way extraction, merge, analysis, visualization, and data sharing are currently managed. In this paper, we present our research on End-User Development for the eWellness domain and in particular the design and development of the SmartFit framework.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {292–293},
numpages = {2},
keywords = {sociotechnical design, end-user development, lifelogging, Internet of Things, eWellness},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926071,
author = {Bianchini, Devis and Fogli, Daniela and Ragazzi, Davide},
title = {TAB Sharing: A Gamified Tool for e-Participation},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926071},
doi = {10.1145/2909132.2926071},
abstract = {This paper presents TAB Sharing, a mobile application aimed at empowering citizens in creating and sharing proposals with other citizens and public administration (PA). Proposals are conceived as solutions to problems occurring in a community or initiatives that should be pursued. Gamification elements have been included in TAB Sharing to sustain citizens' participation and encourage them in providing as much details as possible in their proposals to better support PA decision-making.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {294–295},
numpages = {2},
keywords = {e-participation, gamification, empowerment},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926069,
author = {Cabral, Diogo and Silva, Jo\~{a}o M. F. and Fernandes, Carla and Correia, Nuno},
title = {Annotating Live Video with Tablet Computers: A Preliminary User Study},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926069},
doi = {10.1145/2909132.2926069},
abstract = {Tablet computers provide a natural support for digital annotation associated to any type of media, including live video. However, annotations of live video require real-time motion tracking, maintaining the association between annotations and moving objects, as well as interfaces that facilitate the annotation task of a live event. This work presents the initial users' feedback on a video annotator that includes two real-time trackers, Kinect and TLD, and on two annotation methods that aim to help annotating live video, "Hold and Overlay" and "Hold and Speed Up".},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {296–297},
numpages = {2},
keywords = {Tablet PCs, Preliminary User Study, Real-Time Motion Tracking, Pen-based Video Annotations, Real-time Video Annotations},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926075,
author = {Catala, L\'{e}onor Ferrer and Favetta, Franck and Cunty, Claire and Berjawi, Bilal and Duchateau, Fabien and Miquel, Maryvonne and Laurini, Robert},
title = {Visualizing Integration Uncertainty Enhances User's Choice in Multi-Providers Integrated Maps},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926075},
doi = {10.1145/2909132.2926075},
abstract = {This poster presents an experiment to assess how representation of uncertainty of cartographic integration of multi-providers services is used by end-users1.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {298–299},
numpages = {2},
keywords = {Interface for e-Tourism, Geographic Information Systems, Uncertainty Visualization, Spatial Integration, Spatial Entity Matching},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926065,
author = {Catarci, Tiziana and Leotta, Francesco and Marrella, Andrea and Mecella, Massimo and Sora, Daniele and Cottone, Pietro and Lo Re, Giuseppe and Morana, Marco and Ortolani, Marco and Agate, Vincenzo and Meschino, Giovanni Renato and Pecoraro, Giovanni and Pergola, Gabriele},
title = {Your Friends Mention It. What About Visiting It? A Mobile Social-Based Sightseeing Application},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926065},
doi = {10.1145/2909132.2926065},
abstract = {In this short poster paper, we present an application for suggesting attractions to be visited by users, based on social signal processing techniques.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {300–301},
numpages = {2},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926077,
author = {Celentano, Augusto and Collarile, Luigi},
title = {Visual Synchronization of Music Notation in Ancient Choir-Books},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926077},
doi = {10.1145/2909132.2926077},
abstract = {This paper explores some techniques for visually synchronizing the music notation in ancient choir-books in which, differently from modern music scores, the different voices are independently placed in the four quadrants of an open volume large page. We propose three visual techniques to show the synchronization between the different voices in comparison with a modern music notation.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {302–303},
numpages = {2},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926080,
author = {De Carolis, Berardina and Palestra, Giuseppe},
title = {Gaze-Based Interaction with a Shop Window},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926080},
doi = {10.1145/2909132.2926080},
abstract = {This paper describes the first prototype of a gaze-based system designed for providing interactively information about dresses shown, on physical mannequins, in a shop window. Using the system the user may look at available sizes, colors, price and similar products. Due to the nature of such a system, the interaction must be touchless and natural. The developed solution uses Microsoft Kinect 2 as a device to achieve a natural gaze-based pointing approach. Results show that users evaluate positively the approach and felt positively engaged during the interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {304–305},
numpages = {2},
keywords = {Natural User Interfaces, Information Interfaces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926057,
author = {Felberbaum, Yasmin and Lanir, Joel},
title = {Step by Step: Investigating Foot Gesture Interaction},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926057},
doi = {10.1145/2909132.2926057},
abstract = {A promising new way of interacting with computing devices is by using our feet. Foot interaction has the potential of being an intuitive, easy to use and enjoyable way of interaction. However, there are very few guidelines for using foot interaction, or specifically foot gestures. In this research we conduct a user elicitation study for foot interaction on a horizontal surface to produce user-defined gesture sets for actions taken from two domains -- typical GUI actions and avatar controls. We analyze how foot gestures differentiate from hand gestures, point out foot gesture properties and discuss general observations.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {306–307},
numpages = {2},
keywords = {gestures, user-defined gesture set, Foot Interaction, Guessability study},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926083,
author = {Fogli, Daniela and Lanzilotti, Rosa and Piccinno, Antonio and Tosi, Paolo},
title = {AmI@Home: A Game-Based Collaborative System for Smart Home Configuration},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926083},
doi = {10.1145/2909132.2926083},
abstract = {This paper describes AmI@Home, a collaborative system prototype for smart home management and configuration. In particular, the system is based on event-condition-action rules. Rule construction and manipulation occur through gamification mechanisms supporting social interaction, collaboration and competition, in order to engage all family members in shaping their smart home.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {308–309},
numpages = {2},
keywords = {smart home, rule-based system, gamification},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926073,
author = {Christiernin, Linn Gustavsson and Augustsson, Svante},
title = {Interacting with Industrial Robots: A Motion-Based Interface},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926073},
doi = {10.1145/2909132.2926073},
abstract = {Collaborative industrial robot cells are becoming more and more interesting for industry through the new Industrie 4.0 initiative. In this paper we report early work on motion-based interaction with industrial robots. Human motion is tracked by a Kinect camera and translated into robot code. A group of tests subjects are asked to interact with the system and their activities are observed. Lessons learned on interaction challenges in a robot cell are reported.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {310–311},
numpages = {2},
keywords = {Industrial Robot, Interaction, Motion-based interface},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926068,
author = {Hanteer, Obaida and Marrella, Andrea and Mecella, Massimo and Catarci, Tiziana},
title = {A Petri-Net Based Approach to Measure the Learnability of Interactive Systems},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926068},
doi = {10.1145/2909132.2926068},
abstract = {We propose an approach to measure the learnability of an interactive system. Our approach relies on recording in a user log all the user actions that take place during a run of the system and on replaying them over one or more interaction models of the system. Each interaction model describes the expected way of executing a relevant task provided by the system. The proposed approach is able to identify deviations between the interaction models and the user log and to assess the weight of such deviations through a fitness value, which estimates how much a log adheres to the models. Our thesis is that by measuring the rate of such a fitness value for subsequent executions of the system we can not only understand if the system is learnable with respect to its relevant tasks, but also to identify potential learning issues.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {312–313},
numpages = {2},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926072,
author = {Humayoun, Shah Rukh and Ezaiza, Hafez and AlTarawneh, Ragaad and Ebert, Achim},
title = {Social-Circles Exploration through Interactive Multi-Layered Chord Layout},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926072},
doi = {10.1145/2909132.2926072},
abstract = {Due to the usage of social media platforms for different purposes in our current digital social life, people are interested to explore and understand their personal networks (called the ego networks) and how their contacts are associated with different social circles on these social media platforms. In this work, we propose an enhanced multi-layered Chord layout to show people' ego networks and the associated social circles in interactive visual forms. This proposed solution enables the users to explore intuitively how their contacts are associated to different social circles through a set of provided interaction and filtering options.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {314–315},
numpages = {2},
keywords = {Chord Layout, Ego Networks, Social Networks, Circle Discovery, Personal Visualization},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926076,
author = {Johnson, Matthew and Humer, Irene and Zimmerman, Brian and Shallow, Joshua and Tahai, Liudmila and Pietroszek, Krzysztof},
title = {Low-Cost Latency Compensation in Motion Tracking for Smartphone-Based Head Mounted Display},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926076},
doi = {10.1145/2909132.2926076},
abstract = {A smartphone-based head mounted display combined with a motion tracking device provides an affordable wireless interactive virtual reality system. A common problem of such a setup is high end-to-end latency. We evaluate low-cost motion trajectory prediction techniques and show that latency compensation can be realized without using much of the smartphone's computational resources. We apply the technique to reduce motion tracking lag in a smartphone-based virtual reality lightsaber fighting game.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {316–317},
numpages = {2},
keywords = {motion tracking, immersive 3D environments, virtual reality, freehand interaction, spatial input, latency compensation},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926084,
author = {Kanev, Kamen and De Marsico, Maria and Bottoni, Paolo and Mecca, Alessio},
title = {Mobiles and Wearables: Owner Biometrics and Authentication},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926084},
doi = {10.1145/2909132.2926084},
abstract = {We discuss the design and development of HCI models for authentication based on gait and gesture that can be supported by mobile and wearable equipment. The paper proposes to use such biometric behavioral traits for partially transparent and continuous authentication by means of behavioral patterns.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {318–319},
numpages = {2},
keywords = {wearable monitoring, transparent authentication, mobile motion tracking, continuous biometric input, User identification},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926074,
author = {Kang, Dongwann and Yoon, Kyunghyun},
title = {A Study on Developing the Interface of Mobile E-Learning Application for Children's Foreign Language Education},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926074},
doi = {10.1145/2909132.2926074},
abstract = {In this paper, we present a smartphone based e-book application with interactive illustration authoring tool. The target readers of our e-book are children who study foreign language. Our application aims to educate preschool age children's foreign language by telling folktales with illustrations. To encourage effective learning, our application provides a tool that enables the user to create collage-based illustrations on the application by hand. In this function, the reader can make one's own illustration by employing colored paper collage style. We implement the application on Android-based smartphone.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {320–321},
numpages = {2},
keywords = {e-book, mobile, collage, e-learning},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926085,
author = {Katsini, Christina and Avouris, Nikolaos and Lanzilotti, Rosa},
title = {Usability Engineering Practices in Software Development Organizations: The Greek and the Italian Case Study},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926085},
doi = {10.1145/2909132.2926085},
abstract = {This paper reports the results of a study of software development organizations' approach towards usability, conducted on software development organizations in Greece, extending a survey conducted in Southern Italy in 2011. The results show that the organization performing usability evaluation is nearly the same in both countries as well as the key advantages and the problems in performing usability evaluation emphasized by Italian and Greek respondents. A difference in the understanding of usability evaluation concept between the two studies emerged.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {322–323},
numpages = {2},
keywords = {survey, software development, Usability engineering},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926078,
author = {Lopez, S. and Revel, A. and Lingrand, D. and Precioso, F. and Dusaucy, V. and Giboin, A.},
title = {Catching Relevance in One Glimpse: Food or Not Food?},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926078},
doi = {10.1145/2909132.2926078},
abstract = {Retrieving specific categories of images among billions of images usually requires an annotation step. Unfortunately, keywords-based techniques suffer from the semantic gap existing between a semantic concept and its digital representation. Content Based Image Retrieval (CBIR) systems tackle this issue simply considering semantic proximities can be mapped to similarities in the image space. Introducing relevance feedbacks involves the user in the task, but extends the annotation step.To reduce the annotation time, we want to prove that implicit relevance feedback can replace an explicit one. In this study, we will evaluate the robustness of an implicit relevance feedback system only based on eye-tracking features (gaze-based interest estimator, GBIE). In [5], we showed that our GBIE was representative for any set of users using "neutral images". Here, we want to prove that it remains valid for more "subjective categories" such as food recipe.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {324–325},
numpages = {2},
keywords = {Visual Preference Paradigm, Implicit Feedback, Gaze features, Experimentation, Image tag, Mental search},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926061,
author = {Masoodian, Masood and Luz, Saturnino},
title = {Time-Load: Visualization of Energy Consumption Loads Over Time},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926061},
doi = {10.1145/2909132.2926061},
abstract = {It has been suggested that providing feedback allows people to better understand their energy consumption behaviour and take the necessary actions to reduce energy consumption. Here, we present the time-load visualization, which shows percentages of energy usage by different categories of devices that contribute to the total energy consumption load over time. Time-load aims to assist users with visualization of variations in energy consumption by different categories over time, and their respective, and often very unbalanced, contributions to the total energy usage load.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {326–327},
numpages = {2},
keywords = {Energy usage visualization, energy usage load, energy usage monitoring, time-load visualization, timelines},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926081,
author = {Matassa, Assunta and Morreale, Fabio},
title = {Supporting Singers with Tangible and Visual Feedback},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926081},
doi = {10.1145/2909132.2926081},
abstract = {Most of musicians can control their performance by relying on different sensorial modalities that complement the auditory cue. Vision, in particular, offers most instrumentalists an essential support: it helps them developing techniques, identifying errors, correcting expressiveness, and memorise complex passages. By contrast, when performing a piece, singers can almost exclusively rely on the auditory feedback coming from their voice to adjust their singing. This paper frames this issue and proposes possible alternatives to improve singers' awareness by adding visual and tangible feedback to their performance.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {328–329},
numpages = {2},
keywords = {Multisensory Interaction, Physicality, Breath-controlled Interface, Tangible User Interface},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926060,
author = {Mokatren, Moayad and Kuflik, Tsvi and Shimshoni, Ilan},
title = {Using Eye-Tracking for Enhancing the Museum Visit Experience},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926060},
doi = {10.1145/2909132.2926060},
abstract = {A smart context-aware mobile guide may provide the visitor with personalized relevant information from the vast amount of content available at the museum, adapted for his or her personal needs. Earlier studies relied on using sensors for location-awareness and interest detection. This work explores the potential of mobile eye-tracking and vision technology in enhancing the museum visit experience. We report here on satisfactory preliminary results from examining the performance of a mobile eye tracker in a realistic setting.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {330–331},
numpages = {2},
keywords = {Mobile eye tracking, Mobile guide, Context aware service, Smart environment, Personalized information},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926079,
author = {Mora, Simone and Divitini, Monica and Gianni, Francesco},
title = {Tiles: An Inventor Toolkit for Interactive Objects},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926079},
doi = {10.1145/2909132.2926079},
abstract = {We present the groundwork for Tiles: an inventor toolbox to support the development of interactive objects by non-experts. Tiles is composed by (i) a set of physical input/output primitives to describe interaction styles with technology-augmented objects, (ii) extensible hardware modules easily embeddable in everyday things that implement the primitives, (iii) APIs to code application logics using popular programming languages. We are currently exploring the opportunities of using Tiles to develop applications for learning, games and advanced visual interfaces.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {332–333},
numpages = {2},
keywords = {Internet of Things, Tangible User Interfaces, Interactive Objects},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926062,
author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo and Mininni, Giuseppe and Taronna, Annarita},
title = {The EmoQuest Project: Emotions in Q&amp;A Sites},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926062},
doi = {10.1145/2909132.2926062},
abstract = {In this paper, we describe the overall goals and expected contribution of the EmoQuest project. EmoQuest is a three-year multi-disciplinary research project whose main goal is to understand the role of emotions in social media-based knowledge sharing, specifically in online Question and Answer (Q&amp;A) sites. The main research domain of EmoQuest is Computer Supported Cooperative Work (CSCW), with expected outputs in Human-Computer Interaction, Software Engineering, Linguistics, and Psychology.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {334–335},
numpages = {2},
keywords = {Social Computing, Applied Linguistics, Human Factors, Sentiment Analysis, Computer-Supported Cooperative Work, Question &amp; Answer Sites},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926070,
author = {Panizzi, Emanuele},
title = {The SeismoCloud App: Your Smartphone as a Seismometer},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926070},
doi = {10.1145/2909132.2926070},
abstract = {We designed and developed an app, for iOS and Android, which uses internal device accelerometer to detect earthquakes that may occur while the smartphone is stable on a at surface and to deliver a crowdsourced early warning to users in the region where the earthquake might be dangerous. We describe our interface for the Android operating system and we compare our system to the other main research work.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {336–337},
numpages = {2},
keywords = {crowdsourcing, mobile, smartphone, earthquake early warning, sensors},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926063,
author = {Pittarello, Fabio and Franchin, Eugenio},
title = {PlayVR: A VR Experience for the World of Theater},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926063},
doi = {10.1145/2909132.2926063},
abstract = {This paper describes the design of a VR educational experience focused on the world of theater. The PlayVR project explores several dimensions of this experience, corresponding to different levels of cognitive and emotional involvement: from the simple tour through the locations of an Italian theater to the possibility of exploring the different scenographies and to play with the characters of a comedy.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {338–339},
numpages = {2},
keywords = {Oculus, interaction, VR, theater, education, embodiment},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926064,
author = {Risseeuw, Martin and Cavada, Dario and Not, Elena and Zancanaro, Massimo and Marshall, Mark T. and Petrelli, Daniela and Kubitza, Thomas},
title = {Authoring Augmented Digital Experiences in Museums},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926064},
doi = {10.1145/2909132.2926064},
abstract = {It is of paramount importance that cultural heritage professionals are directly involved in the design of digitally augmented experiences in their museum spaces. We propose an approach based on a catalogue of reusable narrative and interaction strategies with step-by-step instructions on how to adapt and instantiate them for a specific museum and type of visitors. This work is conducted in the context of the European-funded project meSch.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {340–341},
numpages = {2},
keywords = {Tangible and Embodied Interaction, Authoring Tool, Cultural Heritage},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926058,
author = {Sandnes, Frode Eika},
title = {PanoramaGrid: A Graph Paper Tracing Framework for Sketching 360-Degree Immersed Experiences},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926058},
doi = {10.1145/2909132.2926058},
abstract = {A novel framework that allows designers without 3D-modelling experience to draw three-dimensional panoramic sketches by hand with the help of a support lines is proposed. Sketches are viewed with panoramic viewing software, giving observers interactive three-dimensional 360-degree immersed experiences.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {342–343},
numpages = {2},
keywords = {Sketching, panorama, immersed experiences},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926056,
author = {Tarallo, Donald},
title = {Instigating Imagination: Teaching Interface and Typography as Metaphor},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926056},
doi = {10.1145/2909132.2926056},
abstract = {In today's template-driven environment of website design it is a challenge to teach students the concept of metaphor production for visual interfaces and inspire them to explore the inherent creative potential of screen-based media. This poster showcases student work from an introductory web design course project created to encourage students to think of a web space, interface, and typography in terms of metaphor. To this end, students produced small websites using text from Italo Calvino's Invisible Cities in efforts to form visual metaphors of the text's content based on their own novel subjective interpretations. The value of this assignment is in its alternative approach to teaching introductory web and interface design.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {344–345},
numpages = {2},
keywords = {Visual Interface, Experimental, Graphic Design, Metaphor, Education, Typography},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926067,
author = {Zarraonandia, Telmo and D\'{\i}az, Paloma and Aedo, Ignacio and Montero, Alvaro},
title = {Inmersive End User Development for Virtual Reality},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926067},
doi = {10.1145/2909132.2926067},
abstract = {In this paper we present VR GREP, an immersive End User Development (EUD) tool that supports authoring and modifying general purpose immersive VR environments, without having technical knowledge. The system aims to facilitate exploring the potential of immersive tools to support situated design practices.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {346–347},
numpages = {2},
keywords = {Interaction, Authoring, Virtual Reality, EUD, Immersive Environments},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926091,
author = {Buono, Paolo},
title = {A Circular Visualization Technique for Collaboration and Quantifying Self},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926091},
doi = {10.1145/2909132.2926091},
abstract = {People awareness in various contexts has been widely considered in the literature. A form of awareness is the quantification of self, which requires a number of conditions to be implemented. The most important are: producing, computing and making sense of data. Sensors produce data at very high rates. A lot of research, in the field of data bases, has focused on how to store and compute data efficiently. Data presentation is still challenging, because the possibilities of producing interactive visualizations on the Web and on different devices are increasing. The contribution of this demo paper is to propose a visualization technique and a web-based tool enabling the visualization of personal data produced during the 24 hours of the day. The aim of this tool is to help people to understand their own behavior. Such data can also be compared with other people's data to improve the analysis. This demo focuses on two main contexts: visualizing working data of a group of people living in different time zones in order to improve the awareness of the behavior of the group; visualizing energy consumption data in order to provide an idea of the behavior of people in the domestic context. The data for the first example are gathered from the activity people perform with their computer (e.g. email, chat, keyboard strokes) while the data of the second context are gathered from a low-cost Arduino device capable of providing instant electricity consumption information.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {348–349},
numpages = {2},
keywords = {collaboration, Circular visualization, time series, activity traces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926089,
author = {Calabria, Sergio and Cullen, Charles and Duggan, Bryan},
title = {VR 3D Visualization Interface},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926089},
doi = {10.1145/2909132.2926089},
abstract = {Human activities produce large amounts of data, from online transactions to scientific experiments, this vast trove of information is being collected on a daily basis and great efforts are being made to produce tools to analyze it, in an attempt to gather insight and knowledge from this inexhaustible source. The use of 3D techniques to display information, and sorting techniques to arrange it are not new. Node-edge graphs predate computers, but their visualization in real-time, three-dimensional virtual reality is quite recent. Some efforts have been conducted by visualization researchers to verify the validity of these techniques, and some HCI experiments have been carried out in navigating 3D spaces. This paper describes a HCI framework to facilitate engagement in an immersive 3D virtual reality (VR) visualization, by means of a compound control system consisting of existing technologies, for navigation and highlighting, and a purpose built controller for selection and manipulation of data; within a software environment that was built using an existing real time 3D game engine.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {350–351},
numpages = {2},
keywords = {Interaction, Visualization, Controls, Interfaces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926090,
author = {Colombo, Simone and Garzotto, Franca and Gelsomini, Mirko and Melli, Mattia and Clasadonte, Francesco},
title = {Dolphin Sam: A Smart Pet for Children with Intellectual Disability},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926090},
doi = {10.1145/2909132.2926090},
abstract = {Our research aims at helping children with intellectual disability (ID) to "learn through play" by interacting with digitally enriched physical toys. Inspired by the practice of Dolphin Therapy (a special form of Pet Therapy) and, specifically, by the activities that ID children perform at Dolphinariums, we have developed a "smart" stuffed dolphin called SAM that engages children in a variety of play tasks. SAM emits different stimuli (sound, vibration, and light) with its body in response to children's manipulation. Its behavior is integrated with lights and multimedia animations or video displayed in the ambient and can be customized by therapists to address the specific needs of each child.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {352–353},
numpages = {2},
keywords = {autism, smart object, pet therapy, Children, intellectual disability, Arduino},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926088,
author = {Di Geronimo, Linda and Norrie, Moira C.},
title = {Rapid Development of Web Applications That Use Tilting Interactions in Single and Multi-Device Scenarios},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926088},
doi = {10.1145/2909132.2926088},
abstract = {Tilt-and-Tap (TAT) is a jQuery-like framework that supports the rapid development of web applications that use tilting gestures for interaction. Such gestures enable users to interact with, and navigate through, websites by simply moving their mobile device in combination with screen taps. Building on this, we have also developed a second framework Cross-Tilt-and-Tap (CTAT) that supports the rapid development of applications that use motion-basd gestures in a range of multi-device scenarios. Our demo not only shows a range of applications that we have developed using the frameworks, but also the frameworks in action by demonstrating the steps involved in developing and adapting applications.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {354–355},
numpages = {2},
keywords = {Frameworks, Mobile, Motion Gestures},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926087,
author = {Di Rienzo, Antonella and Tagliaferri, Paolo and Arenella, Francesco and Garzotto, Franca and Fr\`{a}, Cristina and Cremonesi, Paolo and Valla, Massimo},
title = {Bridging Physical Space and Digital Landscape to Drive Retail Innovation},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926087},
doi = {10.1145/2909132.2926087},
abstract = {This paper describes a contemporary concept store, offering a technology-rich blend of entertainment and interactivity targeted to help customers in their shopping experiences while shortening the time they waste queuing.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {356–357},
numpages = {2},
keywords = {smart spaces, mobile interaction, touch screens, multi-device interaction, personalization, smart retail},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2926086,
author = {Meng, Xiaojun and Foong, Pin Sym and Perrault, Simon and Zhao, Shengdong},
title = {5-Step Approach to Designing Controlled Experiments},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2926086},
doi = {10.1145/2909132.2926086},
abstract = {Controlled experiment, an approach that has been adopted from research methods in psychology, is now widely used in HCI. Design an effective controlled experiment is not necessarily easy, even for experienced researchers. Existing controlled experiment designing tools focused more on the process of designing, but they are often not intuitive and guided enough for less experienced users to use. In this demo paper, we introduce NexP (Next Experiment Tool), a web-based open-source tool for designing controlled experiments. NexP introduces a 5-step approach to guide users through the experimental design process, and helped them to better understand the experimental design process, making it both a useful and educational tool.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {358–359},
numpages = {2},
keywords = {Design Controlled Experiment, NexP, Experimental Design Tool},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2927470,
author = {Gena, Cristina and De Carolis, Berardina and Kuflik, Tsvi and Nunnari, Fabrizio},
title = {Advanced Visual Interfaces for Cultural Heritage},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927470},
doi = {10.1145/2909132.2927470},
abstract = {Cultural heritage traditionally draws a lot of research attention when it comes to exploring the potential benefits from application of novel technology in realistic settings. The domain is rich of physical as well as virtual sites objects and infinite information about them. Hence, it is only natural that whenever new technology appears, it is experimented in cultural heritage -- from early dialog systems to state of the art Humanoid robots, eye trackers, virtual/augmented reality devices, and the Internet of Things (IoT). The AVI-CH workshop nicely demonstrate this with the diversity of topics presented by the papers accepted to the workshop.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {360–362},
numpages = {3},
keywords = {Cultural Heritage, Workshop, Advanced Visualization},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2927471,
author = {Bornschlegl, Marco X. and Manieri, Andrea and Walsh, Paul and Catarci, Tiziana and Hemmje, Matthias L.},
title = {Road Mapping Infrastructures for Advanced Visual Interfaces Supporting Big Data Applications in Virtual Research Environments},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927471},
doi = {10.1145/2909132.2927471},
abstract = {Handling the complexity of relevant data requires new techniques about data access, visualization, perception, and interaction for innovative and successful strategies. In order to address human-computer interaction, cognitive eficiency, and interoperability problems, a generic information visualization, user empowerment, as well as service integration and mediation approach based on the existing state-of-the-art in the relevant areas of computer science has to be achieved.This workshop will address these issues with a special focus on supporting distributed Big Data analysis in Virtual Research Environments (VREs). In this way, the overall scope and goal of the workshop is to bring together researchers in these areas to achieve a road map, which can support the acceleration in research activities by means of transforming, enriching, and deploying advanced visual user interfaces for managing and using e-Science infrastructures. Advancements in this fields of research can i.e. support the, creation, configuration, management, and usage of distributed Big Data analysis in VREs.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {363–367},
numpages = {5},
keywords = {Virtual Research Environments, User Empowerment, Distributed Big Data Analysis, Information Visualization, Advanced Visual User Interfaces},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2927472,
author = {Dix, Alan and Malizia, Alessio and Gabrielli, Silvia},
title = {HCI and the Educational Technology Revolution},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927472},
doi = {10.1145/2909132.2927472},
abstract = {While educational technology has a long pedigree, the last few years have seen dramatic changes. These have included the rise and institutionalisation of MOOCs, and other web-based initiatives such as Kahn Academy and Peer-to-Peer University (P2PU). Classrooms have also been transformed with growing use of mobile devices and forms of flipped classroom; and educational progress and engagement has been increasingly measured leading to institutional and individual learning analytics. This workshop seeks to understand the interaction of these issues with human--computer interaction in a number of ways. First to ask what HCI has to contribute to these in terms of the design of authoring and learning platforms, and the wider socio-political implications of increasingly metric-driven governance? Second to discuss how will these changes affect HCI education? Together practice-based and theoretical approaches will help us build a clear understanding of the current state and future challenges for educational technology and HCI.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {368–371},
numpages = {4},
keywords = {flip classroom, OER, MOOCs, HCI, Education, peer learning, open education, learning analytics},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2927473,
author = {Ardito, Carmelo and Bellucci, Andrea and Desolda, Giuseppe and Divitini, Monica and Mora, Simone},
title = {SERVE: Smart Ecosystems CReation by Visual DEsign Authoring Internet of Things and Smart Objects Applications},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927473},
doi = {10.1145/2909132.2927473},
abstract = {Recent technology advances support the interconnection of smart objects, enabling their communication according to the Internet of Things (IoT) paradigm. IoT is promising important changes in our lives. The opportunities offered by such technologies can be amplified, by investigating new approaches that, thanks to high-level abstractions, can enable non-expert users to compose data and functionality of things, as well as the communication among them, by means of "natural" composition paradigm. Today, in fact, this is a prerogative almost always reserved to developers who, through the use of specific programming languages, provide pre-packaged solutions to users. The workshop aims at stressing the Human-Computer Interaction perspective, i.e., it acknowledges the importance of enabling even non-technical users to manipulate data and functionality of things in a simple and natural way.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {372–375},
numpages = {4},
keywords = {End-User Empowerment, Internet of Things (IoT), Task automation, End-User Development (EUD)},
location = {Bari, Italy},
series = {AVI '16}
}

@inproceedings{10.1145/2909132.2927474,
author = {Cabitza, Federico and Locoro, Angela and Fogli, Daniela and Giacomin, Massimiliano},
title = {Valuable Visualization of Healthcare Information: From the Quantified Self Data to Conversations},
year = {2016},
isbn = {9781450341318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909132.2927474},
doi = {10.1145/2909132.2927474},
abstract = {Big data analytics in healthcare would be almost useless, without suitable tools allowing users "see" them, and gain insight for their situated decisions. The VVH (Valuable Visualization in Healthcare) workshop focuses on the role of interactive data visualization tools by which people can make sense of healthcare data; these data include sensor data, the messages exchanged in social media, the emails between patients and their doctors, the content of patient records as well as the discussions among different specialists that led to such record content. All these data are used by different types of users, like doctors, nurses, policy makers and common citizens. The VVH workshop aims at contributing on: the assessment of the usability of advanced interactive tools of health-related data visualization; the assessment of the quality of the information and value for insight that these tools make available to their users; the collection of reports of either success stories or failures in the appropriation and use of complex and multidimensional healthcare datasets; the collection of methodological and design-oriented contributions that could share methods, techniques, and heuristics for the design of interactive tools and applications supporting data work, data telling and data interpretation in healthcare.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {376–380},
numpages = {5},
keywords = {Human-Data Interaction, Data Visualization, Social Value of Information},
location = {Bari, Italy},
series = {AVI '16}
}

