@inproceedings{10.1145/3206505.3206607,
author = {Nishida, Toyoaki},
title = {Envisioning Conversation: Toward Understanding and Augmenting Common Ground},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206607},
doi = {10.1145/3206505.3206607},
abstract = {Our intellectual life draws on daily conversations that allow us to communicate thoughts, ideas, emotions, etc. To conduct smooth and reliable interactions, participants need to share a solid basis of common ground prior to conversation, which consists of knowledge, beliefs, and suppositions regarding the topics to discuss. Importance of common ground applies to artificial agents as well. A capability of jointly building and maintaining the common ground with people on the fly is indispensable to establish a productive relationship. Understanding and augmenting common ground is challenging, as it is both tacit and dynamic in the sense that the common ground for a situation contains plenty of tacit dimensions and it is dynamically updated as the interaction proceeds.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {1},
numpages = {1},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206608,
author = {Inselberg, Alfred},
title = {Visual Analytics for High Dimensional Data},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206608},
doi = {10.1145/3206505.3206608},
abstract = {A dataset with M items has 2M subsets anyone of which may be the one satisfying our objective. With a good data display and interactivity our fantastic pattern-recognition defeats this combinatorial explosion by extracting insights from the visual patterns. This is the core reason for data visualization. With parallel coordinates the search for relations in multivariate data is transformed into a 2-D pattern recognition problem. Together with criteria for good query design, we illustrate this on several real datasets (financial, process control, credit-score, one with hundreds of variables) with stunning results. A geometric classification algorithm yields the classification rule explicitly and visually. The minimal set of variables, features, are found and ordered by their predictive value. A model of a country's economy reveals sensitivities, impact of constraints, trade-offs and economic sectors unknowingly competing for the same resources. An overview of the methodology provides foundational understanding; learning the patterns corresponding to various multivariate relations. These patterns are robust in the presence of errors and that is good news for the applications. A topology of proximity emerges opening the way for visualization in Big Data.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {2},
numpages = {1},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206609,
author = {Czerwinski, Mary},
title = {Using Technology for Health and Wellbeing},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206609},
doi = {10.1145/3206505.3206609},
abstract = {How can we create technologies to help us reflect on and change our behavior, improving our health and overall wellbeing? In this talk, I will briefly describe the last several years of work our research team has been doing in this area. We have developed wearable technology to help families manage tense situations with their children, mobile phone-based applications for handling stress and depression, as well as logging tools that can help you stay focused or recommend good times to take a break at work. The goal in all of this research is to develop tools that adapt to the user so that they can maximize their productivity and improve their health.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {3},
numpages = {1},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206606,
author = {Catarci, Tiziana and Amendola, Massimo and Bertacchini, Francesca and Bilotta, Eleonora and Bracalenti, Marco and Buono, Paolo and Cocco, Antonello and Costabile, Maria Francesca and Desolda, Giuseppe and Di Nocera, Francesco and Federici, Stefano and Gaudino, Giancarlo and Lanzilotti, Rosa and Marrella, Andrea and Mele, Maria Laura and Pantano, Pietro S. and Poggi, Isabella and Tarantino, Laura},
title = {Digital Interaction: Where Are We Going?},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206606},
doi = {10.1145/3206505.3206606},
abstract = {In the framework of the AVI 2018 Conference, the interuniversity center ECONA has organized a thematic workshop on "Digital Interaction: where are we going?". Six contributions from the ECONA members investigate different perspectives around this thematic.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {4},
numpages = {5},
keywords = {visual interfaces, accessibility, multimodal interaction, human factors in cybersecurity, user experience, participatory design, usability evaluation},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206605,
author = {Obrist, Marianna and Marti, Patrizia and Velasco, Carlos and Tu, Yunwen (Tutu) and Narumi, Takuji and M\o{}ller, Naja L. Holten},
title = {The Future of Computing and Food: Extended Abstract},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206605},
doi = {10.1145/3206505.3206605},
abstract = {The excitement around computing technology in all aspects of life requires that we tackle fundamental issues of healthcare, leisure, labor, education, and food to create the society we want. The aim of this satellite event was to bring together a variety of different stakeholders, ranging from local food producers, chefs, designers, engineers, data scientists, and sensory scientists, to discuss the interwoven future of computing technology and food. This event was co-located with the AVI 2018 conference and supported by the ACM Future of Computing Academy (ACM-FCA). The event followed a co-creation approach that encourages conjoined creative and critical thinking that feeds into the formulation of a manifesto on the future of computing and food. We hope this will inspire future discussions on the transformative role of computing technology on food.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {5},
numpages = {3},
keywords = {computing food, multisensory experiences, HCI, computational gastronomy, HFI, molecular gastronomy},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206597,
author = {De Carolis, Berardina Nadja and Gena, Cristina and Kuflik, Tsvi and Origlia, Antonio and Raptis, George E.},
title = {AVI-CH 2018: Advanced Visual Interfaces for Cultural Heritage},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206597},
doi = {10.1145/3206505.3206597},
abstract = {Cultural Heritage (CH) is a challenging domain of application for novel Information and Communication Technologies (ICT), where visualization plays a major role in enhancing visitors' experience, either onsite or online. Technology-supported natural human-computer interaction is a key factor in enabling access to CH assets. Advances in ICT ease visitors to access collections online and better experience CH onsite. The range of visualization devices - from tiny smart watch screens and wall-size large situated public displays to the latest generation of immersive head-mounted displays - together with the increasing availability of real-time 3D rendering technologies for online and mobile devices and, recently, Internet of Things (IoT) approaches, require exploring how they can be applied successfully in CH. Following the successful workshop at AVI 2016 and the large numbers of recent events and projects focusing on CH and, considering that 2018 has been declared the European Year of Cultural Heritage, the goal of the workshop is to bring together researchers and practitioners interested in presenting and discussing the potential use of state-of-the-art advanced visual interfaces in enhancing our daily CH experience.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {3},
keywords = {workshop, advanced visualization, cultural heritage},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206599,
author = {Barricelli, Barbara Rita and Fischer, Gerhard and Fogli, Daniela and M\o{}rch, Anders and Piccinno, Antonio and Valtolina, Stefano},
title = {Cultures of Participation in the Digital Age: Design Trade-Offs for an Inclusive Society},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206599},
doi = {10.1145/3206505.3206599},
abstract = {This new edition of CoPDA workshop, the 5th since 2013, is dedicated to the discussion of design trade-offs that have to be addressed for embracing diversity and implementing an inclusive society. With this workshop, we invite researchers and practitioners to discuss and exchange experiences able to inform design processes in a Cultures of Participation perspective, focusing on theoretical frameworks, practical experiences, case studies, and research projects, both in Academy and Industry.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {7},
numpages = {3},
keywords = {user diversity, design trade-off, meta-design, inclusive design},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206598,
author = {De Marsico, Maria and Gadia, Davide and Maggiorini, Dario and Mariani, Ilaria and Ripamonti, Laura A.},
title = {GHItaly18: 2nd International Workshop on Games-Human Interaction},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206598},
doi = {10.1145/3206505.3206598},
abstract = {This short paper presents the second international workshop on Games-Human Interaction - GHItaly 2018. The goal of this series of workshops is to focus on advanced aspects of the design and development of game interfaces. The quality of the resulting interaction is a highly relevant issue for creating an engaging and satisfactory user experience, especially when deeply multidimensional artefacts such as video games are concerned.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {8},
numpages = {3},
keywords = {biometric measures for interaction, usability, social interaction, immersive VR systems, HCI, storytelling, game design, distributed and online systems, artificial intelligence, player experience, gamification},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206600,
author = {Wilde, Adriana and Vasilchenko, Anna and Dix, Alan},
title = {HCI and the Educational Technology Revolution #HCIEd2018: A Workshop on Video-Making for Teaching and Learning Human-Computer Interaction},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206600},
doi = {10.1145/3206505.3206600},
abstract = {Over the years, the HCI Educators series has studied a number of challenges for the teaching and learning of Human-Computer Interaction at a time of radical educational change. Though video has historically played an important part on the teaching and development of HCI, only recently video-making and editing technologies have become accessible in an unprecedented way, allowing students to become proficient video "prosumers" (producers and consumers). Further, there are numerous educational gains to be had through these technologies. Through a very interactive workshop we explore how can video be used in practice to leverage skills and foster creativity whilst facilitating knowledge acquisition.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {9},
numpages = {3},
keywords = {curriculum design, video making, HCI education, assessment},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206601,
author = {Angelini, Marco and Santucci, Giuseppe},
title = {ITA.WA.: 1st Italian Visualization &amp; Visual Analytics Workshop},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206601},
doi = {10.1145/3206505.3206601},
abstract = {Data-driven approaches to problem solving and data analysis are becoming more and more important problems to consider and on which apply research ideas. In this respect, the capability to explore data, understands how algorithmic approaches work and steer them toward the desired goals make Visualization and Visual Analytics strong research fields in which to invest efforts. While this importance has been understood by several countries (e.g., USA, Germany, France) in Italy the research efforts in these fields are still disjointed. With the first edition of the ITA.WA. (Italian Visualization &amp; Visual Analytics) workshop the goal is to make a step toward the creation of an Italian research community on these topics, allowing identification of research direction, joining forces in achieving them and developing common guidelines and programs for teaching activities.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {10},
numpages = {3},
keywords = {visual analytics, teaching activities, visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206602,
author = {Lee, Bongshin and Srinivasan, Arjun and Stasko, John and Tory, Melanie and Setlur, Vidya},
title = {Multimodal Interaction for Data Visualization},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206602},
doi = {10.1145/3206505.3206602},
abstract = {Multimodal interaction offers many potential benefits for data visualization. It can help people stay in the flow of their visual analysis and presentation, with the strengths of one interaction modality offsetting the weaknesses of others. Furthermore, multimodal interaction offers strong promise for leveraging data visualization on diverse display hardware including mobile, AR/VR, and large displays. However, prior research on visualization and interaction techniques has mostly explored a single input modality such as mouse, touch, pen, or more recently, natural language. The unique challenges and opportunities of synergistic multimodal interaction for data visualization have yet to be investigated. This workshop will bring together researchers with expertise in visualization, interaction design, and natural user interfaces. We aim to build a community of researchers focusing on multimodal interaction for data visualization, explore opportunities and challenges in our research, and establish an agenda for multimodal interaction research specifically for data visualization.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {11},
numpages = {3},
keywords = {interaction design, data visuallization, multimodal interaction},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206603,
author = {Kammer, Dietrich and Keck, Mandy and Both, Andreas and Jacucci, Giulio and Groh, Rainer},
title = {VisBIA 2018: Workshop on Visual Interfaces for Big Data Environments in Industrial Applications},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206603},
doi = {10.1145/3206505.3206603},
abstract = {Industrial applications can benefit considerably from the overwhelming amount of still growing resources such as websites, images, texts, and videos that the internet offers today. The resulting Big Data Problem does not only consist of handling this immense volume of data. Moreover, data needs to be processed, cleaned, and presented in a user-friendly, intuitive, and interactive way. This workshop addresses visualization and user interaction challenges posed by the four V's: Volume (huge data amounts in the range of tera and petabytes), Velocity (the speed in which data is created, processed, and analysed), Variety (the different heterogeneous data types, sources, and formats), and Veracity (authenticity and validity of data). Big Data driven interfaces combine suitable backend and frontend technologies as well as automatic and semi-automatic approaches in order to analyze data in various business contexts. An important aspect is human intervention in developing and training data-driven applications (human in the loop). Our focus is on Visual Big Data Interfaces in industrial contexts such as e-commerce, e-learning and business intelligence. We address interfaces for three important user groups: data scientists, data workers and end users.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {12},
numpages = {3},
keywords = {big data, multidimensional scaling, visual cluster analysis},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206545,
author = {Trullemans, Sandra and Ebrahimi, Payam and Signer, Beat},
title = {Designing Prosthetic Memory: Audio or Transcript, That is the Question},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206545},
doi = {10.1145/3206505.3206545},
abstract = {Audio recordings and the corresponding transcripts are often used as prosthetic memory (PM) after meetings and lectures. While current research is mainly developing novel features for prosthetic memory, less is known on how and why audio recordings and transcripts are used. We investigate how users interact with audio and transcripts as prosthetic memory, whether interaction strategies change over time, and analyse potential differences in accuracy and efficiency. In contrast to the subjective user perception, our results show that audio recordings and transcripts are equally efficient, but that transcripts are generally preferred due to their easily accessible contextual information. We further identified that prosthetic memory is not only used as a recall aid but frequently also consulted for verifying information that has been recalled from organic memory (OM). Our findings are summarised in a number of design implications for prosthetic memory solutions.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {13},
numpages = {9},
keywords = {recall, verification, prosthetic memory, personal information management, note-taking, speech retrieval},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206524,
author = {Fruchard, Bruno and Lecolinet, Eric and Chapuis, Olivier},
title = {Impact of Semantic Aids on Command Memorization for On-Body Interaction and Directional Gestures},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206524},
doi = {10.1145/3206505.3206524},
abstract = {Previous studies have shown that spatial memory and semantic aids can help users learn and remember gestural commands. Using the body as a support to combine both dimensions has therefore been proposed, but no formal evaluations have yet been reported. In this paper, we compare an on-body interaction technique (BodyLoci) to mid-air Marking menus in a virtual reality context. We consider three levels of semantic aids: no aid, story-making, and story-making with background images. Our results show important improvement when story-making is used, especially for Marking menus (28.5% better retention). Both techniques performed similarly without semantic aids, but Marking menus outperformed BodyLoci when using them (17.3% better retention). While our study does not show a benefit in using body support, it suggests that inducing users to leverage simple learning techniques, such as story-making, can substantially improve recall, and thus make it easier to master gestural techniques. We also analyze the strategies used by the participants for creating mnemonics to provide guidelines for future work.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {14},
numpages = {9},
keywords = {semantic aids, command selection, memorization, virtual reality, on-body interaction, marking menus},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206509,
author = {Appert, Caroline and Pietriga, Emmanuel and Bartenlian, \'{E}l\'{e}onore and Gonz\'{a}lez, Rafael Morales},
title = {Custom-Made Tangible Interfaces with Touchtokens},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206509},
doi = {10.1145/3206505.3206509},
abstract = {TouchTokens were introduced recently as a means to design low-cost tangible interfaces. The technique consists in recognizing multi-touch patterns associated with specific tokens, and works on any touch-sensitive surface, with passive tokens that can be made out of any material. TouchTokens have so far been limited to a few basic geometrical shapes only, which puts a significant practical limit to how tailored token sets can be. In this article, we introduce TouchTokenBuilder and TouchTokenTracker that, taken together, aim at facilitating the development of tailor-made tangible interfaces. TouchTokenBuilder is an application that assists interface designers in creating token sets using a simple direct-manipulation interface. TouchTokenTracker is a library that enables tracking the tokens' full geometry. We report on experiments with those tools, showing the strengths and limitations of tangible interfaces with passive tokens.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {15},
numpages = {9},
keywords = {multi-touch surfaces, tangible interaction, customization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206510,
author = {Rosso, Juan and Coutrix, C\'{e}line and Jones, Matt and Nigay, Laurence},
title = {Simulating an Extendable Tangible Slider for Eyes-Free One-Handed Interaction on Mobile Devices},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206510},
doi = {10.1145/3206505.3206510},
abstract = {Sliders are widely used on mobile devices. Envisioning mobile devices that can dynamically deform to raise tangible controls from the screen surface, tangible sliders offer the benefit of eyes-free interaction. However, reaching for distant values with one hand is problematic: users namely need to change their handgrip, which is not comfortable. To overcome this problem, this paper sets out to experimentally study an extendable tangible slider to support one-handed clutching. The tangible slider's knob extends to maintain the thumb's movement within its comfortable area. We first built a low-fidelity prototype made of a knob long enough to allow clutching. This low-fidelity prototype significantly improves performance when reaching distant targets, as compared to a standard tangible slider. We then built a higher-fidelity prototype, introducing actuation and allowing for a shorter knob. When used for clutching, the knob moves back towards the users' thumb. Experimental results show that the motion of the actuated knob does not interrupt eyes-free interaction during manipulation. In comparison, a graphical extendable slider performed 0.9s slower due to the required visual attention. However, the results suggest that the motion of the actuated knob affects performance, as the higher-fidelity prototype performed 0.6s slower than the low-fidelity prototype.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {16},
numpages = {9},
keywords = {extendable slider, thumb interaction, tangible interaction, mobility, shape-changing interfaces},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206518,
author = {Speicher, Marco and Hell, Philip and Daiber, Florian and Simeone, Adalberto and Kr\"{u}ger, Antonio},
title = {A Virtual Reality Shopping Experience Using the Apartment Metaphor},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206518},
doi = {10.1145/3206505.3206518},
abstract = {In contrast to conventional retail stores, online shopping comes with many advantages, like unrestricted opening hours and is more focused on functionality. However, these pros often come at a cost of complex search and limited product visualization. Virtual Reality (VR) has the potential to create novel shopping experiences that combine the advantages of e-commerce sites and conventional stores. In this work, we propose a VR shop concept where product placement is not organized in shelves but through spatial placement in appropriate locations in an apartment environment. We thus investigated how the spatial arrangement of products in a non-retail environment affects the user, and how the actual shopping task can be supported in VR. In order to answer these questions, we designed two product selection and manipulation techniques (grabbing and pointing) and two VR shopping cart concepts (a realistic basket and an abstract one) and evaluated them in a user study. The results indicate that product interaction using pointing in combination with the abstract cart concept performs best with regard to error rate, user experience and workload. Overall, the proposed apartment metaphor provides excellent customer satisfaction, as well as a particularly high level of immersion and user experience, and it opens up new possibilities for VR shopping experiences that go far beyond mimicking real shop environments in VR.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {17},
numpages = {9},
keywords = {shopping experience, virtual reality, immersive virtual environment, 3D user interfaces},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206522,
author = {Khamis, Mohamed and Oechsner, Carl and Alt, Florian and Bulling, Andreas},
title = {VRpursuits: Interaction in Virtual Reality Using Smooth Pursuit Eye Movements},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206522},
doi = {10.1145/3206505.3206522},
abstract = {Gaze-based interaction using smooth pursuit eye movements (Pursuits) is attractive given that it is intuitive and overcomes the Midas touch problem. At the same time, eye tracking is becoming increasingly popular for VR applications. While Pursuits was shown to be effective in several interaction contexts, it was never explored in-depth for VR before. In a user study (N=26), we investigated how parameters that are specific to VR settings influence the performance of Pursuits. For example, we found that Pursuits is robust against different sizes of virtual 3D targets. However performance improves when the trajectory size (e.g., radius) is larger, particularly if the user is walking while interacting. While walking, selecting moving targets via Pursuits is generally feasible albeit less accurate than when stationary. Finally, we discuss the implications of these findings and the potential of smooth pursuits for interaction in VR by demonstrating two sample use cases: 1) gaze-based authentication in VR, and 2) a space meteors shooting game.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {18},
numpages = {8},
keywords = {gaze interaction, pursuits, eye tracking, virtual reality},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206526,
author = {Dias, Paulo and Afonso, Luis and Eliseu, S\'{e}rgio and Santos, Beatriz Sousa},
title = {Mobile Devices for Interaction in Immersive Virtual Environments},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206526},
doi = {10.1145/3206505.3206526},
abstract = {Gamepads and 3D controllers are the main controllers used in most Virtual Environments. Despite being simple to use, these input devices have a number of limitations as fixed layout and difficulty to remember the mapping between buttons and functions. Mobile devices present interesting characteristics that might be valuable in immersive environments: more flexible interfaces, touchscreen combined with onboard sensors that allow new interaction and easy acceptance since these devices are used daily by most users. The work described in this article proposes a solution that uses mobile devices to interact with Immersive Virtual Environments for selection and navigation tasks. The proposed solution uses the mobile device camera to track the Head-Mounted-Display position and present a virtual representation of the mobile device screen; it was tested using an Immersive Virtual Museum as use case. Based on this prototype, a study was performed to compare controller based and mobile based interaction for navigation and selection showing that using mobile devices is viable in this context and offers interesting interaction opportunities.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {19},
numpages = {9},
keywords = {immersive virtual reality, 3D interaction, mobile devices},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206549,
author = {Miniukovich, Aliaksei and Sulpizio, Simone and De Angeli, Antonella},
title = {Visual Complexity of Graphical User Interfaces},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206549},
doi = {10.1145/3206505.3206549},
abstract = {Graphical User Interfaces (GUIs) of low visual complexity tend to have higher aesthetics, usability and accessibility, and result in higher user satisfaction. Despite a few authors recently used or studied visual complexity, the concept of visual complexity still needs to be better defined for the use in HCI research and GUI design, with its underlying aspects systematized and operationalized, and different measures validated. This paper reviews the aspects of GUI visual complexity and operationalizes four aspects with nine computation-based measures in total. Two user studies validated the measures on two types of stimuli - webpages (study 1, n = 55) and book pages (study 2, n = 150) - with two user groups, dyslexics (people with reading difficulties) and typical readers. The same complexity aspects could be expected to determine complexity perception for both GUI types, whereas different complexity aspects could be expected to determine complexity perception for dyslexics, relative to typical readers. However, the studies showed little to no difference between dyslexics and average readers, whereas web pages did differ from book pages in what aspects made them seem complex. It was not the intergroup differences, but the stimulus type that defined criteria to judge visual complexity. Future research and visual design could rely on the visual complexity aspects outlined in this paper.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {20},
numpages = {9},
keywords = {GUI research and design, facets of visual complexity, ebooks, dyslexia, webpages, computation of visual complexity},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206552,
author = {Amini, Fereshteh and Riche, Nathalie Henry and Lee, Bongshin and Leboe-McGowan, Jason and Irani, Pourang},
title = {Hooked on Data Videos: Assessing the Effect of Animation and Pictographs on Viewer Engagement},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206552},
doi = {10.1145/3206505.3206552},
abstract = {Pictographic representations and animation techniques are commonly incorporated into narrative visualizations such as data videos. General belief is that these techniques may enhance the viewer experience, thus appealing to a broad audience and enticing the viewer to consume the entire video. However, no study has formally assessed the effect of these techniques on data insight communication and viewer engagement. In this paper, we first propose a scale-based questionnaire covering five factors of viewer engagement we identified from multiple application domains such as game design and marketing. We then validate this questionnaire through a crowdsourcing study on Amazon's Mechanical Turk to assess the effect of animation and pictographs in data videos. Our results reveal that each technique has an effect on viewer engagement, impacting different factors. In addition, insights from these studies lead to design considerations for authoring engaging data videos.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {21},
numpages = {9},
keywords = {information visualization, engagement, data video, pictograph, animation, narrative visualization, animated infographic},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206514,
author = {Fischer, Gerhard},
title = {Identifying and Exploring Design Trade-Offs in Human-Centered Design},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206514},
doi = {10.1145/3206505.3206514},
abstract = {Human-centered design should not only be grounded in understanding new media and technologies in terms of productivity, efficiency, reliability, and from economic perspectives, but it needs to explore innovative socio-technical environments contributing to human creativity, gratification, enjoyment, and quality of life. It represents a wicked problem with no "correct" solutions or "right" answers; the quality and success of design solutions are not only a question of fact, but a question of value and interests of the involved stakeholders.Design trade-offs are the most basic characteristics of design. They are universal and they make us aware that there are "no decontextualized sweet spots". In contrast to design guidelines, they widen rather than narrow design spaces by (1) avoiding simple solutions to complex problems and (2) by identifying and exploring interesting new approaches with the objective to synthesize the strengths and reduce the weaknesses of the binary choices defining the trade-offs.The paper articulates a conceptual framework for human-centered design focused on a design trade-off perspective. The framework is inspired from a brief analysis of design trade-offs in large scale developments (self-driving cars, sharing economy, and big data). Based on our own research activities, it is elaborated with specific design trade-offs (context-aware information delivery, meta-design, and cultures of participation) and further illustrated with the description of the Envisionment and Discovery Collaboratory, a socio-technical environment to frame and solve wicked problems in urban planning.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {22},
numpages = {9},
keywords = {cultures of participation, transformative frameworks, human-centered design, meta-design, design trade-offs, envisionment and discovery collaboratory, context-aware information delivery},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206512,
author = {Delprino, Federica and Piva, Chiara and Tommasi, Giovanni and Gelsomini, Mirko and Izzo, Niccol\`{o} and Matera, Maristella},
title = {ABBOT: A Smart Toy Motivating Children to Become Outdoor Explorers},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206512},
doi = {10.1145/3206505.3206512},
abstract = {This article illustrates ABBOT, a pervasive interactive game for children at the early years of primary school that aims to stimulate exploration of outdoor environments. ABBOT combines a smart tangible object to play outdoors, with a mobile app to access new content related to the discovered natural elements. The tangible object helps children capture images of the elements they find interesting in the physical environment. Through simple interactive games on a tablet, at home children can continue to interact with the collected digital materials and can also access new related content. The article illustrates the design of ABBOT; it also reports on an exploratory study with 160 kids of a preschool and a primary school that helped us assess the attitude of kids towards the game.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {23},
numpages = {9},
keywords = {outdoor exploration, pervasive games, mobile apps, outdoor learning, smart toys, smart object design},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206521,
author = {Marconi, Annapaola and Schiavo, Gianluca and Zancanaro, Massimo and Valetto, Giuseppe and Pistore, Marco},
title = {Exploring the World through Small Green Steps: Improving Sustainable School Transportation with a Game-Based Learning Interface},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206521},
doi = {10.1145/3206505.3206521},
abstract = {In this paper, we present a playful digital activity for primary school classrooms that promotes sustainable and active mobility by leveraging the daily journey to school into a collaborative educational experience. In the class game, stretches of distance travelled in a sustainable way by each child contributes to the advancement of the whole school on a collective virtual trip. During the trip, several virtual stops are associated with the discovery of playful learning material. The approach has been evaluated in a primary school with 87 pupils and 6 teachers actively involved in the learning activity for 12 continuous weeks. The findings from the questionnaires with parents and the interviews with teachers show a positive effect in terms of children's behavioural change as well as educational value. Indications on the use of class and school collaborative gamification activities for supporting sustainable behavioural change are discussed.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {24},
numpages = {9},
keywords = {interactive learning interfaces, children's environmental education, gamification for behavioural change, sustainable mobility},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206511,
author = {Chen, Qin and Perrault, Simon T. and Roy, Quentin and Wyse, Lonce},
title = {Effect of Temporality, Physical Activity and Cognitive Load on Spatiotemporal Vibrotactile Pattern Recognition},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206511},
doi = {10.1145/3206505.3206511},
abstract = {Previous research demonstrated the ability for users to accurately recognize Spatiotemporal Vibrotactile Patterns (SVP): sequences of vibrations on different motors occurring either sequentially or simultaneously. However, the experiments were only run in a lab setting and the ability for users to recognize SVP in a real-world environment remains unclear. In this paper, we investigate how several factors may affect recognition: (1) physical activity (running), (2) cognitive task (i.e. primary task, typing), (3) distribution of the vibration motors across body parts and (4) temporality of the patterns. Our results suggest that physical activity has very little impact, specifically compared to cognitive task, location of the vibrations or temporality. We discuss these results and propose a set of guidelines for the design of SVPs.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {25},
numpages = {9},
keywords = {spatiotemporal vibrotactile pattern, tactile feedback, physical activity, cognitive load, wearable computing},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206515,
author = {Ando, Takahiro and Masaki, Ayano and Liu, Qing and Ooka, Takafumi and Sakurai, Sho and Hirota, Koichi and Nojima, Takuya},
title = {Squachu: A Training Game to Improve Oral Function via a Non-Contact Tongue-Mouth-Motion Detection System},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206515},
doi = {10.1145/3206505.3206515},
abstract = {Impaired articulation, chewing, or swallowing can have a strong impact on Quality of Life. These behaviors can be affected by decreased oral muscle function caused by aging and/or certain medical conditions. Aspiration pneumonitis is a major cause of death among elderly people, and is triggered by impaired swallowing, mainly due to decreased oral muscle strength. To reduce the risk of aspiration pneumonitis, elderly people are often advised to exercise and train their oral muscles. Unfortunately, such training tends to be monotonous and it is often difficult for trainees to detect the effects of the training, i.e., improved oral function. To address this issue, we developed an oral muscle training game called Squachu. Squachu is composed of a non-contact tongue-mouth-motion detection system and a game engine. As motion is tracked using a depth camera, players play the game via mouth and tongue motion and do not have to wear any specialized equipment. Thus, the game is hygienic and easily accessible for daily play.Players tend to assume the game score as a rough indicator of their oral function. Scores can be improved by frequent play of Squachu. Here, we describe the results of a month-long user test. We asked eight participants, aged 74--96, to play Squachu daily. After one month, we found improvements in three out of seven diagnostic criteria for the deterioration of oral function. We also found that some participants became more interested in voluntary game play, as they checked and compared their game scores with others.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {26},
numpages = {8},
keywords = {serious game, oral function, depth sensor, tongue, lip, dysphagia},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206507,
author = {Raheb, Katerina El and Tsampounaris, George and Katifori, Akrivi and Ioannidis, Yannis},
title = {Choreomorphy: A Whole-Body Interaction Experience for Dance Improvisation and Visual Experimentation},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206507},
doi = {10.1145/3206505.3206507},
abstract = {Choreomorphy is inspired by the Greek words "choros" (dance) and "morphe" (shape). Visual metaphors, such as the notion of transformation, and visual imagery are widely used in various movement and dance practices, education, and artistic creation. Motion capture and comprehensive movement representation technologies, if appropriately employed can become valuable tools in this field. Choreomorphy is a system for a whole-body interactive experience, using Motion Capture and 3D technologies, that allows the users to experiment with different body and movement visualisations in real-time. The system offers a variety of avatars, visualizations of movement and environments which can be easily selected through a simple GUI. The motivation of designing this system is the exploration of different avatars as "digital selves" and the reflection on the impact of seeing one's own body as an avatar that can vary in shape, size, gender and human vs. non-human characteristics, while dancing and improvising. Choreomorphy is interoperable with different motion capture systems, including, but not limited to inertial, optical, and Kinect. The 3D representations and interactions are constantly updated through an explorative co-design process with dance artists and professionals in different sessions and venues.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {27},
numpages = {9},
keywords = {whole-body interaction, whole-body interaction experience, 3D models, avatars, human movement, dance education, improvisation, dance improvisation, inertial motion capture, movement recordings, motion capture, visualization, multimodal interaction},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206527,
author = {Vechev, Velko and Dancu, Alexandru and Perrault, Simon T. and Roy, Quentin and Fjeld, Morten and Zhao, Shengdong},
title = {Movespace: On-Body Athletic Interaction for Running and Cycling},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206527},
doi = {10.1145/3206505.3206527},
abstract = {Wearables are increasingly used during training to quantify performance and provide valuable real-time information. However, interacting with these devices in motion may disrupt the movements of the activity. We propose a method of interaction involving tapping specific locations on the body, identify candidate locations for running and cycling, and compare them in a series of controlled experiments with athletes. A purpose-built prototype measures speed of interaction and gives feedback cues for athletes to report the physical effects on the activity itself. Our results suggest that specific locations are faster and have minimal disruption to movement, even under induced fatigue conditions. The overall method is fast - 1.31s for running and 1.65s for cycling. Preferred locations differ significantly across sports, with stable body parts ranking higher. We effectively demonstrated the use of a single hand for interaction during running with two distinct tap gestures. A set of guidelines inform the design of new sports technologies.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {28},
numpages = {9},
keywords = {sports training, cycling, on-body interaction, interaction in motion, running},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206529,
author = {Garcia, Jose F. and Simeone, Adalberto L. and Higgins, Matthew and Powell, Wendy and Powell, Vaughan},
title = {Inside Looking out or Outside Looking in? An Evaluation of Visualisation Modalities to Support the Creation of a Substitutional Virtual Environment},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206529},
doi = {10.1145/3206505.3206529},
abstract = {Current Virtual Reality systems only allow users to draw a rectangular perimeter to mark the room-scale area they intend to use. Domestic environments can include furniture and other obstacles that hinder the ease with which users can naturally walk. By leveraging the benefits of passive haptics, users can match physical objects with virtual counterparts, to create substitutional environments. In this paper we explore two visualisation modalities to aid in the creation of a coarse virtual representation of the physical environment, by marking out the volumes of space where physical obstacles are located, to support the substitution process. Our study investigates whether this process is better supported by an inside-looking-out 3D User Interface (that is, viewing the outside world while immersed in Virtual Reality) or from an outside-looking-in one (while viewing the Virtual Environment through an external device, such as a tablet). Results show that the immersive option resulted in better accuracy and was the one with the highest overall preference ratings.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {29},
numpages = {8},
keywords = {virtual environments, substitutional reality, virtual reality},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206554,
author = {Mollashahi, Ehsan Sotoodeh and Uddin, Md. Sami and Gutwin, Carl},
title = {Improving Revisitation in Long Documents with Two-Level Artificial-Landmark Scrollbars},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206554},
doi = {10.1145/3206505.3206554},
abstract = {Document readers with linear navigation controls do not work well when users need to navigate to previously-visited locations, particularly when documents are long. Existing solutions - bookmarks, search, history, and read wear - are valuable but limited in terms of effort, clutter, and interpretability. In this paper, we investigate artificial landmarks as a way to improve support for revisitation in long documents - inspired by visual augmentations seen in physical books such as coloring on page edges or indents cut into pages. We developed several artificial-landmark visualizations that can represent locations even in documents that are many hundreds of pages long, and tested them in studies where participants visited multiple locations in long documents. Results show that providing two columns of landmark icons led to significantly better performance and user preference. Artificial landmarks provide a new mechanism to build spatial memory of long documents - and can be used either alone or with existing techniques like bookmarks, read wear, and search.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {30},
numpages = {9},
keywords = {revisitation, scrollbars, spatial memory, artificial landmarks},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206531,
author = {Burd, Randy and Espy, Kimberly Andrews and Hossain, Md Iqbal and Kobourov, Stephen and Merchant, Nirav and Purchase, Helen},
title = {GRAM: Global Research Activity Map},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206531},
doi = {10.1145/3206505.3206531},
abstract = {The Global Research Activity Map (GRAM) is an interactive web-based system for visualizing and analyzing worldwide scholarship activity as represented by research topics. The underlying data for GRAM is obtained from Google Scholar academic research profiles and is used to create a weighted topic graph. Nodes correspond to self-reported research topics and edges indicate co-occurring topics in the profiles. The GRAM system supports map-based interactive features, including semantic zooming, panning, and searching. Map overlays can be used to compare human resource investment, displayed as the relative number of active researchers in particular topic areas, as well scholarly output in terms of citations and normalized citation counts. Evaluation of the GRAM system, with the help of university research management stakeholders, reveals interesting patterns in research investment and output for universities across the world (USA, Europe, Asia) and for different types of universities. While some of these patterns are expected, others are surprising. Overall, GRAM can be a useful tool to visualize human resource investment and research productivity in comparison to peers at a local, regional and global scale. Such information is needed by university administrators to identify institutional strengths and weaknesses and to make strategic data-driven decisions.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {31},
numpages = {9},
keywords = {knowledge discovery, topics map, interactive visualization system},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206544,
author = {Rekimoto, Jun and Uragaki, Keishiro and Yamada, Kenjiro},
title = {Behind-the-Mask: A Face-through Head-Mounted Display},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206544},
doi = {10.1145/3206505.3206544},
abstract = {A head-mounted display (HMD), which is common in virtual reality (VR) systems, normally hides the user's face. This feature prohibits to realize a face-to-face communication, in which two or more users share the same virtual space, or show a participant's face on a surrogate-robot's face when the user remotely connects to the robot through an HMD for tele-immersion. Considering that face-to-face communication is one of the fundamental requirements of real-time communications, and is widely realized and used by many nonVR telecommunication systems, an HMD's face hiding feature is considered to be a serious problem and limits the possibility of VR. To address this issue, we propose the notion of "Face-through HMD" and present a face-capturing HMD configuration called "Behind-the-Mask" with infrared (IR) cut filters and side cameras that can be attached to existing HMDs. As an IR cut filter only reflects infrared light and transmits visible light, it is transparent to the user's eye but reflects the user's face with infrared lights. By merging a prescanned 3D face model of the user with the face image obtained from our HMD, the 3D face model of the user with eyes and mouth movement can be reconstructed. We consider that our proposed HMD can be used in many VR applications.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {32},
numpages = {5},
keywords = {virtual reality, facial image, head-mounted display (HMD), elecommunication, face-through HMD, avatar},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206546,
author = {Katsini, Christina and Raptis, George E. and Fidas, Christos and Avouris, Nikolaos},
title = {Does Image Grid Visualization Affect Password Strength and Creation Time in Graphical Authentication?},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206546},
doi = {10.1145/3206505.3206546},
abstract = {Nowadays, technological advances introduce new visualization and user interaction possibilities. Focusing on the user authentication domain, graphical passwords are considered a better fit for interaction environments which lack a physical keyboard. Nonetheless, the current graphical user authentication schemes are deployed in conventional layouts, which introduce security vulnerabilities associated with the strength of the user selected passwords. Aiming to investigate the effectiveness of advanced visualization layouts in selecting stronger passwords, this paper reports a between-subject study, comparing two different design layouts a two-dimensional and a three dimensional. Results provide evidence that advanced visualization techniques provide a more suitable framework for deploying graphical user authentication schemes and underpin the need for considering such techniques for providing assistive and/or adaptive mechanisms to users aiming to assist them to create stronger graphical passwords.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {33},
numpages = {5},
keywords = {graphical password strength, graphical passwords, image grid, recognition-based graphical authentication, usable security},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206520,
author = {Tanaka, Kyosuke and Tochihara, Naoya and Sato, Toshiki and Koike, Hideki},
title = {A Real-Time Image Processing Framework with an Aerial Overhead Camera for Sports},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206520},
doi = {10.1145/3206505.3206520},
abstract = {Recently, large horizontal interactive surfaces have begun to be developed. In these systems, an overhead camera is often used to detect the position of objects on the surface even if they are not in contact with the surface. However the issues that these systems face are that they are expensive and a camera cannot be easily attached on top of the surface in some situations. This paper proposes a framework that uses a camera on a drone (UAV) as an overhead camera unit to convert arbitrary horizontal rectangular regions into interactive surfaces. Although commercially available drones that are equipped with cameras have high latencies and are difficult to use in real-time interactive systems, we solved this latency issue using a small PC that performs primitive image processing tasks onboard. First, we describe a drone unit that has an infrared camera and a small PC for real-time image processing, such as surface detection and object detection. Second, we describe novel infrared markers for the robust detection of the four corners of a rectangular region and the objects within that region. Finally, we describe an interactive sports coaching application in which a drone unit is used as an overhead camera both for a large playing field and small tabletop.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {34},
numpages = {5},
keywords = {interactive surface, drone, unmanned aerial vehicle, infrared marker, real-time image processing, hexacopter},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206508,
author = {Bellucci, Andrea and Ruiz, Alberto and D\'{\i}az, Paloma and Aedo, Igancio},
title = {Investigating Augmented Reality Support for Novice Users in Circuit Prototyping},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206508},
doi = {10.1145/3206505.3206508},
abstract = {Building an electronic circuit is an error-prone activity for novice users; many errors can occur, such as incorrect wirings or wrong component values. This work explores the use of Augmented Reality (AR) as a technology to mitigate the issues that arise when users construct circuits. We present a study that investigates the effectiveness, usability, and cognitive load of AR visual instructions for circuit prototyping tasks. A mobile-based, window-on-the-world AR tool is compared to traditional media such as paper-based or monitor-displayed electronic drawings. Results show that superimposing components and instructions through AR reduces the number of errors, allows users to easily troubleshoot them and reduces users' mental workload.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {35},
numpages = {5},
keywords = {augmented reality, physical computing, usability study},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206555,
author = {Balducci, Fabrizio and Buono, Paolo},
title = {Building a Qualified Annotation Dataset for Skin Lesion Analysis Trough Gamification},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206555},
doi = {10.1145/3206505.3206555},
abstract = {The deep learning approach has increased the quality of automatic medical diagnoses at the cost of building qualified datasets to train and test such supervised machine learning methods. Image annotation is one of the main activity of dermatologists and the quality of annotation depends on the physician experience and on the number of studied cases: manual annotations are very useful to extract features like contours, intersections and shapes that can be used in the processes of lesion segmentation and classification made by automatic agents. This paper proposes the design of an interactive multimedia platform that enhance the annotation process of medical images, in the domain of dermatology, adopting gamification and "games with a purpose" (GWAP) strategies in order to improve the engagement and the production of qualified datasets also fostering their sharing and practical evaluation. A special attention is given to the design choices, theories and assumptions as well as the implementation and technological details.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {36},
numpages = {5},
keywords = {gamification, GWAP, dermatology, machine learning, annotation},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206551,
author = {Vitiello, Giuliana and Sebillo, Monica},
title = {The Importance of Empowerment Goals in Elderly-Centered Interaction Design},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206551},
doi = {10.1145/3206505.3206551},
abstract = {This paper deals with the problem of empowering elderly people through adequate interactive applications. We describe a user-centered iterative design methodology which is oriented towards empowerment goals. We present the results of a preliminary study that has involved the project work activities performed by groups of computer science students from the course of human-computer interaction. The projects cover different areas of elderly people's life and the adopted approach has provided interesting insight into the empowerment goals which prevail in each area. The results of the study suggest that, especially when dealing with categories of disadvantaged users, empowerment goals are paramount to gain true usability of a given application.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {37},
numpages = {5},
keywords = {human empowerment factors, user-centered participatory design, requirements engineering, user experience, elderly user},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206516,
author = {Ardissono, Liliana and Delsanto, Matteo and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Scanu, Daniele},
title = {Map-Based Visualization of 2D/3D Spatial Data via Stylization and Tuning of Information Emphasis},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206516},
doi = {10.1145/3206505.3206516},
abstract = {In Geographical Information search, map visualization can challenge the user because results can consist of a large set of heterogeneous items, increasing visual complexity. We propose a novel visualization model to address this issue. Our model represents results as markers, or as geometric objects, on 2D/3D layers, using stylized and highly colored shapes to enhance their visibility. Moreover, the model supports interactive information filtering in the map by enabling the user to focus on different data categories, using transparency sliders to tune the opacity, and thus the emphasis, of the corresponding data items. A test with users provided positive results concerning the efficacy of the model.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {38},
numpages = {5},
keywords = {2D/3D geographical maps, search results visualization, opacity tuning, visual information filtering},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206550,
author = {Gelderblom, Helene and Menge, Leanne},
title = {The Invisible Gorilla Revisited: Using Eye Tracking to Investigate Inattentional Blindness in Interface Design},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206550},
doi = {10.1145/3206505.3206550},
abstract = {Interface designers often use change and movement to draw users' attention. Research on change blindness and inattentional blindness challenges this approach. In Simons and Chabris' 1999, "Gorillas in our midst" experiment, they showed how people that are focused on a task are likely to miss the occurrence of an unforeseen event (a man in a gorilla suit in their case), even if it appears in their field of vision. This relates to interface design because interfaces often include moving elements such as rotating banners or advertisements, which designers obviously want users to notice. We investigated how inattentional blindness affect users' perception through an eye tracking investigation on Simons and Chabris' video as well as on the web site of an airline that uses a rotating banner to advertise special deals. In both cases users performed tasks that required their full attention and were then interviewed to determine to what extent they perceived the changes or new information. We compared the results of the two experiments to see how Simons and Chabris' theory applies to interface design. Our findings show that although 43% of the participants had fixations on the gorilla, only 22% said that they noticed it. On the web site, 75% of participants had fixations on the moving banner but only 33% could recall any information related to it. We offer reasons for these results and provide designers with advice on how to address the effect of inattentional blindness and change blindness in their designs.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {39},
numpages = {9},
keywords = {selective disregard, eye tracking, change blindness, inattentional blindness, interface design, perceptual load},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206548,
author = {Humayoun, Shah Rukh and Hasan, Syed Moiz and AlTarawneh, Ragaad and Ebert, Achim},
title = {Visualizing Software Hierarchy and Metrics over Releases},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206548},
doi = {10.1145/3206505.3206548},
abstract = {Analysis and understanding of large software systems requires exploring not only the software structure but also associated software metrics over the development releases. Information visualization helps in this regard greatly through interactive visualizations in comparison to exploring these through the source code or traditional software diagrams like UML diagrams. In this paper, we present our developed visualization tool, called HiMVis, that visualizes software hierarchies and metrics through multi-views visualizations on the same screen. HiMVis visualizes packages and class hierarchies through two space-filling interactive layouts. Further, it shows on demand through multiple views more than fifty software metrics information associated to a particular class or interface over all development releases. We provide a number of interaction and filtering options in the tool to make the exploration of the underlying software system more intuitive. We also conducted a brief user study with 10 participants to determine the usability of the developed tool.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {40},
numpages = {5},
keywords = {software architecture, information visualization, software metrics},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206506,
author = {Paul, Celeste Lyn and Bradel, Lauren},
title = {Size Matters: The Effects of Interactive Display Size on Interaction Zone Expectations},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206506},
doi = {10.1145/3206505.3206506},
abstract = {The goal of our research was to understand the effects of display size on interaction zones as it applies to interactive systems. Interaction zone models for interactive displays are often static and do not consider the size of the display in their definition. As the interactive display ecosystem becomes more size diverse, current models for interaction are limited in their applicability. This paper describes the results of an exploratory study in which participants interacted with and discussed expectations with interactive displays ranging from personal to wall-sized. Our approach was open-ended rather than grounded in existing interaction zone models in order to explore potential differences in interaction zones and distances. We found that the existence of different interaction zones and the distance at which these zones are relevant are dependent on display size. In discussion of the results, we explore implications of our findings and offer guidelines for the design of interactive display systems.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {41},
numpages = {5},
keywords = {interaction zone, proxemics, embodied interaction, large displays},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206547,
author = {Sheehan, Shane and Masoodian, Masood and Luz, Saturnino},
title = {COMFRE: A Visualization for Comparing Word Frequencies in Linguistic Tasks},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206547},
doi = {10.1145/3206505.3206547},
abstract = {Comparing frequency distributions is a basic task in statistics and in disciplines that rely on statistical analysis, such as corpus linguistics. However, support for comparing word frequencies between different corpora in corpus linguistics tasks such as lexical analysis and corpus-based translation studies, is often limited to fairly basic techniques like tabular word lists. While other visualizations such as word clouds do exist, they are not widely used in linguistic analysis tasks due to their lack of precision, unsuitability to dealing with the high frequencies of common words, and lack of effective mechanisms for direct manipulation. In this paper, we propose a visualization for comparing word frequencies across two corpora using a combination of slope charts and histogram contours. An interactive implementation of this visualization is also presented. The design of visualization, and the development of the prototype, have been guided through the involvement of expert linguist users.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {42},
numpages = {5},
keywords = {word clouds, set frequencies, frequency comparisons, histograms, slope charts, word lists, linguistics},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206519,
author = {Thompson, John and Srinivasan, Arjun and Stasko, John},
title = {Tangraphe: Interactive Exploration of Network Visualizations Using Single Hand, Multi-Touch Gestures},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206519},
doi = {10.1145/3206505.3206519},
abstract = {Touch-based displays are becoming a popular medium for interacting with visualizations. Network visualizations are a frequently used class of visualizations across domains to explore entities and relationships between them. However, little work has been done in exploring the design of network visualizations and corresponding interactive tasks such as selection, browsing, and navigation on touch-based displays. Network visualizations on touch-based displays are usually implemented by porting the conventional pointer based interactions as-is to a touch environment and replacing the mouse cursor with a finger. However, this approach does not fully utilize the potential of naturalistic multi-touch gestures afforded by touch displays. We present a set of single hand, multi-touch gestures for interactive exploration of network visualizations and employ these in a prototype system, Tangraphe. We discuss the proposed interactions and how they facilitate a variety of commonly performed network visualization tasks including selection, navigation, adjacency-based exploration, and layout modification. We also discuss advantages of and potential extensions to the proposed set of one-handed interactions including leveraging the non-dominant hand for enhanced interaction, incorporation of additional input modalities, and integration with other devices.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {43},
numpages = {5},
keywords = {network visualization, multi-touch interaction, interaction design},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206530,
author = {Tiab, John and Boring, Sebastian and Strohmeier, Paul and Markussen, Anders and Alexander, Jason and Hornb\ae{}k, Kasper},
title = {Tiltstacks: Composing Shape-Changing Interfaces Using Tilting and Stacking of Modules},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206530},
doi = {10.1145/3206505.3206530},
abstract = {Many shape-changing interfaces use an array of actuated rods to create a display surface; each rod working as a pixel. However, this approach only supports pixel height manipulation and cannot produce more radical shape changes of each pixel (and thus of the display). Examples of such changes include non-horizontal pixels, pixels that overhang other pixels, or variable gaps between pixels. We present a concept for composing shape-changing interfaces by vertically stacking tilt-enabled modules. Together, stacking and tilting allow us to create a more diverse range of display surfaces than using arrays. We demonstrate this concept through TiltStacks, a shape-changing prototype built using stacked linear actuators and displays. Each tiltable module provides three degrees of freedom (z-movement, roll, and pitch); two more degrees of freedom are added through stacking modules (i.e., planar x- and y-movement).},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {44},
numpages = {5},
keywords = {stacking, shape-changing interfaces, tilting, compositional concept},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206528,
author = {Trullemans, Sandra and Ebrahimi, Payam and Signer, Beat},
title = {Crossing Spaces: Towards Cross-Media Personal Information Management User Interfaces},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206528},
doi = {10.1145/3206505.3206528},
abstract = {Nowadays, digital and paper documents are used simultaneously during daily tasks. While significant research has been carried out to support the re-finding of digital documents, less effort has been made to provide similar functionality for paper documents. In this paper, we present a solution that enables the design of cross-media Personal Information Management (PIM) user interfaces helping users in re-finding documents across digital and physical information spaces. We propose three main design requirements for the presented cross-media PIM user interfaces. Further, we illustrate how these design requirements have been applied in the development of three proof-of-concept applications and describe a software framework supporting the design of these interfaces. Finally, we discuss opportunities for future improvements of the presented cross-media PIM user interfaces.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {45},
numpages = {5},
keywords = {prosthetic memory, paper-digital interaction, cross-media PIM},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206553,
author = {Sassatelli, Lucile and Pinna-D\'{e}ry, Anne-Marie and Winckler, Marco and Dambra, Savino and Samela, Giuseppe and Pighetti, Romaric and Aparicio-Pardo, Ramon},
title = {Snap-Changes: A Dynamic Editing Strategy for Directing Viewer's Attention in Streaming Virtual Reality Videos},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206553},
doi = {10.1145/3206505.3206553},
abstract = {Cinematic Virtual Reality (VR) has the potential of touching the masses with new exciting experiences, but faces two main hurdles: one is the ability to stream these videos, another is their design and creation. Indeed, rates are much higher and in addition to discomfort and sickness that might arise in fully immersive experience with a headset, users might get lost when exploring a 360° videos and miss main elements required to understand the underlying plot. We take an innovative approach by addressing jointly the creation and streaming problems. We introduce a technique called snap-changes, aimed at directing viewers to points of interest pre-defined by the content producer. We design a VR editing tool and a custom 360° video player, to provide the content creator with the ability to drive the user's attention, and report results from two sets of user experiments that indicate that snap-changes indeed help reduce user's head motion.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {46},
numpages = {5},
keywords = {360° video, streaming, video editing, attention driving, user studies},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206517,
author = {Azuma, Kayo and Koike, Hideki},
title = {A Study on Gaze Guidance Using Artificial Color Shifts},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206517},
doi = {10.1145/3206505.3206517},
abstract = {In Web or digital signage, content providers want to guide users' attention to the intended regions. Using active visual stimuli, such as animated or flashing objects, is effective for gaze guidance; however, it has been reported that such an approach often results in unpleasant feelings for users. This paper proposed a new method for gaze guidance using artificial color shifts that does not induce unpleasant feelings in the user. We created an image filter that separates the image in three layers, i.e., cyan, magenta, and yellow, and slightly shifted each layer the left, right, and down, respectively. The filter was applied to the entire image except the region where the user's gaze was to be guided. We conducted experiments using a gaze tracker. The experimental results showed that the proposed method can guide the user's gaze to a particular region with less unpleasant feelings.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {47},
numpages = {5},
keywords = {visual guidance, eye tracking, color shift, gaze guidance, gaze interaction},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206525,
author = {Gorisse, Geoffrey and Christmann, Olivier and Houzangbe, Samory and Richir, Simon},
title = {From Robot to Virtual Doppelganger: Impact of Avatar Visual Fidelity and Self-Esteem on Perceived Attractiveness},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206525},
doi = {10.1145/3206505.3206525},
abstract = {This paper presents the first study of a series of experiments aiming to investigate the impact of avatar visual fidelity on user experience with emphasis on the sense of embodiment in immersive virtual environments (IVE). This first experiment requires participants to evaluate three characters with several levels of visual fidelity: a robot, a suit and a virtual doppelganger. Statistical analyses were performed to assess the impact of truthfulness (degree of similarity between users and avatars) and self-esteem on perceived avatars' attractiveness. Our results demonstrate the positive impact of truthfulness on the evaluation of attractiveness and suggest a correlation between self-esteem and avatar selection.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {48},
numpages = {5},
keywords = {attractiveness, self-esteem, embodiment, avatar, virtual reality, doppelganger},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206513,
author = {Gennari, Rosella and Melonio, Alessandra and Rizvi, Mehdi},
title = {Investigating Class Conversations with Classtalk: A Study with Tangible Object Prototypes in a Primary School},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206513},
doi = {10.1145/3206505.3206513},
abstract = {Interactive tangible objects can help orchestrate conversations in school classes. If such tangibles are created with a meta-design approach, for the specific context of their users, they evolve according to their usage. Specifically, tangible object prototypes are created; prototypes are adopted by their users in ecological studies; their usage is reflected over by users and designers to investigate design possibilities, which are rapidly prototyped and again adopted by users. This paper reports on the meta-design and latest evolution of ClassTalk, a tangible for conversations in primary school classes. It shows how new design ideas emerged by making users adopt ClassTalk prototypes, and by moving designers into users' context.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {49},
numpages = {5},
keywords = {evolutionary prototyping, teachers, class, meta-design, conversation, children, tangible},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206571,
author = {Abbas, Ghazanfer and Humayoun, Shah Rukh and AlTarawneh, Ragaad and Ebert, Achim},
title = {Simple Shape-Based Touch Behavioral Biometrics Authentication for Smart Mobiles},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206571},
doi = {10.1145/3206505.3206571},
abstract = {One of the main concerns during usage of the current smart mobile devices in public is the vulnerability of password hacking by shoulder-suffering or smudge attack. The traditional user authentication techniques such as PIN code or patterns-based password are easy target of such attacks. In this paper, we propose using simple shapes (e.g., circle, triangle, etc.) to get users' touch behavioral biometrics data. The users are asked to draw over these shapes while the developed system extracts 25 different features (e.g., finger middle stroke and its pressure, velocity, mobile orientation, etc.) for the model training and authentication purpose. The proposed solution is simple for all kinds of users and could solve the problem of password hacking through shoulder-suffering or smudge attacks.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {50},
numpages = {3},
keywords = {biometrics authentication, machine learning application},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206576,
author = {Agate, Vincenzo and Gaglio, Salvatore},
title = {A Gesture Recognition Framework for Exploring Museum Exhibitions},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206576},
doi = {10.1145/3206505.3206576},
abstract = {In this paper we present a gesture recognition framework for providing the visitors of a museum exhibition with a non intrusive interface for the multimedia enjoyment of digital contents. Early experiments were carried out at the Computer History Museum Exhibition of the University of Palermo.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {51},
numpages = {3},
keywords = {human computer interaction, ambient intelligence, gesture recognition},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206564,
author = {Alborno, Paolo and Jakubowski, Kelly and Camurri, Antonio and Volpe, Gualtiero},
title = {A System to Support Non-IT Researchers in the Automated Analysis of Human Movement},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206564},
doi = {10.1145/3206505.3206564},
abstract = {Analysis of human movement data is a core topic of many research studies in human-human and human-computer interaction. Whereas, on the one side, automated movement analysis is often based on the application of sophisticated computer science techniques (e.g., motion tracking from video recordings), on the other side the interdisciplinary nature of research in this area requires the availability of tools that can be used by researchers who may not have an advanced computer science expertise. This paper presents a system enabling users, who are not necessarily computer scientists, to perform motion tracking from a dataset of video recordings. The system - consisting of a set of (freely downloadable) tools accessible by means of user friendly graphical interfaces - was designed, developed, and tested in the context of a project for automated analysis of entrainment in ensemble music performance, following the needs and requirements of musicologists and psychologists.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {52},
numpages = {3},
keywords = {non-IT users, music performance, non-verbal communication, movement analysis},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206557,
author = {Andrienko, Gennady and Andrienko, Natalia},
title = {Creating Maps of Artificial Spaces to Explore Trajectories},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206557},
doi = {10.1145/3206505.3206557},
abstract = {We propose an approach to interactive visual exploration of trajectories of moving objects in which trajectories are mapped onto different coordinate systems enabling the analyst to look at different aspects of the movement. Geographic visualization techniques can be applied to these coordinate systems in the same way as in usual geographic map displays.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {53},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206558,
author = {Andrienko, Gennady and Andrienko, Natalia and Budziak, Guido and von Landesberger, Tatiana and Weber, Hendrik},
title = {Exploring Pressure in Football},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206558},
doi = {10.1145/3206505.3206558},
abstract = {From1 a set of trajectories of the players and the ball in a football (soccer) game, we computationally estimate, for each time frame, the pressure of the defending players upon the ball and the opponents. The extracted pressure relationships are visualized in detailed and summarized forms. Interactive filtering enables exploration of the pressure relationships in selected game episodes or in game situations satisfying specific query conditions..},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {54},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206579,
author = {Angelini, Marco and Blasilli, Graziano and Lenti, Simone and Santucci, Giuseppe},
title = {Visual Exploration and Analysis of the Italian Cybersecurity Framework},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206579},
doi = {10.1145/3206505.3206579},
abstract = {In the last years, several standards and frameworks have been developed to help organizations to increase the security of their Information Technology (IT) systems. In order to deal with the continuous evolution of the cyber-attacks complexity, such solutions have to cope with an overwhelming set of concepts, and are perceived as complex and hard to implement. The exploration of the cyber-security state of an organization can be made more effective and proficient if supported by the right level of automation. This paper presents the implementation of a visual analytics solution, called CybeR secUrity fraMework BrowSer (CRUMBS) [2], targeted at dealing with the Italian Adaptation of the Cyber Security Framework (IACSF), derived by the National Institute of Standards and Technology (NIST) proposal [1], adaptation that, in its full complexity, presents the security managers with hundreds of scattered concepts, like functions, categories, subcategories, priorities, maturity levels, current and target profiles, and controls, making its adoption a complex activity. The prototype is available at: http://awareserver.dis.uniroma1.it:11768/crumbs/.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {55},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206566,
author = {Ardissono, Liliana and Delsanto, Matteo and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Scanu, Daniele},
title = {Transparency-Based Information Filtering on 2D/3D Geographical Maps},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206566},
doi = {10.1145/3206505.3206566},
abstract = {The presentation of search results in GIS can expose the user to cluttered geographical maps, challenging the identification of relevant information. In order to address this issue, we propose a visualization model supporting interactive information filtering on 2D/3D maps. Our model is based on the introduction of transparency sliders that enable the user to tune the opacity, and thus the emphasis, of data categories in the map. In this way, he or she can focus the maps on the most relevant types of information for the task to be performed. A test with users provided positive results concerning the efficacy of our model.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {56},
numpages = {3},
keywords = {search results visualization, visual information filtering, opacity tuning, 2D/3D geographical maps},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206587,
author = {Caivano, Danilo and Cassano, Fabio and Lanzilotti, Rosa and Piccinno, Antonio},
title = {Towards an IoT Model for the Assessment of Smart Devices},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206587},
doi = {10.1145/3206505.3206587},
abstract = {The current Internet of Things (IoT) market proposes a wide variety of devices with complex design and different functionality. In addition, the same IoT device can be used in different domains, from home to industry, to healthcare. The management of such devices occurs in different ways, for example through visual interaction using high level programming languages (e.g. Event-Condition-Action rules) or through high level API. Generally, end users are not technical experts and are not able to configure their IoT devices, thus they need external tools (or visual interaction paradigm) to exploit and better control them. In this work, we present a model for IoT devices which allows to assess those devices and their suitability for a certain domain according to four dimensions: communication, target, data manipulation and development. The model aims at better understanding the device capabilities and, consequently, facilitating the choice of the devices that better suit the domain in which they should be used.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {57},
numpages = {3},
keywords = {IoT, smart device, IoT model},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206582,
author = {Chesta, Cristina and Corcella, Luca and Manca, Marco and Patern\`{o}, Fabio and Santoro, Carmen},
title = {Trigger-Action Programming for Context-Aware Elderly Support in Practice},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206582},
doi = {10.1145/3206505.3206582},
abstract = {Remote monitoring services should be strongly personalised to the specific needs, preferences, abilities and motivations of elderly, a population segment whose characteristics can largely vary and even dynamically evolve over time for the same individual, depending on changing needs and usage contexts. We present a demo showing how a platform supporting End User Development (EUD) of context-dependent applications has been customized for remotely assisting elderly people at home. The user-editable personalisation features are specified by using trigger-action rules. The platform has been integrated with various sensors and appliances, and an application for remotely monitoring older adults at home. The resulting environment supports the possibility of creating trigger-action rules that can actually be executed when relevant sensors indicate that specific events or conditions occurred.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {58},
numpages = {3},
keywords = {trigger-action programming, elderly, EUD},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206562,
author = {Desolda, Giuseppe and Malizia, Alessio and Turchi, Tommaso},
title = {A Tangible-Programming Technology Supporting End-User Development of Smart-Environments},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206562},
doi = {10.1145/3206505.3206562},
abstract = {In recent years, smart objects are increasingly pervading the environments we live in. For HCI researchers, an important challenge is how non-technical users can establish the behavior of such devices. This poster presents a new technology implementing a tangible-programming paradigm, which allows non-programmers to synchronize the behavior of ecologies of smart objects, thus determining the creation and customization of smart environments.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {59},
numpages = {3},
keywords = {internet of thing, tangible programming, end-user development},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206559,
author = {Dubois, Emmanuel and Celentano, Augusto},
title = {Interaction Modularity in Multi-Device Systems: A Conceptual Approach},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206559},
doi = {10.1145/3206505.3206559},
abstract = {In this paper we propose a conceptual view on the modularization of multi-device interactive systems. Our view is supported by the re-visitation and adaptation of some software engineering concepts. We provide the definition of interaction module and its interface and discuss how these concepts contribute to the design of such systems.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {60},
numpages = {3},
keywords = {multi-device, interaction interface, interaction module},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206595,
author = {Etchart, Mariano and Caprarelli, Alessandro},
title = {A Wearable Immersive Web-Virtual Reality Approach to Remote Neurodevelopmental Disorder Therapy},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206595},
doi = {10.1145/3206505.3206595},
abstract = {Our research exploits the learning potential of Wearable Immersive Virtual Reality (WIVR) applied to children with neurodevelopmental disorders (NDD), particularly autism spectrum disorder. We introduce Be Trendy, a novel WIVR application that utilizes the benefits of immersive virtual reality to improve and challenge the cognitive capabilities of children such as learning, attention-span, memory and social skills. Two significant features of this application, modularity and remoteness, are highlighted and we assess its value and how it may be embedded with current NDD interventions. In this paper we evaluate the current state of art, present our solution and finally we offer some suggestions for how this project can be taken further.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {61},
numpages = {3},
keywords = {immersion, virtual reality, web-based interactions, disabilities},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206594,
author = {Ferro, Lauren S. and Marrella, Andrea},
title = {VERTO: A Visual Notation for Declarative Process Models},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206594},
doi = {10.1145/3206505.3206594},
abstract = {Declarative approaches to business process modeling allow to represent loosely-structured (declarative) processes in flexible scenarios as a set of constraints on the allowed flow of activities. However, current graphical notations for declarative processes are difficult to interpret. As a consequence, this has affected widespread usage of such notations, by increasing the dependency on experts to understand their semantics. In this paper, we tackle this issue by introducing a novel visual declarative notation targeted to a more understandable modeling of declarative processes.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {62},
numpages = {3},
keywords = {declarative process, visual notation, business process modeling},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206592,
author = {Guchev, Vladimir and Buono, Paolo and Gena, Cristina},
title = {Towards Intelligible Graph Data Visualization Using Circular Layout},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206592},
doi = {10.1145/3206505.3206592},
abstract = {Polar coordinates have been widely used in various techniques of interactive data visualization. The spatial organization through circular and radial layouts is implemented in a wide range of statistical charts and plots and is applicable for space-filling techniques and for node-link-group diagrams. Different arrangements of dots, lines and areas in polar coordinates create grids for data distribution, aggregation and linking.This work is devoted to the study of visual notations of data and their relationships and proposes an outline of their application in designing node-link-group diagrams, in order to arrange the geometric solutions at functional and logical levels of the visual representation.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {63},
numpages = {3},
keywords = {node-link-group diagram, guidelines, graph drawing, circular and radial layout, data visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206585,
author = {Hube, Natalie and M\"{u}ller, Mathias and Groh, Rainer},
title = {Facilitating Exploration on Exhibitions with Augmented Reality},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206585},
doi = {10.1145/3206505.3206585},
abstract = {At exhibitions, visitors are usually in a completely unknown environment. Although visitors generally are informed about the topic before a visit, interests are still difficult to extract from the mass of exhibition stands and offers. In this paper we describe a concept using head-coupled AR together with recommender mechanisms for exhibitions. We present a conceptual development for a first prototype with focus on navigational aspects as well as explicit and implicit recommendations to generate input data for visually displayed recommendations.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {64},
numpages = {3},
keywords = {information visualization, augmented reality, human computer interaction, recommender systems},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206570,
author = {Humayoun, Shah Rukh and Bhambri, Kritika and AlTarawneh, Ragaad},
title = {BiD-Chord: An Extended Chord Diagram for Showing Relations between Bi-Categorical Dimensional Data},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206570},
doi = {10.1145/3206505.3206570},
abstract = {In this paper, we propose an extension to the Chord diagram to show the relations between categorical values in two dimensions. We divide the Chord diagram into two equal slices, where the nodes in each slice represent the categorical variables in one dimension. The chords or relations go from nodes of one slice to the nodes of other slice to show the values of these categorical variables with respect to the categorical values in the other dimension. The proposed solution takes less space compared to using multiple charts for such scenarios.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {65},
numpages = {3},
keywords = {categorical data type, chord diagram},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206556,
author = {Kammer, Dietrich and Keck, Mandy and Gr\"{u}nder, Thomas and Groh, Rainer},
title = {Big Data Landscapes: Improving the Visualization of Machine Learning-Based Clustering Algorithms},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206556},
doi = {10.1145/3206505.3206556},
abstract = {With the internet, massively heterogeneous data sources need to be understood and classified to provide suitable services to users such as content observation, data exploration, e-commerce, or adaptive learning environments. The key to providing these services is applying machine learning (ML) in order to generate structures via clustering and classification. Due to the intricate processes involved in ML, visual tools are needed to support designing and evaluating the ML pipelines. In this contribution, we propose a comprehensive tool that facilitates the analysis and design of ML-based clustering algorithms using multiple visualization features such as semantic zoom, glyphs, and histograms.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {66},
numpages = {3},
keywords = {big data landscapes, glyphs, clustering, machine learning, visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206565,
author = {Kasper, Janine and Richter, Robert and Thalmann, Felix and Groh, Rainer},
title = {VISKOMMP: Graph Visualization Meets Meeting Documentation},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206565},
doi = {10.1145/3206505.3206565},
abstract = {In VISKOMMP (visual, collaborative, multi-meeting minutes system) we aim at supporting users during all stages of meeting-participation with focus on the preservation and accessibility of the produced information. For the efficient use of the knowledge generated during meetings, a comprehensive view of the aggregated data, independent of single events or documents, is necessary. An approach is presented which interlinks the heterogeneous information that is generated during meetings with the enterprise-knowledge. Created content and the established connections are further presented to the user in a comprehensible way. To this end, semantic technologies are utilized and an own ontology is designed, which covers the domains of project-management and meetings.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {67},
numpages = {3},
keywords = {visualization, ontologies, semantic processing, smart meeting systems, knowledge management, organizational management information systems},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206567,
author = {Keck, Mandy and Groh, Rainer},
title = {Towards a Construction Kit for Visual Search Interfaces},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206567},
doi = {10.1145/3206505.3206567},
abstract = {In recent years, many novel approaches have been propsed to exploring complex data sets. However, little guidance is available for designers to create similar solutions and to reuse established patterns. This paper is building upon our previous work that covers the development of a construction kit to support designers in creating new visual search interfaces. It provides a set of elements and patterns that can be easily combined with each other. In this paper, we present different application scenarios for using the construction kit within the design process.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {68},
numpages = {3},
keywords = {design patterns, product search, information visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206583,
author = {Langner, Ricardo and Horak, Tom and Dachselt, Raimund},
title = {Demonstrating Vistiles: Visual Data Exploration Using Mobile Devices},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206583},
doi = {10.1145/3206505.3206583},
abstract = {We demonstrate the prototype of the conceptual VisTiles framework. VisTiles allows exploring multivariate data sets by using multiple coordinated views that are distributed across a set of mobile devices. This setup allows users to benefit from dynamic and user-defined interface arrangements and to easily initiate co-located data exploration sessions. The current web-based prototype runs on commodity devices and is able to determine the spatial device arrangement by either a cross-device pinch gesture or an external tracking system. Multiple data sets are provided that can be explored by different visualizations (e.g., scatterplots, parallel coordinate plots, stream graphs). With this demonstration, we showcase the general concepts of VisTiles and discuss ideas for enhancements as well the potential for application cases beyond data analysis.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {69},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206575,
author = {Lee, Jun and Kim, Kyoung-Sook and Lee, Ryong and Lee, Sang-Hwan},
title = {Visual Insight of Spatiotemporal IoT-Generated Contents},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206575},
doi = {10.1145/3206505.3206575},
abstract = {The rapid evolution of the Internet of Things (IoT) and Big Data technology has been generating a large amount and variety of sensing contents, including numeric measured values (e.g., timestamps, geolocations, or sensor logs) and multimedia (e.g., images, audios, and videos). In analyzing and understanding heterogeneous types of IoT-generated contents better, data visualization is an essential component of exploratory data analyses to facilitate information perception and knowledge extraction. This study introduces a holistic approach of storing, processing, and visualizing IoT-generated contents to support context-aware spatiotemporal insight by combining deep learning techniques with a geographical map interface. Visualization is provided under an interactive web-based user interface to help the an efficient visual exploration considering both time and geolocation by easy spatiotemporal query user interface1.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {70},
numpages = {3},
keywords = {object detection, geovisualization, spatiotemporal analysis, deep learning, IoT-generated contents},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206577,
author = {Mancini, Maurizio and Varni, Giovanna},
title = {A Framework for Creative Embodied Interfaces},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206577},
doi = {10.1145/3206505.3206577},
abstract = {Creative joint activity is a form of real-time dynamic problem solving in which people collaborate to reach a common creative goal (e.g., to solve a mathematical problem, to improvise a piece of music, to write a novel, to sketch a story, and so on). While there exist interfaces able to produce social, emotional, communicative signals while collaborating with single human users to go through the creative process, the design of embodied interfaces able to observe and simultaneously effectively support creative joint activity with multiple human users is still an emerging research field. We define Creative Embodied Interfaces (CEIs) such interfaces, having either anthropomorphic or non-anthropomorphic aspect, and being either physically or virtually present in the real world. We argue that CEIs will enable a novel interaction paradigm that could be exploited in several fields such as science, education, health-care, arts, entertainment, social inclusion, companionship. This paper is aimed at providing definition and a first framework of CEIs combining psychological theories of creativity and computational models of social signal analysis/synthesis in avatars.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {71},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206560,
author = {Maruyama, Yusuke and Kono, Yasuyuki},
title = {Estimating Finger Postures by Attaching an Omnidirectional Camera to the Center of a User's Palm},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206560},
doi = {10.1145/3206505.3206560},
abstract = {This research describes the development of a system that estimates the natural postures of a user's fingers from the images captured by an omnidirectional video camera attached to the center of the user's palm in real time. The finger postures can be estimated by detecting the fingertips on each image and referring to the following preset information: the positional relationship between the camera and the user's fingers/fingertips, the length between the finger joints, and the interdependencies between the finger joints.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {72},
numpages = {3},
keywords = {finger posture, finger detection, omnidirectional camera},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206593,
author = {Masui, Toshiyuki},
title = {EpisoDAS: DAS-Based Password Generation Using Episodic Memories},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206593},
doi = {10.1145/3206505.3206593},
abstract = {We introduce a simple and powerful visual interaction technique for managing strong passwords. Passwords have been used for authentication for decades, but appropriate handling of passwords is difficult because people can easily forget passwords and they can be easily attacked. Better authentication methods have been investigated, and various visual interaction methods have been proposed, including the DAS (draw-a-secret) method. Using DAS, users can log into a service just by drawing a secret pattern on the screen, but remembering complex secret patterns is as difficult as remembering passwords. We developed EpisoDAS, with which users can generate strong passwords based on their secret episodic memories with a simple DAS interface. A user can draw a secret pattern and generate a password, based on their secret episodic memories that they cannot easily forget.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {73},
numpages = {2},
keywords = {visual passwords, passwords, user authentication, episodic memories, draw-a-secret, episoDAS, episopass},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206573,
author = {Matayoshi, Yasutsuna and Oshima, Ryo and Nakamura, Satoshi},
title = {Mojirage: Average Handwritten Note},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206573},
doi = {10.1145/3206505.3206573},
abstract = {Because some people appreciate their own handwritten characters blended with the handwritten characters of others, we propose a method for generating good handwriting by the real-time blending of users' handwritten characters with their own past handwritten characters or others' handwritten characters. We also realize the prototype system and show its usefulness.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {74},
numpages = {3},
keywords = {handwriting, electronic notebook, note taking},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206574,
author = {Matsuda, Kouhei and Nakamura, Satoshi},
title = {PhoToDo: Image-Based Task Management System by Visual Trigger},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206574},
doi = {10.1145/3206505.3206574},
abstract = {Many people manage their tasks using tools such as notebooks or personal task management applications in their smartphones. In fact, according to Microsoft's research, 78% of respondents in the United States currently have at least one task management app [1]. However, conventional task lists are sometimes troublesome because tasks usually need to be expressed in words. In addition, it takes time to understand tasks when they are described in words. However, it is known that a person can instantaneously process an image and has the ability to process many images at once [2][3]. Therefore, we propose a system called "PhoToDo" that enables people to use visual images to manage tasks. By using PhoToDo, users can instantly visualize all their tasks and efficiently manage them. In this paper, we propose and implement our system and show its effectiveness by conducting experimental tests.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {75},
numpages = {3},
keywords = {task management, personal information management, interaction design, visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206584,
author = {Mori, Giulio and Patern\`{o}, Fabio and Santoro, Carmen},
title = {Towards Understanding the Usability of Vibrotactile Support for Indoor Orientation},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206584},
doi = {10.1145/3206505.3206584},
abstract = {This study aims to understand the potential of using vibrotactile stimulation for indoor orientation in complex, unfamiliar buildings. Four vibrotactile prototypes have been analysed and tested in initial trials in order to investigate the benefits and the problems of each solution. The main goal of this study is to reach a better understanding of the design aspects that make a vibrotactile solution intuitive and effective.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {76},
numpages = {3},
keywords = {vibrotactile, orientation &amp; navigation, usability},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206590,
author = {Nezis, Angelos and Papageorgiou, Haris and Georgiadis, Pavlos and Jiskra, Petr and Pappas, Dimitris and Pontiki, Maria},
title = {Towards a Fully Personalized Food Recommendation Tool},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206590},
doi = {10.1145/3206505.3206590},
abstract = {We present1 a personalized ingredient-based Deep Learning recommender on the food domain that exploits ingredients and nutrition information to create recipe representations and propose to every user a more personalized and healthier meal. The recommender will be a critical component in our Meal Prediction Tool (MPT) designed with a focus on the personalization of services, increasing business efficiency and sustainability in the hospitality, restaurant and catering (HoReCa) industry.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {77},
numpages = {3},
keywords = {deep learning, food recommender, horeca industry},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206563,
author = {Panizzi, Emanuele},
title = {Keyboard with Tactile Feedback on Smartphone Touch Screen},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206563},
doi = {10.1145/3206505.3206563},
abstract = {Pressing buttons on a smartphone touch screen is difficult if you are not looking at the screen. We developed a numerical keyboard that provides a tactile feedback using phone short vibrations. The feedback is provided both when the user swipes the keyboard and when he presses keys. We describe how we implemented it on iPhone7, using the iPhone 3Dtouch capability and the UIFeedbackGenerator.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {78},
numpages = {3},
keywords = {tactile feedback, haptic feedback, iphone 3Dtouch, touch screen, keyboard},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206568,
author = {Rizzo, Nicola and Corti, Giancarlo and Guidi, Roberto},
title = {BUSt: Distributed User Interface for an IoT Framework},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206568},
doi = {10.1145/3206505.3206568},
abstract = {This paper describes a distributed user interface which enables people to use Things, intended as network enabled software or hardware resources, from multiple web-based GUIs. The demo will show a use case where lecturers can connect their smart phone to the system using a Progressive Web App to access data, run web applications or play multimedia files on a remote display.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {79},
numpages = {2},
keywords = {web interfaces, IoT, adaptive and context aware user interfaces, multitouch interaction, collaboration},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206569,
author = {Rogers, Bill and Masoodian, Masood and Apperley, Mark},
title = {A Virtual Cocktail Party: Supporting Informal Social Interactions in a Virtual Conference},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206569},
doi = {10.1145/3206505.3206569},
abstract = {Whilst the primary purpose of conferences is work --- formal exchange and sharing of information --- they almost always also include elements of play: informal social and entertainment elements, such as receptions, dinners, and tourism activities. These activities also provide the opportunity for 'serious' discussions, meeting people, and networking, and are an essential part of a good conference. Despite this, most virtual conferencing tools fail to provide support for such activities, instead focusing on austere goals related to saving money, time, and travel. This paper describes the concept of a Virtual Cocktail Party (VCP) tool to integrate into a virtual conference environment. In VCP the 'party' is presented as a mixture of individuals and small conversation groups 'circulating' at the virtual venue. Exploiting an automated speech-to-text system, words from conversations are shown in word-clouds displayed around conversation groups, sufficient to identify topics of conversation allowing participants to decide whether or not to join a group.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {80},
numpages = {3},
keywords = {virtual party, virtual conference, speech-to-text, speech recognition, conversation visualization, virtual socialization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206561,
author = {Rothe, Sylvia and Hu\ss{}mann, Heinrich},
title = {Spatial Statistics for Analyzing Data in Cinematic Virtual Reality},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206561},
doi = {10.1145/3206505.3206561},
abstract = {Cinematic Virtual Reality has been increasing in popularity over the last years. Watching 360° movies with head mounted displays, viewers can freely choose the direction of view, and thus the visible section of the movie. In order to explore the viewers' behavior, methods are needed for collecting and analyzing data. In our experiments we compare the viewing behavior for movies with spatial and non-spatial sound and tracked the head movements of the participants. This work-in-progress describes two approaches of spatial statistics - analysis of Space Time Cubes and Getis Ord Gi* statistic - for analyzing head tracking data.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {81},
numpages = {3},
keywords = {spatial sound, cinematic virtual reality, getis ord gi*, spatial statistics, space time cube, 360° movie},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206586,
author = {S\'{a}nchez-Francisco, M\'{o}nica and D\'{\i}az, Paloma and Aedo, Ignacio},
title = {Improving Urban Environment Awareness through Pervasive AR Games},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206586},
doi = {10.1145/3206505.3206586},
abstract = {Augmented Reality1 (AR) and Pervasive games can be combined to improve citizens awareness about their urban environment. While AR will provide additional information in situ, pervasive games can be used to implement playable cities that support informal learning about the environment through ludic activities performed physically in the same environment. In this paper, we present a prototype of an AR pervasive game application for an Android device that invites citizens to play and acquire knowledge about their city.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {82},
numpages = {3},
keywords = {pervasive games, augmented reality, civic engagement, playable city},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206580,
author = {Santos, Andr\'{e}s and Zarraonandia, Telmo and D\'{\i}az, Paloma and Aedo, Ignacio},
title = {A Virtual Reality Map Interface for Geographical Information Systems},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206580},
doi = {10.1145/3206505.3206580},
abstract = {The Virtual Reality (VR) technology offers new possibilities to implement Geographical Information Systems (GIS), allowing the user to visualize and interact with map interfaces in more natural and immersive way. Moreover, they can provide the user with a wide field of view similar to the one obtained when using large displays. In this paper, we present a VR application that allows the user to display and interact with a GIS map using different types of interaction modalities.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {83},
numpages = {3},
keywords = {map interfaces, geographical information systems, virtual reality},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206589,
author = {Sapio, Francesco and Marrella, Andrea and Catarci, Tiziana},
title = {Integrating Body Scanning Solutions into Virtual Dressing Rooms},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206589},
doi = {10.1145/3206505.3206589},
abstract = {The world is entering its 4th Industrial Revolution, a new era of manufacturing characterized by ubiquitous digitization and computing. One industry to benefit and grow from this revolution is the fashion industry, in which Europe (and Italy in particular) has long maintained a global lead. To evolve with the changes in technology, we developed the IT-SHIRT project. In the context of this project, a key challenge relies on developing a virtual dressing room in which the final users (customers) can virtually try different clothes on their bodies. In this paper, we tackle the aforementioned issue by providing a critical analysis of the existing body scanning solutions, identifying their strengths and weaknesses towards their integration within the pipeline of virtual dressing rooms.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {84},
numpages = {3},
keywords = {body scanning, IT-SHIRT, virtual dressing room},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206588,
author = {Mollashahi, Ehsan Sotoodeh and Uddin, Md. Sami and Gutwin, Carl},
title = {Two-Level Artificial-Landmark Scrollbars to Improve Revisitation in Long Documents},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206588},
doi = {10.1145/3206505.3206588},
abstract = {Navigating to previously-visited pages is a trivial yet fundamental task in linear control-based document viewers. These widgets e.g., scrollbars often do not work well particularly for long documents. Existing solutions try to tackle this issue with bookmarks, search, history, and read wear but limited in terms of effort, clutter, and interpretability. To improve the revisitation support in long documents, we investigated the use of artificial landmarks similar to the visual augmentations available in physical books: coloring on page edges or indents cut into pages. We developed several artificial-landmark visualizations to represent page-locations in the scrollbar for many hundreds of pages long documents, and tested them in studies where participants visited multiple locations in long documents. Results indicate that using two columns of landmark icons significantly improved revisitation performance and preferred by users. Our two-level artificial-landmark augmented scrollbars can be a new way to support spatial memory development of long documents - and can be used either in isolation or in congregation with current techniques.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {85},
numpages = {2},
keywords = {artificial landmarks, spatial memory, scrollbars, revisitation},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206578,
author = {Takada, Tetsuji and Abe, Takaaki},
title = {Emoji-Nized Log Browser: Visualization of Server-Logs by Emoji for System Administrators},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206578},
doi = {10.1145/3206505.3206578},
abstract = {We propose a log data visualization system by Emoji (pictorial symbol). A log inspection and its monitoring are essential task for system administrators to be aware of anomalous status of the IT systems and security incidents. Information visualization is a promising approach to assist the task. However, there is no visualization work to support understanding the textual data more directly. We, then, propose a text visualization by replacing text data with emoji based on a user-defined rule. In this paper, we explain an idea of visualization by emoji, and we also introduce a prototype system based on the idea for an access log of a web server. We also discuss about expected advantages and future work.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {86},
numpages = {3},
keywords = {emoji, log data, information visualization, system administrator, pictgraph},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206596,
author = {Vasilchenko, Anna and Wilde, Adriana and Snow, Stephen and Balaam, Madeline and Devlin, Marie},
title = {Video Coursework: Opportunity and Challenge for HCI Education},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206596},
doi = {10.1145/3206505.3206596},
abstract = {Human-Computer Interaction (HCI) is a challenging subject to study due to its highly multidisciplinary nature and the fast change of advancing technology. Keeping pace with these changes requires innovation in pedagogical approach, such as student-authored video, which is presented here. In case studies from two UK universities, students were assessed on video making. The results suggest increased student engagement and satisfaction, as well as acquisition of design skills taught in HCI, not typically taught elsewhere in computer science. Here we share our experiences of using this practice along with key challenges and some preliminary findings from analysis of the student artefact-creation process. We also outline future research directions in this space.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {87},
numpages = {3},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206591,
author = {Wittenburg, Kent and Lee, Teng-Yok},
title = {Equal-Height Treemaps for Multivariate Data},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206591},
doi = {10.1145/3206505.3206591},
abstract = {A well-known limitation of classic continuous treemaps is that they generally provide two (or at most a few) visual mappings for data variables apart from the hierarchical relationships. Typically, one variable maps to cell area; another maps to color. However, many data-centric tasks require human users to consider multiple variables simultaneously. The current work introduces the concept of equal-height, variable-width cells in treemaps, which affords the packing of multiple variables into the cell areas of the terminals of the hierarchy. We demonstrate how color and some largely width-invariant graphs can be utilized in the cell areas to add additional visual information in a multi-variate treemap. Examples come from machine learning and from finance applications.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {88},
numpages = {3},
keywords = {hierarchical data visualization, treemaps, multivariate data visualization, information visualization},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

@inproceedings{10.1145/3206505.3206572,
author = {Xu, Xingya and Shibata, Hirohito},
title = {A Multimodal Interface System to Support Drawing Diagrams in Talking},
year = {2018},
isbn = {9781450356169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206505.3206572},
doi = {10.1145/3206505.3206572},
abstract = {We propose a multimodal user interface system using pen and voice to draw diagrams, especially system configuration figures. We have built a system called TalkingDraw, which supports real time drawing in talking and does not interfere natural talking.},
booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
articleno = {89},
numpages = {3},
keywords = {multimedia interface, gesture recognition, interaction technique, speech recognition},
location = {Castiglione della Pescaia, Grosseto, Italy},
series = {AVI '18}
}

