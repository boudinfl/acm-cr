@inproceedings{10.1145/3399715.3400871,
author = {Burnett, Margaret},
title = {Doing Inclusive Design: From GenderMag in the Trenches to Inclusive Mag in the Research Lab},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400871},
doi = {10.1145/3399715.3400871},
abstract = {How can user interface and user experience (UI/UX) professionals assess whether their software supports diverse users? And if they find problems, how can they fix them? We begin this keynote address with a summary of GenderMag, a systematic inspection method for finding and fixing "gender inclusivity bugs---biases against different genders in software interfaces and workflows. We then show what UI/UX professionals are doing with it in the real world, from their bias finds &amp; fixes to their practices &amp; pitfalls in using it. Finally, we present InclusiveMag, a meta-method that can be used by HCI researchers to generate systematic inclusiveness methods for other dimensions of diversity.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {1},
numpages = {6},
keywords = {gender biases, Inclusive software, GenderMag},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400872,
author = {Palanque, Philippe},
title = {Ten Objectives and Ten Rules for Designing Automations in Interaction Techniques, User Interfaces and Interactive Systems},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400872},
doi = {10.1145/3399715.3400872},
abstract = {Automation, as a design goal, focusses mainly on the migration of tasks from a human operator to a mechanical or digital system. Designing automation thus usually consists in removing tasks or activities from that operator and in designing systems that will be able to perform them. When these automations are not adequately designed (or correctly understood by the operator), they may result in so called automation surprises [1], [2] that degrade, instead of enhance, the overall performance of the couple (operator, system). Usually, these tasks are considered at a high level of abstraction (related to work and work objectives) leaving unconsidered low-level, repetitive tasks. This paper proposes a decomposition of automation for interactive systems highlighting the diverse objectives it may target at. Beyond, multiple complementary views of automation for interactive systems design are presented to better define the multiform concept of automation. It provides numerous concrete examples illustrating each view and identifies ten rules for designing interactive systems embedding automations.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {2},
numpages = {10},
keywords = {User Interfaces, Interaction Techniques, Automation, Interactive Systems, Design Space},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400873,
author = {Schmidt, Albrecht},
title = {Interactive Human Centered Artificial Intelligence: A Definition and Research Challenges},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400873},
doi = {10.1145/3399715.3400873},
abstract = {Artificial Intelligence (AI) has become the buzzword of the last decade. Advances so far have been largely technical with a focus on machine learning (ML). Only recently have we begun seeing a shift towards focusing on the human aspects of artificial intelligence, centered on the narrow view of making AI interactive and explainable. In this paper I suggest a definition for "Interactive Human Centered Artificial Intelligence and outline the required properties. Staying in control is essential for humans to feel safe and have self-determination. Hence, we need to find ways for humans to understand AI based systems and means to allow human control and oversight. In our work, we argue that levels of abstractions and granularity of control are a general solution to this. Furthermore, it is essential that we make explicit why we want AI and what are the goals of AI research and development. We need to state the properties that we expect of future intelligent systems and who will benefit from a system or service. For me, AI and ML are very much comparable to raw materials (like stone, iron, or bronze). Historical periods are named after these materials as they fundamentally changed what humans can build and what tools humans can engineer. Hence, I argue that in the AI age we need to shift the focus from the material (e.g. the AI algorithms, as there will be plenty of material) towards the tools and infrastructures that are enabled which are beneficial to humans. It is apparent that AI will allow the automation of mental routine tasks and that it will extend our ability to perceive the world and foresee events. For me, the central question is how to create these tools for amplifying the human mind without compromising human values.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {3},
numpages = {4},
keywords = {Interactive Human Centered Artificial Intelligence, Human Computer Interaction, Artificial Intelligence},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399921,
author = {Aseniero, Bon Adriel and Perin, Charles and Willett, Wesley and Tang, Anthony and Carpendale, Sheelagh},
title = {Activity River: Visualizing Planned and Logged Personal Activities for Reflection},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399921},
doi = {10.1145/3399715.3399921},
abstract = {We present Activity River, a personal visualization tool which enables individuals to plan, log, and reflect on their self-defined activities. We are interested in supporting this type of reflective practice as prior work has shown that reflection can help people plan and manage their time effectively. Hence, we designed Activity River based on five design goals (visualize historical and contextual data, facilitate comparison of goals and achievements, engage viewers with delightful visuals, support authorship, and enable flexible planning and logging) which we distilled from the Information Visualization and Human-Computer Interaction literature. To explore our approach's strengths and limitations, we conducted a qualitative study of Activity River using a role-playing method. Through this qualitative exploration, we illustrate how our participants envisioned using our visualization to perform dynamic and continuous reflection on their activities. We observed that they were able to assess their progress towards their plans and adapt to unforeseen circumstances using our tool.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {4},
numpages = {9},
keywords = {Life-logging, Personal Visualization, Reflection tools},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399842,
author = {Bailly, Charles and Leitner, Fran\c{c}ois and Nigay, Laurence},
title = {Bring2Me: Bringing Virtual Widgets Back to the User's Field of View in Mixed Reality},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399842},
doi = {10.1145/3399715.3399842},
abstract = {Current Mixed Reality (MR) Head-Mounted Displays (HMDs) offer a limited Field Of View (FOV) of the mixed environment. Turning the head is thus necessary to visually perceive the virtual objects that are placed within the real world. However, turning the head also means loosing the initial visual context. This limitation is critical in contexts like augmented surgery where surgeons need to visually focus on the operative field. To address this limitation we propose to bring virtual objects/widgets back to the users' FOV instead of forcing the users to turn their head. We carry an initial investigation to demonstrate the approach by designing and evaluating three new menu techniques to first bring the menu back to the users' FOV before selecting an item. Results show that our three menu techniques are 1.5s faster on average than the baseline head-motion menu technique and are largely preferred by participants.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {5},
numpages = {9},
keywords = {Pointing task, HMD, Mixed Reality},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399862,
author = {Achberger, Alexander and Cutura, Ren\'{e} and T\"{u}rksoy, Oguzhan and Sedlmair, Michael},
title = {Caarvida: Visual Analytics for Test Drive Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399862},
doi = {10.1145/3399715.3399862},
abstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {9},
keywords = {object detection, human computer interaction, automotive, visual analytics, information visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399821,
author = {McGuffin, Michael J. and Fuhrman, Christopher P.},
title = {Categories and Completeness of Visual Programming and Direct Manipulation},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399821},
doi = {10.1145/3399715.3399821},
abstract = {Recent innovations in visual programming and the use of direct manipulation for programming have demonstrated promise, but also raise questions about how far these approaches can be generalized. To clarify these issues, we present a categorization of systems for visual programming, programming-by-example, and similar systems. By examining each category, we elucidate the advantages, limitations, and ways to extend systems in each category. Our work makes it easier for researchers and designers to understand how visual programming languages (VPLs) and similar systems relate to each other, and how to extend them. We also indicate directions for future research.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {7},
numpages = {8},
keywords = {programming by example, direct manipulation, output-directed programming, taxonomy, visual programming languages, programming by demonstration},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399828,
author = {Melonio, Alessandra and Rizvi, Mehdi and Roumelioti, Eftychia and De Angeli, Antonella and Gennari, Rosella and Matera, Maristella},
title = {Children's Beliefs and Understanding of Smart Objects: An Exploratory Study},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399828},
doi = {10.1145/3399715.3399828},
abstract = {Children's role in the design of new technology has been widely investigated. Recently, the research focus has shifted, from the technology they help create, towards what children gain by participating in design workshops. This paper intercepts this line of research. It reports on a design workshop with 27 children, aged from 11 to 14 years old, ideating, programming and prototyping smart objects for their town park. Data were gathered in relation to children's beliefs, before and after the workshop, and in relation to their understanding of design, after the workshop. The analysis of the gathered data suggests that the workshop positively affected children's beliefs and understanding of design, giving indications for future work concerning design as means of empowerment.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {8},
numpages = {8},
keywords = {children centred design, card-based game, smart objects, making, interaction design},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399814,
author = {Heyen, Frank and Munz, Tanja and Neumann, Michael and Ortega, Daniel and Vu, Ngoc Thang and Weiskopf, Daniel and Sedlmair, Michael},
title = {ClaVis: An Interactive Visual Comparison System for Classifiers},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399814},
doi = {10.1145/3399715.3399814},
abstract = {We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {9},
numpages = {9},
keywords = {visual analytics, machine learning, classifier comparison, Visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399875,
author = {Cutura, Rene and Aupetit, Micha\"{e}l and Fekete, Jean-Daniel and Sedlmair, Michael},
title = {Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399875},
doi = {10.1145/3399715.3399875},
abstract = {We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {10},
numpages = {9},
keywords = {Dimensionality Reduction, High-Dimensional Data, Matrix Visualization, Subspace Analysis, Visual Comparison},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399743,
author = {Prouzeau, Arnaud and Wang, Yuchen and Ens, Barrett and Willett, Wesley and Dwyer, Tim},
title = {Corsican Twin: Authoring In Situ Augmented Reality Visualisations in Virtual Reality},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399743},
doi = {10.1145/3399715.3399743},
abstract = {We introduce Corsican Twin, a tool for authoring augmented reality data visualisations in virtual reality using digital twins. The system provides users with the contextual information necessary to design embedded and situated data visualisations in a safe and convenient remote setting. We created system via a co-design process which involved people with little or no programming experience. Using the system, we illustrate three potential use cases for situated visualizations in the context of building maintenance, including: (1) on-site equipment debugging and diagnosis; (2) remote incident playback; and (3) operations simulations for future buildings. From feedback gathered during formative evaluations of our prototype tool with domain experts, we discuss implications, opportunities, and challenges for future in situ visualisation design tools.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {11},
numpages = {9},
keywords = {In situ Visualisation, Immersive Analytics, Authoring Tool},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399918,
author = {Angelini, Marco and Blasilli, Graziano and Lenti, Simone and Palleschi, Alessia and Santucci, Giuseppe},
title = {CrossWidgets: Enhancing Complex Data Selections through Modular Multi Attribute Selectors},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399918},
doi = {10.1145/3399715.3399918},
abstract = {Filtering is one of the basic interaction techniques in Information Visualization, with the main objective of limiting the amount of displayed information using constraints on attribute values. Research focused on direct manipulation selection means or on simple interactors like sliders or check-boxes: while the interaction with a single attribute is, in principle, straightforward, getting an understanding of the relationship between multiple attribute constraints and the actual selection might be a complex task. To cope with this problem, usually referred as cross-filtering, the paper provides a general definition of the structure of a filter, based on domain values and data distribution, the identification of visual feedbacks on the relationship between filters status and the current selection, and guidance means to help in fulfilling the requested selection. Then, leveraging on the definition of these design elements, the paper proposes CrossWidgets, modular attribute selectors that provide the user with feedback and guidance during complex interaction with multiple attributes. An initial controlled experiment demonstrates the benefits that CrossWidgets provide to cross-filtering activities.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {12},
numpages = {9},
keywords = {visual filtering, visual guidance, crossfilter, user feedback},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399860,
author = {Veneruso, Silvestro V. and Ferro, Lauren S. and Marrella, Andrea and Mecella, Massimo and Catarci, Tiziana},
title = {CyberVR: An Interactive Learning Experience in Virtual Reality for Cybersecurity Related Issues},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399860},
doi = {10.1145/3399715.3399860},
abstract = {The use of videogames has become an established tool to educate users about various topics. Videogames can promote challenges, co-operation, engagement, motivation, and the development of problem-solving strategies, which are all aspects with an important educational potential. In this paper, we present the design and realization of CyberVR, a Virtual Reality (VR) videogame that acts as an interactive learning experience to improve the user awareness of cybersecurity-related issues. We report the results of a user study showing that CyberVR is equally effective but more engaging as learning method toward cybersecurity education than traditional textbook learning.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {13},
numpages = {8},
keywords = {Cybersecurity, Educational game, Game-based learning},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399820,
author = {Abate, Andrea F. and Castiglione, Aniello and Nappi, Michele and Passero, Ignazio},
title = {DELEX: A DEep Learning Emotive EXperience: Investigating Empathic HCI},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399820},
doi = {10.1145/3399715.3399820},
abstract = {Recent advances in Machine Learning have unveiled interesting possibilities for real-time investigating about user characteristics and expressions like, but not limited to, age, sex, body posture, emotions and moods. These new opportunities lay the foundations for new HCI tools for interactive applications that adopt user emotions as a communication channel.This paper presents an Emotion Controlled User Experience that changes according to user feelings and emotions analysed at runtime. Aiming at obtaining a preliminary evaluation of the proposed ecosystem, a controlled experiment has been performed in an engineering and software development company, where 60 people have been involved as volunteers. The subjective evaluation has been based on a standard questionnaire commonly adopted for measuring user perceived sense of immersion in Virtual Environments. The results of the controlled experiment encourage further investigations strengthen by the analysis of objective performance measurements and user physiological parameters.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {14},
numpages = {8},
keywords = {Deep Learning, User Emotions, Computer Vision, User Experience},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399919,
author = {Pittarello, Fabio and Pagini, Veronica and Zuffellato, Leonardo},
title = {Design and Evaluation of an Educational Virtual Reality Application for Learning How to Perform on a Stage},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399919},
doi = {10.1145/3399715.3399919},
abstract = {This work is focused on the design of a VR application for helping actors during the training phase of a play. The project starts from the analysis of a previous work and from additional research, involving real actors, for evaluating the parameters of interest in an actor's training. These results have been used as the starting point for the design of an immersive VR application, where different interaction techniques have been designed and experimented for supporting the actors. These techniques have been applied to a case study, a play written by the well-known Italian playwright Carlo Goldoni, and tested with a group of high-school students, considering for the evaluation a set of variables which are meaningful for VR experiences and for learning.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {15},
numpages = {8},
keywords = {learning, design, theatre, VR, evaluation, acting, haptic feedback},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399861,
author = {Di Gregorio, Marianna and Nota, Giancarlo and Romano, Marco and Sebillo, Monica and Vitiello, Giuliana},
title = {Designing Usable Interfaces for the Industry 4.0},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399861},
doi = {10.1145/3399715.3399861},
abstract = {In Industry 4.0, Human Machine Interfaces are largely used in order to increase the performances of production processes at the same time reducing the number of emergencies and accidents. In manufacturing, the most typical system used to monitor the production is the Andon. It is a graphical system exploited in plants to notify operators who deal with management, maintenance and production performance of the presence of a problem. Of course, the usability of such interfaces is essential to allow an operator to identify and react more effectively to potentially critical situations. Improving the usability of such interfaces is a big challenge due to the increasing complexity of the data that must be processed and understood quickly by operators. In this paper, we present a set of guidelines to help professional developers to design usable interfaces for monitoring industrial production in manufacturing. Such guidelines are based on usability principles and formalized by reviewing existing industrial interfaces. Using a realistic case study prepared with manufacturing experts, we propose an Andon interface that we developed to test the efficacy of these guidelines on a last generation touch-wall device.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {16},
numpages = {9},
keywords = {Andon, HMI, Multitouch display, OEE},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399915,
author = {Perry, Scott and Yin, Mason Sun and Gray, Kathryn and Kobourov, Stephen},
title = {Drawing Graphs on the Sphere},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399915},
doi = {10.1145/3399715.3399915},
abstract = {Graphs are most often visualized in the two dimensional Euclidean plane, but spherical space offers several advantages when visualizing graphs. First, some graphs such as skeletons of three dimensional polytopes (tetrahedron, cube, icosahedron) have spherical realizations that capture their 3D structure, which cannot be visualized as well in the Euclidean plane. Second, the sphere makes possible a natural "focus + context visualization with more detail in the center of the view and less details away from the center. Finally, whereas layouts in the Euclidean plane implicitly define notions of "central and "peripheral nodes, this issue is reduced on the sphere, where the layout can be centered at any node of interest.We first consider a projection-reprojection method that relies on transformations often seen in cartography and describe the implementation of this method in the GMap visualization system. This approach allows many different types of 2D graph visualizations, such as node-link diagrams, LineSets, BubbleSets and MapSets, to be converted into spherical web browser visualizations. Next we consider an approach based on spherical multidimensional scaling, which performs graph layout directly on the sphere. This approach supports node-link diagrams and GMap-style visualizations, rendered in the web browser using WebGL.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {17},
numpages = {9},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399928,
author = {Kosch, Thomas and Hassib, Mariam and Reutter, Robin and Alt, Florian},
title = {Emotions on the Go: Mobile Emotion Assessment in Real-Time Using Facial Expressions},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399928},
doi = {10.1145/3399715.3399928},
abstract = {Exploiting emotions for user interface evaluation became an increasingly important research objective in Human-Computer Interaction. Emotions are usually assessed through surveys that do not allow information to be collected in real-time. In our work, we suggest the use of smartphones for mobile emotion assessment. We use the front-facing smartphone camera as a tool for emotion detection based on facial expressions. Such information can be used to reflect on emotional states or provide emotion-aware user interface adaptation. We collected facial expressions along with app usage data in a two-week field study consisting of a one-week training phase and a one-week testing phase. We built and evaluated a person-dependent classifier, yielding an average classification improvement of 33% compared to classifying facial expressions only. Furthermore, we correlate the estimated emotions with concurrent app usage to draw insights into changes in mood. Our work is complemented by a discussion of the feasibility of probing emotions on-the-go and potential use cases for future emotion-aware applications.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {18},
numpages = {9},
keywords = {Emotion-Aware Interfaces, Affective Computing, Emotion Recognition, Mobile Sensing},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399850,
author = {Smith, Jesse and Wang, Isaac and Wei, Winston and Woodward, Julia and Ruiz, Jaime},
title = {Evaluating the Scalability of Non-Preferred Hand Mode Switching in Augmented Reality},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399850},
doi = {10.1145/3399715.3399850},
abstract = {Mode switching allows applications to support a wide range of operations (e.g. selection, manipulation, and navigation) using a limited input space. While the performance of different mode switching techniques has been extensively examined for pen- and touch-based interfaces, investigating mode switching in augmented reality (AR) is still relatively new. Prior work found that using non-preferred hand is an efficient mode switching technique in AR. However, it is unclear how the technique performs when increasing the number of modes, which is more indicative of real-world applications. Therefore, we examined the scalability of non-preferred hand mode switching in AR with two, four, six, and eight modes. We found that as the number of modes increase, performance plateaus after the four-mode condition. We also found that counting gestures have varying effects on mode switching performance in AR. Our findings suggest that modeling mode switching performance in AR is more complex than simply counting the number of available modes. Our work lays a foundation for understanding the costs associated with scaling interaction techniques in AR.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {19},
numpages = {9},
keywords = {Mode switching, Non-preferred hand, Augmented reality},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399716,
author = {Chopra, Shreya and Maurer, Frank},
title = {Evaluating User Preferences for Augmented Reality Interactions with the Internet of Things},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399716},
doi = {10.1145/3399715.3399716},
abstract = {We investigate user preferences for controlling IoT devices with headset-based Augment Reality (AR), comparing gestural control and voice control. An elicitation study is performed with 16 participants to gather their preferred voice commands and gestures for a set of referents. We analyzed 784 inputs (392 gestures and 392 voice) as well as observations and interviews to develop an empirical basis for design recommendations that form a guideline for future designers and implementors of such voice commands and gestures for interacting with the IoT via headset-based AR.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {20},
numpages = {9},
keywords = {gestural interaction, Augmented Reality, elicitation study, human computer interaction, Internet of Things, user-centered design, voice commands},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399845,
author = {Vona, Francesco and Torelli, Emanuele and Beccaluva, Eleonora and Garzotto, Franca},
title = {Exploring the Potential of Speech-Based Virtual Assistants in Mixed Reality Applications for People with Cognitive Disabilities},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399845},
doi = {10.1145/3399715.3399845},
abstract = {Mixed Reality (MR) has been receiving increasing interest in the rehabilitation of people with Cognitive Disabilities. The power of MR in the context of therapies is the possibility to maintain a physical and psychological relationship with the surrounding environment while experiencing customized multimedia content and tasks that are appropriate for the needs of these users. The purpose of this work is to explore the potential of MR applications integrated with interactive speech-based Virtual Assistants. We present HoloLearn, an application for Microsoft HoloLens, which aims to help people with Cognitive Disabilities to improve their autonomy and learn simple activities of their everyday life (e.g., setting the table). The paper discusses the design features of the Virtual Assistant created and the results of a controlled study. The participants involved were 15 subjects with Cognitive Disabilities who used HoloLearn in two experimental conditions - with and without a Virtual Assistant. The results, although preliminary, indicate that enriching a MR experience with the presence of a speech-based Virtual Assistant would improve the user performance in the execution of the MR activities.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {21},
numpages = {9},
keywords = {Cognitive Disability, Mixed Reality, Virtual Assistant, Controlled Study},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399920,
author = {Brown, Sarah Anne and Chu, Sharon Lynn and Rani, Neha},
title = {Externalizing Mental Images by Harnessing Size-Describing Gestures: Design Implications for a Visualization System},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399920},
doi = {10.1145/3399715.3399920},
abstract = {People use a significant amount of gestures when engaging in creative brainstorming. This is especially typical for creative workers who frequently convey ideas, designs, and stories to team members. These gestures produced during natural conversation contain information that is not necessarily conveyed through speech. This paper investigates the design of a system that uses people's gestures in natural communication contexts to produce external visualizations of their mental imagery, focusing on gestures that describe dimension-related information. While much psycholinguistics research address how gestures relate to the representations of concepts, little HCI work has explored the possibilities of harnessing gestures to support thinking.We conducted a study to explore how people gesture using a basic gesture-based visualization system in simulated creative gift design scenarios, towards the goal of deriving design implications. Both quantitative and qualitative data were collected from the study, allowing us to ascertain what features (e.g., users' spatial frames of reference and listener types) of a gesture-based visualization system need to be accounted for in design. Results showed that our system managed to visualize users' envisioned gift dimensions, but that visualized object area significantly affected users' perceived accuracy of the system. We extract themes as to what dimensions are important in the design of a gesture-based visualization system, and the possible uses of such a system from the participants' perspectives. We discuss implications for the design of gesture-based visualization systems to support creative work and possibilities for future directions of research.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {22},
numpages = {9},
keywords = {creative brainstorming, storytelling, gesture recognition systems, gesture-based visualization, design},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399905,
author = {Corno, Fulvio and De Russis, Luigi and Roffarello, Alberto Monge},
title = {HeyTAP: Bridging the Gaps Between Users' Needs and Technology in IF-THEN Rules via Conversation},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399905},
doi = {10.1145/3399715.3399905},
abstract = {In the Internet of Things era, users are willing to personalize the joint behavior of their connected entities, i.e., smart devices and online service, by means of IF-THEN rules. Unfortunately, how to make such a personalization effective and appreciated is still largely unknown. On the one hand, contemporary platforms to compose IF-THEN rules adopt representation models that strongly depend on the exploited technologies, thus making end-user personalization a complex task. On the other hand, the usage of technology-independent rules envisioned by recent studies opens up new questions, and the identification of available connected entities able to execute abstract users' needs become crucial. To this end, we present HeyTAP, a conversational and semantic-powered trigger-action programming platform able to map abstract users' needs to executable IF-THEN rules. By interacting with a conversational agent, the user communicates her personalization intentions and preferences. User's inputs, along with contextual and semantic information related to the available connected entities, are then used to recommend a set of IF-THEN rules that satisfies the user's needs. An exploratory study on 8 end users preliminary confirms the effectiveness and the appreciation of the approach, and shows that HeyTAP can successfully guide users from their needs to specific rules.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {23},
numpages = {9},
keywords = {Internet of Things, Semantic Web, Recommender System, Trigger-Action Programming, Abstraction, Conversational Agent},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399824,
author = {Scuri, Sabrina and Nisi, Valentina},
title = {Immersion in Desktop-Based Web Browsing: A Framework},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399824},
doi = {10.1145/3399715.3399824},
abstract = {In this article, we report on findings from an empirical study designed to assess factors determining the experience of immersion in desktop-based web-browsing, which is identified as occurring when a user reports a deep sense of cognitive and perceptual absorption in the content and/or the interaction with a website. A within-subjects study was designed and administered to participants (N = 29) who interacted with three different websites. Results of the self-report measure have been factor analyzed by Principal Component Analysis (PCA). The PCA yielded three factors (media form and content; control and continuity; consistency), which appear to influence the determinants of an immersive experience, namely vividness and interactivity, in different ways. Directions for further research are briefly discussed.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {24},
numpages = {9},
keywords = {User Experience, Immersion, Web Design, Interaction, Vividness},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399841,
author = {Moreira, Jo\~{a}o and Mendes, Daniel and Gon\c{c}alves, Daniel},
title = {Incidental Visualizations: Pre-Attentive Primitive Visual Tasks},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399841},
doi = {10.1145/3399715.3399841},
abstract = {In InfoVis design, visualizations make use of pre-attentive features to highlight visual artifacts and guide users' perception into relevant information during primitive visual tasks. These are supported by visual marks such as dots, lines, and areas. However, research assumes our pre-attentive processing only allows us to detect specific features in charts. We argue that a visualization can be completely perceived pre-attentively and still convey relevant information. In this work, by combining cognitive perception and psychophysics, we executed a user study with six primitive visual tasks to verify if they could be performed pre-attentively. The tasks were to find: horizontal and vertical positions, length and slope of lines, size of areas, and color luminance intensity. Users were presented with very simple visualizations, with one encoded value at a time, allowing us to assess the accuracy and response time. Our results showed that horizontal position identification is the most accurate and fastest task to do, and the color luminance intensity identification task is the worst. We believe our study is the first step into a fresh field called Incidental Visualizations, where visualizations are meant to be seen at-a-glance, and with little effort.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {25},
numpages = {9},
keywords = {incidental visualizations, user study, pre-attentive, cognitive perception, psychophysics, primitive visual tasks},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399922,
author = {Xie, Liwenhan and O'Donnell, James and Bach, Benjamin and Fekete, Jean-Daniel},
title = {Interactive Time-Series of Measures for Exploring Dynamic Networks},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399922},
doi = {10.1145/3399715.3399922},
abstract = {We present MeasureFlow, an interface to visually and interactively explore dynamic networks through time-series of network measures such as link number, graph density, or node activation. When networks contain many time steps, become large and more dense, or contain high frequencies of change, traditional visualizations that focus on network topology, such as animations or small multiples, fail to provide adequate overviews and thus fail to guide the analyst towards interesting time points and periods. MeasureFlow presents a complementary approach that relies on visualizing time-series of common network measures to provide a detailed yet comprehensive overview of when changes are happening and which network measures they involve. As dynamic networks undergo changes of varying rates and characteristics, network measures provide important hints on the pace and nature of their evolution and can guide an analysts in their exploration; based on a set of interactive and signal-processing methods, MeasureFlow allows an analyst to select and navigate periods of interest in the network. We demonstrate MeasureFlow through case studies with real-world data.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {26},
numpages = {9},
keywords = {exploratory data analysis, interaction, network measures, dynamic networks, signal processing},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399847,
author = {Francese, Rita and Risi, Michele and Tortora, Genoveffa},
title = {MiniJava: Automatic Miniaturization of Java Applications},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399847},
doi = {10.1145/3399715.3399847},
abstract = {The use of smartphones is dramatically increasing. As a consequence, many organizations have the need of migrating their Java desktop applications towards the mobile technology. In this paper we present a miniaturization approach (process and supporting tool) named miniJava for the automatic miniaturization of Java desktop applications towards Android. The Java business logic is unvaried, while the calls to the Java objects of the interface are mapped into call to objects of the target technology. Semi-automatic layout fragmentation enables us to partition a desktop Java interface in various mobile screens. The approach also migrates the application files and enables the network connection. We conduct a user study where we assess the user perception in terms of user experience and affective reaction of the miniaturized application generated by a real Java desktop application which also has real Android variant. The end-user sample consisted of 18 participants. Results of this preliminary evaluation are encouraging: they do not reveal particular problems when using the miniaturized version automatically generated of the real desktop app with respect to its original Android variant, except for the novelty, which is better perceived for the native Android one.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {27},
numpages = {8},
keywords = {mobile development, application migration, Graphical user interfaces},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399832,
author = {Ragan, Eric D. and Pachuilo, Andrew and Goodall, John R. and Bacim, Felipe},
title = {Preserving Contextual Awareness during Selection of Moving Targets in Animated Stream Visualizations},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399832},
doi = {10.1145/3399715.3399832},
abstract = {In many types of dynamic interactive visualizations, it is often desired to interact with moving objects. Stopping moving objects can make selection easier, but pausing animated content can disrupt perception and understanding of the visualization. To address such problems, we explore selection techniques that only pause a subset of all moving targets in the visualization. We present various designs for controlling pause regions based on cursor trajectory or cursor position. We then report a dual-task experiment that evaluates how different techniques affect both target selection performance and contextual awareness of the visualization. Our findings indicate that all pause techniques significantly improved selection performance as compared to the baseline method without pause, but the results also show that pausing the entire visualization can interfere with contextual awareness. However, the problem with reduced contextual awareness was not observed with our new techniques that only pause a limited region of the visualization. Thus, our research provides evidence that region-limited pause techniques can retain the advantages of selection in dynamic visualizations without imposing a negative effect on contextual awareness.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {28},
numpages = {9},
keywords = {selection techniques, animation, human-computer interaction, Visualization, information visualization, streaming data},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399873,
author = {Nusrat, Sabrina and Alam, Jawaherul and Kobourov, Stephen},
title = {Recognition and Recall of Geographic Data In Cartograms},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399873},
doi = {10.1145/3399715.3399873},
abstract = {We investigate the memorability of two types of cartograms, both in terms of recognition of the visualization and recall of the data. A cartogram, or a value-by-area map, is a representation of a map in which geographic regions are modified to reflect a given statistic, such as population or income. Of the many different types of cartograms, the contiguous and Dorling types are among the most popular and most effective. With this in mind, we evaluate the memorability of these two cartogram types with a human-subjects study, using task-based experimental data and cartogram visualization tasks based on Bertin's map reading levels. In particular, our results indicate that Dorling cartograms are associated with better recall of general patterns and trends. This, together with additional significant differences between the two most popular cartogram types, has implications for the design and use of cartograms, in the context of memorability.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {29},
numpages = {9},
keywords = {user study, memorability, cartograms, geo-referenced visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399913,
author = {Wang, Xiaoyi and Micallef, Luana and Hornb\ae{}k, Kasper},
title = {RegLine: Assisting Novices in Refining Linear Regression Models},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399913},
doi = {10.1145/3399715.3399913},
abstract = {The process of verifying linear model assumptions and remedying associated violations is complex, even when dealing with simple linear regression. This process is not well supported by current tools and remains time-consuming, tedious, and error-prone. We present RegLine, a visual analytics tool supporting the iterative process of assumption verification and violation remedy for simple linear regression models. To identify the best possible model, RegLine helps novices perform data transformations, deal with extreme data points, analyze residuals, validate models by their assumptions, and compare and relate models visually. A qualitative user study indicates that these features of RegLine support the exploratory and refinement process of model building, even for those with little statistical expertise. These findings may guide visualization designs on how interactive visualizations can facilitate refining and validating more complex models.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {30},
numpages = {9},
keywords = {exploratory data analysis, data transformation, residual analysis, linear regression, model verification and remedy},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399831,
author = {Dix, Alan},
title = {Taking the Long View: Structured Expert Evaluation for Extended Interaction},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399831},
doi = {10.1145/3399715.3399831},
abstract = {This paper proposes first steps in the development of practical techniques for the expert evaluation of long-term interactions driven by the need to perform expert evaluation of such systems in a consultancy framework. Some interactions are time-limited and goal-driven, for example withdrawing money at an ATM. However, these are typically embedded within longer-term interactions, such as with the banking system as a whole. We have numerous evaluation and design tools for the former, but long-term interaction is less well served. To fill this gap new evaluation prompts are presented, drawing on the style of cognitive walkthroughs to support extended interaction.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {31},
numpages = {9},
keywords = {expert evaluation, cognitive walkthrough, interaction design, Long-term interaction},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399851,
author = {Perea, Patrick and Morand, Denis and Nigay, Laurence},
title = {Target Expansion in Context: The Case of Menu in Handheld Augmented Reality},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399851},
doi = {10.1145/3399715.3399851},
abstract = {Target expansion techniques facilitate pointing by enlarging the effective sizes of targets. As opposed to the numerous studies on target expansion solely focusing on optimizing pointing, we study the compound task of pointing at a Point of Interest (POI) and then interacting with the POI menu in handheld Augmented Reality (AR). A POI menu in AR has a fixed position because it contains relevant information about its location in the real world. We present two techniques that make the cursor jump to the closest opened POI menu after pointing at a POI. Our experimental results show that 1) for selecting a POI the expansion techniques are 31 % faster than the baseline screen-centered crosshair pointing technique, 2) the expansion techniques with/without a jumping cursor to the closest opened POI menu offer similar performances and 3) Touch relative pointing is preferred by participants because it minimizes physical movements.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {32},
numpages = {9},
keywords = {Menu, Pointing, Handheld Augmented reality, Target expansion},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399852,
author = {Kimura, Naoki and Hayashi, Kentaro and Rekimoto, Jun},
title = {TieLent: A Casual Neck-Mounted Mouth Capturing Device for Silent Speech Interaction},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399852},
doi = {10.1145/3399715.3399852},
abstract = {With the increased use of smart speakers, silent speech interaction (SSI) is attracting attention. Unfortunately, traditional silent speech interaction methods require the addition of obtrusive sensors and devices around the user's face, making wearability and portability a challenge. Considering that most uses for smart speakers do not require many words, we suggest a more casual approach, TieLent, which can easily be worn between the neck and the chest. TieLent's RGB camera is set away from the user's face, presenting less interference with the user. Although TieLent's camera is not able to capture the whole mouth, when combined with our image-to-speech neural network model, it is able to generate the recognizable speech of 15 commands with an average accuracy of 94%.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {33},
numpages = {8},
keywords = {Image-to-Speech, Visual Speech Recognition, Silent Speech Interface},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399874,
author = {Suzunaga, Saya and Itoh, Yuichi and Inoue, Yuki and Fujita, Kazuyuki and Onoye, Takao},
title = {TuVe: A Shape-Changeable Display Using Fluids in a Tube},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399874},
doi = {10.1145/3399715.3399874},
abstract = {We propose TuVe, a novel shape-changing display consisting of a flexible tube and fluids, in which the droplets flowing through the tube compose the display medium that represents information. In this system, every colored droplet is flowed by controlling valves and a pump connected to the tube. The display part employs a flexible tube that can be shaped to any structure (e.g., wrapped around a specific object), which is achieved by a calibration made to capture the tube structure using image processing with a camera. A performance evaluation reveals that our prototype succeeds in controlling each droplet with a positional error of 2 mm or less, which is small enough to show such simple characters as alphabetic characters using a 7 \texttimes{} 7-pixel resolution display. We also discuss example applications, such as large public displays and flow-direction visualization, that illustrate the characteristics of the TuVe display.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {34},
numpages = {9},
keywords = {Nonvolatile Display, Tube Shape Display, Droplet As Pixel, Substantial Display, Flexible Display},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399916,
author = {Vargas, Abel and D\'{\i}az, Paloma and Zarraonandia, Telmo},
title = {Using Virtual Reality and Music in Cognitive Disability Therapy},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399916},
doi = {10.1145/3399715.3399916},
abstract = {Traditional therapy methods for people with cognitive disabilities involve long periods of time performing exercises that can be tedious or unattractive for both patients and the professionals who work with them in specialized centers. In this work we aim at investigating the possibilities that combining music and virtual reality technology might offer for improving this scenario. Following this purpose, we applied an action research approach to co-develop with stakeholders a highly-configurable and accessible application to support exercising abilities usually targeted in rehabilitation therapy, such as coordination and motor skills, memory, and spatial perception. The results of an experiment carried out with both professionals and patients showed promising results, suggesting that this type of technology could improve the rehabilitation process, and make it more engaging.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {35},
numpages = {9},
keywords = {motor skills, therapy, health care information systems, virtual reality},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399848,
author = {De Marsico, Maria and Panizzi, Emanuele and Mattei, Francesca Romana and Musolino, Antonio and Prandini, Manuel and Riso, Marzia and Sforza, Davide},
title = {Virtual Bowling: Launch as You All Were There!},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399848},
doi = {10.1145/3399715.3399848},
abstract = {This work proposes BowlingVR, an advanced Virtual Reality (VR) multiplayer game that tackles two main goals: the first one is to provide a realistic User eXperience (UX) to the user, by reproducing the dynamics and physical context of a real bowling challenge; the second one is to allow a remote, distributed, socially satisfying gameplay, providing the user the illusion of the real presence of the remote players. The prototype was evaluated using a modified version of SUXES, a kind of user interview schema that was originally devised for multimedia applications and that has been modified in order to better compare the responses of different users and get a more reliable estimation of user appreciation.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {36},
numpages = {9},
keywords = {VR interaction, Virtual Reality, VR evaluation},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399827,
author = {Vogogias, Athanasios and Archambault, Daniel and Bach, Benjamin and Kennedy, Jessie},
title = {Visual Encodings for Networks with Multiple Edge Types},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399827},
doi = {10.1145/3399715.3399827},
abstract = {This paper reports on a formal user study on visual encodings of networks with multiple edge types in adjacency matrices. Our tasks and conditions were inspired by real problems in computational biology. We focus on encodings in adjacency matrices, selecting four designs from a potentially huge design space of visual encodings. We then settle on three visual variables to evaluate in a crowdsourcing study with 159 participants: orientation, position and colour. The best encodings were integrated into a visual analytics tool for inferring dynamic Bayesian networks and evaluated by computational biologists for additional evidence. We found that the encodings performed differently depending on the task, however, colour was found to help in all tasks except when trying to find the edge with the largest number of edge types. Orientation generally outperformed position in all of our tasks.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {37},
numpages = {9},
keywords = {user study, dynamic Bayesian networks, multilayer networks, evaluation, crowdsourcing, adjacency matrices},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399906,
author = {Wang, Xiaoyi and Hornb\ae{}k, Kasper},
title = {Visual Exploration of Time-Series Forecasts Through Structured Navigation},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399906},
doi = {10.1145/3399715.3399906},
abstract = {Evaluating the forecasting ability of time-series involves observations of multiple charts representing different aspects of model accuracy. However, the sequence of the charts observed by users is not controlled and it is difficult for users to discover relations among charts. Therefore, we propose a method for constructing a navigation structure that shows these relations based on the syntax and semantics of the charts. An excerpt from the structure is used as a context menu that allows users to navigate through a series of charts and explore their relations in a structured way. A qualitative study is conducted to evaluate the system and the results show that our approach helps users explore the connections among charts and enhances the understanding of time-series forecasting performance.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {38},
numpages = {9},
keywords = {navigation, time series, model evaluation},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399815,
author = {Louis, Thibault and Troccaz, Jocelyne and Rochet-Capellan, Am\'{e}lie and Hoyek, Nady and B\'{e}rard, Fran\c{c}ois},
title = {When High Fidelity Matters: AR and VR Improve the Learning of a 3D Object},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399815},
doi = {10.1145/3399715.3399815},
abstract = {Virtual and Augmented Reality Environments have long been seen as having strong potential for educational applications. However, research showing actual evidences of their benefits is sparse. Indeed, some recent studies point to unnoticeable benefits, or even a detrimental effect due to an increase of cognitive demand for the students when using these environments. In this work, we question if a clear benefit of AR and VR can be robustly measured for a specific education-related task: learning a 3D object.We ran a controlled study in which we compared three interaction techniques. Two techniques are VR- and AR-based; they offer a High Fidelity (HF) virtual reproduction of observing and manipulating physical objects. The third technique is based on a multi-touch tablet and was used as a baseline. We selected a task of 3D object learning as one potentially benefitting from the HF reproduction of object manipulation. The experiment results indicate that VR and AR HF techniques can have a substantial benefit for education as the object was recognized more than 27% faster when learnt using the HF techniques than when using the tablet.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {39},
numpages = {9},
keywords = {Mental rotation, Virtual reality, User study, Spatial augmented reality, Learning task, Head mounted display},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399836,
author = {Plasson, Carole and Cunin, Dominique and Laurillau, Yann and Nigay, Laurence},
title = {3D Tabletop AR: A Comparison of Mid-Air, Touch and Touch+Mid-Air Interaction},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399836},
doi = {10.1145/3399715.3399836},
abstract = {This paper contributes a first comparative study of three techniques for selecting 3D objects anchored to the table in tabletop Augmented Reality (AR). The impetus for this study is that touch interaction makes more sense when the targeted objects are anchored to the table. We experimentally compare touch and a mixed (touch+mid-air) techniques with the common direct mid-air technique. The touch and mixed techniques involve a decomposition of the 3D task into a 2D task by touch on the table followed by a 1D task by touch or mid-air interaction. Results show that: (1) The touch and mixed techniques present completion times similar to the mid-air technique and are more accurate than the mid-air technique; (2) The mixed technique defines a good compromise between accuracy of touch interaction and speed of mid-air interaction.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {40},
numpages = {5},
keywords = {HMD, Augmented Reality, Tabletop, 3D interaction, Touch, Mid-air},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399843,
author = {Berton, Rudy and Kolasinska, Agnieszka and Gaggi, Ombretta and Palazzi, Claudio E. and Quadrio, Giacomo},
title = {A Chrome Extension to Help People with Dyslexia},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399843},
doi = {10.1145/3399715.3399843},
abstract = {Even if the World Wide Web is one of the main content and service providers, unfortunately, these contents and services are not really available for everyone. People affected by impairments often have difficulties in navigating Web pages for a wide range of reasons. In this paper, we focus on people affected by dyslexia. These users experience difficulties in reading acquisition, despite normal intelligence and adequate access to conventional instruction. For this reason, we have created Help me read!, a Chrome extension that allows to change many features of a Web page. Furthermore, it allows to isolate and enlarge one word at a time. This feature is crucial as it allows people with dyslexia to focus on each single word, thus overcoming one of their main difficulties.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {41},
numpages = {5},
keywords = {dyslexia, web browser, accessibility, web pages},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399859,
author = {Gloder, Alberto and Ducceschi, Luca and Zancanaro, Massimo},
title = {A Language-Based Interface for Analysis of Digital Storytelling},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399859},
doi = {10.1145/3399715.3399859},
abstract = {In this paper, we introduce a tool aimed at supporting deep qualitative analysis of digital comics. The tool exploits language-based technologies to facilitate the exploration of relatively large sets of comics. The core idea is that the specific words used in the comics are both an important element of the analysis and an index to navigate and explore the dataset. The design concept has been validated in a pilot study and the findings provide evidence that the approach meets the needs of qualitative analysts with the potential of improving their practices.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {42},
numpages = {5},
keywords = {Digital storytelling, Natural language processing, Data analysis, Comics, Machine Learning},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399849,
author = {de Ara\'{u}jo Lima, Raul and Barbosa, Simone Diniz Junqueira},
title = {A Question-Oriented Visualization Recommendation Approach for Data Exploration},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399849},
doi = {10.1145/3399715.3399849},
abstract = {The increasingly rapid growth of data production and the consequent need to explore data to obtain answers to the most varied questions have promoted the development of tools to facilitate the manipulation and construction of data visualizations. However, building useful data visualizations is not a trivial task: it may involve a large number of subtle decisions from experienced designers. In this paper, we present an approach that uses a set of heuristics to recommend data visualizations associated with questions, in order to facilitate the understanding of the recommendations and assisting the visual exploration process. Our approach was implemented and evaluated through the VisMaker tool. We carried out two studies comparing VisMaker with Voyager 2 and analyzed some aspects of the recommendation approaches through the participants' feedbacks. As a result, we found some advantages of our approach and gathered comments to help improve the development of visualization recommender tools.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {43},
numpages = {5},
keywords = {visual data exploration, information visualization, visualization recommendation, visualization tool},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399833,
author = {Mattioli, Andrea and Patern\`{o}, Fabio},
title = {A Visual Environment for End-User Creation of IoT Customization Rules with Recommendation Support},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399833},
doi = {10.1145/3399715.3399833},
abstract = {Personalization rules based on the trigger-action paradigm have recently garnered increasing interest in Internet of Things (IoT) applications. However, composing trigger-action rules can be a challenging task for end users, especially when the rules' complexity increases. Users have to decide about various aspects: which triggers and actions to select, how to combine multiple triggers or actions, and whether some previously defined rule can help in the composition process. We propose a visual environment, Block Rule Composer, to address these problems. It consists of a tool for creating rules based on visual blocks, integrated with recommendation techniques in order to provide intelligent support during rule creation. We also report on a first test which provided positive indications and suggestions for further design improvements.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {44},
numpages = {5},
keywords = {Recommendation systems, End user development, Internet of Things, Trigger-action programming, Block-based programming},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399818,
author = {Baldassarre, Maria Teresa and Barletta, Vita Santa and Caivano, Danilo and Piccinno, Antonio},
title = {A Visual Tool for Supporting Decision-Making in Privacy Oriented Software Development},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399818},
doi = {10.1145/3399715.3399818},
abstract = {Nowadays, the dimension and complexity of software development projects increase the possibility of cyber-attacks, information exfiltration and data breaches. In this context, developers play a primary role in addressing privacy requirements and, consequently security, in software applications. Currently, only general guidelines exist that are difficult to put in operation due to the lack of the required security skills and knowledge, and to the use of legacy software development processes that do not deal with privacy and security aspects. This paper presents a knowledge base, the Privacy Knowledge Base (PKB), and the VIS-PRISE prototype (Visually Inspection to Support Privacy and Security) a visual tool that support developers' decisions to integrate privacy and security requirements in all software development phases. An initial experimental study with junior developers is also presented.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {45},
numpages = {5},
keywords = {Privacy by Design, Privacy Software Application, Human-Centered Privacy},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399929,
author = {Bozzelli, Guido and De Nino, Maurizio and Pero, Chiara and Ricciardi, Stefano},
title = {AR Based User Adaptive Compensation of Metamorphopsia},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399929},
doi = {10.1145/3399715.3399929},
abstract = {The increasing diffusion of augmented reality applications, fostered by the commercial availability of see-through enabled head mounted displays, is opening new opportunities to exploit the potential of this technology to aid subjects with visual impairments in their everyday tasks. The capability of showing a corrected version of the visual field by means of real-time processing of a video stream representing the surrounding environment, is the foundation of the proposed method to compensate a serious visual deficit, known as metamorphopsia, resulting in a geometrical deformation of part of the subject's visus. To this regard, we describe an approach for interactive measurement of user's impaired visual biometrics and for real-time compensation or reduction of this deficiency. This goal is achieved by mapping the video streams acquired from the stereoscopic video see-through cameras each onto a 2D polygonal mesh and offsetting its vertices until the correct vision, for each eye, is restored.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {46},
numpages = {5},
keywords = {video see-through, 3D real-time processing, augmented reality, eye biometrics, adaptive user interface},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399819,
author = {Abdrabou, Yasmeen and Abdelrahman, Yomna and Ayman, Ahmed and Elmougy, Amr and Khamis, Mohamed},
title = {Are Thermal Attacks Ubiquitous? When Non-Expert Attackers Use Off the Shelf Thermal Cameras},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399819},
doi = {10.1145/3399715.3399819},
abstract = {Recent work showed that using image processing techniques on thermal images taken by high-end equipment reveals passwords entered on touchscreens and keyboards. In this paper, we investigate the susceptibility of common touch inputs to thermal attacks when non-expert attackers visually inspect thermal images. Using an off-the-shelf thermal camera, we collected thermal images of a smartphone's touchscreen and a laptop's touchpad after 25 participants had entered passwords using touch gestures and touch taps. We show that visual inspection of thermal images by 18 participants reveals the majority of passwords. Touch gestures are more vulnerable to thermal attacks (60.65% successful attacks) than touch taps (23.61%), and attacks against touchscreens are more accurate than on touchpads (87.04% vs 56.02%). We discuss how the affordability of thermal attacks and the nature of touch interactions make the threat ubiquitous, and the implications this has on security.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {47},
numpages = {5},
keywords = {Security, Privacy, Thermal Imaging, Graphical Authentication},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399838,
author = {Guarese, Renan and Becker, Jo\~{a}o and Fensterseifer, Henrique and Walter, Marcelo and Freitas, Carla and Nedel, Luciana and Maciel, Anderson},
title = {Augmented Situated Visualization for Spatial and Context-Aware Decision-Making},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399838},
doi = {10.1145/3399715.3399838},
abstract = {Whenever accessing indoor spaces such as classrooms or auditoriums, people might attempt to analyze and choose an appropriate place to stay while attending an event. Several criteria may be accounted for, and most are not always self-evident or trivial. This work proposes the use of data visualization allied to an Augmented Reality (AR) user interface to help users defining the most convenient seats to take. We consider sets of arbitrary demands and project information directly atop the seats and all around the room. Users can also narrow down the search by switching and combining the attributes being displayed, e.g., temperature, wheelchair accessibility. The proposed approach was tested against a comparable 2D interactive visualization of the same data in usability assessments of seat-choosing tasks with a set of users (N = 16) to validate the solution. Qualitative and quantitative data indicated that the AR-based solution is promising, suggesting that AR may help users make more accurate decisions, even in an ordinary daily task. Regarding Augmented Situated Visualization, our results open new avenues for the exploration of context-aware data.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {48},
numpages = {5},
keywords = {Data Visualization, Human Computer Interaction, Augmented Reality, Situated Visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399744,
author = {Calisto, Francisco Maria and Nunes, Nuno and Nascimento, Jacinto C.},
title = {BreastScreening: On the Use of Multi-Modality in Medical Imaging Diagnosis},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399744},
doi = {10.1145/3399715.3399744},
abstract = {This paper describes the field research, design and comparative deployment of a multimodal medical imaging user interface for breast screening. The main contributions described here are threefold: 1) The design of an advanced visual interface for multimodal diagnosis of breast cancer (BreastScreening); 2) Insights from the field comparison of Single-Modality vs Multi-Modality screening of breast cancer diagnosis with 31 clinicians and 566 images; and 3) The visualization of the two main types of breast lesions in the following image modalities: (i) MammoGraphy (MG) in both Craniocaudal (CC) and Mediolateral oblique (MLO) views; (ii) UltraSound (US); and (iii) Magnetic Resonance Imaging (MRI). We summarize our work with recommendations from the radiologists for guiding the future design of medical imaging interfaces.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {49},
numpages = {5},
keywords = {medical imaging, human-computer interaction, annotations, healthcare systems, breast cancer, user-centered design, multimodality},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399719,
author = {Spence, Robert and Redmond, Leah},
title = {Circles of Affordance: Proposal for a Diagnostic Tool to Support Usability Studies},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399719},
doi = {10.1145/3399715.3399719},
abstract = {We propose, for interactive systems, a representation that is potentially useful as a diagnostic tool. It is based on the concept of affordances that can be offered to and deployed by a user. The proposal is illustrated by reference to an interface designed for a smartphone app that allows a person with Type-1 diabetes to self-manage their condition.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {50},
numpages = {4},
keywords = {app, mobile, phone, usability, affordance},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399914,
author = {Bortoli, Marta and Furini, Marco and Mirri, Silvia and Montangero, Manuela and Prandi, Catia},
title = {Conversational Interfaces for a Smart Campus: A Case Study},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399914},
doi = {10.1145/3399715.3399914},
abstract = {The spoken language is the most natural interface for a human being and, thanks to the scientific-technological advances made in recent decades, nowadays we have voice assistance devices to interact with a machine through the use of natural language. Vocal user interfaces (VUI) are now included in many technological devices, such as desktop and laptop computers, smartphones and tablets, navigators, and home speakers, being welcomed by the market. The use of voice assistants can also be interesting and strategic in educational contexts and in public environments. This paper presents a case study based on the design, development, and assessment of a prototype devoted to assist students' during their daily activities in a smart campus context.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {51},
numpages = {5},
keywords = {Conversational interface, conversational assistants, smart campus, human computer interaction},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399826,
author = {Cascavilla, Giuseppe and Slabber, Johann and Palomba, Fabio and Di Nucci, Dario and Tamburri, Damian A. and van den Heuvel, Willem-Jan},
title = {Counterterrorism for Cyber-Physical Spaces: A Computer Vision Approach},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399826},
doi = {10.1145/3399715.3399826},
abstract = {Simulating terrorist scenarios in cyber-physical spaces---that is, urban open or (semi-) closed spaces combined with cyber-physical systems counterparts---is challenging given the context and variables therein. This paper addresses the aforementioned issue with ALTer a framework featuring computer vision and Generative Adversarial Neural Networks (GANs) over terrorist scenarios. We obtained the data for the terrorist scenarios by creating a synthetic dataset, exploiting the Grand Theft Auto V (GTAV) videogame, and the Unreal Game Engine behind it, in combination with OpenStreetMap data. The results of the proposed approach show its feasibility to predict criminal activities in cyber-physical spaces. Moreover, the usage of our synthetic scenarios elicited from GTAV is promising in building datasets for cybersecurity and Cyber-Threat Intelligence (CTI) featuring simulated video gaming platforms. We learned that local authorities can simulate terrorist scenarios for their cities based on previous or related reference and this helps them in 3 ways: (1) better determine the necessary security measures; (2) better use the expertise of the authorities; (3) refine preparedness scenarios and drills for sensitive areas.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {52},
numpages = {5},
keywords = {Generative Adversarial Neural Networks, Computer Vision, Cyber-Physical Spaces, Counterterrorism},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399816,
author = {Paratore, Maria Teresa and Buzzi, Maria Claudia and Buzzi, Marina},
title = {Designing a Self-Help Mobile App to Cope with Avoidance Behavior in Panic Disorder},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399816},
doi = {10.1145/3399715.3399816},
abstract = {Panic disorder (PD) is an anxiety disorder that in recent years has spread worldwide. PD is diagnosed when a person has recurring panic attacks, characterized by physical symptoms and disturbing thoughts and feelings that arise rapidly, reach their peak in a few minutes and soon disappear. Panic attacks, despite being harmless and relatively short, are highly distressing and deeply affect the lives of patients, who very often develop agoraphobia, an anxiety disorder that leads to systematic avoidance of places where previous attacks have occurred. PD is often a chronic condition that does not respond well to pharmacological treatment. However, psychotherapeutic approaches such as mindfulness have proved to be quite effective and their delivery through self-care eHealth tools has been encouraged by the World Health Organization.In this paper, we present a self-aid mobile app designed by and for patients affected by PD with mild agoraphobia. The app is aimed at helping users cope with avoidance behavior. Thanks to geolocation, the app automatically detects the proximity of a "critical place (i.e., where a previous attack has occurred) and suggests mindfulness strategies for coping with stress, in order to prevent anxiety escalation and panic. This paper describes the therapeutic background of the proposed application, as well as the mHealth best practices we strove to adopt in the design phase. Preliminary trials conducted with one patient are encouraging; nonetheless, we point out the need for further and more extensive tests to fully assess the effectiveness of our approach.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {53},
numpages = {5},
keywords = {Mindfulness, Persuasive design, mHealth, Panic disorder, Self-management},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399823,
author = {Prange, Sarah and Mecke, Lukas and Nguyen, Alice and Khamis, Mohamed and Alt, Florian},
title = {Don't Use Fingerprint, It's Raining! How People Use and Perceive Context-Aware Selection of Mobile Authentication},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399823},
doi = {10.1145/3399715.3399823},
abstract = {This paper investigates how smartphone users perceive switching from their primary authentication mechanism to a fallback one, based on the context. This is useful in cases where the primary mechanism fails (e.g., wet fingers when using fingerprint). While prior work introduced the concept, we are the first to investigate its perception by users and their willingness to follow a system's suggestion for a switch. We present findings from a two-week field study (N=29) using an Android app, showing that users are willing to adopt alternative mechanisms when prompted. We discuss how context-awareness can improve the perception of authentication reliability and potentially improve usability and security.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {54},
numpages = {5},
keywords = {Android, Context-Aware Authentication, User Perception, Fingerprint, Field Study, Biometrics, Mobile Devices},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399837,
author = {Ragan, Eric D. and Stamps, Andrew S. and Goodall, John R.},
title = {Empirical Study of Focus-Plus-Context and Aggregation Techniques for the Visualization of Streaming Data},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399837},
doi = {10.1145/3399715.3399837},
abstract = {Analysis of streaming data often involves both real-time monitoring of incoming data as well as contextual awareness of data history. A focus-plus-context approach can support both goals, with variable levels of visual aggregation making it possible to provide a high level of detail for incoming and recent data while providing contextual information about recent history. Visual aggregation reduces data resolution in order to show the context of data over large periods of time within a limited display space. With a controlled experiment, we evaluated the effectiveness of different types of aggregation for four types of stream-analysis tasks. Overall, the results show that a focus-plus-context design has little negative impact on the ability to successfully monitor and analyze streaming data, making it possible to show longer periods of time than other approaches. However, visual aggregation can be problematic for trend recognition tasks. This research demonstrates how the effectiveness of the visualization depends on the specifics of the analysis task.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {55},
numpages = {5},
keywords = {Visualization, information visualization, human-computer interaction, streaming data},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399844,
author = {Woodward, Julia and Cato, Jahelle and Smith, Jesse and Wang, Isaac and Benda, Brett and Anthony, Lisa and Ruiz, Jaime},
title = {Examining Fitts' and FFitts' Law Models for Children's Pointing Tasks on Touchscreens},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399844},
doi = {10.1145/3399715.3399844},
abstract = {Fitts' law has accurately modeled both children's and adults' pointing movements, but it is not as precise for modeling movement to small targets. To address this issue, prior work presented FFitts' law, which is more exact than Fitts' law for modeling adults' finger input on touchscreens. Since children's touch interactions are more variable than adults, it is unclear if FFitts' law should be applied to children. We conducted a 2D target acquisition task with 54 children (ages 5-10) to examine if FFitts' law can accurately model children's touchscreen movement time. We found that Fitts' law using nominal target widths is more accurate, with a R2 value of 0.93, than FFitts' law for modeling children's finger input on touchscreens. Our work contributes new understanding of how to accurately predict children's finger touch performance on touchscreens.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {56},
numpages = {5},
keywords = {children, FFitts' law, touchscreen, finger input, Fitts' law},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399846,
author = {Woodward, Julia and Smith, Jesse and Wang, Isaac and Cuenca, Sofia and Ruiz, Jaime},
title = {Examining the Presentation of Information in Augmented Reality Headsets for Situational Awareness},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399846},
doi = {10.1145/3399715.3399846},
abstract = {Augmented Reality (AR) headsets are being employed in industrial settings (e.g., the oil industry); however, there has been little work on how information should be presented in these headsets, especially in the context of situational awareness. We present a study examining three different presentation styles (Display, Environment, Mixed Environment) for textual secondary information in AR headsets. We found that the Display and Environment presentation styles assisted in perception and comprehension. Our work contributes a first step to understanding how to design visual information in AR headsets to support situational awareness.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {57},
numpages = {5},
keywords = {Augmented reality, situational awareness},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399858,
author = {Yamato, Yuki and Suzuki, Yutaro and Sekimori, Kodai and Shizuki, Buntarou and Takahashi, Shin},
title = {Hand Gesture Interaction with a Low-Resolution Infrared Image Sensor on an Inner Wrist},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399858},
doi = {10.1145/3399715.3399858},
abstract = {We propose a hand gesture interaction method using a low-resolution infrared image sensor on an inner wrist. We attach the sensor to the strap of a wrist-worn device, on the palmar side, and apply machine-learning techniques to recognize the gestures made by the opposite hand. As the sensor is placed on the inner wrist, the user can naturally control its direction to reduce privacy invasion. Our method can recognize four types of hand gestures: static hand poses, dynamic hand gestures, finger motion, and the relative hand position. We developed a prototype that does not invade surrounding people's privacy using an 8 x 8 low-resolution infrared image sensor. Then we conducted experiments to validate our prototype, and our results imply that the low-resolution sensor has sufficient capabilities for recognizing a rich array of hand gestures. In this paper, we introduce an implementation of a mapping application that can be controlled by our specified hand gestures, including gestures that use both hands.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {58},
numpages = {5},
keywords = {Infrared Image Sensor, Hand Gesture, Privacy Concerns, Wearables, Inner Wrist, Interaction Techniques},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399839,
author = {Maeda, Kosuke and Koike, Hideki},
title = {MirAIProjection: Real-Time Projection onto High-Speed Objects by Predicting Their 3D Position and Pose Using DNNs},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399839},
doi = {10.1145/3399715.3399839},
abstract = {Allowing projections on moving objects is associated with a problem that a projection might shift due to the delay between tracking and projection. In the present paper, we proposed a new prediction model based on deep neural networks that can be used to predict both pose and position of the target object. As a result, we developed a real-time tracking and projection system named"MirAIProjection that employs motion-capture cameras and common projectors. We conducted several experiments to evaluate the effectiveness of the proposed system and demonstrated that the proposed system could reduce the slipping and increase the accuracy and robustness of the projection.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {59},
numpages = {5},
keywords = {Deep neural network, Projection mapping, Dynamic projection, Motion forecasting},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399903,
author = {Buono, Paolo and Locoro, Angela},
title = {Modelling Data Visualization Interactions: From Semiotics to Pragmatics and Back to Humans},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399903},
doi = {10.1145/3399715.3399903},
abstract = {This paper makes a point of current perspectives on Data Visualization research that were essentially conceived to provide guidelines for finding the best mapping between data and visual representations. Going back to foundational concepts of HCI that rely on manipulation of visual symbols, we propose a new perspective, with the aim to focus on a different configuration, that considers visual signs, professional contexts and user practices. We argue that, so far, user practices have been neglected or left behind in design, evaluation and recommendation scenarios, reducing them to the pure relational focus among kind of data, kind of charts and in lab tasks. This may underestimate the potential of the pragmatic side of this relation, where humans manipulate and interpret signs on the basis of their "practical knowledge, a factor that should be considered to improve human interactions with Data Visualization tools. The perspective discussed here would bring into light and help frame open problems such as interactions in routine tasks and the interpretation of data through visual interactive tools in daily professional practices. By proposing a light but formal model of investigation of these pragmatic interactions, we would like to contribute to the current debate around data visualization as the new strategic tool for dealing with the growing complexity of big data streams, digitization of life, sensor and hardware-embedded intelligence.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {60},
numpages = {5},
keywords = {pragmatics, data visualization, category theory, semiotics},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399829,
author = {Spreafico, Andrea and Carenini, Giuseppe},
title = {Neural Data-Driven Captioning of Time-Series Line Charts},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399829},
doi = {10.1145/3399715.3399829},
abstract = {The success of neural methods for image captioning suggests that similar benefits can be reaped for generating captions for information visualizations. In this preliminary study, we focus on the very popular line charts. We propose a neural model which aims to generate text from the same data used to create a line chart. Due to the lack of suitable training corpora, we collected a dataset through crowdsourcing. Experiments indicate that our model outperforms relatively simple non-neural baselines.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {61},
numpages = {5},
keywords = {Dataset, Neural Network, Visualization, Machine Learning, Natural Language Generation},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399853,
author = {Costagliola, Gennaro and De Rosa, Mattia and Fuccella, Vittorio and Minas, Mark},
title = {ParVis: A Visual Tool for Exploring Parser Execution Traces},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399853},
doi = {10.1145/3399715.3399853},
abstract = {In this paper, we present ParVis, an interactive visual system for the animated visualization of logged parser trace executions. The system allows a parser implementer to create a visualizer for generated parsers by simply defining a JavaScript module that maps each logged parser instruction into a set of events driving the visual system interface. The result is a set of interacting graphical/text windows that allows users to explore logged parser executions and helps them to have a complete understanding of how the parser behaves during its execution on a given input.We used our system to visualize the behavior of textual as well as visual parsers and describe here its use with the well known CUP parser generator. Preliminary tests with users have provided good feedback on its use.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {62},
numpages = {5},
keywords = {program visualization, parser visualization, visualization, human-computer interaction, visual parsing, graph parsing},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399817,
author = {Nagafuchi, Reona and Matoba, Yasushi and Ikematsu, Kaori and Ishii, Ayaka and Kawahara, Yoshihiro and Siio, Itiro},
title = {Polka: A Water-Jet Printer for Painting on the Grounds},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399817},
doi = {10.1145/3399715.3399817},
abstract = {We propose a method for controlling a device that draws letters and illustrations on large, flat surfaces. Similarly to conventional inkjet printers, the device ejects a volume of water from its nozzle, with the water droplets then forming dots on surfaces such as soil, concrete, and sand. Both the direction of the nozzle and the water pressure can be controlled to enable the device to draw arbitrary two-dimensional patterns within a semicircular region with a radius of six meters. The device can draw letters and illustrations. We have investigated the pressure, the shape of the nozzle, and droplet size in order to avoid further division of the droplets into even smaller droplets while traversing the air. In this paper, we introduce the mechanism of this device and demonstrate how a user can take advantage of this new drawing tool.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {63},
numpages = {5},
keywords = {Public Display, Water-jet Printing, Sprinkler},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399835,
author = {Tharatipyakul, Atima and Choo, Kenny T. W. and Perrault, Simon T.},
title = {Pose Estimation for Facilitating Movement Learning from Online Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399835},
doi = {10.1145/3399715.3399835},
abstract = {There exists a multitude of online video tutorials to teach physical movements such as exercises. Yet, users lack support to verify the accuracy of their movements when following such videos and have to rely on their own perception. To address this, we developed a web-based application that performs human pose estimation using both video inputs from the online video and web camera, then provides different types of visual feedback to a user. Our study suggests that a user's skeleton overlaid on the user's camera feed improves user performance, whereas a user's skeleton on its own or trainer's skeleton with the trainer video offered limited benefits. Our application demonstrates the potential to enhance learning physical movements from online videos and provides a basis for other guidance systems to design suitable visualizations.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {64},
numpages = {5},
keywords = {visualization, Movement guidance, pose estimation},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399917,
author = {De Carolis, Berardina and Argentieri, Domenico},
title = {IBall to Swim: A Serious Game for Children with Autism Spectrum Disorder},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399917},
doi = {10.1145/3399715.3399917},
abstract = {Recent studies show that children affected by Autism Spectrum Disorder (ASD) are more exposed to pathologies related to obesity and lack of movement. Moreover, they are approximately twice as likely to die from drowning than neurotypical ones. Therefore, acquiring good water safety skills is of extreme importance and, at the same time, aquatic activities are a valid opportunity to do some physical activity and reduce sedentary behaviors. "iBall to Swim is a serious game, based on IoT, that through a playful approach allows children with ASD to do activities in an aquatic environment, developing and improving motor skills. The system is made of a swimming ball augmented with lighting, a wetsuit with a heartbeat monitor and wireless bone conduction headphones. A mobile application is used to integrate these components and to measure and monitor the child's performance. To test whether the technology contributed to improve children's motor skills, we performed a test with eleven children with ASD. Their improvement in motor skills has been studied during a water training phase both with the help of the serious game and without. Results show that there was a general improvement in their performance and children were keeping swimming autonomously and for a longer distance when they were stimulated by the game. Furthermore, the children reported enjoyment and the parents asked whether the game could be used routinely with their children. These encouraging findings suggest that "iBall to Swim is a promising way to enhance the learning of the basic notions of swimming and it can be considered a valid tool to help to improve ASD children's health and wellbeing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {65},
numpages = {5},
keywords = {IoT, ASD, swimming, assistive technologies, serious game},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399825,
author = {Ceccaldi, Eleonora and Volpe, Gualtiero},
title = {Towards a Cognitive-Inspired Automatic Unitizing Technique: A Feasibility Study},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399825},
doi = {10.1145/3399715.3399825},
abstract = {In this paper, we present and assess a novel technique for unitizing inspired by a cognitive theory on event structure perception. Unitizing indicates the process of dividing an observation into smaller units. Unitizing is often performed automatically, e.g., by selecting fixed-length windows. Although fast, such approach might result in unit boundaries being placed mid-interaction, eventually affecting observation, annotation, and labeling. We conceived a unitizing technique based on the Event Segmentation theory. In brief, changes drive the perception of boundaries between events (or units): an unexpected change in the observed situation might mean the current event ended and a new one begun. Our technique relies on observed changes for identifying unit boundaries. The first sketch of our technique was recently tested, proving it effective in overcoming the aforementioned shortcomings of fixed-window unitizing. Here, we further explore its feasibility by testing it in a different domain, i.e., solo stage performances, in order to explore the feasibility of adopting our unitizing approach across domains. Our results further support the idea of leveraging the Event Segmentation Theory for the design of an automatic technique for video unitizing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {66},
numpages = {5},
keywords = {linguistic functions, Event Segmentation unitizing, Video parsing, event structure perception},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399830,
author = {Sultanum, Nicole and Murad, Christine and Wigdor, Daniel},
title = {Understanding and Supporting Academic Literature Review Workflows with LitSense},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399830},
doi = {10.1145/3399715.3399830},
abstract = {It is increasingly difficult for researchers to navigate and reach an understanding of a growing body of literature in a field of research. While past works in HCI and data visualization sought to support such activities, few investigated how these workflows are conducted in practice and how practices change in view of support tools. This work contributes a more holistic understanding of this space via a user-centered approach encompassing (a) a formative study on literature review practices of 15 researchers which informed (b) the design of LitSense, a proof-of-concept tool to support literature review workflows, and (c) a week-long study with 12 researchers performing a literature review with Litsense.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {67},
numpages = {5},
keywords = {Longitudinal study, Sensemaking, Bottom-up literature review},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399834,
author = {Dupont, Lancelot and Jouffrais, Christophe and Perrault, Simon T.},
title = {Vibrotactile Feedback for Vertical 2D Space Exploration},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399834},
doi = {10.1145/3399715.3399834},
abstract = {Visually impaired people encounter many challenges in their everyday life, especially when it comes to navigating and representing space. The issue of shopping is addressed mostly on the level of navigation and product detection, but conveying clues about the object position to the user is rarely implemented. This work presents a prototype of vibrotactile wristband using spatiotemporal patterns to help visually impaired users reach an object in the 2D plane in front of them. A pilot study on twelve blindfolded sighted subjects showed that discretizing space in a seven by seven targets matrix and conveying clues with a discrete pattern on the vertical axis and a continuous pattern on the horizontal axis is an intuitive and effective design.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {68},
numpages = {5},
keywords = {object acquisition, wearable computing, visually impaired users, Vibrotactile feedback},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399813,
author = {Bogina, Veronika and Sheidin, Julia and Kuflik, Tsvi and Berkovsky, Shlomo},
title = {Visualizing Program Genres' Temporal-Based Similarity in Linear TV Recommendations},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399813},
doi = {10.1145/3399715.3399813},
abstract = {There is an increasing evidence that data visualization is an important and useful tool for quick understanding and filtering of large amounts of data. In this paper, we contribute to this body of work with a study that compares chord and ranked list for presentation of a temporal TV program genre similarity in next-program recommendations. We consider genre similarity based on the similarity of temporal viewing patterns. We discover that chord presentation allows users to see the whole picture and improves their ability to choose items beyond the ranked list of top similar items. We believe that similarity visualization may be useful for the provision of both the recommendations and their explanations to the end users.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {69},
numpages = {5},
keywords = {Visualization, recommender system, similarity},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399945,
author = {Brunetti, Davide and Cena, Federica and Gena, Cristina and Mensa, Enrico and Vernero, Fabiana},
title = {A Color Map to Compare Reactions Tools in Interactive Systems},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399945},
doi = {10.1145/3399715.3399945},
abstract = {In this paper we study whether visualizations based on color maps can encourage the intuitive interpretation of detailed descriptions, as the ones proposed in the formal model UPRISE, designed to analyze interactive system components, such as reaction tools, which allow users to provide reactions. We carried out a between-subjects experiment where 56 participants had to group 6 systems according to similarity using either color maps or textual descriptions. Results showed that color maps seem to favour inter-user agreement in comparison and grouping tasks.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {70},
numpages = {3},
keywords = {color maps visualization, user reactions, interactive systems},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399942,
author = {Graf, Gaetano and Palleis, Henri and Hussmann, Heinrich},
title = {A Design Space for Advanced Visual Interfaces for Teleoperated Autonomous Vehicles},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399942},
doi = {10.1145/3399715.3399942},
abstract = {Autonomous Vehicles (AVs) are facilitating the development of a diverse set of applications, from human-less delivering to alternative mobility services. There are a variety of challenges for AVs that might be assessed and solved by remote operators, such as sensor data ambiguity, temporary changes to infrastructure, or unexpected interventions by other road users. With this paper, we propose a design space to support the development of appropriate user interfaces for remote situational awareness and teleoperation. The design space is envisioned to discover and evaluate design alternatives before the implementation and to provide a systematic approach towards the development of novel interfaces.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {71},
numpages = {3},
keywords = {Teleoperation, Visual Interfaces, Interaction Design, Design Space, Autonomous Vehicle, User interface},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399969,
author = {Marras, Mirko and Fenu, Gianni},
title = {A Framework for Biometric Recognition in Online Content Delivery Platforms},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399969},
doi = {10.1145/3399715.3399969},
abstract = {In this paper, we introduce a modular framework that aims to empower online platforms with biometric-related capabilities, minimizing the user's interaction cost. First, we describe core concepts and architectural aspects characterizing the proposed framework. Then, as a use case, we integrate it in an e-learning platform to provide biometric recognition at the time of login and continuous identity verification in well-rounded areas of the platform.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {72},
numpages = {3},
keywords = {Usable Security, Continuous, E-Learning, Authentication, Biometrics},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399940,
author = {Spence, Robert and Redmond, Leah},
title = {A New Notation for Interactive Systems},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399940},
doi = {10.1145/3399715.3399940},
abstract = {We describe a new notation for interactive systems. It is based on links between the nodes that represent the various affordances available to the user of an interactive system. Like any notation its main benefit may be that of facilitating communication between the customer who commissions the system, the interaction designer and the implementer. The origin of the new notation was a proposed representation of an app designed to support a person with Type-1 diabetes in the management of their condition, and the use of that representation to investigate navigational transitions carried out by the user. Potential generalization of the notation, as well as its benefits and weaknesses, are being investigated.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {73},
numpages = {2},
keywords = {phone, affordance, usability, app, mobile},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399937,
author = {Kimura, Risa and Jiang, Keren and Zhang, Di and Nakajima, Tatsuo},
title = {A Playful Citizen Science Tool for Casual Users},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399937},
doi = {10.1145/3399715.3399937},
abstract = {We present a playful citizen science tool to explore various protein docking through dance like body actions for casual users. For more attracting casual users, the tool offers a social watching functionality based on a virtual reality platform that presents multiple persons' visual perspectives in a virtual space. We also investigate some preliminary insights of our current tool.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {74},
numpages = {3},
keywords = {Social Watching, Casual Users, Citizen Science, Body Actions},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399967,
author = {Barra, Silvio and Carcangiu, Alessandro and Carta, Salvatore and Podda, Alessandro Sebastian and Riboni, Daniele},
title = {A Voice User Interface for Football Event Tagging Applications},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399967},
doi = {10.1145/3399715.3399967},
abstract = {Manual event tagging may be a very long and stressful activity, due the monotonous operations involved. This is particularly true when dealing with online video tagging, as for football matches, in which the burden of events to tag can consist of many thousands of actions, according to the desired level of granularity. In this work we describe an actual solution, developed for an existing football match tagging application, in which the GUI has been enhanced and integrated with a Voice User Interface, aiming at reducing tagging time and error rate. Empirical tests have revealed the efficiency and the benefits brought by the developed solution.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {75},
numpages = {3},
keywords = {Integrated Voice User Interface, Online Video Tagging, Match tagging},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399946,
author = {Trujillo, Amaury and Buzzi, Maria Claudia and Buzzi, Marina and Senette, Caterina},
title = {A Web App for Teaching Piano to Students with Autism},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399946},
doi = {10.1145/3399715.3399946},
abstract = {This work aims to promote musical learning in order to ease the inclusion of students with autism in the classroom and during musical performances. To this end, a web application was developed using a user-centered design approach involving professionals experienced in training people with autism, to help end users learn the basic concepts of music. The app allows teachers or caregivers to set up a customized learning environment according to each student's needs, so that these students can play songs in collaboration with classmates. Preliminary results suggest a key role for technology in facilitating music teaching and in supporting the learning process for students with autism.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {76},
numpages = {3},
keywords = {Accessibility, Usability, Music, Web applications, Autism},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399952,
author = {Di Martino, Sergio and Vitale, Vincenzo Norman},
title = {An Haptic Interface for Industrial High-Precision Manufacturing Tasks},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399952},
doi = {10.1145/3399715.3399952},
abstract = {Within the Industry 4.0 context, a great number of machineries has been equipped with multiple sensors collecting vast amounts of heterogeneous data, including multimedia ones. In the context of high-precision industrial manufacturing, the output of these sensors can be exploited to leverage on human intelligence for monitoring the quality of the production. Nevertheless, in complex scenarios, the amount of sensed data could lead to a visual and acoustic overload for the Decision Maker. In this poster we propose a multi-modal user interface (UI) we devised to support the Decision Maker in monitoring the outcome of high-precision manufacturing machineries. In particular, to mitigate the acoustic and visual overloads, we propose the use of the haptic channel, both to control the playback of the collected data stream, and to get feedbacks about anomalous situations.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {77},
numpages = {3},
keywords = {Haptic and Multi-modal Interfaces, Data Visualization, Industry 4.0},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399939,
author = {Bernasconi, Eleonora and Ceriani, Miguel and Mecella, Massimo and Catarci, Tiziana and Capanna, Maria Cristina and Di Fazio, Clara and Marcucci, Roberto and Pender, Erik and Petriccione, Fabio Maria},
title = {ARCA. Semantic Exploration of a Bookstore},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399939},
doi = {10.1145/3399715.3399939},
abstract = {In this demo paper, we present ARCA, a visual-search based system that allows the semantic exploration of a bookstore. Navigating a domain-specific knowledge graph, students and researchers alike can start from any specific concept and reach any other related concept, discovering associated books and information. To achieve this paradigm of interaction we built a prototype system, flexible and adaptable to multiple contexts of use, that extracts semantic information from the contents of a books' corpus, building a dedicated knowledge graph that is linked to external knowledge bases.The web-based user interface of ARCA integrates text-based search, visual knowledge graph navigation, and linear visualization of filtered books (ordered according to multiple criteria) in a comprehensive coordinated view aimed at exploiting the underlying data while avoiding information overload and unnecessary cluttering. A proof-of-concept of ARCA is available online at http://arca.diag.uniroma1.it},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {78},
numpages = {3},
keywords = {books' catalog, visual search interface, knowledge graph},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399955,
author = {De Stefano, Manuel and Gambardella, Michele Simone and Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea},
title = {CASpER: A Plug-in for Automated Code Smell Detection and Refactoring},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399955},
doi = {10.1145/3399715.3399955},
abstract = {During software evolution, code is inevitably subject to continuous changes that are often performed by developers within short and strict deadlines. As a consequence, good design practices are often sacrificed, possibly leading to the introduction of sub-optimal design or implementation solutions, the so-called code smells. Several studies have shown that the presence of code smells makes the source code more change- and fault-prone, reduces productivity, and causes greater rework and more significant design efforts for developers. Refactoring is the practice that developers may use to remove code smells without changing the external behavior of the source code. However, it requires much time and effort and is poorly automated, often leading developers to prefer keeping low-quality code instead of spending time in designing and performing refactoring operations. To mitigate this problem and support developers throughout the process of code smell identification and refactoring, in this paper we present cASpER, a IntelliJ IDEA plugin that provides visual and semi-automatic support for detection and refactoring four different types of code smells.Tool. Jetbrains: https://plugins.jetbrains.com/plugin/13659-casperVideo. https://youtu.be/HBWF8fFJM8s},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {79},
numpages = {3},
keywords = {Code smells, Refactoring, Automated Software Engineering},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399953,
author = {Afkari, Hoorieh and Maqui, Val\'{e}rie and Arend, B\'{e}atrice and Heuser, Svenja and Sunnen, Patrick},
title = {Designing Different Features of an Interactive Tabletop Application to Support Collaborative Problem-Solving},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399953},
doi = {10.1145/3399715.3399953},
abstract = {The design space of tangible and multi-touch tabletop interfaces is complex, and little is known about how the different characteristics of tangible and multi-touch interactive features affect collaboration strategies. With this work, we report on five different features designed for an interactive tabletop application to support collaborative problem-solving. We present the design details and describe preliminary results obtained from a user study with 15 participants.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {80},
numpages = {3},
keywords = {Collaboration, Interactive Tabletops},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399936,
author = {Wang, Yuzhou and Masoodian, Masood},
title = {Designing Visual Tools to Facilitate Human-Centered Design},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399936},
doi = {10.1145/3399715.3399936},
abstract = {Human-Centred Design (HCD) relies on the use of many methods (e.g. interviews, observations) originating from other disciplines such as social sciences (e.g. ethnography). Such methods often rely on the use of visual tools (e.g. photographs and illustrations) to better facilitate the involvement of the participants in the design process. Most HCD practitioners, however, do not have the necessary visual design skills, and as such, need to work with visual communication designers to co-create visual tools to support their design projects. In this poster, we present a multidisciplinary approach to guide the process of co-creating such visual tools.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {81},
numpages = {3},
keywords = {co-design, visual methods, Visual material, visual design, qualitative research, human-centered design},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399951,
author = {Coppola, Giuseppe and Costagliola, Gennaro and De Rosa, Mattia and Fuccella, Vittorio},
title = {Domus: A Multi-User TUI Game for Multi-Touch Tables},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399951},
doi = {10.1145/3399715.3399951},
abstract = {In this paper, we present Domus, an educational game for multi-touch tables that makes use of a Tangible User Interface (TUI). In particular, it uses as game pieces physical objects called tangibles, which have conductive feet arranged in different patterns so that the device can recognize their position/rotation.Domus is designed to be used in museum environments. The game board, in fact, simulates the environments of an ancient Roman domus, inspired by those present in Pompeii. Depending on the position of the tangibles on the screen and on the actions carried out by the users, the system will provide notions and multimedia contents concerning the daily life of the ancient Romans, thus allowing gradual learning during the gaming session.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {82},
numpages = {3},
keywords = {user interaction, tui, tangible, tangible user intarface},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399947,
author = {Artizzu, Valentino and Fara, Davide and Macis, Riccardo and Spano, Lucio Davide},
title = {FeedBucket: Simplified Haptic Feedback for VR and MR},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399947},
doi = {10.1145/3399715.3399947},
abstract = {Standard development libraries for Virtual and Mixed Reality support haptic feedback through low-level parameters, which do not guide developers in creating effective interactions. In this paper, we report some preliminary results on a simplified structure for the creation, assignment and execution of haptic feedback for standard controllers with the optional feature of synchronizing an haptic pattern to an auditory feedback. In addition, we present the results of a preliminary test investigating the users' ability in recognizing variations in intensity and/or duration of the stimulus, especially when the two dimensions are combined for encoding information.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {83},
numpages = {3},
keywords = {virtual reality, mixed reality, haptic library, haptic feedback},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399938,
author = {Shimasato, Keita and Kono, Yasuyuki},
title = {Gaze-Based Moving Target Acquisition Using Pseudo Stopping for the Time Predicted via Fitts' Law},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399938},
doi = {10.1145/3399715.3399938},
abstract = {This paper describes a technique of gaze-based moving target acquisition achieved by pseudo stopping the target for the time predicted via Fitts' Law, after saccades have been detected. This technique only requires eye-movements for the acquisition of moving targets. The results indicate that participants were able to acquire targets moving at various speeds and with different widths.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {84},
numpages = {3},
keywords = {Fitts' Law, gaze interaction, moving target acquisition},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399950,
author = {Takada, Tetsuji and Hattori, Yumeji},
title = {Giving Motivation for Using Secure Credentials through User Authentication by Game},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399950},
doi = {10.1145/3399715.3399950},
abstract = {One of the issues in the knowledge-based user authentication is that users do not set and use a secure credential. Some methods exist to be able to resolve this issue, such as password policy, education, and password meter. However, these countermeasures impose a usability cost that is difficult for many users to accept. Therefore, these measures have not propelled users to use secure credentials in user authentication. We consider that motivating users is necessary to voluntarily accept the cost of using secure credentials. Thus, we attach a role-playing game function to pattern-based user authentication, and provide an incentive to users through user authentication. We conducted a small experiment with eight participants, and the result demonstrated that the prototype system has the potential to prompt users to use secure credentials.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {85},
numpages = {3},
keywords = {usability, passwordmeter, gamification, nudge, user authentication, security, motivation},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399970,
author = {Schufrin, Marija and Sessler, David and Reynolds, Steven Lamarr and Ahmad, Salmah and Mertz, Tobias and Kohlhammer, J\"{o}rn},
title = {Information Visualization Interface on Home Router Traffic Data for Laypersons},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399970},
doi = {10.1145/3399715.3399970},
abstract = {With the aim to increase the awareness of the everyday internet user for the own home network traffic, we present two interactive visualization interfaces for visual exploration of home router traffic records. Thereby we differentiate between users with a present intrinsic motivation for the topic and those with absent intrinsic motivation. Therefore, gamification in the first interface is used to maintain motivation of the first type of user, while the storytelling concept based on the hero's journey in the second interface aims at increasing the perceived incentives for the second user group.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {86},
numpages = {3},
keywords = {cyber-security awareness, usable security, gamification, Information visualization, storytelling},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399944,
author = {Kwon, Taewoong and Shin, Iksoo and Kim, Kyuil and Song, Jungsuk and Lee, Jun},
title = {Integrated Visual Analytics Approach against Multivariate Cybersecurity Attack},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399944},
doi = {10.1145/3399715.3399944},
abstract = {As security threats rapidly spread all over the world, it is critical that network traffic is monitored and protected from abnormal attacks during 24/7. Even though various security devices (Rep., network intrusion detection system, NIDS) had utilized to guarantee a solid network security, it still depends on human being due to complex patterns from unknown threats. This study introduces a graphical interactive system for representing and understanding multivariate cybersecurity attacks. In particular, the interface enhances intuitive judgments combined with machine learning-based analysis of suspicious traffic},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {87},
numpages = {3},
keywords = {Visual Insight, Real-world Application, Intrusion Detection, Machine Learning, Cybersecurity},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399957,
author = {Pietroszek, Krzysztof},
title = {Interaction in Volumetric Film: An Overview},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399957},
doi = {10.1145/3399715.3399957},
abstract = {Volumetric filmmaking is a novel and inherently interactive medium. In volumetric film the viewer takes over the director's responsibility for selecting the point of view from which the story is being told. The viewer becomes the cinematographer and the editor of the film at the moment of viewing. In this paper, we provide an overview of interaction modes in volumetric film and compare volumetric film to both traditional film and 360 video.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {88},
numpages = {2},
keywords = {volumetric film, volumetric capture, augmented reality, mixed reality, virtual reality, interactive narrative, photogrammetry},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399958,
author = {Mancini, Maurizio and Gallagher, Conor Patrick and Niewiadomski, Radoslaw and Huisman, Gijs and Bruijnes, Merijn},
title = {Introducing Artificial Commensal Companions},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399958},
doi = {10.1145/3399715.3399958},
abstract = {The term commensality refers to "sharing food and eating together in a social group. In this paper, we hypothesize that it would be possible to have the same kind of experience in a HCI setting, thanks to a new type of interface that we call Artificial Commensal Companion (ACC), that would be beneficial, for example, to people who voluntarily choose or are constrained to eat alone. To this aim, we introduce an interactive system implementing an ACC in the form of a robot with non-verbal socio-affective capabilities. Future tests are already planned to evaluate its influence on the eating experience of human participants.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {89},
numpages = {3},
keywords = {hci, companion, robot, food, commensality, nonverbal, multimodal interaction, eating},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399930,
author = {Isomursu, Pekka and Virkkula, Minna and Niemel\"{a}, Karoliina and Juntunen, Jouni and Kumpuoja, Janne},
title = {Modified AttrakDiff in UX Evaluation of a Mobile Prototype},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399930},
doi = {10.1145/3399715.3399930},
abstract = {We are developing a novel mobile application with a visual user interface for the visitors of museums and art galleries. In this paper we focus on the AttrakDiff method and the modifications we have made to it for our User Experience (UX) evaluation work. The modifications we have made help in getting a deeper user insight from the AttrakDiff questionnaire than with the original method.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {90},
numpages = {3},
keywords = {User experience, Case study, User study, User evaluation tools},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399968,
author = {Amal, Saeed and Adam, Mustafa and Brusilovsky, Peter and Minkov, Einat and Kuflik, Tsvi},
title = {Personalized Multifaceted Visualization of Scholars Profiles},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399968},
doi = {10.1145/3399715.3399968},
abstract = {When we consider our CV, it is full of entities - where we studied, where we worked, who we collaborated with on a project or on a paper. Entities we are linked to are part of our profile and as such they help to understand who we are and what are we interested in. Hence, we adapt the typed entity-relation graph (profile) concept and based on this presentation we propose a personalized multifaceted graph visualization for the entity profile. In the context of an academic conference, we allow scholars to explore a graph of related entities and a word cloud representing the links, providing the user a comprehensive, compact and structured overview about the explored scholar. We evaluated our proposed personalized multifaceted visualization in a user study with encouraging results which showed that this visualization is engaging, easy to use and helpful.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {91},
numpages = {3},
keywords = {Personalized multifaceted graph visualization, Entity profiling},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399948,
author = {Scuri, Sabrina and Nunes, Nuno Jardim},
title = {PowerShare 2.0: A Gamified P2P Energy Trading Platform},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399948},
doi = {10.1145/3399715.3399948},
abstract = {Peer-to-Peer (P2P) energy trading has emerged as the next-generation energy management mechanism that enables energy consumers and prosumers -- i.e., households that function as both energy producers and consumers - to exchange energy directly with each other. The full development of such a decentralized energy market poses several challenges to the HCI community, especially when it comes to motivating energy users to cooperate with utilities to provide ancillary services that would improve the distribution system as a whole. How to foster this complex cooperation mechanism and ensure participation is still an open research question. Drawing on results from a previous study, we have identified a set of relevant theories and design concepts - namely, Collective Efficacy, Norm Activation Model, and Gamification - that could help answer that question. In this article, we present PowerShare 2.0, a gamified P2P energy trading platform designed based on such concepts and report the initial feedback collected during a preliminary study with both consumers and prosumers.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {92},
numpages = {3},
keywords = {P2P energy trading, Collective efficacy, Gamification, Norm activation model},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399941,
author = {Galesi, Giulio and Giunipero, Luciano and Leporini, Barbara and Verdi, Gianni},
title = {SelfLens: A Portable Tool to Facilitate All People in Getting Information on Food Items},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399941},
doi = {10.1145/3399715.3399941},
abstract = {Independently selecting food items while shopping, or storing and cooking food items correctly can be a very difficult task for people with special needs. Product labels on food packaging contain an ever-increasing amount of information, which can also be in a variety of languages. The amount of information and also the features of the text can make it difficult or impossible to read, in particular for those with visual impairments or the elderly. Several tools or applications are available on the market or have been proposed to support this type of activity (e.g. barcode or QR code reading), but they are limited and may require the user to have specific digital skills. Moreover, repeatedly using an application to read the label contents can require numerous steps on a touch-screen, and consequently be time-consuming. In this work, a portable tool is proposed to support people in reading the contents of labels and acquiring additional information, while they are using the item at home or shopping at the supermarket. The aim of our study is to propose a simple portable assistive technology tool which 1) can be used by anyone, regardless of their digital personal skills 2) does not require a smartphone or complex device, 3) is a low-cost solution for the user.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {93},
numpages = {3},
keywords = {smart home, reading labels, Interactive tool, shopping},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399949,
author = {Humayoun, Shah Rukh and Faizan, Muhammad and Zafar, Zuhair and Berns, Karsten},
title = {Space-Free Gesture Interaction with Humanoid Robot},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399949},
doi = {10.1145/3399715.3399949},
abstract = {In general, humanoid robots mostly use fixed-devices (e.g., camera or sensors) to detect human non-verbal communication, which have limitations in many real-life scenarios. Wearable devices could play an important role in many real-life scenarios. To address this, we propose using Myo armband for human-robot interaction using hand- and arm-based gestures. We present our end-to-end Spagti framework that is used first to train the user gestures using Myo armband and then to interact with a humanoid robot, called ROBIN, in real-time using space-free gestures.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {94},
numpages = {3},
keywords = {Humanoid robot, non-verbal communication, human-robot interaction, space-free interaction},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3401952,
author = {Hamanishi, Natsuki and Rekimoto, Jun},
title = {SuppleView: Rotation-Based Browsing Method by Changing Observation Angle of View for an Actor in Existing Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3401952},
doi = {10.1145/3399715.3401952},
abstract = {In this paper, we proposed the rotation based browsing method for video learning in personal training. SuppleView, which is flexible in respect of the user's physical position while viewing a video, enables coordinate translation free viewing between an observer and an actor. Previous work on video learning have not enough explored the limitation on the observation angle, although its angle effects for observer's comprehension and caused only in video learning not in observation with the actual trainer. The method solve this basic limitation by inferring the 3D pose of frames in a video. Based on those poses, we create an virtual agent with 3D model as an actor of movements, that is same with the movement in an original 2D video. The system transition for the two actors depends on the physical rotation of the user's head so that the angle of view for observing the actor also changes. Hence, the content rendering in proposed viewer could be provided to trainees as in kind-full form for their observation in the point of an observation angle of view. We report the method overview and our prototyping to show the proof of concept.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {95},
numpages = {3},
keywords = {Movement Browsing, Personal Training, Movement Guidance},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399956,
author = {Sheehan, Shane and Luz, Saturnino and Albert, Pierre and Masoodian, Masood},
title = {TeMoCo-Doc: A Visualization for Supporting Temporal and Contextual Analysis of Dialogues and Associated Documents},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399956},
doi = {10.1145/3399715.3399956},
abstract = {A common task in a number of application areas is to create textual documents based on recorded audio data. Visualizations designed to support such tasks require linking temporal audio data with contextual data contained in the resulting documents. In this paper, we present a tool for the visualization of temporal and contextual links between recorded dialogues and their summary documents.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {96},
numpages = {3},
keywords = {contextual visualization, speech visualization, temporal mosaics, temporal visualization, Assessment of medical communication},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399943,
author = {Centofanti, Carlo and D'Errico, Alessandro and Caruso, Federica and Peretti, Sara},
title = {The CrazySquare Solution: A Gamified ICT Tool to Support the Musical Learning in Pre-Adolescents},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399943},
doi = {10.1145/3399715.3399943},
abstract = {In this paper, we present the current prototype of the CrazySquare Project which is aimed to provide a gamified ICT (Information and Communications Technology) solution for musical education. The project is inspired by Gordon's Musical Learning Theory. It is dedicated to the guitar since it is one of the most played instrument in Italian's Middle Schools. The TPACK (Technological Pedagogical Content Knowledge) framework has been used as a way to effectively integrate the technology into teaching activities. Moreover, the CrazySquare project follows an iterative process based on the TEL-oriented UCD approach. Currently, after carrying out an expert-based evaluation with several domain-experts, we are designing the user-based evaluation phase which will conclude the second iteration.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {97},
numpages = {3},
keywords = {Pre-adolescents, Music Education, Technology-Enhanced Learning, Playing Guitar},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399960,
author = {Amato, Federica and Di Gregorio, Marianna and Monaco, Clara and Sebillo, Monica and Tortora, Genoveffa and Vitiello, Giuliana},
title = {The Therapeutic Use of Humanoid Robots for Behavioral Disorders},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399960},
doi = {10.1145/3399715.3399960},
abstract = {In this work, we illustrate an innovative treatment for patients affected by Behavioral Disorders, that relies on the use of Pepper humanoid robot. This new therapeutic methodology was created to support and make the therapist's work more attractive. Pepper is equipped with a tablet and two identical cameras. The tablet is used to let the patient interact with the application, while the cameras are used to capture their real-time emotions to understand the degree of attention and any difficulty that they may have. The interaction with the tablet takes place through some exercises in the form of games. The exercises performed by the subject are analyzed and combined with the data captured by the cameras. The combination of these data is processed to propose appropriate levels of therapeutic activities. This process leads to the digitization of the patient's healing path so that any improvement (or worsening) is monitored and causes Pepper to become a reliable and predictable technological intermediary for the child. The work has been developed in collaboration with a diagnostic and therapeutic center. Interacting with a humanoid robot, children exhibit a higher engagement, which can be explained, according to the psychologists, by the fact that a robot is emotionally less rich than human beings, and the patient feels less scared.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {98},
numpages = {3},
keywords = {Socially Assisted Robotics, Emotion recognition, Behavioral Disorder},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399959,
author = {Veneruso, Silvestro V. and Catarci, Tiziana and Ferro, Lauren S. and Marrella, Andrea and Mecella, Massimo},
title = {V-DOOR: A Real-Time Virtual Dressing Room Application Using Oculus Rift},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399959},
doi = {10.1145/3399715.3399959},
abstract = {In recent years and with its accessibility, the use of online shopping for clothing has increased. Virtual Dressing Rooms (VDRs) represent an effective way to enact the ability to "try before buying, thus removing an important obstacle for online shopping. While most of the VDR tools that have been realized so far are based on Augmented Reality and are installed directly inside the retail shops, this paper proposes a real-time VDR application titled V-DOOR that leverages the features of Oculus Rift to create an immersive experience that enables customers to try on clothes virtually in the comfort of their own home rather than physically in the retail shop.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {99},
numpages = {3},
keywords = {Virtual Dressing Room, Oculus Rift, Virtual Reality, IT-SHIRT},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399962,
author = {Cabral, Eric M. and Milios, Evangelos E. and Minghim, Rosane},
title = {Visual Analysis of Interactive Document Clustering Streams},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399962},
doi = {10.1145/3399715.3399962},
abstract = {Interactive clustering techniques play a key role by putting the user in the clustering loop, allowing her to interact with document group abstractions instead of full-length documents. It allows users to focus on corpus exploration as an incremental task. To explore Information Discovery's incremental aspect, this article proposes a visual component to depict clustering membership changes throughout a clustering iteration loop in both static and dynamic data sets. The visual component is evaluated with an expert user and with an experiment with data streams.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {100},
numpages = {3},
keywords = {Text visualization, Document streams, Visual analytics, Document clustering},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399954,
author = {Pecorelli, Fabiano and Di Lillo, Gianluca and Palomba, Fabio and De Lucia, Andrea},
title = {VITRuM: A Plug-In for the Visualization of Test-Related Metrics},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399954},
doi = {10.1145/3399715.3399954},
abstract = {Software testing is the first weapon against software faults, used by developers to preventively locate implementation errors in the exercised production code that may cause critical failures to the inner-working of software systems. According to recent findings, the effectiveness of testing might be not only due to its ability to cover the production code but also to some other properties, like code quality. Among other aspects, the literature reported that an advanced visualization of test-related metrics, e.g., test code coverage on production code, result to be a key strength for developers when dealing with software faults. In this paper, we propose VITRuM (VIsualization of Test-Related Metrics), an IntelliJ plug-in able to provide developers with an advanced visual interface of both static and dynamic test-related metrics that has the potential of making them more able to diagnose production code faults. The plug-in is available in the official JetBrains Plugins Repository. A video showing the tool in action is available at https://youtu.be/kFE81eYPgUg.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {101},
numpages = {3},
keywords = {Test Code Quality, Software Testing, Advanced Visual Interfaces},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3399961,
author = {Battistoni, Pietro and Di Gregorio, Marianna and Giordano, Domenico and Sebillo, Monica and Tortora, Genoveffa and Vitiello, Giuliana},
title = {Wearable Interfaces and Advanced Sensors to Enhance Firefighters Safety in Forest Fires},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399961},
doi = {10.1145/3399715.3399961},
abstract = {The forest fires represent a social emergency that requires significant economic and organizational commitment. Safety and the lack of reliable and timely localization of firefighters is a big problem. In this paper, we present Karya Advanced Sensor, an automatic, accurate, and reliable IT solution able to locate firefighters in harsh environments and support decision making activities at control rooms. The system consists of sensors perfectly integrated into firefighters' uniforms, which are used to monitor in real-time individual operators' activities as well as the entire fire area. In particular, in case a firefighter gets injured, the system will activate the rescue teams quickly, as there will be a constant link between the firefighters and the medical assistance. The firefighter can also specify the reason for the accident, which is critical information for a more timely and appropriate health intervention. Moreover, the system is able to perform an automatic real-time mapping of forest fires and possibly estimate its propagation rate, providing precious support to control rooms, which are the center of the team coordination.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {102},
numpages = {3},
keywords = {Firefighters Safety, Wearable Interfaces, Forest Fire, Sensor, Safety},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400859,
author = {Ader, Lilian G. Motti and Caulfied, Brian and Bossavit, Beno\^{\i}t and El Raheb, Katerina and Raynal, Mathieu and Vigouroux, Nadine and Ting, Karine Lan Hing and Irani, Pourang P. and Vanderdonckt, Jean},
title = {Visual User Interfaces for Human Motion},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400859},
doi = {10.1145/3399715.3400859},
abstract = {Visual interfaces are important in human motion to capture it, to visualize it, and to facilitate motion-based interactive systems. This workshop aims at providing a platform for researchers, designers and users to discuss the challenges related to the design of visual interfaces for motion-based interaction, in terms of visualization (e.g. graphical user interface, multimodal feedback, evaluation) and processing (e.g., data collection, treatment, interpretation, recognition) of human movement (e.g., motor skills, amplitude of movements, limitations). We will share experiences, lessons learned and elaborate tools for developing all the possible applications going forward.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {103},
numpages = {2},
keywords = {motion capture, gesture recognition, Motion-based interaction, wearable devices, movement computing},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400863,
author = {Keck, Mandy and Kammer, Dietrich and Ferreira, Alfredo and Giachetti, Andrea and Groh, Rainer},
title = {VIDEM 2020: Workshop on Visual Interface Design Methods},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400863},
doi = {10.1145/3399715.3400863},
abstract = {Currently, both understanding and developing interactive visual interfaces become ever more challenging, since different visual solutions exist that involve large interdisciplinary teams and deal with massive amounts of data, a wide range of interaction techniques, and domain-specific aspects. At the same time, traditional design methods become obsolete with more and more resources and knowledge that needs to be acquired. The proposed workshop provides a forum for discussing experimental and theoretical techniques, frameworks, and prototyping methods to design visual interfaces in different domains such as data visualization, tangible and embedded interaction, extended and mixed reality, multi-modal interfaces, and others.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {104},
numpages = {2},
keywords = {prototyping techniques, design methods, data visualization, data visualization literacy, teaching methods},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400864,
author = {De Russis, Luigi and Kumar, Neha and Mathur, Akhil},
title = {Data4Good: Designing for Diversity and Development},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400864},
doi = {10.1145/3399715.3400864},
abstract = {We are witnessing unprecedented datafication of the society we live in, alongside rapid advances in the fields of Artificial Intelligence and Machine Learning. However, emergent data-driven applications are systematically discriminating against many diverse populations. A major driver of the bias are the data, which typically align with predominantly Western definitions and lack representation from multilingually diverse and resource-constrained regions across the world. Therefore, data-driven approaches can benefit from integration of a more human-centred orientation before being used to inform the design, deployment, and evaluation of technologies in various contexts. This workshop seeks to advance these and similar conversations, by inviting researchers and practitioners in interdisciplinary domains to engage in conversation around how appropriate human-centred design can contribute to addressing data-related challenges among marginalised and under-represented/underserved groups.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {105},
numpages = {2},
keywords = {AI for social good, multilingual/multicultural contexts, diversity, interdisciplinary computing, data literacies},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400869,
author = {Antoniou, Angeliki and De-Carolis, Berardina and Raptis, George and Gena, Cristina and Kuflik, Tsvi and Dix, Alan and Origlia, Antonio and Lepouras, George},
title = {AVI2CH 2020: Workshop on Advanced Visual Interfaces and Interactions in Cultural Heritage},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400869},
doi = {10.1145/3399715.3400869},
abstract = {AVI2CH is a meeting place for researchers and practitioners focusing on the application of advanced information and communication technology in cultural heritage (CH) with a specific focus on user interfaces, visualization and interaction. It builds on a series of PATCH workshops, since 2007 including three at AVI and also a series of European workshops on cultural informatics. Eleven papers range from novel interfaces in museums to wider community engagement; all share a common mission to ensure that the latest digital technology helps preserve the past in ways that enrich the lives of current and future generations},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {106},
numpages = {2},
keywords = {User Interface, Advanced Visualization, Cultural Heritage},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400870,
author = {Desolda, Giuseppe and Deufemia, Vincenzo and Gena, Cristina and Matera, Maristella and Patern\`{o}, Fabio and Treccani, Barbara},
title = {EMPATHY: Empowering People in Dealing with Internet of Things Ecosystems (Workshop)},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400870},
doi = {10.1145/3399715.3400870},
abstract = {The Internet of Things is a pervasive technology widely adopted in several contexts ranging from smart homes to automotive cars. In this context, the End User Development is gaining momentum: different solutions support non-technical users to define the smart objects behavior to better satisfy their needs. This workshop aimed to stimulates participants in providing discussions in line with the workshop themes and goals, for example, user mental models, acceptance of EUD solutions, the role of AI in EUD tools, humanoid robots were discussed.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {107},
numpages = {1},
keywords = {Internet of Things (IoT), End-User Development (EUD)},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400862,
author = {Angelini, Marco and Santucci, Giuseppe},
title = {ITAVIS: 2nd Italian Visualization &amp; Visual Analytics Workshop},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400862},
doi = {10.1145/3399715.3400862},
abstract = {Data-driven analysis, AI, machine learning and modern data science pipelines of analysis are becoming more and more important possibilities when coping with problem solving. In this respect, the capability to explore data, understands how algorithmic approaches work and steer them toward the desired goals make Visualization and Visual Analytics strong research fields in which to invest efforts. While this importance has been understood by several countries (e.g., USA, Germany, France) that created strong national communities around these research fields, in Italy the research efforts in these fields are still disjointed. With the second edition of ITAVIS we want to consolidate and expand on the encouraging results obtained from the first edition (ITA.WA.- Italian Visualization &amp; Visual Analytics workshop). The goal is to make an additional step toward the creation of an Italian research community on these topics, allowing identification of research directions, joining forces in achieving them, linking researchers and practitioners and developing common guidelines and programs for teaching activities on the fields of Visualization and Visual Analytics.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {108},
numpages = {2},
keywords = {Visual Analytics, teaching activities, Visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

@inproceedings{10.1145/3399715.3400860,
author = {Reis, Thoralf and Bornschlegl, Marco X. and Hemmje, Matthias L.},
title = {Big Data Analysis, AI, and Visualization Workshop: Road Mapping Infrastructures for Artificial Intelligence Supporting Advanced Visual Big Data Analysis},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3400860},
doi = {10.1145/3399715.3400860},
abstract = {The overall scope and goal of the workshop is to bring together researchers active in the areas of Artificial Intelligence (AI), Big Data Analysis, and Visualization to achieve a road map, which can support the acceleration in research and data science activities by means of transforming, enriching, and deploying AI models and algorithms as well as intelligent advanced visual user interfaces supporting creation, configuration, management, and usage of distributed Big Data Analysis. Big Data Analysis and AI mutually support each other: AI-powered algorithms empower data scientists to analyze Big Data and thereby exploit its full potential whereas Big Data enables AI experts to comfortably design, validate, and deploy AI models. One of the workshop's objectives is the examination of the importance and necessity of a third, a more straightforward relationship of Big Data and AI: AI supporting all user stereotypes and organizations involved in Big Data Analysis on their exploration journey from raw input data to insight and effectuation.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {109},
numpages = {2},
keywords = {AI2VIS4BigData, Big Data Analysis, AI, Visualization},
location = {Salerno, Italy},
series = {AVI '20}
}

