@inproceedings{10.1145/2254556.2254558,
author = {Maybury, Mark T.},
title = {Usable Advanced Visual Interfaces in Aviation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254558},
doi = {10.1145/2254556.2254558},
abstract = {Aviation systems represent a rich, difficult, and critical environment for usability. The life critical importance of aviation systems complemented by with their complexity and the diversity and uncertainty of environments in which they operate, place them necessarily on the avant-guard of usability. We describe the broad range of aviation missions, operators, and operating environments and relate these to key dimensions of usability and visual interfaces. We illustrate this concretely with two case examples: ground control stations for remotely piloted aircraft and air traffic management systems. We explore how in both these cases advances in machine autonomy and human control promise to enhance operator situational awareness and control, however, not without challenges. We share guidelines to ensure effective usability to include: require usability as a key performance parameter, architect and design systems and operations that incorporate affordances and fault tolerance, employ standards to increase learnability and interoperability, instrument environments to tailor usability, and assess effects (Maybury 2012). Finally, remaining challenges and future directions are discussed.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {2–3},
numpages = {2},
keywords = {usability, tailored interaction, remotely piloted aircraft, learnability, aviation, ground control stations, autonomy, control, interoperability, instrumentation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254559,
author = {Kurlander, David},
title = {Advanced Interface Productization: Lessons Learned},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254559},
doi = {10.1145/2254556.2254559},
abstract = {Over the years I have managed several research projects relating to advanced interfaces and have turned them into shipping products. Microsoft Comic Chat uses automatic illustration generation and the visual language of comics to present online conversations. Microsoft's Mobile Internet Toolkit renders web interfaces on a variety of mobile devices. Microsoft Surface is a multi-touch, object-sensing table-top display. More recently I have been advising start-ups in the mobile content/ubiquitous computing area. Juggling the competing interests of research and product development has been often interesting, sometimes painful, and always challenging. In my invited talk, I will discuss lessons learned while productizing interface technology, including selecting the product to ship, balancing research and product requirements, navigating management's whims of the day, setting goals, and evaluating the results, as well as what has worked and what has not, and why certain efforts have been more successful than others. Here I present a sampling of these lessons.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {4–5},
numpages = {2},
keywords = {user interfaces, comics, table-top displays, automatic illustration generation, internet chat, object sensing interfaces, experience, mobile web, mobile devices, adaptive interfaces, productization, research, multi-touch interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254560,
author = {Li, Yang},
title = {Gesture-Based Interaction: A New Dimension for Mobile User Interfaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254560},
doi = {10.1145/2254556.2254560},
abstract = {Today, smart phones with touchscreens and sensors are the predominant, fastest growing class of consumer computing devices. However, because these devices are used in diverse situations, and have unique capabilities and form factors, they also raise new user interface challenges, and at the same time, offer great opportunities for impactful HCI research.In this talk, I will focus on gesture-based interaction, an important interaction behavior enabled by touchscreens and built-in sensors, which sets mobile interaction apart from traditional graphical user interfaces.I will first talk about gesture shortcuts in the context of Gesture Search [1], a tool that allows users to quickly access applications and data on the phone by simply drawing a few gestures (http://www.google.com/mobile/gesture-search). Gesture Search flattens mobile phones' UI hierarchy by alleviating the need for navigating the interface. Gesture Search has been released and is invoked hundreds of thousands of times per day by a large user population.I will then cover several related projects that furthered our investigation into gesture shortcuts, including using gestures for target acquisition [3], crowd sourcing-based gesture recognition [5] and our early exploration on motion gestures [4, 6, 7].Finally, I will turn to discuss multi-touch gestures for direct manipulation of an interface, the dominant class of gesture-based interaction on existing commercial devices. Multi-touch gestures are intuitive and efficient to use, but can be difficult to implement. I will discuss tools to support developers, allowing them to more easily create multi-touch interaction behaviors by demonstration [2].These projects investigated various aspects of gesture-based interaction on mobile devices. They help open a new dimension for mobile interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {6},
numpages = {1},
keywords = {direct manipulation, crowd sourcing, touchscreen and motion gestures, programming by demonstration, mobile interaction, target acquisition, shortcuts, gesture-based interaction, gesture recognition},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254561,
author = {Shedroff, Nathan and Noessel, Chris},
title = {Make It so: Learning from Sci-Fi Interfaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254561},
doi = {10.1145/2254556.2254561},
abstract = {Interfaces from science fiction films and television offer lessons to interaction designers and other developers of real world interfaces that are humorous, prophetic, inspiring, and practical.Science fiction interfaces are more than fun. They reflect current interface understandings on the part of developers and expectations on the part of users. Production designers are allowed to develop "blue-sky" examples that, while lacking rigorous development with users, coalesce influential examples for practicing designers. Interaction designers can learn from these examples. This presentation will describe some of the insights found from a 5 year mission to all corners of science fiction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {7–8},
numpages = {2},
keywords = {apologetics, interface design, interaction design, science fiction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254563,
author = {Ghiani, Giuseppe and Patern\`{o}, Fabio and Santoro, Carmen},
title = {Push and Pull of Web User Interfaces in Multi-Device Environments},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254563},
doi = {10.1145/2254556.2254563},
abstract = {In this work we present an environment able to support users in seamless access to Web applications in multi-device contexts. The environment supports dynamic push and pull of interactive Web applications, or parts of them, across desktop and mobile devices while preserving their state.We describe mechanisms for sharing information regarding devices, users, and Web applications with various levels of privacy and report on first experiences with the proposed environment.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {10–17},
numpages = {8},
keywords = {multi-device environments, continuity, migratory Web user interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254564,
author = {Segura, Vin\'{\i}cius C. V. B. and Barbosa, Simone D. J. and Sim\~{o}es, Fabiana Pedreira},
title = {UISKEI: A Sketch-Based Prototyping Tool for Defining and Evaluating User Interface Behavior},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254564},
doi = {10.1145/2254556.2254564},
abstract = {Sketching is viewed as an efficient way to design the user interface. However, there are few tools that help designers go from sketching the user interface to simulating its behavior with endusers during early evaluation. We have developed a pen-based tool called UISKEI, which goes beyond allowing designers to define simple navigation between user interface snapshots, into allowing them to define more complex interactive behavior with conditional changes in the state of the user interface. We have conducted two studies on how UISKEI compares to similar prototyping techniques: paper and pencil; widget-based (Balsamiq) and other sketch-based user interface prototyping tools (DENIM and SketchiXML). The studies revealed that UISKEI's sketch-based definition of the user interface behavior for later simulation is better than the analogous mechanisms provided by the other tools.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {18–25},
numpages = {8},
keywords = {user interface sketching, sketching user interface behavior, early prototyping and evaluation, pen-based interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254565,
author = {Dix, Alan},
title = {Asynchronous Active Values for Client-Side Interactive Service Coordination},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254565},
doi = {10.1145/2254556.2254565},
abstract = {This paper describes Asynchronous Active Values (AAV), a framework for the production of reactive web interfaces that use API-based web service back-ends. Such interfaces are now becoming common due to API-oriented application development and more sophisticated post-Web2.0 mashups. A significant feature of such interfaces is the need for feedback when parts of the page display are in some way temporarily invalid, or in flux, while potentially slow API calls are responding to requests. AAV extends existing methods such as access-oriented programming and the observer pattern, by including a 'changing' event in addition to the normal 'onChange' to enable intermediate feedback.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {26–33},
numpages = {8},
keywords = {user-interface architecture, web development, AJAX, asynchronous update},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254566,
author = {Trainer, Erik H. and Redmiles, David F.},
title = {Foundations for the Design of Visualizations That Support Trust in Distributed Teams},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254566},
doi = {10.1145/2254556.2254566},
abstract = {We seek to provide design principles for software tools intended to support the development of trust in distributed teams. As such, we present a "design space" for such tools that consists of three elements: trust factors, collaborative traces, and visual representations. Trust factors are aspects of work shown in the research literature to influence one's perceived trustworthiness of their team members. Collaborative traces are representations of past and current work done by team members manipulating project artifacts. Collaborative traces provide information about a trust factor. Visual representations consist of a set of visual abstractions of the collaborative traces arranged in a layout that provides users with the ability to formulate accurate perceptions of their team members' trustworthiness with respect to a particular trust factor. Because the eventual goal of this research is to produce visual interfaces that support the development of trust, we apply elements from the design space to the design of three example visualizations. We conclude by assessing the value of the model and outlining future work.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {34–41},
numpages = {8},
keywords = {visual interface design, trust, interactive and collaborative interfaces, distributed teams},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254567,
author = {Yokokubo, Anna and S\"{a}\"{a}skilahti, Kirsti and Kangaskorte, Riitta and Luimula, Mika and Siio, Itiro},
title = {CADo: A Supporting System for Flower Arrangement},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254567},
doi = {10.1145/2254556.2254567},
abstract = {Flower arrangement is enjoyed all over the world as a way of gardening and in decoration of rooms. However, it is difficult to make arrangements as beautiful as possible for beginners. Some rules about layout and color combination of flower arrangement are required in order for beginners to beautifully arrange the flowers. Moreover, the process is sometimes unreversible; that is, if a user cuts a stem too short, it may irrevocably spoil the arrangement. To solve these problems, we propose a computer-supported flower arrangement simulator, "CADo", which helps beginners enjoy flower arrangements using familiar materials. The name CADo is a combination of Kado (the Japanese art of flower arrangement) and CAD (computer aided design). CADo helps users with arrangement suggestions based on traditional layout and color combination of flower arrangement, and instructs users on how to cut flowers and to arrange them step by step. First, we performed a feasibility study to implement the proper rules of flower arrangement to our system, after which the system was built and developed. Finally, we verified the effectiveness of our system through evaluative experiment.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {42–45},
numpages = {4},
keywords = {simulation, mobile interaction design, interface evaluation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254568,
author = {Fogli, Daniela},
title = {Designing Visual Interactive Systems in the E-Government Domain},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254568},
doi = {10.1145/2254556.2254568},
abstract = {This paper proposes an approach to improve the work practice currently adopted by a local municipality to create e-government services. The approach is based on meta-design and end-user development. These paradigms allow empowering domain experts, namely civil servants with competencies in government procedures, to design and develop e-government applications to be used by citizens and administrative employees. This requires, on the one hand, to pay attention on the visual interaction aspects of the tools designed to support domain experts, and, on the other hand, to create suitable mechanisms for the automatic generation of user interfaces, application logic and databases. These goals are achieved by conceiving meta-design as a participatory design activity aimed at creating meta-models of e-government applications and suitable tools exploiting such meta-models for code generation.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {46–49},
numpages = {4},
keywords = {e-government, meta-design, end-user development},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254569,
author = {Fragoso, Suely and Rebs, Rebeca Recuero and Barth, Daiani Ludmila},
title = {Interface Affordances and Social Practices in Online Communication Systems},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254569},
doi = {10.1145/2254556.2254569},
abstract = {This article discusses the relation between interface affordances and social interaction practices in online communication systems (OCS) The underlying premise is that social, cultural and cognitive elements are as important as structural, functional and aesthetic features in the development of technology adoption patterns. This hypothesis was examined through a comparative study of the preferred modes of interaction of a sample of users of two OCS that follow the 'microblogging' model (Twitter and Plurk) and instant messengers (MSN, Gtalk and/or Skype). Users responded to an online questionnaire and an in-depth interview. Each user also provided us with images of their screens during typical interaction sessions using the above OCS on desktop and mobile devices. The results led to the proposal of a categorization of socio-technical affordances and confirmed the mutual influence between online conversational practices and interface affordances. Moreover, the adoption of a socially-situated perspective has proven essential for the analysis of user-system interaction, as well as interpersonal interaction, using both mobile and fixed internet access.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {50–57},
numpages = {8},
keywords = {interpersonal communication, social practices, affordances},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254570,
author = {Endert, Alex and Bradel, Lauren and Zeitz, Jessica and Andrews, Christopher and North, Chris},
title = {Designing Large High-Resolution Display Workspaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254570},
doi = {10.1145/2254556.2254570},
abstract = {Large, high-resolution displays have enormous potential to aid in scenarios beyond their current usage. Their current usages are primarily limited to presentations, visualization demonstrations, or conducting experiments. In this paper, we present a new usage for such systems: an everyday workspace. We discuss how seemingly small large-display design decisions can have significant impacts on users' perceptions of these workspaces, and thus the usage of the space. We describe the effects that various physical configurations have on the overall usability and perception of the display. We present conclusions on how to broaden the usage scenarios of large, high-resolution displays to enable frequent and effective usage as everyday workspaces while still allowing transformation to collaborative or presentation spaces.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {58–65},
numpages = {8},
keywords = {large high-resolution displays},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254571,
author = {Ingram, Amy and Wang, Xiaoyu and Ribarsky, William},
title = {Towards the Establishment of a Framework for Intuitive Multi-Touch Interaction Design},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254571},
doi = {10.1145/2254556.2254571},
abstract = {Intuition is an important yet ill-defined factor when designing effective multi-touch interactions. Throughout the research community, there is a lack of consensus regarding both the nature of intuition and, more importantly, how to systematically incorporate it into the design of multi-touch gestural interactions. To strengthen our understanding of intuition, we surveyed various domains to determine the level of consensus among researchers, commercial developers, and the general public regarding which multi-touch gestures are intuitive, and which of these gestures intuitively lead to which interaction outcomes. We reviewed more than one hundred papers regarding multi-touch interaction, approximately thirty of which contained key findings we report herein. Based on these findings, we have constructed a framework of five factors that determine the intuition of multi-touch interactions, including direct manipulation, physics, feedback, previous knowledge, and physical motion. We further provide both design recommendations for multi-touch developers and an evaluation of research problems which remain due to the limitations of present research regarding these factors. We expect our survey and discussion of intuition will raise awareness of its importance, and lead to the active pursuit of intuitive multi-touch interaction design.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {66–73},
numpages = {8},
keywords = {gesture, intuition, multi-touch, interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254572,
author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
title = {Termite: Visualization Techniques for Assessing Textual Topic Models},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254572},
doi = {10.1145/2254556.2254572},
abstract = {Topic models aid analysis of text corpora by identifying latent topics based on co-occurring words. Real-world deployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {74–77},
numpages = {4},
keywords = {seriation, topic models, text visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254573,
author = {Celentano, Augusto and Dubois, Emmanuel},
title = {Metaphor Modelling for Tangible Interfaces Evaluation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254573},
doi = {10.1145/2254556.2254573},
abstract = {In this paper we discuss metaphor modeling and interpretation in human computer interaction. We aim at evaluating metaphors in Tangible User Interfaces by matching entities and operations between the two domains of the metaphor: the physical interaction domain and the digital domain. A case study in tangible interaction is analyzed and a structured definition of interaction metaphor is derived and evaluated.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {78–81},
numpages = {4},
keywords = {interaction, model, domain mapping, metaphor},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254575,
author = {Jetter, Hans-Christian and Leifert, Svenja and Gerken, Jens and Schubert, S\"{o}ren and Reiterer, Harald},
title = {Does (Multi-)Touch Aid Users' Spatial Memory and Navigation in 'panning' and in 'Zooming &amp; Panning' UIs?},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254575},
doi = {10.1145/2254556.2254575},
abstract = {Recent findings from Embodied Cognition reveal strong effects of arm and hand movement on spatial memory. This suggests that input devices may have a far greater influence on users' cognition and users' ability to master a system than we typically believe -- especially for spatial panning or zooming &amp; panning user interfaces. We conducted two experiments to observe whether multi-touch instead of mouse input improves users' spatial memory and navigation performance for such UIs. We observed increased performances for panning UIs but not for zooming &amp; panning UIs. We present our results, provide initial explanations and discuss opportunities and pitfalls for interaction designers.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {83–90},
numpages = {8},
keywords = {user study, touch, spatial memory, zooming, navigation, multi-touch, panning, embodied cognition, ZUI},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254576,
author = {Ragan, Eric D. and Endert, Alex and Bowman, Doug A. and Quek, Francis},
title = {How Spatial Layout, Interactivity, and Persistent Visibility Affect Learning with Large Displays},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254576},
doi = {10.1145/2254556.2254576},
abstract = {Visualizations often use spatial representations to aid understanding, but it is unclear what properties of a spatial information presentation are most important to effectively support cognitive processing. This research explores how spatial layout and view control impact learning and investigates the role of persistent visibility when working with large displays. We performed a controlled experiment with a learning activity involving memory and comprehension of a visually represented story. We compared performance between a slideshow-type presentation on a single monitor and a spatially distributed presentation among multiple monitors. We also varied the method of view control (automatic vs. interactive). Additionally, to separate effects due to location or persistent visibility with a spatially distributed layout, we controlled whether all story images could always be seen or if only one image could be viewed at a time. With the distributed layouts, participants maintained better memory of the associated locations where information was presented. However, learning scores were significantly better for the slideshow presentation than for the distributed layout when only one image could be viewed at a time.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {91–98},
numpages = {8},
keywords = {use of space, memory, interactivity, learning, large displays},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254577,
author = {Rashid, Umar and Nacenta, Miguel A. and Quigley, Aaron},
title = {The Cost of Display Switching: A Comparison of Mobile, Large Display and Hybrid UI Configurations},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254577},
doi = {10.1145/2254556.2254577},
abstract = {Attaching a large external display can help a mobile device user view more content at once. This paper reports on a study investigating how different configurations of input and output across displays affect performance, subjective workload and preferences in map, text and photo search tasks. Experimental results show that a hybrid configuration where visual output is distributed across displays is worst or equivalent to worst in all tasks. A mobile device-controlled large display configuration performs best in the map search task and equal to best in text and photo search tasks (tied with a mobile-only configuration). After conducting a detailed analysis of the performance differences across different UI configurations, we give recommendations for the design of distributed user interfaces.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {99–106},
numpages = {8},
keywords = {map search, photo search, multi-display environments, mobile input, text search, distributed user interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254579,
author = {Kurihara, Kazutaka},
title = {CinemaGazer: A System for Watching Videos at Very High Speed},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254579},
doi = {10.1145/2254556.2254579},
abstract = {This paper presents a technology that enables the watching of videos at very high speed. Subtitles are widely used in DVD movies, and provide useful supplemental information for understanding video contents. We propose a "two-level fast-forwarding" scheme for videos with subtitles, which controls the speed of playback depending on the context: very fast during segments without language, such as subtitles or speech, and "understandably fast" during segments with such language. This makes it possible to watch videos at a higher speed than usual while preserving the entertainment values of the contents. We also propose "centering" and "fading" features for the display of subtitles to reduce fatigue when watching high-speed video. We implement a versatile video encoder that enables movie viewing with two-level fast-forwarding on any mobile device by specifying the speed of playback, the reading rate, or the overall viewing time. The effectiveness of our proposed method was demonstrated in an evaluation study.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {108–115},
numpages = {8},
keywords = {audience gaze localization, two-level fast-forwarding, video},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254580,
author = {Francese, Rita and Passero, Ignazio and Tortora, Genoveffa},
title = {Wiimote and Kinect: Gestural User Interfaces Add a Natural Third Dimension to HCI},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254580},
doi = {10.1145/2254556.2254580},
abstract = {The recent diffusion of advanced controllers, initially designed for the home game console, has been rapidly followed by the release of proprietary or third part PC drivers and SDKs suitable for implementing new forms of 3D user interfaces based on gestures. Exploiting the devices currently available on the game market, it is now possible to enrich, with low cost motion capture, the user interaction with desktop computers by building new forms of natural interfaces and new action metaphors that add the third dimension as well as a physical extension to interaction with users. This paper presents two systems specifically designed for 3D gestural user interaction on 3D geographical maps. The proposed applications rely on two consumer technologies both capable of motion tracking: the Nintendo Wii and the Microsoft Kinect devices. The work also evaluates, in terms of subjective usability and perceived sense of Presence and Immersion, the effects on users of the two different controllers and of the 3D navigation metaphors adopted. Results are really encouraging and reveal that, users feel deeply immerse in the 3D dynamic experience, the gestural interfaces quickly bring the interaction from novice to expert style and enrich the synthetic nature of the explored environment exploiting user physicality.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {116–123},
numpages = {8},
keywords = {empirical evaluation, natural user interfaces, motion capture, 3D interfaces, Kinect, human computer interaction, wiimote},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254581,
author = {de Almeida, Rodrigo A. and Pillias, Cl\'{e}ment and Pietriga, Emmanuel and Cubaud, Pierre},
title = {Looking behind Bezels: French Windows for Wall Displays},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254581},
doi = {10.1145/2254556.2254581},
abstract = {Using tiled monitors to build wall-sized displays has multiple advantages: higher pixel density, simpler setup and easier calibration. However, the resulting display walls suffer from the visual discontinuity caused by the bezels that frame each monitor. To avoid introducing distortion, the image has to be rendered as if some pixels were drawn behind the bezels. In turn, this raises the issue that a non-negligible part of the rendered image, that might contain important information, is visually occluded. We propose to draw upon the analogy to french windows that is often used to describe this approach, and make the display really behave as if the visualization were observed through a french window. We present and evaluate two interaction techniques that let users reveal content hidden behind bezels. ePan enables users to offset the entire image through explicit touch gestures. GridScape adopts a more implicit approach: it makes the grid formed by bezels act like a true french window using head tracking to simulate motion parallax, adapting to users' physical movements in front of the display. The two techniques work for both single- and multiple-user contexts.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {124–131},
numpages = {8},
keywords = {wall-sized displays, motion parallax, visualization, bezels},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254582,
author = {Gizatdinova, Yulia and \v{S}pakov, Oleg and Surakka, Veikko},
title = {Comparison of Video-Based Pointing and Selection Techniques for Hands-Free Text Entry},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254582},
doi = {10.1145/2254556.2254582},
abstract = {Video-based human-computer interaction has received increasing interest over the years. However, earlier research has been mainly focusing on technical characteristics of different methods rather than on user performance and experiences in using computer vision technology. This study aims to investigate performance characteristics of novice users and their subjective experiences in typing text with several video-based pointing and selection techniques. In Experiment 1, eye tracking and head tracking were applied for the task of pointing at the keys of a virtual keyboard. The results showed that gaze pointing was significantly faster but also more erroneous technique as compared with head pointing. Self-reported subjective ratings revealed that it was generally better, faster, more pleasant and efficient to type using gaze pointing than head pointing. In Experiment 2, mouth open and brows up facial gestures were utilized for confirming the selection of a given character. The results showed that text entry speed was approximately the same for both selection techniques, while mouth interaction caused significantly fewer errors than brow interaction. Subjective ratings did not reveal any significant differences between the techniques. Possibilities for design improvements are discussed.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {132–139},
numpages = {8},
keywords = {computer vision, text entry, face detection, eye tracking, virtual keyboard, visual gesture, video-based interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254583,
author = {Zarek, Adam and Wigdor, Daniel and Singh, Karan},
title = {SNOUT: One-Handed Use of Capacitive Touch Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254583},
doi = {10.1145/2254556.2254583},
abstract = {SNOUT is a novel interface overlay designed for occasional no-hand or one-handed use of handheld capacitive touch devices. Inspired by the desire to use these devices in scenarios where visually focused bimanual input is awkward, we performed a pair of studies intended to evaluate the potential of the nose to provide touch input. These studies influenced our design principles, resulting in the construction of a 'nose mode' which enables object selection, continuous parameter control, and speech-based text entry. Selection is accomplished via a nose tap, using a colour overlay and peripheral colour feedback to correct mistakes. The other two techniques are activated by a nose tap, but use the accelerometer to control parameters and speech-to-text for text entry. An evaluation of SNOUT shows it to effectively render handheld capacitive touch devices operational in scenarios where they are presently unusable.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {140–147},
numpages = {8},
keywords = {mobile devices, UI overlay, smartphone, touch screen, nose input, accessibility, tablet},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254584,
author = {Cuccurullo, Stefania and Francese, Rita and Murad, Sharefa and Passero, Ignazio and Tucci, Maurizio},
title = {A Gestural Approach to Presentation Exploiting Motion Capture Metaphors},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254584},
doi = {10.1145/2254556.2254584},
abstract = {Speaking in public may be a challenging task in terms of self-control and attention to the concepts to expose and to non-verbal communication. Presentation software, like Microsoft PowerPoint™ or OpenOffice, may support the speaker in organizing and controlling the flow of his/her discussion by commanding the slide change. In this paper we describe an approach exploiting the availability of the Microsoft Kinect™ advanced game controller to manage a presentation software through a Natural User Interface (NUI). The approach, named Kinect Presenter (KiP), adopts motion capture to recognize body gestures representing interaction metaphors. We perform a preliminary evaluation aiming at assessing the degree of support provided by the proposed interaction approach to the speaker activities. The assessment is based on the combined usage of two techniques: a questionnaire-based survey and an empirical analysis. The context of this study was constituted of Bachelor and PhD students in Computer Science at the University of Salerno, and teachers and employees from the same university. First results were adequate both in terms of satisfaction and performances, also when compared with a wireless mouse-based interaction approach.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {148–155},
numpages = {8},
keywords = {gesture-recognition, gesture-based presentation, Kinect},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254585,
author = {Hoste, Lode and Dumas, Bruno and Signer, Beat},
title = {SpeeG: A Multimodal Speech- and Gesture-Based Text Input Solution},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254585},
doi = {10.1145/2254556.2254585},
abstract = {We present SpeeG, a multimodal speech- and body gesture-based text input system targeting media centres, set-top boxes and game consoles. Our controller-free zoomable user interface combines speech input with a gesture-based real-time correction of the recognised voice input. While the open source CMU Sphinx voice recogniser transforms speech input into written text, Microsoft's Kinect sensor is used for the hand gesture tracking. A modified version of the zoomable Dasher interface combines the input from Sphinx and the Kinect sensor. In contrast to existing speech error correction solutions with a clear distinction between a detection and correction phase, our innovative SpeeG text input system enables continuous real-time error correction. An evaluation of the SpeeG prototype has revealed that low error rates for a text input speed of about six words per minute can be achieved after a minimal learning phase. Moreover, in a user study SpeeG has been perceived as the fastest of all evaluated user interfaces and therefore represents a promising candidate for future controller-free text input.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {156–163},
numpages = {8},
keywords = {SpeeG, speech recognition, gesture input, Kinect sensor, multimodal text input},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254586,
author = {Tokui, Taro and Yamasaki, Masami and Koike, Hideki},
title = {Image Correction Techniques for 3D Interactive Surface Using a Transparent Elastic Gels},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254586},
doi = {10.1145/2254556.2254586},
abstract = {There are many kinds of three dimensional displays that have been developed to date. Most of them provide 3D visual sensation to the users, but they do not provide 3D haptic feedback. On the other hand, an interactive surface system using transparent gels enables users to touch a 3D surface. The main issue of the system, however, is that the image is distorted due to "lens effect" of the gels. This paper describes a method to solve the image distortion through the use of a light field display (LFD) which is discussed in detail in section 2. By combining the LFD and transparent gel interface, it becomes possible to show correct 3D images on gels from any viewing position.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {164–167},
numpages = {4},
keywords = {interactive surface, Input devices and strategies, light field display},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254587,
author = {Rekimoto, Jun},
title = {Squama: Modular Visibility Control of Walls and Windows for Programmable Physical Architectures},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254587},
doi = {10.1145/2254556.2254587},
abstract = {In this paper we present Squama, a programmable physical window or wall that can independently control the visibility of its elemental small square tiles. This is an example of programmable physical architecture, our vision for future architectures where the physical features of architectural elements and facades can be dynamically changed and reprogrammed according to people's needs. When Squama is used as a wall, it dynamically controls the transparency through its surface, and simultaneously satisfies the needs for openness and privacy. It can also control the amount of sunlight and create shadows, called programmable shadows, in order to afford indoor comfort without completely blocking the outer view. In this paper, we discuss how in future, architectural space can become dynamically changeable and introduce the Squama system as an initial instance for exemplifying this concept.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {168–171},
numpages = {4},
keywords = {architectural space, interaction design, programmable architecture, programmable matter},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254589,
author = {Negulescu, Matei and Ruiz, Jaime and Li, Yang and Lank, Edward},
title = {Tap, Swipe, or Move: Attentional Demands for Distracted Smartphone Input},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254589},
doi = {10.1145/2254556.2254589},
abstract = {Smartphones are frequently used in environments where the user is distracted by another task, for example by walking or by driving. While the typical interface for smartphones involves hardware and software buttons and surface gestures, researchers have recently posited that, for distracted environments, benefits may exist in using motion gestures to execute commands. In this paper, we examine the relative cognitive demands of motion gestures and surface taps and gestures in two specific distracted scenarios: a walking scenario, and an eyes-free seated scenario. We show, first, that there is no significant difference in reaction time for motion gestures, taps, or surface gestures on smartphones. We further show that motion gestures result in significantly less time looking at the smartphone during walking than does tapping on the screen, even with interfaces optimized for eyes-free input. Taken together, these results show that, despite somewhat lower throughput, there may be benefits to making use of motion gestures as a modality for distracted input on smartphones.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {173–180},
numpages = {8},
keywords = {eyes-free interaction, smartphones, motion gestures},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254590,
author = {Kratz, Sven and Rohs, Michael and Guse, Dennis and M\"{u}ller, J\"{o}rg and Bailly, Gilles and Nischt, Michael},
title = {PalmSpace: Continuous around-Device Gestures vs. Multitouch for 3D Rotation Tasks on Mobile Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254590},
doi = {10.1145/2254556.2254590},
abstract = {Rotating 3D objects is a difficult task on mobile devices, because the task requires 3 degrees of freedom and (multi-)touch input only allows for an indirect mapping. We propose a novel style of mobile interaction based on mid-air gestures in proximity of the device to increase the number of DOFs and alleviate the limitations of touch interaction with mobile devices. While one hand holds the device, the other hand performs mid-air gestures in proximity of the device to control 3D objects on the mobile device's screen. A flat hand pose defines a virtual surface which we refer to as the PalmSpace for precise and intuitive 3D rotations. We constructed several hardware prototypes to test our interface and to simulate possible future mobile devices equipped with depth cameras. We conducted a user study to compare 3D rotation tasks using the most promising two designs for the hand location during interaction -- behind and beside the device -- with the virtual trackball, which is the current state-of-art technique for orientation manipulation on touch-screens. Our results show that both variants of PalmSpace have significantly lower task completion times in comparison to the virtual trackball.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {181–188},
numpages = {8},
keywords = {3D rotation, depth camera, input devices, mobile interaction, 3D user interfaces, around-device interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254591,
author = {Ginige, Athula and Romano, Marco and Sebillo, Monica and Vitiello, Giuliana and Di Giovanni, Pasquale},
title = {Spatial Data and Mobile Applications: General Solutions for Interface Design},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254591},
doi = {10.1145/2254556.2254591},
abstract = {Nowadays, spatial data are totally widespread in mobile applications. They are present in games, map applications, web community applications and office automations. However this kind of spatial information potentially needs a large display area and the hardware constraint related to the limited screen dimensions creates many usability challenges. Our investigation in the last few years to find solutions to these challenges has led us to the discovery of general usability principles that a well-designed interface should adopt. In this paper we describe the mental path we have followed to derive those principles from the experience gained in developing mobile interfaces for different application domains. The principles are formalized in terms of two interaction design patterns, specific for mobile interfaces managing spatial data. They extend existing HCI patterns and are completed, as usual, with concrete examples of their applications.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {189–196},
numpages = {8},
keywords = {multimodal interfaces, context aware interaction, information visualization, mobile interaction design, interface metaphors},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254592,
author = {Cutugno, Francesco and Leano, Vincenza Anna and Rinaldi, Roberto and Mignini, Gianluca},
title = {Multimodal Framework for Mobile Interaction},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254592},
doi = {10.1145/2254556.2254592},
abstract = {In recent years multimodal interaction is becoming of great interest thanks to the increasing availability of mobile devices. In this view, many applications making use of speech, gestures on the touch screen and other interaction modalities are presently becoming to appear on the different app-markets. Multimodality requires procedures to integrate different events to be interpreted as a single intention of the user. There is no agreement on how this integration must be realized as well as a shared approach, able to abstract a set of basic functions to be used in any possible multimodal application, is still missing. Designing and implementing multimodal systems is still a difficult task. In response to this situation, the goal of our research is to explore how a simple framework can be used to support the design of multimodal user interfaces.In this paper we propose a framework that aims to help the design of simple multimodal commands in the mobile environment (more specifically in Android applications). The proposed system is based on the standard licensed by the W3C consortium for the Multimodal Interaction [8] [9] and on the definition of a set of CARE [2] properties; moreover the system makes use of some features available in the SMUIML language [3]. We will finally present a case study implementing a mobile GIS application based on the proposed framework.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {197–203},
numpages = {7},
keywords = {mobile interaction design, map-based interaction, multimodal interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254593,
author = {Partala, Timo and Salminen, Miikka},
title = {User Experience of Photorealistic Urban Pedestrian Navigation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254593},
doi = {10.1145/2254556.2254593},
abstract = {With advances in satellite and street-level imaging, photorealistic mobile maps have gained widespread popularity. The aim of this research was to study the user experience of mobile navigation with three different mobile maps: a traditional graphical map representation was compared to a photorealistic satellite map and a photorealistic street-level view. Nine subjects used all three visualizations in urban pedestrian navigation and gave evaluations of navigation support, user experience (AttrakDiff), task load (NASA TLX), and overall preference using questionnaire methods. The results indicated that the photorealistic maps were more stimulating to the user than the graphical map and the photorealistic street-level view also enabled more effective identification of nearby landmarks than the other map versions. However, the photorealistic maps were perceived as less pragmatic than the graphical map and the street-level view also demanded a higher task load. The graphical map was the most often preferred visualization.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {204–207},
numpages = {4},
keywords = {street view, photorealism, mobile navigation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254594,
author = {Kato, Haruhisa and Yoneyama, Akio},
title = {A Line-Based Palm-Top Detector for Mobile Augmented Reality},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254594},
doi = {10.1145/2254556.2254594},
abstract = {We propose a marker-less Augmented Reality (AR) application based on a realtime hand posture estimation technique for smartphones. A conventional marker-less AR system does not have sufficient accuracy and speed in the detection of a mobile device. This paper presents a fast hand posture estimation algorithm based on a combination of feature points and feature lines that consist of the boundary of fingers. The proposed method realized rendering of virtual 3D models on a hand over 12 frames per second (fps) on a smart-phone. Simulation results show that we can archive about 73% complexity reductions and be more accurate than the conventional method.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {208–211},
numpages = {4},
keywords = {hand posture estimation, marker-less augmented reality, ho-mography matrix},
location = {Capri Island, Italy},
series = {AVI '12}
}

@dataset{10.1145/review-2254556.2254594_R48106,
author = {Palomares, Jose Manuel M.},
title = {Review ID:R48106 for DOI: 10.1145/2254556.2254594},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-2254556.2254594_R48106}
}

@inproceedings{10.1145/2254556.2254595,
author = {Mulloni, Alessandro and Seichter, Hartmut and Schmalstieg, Dieter},
title = {Indoor Navigation with Mixed Reality World-in-Miniature Views and Sparse Localization on Mobile Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254595},
doi = {10.1145/2254556.2254595},
abstract = {We present the design of an interface that provides continuous navigational support for indoor scenarios where localization is only available at sparse, discrete locations (info points). Our interface combines turn-by-turn instructions with a World-in-Miniature (WIM). In a previous study, we showed that using an Augmented Reality WIM at info points, and turn-by-turn instructions elsewhere, is a valid support for navigation inside an unknown building. In particular, we highlighted that users value the WIM as a tool for monitoring their location in the building. In this work, we focus on using the WIM continuously, not only at info points, to support navigation. We adapt the WIM views to the quality of localization by transitioning within Mixed Reality: we use Augmented Reality to provide an overview of the whole path at info points and Virtual Reality to communicate the next instruction when localization is not available. Our results from a new user study validate our interface design and show that users exploit not only turn-by-turn instructions but also the WIM throughout the path, to navigate with our interface. This paper provides insight on how a low-infrastructure indoor solution can support human navigational abilities effectively.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {212–215},
numpages = {4},
keywords = {indoor navigation, augmented reality, mixed reality},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254597,
author = {Javed, Waqas and Ghani, Sohaib and Elmqvist, Niklas},
title = {GravNav: Using a Gravity Model for Multi-Scale Navigation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254597},
doi = {10.1145/2254556.2254597},
abstract = {We present gravity navigation (GravNav), a family of multi-scale navigation techniques that use a gravity-inspired model for assisting navigation in large visual 2D spaces based on the interest and salience of visual objects in the space. GravNav is an instance of topology-aware navigation, which makes use of the structure of the visual space to aid navigation. We have performed a controlled study comparing GravNav to standard zoom and pan navigation, with and without variable-rate zoom control. Our results show a significant improvement for GravNav over standard navigation, particularly when coupled with variable-rate zoom. We also report findings on user behavior in multi-scale navigation.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {217–224},
numpages = {8},
keywords = {zooming, topology-aware navigation, guided navigation, panning, space-scale diagrams, multi-scale spaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254598,
author = {Hurter, Christophe and Lesbordes, R\'{e}mi and Letondal, Catherine and Vinot, Jean-Luc and Conversy, St\'{e}phane},
title = {Strip'TIC: Exploring Augmented Paper Strips for Air Traffic Controllers},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254598},
doi = {10.1145/2254556.2254598},
abstract = {The current environment used by French air traffic controllers mixes digital visualization such as radar screens and tangible artifacts such as paper strips. Tangible artifacts do not allow controllers to update the system with the instructions they give to pilots. Previous attempts at replacing them in France failed to prove efficient. This paper is an engineering paper that describes Strip'TIC, a novel system for ATC that mixes augmented paper and digital pen, vision-based tracking and augmented rear and front projection. The system is now working and has enabled us to run workshops with actual controllers to study the role of writing and tangibility in ATC. We describe the system and solutions to technical challenges due to mixing competing technologies.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {225–232},
numpages = {8},
keywords = {air traffic control, tangible interfaces, paper computing, interactive paper, digital pen, visualization, augmented paper},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254599,
author = {D\"{o}rk, Marian and Carpendale, Sheelagh and Williamson, Carey},
title = {Fluid Views: A Zoomable Search Environment},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254599},
doi = {10.1145/2254556.2254599},
abstract = {We present Fluid Views, a web-based search environment designed to bridge overview and detail by integrating dynamic queries, semantic zooming, and dual layers. The most common form of search results is long ranked and paginated lists, which are seldom examined beyond the top ten items. To support more exploratory forms of information seeking, we bring together the notion of relevance with the power of visual encoding. In Fluid Views, results portray relevance via size and detail in a dynamic top layer and semantic similarity via position on a base map. We designed Fluid Views with temporal, spatial, and content-defined base maps for both textual and visual resources, and tested our prototype system on books, blogs, and photos. Interviews with library professionals indicate the potential of Fluid Views for exploring collections and exciting directions for future research.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {233–240},
numpages = {8},
keywords = {exploratory search, information visualization, zoomable user interface, web search, information seeking, level of detail},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254600,
author = {Klum, Stefanie and Isenberg, Petra and Langner, Ricardo and Fekete, Jean-Daniel and Dachselt, Raimund},
title = {Stackables: Combining Tangibles for Faceted Browsing},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254600},
doi = {10.1145/2254556.2254600},
abstract = {We introduce Stackables: tangibles designed to support faceted information seeking in a variety of contexts. We are faced, more than ever, with tasks that require us to find, access, and act on information by ourselves or together with others. Current interfaces for browsing and search in large data spaces, however, largely focus on the support of either individual or collaborative activities. Stackables were designed to bridge this gap and be useful in meetings, for sharing results from individual search activities, and for realistic datasets including multiple facets with large value ranges. Each Stackable tangible represents search parameters that can be shared amongst collaborators, modified during an information seeking process, and stored and transferred. We describe Stackables, their flexible and expressive combination to formulate queries, and the underlying interaction concept in detail. An evaluation provides initial evidence of their usability in targeted and exploratory information seeking tasks.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {241–248},
numpages = {8},
keywords = {faceted browsing, faceted search, tangible UIs, collaboration},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254601,
author = {Schwarz, Tobias and Butscher, Simon and Mueller, Jens and Reiterer, Harald},
title = {Content-Aware Navigation for Large Displays in Context of Traffic Control Rooms},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254601},
doi = {10.1145/2254556.2254601},
abstract = {Context-sensitive information is of vital importance to the monitoring of processes in control rooms, but its incorporation in current state-of-the-art user interfaces is inadequate. By looking at a traffic control center as an example, this paper proposes an interaction concept for monitoring complex networks on large remote displays. Our approach is based on content-aware navigation with the goal to improve the navigation of the road network and the availability of context-sensitive information. We conducted a study comparing different navigation techniques as well as two techniques for visualizing context-sensitive information.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {249–252},
numpages = {4},
keywords = {control room, map-based interaction, content-aware navigation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254603,
author = {Abate, Andrea F. and Narducci, Fabio and Ricciardi, Stefano},
title = {An Augmented Interface to Audio-Video Components},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254603},
doi = {10.1145/2254556.2254603},
abstract = {In the last years, the growing diffusion of lightweight portable computing device like netbooks, tablets, and smartphones, featuring adequate processing power coupled with trackpad/touchpad interface, one or two webcams and eventually additional sensors (accelerometers, gps, gyroscopes, digital compass, etc.) has provided a low-cost platform to augmented reality applications, usually relying on more dedicated but also expensive and bulky technologies like motion tracking systems and see-through head mounted displays. In this paper we present and describe an AR application aimed to showcase how it is possible to effectively augment AV (Audio-Video) components, a kind of hi-tech gear today diffused in most home environments, by means of context dependent graphics contents. Visual aids in the form of both static and animated graphics are displayed according to the current status of the component (outputted via a serial interface) or simply based on the selection operated by the user through the trackpad. Moreover the system is able to help user to focus his/her attention on the physical interface on the AV component (e.g. a knob, a button or a connector in the back panel) either via an augmenting strategy (e.g. by adding virtual info) or by means of a diminishing approach (hiding all the other not relevant features). The proposal is easily extendible to a broad category of low-cost AR applications as it is based on cheap hardware (netbooks/tablets) and exploits marker based motion tracking through external webcam and the lcd screen as a see-through display.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {254–257},
numpages = {4},
keywords = {motion tracking, augmented reality, AV components},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254604,
author = {Borgia, Fabrizio and De Marsico, Maria and Panizzi, Emanuele and Pietrangeli, Lorenzo},
title = {<i>ARMob</i> - Augmented Reality for Urban Mobility in <i>RMob</i>},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254604},
doi = {10.1145/2254556.2254604},
abstract = {This paper describes the design and development of a location-based Augmented Reality (AR) application for mobile devices. The application provides real time data about transportation in a urban area. It can be set along the line of the continuous creation of richer and more complex interaction modalities between users and data. A relevant element in this strategy is the visual enrichment of the real scene perceived though the mobile camera, by superimposing to it a set of user-relevant information. In the presented work, this information is related to nearby bus stops and to the arrival of next buses. More details, such as routes, distances etc. can be displayed on demand in order to gain awareness of the surrounding infomobility data. The presented application is included in a multiservice framework named RMob, developed for the city of Rome.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {258–261},
numpages = {4},
keywords = {IOS, augmented reality, mobile interaction, urban mobility, Android},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254605,
author = {Tobita, Hiroaki and Kuzi, Takuya},
title = {Face-to-Avatar: Augmented Face-to-Face Communication with Aerotop Telepresence System},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254605},
doi = {10.1145/2254556.2254605},
abstract = {We have developed a face-to-avatar system that integrates a blimp with a virtual avatar for a unique telepresence system. Our aerotop telepresence system has two advantages comparing with conventional telepresence systems. One is to provide unique communication between user and physical blimp avatar. The blimp works as an avatar and contains several pieces of equipment, including a projector and a speaker. The user's presence is dramatically enhanced compared to using conventional virtual avatars (e.g., CG and images) because the avatar is a physical object that can move freely in the real world. The other is that the user's senses are augmented because the blimp detects dynamic information in the real world. For example, the camera provides the user with a special floating view, and the microphone catches a wide variety of sounds such as conversations and environmental noises. This paper describes our face-to-avatar concept and its implementation.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {262–265},
numpages = {4},
keywords = {communication, interactive system, animation, comic, creation, visualization technique, browsing},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254606,
author = {Cho, Isaac and Dou, Wenwen and Wartell, Zachary and Ribarsky, William and Wang, Xiaoyu},
title = {Evaluating Depth Perception of Volumetric Data in Semi-Immersive VR},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254606},
doi = {10.1145/2254556.2254606},
abstract = {Displays supporting stereoscopy and head-coupled motion parallax can enhance human perception of complex 3D datasets. This has been studied extensively for datasets containing 3D surfaces and 3D networks but less for so volumetric data. Volumetric data is characterized by a heavy presence of transparency, occlusion and highly ambiguous spatial structure. There are many different rendering and visualization algorithms and interactive techniques that enhance perception of volume data and these techniques' effectiveness have been evaluated. However, the effect of VR displays on perception of volume data is less well studied. Therefore, we conduct two experiments on how various display conditions affect a participant's depth perception accuracy of a volumetric dataset. A demographic pre-questionnaire also allows us to separate the accuracy differences between participants with more and less experience with 3D games and VR. Our results show an overall benefit for stereo with head-tracking for enhancing perception of depth in volumetric data. Our study also suggests that familiarity with 3D games and VR type technology affects the users'ability to perceive such data and affects the accuracy boost due to VR displays.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {266–269},
numpages = {4},
keywords = {head-tracking, volumetric data, stereoscopic, depth perception},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254607,
author = {Brush, A. J. Bernheim and Johns, Paul},
title = {SpeechToast: Augmenting Notifications with Speech Input Focus},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254607},
doi = {10.1145/2254556.2254607},
abstract = {To explore the value of speech input focus for handling notifications, we built and deployed SpeechToast, an Outlook Add-in that replaces standard email notifications with a version that includes speech input commands (e.g. "open", "delete"). Notifications shown by SpeechToast have speech input focus when the audio context surrounding the computer is favorable for speech recognition. We deployed SpeechToast to 18 current users of email notifications for 4 weeks. Overall, speech input focus appealed to some participants, while non-users indicated their willingness to have it enabled as long as it did not detract from their experience. Our research suggests that selectively enabling speech input focus could provide natural and intuitive interactions that complement other input modalities.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {270–273},
numpages = {4},
keywords = {speech, field study, speech input focus, notification},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254608,
author = {Corato, Francesco and Frucci, Maria and Di Baja, Gabriella Sanniti},
title = {Virtual Training of Surgery Staff for Hand Washing Procedure},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254608},
doi = {10.1145/2254556.2254608},
abstract = {This work concerns the development of a virtual training application in the medical field; it is based on augmented reality and image analysis techniques. Specifically, we tackled the problem of instructing the surgery staff to follow correctly the hand washing procedure necessary before accessing the operating theater, and of monitoring the correctness of the procedure performed by the physicians. This procedure is recommended by the World Health Organization (WHO) and is adopted by all health institutions of civilized countries. Lack of strict fulfillment of this recommendation is one of the major causes of infection risk in the operating rooms. By segmenting the scene captured by a webcam and by using image analysis tools, we detect the position of the forearms of the surgeon with respect to the basin and the water flow, and identify the beginning and the end of each phase of the hand washing procedure performed by the surgeon. The correct execution of the various phases, performed by a puppet in 3D, is projected so as to overlap the real scene captured by the webcam, while the correct procedure is also displayed as text. The application is developed under the supervision of the Risk-Management Department of the European Institute of Oncology (IEO) in Milan, Italy, and is designed to be used within the Surgical Care Area, directly in the washing area.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {274–277},
numpages = {4},
keywords = {image analysis, virtual training, augmented reality},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254610,
author = {Korozi, Maria and Ntoa, Stavroula and Antona, Margherita and Leonidis, Asterios and Stephanidis, Constantine},
title = {Towards Building Pervasive UIs for the Intelligent Classroom: The PUPIL Approach},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254610},
doi = {10.1145/2254556.2254610},
abstract = {Information and Communication technologies have the potential to permeate the classroom and modernize the educational process. However, in the context of a smart classroom, building educational applications poses unique challenges from an HCI perspective, due to the diversity of user and context requirements. This paper introduces a framework that facilitates the design, development and deployment of pervasive educational applications that can automatically transform according to the context of use to ensure their usability. The collection of widgets incorporates both common basic widgets (e.g., buttons, images) and mini interfaces frequently used in educational applications, as ready-to-use modules. The designer can either (i) combine and customize widgets from both categories to build an interface just once, or (ii) build and incorporate it as a custom-made mini interface in the collection for future reuse. Finally, the framework's usability has been evaluated with users obtaining very positive results and potential suggestions for extensions.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {279–286},
numpages = {8},
keywords = {pervasive interfaces, widget library, education, smart classroom},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254611,
author = {Fischer, Gerhard},
title = {Context-Aware Systems: The 'right' Information, at the 'Right' Time, in the 'Right' Place, in the 'Right' Way, to the 'Right' Person},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254611},
doi = {10.1145/2254556.2254611},
abstract = {Based on the assumption that the scarce resource for many people in the world today is not information but human attention, the challenge for future human-centered computer systems is not to deliver more information "to anyone, at anytime, and from anywhere," but to provide "the 'right' information, at the 'right' time, in the 'right' place, in the 'right' way, to the 'right' person."This article develops a multidimensional framework for context-aware systems to address this challenge, transcending existing frameworks that limited their concerns to particular aspects of context-awareness and paid little attention to potential pitfalls. The framework is based on insights derived from the development and assessment of a variety of different systems that we have developed over the last twenty years to explore different dimensions of context awareness.Specific challenges, guidelines, and design trade-offs (promises and pitfalls) are derived from the framework for designing the next generation of context-aware systems. These systems will support advanced interactions for assisting humans (individuals and groups) to become more knowledgeable, more productive, and more creative by emphasizing context awareness as a fundamental design requirement.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {287–294},
numpages = {8},
keywords = {context awareness, user models, promises, design intent, serendipity, information access and delivery, filter bubbles, socio-technical systems, pitfalls, task models, adaptive and adaptable systems},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254612,
author = {Paprocki, Martin and Krog, Kim and Kristoffersen, Morten Bak and Kraus, Martin},
title = {Software-Based Adjustment of Mobile Autostereoscopic Graphics Using Static Parallax Barriers},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254612},
doi = {10.1145/2254556.2254612},
abstract = {We show that the autostereoscopic display of stereoscopic images using a static parallax barrier can be improved by adapting the rendering to the angle under which the user is looking at a mobile display; thus, ghosting artifacts and depth reversals can often be avoided even if the user tilts the mobile device. Instead of moving the barrier itself to compensate for a misplacement of the viewing zones in relation to the user, we employ dynamic pixel column shifts to provide a similar compensation in software. This requires a parallax barrier where each section covers two pixel columns at a time instead of one. The proposed method has been implemented using OpenGL shaders and a parallax barrier that was designed for a display of exactly half the resolution of the employed display. Technical tests showed a good separation of the left and right images for viewing angles of up to ± 30°. Preliminary user tests indicate that an improvement in the stereoscopic experience can be achieved.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {295–298},
numpages = {4},
keywords = {ghosting, autostereoscopy, display, parallax barriers, mobile device, context aware interface, adaptive interface, GPU},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254613,
author = {Tobita, Hiroaki and Kuzi, Takuya},
title = {SmartWig: Wig-Based Wearable Computing Device for Communication and Entertainment},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254613},
doi = {10.1145/2254556.2254613},
abstract = {We have developed a unique wearable wig-based computing device. Our device looks natural and enhances users' activities and appearance. Previous wearable computing research has mainly focused on how to provide effective computational functions for users. However, the researchers have not taken appearance into account, so users who wear them usually looks strange when in public. Thus, these devices have not been used practically. To make wearable computing look more practical, we believe appearance is one of the most important elements, so we enhanced both function and appearance to create a practical wearable computing device. Wigs have several advantages. For instance, they have enough space to put wearable sensors inside them and head area is very sensitive even to weak feedback. Also, wig technology has developed to the point where wigs can look very similar to real hair. In this paper, we describe our computational wig, focusing on the implementation and applications.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {299–302},
numpages = {4},
keywords = {wearable computing, sensing, interactive system, stylish appearance, presentation, wig device, navigation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254614,
author = {Hsiao, I-Han and Guerra, Julio and Parra, Denis and Bakalov, Fedor and K\"{o}nig-ries, Birgitta and Brusilovsky, Peter},
title = {Comparative Social Visualization for Personalized E-Learning},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254614},
doi = {10.1145/2254556.2254614},
abstract = {Social learning has confirmed its value in enhancing the learning outcomes across a wide spectrum. To support social learning, a visual approach is a common technique to represent and organize multiple students' data in an informative way. This paper presents a design of comparative social visualization for E-learning, which encourages information discovery and social comparisons. Classroom studies confirmed the motivational impact of personalized social guidance provided by the visualization in the target context. The visualization encouraged students to do some work ahead of the course schedule. Moreover, class leaders provided an implicit social guidance for the rest of the class and successfully led the way to discover the most relevant resources creating good trails for the rest of the class. We summarized the evidence of students' engagement and performance through the social visualization interface.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {303–307},
numpages = {5},
keywords = {social visualization, social comparison, social learning, personalized E-learning, social guidance},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254615,
author = {Fujinami, Kaori and Sokan, Akifumi},
title = {Nondirective Information Presentation for On-Site Safety Training in Chemistry Experiments},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254615},
doi = {10.1145/2254556.2254615},
abstract = {A chemistry experiment in a school should be conducted safely, yet provide an effective education. In this paper, we explore the impacts of a nondirective presentation on on-site safety training, particularly for experimental operations. The nondirective presentation is intended for a student's future independent operation by facilitating active thinking, which is realized by two aspects of presentation ambiguity: multiple interpretations of the content of a message and the message positioning on a table. Simulation-based experiments suggest that a message presented at a dedicated (static) area with a less semantically ambiguous message (Low semantic - High spatial ambiguities, LH) is the most appropriate combination of ambiguity levels compared to the other three combinations, LL, HL, and HH. Additionally, we examine the necessity of applying different levels relevant to the danger.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {308–311},
numpages = {4},
keywords = {information presentation, on-site safety training, ambiguity, augmented reality},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254616,
author = {Varni, Giovanna and Mancini, Maurizio and Volpe, Gualtiero},
title = {Embodied Cooperation Using Mobile Devices: Presenting and Evaluating the Sync4All Application},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254616},
doi = {10.1145/2254556.2254616},
abstract = {Embodied cooperation "arises when two co-present, individuals in motion coordinate their goal-directed actions". The adoption of the embodied cooperation paradigm for the development of embodied and social multimedia systems opens new perspectives for future User Centric Media. Systems for embodied music listening, which enable users to influence music in real-time by movement and gesture, can greatly benefit from the embodied cooperation paradigm. This paper presents the design and the evaluation of an application, Sync4All, based on such a paradigm, allowing users to experience social embodied music listening. Each user rhythmically and freely moves a mobile phone trying to synchronise her movements with those of the other ones. The level of such a synchronisation influences the music experience. The evaluation of Sync4All was aimed at finding out which is the overall attitude of the users towards the application, and how the participants perceived embodied cooperation and music embodiment.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {312–319},
numpages = {8},
keywords = {mobile music applications, nonverbal social behaviour, embodied cooperation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254617,
author = {Chang, Ching-Tzun and Takahashi, Shin and Tanaka, Jiro},
title = {WithYou - a Communication System to Provide out Together Feeling},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254617},
doi = {10.1145/2254556.2254617},
abstract = {In this paper, we present a video-based communication system that provides an Out Together Feeling. In other words, it makes a pair of users, one outdoors and the other indoors, feel as if they are going outside together. To achieve this, it is important that both users (1) can freely peruse the outdoor user's surroundings, (2) can see what the outdoor user is looking at, and (3) can focus together on the same point. To realize these features, we have designed and implemented a system called WithYou. It consists of two subsystems: a wearable system for the outdoor user and an immersive space for the indoor user. The indoor user wears an HMD and watches video from a Pan/Tilt/Zoom camera mounted on the outdoor user's chest. Thus, the indoor user can look around by simply turning his/her head to the left or right. The orientation of the outdoor user's face is also displayed on the HMD screen to indicate where he/she is looking. In the preliminary test, both users experienced the Out Together Feeling to some extent.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {320–323},
numpages = {4},
keywords = {wearable mobile and human robot interaction, communication support, tele-presence},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254618,
author = {Oliveira, Allan and Araujo, Regina B.},
title = {Creation and Visualization of Context Aware Augmented Reality Interfaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254618},
doi = {10.1145/2254556.2254618},
abstract = {Augmented Reality User Interface (AR UI) is a growing research area, which faces many design and implementation challenges. The design of a proper application ARUI is a process that demands time and expertise in the AR field. These interfaces are projected on the real world, complementing it, while providing useful information for the user's goal. The implementation of an AR UI is a time consuming task and current solutions, using interface components, lack the variety (and customization) of interface elements to create different types of applications. This paper presents a pattern-driven AR interface visualization framework (VISAR), which allows the development of context-aware AR interfaces using interface components (patterns). These patterns accelerate implementation and design time by offering an interface element for every user task. They also enable the design of adaptive interfaces, and provide share and reuse of interfaces' components. The paper also presents an AR Interface Editor (VISAR IE) that supports interface design by defining adaptation rules and offering interface patterns choices.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {324–327},
numpages = {4},
keywords = {adaptive user interface, augmented reality, context aware AR interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254619,
author = {Onorati, Teresa and Malizia, Alessio and Olsen, Kai A. and Diaz, Paloma and Aedo, Ignacio},
title = {I Feel Lucky: An Automated Personal Assistant for Smartphones},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254619},
doi = {10.1145/2254556.2254619},
abstract = {Third generation devices, such as smartphones, have the possibility of knowing the position of the user. By combining time, position and data in the Cloud it is possible to make smart deductions as to what information the user needs. These deductions can be performed by an automated assistant that have access to the user's e-mail and SMS messages, calendar, phone book, notes, etc., as well as to the position of the user. The assistant can present the information in an "I feel lucky" display on the user's smartphone. These concepts have been evaluated in a case study with a group of students, simulating the output of such an interface.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {328–331},
numpages = {4},
keywords = {input free user interface, cloud data, personal assistant},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254620,
author = {N\'{o}brega, Rui and Correia, Nuno},
title = {Magnetic Augmented Reality: Virtual Objects in Your Space},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254620},
doi = {10.1145/2254556.2254620},
abstract = {This paper proposes a framework to support interaction with virtual objects integrated in a real life scenario. In the proposed interaction concept, the user can reshape or re-design a real space using virtual objects using several pictures of the desired space. The images are analyzed for known features such as surfaces, floor or room orientation. Using these elements, it is possible to devise an augmented reality system where the user can add virtual objects to the scenario that react to its content. These are magnetic objects because they attach itself to elements on the scene such as floor or walls. This approach liberates the user from moving objects in three-dimensional space. Instead they just push around objects in a more natural interface. The paper studies the impact of adding these magnetism effects in the interaction with an augmented reality-editing interface.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {332–335},
numpages = {4},
keywords = {image processing, interior design, augmented reality},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254621,
author = {S\o{}rensen, Henrik and Kjeldskov, Jesper},
title = {Distributed Interaction: A Multi-Device, Multi-User Music Experience},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254621},
doi = {10.1145/2254556.2254621},
abstract = {In order to take full advantage of the opportunities provided by an increase in consumer device interoperability, we need to explore the design of multi-device systems beyond data sharing between devices. This paper presents an approach to distributed interfaces as technology supporting social interaction in a non-work environment. It consists of a collaborative music system, which in addition to benefit from distributed music storage also distributes playback control onto several mobile devices and provides output through a common situated display. The system has been tested in three different real-life contexts in order to explore the interaction space of non-work multi-device, multi-user environments.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {336–339},
numpages = {4},
keywords = {interaction design, music player, digital ecosystems, ubiquitous computing, distributed interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254623,
author = {Dessart, Charles-Eric and Motti, Vivian Genaro and Vanderdonckt, Jean},
title = {Animated Transitions between User Interface Views},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254623},
doi = {10.1145/2254556.2254623},
abstract = {User interface development life cycle often involve several different views of the user interface over time either at the same level of abstraction or at different levels of abstraction. The relationship between these different views is often supported by tiling coordinated windows containing these related views simultaneously, thus leaving the developer with the responsibility to effectively and efficiently link the corresponding elements of these different views. This paper attempts to overcome the shortcomings posed by the coordinated visualization of multiple views by providing UsiView, a user interface rendering engine in which one single window ensures an animated transition between these different user interface views dynamically an internal view, an external view, and a conceptual view. Examples include the following cases: an authoring environment ensures an animated transition between an internal view (e.g., HTML5) and its external view (e. g., a web page), an Integrated Development Environment ensures an animated transition between its conceptual view and its external view; a model-driven engineering environment ensures an animated transition between the conceptual view at different levels of abstraction, e.g., from task to abstract user interface to concrete user interface until final user interface. The paper discusses the potential advantages of using animated transitions between user interface views during the development life cycle.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {341–348},
numpages = {8},
keywords = {animation, animated transition, model evolution animation, user interface view, user interface development method},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254624,
author = {Umemoto, Kazutoshi and Yamamoto, Takehiro and Nakamura, Satoshi and Tanaka, Katsumi},
title = {Search Intent Estimation from User's Eye Movements for Supporting Information Seeking},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254624},
doi = {10.1145/2254556.2254624},
abstract = {In this paper, we propose a two-stage system using user's eye movements to accommodate the increasing demands to obtain information from the Web in an efficient way. In the first stage the system estimates a user's search intent as a set of weighted terms extracted based on the user's eye movements while browsing Web pages. Then in the second stage, the system shows relevant information to the user by using the estimated intent for re-ranking search results, suggesting intent-based queries, and emphasizing relevant parts of Web pages. The system aims to help users to efficiently obtain what they need by repeating these steps throughout the information seeking process. We proposed four types of search intent estimation methods (MLT, nMLT, DLT and nDLT) considering the relationship among intents, term frequencies and eye movements. As a result of an experiment designed for evaluating the accuracy of each method with a prototype system, we confirmed that the nMLT method works best. In addition, by analyzing the extracted intent terms for eight subjects in the experiment, we found that the system could estimate the unique search intent of each user even if they performed the same search tasks.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {349–356},
numpages = {8},
keywords = {re-ranking, query suggestion, search intent, eye tracking},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254625,
author = {Martin, Beno\^{\i}t and Isokoski, Poika and Karmann, Gregory and Rollinger, Thomas},
title = {Continuous Edgewrite: Dictionary-Based Disambiguation Instead of Explicit Segmentation by the User},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254625},
doi = {10.1145/2254556.2254625},
abstract = {Edgewrite is a text entry method where the user follows the edges of a physical guiding rectangle to enter corner sequences that are interpreted as characters. The original Edgewrite character set resembles the Latin alphabet and includes explicit character segmentation by lifting the stylus (or centering the joystick, etc). We present a variant of Edgewrite that we call the continuous Edgewrite. It relies on a dictionary instead of user's character segmentation to disambiguate words. New users can use the continuous Edgewrite with the help of an interactive visualization of possible continuations while writing. In a 6-session user study we measured initial text transcription performance (increased from 1 to 5.4 wpm) and the ratio of observed explicit segmentations to optimal continuous writing (decreased from 2.5 to 1.5). These results show that it is possible to learn to use the continuous writing mode, but also that the learning takes some time.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {357–364},
numpages = {8},
keywords = {continuous writing, text entry, dictionary, segmentation, disambiguation, EdgeWrite},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254626,
author = {Zhao, Shengdong and Chevalier, Fanny and Ooi, Wei Tsang and Lee, Chee Yuan and Agarwal, Arpit},
title = {AutoComPaste: Auto-Completing Text as an Alternative to Copy-Paste},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254626},
doi = {10.1145/2254556.2254626},
abstract = {The copy-paste command is a fundamental and widely used operation in daily computing. It is generally regarded as a simple task but the process can become tedious when frequent window switching is required to copy-paste across different documents. Auto-completion is another popular operation aimed at reducing users' typing effort. It contrasts to copy-paste by allowing for text completion without switching windows. However, the available content for completion is predefined. We introduce AutoComPaste, an enhanced autocompletion technique for cross-document copy-paste. AutoComPaste allows users to copy-paste different granularity of text from all opened documents without window switching. Our theoretical analysis and empirical study show that AutoComPaste nicely complements traditional copy-paste techniques and outperforms the traditional copy-paste techniques when users have knowledge of the content to be copied.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {365–372},
numpages = {8},
keywords = {copy-paste, windows management, autocompletion},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254627,
author = {Brade, Marius and Br\"{a}ndel, Christian and Groh, Rainer},
title = {Using the Power of Associations: BrainDump - a Revised Nature Inspired Visual Interface for Sensemaking},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254627},
doi = {10.1145/2254556.2254627},
abstract = {Creative work -- especially in business -- is often connected to highly complex data. While current software tools support manifold areas in working with complex data, they are very limited to support creative work. Little research has been done on what kinds of representations are supporting the externalization of mental efforts while making sense of new information. It is especially very challenging to provide appropriate representations for abstract associations.In this paper a new interactive visualization in form of a revised prototype based on a user test of a previous version [2], [3] is proposed. Knowledge workers are enabled to use a highly flexible visual map to represent and refine their current understanding of a task. Derived from experiments with natural physical substances a metaphor based on fluids, cell structure and soap bubbles is used. This kind of visualization allows the user to pin down associations and to clarify anticipations about relations visually.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {373–376},
numpages = {4},
keywords = {knowledge creation, fluid interaction, abstract knowledge representation, visual sensemaking, interactive visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254628,
author = {Pittarello, Fabio and Gatto, Ivano},
title = {A Visual Interface for Querying Ontologically and Socially Annotated 3D Worlds for the Web},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254628},
doi = {10.1145/2254556.2254628},
abstract = {This work proposes a visual interface for querying sets of 3D worlds for the web, designed by different authors and annotated with both ontological top-down and social bottom-up styles. The goal is to overcome the current situation where most of these worlds are like independent islands that can be accessed only using the modalities that have been designed by their authors. Usually these worlds can't be searched, because there is no high level information associated to the 3D entities that compose them. Our proposal is based on a specification, a software architecture and a set of interfaces that permit a full exploitation of the geometric and textual information contained inside these 3D environments. The applications can be different, from the retrieval of single entities for localization purposes by end users to the retrieval of complex spatial patterns for functional or typological analyses by domain experts. A case study related to cultural heritage is presented.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {377–381},
numpages = {5},
keywords = {information finding, social web, ontology, Web3D, folksonomy, annotation, cultural heritage, visual languages, search},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254629,
author = {Martens, Jean-Bernard},
title = {Statistics from an HCI Perspective: Illmo - Interactive Log Likelihood Modeling},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254629},
doi = {10.1145/2254556.2254629},
abstract = {While statistics is recognized as an important topic in the area of human-computer interaction (HCI) and other scientific fields, it is also a topic that spurns continuous debate. The current practice is that, despite extensive developments in the area of statistics in the last decades, most practitioners stick to the most simple parametric methods. Increasingly, we see the argument arise that scientists will have to resort to more advanced methods, which are unfortunately only available in advanced statistical packages that require a specialized syntax and a substantial understanding of the underlying statistical principles. This paper introduces Illmo, a program for performing statistics in a more intuitive way. In order to operate the program successfully, the user only needs to understand a single statistical principle, i. e., the likelihood as a goodness-of-fit measure between the observed data and the proposed statistical model. Illmo is unique in the sense that its visual interface not only provides extensive graphical renderings of the data analysis results, but also assists explicitly in navigating between different (but related) statistical techniques.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {382–385},
numpages = {4},
keywords = {likelihood, interaction, statistics},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254630,
author = {Celentano, Augusto},
title = {Information Design in Interactive Applications for Unfamiliar Cultural Domains},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254630},
doi = {10.1145/2254556.2254630},
abstract = {The design of interactive applications for the culture domain has specific issues due to the width and diversity of the domain, and to the spread of users education and skill. Exposing knowledge in cultural domains unfamiliar to users requires a careful design of information content and flexible exploration styles to match the needs of users with different backgrounds and goals. In this paper we'll discuss some issues and lessons learned in an interdisciplinary project for improved art fruition with interactive mobile guides.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {386–389},
numpages = {4},
keywords = {user logging, information design, mobile art guide, navigation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254631,
author = {Bianchini, C. S. and Borgia, F. and Bottoni, P. and De Marsico, M.},
title = {SWift: A SignWriting Improved Fast Transcriber},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254631},
doi = {10.1145/2254556.2254631},
abstract = {We present SWift, (Sign Writing improved fast transcriber), an advanced editor for computer-aided writing and transcribing using Sign Writing (SW). SW is devised to allow deaf people and linguists alike to exploit an easy-to-grasp written form of (any) sign language. Similarly, SWift has been developed for everyone who masters SW, and is not exclusively deaf-oriented. Using SWift, it is possible to compose and save any sign, using elementary components called glyphs. A guided procedure facilitates the composition process. SWift is aimed at helping to break down the "electronic" barriers that keep the deaf community away from Information and Communication Technology (ICT). The editor has been developed modularly and can be integrated everywhere the use of the SW, as an alternative to written vocal language, may be advisable.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {390–393},
numpages = {4},
keywords = {sign writing, accessibility, deafness, sign languages},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254632,
author = {Misawa, Kana and Ishiguro, Yoshio and Rekimoto, Jun},
title = {LiveMask: A Telepresence Surrogate System with a Face-Shaped Screen for Supporting Nonverbal Communication},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254632},
doi = {10.1145/2254556.2254632},
abstract = {We propose a telepresence system with a real human face-shaped screen. This system tracks the remote user's face and extracts the head motion and the face image. The face-shaped screen moves along three degree-of-freedom (DOF) by reflecting the user's head gestures. As the face-shaped screen is molded based on the 3D-shape scan data of the user, the projected image is accurate even when it is seen from different angles. We expect this system can accurately convey the user's nonverbal communication, in particular the user's gaze direction in 3D space that is not correctly transmitted by using a 2D screen (which is known as "the Mona Lisa effect"). To evaluate how this system can contribute to the communication, we conducted three experiments. The first one examines the blind angle of a face-shaped screen and a flat screen, and compares the ease with which users can distinguish facial expressions. The second one evaluates how the direction in which the remote user's face points can be correctly transmitted. The third experiment evaluates how the gaze direction can be correctly transmitted. We found that the recognizable angles of the face-shaped screen were larger, and that the recognition of the head directions was better than on a flat 2D screen. More importantly, we found that the face-shaped screen accurately conveyed the gaze direction, resolving the problem of the Mona Lisa effect.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {394–397},
numpages = {4},
keywords = {telepresence, face shape, eye gaze},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254633,
author = {Houben, Steven and Vermeulen, Jo and Luyten, Kris and Coninx, Karin},
title = {Co-Activity Manager: Integrating Activity-Based Collaboration into the Desktop Interface},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254633},
doi = {10.1145/2254556.2254633},
abstract = {Activity-Based Computing (ABC) has been proposed as an organisational structure for local desktop management and knowledge work. Knowledge work, however, typically occurs in partially overlapping subgroups and involves the use of multiple devices. We introduce co-Activity Manager, an ABC approach that (i) supports activity sharing for multiple collaborative contexts, (ii) includes collaborative tools into the activity abstraction and (iii) supports multiple devices by seamlessly integrated cloud support for documents and activity storage. Our 14 day field deployment in a multidisciplinary software development team showed that activity sharing is used as a starting point for long-term collaboration while integrated communication tools and cloud support are used extensively during the collaborative activities. The study also showed that activities are used in different ways ranging from project descriptions to to-do lists, thereby confirming that a document-driven activity roaming model seems to be a good match for collaborative knowledge work},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {398–401},
numpages = {4},
keywords = {instant messenger, activity sharing, activity-based computing, collaborative work, activity cloud},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254634,
author = {Marques, Nelson and Dias, Ricardo and Fonseca, Manuel J.},
title = {Improving Blog Exploration through Interactive Visualization},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254634},
doi = {10.1145/2254556.2254634},
abstract = {Blogs are widely used today to publish information on a regular basis. However, users have difficulty in exploring their content and in discovering relevant information. This is due, among other things, to blogs rigid structure, with very long pages, and to the lack of mechanisms for effective navigation and exploration. To overcome these problems, we developed an exploration tool, to help users navigate, browse and visualize blogs. It was developed based on the following four design principles i) a blog should provide an overview of its activity and content to help users identify publication patterns and relevant entries; ii) blogs should present rich compact representations of entries to help readers anticipate the content of posts; iii) comments should have a relevant role, since they convey the social component of the blog; iv) blogs should offer an interactive filtering mechanism to help users explore and find relevant posts. A comparative evaluation with users confirmed the validity of the design principles, since our solution was more efficient, effective and usable than the usual blog interface and the Google Reader.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {402–405},
numpages = {4},
keywords = {blog exploration, blog reading, dynamic queries},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254636,
author = {Nacenta, Miguel and Hinrichs, Uta and Carpendale, Sheelagh},
title = {FatFonts: Combining the Symbolic and Visual Aspects of Numbers},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254636},
doi = {10.1145/2254556.2254636},
abstract = {In this paper we explore numeric typeface design for visualization purposes. We introduce FatFonts, a technique for visualizing quantitative data that bridges the gap between numeric and visual representations. FatFonts are based on Arabic numerals but, unlike regular numeric typefaces, the amount of ink (dark pixels) used for each digit is proportional to its quantitative value. This enables accurate reading of the numerical data while preserving an overall visual context. We discuss the challenges of this approach that we identified through our design process and propose a set of design goals that include legibility, familiarity, readability, spatial precision, dynamic range, and resolution. We contribute four FatFont typefaces that are derived from our exploration of the design space that these goals introduce. Finally, we discuss three example scenarios that show how FatFonts can be used for visualization purposes as valuable representation alternatives.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {407–414},
numpages = {8},
keywords = {typography, information visualization, numerals},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254637,
author = {Dias, Ricardo and Fonseca, Manuel J. and Gon\c{c}alves, Daniel},
title = {Interactive Exploration of Music Listening Histories},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254637},
doi = {10.1145/2254556.2254637},
abstract = {Over the past years, music listening histories have become easily accessible due to the expansion of online lifelogging services. These histories represent the sequence of songs listen by users over time. Although this data contains intrinsic users' tastes and listening behaviors, it has been mainly used to personalize recommendations. Tools to help users exploring and reasoning about the information contained in the listening history, only recently have started to emerge. In this paper we describe a new visualization and exploration tool that allows users to interactively browse their listening histories, while leading them to identify listening trends and habits. Our solution combines a rich-featured timeline-based visualization, a set of synchronized-views and an interactive filtering mechanism to provide a flexible, effective and easy to use system for the analysis and knowledge exploration of listening histories. This was complemented with brushing and highlighting techniques to uncover listening trends about artists, albums and songs. Experimental evaluation with users revealed that they were able to complete all the requested tasks with a low error rate, and that they found the solution flexible and easy to use. Additionally, users were able to infer about their main life events and listening changes, which indicates that our combination of visualization techniques is effective in conveying relevant information about the listening habits.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {415–422},
numpages = {8},
keywords = {interactive browsing, visualization, listening history},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254638,
author = {Riehmann, Patrick and Opolka, Jens and Froehlich, Bernd},
title = {The Product Explorer: Decision Making with Ease},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254638},
doi = {10.1145/2254556.2254638},
abstract = {We present the Product Explorer, an interactive parallel coordinates display for facilitating the selection process of typical products offered in online shops. Users can quickly narrow down the product search to a small subset or even a single product by using our visual query interface. Our study confirms that our interactive Product Explorer is a fast and easy-to-use tool for the product selection process of casual users.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {423–432},
numpages = {10},
keywords = {product visualization, categorical data, parallel coordinates},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254639,
author = {Zhao, Jian and Drucker, Steven M. and Fisher, Danyel and Brinkman, Donald},
title = {TimeSlice: Interactive Faceted Browsing of Timeline Data},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254639},
doi = {10.1145/2254556.2254639},
abstract = {Temporal events with multiple sets of metadata attributes, i. e., facets, are ubiquitous across different domains. The capabilities of efficiently viewing and comparing events data from various perspectives are critical for revealing relationships, making hypotheses, and discovering patterns. In this paper, we present TimeSlice, an interactive faceted visualization of temporal events, which allows users to easily compare and explore timelines with different attributes on a set of facets. By directly manipulating the filtering tree, a dynamic visual representation of queries and filters in the facet space, users can simultaneously browse the focused timelines and their contexts at different levels of detail, which supports efficient navigation of multi-dimensional events data. Also presented is an initial evaluation of TimeSlice with two datasets - famous deceased people and US daily flight delays.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {433–436},
numpages = {4},
keywords = {timeline visualization, faceted browser, temporal events},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254640,
author = {Federico, Paolo and Aigner, Wolfgang and Miksch, Silvia and Windhager, Florian and Smuc, Michael},
title = {Vertigo Zoom: Combining Relational and Temporal Perspectives on Dynamic Networks},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254640},
doi = {10.1145/2254556.2254640},
abstract = {A well-designed visualization of dynamic networks has to support the analysis of both temporal and relational features at once. In particular to solve complex synoptic tasks, the users need to understand the topological structure of the network, its evolution over time, and possible interdependencies. In this paper, we introduce the application of the vertigo zoom interaction technique, derived from filmmaking, to information visualizations. When applied to a two-and-a-half-dimensional view, this interaction technique enables smooth transitions between the relational perspective (node-link diagrams and scatter plots) and the time perspective (trajectories and line charts), supporting a seamless visual analysis and preserving the user's mental map.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {437–440},
numpages = {4},
keywords = {information visualization, interaction, vertigo zoom, 2.5D, dynamic networks},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254641,
author = {Daiber, Florian and Falk, Eric and Kr\"{u}ger, Antonio},
title = {Balloon Selection Revisited: Multi-Touch Selection Techniques for Stereoscopic Data},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254641},
doi = {10.1145/2254556.2254641},
abstract = {With the increasing distribution of multi-touch capable devices multi-touch interaction becomes more and more ubiquitous. Especially the interaction with complex data (e.g. medical or geographical data), which until today mostly rely on mice and keyboard input or intense instrumentation, can benefit from this development. Multi-touch interaction offers new ways to deal with 3D data allowing a high degree of freedom (DOF) without instrumenting the user. This paper evaluates indirect multi-touch 3D selection techniques that can be used to interact with stereoscopic data. In this paper two gestural multi-touch selection techniques are presented and investigated with respect to positions on a stereoscopic multi-touch display and special consideration of objects displayed with different parallaxes. In an experiment it was shown that position and parallax have a significant impact on the interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {441–444},
numpages = {4},
keywords = {gestural interaction, 3D user interfaces, stereoscopic display, selection techniques},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254642,
author = {Guerreiro, Jo\~{a}o and Gon\c{c}alves, Daniel},
title = {Visualiz'em: "Show Me More about Him!"},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254642},
doi = {10.1145/2254556.2254642},
abstract = {Our Personal Information is now scattered among several applications and personal devices in an unrelated, but still connected, information network. We have never had so much information at our disposal as we have now, giving us the opportunities to find information about other people, a very frequent need. While previous works have proposed several solutions to relate data from multiple sources that does not happen at the presentation level. When needing information about people, we still have to navigate among sources to find it. Besides troublesome, it is difficult to have an overall characterization of what each person represents to us. We argue that gathering information from all these sources and presenting it in a unified interface benefits the user by providing a quick and meaningful overview of who that person is and how he/she is related to him/her. We present Visualiz'em, a personal visualization tool based on three interconnected views: Profile, Tagcloud and Timeline. When compared to traditional applications Visualiz'em provides a faster and richer overview of whom a person is and his/her relationship with the user. Moreover, results show that our visualization tool promotes serendipitous behaviors, allowing users to easily explore data and find interaction patterns.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {445–448},
numpages = {4},
keywords = {person search, social data, presentation-level integration, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254643,
author = {Risi, Michele and Scanniello, Giuseppe},
title = {MetricAttitude: A Visualization Tool for the Reverse Engineering of Object Oriented Software},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254643},
doi = {10.1145/2254556.2254643},
abstract = {In this paper, we present a visualization approach for the reverse engineering of object-oriented (OO) software systems and its implementation in MetricAttitude, an Eclipse Rich Client Platform application. The goal of our proposal is to ease both the comprehension of a subject system and the identification of fault-prone classes. The approach graphically represents a suite of object-oriented design metrics (e.g., Weighted Methods per Class) and "traditional" code-size metrics (e.g., Lines Of Code). To assess the validity of MetricAttitude and its underlying approach, we have conducted a case study on the framework Eclipse 3.5. The study has provided indications about the tool scalability, interactivity, and completeness. The results also suggest that our proposal can be successfully used in the identification of fault-prone classes.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {449–456},
numpages = {8},
keywords = {program comprehension, metrics, software maintenance, reverse engineering, software visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254644,
author = {Stoffel, Florian and Janetzko, Halldor and Mansmann, Florian},
title = {Proportions in Categorical and Geographic Data: Visualizing the Results of Political Elections},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254644},
doi = {10.1145/2254556.2254644},
abstract = {Colorpleth maps are commonly used to display election results, either by using one distinct color for representing the winning party in each district or by showing a proportion between two parties on a bi-polar colormap, for example, from red to blue representing Republicans vs. Democrats. Showing only the largest party may disable insights into the data whereas using bipolar colormaps works only reasonably well in cases of two parties. To overcome these limitations we introduce a new technique for visualizing proportions in such categorical data. In particular, we combine bipolar colormaps with an adapted double-rendering of polygons to simultaneously visually represent the first two categories and the spatial location. Our technique enables the recognition of close election results as well as clear majorities in a scalable manner. We proof our concept by applying our technique in a prototype implementation used to display election results from the U. S. Presidential election in 2008 and elections of the German Bundestag in 2005 and 2009. Different interesting findings are presented, which would not be recognizable when visualizing only the winner. As we additionally represent the party with the second most votes, we are able to show changes in the spatial distribution of the votes as well as outlier regions with exceptional results. Our visualization technique therefore enables valuable insights into categorical data with a spatial reference.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {457–464},
numpages = {8},
keywords = {visual analytics, knowledge discovery, data mining, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254645,
author = {Barata, Gabriel and Nicolau, Hugo and Gon\c{c}alves, Daniel},
title = {AppInsight: What Have I Been Doing?},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254645},
doi = {10.1145/2254556.2254645},
abstract = {In this paper we present AppInsight, a visualization tool that enables users to reminisce on their computer usage history and derive meaningful insights about behaviors and trends. Human memory has the ability to re-experience episodes from our lives when supplied with suitable contextual cues, such as places, music, and so on. We explore a small set of properties, such as the application's name, URL and window title as contextual cues, in order to characterize the users' activity on their personal computers and how it evolves over time. Our user study shows that users enjoyed viewing their computer usage history and were able to both recall past events and introspect about their lives. Moreover, one of the most surprising outcomes is that they found several different applications for our tool, such as improving usage behaviors, controlling productivity, generating activity reports, and monitoring users in psychological studies. Finally, we discuss some lessons learned from our study and propose future research directions.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {465–472},
numpages = {8},
keywords = {information visualization, computer usage, desktop activity},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254646,
author = {Namoun, Abdallah and Wajid, Usman and Mehandjiev, Nikolay and Owrak, Ali},
title = {User-Centered Design of a Visual Data Mapping Tool},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254646},
doi = {10.1145/2254556.2254646},
abstract = {Understanding the meaning and the type of a business document received by a company is important in order to determine an appropriate response. We have developed a visual tool allowing ordinary users to express mappings between arriving documents and their elements on one side and the different document types on the other. The tool is used to set up and continuously update an automatic semantic analysis mechanism which determines the document type from a set of information items contained in the document, thus allowing automatic processing associated with the types to be applied to the arriving document instances. The activities performed by end users within the visual data mapping tool are quite complex and require user-centric design to ensure tool is useful and usable. In this paper we describe the user-centric process informing the design of the tool. We conducted two workshops to discover the mental models of our users regarding the information content in their documents and their requirements towards the design of the tool. Subsequently, these findings were used to design and implement the visual data mapping tool. The resulting system was evaluated by target end users who proficiently demonstrated the usability of the developed concepts and features.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {473–480},
numpages = {8},
keywords = {visual data mapping tool, user-centered design, business documents, observational studies, end user development},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254647,
author = {Garcia, Tiago and Aires, Jo\~{a}o and Gon\c{c}alves, Daniel},
title = {Who Have I Been Talking To?},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254647},
doi = {10.1145/2254556.2254647},
abstract = {One of the most frequently used ways to communicate with others is email. We have created "Who Have I Been Talking To?", a visualization of the users' email that can provide insights about their relationships with others how one relation has evolved over time (reflected by the number of messages exchanged); the nature of the relationship (comparing the subjects of messages and how asymmetric is the ratio between sent and received email); and allowing users to compare their relationships with different people. We found that a variant of a stacked bar chart over an interactive timeline allowed all of those goals to be achieved. User studies showed that the users were able to understand the visualization and gain meaningful insights about their electronic communications with others.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {481–484},
numpages = {4},
keywords = {personal information management, visualization, email, relationships},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254648,
author = {Schwarz, Tobias and Hennecke, Fabian and Lauber, Felix and Reiterer, Harald},
title = {Perspective+detail: A Visualization Technique for Vertically Curved Displays},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254648},
doi = {10.1145/2254556.2254648},
abstract = {This paper describes the perspective+detail visualization concept, which extends the conventional overview+detail pattern by adding a perspective viewing area and a further, text-based area containing partial details. Known problems of such extensions are mitigated using a vertically curved display. Our concept aims to bridge both the geographical and the content gap between the two main display areas and to provide an improved overview. An experimental case study based on a typical traffic control room operator task provided first insights on our visualization. These initial findings revealed that perspective+detail assists users in fulfilling their tasks and keeping their orientation in the information space.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {485–488},
numpages = {4},
keywords = {curved display, interactive surfaces, map-based interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254649,
author = {Ma, J. and Murphy, D. and O'Mathuna, C. and Hayes, M. and Provan, G.},
title = {Visualizing Uncertainty in Multi-Resolution Volumetric Data Using Marching Cubes},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254649},
doi = {10.1145/2254556.2254649},
abstract = {Data sets acquired from complex scientific simulation, high precision engineering experiment and high-speed computer network have been exponentially increased, and visualization and analysis of such large-scale of data sets have been identified as a significant challenge to the visualization community. Over the past years many scientists have made attempt to address this problem by proposing various data reduction techniques. Consequently the size of data can be reduced and issues associated to the visualization can be improved (e.g. real-time interaction and visual overload). However, during the process of data reduction, the information of original data sets was approximated and potential errors were introduced. It leads to a new problem with regard to the integrity of the data and might mislead users for incorrect decision making. Therefore in this paper we aim to solve the problem by introducing three novel uncertainty visualization methods, which depict both the multi-resolution (MR) approximations of the original data set and the errors associated with each of its low resolution representations. As a result we faithfully represent the MR data sets and allow users to make suitable decisions from the visual output. We applied our techniques on a data set from medical domain to demonstrate their effectiveness and usability.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {489–496},
numpages = {8},
keywords = {scientific visualization, marching cubes, uncertainty visualization, multi-resolution volumetric data},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254651,
author = {Kairam, Sanjay and MacLean, Diana and Savva, Manolis and Heer, Jeffrey},
title = {GraphPrism: Compact Visualization of Network Structure},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254651},
doi = {10.1145/2254556.2254651},
abstract = {Visual methods for supporting the characterization, comparison, and classification of large networks remain an open challenge. Ideally, such techniques should surface useful structural features -- such as effective diameter, small-world properties, and structural holes -- not always apparent from either summary statistics or typical network visualizations. In this paper, we present GraphPrism, a technique for visually summarizing arbitrarily large graphs through combinations of 'facets', each corresponding to a single node- or edge-specific metric (e.g., transitivity). We describe a generalized approach for constructing facets by calculating distributions of graph metrics over increasingly large local neighborhoods and representing these as a stacked multi-scale histogram. Evaluation with paper prototypes shows that, with minimal training, static GraphPrism diagrams can aid network analysis experts in performing basic analysis tasks with network data. Finally, we contribute the design of an interactive system using linked selection between GraphPrism overviews and node-link detail views. Using a case study of data from a co-authorship network, we illustrate how GraphPrism facilitates interactive exploration of network data.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {498–505},
numpages = {8},
keywords = {graph visualization, network analysis, scalability},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254652,
author = {Riche, Nathalie Henry and Dwyer, Tim and Lee, Bongshin and Carpendale, Sheelagh},
title = {Exploring the Design Space of Interactive Link Curvature in Network Diagrams},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254652},
doi = {10.1145/2254556.2254652},
abstract = {When exploiting the power of node-link diagrams to represent real-world data such as web structures, airline routes, electrical, telecommunication and social networks, link congestion frequently arises. Such areas in the diagram---with dense, overlapping links---are not readable connectivity, node shapes, labels, and contextual information are obscured. In response, graph-layout research has begun to consider the modification of link shapes with techniques such as link routing and bundling. In this paper, we delve into the interactive techniques afforded by variant use of link curvature, delineating a six-dimensional design space that is populated by four families of interactive techniques: bundling, fanning, magnets, and legends. Our taxonomy encompasses existing techniques and reveals several novel link interactions. We describe the implementation of these techniques and illustrate their potential for exploring dense graphs with multiple types of links.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {506–513},
numpages = {8},
keywords = {link curvature, design space, taxonomy, interaction techniques, edge bundling, graph visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254653,
author = {Guilmaine, David and Viau, Christophe and McGuffin, Michael J.},
title = {Hierarchically Animated Transitions in Visualizations of Tree Structures},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254653},
doi = {10.1145/2254556.2254653},
abstract = {We present an experimental comparison of 4 techniques for smoothly animating changes in a radial tree visualization. Two traditional techniques, linear and staged animation, are compared with two novel techniques a hierarchical animation that proceeds level-by-level through the tree, and a hybrid animation technique that mixes the staged and hierarchical approaches. Users were asked to track changes in nodes of the tree during animated transitions. Results show a significant advantage for the hierarchical and hybrid animation techniques for tracking certain kinds of changes. We then propose guidelines for designing animated transitions. Finally, we present a new popup widget for interactively controlling the progression of an animation that combines advantages of previous widgets.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {514–521},
numpages = {8},
keywords = {smoothly animated transitions, animation, tree visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254654,
author = {Hasco\"{e}t, Mountaz and Dragicevic, Pierre},
title = {Interactive Graph Matching and Visual Comparison of Graphs and Clustered Graphs},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254654},
doi = {10.1145/2254556.2254654},
abstract = {We introduce interactive graph matching, a process that conciliates visualization, interaction and optimization approaches to address the graph matching and graph comparison problems as a whole. Interactive graph matching is based on a multi-layered interaction model and on a visual reification of graph matching functions. We present three case studies and a system named Donatien to demonstrate the interactive graph matching approach. The three case studies involve different datasets a) subgraphs of a lexical network, b) graph of keywords extracted from the InfoVis contest benchmark, and c) clustered graphs computed from different clustering algorithms for comparison purposes.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {522–529},
numpages = {8},
keywords = {graph matching, visual analysis, deterministic layout of graph, multi-layer comparison},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254655,
author = {Bra\c{s}oveanu, Adrian M. P. and Hubmann-Haidvogel, Alexander and Scharl, Arno},
title = {Interactive Visualization of Emerging Topics in Multiple Social Media Streams},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254655},
doi = {10.1145/2254556.2254655},
abstract = {This paper introduces an interactive news flow visualization that reveals emerging topics in dynamic digital content archives. The presented approach combines several visual metaphors and can be easily adapted to present multi-source social media datasets. In the context of this work, we discuss various methods for improving visual interfaces for accessing aggregated media representations. We combine falling blocks with bar graphs and arcs, but keep these elements clearly separated in different areas of the display. The arc metaphor is adapted and enriched with interactive controls to help users understand the dataset's underlining meaning. The paper describes the implementation of the prototype and discusses design issues with a particular emphasis on visual metaphors to highlight hidden relations in digital content. We conclude with a summary of the lessons learnt and the integration of the visualization component into the Media Watch on Climate Change (www.ecoresearch.net/climate), a public Web portal that aggregates environmental information from a variety of online sources including news media, blogs and other social media such as Twitter, YouTube and Facebook.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {530–533},
numpages = {4},
keywords = {information visualization, interface metaphor, associated term, news flow, emerging topic, thread arc},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254656,
author = {Teixeira, Jo\~{a}o and Barata, Gabriel and Gon\c{c}alves, Daniel},
title = {Metabrain: Web Information Extraction and Visualization},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254656},
doi = {10.1145/2254556.2254656},
abstract = {Nowadays, a hitherto unseen amount of information can be found in the World Wide Web. While available, this information is fragmented among different web sites. This is especially true for implicit knowledge, not directly written in any one site, but arising from patterns and interactions between pages. For instance, the number of search results for a particular query string might be a meaningful indicator of its popularity or overall interest. Our research focuses on the design of an interface that allows end-users to access implicit information. A prototype application, Metabrain, embodies our solutions and makes it possible to mine the web for statistically relevant patterns, with the help of simple and straightforward algorithms and user interface. To help the users make sense of that information, Metabrain then allows custom visualizations to be crafted. User studies show that users can search for relevant information up to four times faster than using traditional Web search engines alone. A system usability scale questionnaire confirms the interface is usable and effective.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {534–537},
numpages = {4},
keywords = {implicit knowledge in the web, information extraction, human-computer interaction, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254658,
author = {Deufemia, V. and Paolino, L. and Tortora, G. and Traverso, A. and Mascardi, V. and Ancona, M. and Martelli, M. and Bianchi, N. and De Lumley, H.},
title = {Investigative Analysis across Documents and Drawings: Visual Analytics for Archaeologists},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254658},
doi = {10.1145/2254556.2254658},
abstract = {With the invention and rapid improvement of data-capturing devices, such as satellite imagery and digital cameras, the information that archaeologists must manage in their everyday's activities has rapidly grown in complexity and amount. In this work we present Indiana Finder, an interactive visualization system that supports archaeologists in the examination of large repositories of documents and drawings. In particular, the system provides visual analytic support for investigative analysis such as the interpretation of new archaeological findings, the detection of interpretation anomalies, and the discovery of new insights. We illustrate the potential of Indiana Finder in the context of the digital protection and conservation of rock art natural and cultural heritage sites. In this domain, Indiana Finder provides an integrated environment that archaeologists can exploit to investigate, discover, and learn from textual documents, pictures, and drawings related to rock carvings. This goal is accomplished through novel visualization methods including visual similarity ring charts that may help archaeologists in the hard task of dating a symbol in a rock engraving based on its shape and on the surrounding symbols.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {539–546},
numpages = {8},
keywords = {visual analytics, investigative analysis, rock art archaeology, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254659,
author = {Kandel, Sean and Parikh, Ravi and Paepcke, Andreas and Hellerstein, Joseph M. and Heer, Jeffrey},
title = {Profiler: Integrated Statistical Analysis and Visualization for Data Quality Assessment},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254659},
doi = {10.1145/2254556.2254659},
abstract = {Data quality issues such as missing, erroneous, extreme and duplicate values undermine analysis and are time-consuming to find and fix. Automated methods can help identify anomalies, but determining what constitutes an error is context-dependent and so requires human judgment. While visualization tools can facilitate this process, analysts must often manually construct the necessary views, requiring significant expertise. We present Profiler, a visual analysis tool for assessing quality issues in tabular data. Profiler applies data mining methods to automatically flag problematic data and suggests coordinated summary visualizations for assessing the data in context. The system contributes novel methods for integrated statistical and visual analysis, automatic view suggestion, and scalable visual summaries that support real-time interaction with millions of data points. We present Profiler's architecture --- including modular components for custom data types, anomaly detection routines and summary visualizations --- and describe its application to motion picture, natural disaster and water quality data sets.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {547–554},
numpages = {8},
keywords = {visualization, anomaly detection, data quality, data analysis},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254660,
author = {Endert, Alex and Fox, Seth and Maiti, Dipayan and Leman, Scotland and North, Chris},
title = {The Semantics of Clustering: Analysis of User-Generated Spatializations of Text Documents},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254660},
doi = {10.1145/2254556.2254660},
abstract = {Analyzing complex textual datasets consists of identifying connections and relationships within the data based on users' intuition and domain expertise. In a spatial workspace, users can do so implicitly by spatially arranging documents into clusters to convey similarity or relationships. Algorithms exist that spatialize and cluster such information mathematically based on similarity metrics. However, analysts often find inconsistencies in these generated clusters based on their expertise. Therefore, to support sensemaking, layouts must be co-created by the user and the model. In this paper, we present the results of a study observing individual users performing a sensemaking task in a spatial workspace. We examine the users' interactions during their analytic process, and also the clusters the users manually created. We found that specific interactions can act as valuable indicators of important structure within a dataset. Further, we analyze and characterize the structure of the user-generated clusters to identify useful metrics to guide future algorithms. Through a deeper understanding of how users spatially cluster information, we can inform the design of interactive algorithms to generate more meaningful spatializations for text analysis tasks, to better respond to user interactions during the analytics process, and ultimately to allow analysts to more rapidly gain insight.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {555–562},
numpages = {8},
keywords = {visualization, visual analytics, text analytics, clustering},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254661,
author = {Harrigan, Martin and Archambault, Daniel and Cunningham, P\'{a}draig and Hurley, Neil},
title = {EgoNav: Exploring Networks through Egocentric Spatializations},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254661},
doi = {10.1145/2254556.2254661},
abstract = {EgoNav is a visual analytics system that characterizes egos based on the relationship structure of their egocentric networks and presents the results as a spatialization. An ego, or individual node in a network, is most closely related to its neighbors, and to a lesser degree, to its neighbor's neighbors. For example, in social networks, people are closely related to their friends and family. In financial networks, the affairs of borrowers and lenders are more closely tied to each other. In fact, the relationship structure surrounding an ego, or an egocentric network, can provide characteristic information about the ego itself. Using network motif analysis and dimensionality reduction techniques, the system places egos in similar areas of a spatialization if their egocentric networks are structurally similar. This view of a network discriminates between the various classes of typical and exceptional egos. We demonstrate its effectiveness using appropriate synthetic datasets, real-world mobile phone call and peer-to-peer lending datasets. We subsequently elicit user feedback from experts involved in the investigation of financial fraud to assess the tool's applicability in this domain.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {563–570},
numpages = {8},
keywords = {network motif analysis, spatializations, visual analytics},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254663,
author = {Cabral, Diogo and Valente, Jo\~{a}o G. and Arag\~{a}o, Ur\^{a}ndia and Fernandes, Carla and Correia, Nuno},
title = {Evaluation of a Multimodal Video Annotator for Contemporary Dance},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254663},
doi = {10.1145/2254556.2254663},
abstract = {This paper discusses the evaluation of a video annotator that supports multimodal annotation and is applied to contemporary dance as a creation tool. The Creation-Tool was conceived and designed to assist the creative processes of choreographers and dance performers, functioning as a digital notebook for personal annotations. The prototype, developed for Tablet PCs, allows video annotation in real-time, using a live video stream, or postevent, using a pre-recorded video stream. The tool also allows different video annotation modalities, such as annotation marks, text, audio, ink strokes and hyperlinks. In addition, the system enables different modes of annotation and video visualization. The development followed an iterative design process involving two choreographers, and a usability study was carried out, involving international dance performers participating in a contemporary dance "residence - workshop".},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {572–579},
numpages = {8},
keywords = {real-time video annotations, multimodal video annotations, contemporary dance, performing arts},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254664,
author = {Ardito, C. and Costabile, M. F. and Lanzilotti, R. and De Angeli, A. and Desolda, G.},
title = {A Field Study of a Multi-Touch Display at a Conference},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254664},
doi = {10.1145/2254556.2254664},
abstract = {Large interactive displays are increasingly used in public spaces. Yet, it is still a challenge to understand how people behave when faced with such displays in their real life, not only when they are used for entertainment (or advertising), but also when they mediate more purposeful tasks. Do people feel shy? Are they willing to interact? Are they satisfied with the services offered and will they come back to use them again? To answer such questions, these systems have to be evaluated in the field so as to understand their actual impact on users in the real life. This paper first introduces an initial evaluation framework, aimed at highlighting some of the variables involved in understanding the impact of large display installations. Then, it applies the framework to analyze users' behavior and their experience with a large display installed at an international conference. Results highlighted that people showed greater interest in those services that, despite a lower appeal, supported them in carrying out useful tasks.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {580–587},
numpages = {8},
keywords = {multi-touch display, evaluation, field study},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254665,
author = {Suhonen, Katja and V\"{a}\"{a}n\"{a}nen-Vainio-Mattila, Kaisa and Schrader, Martin and P\"{o}l\"{o}nen, Monika and Salmimaa, Marja and Saarikko, Pasi},
title = {Two User Studies on Creation and Evaluation of Use Scenarios for Mixed Reality Communication},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254665},
doi = {10.1145/2254556.2254665},
abstract = {Mixed reality (MR) technologies and applications, including interpersonal communication, are rapidly evolving. Despite its promise, peoples' actual needs concerning the advanced uses of MR are less studied. To address this knowledge gap, we conducted two focus group user studies in which we explored people's perceptions, expectations, and ideas concerning remote interpersonal MR communication and collaboration. In the first study we examined people's perceptions of MR and the study participants collaboratively created 21 scenarios for MR communication. For the second study, the most promising of these scenarios were selected and refined to develop three different types of scenarios one with emotional content, one emphasizing entertainment and one focused on work-related situations. The scenarios were evaluated by the participants of the second study in the context of a specific MR communication system that uses near-eye displays. The results indicate that the expected advantages of MR in communication are its efficiency, richness and the increased feeling of presence over distance. However, concerns were raised about the technical reliability, usability and accessibility of advanced MR applications. Work and entertainment use contexts were preferred over emotional communication. Maintaining close emotional relationships was perceived to require real physical presence and interaction instead of being technology-mediated.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {588–595},
numpages = {8},
keywords = {scenarios, focus groups, mediated communication, user expectations, collaboration, mixed reality, user experience},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254666,
author = {Humayoun, Shah Rukh and Dubinsky, Yael and Catarci, Tiziana and Nazarov, Eli and Israel, Assaf},
title = {A Model-Based Approach to Ongoing Product Evaluation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254666},
doi = {10.1145/2254556.2254666},
abstract = {Evaluating interface usability and system functionality is time-consuming and effort-intense. The short time-span involved in development iterations, such as those in agile development, makes it challenging for software teams to perform ongoing interface usability evaluation and system functionality testing. We propose a way to perform product ongoing evaluation, thus enabling software teams to perform interface usability evaluation alongside automated system functionality testing. We use formal task models, created in our defined TaMoGolog task modeling language, to conduct product evaluation experiments through TaMUlator. TaMUlator is a tool we developed for use at the Integrated Development Environment (IDE) level. Our case study revealed that software teams can easily engage in system evaluation by using TaMUlator on an iterative basis.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {596–603},
numpages = {8},
keywords = {automated testing, TaMoGolog, TaMUlator, usability evaluation, task models, product evaluation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254667,
author = {Diozzi, Ferruccio and Di Giovanni, Pasquale and Pezzullo, Gianluca and Sannino, Rosa and Vozella, Angela},
title = {Usability Issues for an Aerospace Digital Library},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254667},
doi = {10.1145/2254556.2254667},
abstract = {To help the aerospace community (made by experimental, numerical, system engineers, scientists, managers, stakeholders, customers, authorities...) capitalize and share their outcomes, a digital library (DL) center will be set up, as a result of a national project called SIA (Sistema Informativo Aerospaziale). The objective of the digital library Centre is to demonstrate new ways in which, collections of documents, data and associated services, can facilitate collaboration within the aerospace community. This paper gives an overview of the key functions and how they map into the requirements of the target community. Attention is put on usability characteristics. Two forms of DL usability - interface and organizational - are tackled.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {604–607},
numpages = {4},
keywords = {WDF, taxonomic and ontological structures, collective intelligence, semantic features, cultural models, usability engineering, CGNS},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254668,
author = {Forsell, Camilla and Cooper, Matthew},
title = {A Guide to Reporting Scientific Evaluation in Visualization},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254668},
doi = {10.1145/2254556.2254668},
abstract = {This paper addresses some fundamental issues that need to be considered when reporting an evaluation study in visualization or other areas that develop visual interfaces. Today evaluations are frequently included in publications in these fields, however there exists no uniform standard for reporting formats. Instead there is heterogeneity in the structure and content provided and often important information is missing, making it hard to gain insight and trust in the results and conclusions. Consequently there is a need for an easy to access guidance on how to accomplish sound reporting with high quality. This is the only way in which authors can enable an assessment of the validity of a study and enable it to be replicated by others in order to verify it. This paper presents a first effort to introduce guidelines on what constitutes an effective structure and what content to address and how. It also points out common pitfalls and mistakes when reporting and how these can be avoided. The paper could be used as a guide by authors when describing their evaluation studies and it could also be helpful when reviewing publications presenting such work since the same guidelines for content apply.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {608–611},
numpages = {4},
keywords = {guidelines, evaluation, reporting, visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254669,
author = {Matulic, Fabrice and Norrie, Moira C.},
title = {Supporting Active Reading on Pen and Touch-Operated Tabletops},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254669},
doi = {10.1145/2254556.2254669},
abstract = {With the proliferation and sophistication of digital reading devices, new means to support the task of active reading (AR) have emerged. In this paper, we investigate the use of pen-and-touch-operated tabletops for performing essential processes of AR such as annotating, smooth navigation and rapid searching. We present an application to support these processes and then report on a user study designed to compare the suitability of our setup for three typical tasks against the use of paper media and Adobe Acrobat on a regular desktop PC. From this evaluation, we found out that pen and touch tabletops can successfully combine the advantages of paper and digital devices without their disadvantages. We however also learn from observations and participant feedback that there are still a number of hardware and software limitations that impede the user experience and hence need to be addressed in future systems.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {612–619},
numpages = {8},
keywords = {bimanual pen and touch interaction, active reading, digital tabletops},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254670,
author = {McGee, Fintan and Dingliana, John},
title = {An Empirical Study on the Impact of Edge Bundling on User Comprehension of Graphs},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254670},
doi = {10.1145/2254556.2254670},
abstract = {Edges are one of the primary sources of clutter when viewing graphs as node-link diagrams. One technique to reduce this clutter is to bundle edges together based on a nearby source or destination. Combined with edge translucency, edge bundling is reported to reduce the clutter and reveal higher-level edge patterns. However there is very little empirical data on the impact of edge bundling on user performance, as well as the impact of graph characteristics such as edge density and graph size on the effectiveness of edge bundling as a graph-visualization technique. We have performed user experiments to evaluate the impact of bundling on user performance, using a set of randomly generated undirected compound graphs with varying vertex counts and edge densities. Our results indicate that edge bundling negatively impacts user performance at tracing paths between nodes, both in terms of accuracy and time. They also indicate that while edge bundling may provide no clear significant benefit in terms of accuracy for recognising higher-level cluster connectivity, it does provide a significant improvement in user response time.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {620–627},
numpages = {8},
keywords = {edge bundling, graphs, edge density, user evaluation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254671,
author = {Singh, H. Lally and Gra\v{c}anin, Denis and Matkovi\'{c}, Kre\v{s}imir},
title = {An Approach to Tuning Distributed Virtual Environment Performance by Modifying Terrain},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254671},
doi = {10.1145/2254556.2254671},
abstract = {Distributed Virtual Environments (DVEs) must continue to perform well as users are added. However, DVE performance can become sensitive to user behavior in many ways their actions, their positions, and even the direction that they look. These behavioral elements are important for evaluating virtual terrains. While two terrains may be similar in terms of user experience, task efficiency, immersion, and even aesthetics, they may exhibit substantially different performance out of the DVE when many users are logged in.We discuss an approach --- Software Scalability Engineering (SSE) --- that uses load simulation and iterative modeling to locate causes of undesirable performance, experiment with changes, and verify improvements to DVE systems. Presented here is a case study of using the approach to substantially improve the CPU requirements of the Torque engine. With a key factor determined, we evaluate several modifications to the original terrain. Finally, a modification is selected for its ability to stabilize the simulation time.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {628–631},
numpages = {4},
keywords = {performance analysis, load simulator, simulation performance, user behavior},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254672,
author = {Wittenburg, Kent and Malizia, Alessio and Lupo, Luca and Pekhteryev, Georgiy},
title = {Visualizing Set-Valued Attributes in Parallel with Equal-Height Histograms},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254672},
doi = {10.1145/2254556.2254672},
abstract = {Visualization of set-valued attributes in multi-dimensional information visualization systems remains a relatively unexplored problem. Here we introduce a novel method for visualizing set-valued attributes that we call the singleton set distribution view and integrate it into an interactive multi-dimensional attribute visualization tool utilizing parallel bargrams (aka equal-height histograms) as its main visual motif. We discuss our design rationale and report on the results of an evaluation study.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {632–635},
numpages = {4},
keywords = {multi-dimensional visualization, equal-height histograms, set-valued attributes, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254673,
author = {Cui, Yanqing and Wang, Li},
title = {Motivations for Accessing Social Networking Services on Mobile Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254673},
doi = {10.1145/2254556.2254673},
abstract = {Accessing social networking services is one key user activity on mobile devices. To characterize this experience, we conduct a user study with active users of mobile social networking services in South Korea. In the study, we interviewed 12 users and asked them to keep a diary for a week. The study shows awareness, social connection, and diversion as the key motivations for mobile access to social networking services. The users seldom resort to these services to fulfill curiosity about arbitrary questions or to seek information for productivity purposes, which they do with other mobile Internet services. The study reveals cross-medium alerts as a common trigger for mobile social networking sessions. We discuss implications of this user study for future work to promote mobile access to social networking services.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {636–639},
numpages = {4},
keywords = {motivations, social networking services, diary, mobile devices, social media},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254674,
author = {De Lucia, Andrea and Francese, Rita and Risi, Michele and Tortora, Genoveffa},
title = {Generating Applications Directly on the Mobile Device: An Empirical Evaluation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254674},
doi = {10.1145/2254556.2254674},
abstract = {This paper presents an investigation, based on the combined use of two techniques: a questionnaire-based survey and an empirical analysis, to assess the effectiveness and efficacy of the MicroApp environment to support End-Users in the visual composition of their own applications directly on their mobile phone. The satisfaction of the End-Users has been investigated as well. The context of this study was constituted of students, administrative personnel and consultants of the University of Salerno. The survey shows a positive satisfaction degree of all the involved subjects, while the empirical analysis reveals that the use of the Micro App tool increases the efficiency and, in case of complex tasks, also the simplicity with respect to the use of a PC-based similar tool proposed by Google.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {640–647},
numpages = {8},
keywords = {mobile applications, visual programming, end-user development, empirical study},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254676,
author = {Das, Arindam and Stuerzlinger, Wolfgang},
title = {Comparing Cognitive Effort in Spatial Learning of Text Entry Keyboards and ShapeWriters},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254676},
doi = {10.1145/2254556.2254676},
abstract = {Stable structured layouts of buttons are a primary means of control for input in current graphical user interfaces. Such layouts are ubiquitous---from tiny iPhone screens to large kiosk screens in the malls---they are found everywhere. Yet, there has been relatively little theoretical account that compares the impact of cognitive effort on learning such stable layouts. In this paper, we demonstrate that prior empirical results on cognitive effort in learning stable layouts are theoretically predictable through the memory activation model of a cognitive architecture, ACT-R. We go beyond previous work by quantitatively comparing the level of cognitive effort in terms of a newly introduced parameter in the declarative memory model of ACT-R. We theoretically compare the cognitive effort of two different layouts of graphical buttons with respect to their label representativeness in the domains of traditional keyboard and ShapeWriter.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {649–652},
numpages = {4},
keywords = {ShapeWriter, keyboard, ACT-R, text entry, cognitive effort, modeling},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254677,
author = {Meiller, Dieter and Hemmje, Matthias and Klas, Claus-Peter},
title = {Aesthetic Visualisation of Information: Optimization of Graph Representations},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254677},
doi = {10.1145/2254556.2254677},
abstract = {In this work we will discuss the optimization of aesthetic criteria for graph representation. Most existing algorithms for drawing graphs generate a specific layout and are usually specialized in optimizing certain aesthetic criteria. Generated representations visualize data for specific applications. After all, it could be important to have an individual layout for the application. The communication objective should determine the representation. The individual representation should, however, also be optimized to be cognitively efficient. We suggest a clear distinction between the graph layout and its optimization. A useful field of application could be the visualization of search results for supporting information retrieval.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {653–656},
numpages = {4},
keywords = {genetic algorithms, aesthetics, information retrieval on graphs, graph drawing, information visualisation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254678,
author = {Monkaresi, Hamed and Calvo, Rafael A. and Hussain, M. S.},
title = {Automatic Natural Expression Recognition Using Head Movement and Skin Color Features},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254678},
doi = {10.1145/2254556.2254678},
abstract = {Significant progress has been made in automatic facial expression recognition, yet most state of the art approaches produce significantly better reliabilities on acted expressions than on natural ones. User interfaces that use facial expressions to understand user's affective states need to be most accurate during naturalistic interactions. This paper presents a study where head movement features are used to recognize naturalistic expressions of affect. The International Affective Picture System (IAPS) collection was used as stimulus for triggering different affective states. Machine learning techniques are applied to classify user's expressions based on their head position and skin color changes. The proposed approach shows a reasonable accuracy in detecting three levels of valence and arousal for user-dependent model during naturalistic human-computer interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {657–660},
numpages = {4},
keywords = {machine learning, facial expression recognition, emotion recognition, affective user interface},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254679,
author = {Nakamura, Satoshi and Komatsu, Takanori},
title = {Study of Information Clouding Methods to Prevent Spoilers of Sports Match},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254679},
doi = {10.1145/2254556.2254679},
abstract = {Seeing the final score of a sports match on the Web often spoils the pleasure of a user who is waiting to watch a recording of this match on TV. This paper proposes four information clouding methods to block spoiling information, and describes implementation of a system using these methods as a browser extension. We then experimentally investigate the usefulness of the methods, taking into account their differences, differences in the variety of content, and differences in the user's interest in sports.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {661–664},
numpages = {4},
keywords = {web, anti-spoiler, browser, information clouding, spoilers},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254680,
author = {Varesano, Antonella},
title = {Interaction Atmosphere: Blending Design Style},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254680},
doi = {10.1145/2254556.2254680},
abstract = {Interfaces, created by HCI with functional and ergonomic objectives, due to the proliferation of communication landscape need today new design methodologies to develop standardized solutions. Interaction Design is the discipline that deals with these new design aspects in order to foster interactions simpler and designed for specific contexts. This paper explores the role of creativity and emotions, related to atmosphere interaction in visual interfaces. The research intends to investigate the potential of a blending approach between functionality and creativity that introduces in the user oriented design process some techniques coming from different disciplines. The main goals are: stimulate the creative process, arise inferential aspects about previous positive experience, develop aesthetic awareness of the atmosphere concept as balance of points highlighted in the studies of Cooper, Verplank and Veen. The first aim of this paper is to propose a new blending approach called 'interaction atmosphere' for further application in digital ambient.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {665–668},
numpages = {4},
keywords = {creativity, blending design, inference, semiotic, emotion, interaction atmosphere, body experience, interaction design},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254681,
author = {Landwich, Paul and Klas, Claus-Peter and Hemmje, Matthias},
title = {Supporting User of Information Retrieval Systems by Visualizing the Information Dialog Context},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254681},
doi = {10.1145/2254556.2254681},
abstract = {In this paper we present first results for a new approach designing innovative user interfaces for information retrieval systems. The leading thought of this paper is based on the fact that a dialog between user and system during a search process establishes an information dialog context. We introduce a framework for information retrieval systems to handle the activities and sets elaborated within a search process in order to support the user. In this paper we present a prototype tool which provides and visualizes sets of information objects in order to bring users into the position to keep the developed sets within the search process at hand and actively manage them. Finally, a description of a user study and expert interviews and their evaluation results conducted on the basis of the prototypical tool is provided.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {669–672},
numpages = {4},
keywords = {prototype, user interface, framework, search process, information visualization, dialog, interactive information retrieval, evaluation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254682,
author = {Watanabe, Chiho and Tsukada, Koji and Siio, Itiro},
title = {JewelryStudio: System for Capturing/Browsing Pictures of Jewelry from Multiple Viewpoints},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254682},
doi = {10.1145/2254556.2254682},
abstract = {Users often have difficulty taking pictures of jewelry as it requires advanced photographic techniques. We propose a system called "JewelryStudio" that helps users easily capture pictures of jewelry from multiple angles, and upload them to an online database for various web-based uses. In this paper, we explain the concept, implementation, and evaluation of the JewelryStudio.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {673–676},
numpages = {4},
keywords = {web browser, multiple angles, jewelry, pictures},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254683,
author = {Kobayashi, Kaori and Kitayama, Daisuke and Sumiya, Kazutoshi},
title = {Automatic Properties Adjusting Method Using User Operations for Local Search Smartphone Applications},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254683},
doi = {10.1145/2254556.2254683},
abstract = {We propose a recommendation method of geographical information for mobile devices based on user search operations using local services such as Yelp. We presume that we can estimate the search conditions using user operations for a region and category by analyzing the user operations. There are four operations, changing the region, selecting the categories, viewing the objects, and selecting an object. We use the user operations to extract important factors attached to the user. At this time, we think that users have the requirement that there be an appropriate number of displayed objects. Thus, we propose a recommendation method with an automatic adjusting search property to satisfy the constrained condition for the number of displayed objects. We aim for intelligent interaction such as optimizing the search properties to estimate the user requirements from realizable operations on a general interface. In this paper, we discuss an adjusting method and developed prototype system.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {677–680},
numpages = {4},
keywords = {GIS, online maps, mobile phones, local search},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254684,
author = {Barricelli, Barbara Rita and Valtolina, Stefano and Marzullo, Matilde},
title = {ArchMatrix: A Visual Interactive System for Graph-Based Knowledge Exploration in Archaeology},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254684},
doi = {10.1145/2254556.2254684},
abstract = {This paper presents the design and development of ArchMatrix, a visual interactive system that supports archaeologists in archiving, managing and studying the findings collected during archaeological excavations. This work allowed to deepen the study of participatory design in interdisciplinary communities and was carried out by computer scientists in close collaboration with archaeologists. To overcome the several issues that characterize the communication among different domain experts, participatory design's principles have been applied. The ArchMatrix system implements the Harris Matrix method, used both in archaeology and in architecture to describe the position (absolute and relative) of stratigraphic units. ArchMatrix relies on a graph database that enables to apply advanced information retrieval strategies. In this way, archaeologists are supported in developing new opportunities for investigation to increase their knowledge, improve their traditional working practices and to develop new ones.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {681–684},
numpages = {4},
keywords = {knowledge management, interdisciplinary projects, visual interactive systems, archaeology, participatory design, Harris Matrix},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254685,
author = {Lin, Yi-Ling and Aroyo, Lora},
title = {Interactive Curating of User Tags for Audiovisual Archives},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254685},
doi = {10.1145/2254556.2254685},
abstract = {With the rapidly increasing popularity of social sharing sites, the traditional manual indexing techniques are no longer feasible to cope with the growing amount of multimedia content. Emerging folksonomies of user tags through crowdsourcing provide a potential for the collaborative annotation of various types of online multimedia resources. However, the shortcomings of folksonomies still present researchers with challenges to effectively use the collected user tags in professional or public collections. Examples of such challenges are determining how to tackle the quality of tags, to understand tags' meaning and relevance to the resource material, and to define quality parameters of the final (targeted) annotations of multimedia resources. This work addresses such challenges in a concrete use case -- the crowdsourcing video annotation game called Waisda?. This game is used to collect user tags for videos from the Dutch National Audiovisual Archive 'Sound and Vision'. In this paper we explore the interactive aspects of a post-game crowdsourcing tool called 'Tag Gardening' for curating user tags. We tackle the challenges of bringing out quality and extracting meaning from the user tags in order to finally achieve satisfactory video annotations.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {685–688},
numpages = {4},
keywords = {interactive interfaces, tag gardening, folksonomy, social tagging, cultural heritage},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254686,
author = {Chen, Xiang 'Anthony' and Boring, Sebastian and Carpendale, Sheelagh and Tang, Anthony and Greenberg, Saul},
title = {Spalendar: Visualizing a Group's Calendar Events over a Geographic Space on a Public Display},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254686},
doi = {10.1145/2254556.2254686},
abstract = {Portable paper calendars (i. e., day planners and organizers) have greatly influenced the design of group electronic calendars. Both use time units (hours/days/weeks/etc.) to organize visuals, with useful information (e.g., event types, locations, attendees) usually presented as - perhaps abbreviated or even hidden - text fields within those time units. The problem is that, for a group, this visual sorting of individual events into time buckets conveys only limited information about the social network of people. For example, people's whereabouts cannot be read 'at a glance' but require examining the text. Our goal is to explore an alternate visualization that can reflect and illustrate group members' calendar events. Our main idea is to display the group's calendar events as spatiotemporal activities occurring over a geographic space animated over time, all presented on a highly interactive public display. In particular, our Spalendar (Spatial Calendar) design animates people's past, present and forthcoming movements between event locations as well as their static locations. Detail of people's events, their movements and their locations is progressively revealed and controlled by the viewer's proximity to the display, their identity, and their gestural interactions with it, all of which are tracked by the public display.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {689–696},
numpages = {8},
keywords = {location, visualization, group, situated interaction, calendar},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254687,
author = {Mardell, James and Witkowski, Mark and Spence, Robert},
title = {An Interface for Visual Inspection Based on Image Segmentation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254687},
doi = {10.1145/2254556.2254687},
abstract = {If a person is lost in the wilderness it is increasingly normal for the area in question to be over-flown by an Unmanned Aerial Vehicle (UAV) whose on-board video camera transmits a view of the terrain below. It is then the task of a human operator to visually inspect that view by means of a visual interface specifically designed to enhance the likelihood of the missing person being located.We investigate a novel approach to the visual inspection of the terrain image: that of presenting small segments of that image for very short periods of time, though commensurate with the speed at which the UAV flies.Participants took part in an investigation in which the challenging task was to identify the presence, in typical terrain images, of human beings. Six representative terrain maps were involved, and the six degrees of segmentation explored were such as to provide individual terrain image viewing times between 3.9 s and 108 ms.We report the result of investigating the proposed segmentation approach to visual inspection in the demanding and realistic context of Wilderness Search and Rescue. Our investigation reveals a clear and distinctive change of visual search strategy as segmentation increases, equating to a shift between well-established notions of serial attentive search and parallel (pre-attentive) recognition.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {697–700},
numpages = {4},
keywords = {wilderness search and rescue, user study, presentation methods, rapid serial visual presentation, visual inspection, unmanned aerial vehicles},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254688,
author = {Romero, Rosa and D\'{\i}ez, David and Wittenburg, Kent and D\'{\i}az, Paloma},
title = {Envisioning Grid Vulnerabilities: Multi-Dimensional Visualization for Electrical Grid Planning},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254688},
doi = {10.1145/2254556.2254688},
abstract = {Electrical grid planning aims at optimizing the grid through the control of the performance and placement of electrical assets in order to minimize failures or vulnerabilities. With this purpose, grid planners carry out an initial stage of data exploration using a large volume of incident and equipment data collected over extensive time periods. In current practice these tasks are performed manually, which makes it very difficult to recognize patterns and gain insights into the data. In this paper, we propose a parallel multivariate visualization technique as a suitable approach for improving the existing practice. Based on the usage of an interactive visualization tool called BarExam, we demonstrate the feasibility of this visualization technique for displaying the dataset and present example insights that this visualization technique can provide to grid planners.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {701–704},
numpages = {4},
keywords = {electrical grid planning, set-valued attributes, multi-dimensional visualization, bargrams, information visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254689,
author = {Yoshikawa, Hiromi and Hachisu, Taku and Fukushima, Shogo and Furukawa, Masahiro and Kajimoto, Hiroyuki and Nojima, Takuya},
title = {Studies of Vection Field II: A Method for Generating Smooth Motion Pattern},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254689},
doi = {10.1145/2254556.2254689},
abstract = {Along public pathways, visual signs and audio cues are used by pedestrians to guide them into forming smoother pedestrian flows. However, often ignored or neglected, these signals require greater pedestrian attentiveness and appropriate conscious effort. To solve this problem, we have proposed the concept of "vection field". This is a field of optical flow that cues movement according to a pedestrian's motion. Visual stimulus within this optical flow leads pedestrians innately in specific directions without requiring direct interventions. We have implemented such a field by covering the ground with a lenticular lens screen; in this setup, neither power supply nor position tracking of pedestrians is necessary. An experimental result from our previous study shows that a vection field can direct pedestrians to one side. However, the quality of the optical flow such as image clarity and smoothness of motion was unsatisfactory in that it could cause a reduction in leading inducement. In this paper, we describe in detail a new display method involving a lenticular lens screen that yields an improvement in the quality of the vection field and ultimately pedestrian optical flow. Experiments showed improvements over previous attempts.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {705–708},
numpages = {4},
keywords = {traffic control, lenticular lens, vection},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254690,
author = {Bisson, Gilles and Blanch, Renaud},
title = {Stacked Trees: A New Hybrid Visualization Method},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254690},
doi = {10.1145/2254556.2254690},
abstract = {In this paper, we introduce a new Focus+Context visualization technique, named "Stacked Trees", allowing us to explore large dendrograms produced by hierarchical clustering. This approach displays up to fifty thousands nodes on a standard-sized screen.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {709–712},
numpages = {4},
keywords = {hierarchical clustering, large dendrograms, stacked trees},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254691,
author = {Neirynck, Thomas},
title = {Semantic Road Networks for Recommender Systems},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254691},
doi = {10.1145/2254556.2254691},
abstract = {In visualizations of non-spatial data, the distance similarity metaphor can rarely be preserved at all scales, inhibiting correct judgment of similarity and connectivity between features on a semantic map. The author shows that roads in geographic space are useful indicators of landscape topography, of connectivity and of importance of features across all scales. The author proposes a novel method to generate road-like networks on a Self Organizing Map and illustrates the use of such Semantic Road Networks in the context of a recommender system. The author argues that interactive maps of these Semantic Road Networks improve transparency of the system, and enable a new method of generating recommendations by traveling along routes on a semantic map.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {713–716},
numpages = {4},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254692,
author = {Cremonesi, Paolo and Epifania, Francesco and Garzotto, Franca},
title = {User Profiling vs. Accuracy in Recommender System User Experience},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254692},
doi = {10.1145/2254556.2254692},
abstract = {A Recommender System (RS) filters a large amount of information to identify the items that are likely to be more interesting and attractive to a user. Recommendations are inferred on the basis of different user profile characteristics, in most cases including explicit ratings on a sample of suggested elements. RS research highlights that profile length, i. e., the number of collected ratings, is positively correlated to the accuracy of recommendations, which is considered an important quality factor for RSs. Still, gathering ratings adds a burden on the user, which may negatively affect the UX. A design tension seems to exist, induced by two conflicting requirements -- to raise accuracy by increasing the profile length, and to make the profiling process smooth for the user by limiting the number of ratings. The paper presents a wide empirical study (1080 users involved) which explores this issue. Our work attempts to identify which of the two contrasting forces influenced by profile length -- recommendations accuracy and burden of the rating process - has stronger effects on the perceived quality of the UX with a RS.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {717–720},
numpages = {4},
keywords = {UX design, accuracy, recommender system, user experience quality, empirical study, profile length},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254693,
author = {Garzotto, Franca and Valoriani, Matteo},
title = {"Don't Touch the Oven": Motion-Based Touchless Interaction with Household Appliances},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254693},
doi = {10.1145/2254556.2254693},
abstract = {Motion-based touchless interaction empowers users to interact using movements and gestures, and without the burden of physical contact with technology (e.g., data gloves, body markers, or remote controllers). Most motion-based touchless applications are designed for interaction "in-the-large", where users engage with medium or large displays. Our research explores motion-based touchless interaction "in-the- small" that involves only small displays (of the size, for example, of smart phone screens). We have applied this novel paradigm to develop interactive applications for household appliances, discovering that the "in-the-small" feature raises a number of challenging design issues, exemplified in the paper through a case study.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {721–724},
numpages = {4},
keywords = {interaction design, household appliance, motion-based touchless interaction, home interaction, kinect},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254694,
author = {Lapides, Paul and Sultanum, Nicole and Sharlin, Ehud and Sousa, Mario Costa},
title = {Seamless Mixed Reality Tracking in Tabletop Reservoir Engineering Interaction},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254694},
doi = {10.1145/2254556.2254694},
abstract = {In this paper we present a novel mixed reality tracking system for collaborative tabletop applications that uses decorative markers and embedded application markers to create a continuous and seamless tracking space for mobile devices. Users can view and interact with mixed reality datasets on their mobile device, such as a tablet or smartphone, from distances both far and very near to the tabletop. We implement the tracking system in the context of a collaborative reservoir engineering tool that brings together many experts who need a private workspace to interact with unique datasets, which is supported by our system.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {725–728},
numpages = {4},
keywords = {mixed-reality, tabletop interaction, reservoir engineering},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254695,
author = {Lohmann, Steffen and D\'{\i}az, Paloma},
title = {Representing and Visualizing Folksonomies as Graphs: A Reference Model},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254695},
doi = {10.1145/2254556.2254695},
abstract = {We present a reference model for the representation and visualization of folksonomies as graphs. We discuss the formal representation of folksonomies and derive a hypergraph structure that is core to any visualization approach. We split this hypergraph into subgraphs that can guide the development of folksonomy visualizations. We use these subgraphs to classify existing graph visualizations of folksonomies from the literature and web. We found that most works display the interrelated set of tags, while the relations between resources and/or users are largely neglected by current visualization approaches. Based on these and further findings, we discuss research challenges and potentials for future graph visualizations of folksonomies.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {729–732},
numpages = {4},
keywords = {classification, representation, folksonomy, visualization, reference model, tagging, hypergraph},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254696,
author = {Pietrzak, Thomas and Malacria, Sylvain and Lecolinet, \'{E}ric},
title = {S-Notebook: Augmenting Mobile Devices with Interactive Paper for Data Management},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254696},
doi = {10.1145/2254556.2254696},
abstract = {This paper presents S-Notebook, a tool that makes it possible to "extend" mobile devices with augmented paper. Paper is used to overcome the physical limitations of mobile devices by offering additional space to annotate digital files and to easily create relationships between them. S-Notebook allows users to link paper annotations or drawings to anchors in digital files without having to learn pre-defined pen gestures. The systems stores meta data such as spatial or temporal location of anchors in the document as well as the zoom level of the view. Tapping on notes with the digital pen make appear the corresponding documents as displayed when the notes were taken. A given piece of augmented paper can contain notes associated to several documents, possibliy at several locations. The annotation space can thus serve as a simple way to relate various pieces of one or several digital documents between them. When the user shares his notes, the piece of paper becomes a tangible token that virtually contains digital information.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {733–736},
numpages = {4},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254697,
author = {Maurer, Max-Emanuel and Waxenberger, Rainer and Hausen, Doris},
title = {BroAuth: Evaluating Different Levels of Visual Feedback for 3D Gesture-Based Authentication},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254697},
doi = {10.1145/2254556.2254697},
abstract = {Using digital gadgets we authenticate ourselves regularly. Usually authentication relies on standard PIN or password but novel input hardware facilitates new authentication techniques. In this work we present an authentication mechanism based on body movements captured by a depth sensor. This idea is motivated by the cultural body movements used as welcoming gestures, especially by gang members (secret handshakes). Our authentication technique 'BroAuth' lets the user interact with a virtual partner to perform password input. This is done through touching target zones on the own body and on the body of a virtual partner.In this paper we focus on evaluating usability and security of onscreen feedback for such a system. Three different types of feedback were tested during the input process: Text-only (1D), abstract user representation (2D) and a virtual avatar (live 3D). The most detailed but most insecure 3D feedback performed much worse than the abstract input modalities. Input times and user opinions show that an abstract 2D representation is the best tradeoff between usability and security for such a system.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {737–740},
numpages = {4},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254698,
author = {Graham, Martin and Kennedy, Jessie and Paterson, Trevor and Law, Andy},
title = {Redeeming Pedigree Data with an Interactive Error Cleaning Visualisation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254698},
doi = {10.1145/2254556.2254698},
abstract = {We describe a visual data cleansing application for pedigree genotype data, which is used to redeem otherwise unusable pedigree data sets. Biologists and bioinformaticians dynamically and iteratively mask pieces of information from a dirty data set and graduate towards a usable cleaned version of the data, which can then be saved and used in ongoing biological analyses. Cleansing of such data is complicated over and above simple error cleaning in that change or masking of the pedigree structure may shift errors to new parts of the pedigree. Thus a branching history of data manipulations is kept to allow users to restore the data and visualisation to any of the previous states it has travelled through.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {741–744},
numpages = {4},
keywords = {data cleansing, data wrangling, pedigree visualization},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254699,
author = {da Rosa, Isaias Barreto and Lamas, David Ribeiro},
title = {Designing Mobile Access to DSpace-Based Digital Libraries},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254699},
doi = {10.1145/2254556.2254699},
abstract = {Developing countries face serious problems on building and using digital libraries (DL) due to low computer and Internet penetration rates, lack of financial resources, etc. Thus, since mobile phones are much more used than computers in these countries, they might be a good alternative for accessing DL. Moreover, in the developed world there has been an exponential growth on the usage of mobile phones for data traffic, establishing a good ground for accessing DL on mobile devices. This paper presents a design proposal for making DSpace-based digital libraries accessible on mobile phones. Since DSpace is a popular free and open source DL system used around the world, making it accessible through mobile devices might contribute for improving the global accessibility of scientific and academic publications.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {745–748},
numpages = {4},
keywords = {mobile digital libraries, DSpace, digital divide, developing countries},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254700,
author = {Jovanovic, Mladjan and Starcevic, Dusan and Jovanovic, Zoran},
title = {Formal Specification of Usability Measures in Model-Driven Development of Context-Sensitive User Interfaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254700},
doi = {10.1145/2254556.2254700},
abstract = {Developing usability studies to evaluate user interfaces (UIs) is a task requiring a wide variety of skills. In this paper we propose conceptual architecture for design and integration of usability tests in model-driven development of context-sensitive user interfaces. Upon the extended model for development of context-sensitive user interfaces, we introduce a generic framework that developers can use to design usability tests. This is enabled by using common vocabulary for description of usability test models and human models. Basic principles of the proposed approach are given in brief. As an example, we give formal model of human working memory that can be used in design of various cognitive load metrics.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {749–752},
numpages = {4},
keywords = {multimodal interaction, model-driven development, context-sensitive user interface, usability measures, human-centered design},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254701,
author = {Lohmann, Steffen and Burch, Michael and Schmauder, Hansj\"{o}rg and Weiskopf, Daniel},
title = {Visual Analysis of Microblog Content Using Time-Varying Co-Occurrence Highlighting in Tag Clouds},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254701},
doi = {10.1145/2254556.2254701},
abstract = {The vast amount of contents posted to microblogging services each day offers a rich source of information for analytical tasks. The aggregated posts provide a broad sense of the informal conversations complementing other media. However, analyzing the textual content is challenging due to its large volume, heterogeneity, and time-dependence. In this paper, we exploit the idea of tag clouds to visually analyze microblog content. As a major contribution, tag clouds are extended by an interactive visualization technique that we refer to as time-varying co-occurrence highlighting. It combines colored histograms with visual highlighting of co-occurrences, thus allowing for a time-dependent analysis of term relations. An example dataset of Twitter posts illustrates the applicability and usefulness of the approach.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {753–756},
numpages = {4},
keywords = {microposts, co-occurrence highlighting, microblogging, visual analysis, twitter, histogram, time-series visualization, tag cloud},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254702,
author = {Dumas, Bruno and Broch\'{e}, Tim and Hoste, Lode and Signer, Beat},
title = {ViDaX: An Interactive Semantic Data Visualisation and Exploration Tool},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254702},
doi = {10.1145/2254556.2254702},
abstract = {We present the Visual Data Explorer (ViDaX), a tool for visualising and exploring large RDF data sets. ViDaX enables the extraction of information from RDF data sources and offers functionality for the analysis of various data characteristics as well as the exploration of the corresponding ontology graph structure. In addition to some basic data mining features, our interactive semantic data visualisation and exploration tool offers various types of visualisations based on the type of data. In contrast to existing semantic data visualisation solutions, ViDaX also offers non-expert users the possibility to explore semantic data based on powerful automatic visualisation and interaction techniques without the need for any low-level programming. To illustrate some of ViDaX's functionality, we present a use case based on semantic data retrieved from DBpedia, a semantic version of the well-known Wikipedia online encyclopedia, which forms a major component of the emerging linked data initiative.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {757–760},
numpages = {4},
keywords = {data mining, RDF, visual data exploration, information visualisation},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254703,
author = {Guchev, Vladimir and Mecella, Massimo and Santucci, Giuseppe},
title = {Design Guidelines for Correlated Quantitative Data Visualizations},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254703},
doi = {10.1145/2254556.2254703},
abstract = {Various visualization techniques for correlated multiple quantitative data sets, which allow a researcher to explore a character of content, are fairly common tools in the visual analysis. The available methods and techniques for a displaying of different diagram types, are suitable for a partial solution to this kind of a practical problem. This paper represents an overview for some existing tools and techniques, regarding the quantitative data visualization. As a key part of the paper, the design guidelines for a visual construction of correlated quantitative data values, which are mostly applicable for interactive techniques, are presented.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {761–764},
numpages = {4},
keywords = {tree map, statistical graphics, mosaic plot, bar chart},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254704,
author = {Lambeck, Christian and Kammer, Dietrich and Weyprecht, Pascal and Groh, Rainer},
title = {Bridging the Gap: Advances in Interaction Design for Enterprise Applications in Production Scenarios},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254704},
doi = {10.1145/2254556.2254704},
abstract = {In Enterprise Information Systems, users are facing complex user interfaces with a multitude of functionalities. These interfaces still rely on the WIMP-paradigm including forms, tables and dashboards, which are meant to be used with mouse and keyboard. This contribution argues that there is a gap between the increased process complexity and the traditional concepts in visualization and interaction. In this paper, an exemplary scenario addresses these challenges by exploiting tangible interaction on a tabletop system.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {765–768},
numpages = {4},
keywords = {tabletop system, production planning and scheduling, tangible interaction, enterprise applications, user interface design, multi-touch},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254706,
author = {Peschke, Joshua and G\"{o}bel, Fabian and Gr\"{u}nder, Thomas and Keck, Mandy and Kammer, Dietrich and Groh, Rainer},
title = {DepthTouch: An Elastic Surface for Tangible Computing},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254706},
doi = {10.1145/2254556.2254706},
abstract = {In this paper we describe DepthTouch, an installation which explores future interactive surfaces and features elastic feedback, allowing the user to go deeper than with regular multi-touch surfaces. DepthTouch's elastic display allows the user to create valleys and ascending slopes by depressing or grabbing its textile surface. We describe the experimental approach for eliciting appropriate interaction metaphors from interaction with real materials and the resulting digital prototype.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {770–771},
numpages = {2},
keywords = {tangible computing, natural user interfaces, depth sensors},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254707,
author = {Gomi, Ai and Itoh, Takayuki and Zhang, Kang},
title = {Meal - a Menu Evaluation System with Symbolic Icons in Mobile Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254707},
doi = {10.1145/2254556.2254707},
abstract = {While cooking is a daily activity in our life, many family cooks feel bothered by deciding the menu each and every time because of being required to cook both tasty as well as well-balanced meals for their families. We have been developing a meal recording system, MEAL (Menu Evaluation and Attribute Log), which supports those cooks by easily managing and retrieving meal records on mobile platforms. MEAL displays automatically generated icons as composed representations of information based on food data, so users can easily identify each data item on a small screen. The interface has two main views, a retrieval view and an analyzing view for food records in the user's own food database. The retrieval view displays retrieval results with photos of food and symbolic icons. The analysis view supports visualizing all sets of food data represented by icons. It allows users to easily understand their cooking habits for their family. Since the information gets optimized, the symbolic icons lead to higher visibility on small screens.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {772–773},
numpages = {2},
keywords = {life-log, interaction styles, food, icons, mobile devices},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254708,
author = {Cheng, Kelvin and Li, Jane and M\"{u}ller-Tomfelde, Christian},
title = {Supporting Interaction and Collaboration on Large Displays Using Tablet Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254708},
doi = {10.1145/2254556.2254708},
abstract = {In this paper, we present an interaction technique that supports the use of tablet devices for interaction and collaboration with large displays. Users can interact with a subset of the large workspace on their tablet, while the same area is visualized on the large display as a rectangular frame. We propose an efficient management and navigation interface through the use of an interactive world-in-miniature view and multitouch gestures. Users can intuitively manage their views on their tablets, navigate between different areas of the workspace, or enlarge them for a closer look. Additionally, they can quickly jump to the same view that their collaborators are looking at.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {774–775},
numpages = {2},
keywords = {distant interaction, shared workspace, multitouch tablet device, collaboration},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254709,
author = {Magrini, Laura and Nati, Matteo and Panizzi, Emanuele},
title = {<i>RMob</i> - a Mobile App for Real Time Information in Urban Transportation},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254709},
doi = {10.1145/2254556.2254709},
abstract = {This paper describes the design and development of RMob, a mobile app for real time information in urban transportation. RMob provides fast, reliable and clear information about transportation in Rome, e.g. bus arrival times, urban travel times, bus stop maps, etc. People moving in large urban areas need to predict travel times and choose the fastest routes; they generally take real time decisions without planning ahead and are usually in hurry so they have limited time for interaction with the device and the application. This work aims to providing travellers in the urban Rome area with timely and accurate information, minimizing the interaction and the text input, so encouraging the best travel choice and the use of public transportation. Based on the different travel phases described in the literature, we propose a distinction on the nearby resources (bus stops, bus arrival times, parkings, car/bike sharing stations, etc.) and trip-destination resources (routes, travel times, parkings and stations at destination, etc.). The application was designed using this model that seems appropriate and usable according to our observations. RMob is published online for iOS and Android devices.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {776–777},
numpages = {2},
keywords = {iOS, infomobility, urban mobility, mobile interaction, Android, ITS},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254710,
author = {N\'{o}brega, Rui and Correia, Nuno and Nobre, Carlos and Teixeira, Ana B\'{a}rbara and Oliveira, Leonor and Da Silva, Raquel Henriques},
title = {Navigation in Past Museum Exhibitions Using Multimedia Archives},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254710},
doi = {10.1145/2254556.2254710},
abstract = {Many exhibitions that occurred in the past in art museums left a large amount of records (pictures, videos and texts), which are distributed over several documents or places. This paper proposes a multimedia system to reconstruct the heritage and space from such exhibitions. The virtual reconstruction is based on collecting and recovering the documents and making them accessible using an integrated interface. The evolution of image and photograph visualization allows the virtual reconstruction of physical spaces, offering a new dimension of interaction with the user. Through image analysis techniques, the spatial relation between each photo in the archive is accessed allowing the semi-automatic construction of a virtual artwork exhibition.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {778–779},
numpages = {2},
keywords = {image processing, cultural heritage, web interface},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254711,
author = {Bottoni, Paolo and Labella, Anna and Capuano, Daniele and Levialdi, Stefano and De Marsico, Maria},
title = {Experimenting Dele: A Deaf-Centered e-Learning Visual Environment},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254711},
doi = {10.1145/2254556.2254711},
abstract = {In the proposed demo, a new approach to e-learning environments for deaf people is illustrated. Using a fully iconic web-based environment, a tutor can define, generate and test e-learning courses for deaf people, which are automatically managed, published and served by the system itself.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {780–781},
numpages = {2},
keywords = {embodied cognition, deaf-centered e-learning environment, storytelling},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254712,
author = {Panizzi, Emanuele and Vitulli, Dario},
title = {IPhone Interface for Wireless Control of a Robot},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254712},
doi = {10.1145/2254556.2254712},
abstract = {The described project is an iPhone application to control a robot using the accelerometer and the touch screen of the mobile device. The iPhone and the robot are connected over WiFi. The application interface displays full screen real time images coming from the robot vision system, allowing remote driving of the robot. We implemented 2 different driving modes and interactions: the first one fully exploits the accelerometer to move the robot forward and backward and to change direction, the second one uses the interface buttons for forward and backward movements, while the accelerometer is used only to change direction. In both driving modes there is the possibility to control the robot's mechanical arm to take and release an object through two touchscreen buttons. We carried on several user tests in order to validate and enhance our design. In fact, based on the test results, we could improve the interface as well as the driving experience (e.g. we could tune the correct power to drive the gearmotors of the robot as a function of the device inclination and the pressing duration of the interface controls). Users that tested the final version could smoothly drive the robot along a route with obstacles.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {782–783},
numpages = {2},
keywords = {iOS, mobile interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254713,
author = {Gatto, Ivano and Pittarello, Fabio},
title = {Prototyping a Gestural Interface for Selecting and Buying Goods in a Public Environment},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254713},
doi = {10.1145/2254556.2254713},
abstract = {The study was realized in the context of a project aimed at transferring University research results to industry and started from a request of designing an engaging gesture-based interface for permitting people to buy goods in a public environment. The project was an occasion for focusing on several issues related to gestural interfaces, that are becoming popular but often suffer from poor usability. The solutions adopted for designing the prototype reflect some of the most interesting trends in research related to gestural interfaces and ubiquitous computing. An evaluation with a group of users gave us interesting feedbacks for improving our work.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {784–785},
numpages = {2},
keywords = {proxemic interaction, gestural interface, usability},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254714,
author = {Lupo, Luca and Malizia, Alessio and Diaz, Paloma and Aedo, Ignacio},
title = {SocialStory: A Social Storyboard System for Sharing Experiences in Emergencies},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254714},
doi = {10.1145/2254556.2254714},
abstract = {During and after crisis situations people are interested in getting knowledge about what happened and in obtaining information regarding damages, victims, and possible repercussions. Social network systems represent powerful sources due to their rapid growth in the last few years and the amount of continuous updated information published by people. Studies confirmed that people are interested in reporting facts during hazards, however due to the wide distribution of information and the lack of mechanisms to re-use it, it is an awkward task to be accomplished. For the above reasons in this paper we present SocialStory, a social platform for helping people to retrieve and filter materials available on the Internet and to compose and share personal experiences about crisis situations.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {786–787},
numpages = {2},
keywords = {storytelling, collaborative systems, social networks, social media, crisis situations},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254715,
author = {Benin, Alberto and Cosi, Piero and Leone, G. Riccardo},
title = {A WebGL Talking Head for Mobile Devices},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254715},
doi = {10.1145/2254556.2254715},
abstract = {Luciaweb is a 3D Italian talking avatar based on the new WebGL technology. WebGL is the standard programming library to develop 3D computer graphics inside the web browsers. In the last year we developed a facial animation system based on this library to interact with the user in a bimodal way. The overall system is a client-server application using the http protocol: we have a client (a browser or an app) and a web server. No software download and no plugin are required. All the software reside on the server and the visualization player is delivered inside the html pages that the client ask at the beginning of the connection. On the server side a software called AudioVideo Engine generates the phonemes and visemes information needed for the animation. The demo called Emotional Parrot shows the ability to reproduce the same input in different emotional states. This is the first WebGL software running on iOS device ever.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {788–789},
numpages = {2},
keywords = {talking head, WebGL, mobile, speech synthesis, MPEG-4, iOS},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254716,
author = {Brade, Marius and Br\"{a}ndel, Christian and Groh, Rainer},
title = {BrainDump: Taking Fluid Interaction Literally},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254716},
doi = {10.1145/2254556.2254716},
abstract = {In this paper a new visual interface tool, enabling knowledge workers to use a highly flexible visual map to represent and refine their current understanding of a task, is proposed. For demonstration purposes, a web information gathering task is used, but the concept is not limited to web tasks only. For the proposed visualization a metaphor based on fluids, cell structure and soap bubbles is derived from experiments with natural physical substances. The resulting visual interface allows the user to pin down associations and to clarify anticipations of relations visually.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {790–791},
numpages = {2},
keywords = {knowledge creation, visual sensemaking, abstract knowledge representation, interactive visualization, fluid interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254717,
author = {Takahashi, Nobuhiro and Matoba, Yasushi and Sato, Toshiki and Koike, Hideki},
title = {SHIRI: Buttocks Humanoid That Represents Emotions with Visual and Tactual Transformation of the Muscles},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254717},
doi = {10.1145/2254556.2254717},
abstract = {In this paper, we propose a novel interface design for "human-robot" communication by focusing on visual and tactual transformation of the muscles. Since recent humanoids may appear as humanoid figures using human-like body gestures and behavior, it is hard to say that they have enough elements to cover the complex composition that is a human. The muscles that constitute the human body work by not only turning joints and generating limb and body movements, but also control skin surface shape and firmness, allowing the various levels of touch response. Therefore, we attempt to approach the creation of sensitive and subtle expression by a humanoid robot using organic constructs. In this project, we produce "SHIRI", which represents emotions with organic movements of the Gluteus Maximus Actuator (GMA). In addition, we also implement user interaction for SHIRI and consider how perceptions the user can obtain by communicating with SHIRI.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {792–793},
numpages = {2},
keywords = {input device, human-robot interaction, humanoid robotics, emotion regulation, behavior recognition},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254718,
author = {Pittarello, Fabio and Gatto, Ivano},
title = {An Integrated Ecosystem of Interfaces for Annotating, Querying and Browsing Networks of Web3D Worlds},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254718},
doi = {10.1145/2254556.2254718},
abstract = {This demo paper presents the set of visual interfaces designed by the authors in previous works as components of an ecosystem meant to accumulate and maximize the exploitation of annotated 3D information for the web. In particular the paper shows, through a case study related to cultural heritage, how the users can take advantage of this ecosystem of interfaces to shift seamlessly among different activities of searching, browsing and annotating a network of 3D worlds in the same work session. Such interweaving of activities characterizes the hypertextual web 2.0 but it is a novelty for the 3D domain and we expect that it may bring similar benefits in terms of information finding, reducing dead-ends during search and browsing, stimulating at the same time the users to contribute to the accumulation of further knowledge about the 3D entities stored.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {794–795},
numpages = {2},
keywords = {information finding, ontology, social web, cultural heritage, annotation, folksonomy, browsing, Web3D, search},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254719,
author = {Sato, Toshiki and Takahashi, Nobuhiro and Matoba, Yasushi and Koike, Hideki},
title = {Interactive Surface That Have Dynamic Softness Control},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254719},
doi = {10.1145/2254556.2254719},
abstract = {In the field of interface surface research, the idea of the 'softness' of a surface medium is one significant factor in determining a suitable means of interaction with the user. With direct touch input, for example, the degree of surface softness allows for the generation various touch sensations and tactile feedback. Additionally, the softness also affects the shape of the surface: a soft surface will allow the user to deform the surface at will while a hard surface will maintain its shape easier. In many traditional flexible surfaces to date, this element has been considered static and thus unchangeable. This project, in contrast, considers the softness of a surface to be dynamic and thus further explores the interaction possibilities with this type of surface. We demonstrate the possibilities of dynamically changing surfaces and their derived user interaction.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {796–797},
numpages = {2},
keywords = {tabletop, interactive surface, dynamic softness control, input devices and strategies},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254720,
author = {Onishi, Tomoya and Tokuami, Ryosuke and Kono, Yasuyuki and Nakamura, Satoshi},
title = {Personal Photo Browser That Can Classify Photos by Participants and Situations},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254720},
doi = {10.1145/2254556.2254720},
abstract = {This paper demonstrates a photo browser which rearranges photos referring to the persons who were close to the photographer when the photos were taken by consulting Bluetooth device detection information. Most of Bluetooth devices accompany their owners. Each photo is tagged with Bluetooth device-IDs which were detected around the moment when it was taken. Employing the tag information, the system classifies the user's photo archive into a layered cluster tree in terms of tag similarity, and shows its user the photos of her-selected cluster on either a map or timelines.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {798–799},
numpages = {2},
keywords = {digital photo browser, tagging, Bluetooth, life log},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254721,
author = {De Gregorio, Massimo and Rullo, Alessandro and Rubinacci, Salvatore},
title = {ISIDIS: An Intelligent Videosurveillance System},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254721},
doi = {10.1145/2254556.2254721},
abstract = {In this paper we propose a new approach to active video surveillance intelligence systems based on the integration of artificial neural networks (ANN) and symbolic Artificial Intelligence (AI). In particular, the neurosymbolic hybrid system here presented is formed by virtual neural sensors (WiSARD--like systems) and BDI agents. The coupling of virtual neural sensors with symbolic reasoning for interpreting their outputs, makes this approach both very light from the computational and hardware point of view and quite robust in performances.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {800–801},
numpages = {2},
keywords = {BDI agents, hybrid intelligent systems, weightless systems},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254722,
author = {Korozi, Maria and Leonidis, Asterios and Margetis, George and Koutlemanis, Panagiotis and Zabulis, Xenophon and Antona, Margherita and Stephanidis, Constantine},
title = {Ambient Educational Mini-Games},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254722},
doi = {10.1145/2254556.2254722},
abstract = {This system paper describes two educational mini-games (a multiple-choice quiz and a geography-related game) that combine learning and ambient technology. Their innovative feature is that they offer physical interaction through printed cards on a tabletop setup, where a simple webcam monitors the table's surface and identifies the thrown cards. Following a brief discussion of ambient technology integration in the environment, the overall concept of these games is described and potential future improvements are outlined.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {802–803},
numpages = {2},
keywords = {educational games, ambient intelligence, physical interaction},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254723,
author = {Vrotsou, Katerina and Zhi, Haolin and Peca, Iulian and Andrienko, Gennady and Andrienko, Natalia},
title = {Interactive Exploration of Events and Presence of People in Space and Time through KD-Photomap},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254723},
doi = {10.1145/2254556.2254723},
abstract = {We explore people's activities in space and time by analysing publicly available georefenced photographs. We do this using KD-photomap, a web-based visual analytics system for exploring collections of Flickr photographs and meta-data associated with them. The system provides an interface for flexible browsing of photographs in search of interesting pictures, and places, and also a framework for exploration of presence and identification of events in space and time.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {804–805},
numpages = {2},
keywords = {georeferenced data, interactive exploration, visual analytics},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254725,
author = {Mehandjiev, Nikolay and De Angeli, Antonella},
title = {Challenges in End User Development for Services},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254725},
doi = {10.1145/2254556.2254725},
abstract = {The Third International Workshop on end user development for services (EUD4Services) focuses on the challenges faced by researchers and practitioners in applying End User Development ideas and principles to the area of service-oriented software design and construction. The topic is a natural development following on from two previous workshops which charted the territory of existing tools and research systems, and created a community of researchers and practitioners working in the area. This year edition aims to define a shared research agenda which can direct future work acting as a catalyst of joint effort towards the establishment of EUD for services.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {807–808},
numpages = {2},
keywords = {component-based systems, service-oriented architectures, end-user composition, web services, end-user development, annotations, meta-design, service composition approaches},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254726,
author = {Riche, Nathalie Henry and Inkpen, Kori and Stasko, John and Gross, Tom and Czerwinski, Mary},
title = {Supporting Asynchronous Collaboration in Visual Analytics Systems},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254726},
doi = {10.1145/2254556.2254726},
abstract = {Visual analytics involves complex analytical processes that can often benefit from collaboration. Many researchers have explored co-located synchronous systems to help support collaborative visual analytics; however, the process can often be long and require a series of sessions. Providing support for asynchronous collaboration in visual analytics systems can help divide the problem between several analysts across many sessions to ensure that they can effectively work together toward a solution. Currently, visual analytics systems offer limited support for asynchronous, multi-session work [1]. In this workshop, we seek to bring together researchers from both the CSCW and Visual Analytics communities to discuss avenues for supporting asynchronous collaboration in visual analytics system.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {809–811},
numpages = {3},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254727,
author = {Spampinato, Concetto and Boom, Bas and He, Jiyin},
title = {First International Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254727},
doi = {10.1145/2254556.2254727},
abstract = {The goal of the First International Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications is to bring together practitioners and researchers in computer vision and in HCI to share ideas and experiences in designing and implementing visual interfaces for ground truth data generation.It specifically presents and reports on the construction and analysis of user-oriented tools and interfaces to support automatic or semi-automatic ground truth annotation and labeling in many applications such as object detection, object recognition, scene segmentation and face recognition both in still images and in videos.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {812–814},
numpages = {3},
keywords = {pattern recognition, object detection, ground truth data, image labeling, collaborative interfaces, visual interfaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254728,
author = {Quigley, Aaron and Dix, Alan and Nacenta, Miguel and Rodden, Tom},
title = {Workshop on Infrastructure and Design Challenges of Coupled Display Visual Interfaces: In Conjunction with Advanced Visual Interfaces 2012 (AVI'12)},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254728},
doi = {10.1145/2254556.2254728},
abstract = {An increasing number of interactive displays of very different sizes, portability, projectability and form factors are starting to become part of the display ecosystems that we make use of in our daily lives. Displays are shaped by human activity into an ecological arrangement and thus an ecology. Each combination or ecology of displays offer substantial promise for the creation of applications that effectively take advantage of the wide range of input, affordances, and output capability of these multi-display, multi-device and multi-user environments. Although the last few years have seen an increasing amount of research in this area, knowledge about this subject remains under explored, fragmented, and cuts across a set of related but heterogeneous issues. This workshop brings together researchers and practitioners interested in the challenges posed by infrastructure and design.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {815–817},
numpages = {3},
keywords = {public displays, MDE infrastructure, interaction techniques, display ecology, multi-display user interfaces, ubiquitous computing, coupled displays, distributed user interfaces, multi-display environments},
location = {Capri Island, Italy},
series = {AVI '12}
}

@inproceedings{10.1145/2254556.2254729,
author = {Jetter, Hans-Christian and Geyer, Florian and Reiterer, Harald and Dachselt, Raimund and Fischer, Gerhard and Groh, Rainer and Haller, Michael and Herrmann, Thomas},
title = {Designing Collaborative Interactive Spaces},
year = {2012},
isbn = {9781450312875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254556.2254729},
doi = {10.1145/2254556.2254729},
abstract = {Interactive spaces are ubiquitous computing environments for computer-supported collaboration that exploit and enhance the existing cognitive, physical and social skills of users or groups of users. The workshop aims at documenting and advancing the current state-of-the-art of co-located collaboration in interactive spaces and identifying research challenges and formulating a research agenda by inviting high-quality position and research papers from HCI, Information Visualization, CSCW and CSCL.},
booktitle = {Proceedings of the International Working Conference on Advanced Visual Interfaces},
pages = {818–820},
numpages = {3},
keywords = {interactive surfaces, collaboration, ubiquitous computing, tangible interfaces, interactive spaces},
location = {Capri Island, Italy},
series = {AVI '12}
}

