@inproceedings{10.1145/1871840.1871841,
author = {Goebel, Randy and Bergsma, Shane and Xu, Ying and Ringlstetter, Christoph and Kim, Mi-Young},
title = {The Nature of Noise in Linguistic Corpora},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871841},
doi = {10.1145/1871840.1871841},
abstract = { 'For the past 4-5 years, we've been investigating a variety of computational methods for extracting linguistic structures from relatively large language corpora. These include the use of well-known standard labeled language resources such as those from the Linguistic Data Consortium, as well as a spectrum of unlabeled resources, including the Google n-gram repository and a variety of more specific search engine query and answer resources (e.g., from Sogou).Our specific goals in these investigations also vary, ranging from the induction of "tightness" of compound terms (e.g., the degree to which a collection of terms is compositional), spelling correction, extraction of concepts, methods for predicting nominal gender, and methods for resolving a variety of anaphora. In this spectrum of computational linguistic research, we can identify an emergent perspective on the nature of "noise" in both labeled and unlabeled corpora.Overall, we have found that there are many notions of "noise" across the spectrum of digital resources; our presentation attempts to summarize our observations on that structure. A basic premise to be elaborated is that there are at least two important notions of linguistic noise, including that arising from language use and variance in everything from spelling, grammar, and style in base language use, to what we might call the variance on linguistic structures arising from machine learning methods applied to a succession of labeled linguistic resources. A simple example is the use of any labeled data in subsequent structure induction, e.g., "golden" standards for tokenization or ontology tagging, subsequently used to construct labeled refinements for use in anaphora resolution or ontology matching.If there is value in our summary, it lies in helping to identify a framework for understanding a variety of distinct noise characteristics, which we hope will help reduce the compound impact of noise types that obtain in the application of learning methods within computational linguistics studies},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {1–2},
numpages = {2},
keywords = {natural language processing machine learning},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871843,
author = {D\'{e}jean, Herv\'{e} and Meunier, Jean-Luc},
title = {Document: A Useful Level for Facing Noisy Data},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871843},
doi = {10.1145/1871840.1871843},
abstract = {In this paper we will present a set of experiments using large digitalized collections of books to show that logical structures can be extracted with a good quality when working at document level. The proposed solution relies on a twofold method: first specific logical elements are recognized by a given method. Then models for the recognized elements are generated by combining layout, content and labeling information. Model inference is made possible at document level, a level which promotes frequent occurrences of document structures. These inferred models combining several kinds of information are used to correct noisy data, typically zoning, OCR and labeling errors produced by previous processing steps. This method is illustrated by the detection of two document structures: page numbers and chapter headings, two navigating elements required by digital libraries.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {3–10},
numpages = {8},
keywords = {model, error correction, logical analysis},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871844,
author = {Lamiroy, Bart and Lopresti, Daniel},
title = {A Platform for Storing, Visualizing, and Interpreting Collections of Noisy Documents},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871844},
doi = {10.1145/1871840.1871844},
abstract = {The goal of document image analysis is to produce interpretations that match those of a fluent and knowledgeable human when viewing the same input. Because computer vision techniques are not perfect, the text that results when processing scanned pages is frequently noisy. Building on previous work, we propose a new paradigm for handling the inevitable incomplete, partial, erroneous, or slightly orthogonal interpretations that commonly arise in document datasets. Starting from the observation that interpretations are dependent on application context or user viewpoint, we describe a platform now under development that is capable of managing multiple interpretations for a document and offers an unprecedented level of interaction so that users can freely build upon, extend, or correct existing interpretations. In this way, the system supports the creation of a continuously expanding and improving document analysis repository which can be used to support research in the field.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {11–18},
numpages = {8},
keywords = {interpretation, repository, ground-truth},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871845,
author = {Packer, Thomas L. and Lutes, Joshua F. and Stewart, Aaron P. and Embley, David W. and Ringger, Eric K. and Seppi, Kevin D. and Jensen, Lee S.},
title = {Extracting Person Names from Diverse and Noisy OCR Text},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871845},
doi = {10.1145/1871840.1871845},
abstract = {Named entity recognition applied to scanned and OCRed historical documents can contribute to the discoverability of historical information. However, entity recognition from some historical corpora is much more difficult than from natively digital text because of the marked presence of word errors and absence of page layout information. How difficult can it be and what level of quality can be expected? We apply three typical extraction algorithms to the task of extracting person names from multiple types of noisy OCR documents found in the collection of a major genealogy content provider and compare their performance using a number of quality metrics. We also show an improvement in extraction quality using a majority-vote ensemble of the three extractors. We evaluate the extraction quality with respect to two references: what a human can manually extract from OCR output and from the original document images. We illustrate the challenges and opportunities at hand for extracting names from OCRed data and identify directions for further improvement.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {19–26},
numpages = {8},
keywords = {CRF, noisy OCR, information extraction, named entity recognition, MEMM, scanned document images, NER},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871846,
author = {Futagi, Yoko},
title = {The Effects of Learner Errors on the Development of a Collocation Detection Tool},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871846},
doi = {10.1145/1871840.1871846},
abstract = {Texts produced by language learners often contain a fair amount of noise, such as misspellings, grammar errors and word-choice errors. These pose a challenge to designing an automated tool to process these texts, partly because most existing tools used in text processing preliminary to linguistic analysis, such as POS-tagging and syntactic parsing, are trained on native-speaker data from which errors have been edited out, and are not designed to deal with atypical errors produced by language learners. In designing and implementing an NLP or a computer-assisted language learning (CALL) tool, determining which "non-pertinent" errors (i.e. errors not specifically targeted by the tool) to deal with and how exactly to deal with them can have a measurable impact on the tool performance. This paper discusses how dealing with some of the "non-pertinent" learner errors in the development of an automated tool to detect miscollocations in learner texts significantly reduces potential tool errors.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {27–34},
numpages = {8},
keywords = {collocation detection, computer-assisted language learning, learner errors, learner text},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871847,
author = {Patrick, Jon David and Asgari, Pooyan and Motamedi, Negin},
title = {Improving Accuracy of Identifying Clinical Concepts in Noisy Unstructured Clinical Notes Using Existing Internal Redundancy},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871847},
doi = {10.1145/1871840.1871847},
abstract = {This paper, reports on the results of research which is based originally on the 2009 criteria and corpus of "The Obesity Challenge", defined by Informatics for Integrating Biology and the Bedside (i2b2), a National Center for Biomedical Computing. In the original task, i2b2 asked participants to build software systems that could process a corpus of noisy patient's clinical discharge summaries and report on patients' condition. The ultimate aim was to compare the judgments of physicians in evaluating the patient condition to a machine performance over such a corpus..The authors used a collection of resources to lexically and semantically characterize the diseases and their associated signs, symptoms. In this approach, they combined dictionary look-up, rule-based, and machine-learning methods along with adopting a special internal redundancy algorithm to reduce the usage of customized rules and increase the consistency of the performance over various types of noisy corpora. The performance was strengthened by information extracted from the patient notes via an internal redundancy module to overcome False Positives (FPs) and False Negatives (FNs) arising from the noisy nature of corpus.The methods were applied to a collection of 507 previously unseen noisy patient discharge summaries, and the Judgments were evaluated against a manually provided gold standard. The overall ranking of the participating Research groups were primarily based on the macro-averaged F-measure over 16 Classes of diseases.The implemented method achieved the micro-averaged F-measure of 96.9% (ranked within the top 7 out of 28 research groups) where there was no statistical significant difference between top 7 teams in micro F-measure. The highest F-Measure was 97.2%.The performance achieved was in line with the agreement between human annotators, indicating the potential of text mining for accurate and efficient prediction of disease status from clinical discharge summaries. Comparison of the results of this approach to results of other submitted classical approaches also showed adopting the internal redundancy algorithm for clinical domains can boost the accuracy of classifiers without extensive usage of rules and customization and therefore has potential for a more consistent performance and more efficient processing over various type of noisy corpora.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {35–42},
numpages = {8},
keywords = {internal redundancy, unstructured text, natural language processing, classification, noisy corpus, health informatics, concept matching},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871848,
author = {Babbar, Rohit and Singh, Nidhi},
title = {Clustering Based Approach to Learning Regular Expressions over Large Alphabet for Noisy Unstructured Text},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871848},
doi = {10.1145/1871840.1871848},
abstract = {Regular Expressions have been used for Information Extraction tasks in a variety of domains. The alphabet of the regular expression can either be the relevant tokens corresponding to the entity of interest or individual characters in which case the alphabet size becomes very large. The presence of noise in unstructured text documents along with increased alphabet size of the regular expressions poses a significant challenge for entity extraction tasks, and also for algorithmically learning complex regular expressions. In this paper, we present a novel algorithm for regular expression learning which clusters similar matches to obtain the corresponding regular expressions, identifies and eliminates noisy clusters, and finally uses weighted disjunction of the most promising candidate regular expressions to obtain the final expression. The experimental results demonstrate high value of both precision and recall of this final expression, which reinforces the applicability of our approach in entity extraction tasks of practical importance.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {43–50},
numpages = {8},
keywords = {clustering in noisy text, regular expression learning, rule-based information extraction},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871849,
author = {Fayolle, Julien and Moreau, Fabienne and Raymond, Christian and Gravier, Guillaume},
title = {Reshaping Automatic Speech Transcripts for Robust High-Level Spoken Document Analysis},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871849},
doi = {10.1145/1871840.1871849},
abstract = {High-level spoken document analysis is required in many applications seeking access to the semantic content of audio data, such as information retrieval, machine translation or automatic summarization. It is nevertheless a difficult task that is generally based on transcripts provided by an automatic speech recognition system. Unlike standard texts, transcripts belong to the category of highly noisy data because of word recognition errors that affect, in particular, very significant words such as named entities (e.g. person's names, locations, organizations). Transcripts also contain specificities of spoken language that make ineffective their processing by natural language processing tools designed for texts. To overcome these issues, this paper proposes a method to reshape automatic speech transcripts for robust high-level spoken document analysis. The method consists in conceiving a new word-level confidence measure that may efficiently ensure the reliability of transcribed words, focusing on words that are relevant for high-level spoken document analysis such as named entities. The approach consists in combining different features collected from various sources of knowledge thanks to a machine learning method based on conditional random fields. In addition to standard features (morphosyntactic, linguistic and phonetic), we introduce new semantic features based on the decisions of three robust named entity recognition systems to better estimate the reliability of named entities. Experiments, conducted on the French broadcast news corpus ESTER, demonstrate the added-value of the proposed word-level confidence measure for error detection and named entity recognition, with respect to the basic confidence measure provided by an automatic speech recognition system.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {51–58},
numpages = {8},
keywords = {word-level confidence measures, machine learning, named entities, noisy transcription, automatic speech recognition system, spoken language processing, feature combination},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871850,
author = {Murakami, Koji and Nichols, Eric and Mizuno, Junta and Watanabe, Yotaro and Masuda, Shouko and Goto, Hayato and Ohki, Megumi and Sao, Chitose and Matsuyoshi, Suguru and Inui, Kentaro and Matsumoto, Yuji},
title = {Statement Map: Reducing Web Information Credibility Noise through Opinion Classification},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871850},
doi = {10.1145/1871840.1871850},
abstract = {On the Internet, users often encounter noise in the form of spelling errors or unknown words, however, dishonest, unreliable, or biased information also acts as noise that makes it difficult to find credible sources of information. As people come to rely on the Internet for more and more information, reducing this credibility noise grows ever more urgent. The STATEMENT MAP project's goal is to help Internet users evaluate the credibility of information sources by mining the Web for a variety of viewpoints on their topics of interest and presenting them to users together with supporting evidence in a way that makes it clear how they are related.In this paper, we show how a STATEMENT MAP system can be constructed by combining Information Retrieval (IR) and Natural Language Processing (NLP) technologies, focusing on the task of organizing statements retrieved from the Web by viewpoints. We frame this as a semantic relation classification task, and identify 4 semantic relations: [AGREEMENT], [CONFLICT], [CONFINEMENT], and [EVIDENCE]. The former two relations are identified by measuring semantic similarity through sentence alignment, while the latter two are identified through sentence-internal discourse processing. As a prelude to end-to-end user evaluation of STATEMENT MAP, we present a large-scale evaluation of semantic relation classification between user queries and Internet texts in Japanese and conduct detailed error analysis to identify the remaining areas of improvement.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {59–66},
numpages = {8},
keywords = {discourse processing, credibility analysis, semantic relation classification, STATEMENT MAP, structural alignment, opinion classification},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871851,
author = {Langer, Akhil and Banga, Rohit and Mittal, Ankush and Subramaniam, L. Venkata},
title = {Variant Search and Syntactic Tree Similarity Based Approach to Retrieve Matching Questions for SMS Queries},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871851},
doi = {10.1145/1871840.1871851},
abstract = {Community based Question Answering archives have emerged as a very useful resource for instant access to comprehensive information in response to user queries. However, its access remains restricted to internet users. Access to this resource through Short Message Service (SMS) requires that a high precision automatic similar question matching system be built in order to decrease the search time by decreasing the number of SMS exchanges required. This paper proposes a solution that handles inherent noise in SMS queries through variant search, modeling the problem as one of combinatorial search. Following this, it uses syntactic tree matching to improve the ranking scheme. We present our analysis of the system and conduct experiments to test its feasibility. Experiments show that our approach outperforms the existing approaches significantly.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {67–72},
numpages = {6},
keywords = {similarity scores, syntactic structure, noisy text, SMS queries, question answering, question matching},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871852,
author = {Michelson, Matthew and Macskassy, Sofus A.},
title = {Discovering Users' Topics of Interest on Twitter: A First Look},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871852},
doi = {10.1145/1871840.1871852},
abstract = {Twitter, a micro-blogging service, provides users with a framework for writing brief, often-noisy postings about their lives. These posts are called "Tweets." In this paper we present early results on discovering Twitter users' topics of interest by examining the entities they mention in their Tweets. Our approach leverages a knowledge base to disambiguate and categorize the entities in the Tweets. We then develop a "topic profile," which characterizes users' topics of interest, by discerning which categories appear frequently and cover the entities. We demonstrate that even in this early work we are able to successfully discover the main topics of interest for the users in our study.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {73–80},
numpages = {8},
keywords = {user interests, Twitter},
location = {Toronto, ON, Canada},
series = {AND '10}
}

@inproceedings{10.1145/1871840.1871853,
author = {Laboreiro, Gustavo and Sarmento, Lu\'{\i}s and Teixeira, Jorge and Oliveira, Eug\'{e}nio},
title = {Tokenizing Micro-Blogging Messages Using a Text Classification Approach},
year = {2010},
isbn = {9781450303767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871840.1871853},
doi = {10.1145/1871840.1871853},
abstract = {The automatic processing of microblogging messages may be problematic, even in the case of very elementary operations such as tokenization. The problems arise from the use of non-standard language, including media-specific words (e.g. "2day", "gr8", "tl;dr", "loool"), emoticons (e.g. "(\`{o}_\'{o})", "(=^-^=)"), non-standard letter casing (e.g. "dr. Fred") and unusual punctuation (e.g. ".... ..", "!??!!!?", ",,,"). Additionally, spelling errors are abundant (e.g. "I;m"), and we can frequently find more than one language (with different tokenization requirements) in the same short message. For being efficient in such environment, manually-developed rule-based tokenizer systems have to deal with many conditions and exceptions, which makes them difficult to build and maintain. We present a text classification approach for tokenizing Twitter messages, which address complex cases successfully and which is relatively simple to set up and maintain. For that, we created a corpus consisting of 2500 manually tokenized Twitter messages -- a task that is simple for human annotators -- and we trained an SVM classifier for separating tokens at certain discontinuity characters. For comparison, we created a baseline rule-based system designed specifically for dealing with typical problematic situations. Results show that we can achieve F-measures of 96% with the classification-based approach, much above the performance obtained by the baseline rule-based tokenizer (85%). Also, subsequent analysis allowed us to identify typical tokenization errors, which we show that can be partially solved by adding some additional descriptive examples to the training corpus and re-training the classifier.},
booktitle = {Proceedings of the Fourth Workshop on Analytics for Noisy Unstructured Text Data},
pages = {81–88},
numpages = {8},
keywords = {tokenization, corpus, micro-blogging, twitter, text pre-processing, user-generated content},
location = {Toronto, ON, Canada},
series = {AND '10}
}

