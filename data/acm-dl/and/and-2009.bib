@inproceedings{10.1145/1568296.1568298,
author = {Balk, Hildelies},
title = {Poor Access to Digitised Historical Texts: The Solutions of the IMPACT Project},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568298},
doi = {10.1145/1568296.1568298},
abstract = {While there is an increasing demand for digitally available material (text that is not digital is becoming virtually invisible), digitised material is becoming available too slowly and in too small quantities. And even if the material is digitised, the OCR (optical character recognition) technology does often not produce satisfactory results, especially for historical documents. This is due to various problems such as historic fonts, complex layouts, ink shining through and historical spelling variants.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {1},
numpages = {1},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568300,
author = {Govindaraju, Venu and Cao, Huaigu and Bhardwaj, Anurag},
title = {Handwritten Document Retrieval Strategies},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568300},
doi = {10.1145/1568296.1568300},
abstract = {With the continuous growth of the World Wide Web, there is an urgent need for an efficient information retrieval system which can search and retrieve handwritten documents when presented with user queries. However, unconstrained handwriting recognition remains to be a challenging task with inadequate performance (around 30%, accuracy) thus proving to be a major hurdle in providing a robust search experience in the domain of handwritten documents. In this paper, we describe our recent research with focus on information retrieval from noisy text output by imperfect recognizers applied to handwritten document images. We describe three techniques each exploring a different approach for solving the noisy text retrieval task. The first method uses a novel bootstrapping mechanism to refine the OCR'ed text and uses the cleaned text for retrieval. The second method uses the uncorrected or raw OCR'ed text but modifies the standard vector space model for handling noisy text issues. The third method employs robust image features to index the documents instead of using noisy OCR'ed text. We describe these approaches in detail and also present their performance using standard IR evaluation metrics.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {3–7},
numpages = {5},
keywords = {OCR correction, handwriting analysis, keyword spotting, information retrieval},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568301,
author = {Lopresti, Daniel and Nagy, George},
title = {Tools for Monitoring, Visualizing, and Refining Collections of Noisy Documents},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568301},
doi = {10.1145/1568296.1568301},
abstract = {Developing better systems for document image analysis requires understanding errors, their sources, and their effects. The interactions between various processing steps are complex, with details that can be obscured by the statistical methods that are employed in many cases. In this paper, we describe tools we are building to help the user view and understand the results of common document analysis procedures. Unlike existing platforms for ground-truthing page images, our system also allows users to visualize the results of automated error analyses. Recognition errors can be corrected interactively, with the effort to do so recorded as a measure that is useful in performance evaluation.Beyond this functionality for exploring error behavior, we consider how such tools could be designed to improve the quality of collections of badly recognized documents incrementally as users interact with them on a regular basis. We conclude by discussing topics for future research.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {9–16},
numpages = {8},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568302,
author = {McGillivary, Craig and Hale, Chris and Smith, Elisa H. Barney},
title = {Edge Noise in Document Images},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568302},
doi = {10.1145/1568296.1568302},
abstract = {A degradation model that describes many image degradations produced by desktop scanning is used to study the edge noise that is present in bilevel document images. The standard deviation of the additive noise does not adequately describe the noise present after the image is converted to a bilevel image. A measure of noise called Noise Spread is developed which describes the edge noise and is a function of the scanner parameters. If phase effects are removed this Noise Spread quantity is directly proportional to the expected value of the Hamming distance between scans with and without edge noise. The Noise Spread has also been correlated with the ability to accurately estimate edge locations. A simple method to estimate this quantity is proposed.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {17–24},
numpages = {8},
keywords = {Hamming distance, image noise, edge noise, line fitting, image degradations},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568303,
author = {Gielissen, Tim and Marx, Maarten},
title = {Digital Weight Watching: Reconstruction of Scanned Documents},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568303},
doi = {10.1145/1568296.1568303},
abstract = {Scanned and OCRed data leads to large file sizes if facsimile images are included. This makes storage of, and providing online access to large data sets costly. Manually analyzing such data is cumbersome because of long download and processing times. It may thus be advantageous to reconstruct the scanned documents as documents without scanned images which nevertheless closely resemble the original. We have done this reconstruction for a data set of Dutch parliamentary proceedings with positive results. 1.5% of the original storage space was needed, while the documents resembled the originals to a high degree. We describe the reconstruction process and evaluate the costs, the benefits and the quality.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {25–31},
numpages = {7},
keywords = {scanned documents, information extraction, XML},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568304,
author = {Marinai, Simone},
title = {Text Retrieval from Early Printed Books},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568304},
doi = {10.1145/1568296.1568304},
abstract = {We describe a text indexing and retrieval technique that does not rely on word segmentation and is tolerant to errors in character segmentation. The method is designed to process early printed documents and we evaluate it on the well known Latin Gutenberg Bible.The approach relies on two main components. First, character objects (in most cases corresponding to individual characters) are extracted from the document and clustered together, so as to assign a symbolic class to each indexed object. Second, a query word is compared against the indexed character objects with a Dynamic Time Warping (DTW) based approach. The peculiarity of the matching technique described in this paper is the incorporation of sub-symbolic information in the string matching process. In particular, we take into account the estimated widths of potential subwords that are computed by accumulating lengths of partial matches in the DTW array.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {33–40},
numpages = {8},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568305,
author = {Ouwayed, N. and Bela\"{\i}d, A. and Auger, F.},
title = {Cohen's Class Distributions for Skew Angle Estimation in Noisy Ancient Arabic Documents},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568305},
doi = {10.1145/1568296.1568305},
abstract = {This paper presents a method for the skew angle estimation of noisy handwritten Arabic documents using an energy distribution of the Cohen's class. The presence of noise in raw document images can create local maxima which can disturb the projection histogram analysis. To avoid this drawback, we propose to apply a Cohen's class distribution on the projection profiles histograms obtained using different projection angles. These distributions reduce the importance of these local maxima and are fitted to the non-stationary nature of the histogram profiles. The orientation of the document is given by the highest maximum. The proposed skew angle estimation technique has been evaluated on 864 skewed documents. The results obtained with 9 selected distributions are presented at the end of this paper.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {41–46},
numpages = {6},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568306,
author = {Stamatopoulos, Nikolaos and Louloudis, Georgios and Gatos, Basilis},
title = {A Comprehensive Evaluation Methodology for Noisy Historical Document Recognition Techniques},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568306},
doi = {10.1145/1568296.1568306},
abstract = {In this paper, we propose a new comprehensive methodology in order to evaluate the performance of noisy historical document recognition techniques. We aim to evaluate not only the final noisy recognition result but also the main intermediate stages of text line, word and character segmentation. For this purpose, we efficiently create the text line, word and character segmentation ground truth guided by the transcription of the historical documents. The proposed methodology consists of (i) a semiautomatic procedure in order to detect the text line, word and character segmentation ground truth regions making use of the correct document transcription, (ii) calculation of proper evaluation metrics in order to measure the performance of the final OCR result as well as of the intermediate segmentation stages. The semi-automatic procedure for detecting the ground truth regions has been evaluated and proved efficient and time saving. Experimental results prove that using the proposed technique, the percentage of time saved for the text line, word and character segmentation ground truth creation is more than 90%. An analytic experiment using a commercial OCR engine applied to a historical book is also presented.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {47–54},
numpages = {8},
keywords = {historical document processing and recognition, transcript mapping, segmentation, OCR, document image processing, evaluation, word segmentation, text line segmentation},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568307,
author = {Kesidis, Anastasios and Galiotou, Eleni and Gatos, Basilis and Lampropoulos, Aristomenis and Pratikakis, Ioannis and Manolessou, Ioanna and Ralli, Angela},
title = {Accessing the Content of Greek Historical Documents},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568307},
doi = {10.1145/1568296.1568307},
abstract = {In this paper, we propose an alternative method for accessing the content of Greek historical documents printed during the 17th and 18th centuries by searching words directly in digitized documents based on word spotting, without the use of an optical character recognition engine. We describe a methodology according to which synthetic word images are created from keywords. These images are compared to all the words in the digitized documents while user feedback is used in order to refine the search procedure. In order to improve the efficiency of accessing and searching, we have used natural language processing techniques that comprise (i) a morphological generator for early Modern Greek which provides the users with the ability to search documents using only a word stem and locate all the corresponding inflected word forms and (ii) a synonym dictionary which facilitates access to the semantic context of documents and enriches the results of the search process.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {55–62},
numpages = {8},
keywords = {historical document indexing, natural language processing, computational morphology, word spotting},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568308,
author = {Subramanian, Krishna and Prasad, Rohit and Natarajan, Prem},
title = {Robust Named Entity Detection Using an Arabic Offline Handwriting Recognition System},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568308},
doi = {10.1145/1568296.1568308},
abstract = {Text from Arabic optical handwriting recognition (OHR) systems can provide key indexing information. In particular, the text is rich in named entities (NEs) and detection of such entities is critical for search applications. Traditional approaches for detecting NEs in optical character recognition (OCR) output look for these NEs in the single-best recognition results. Due to the inevitable presence of recognition errors in the single-best output, such approaches usually result in low recall. Given that a lattice is more likely to contain the correct answer, we explore NE detection from word lattices produced by our Arabic handwriting recognition system. Since the improvement in recall is accompanied by a large number of false positives, we use confidence scores based on posterior scores to control precision. We show a 7% improvement in true detects for the same false acceptance rate on using lattices instead of 1-best hypothesis for NE lookup.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {63–68},
numpages = {6},
keywords = {lattice search, named entity detection, optical character recognition, Arabic handwriting recognition},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568309,
author = {Gotscharek, Annette and Neumann, Andreas and Reffle, Ulrich and Ringlstetter, Christoph and Schulz, Klaus U.},
title = {Enabling Information Retrieval on Historical Document Collections: The Role of Matching Procedures and Special Lexica},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568309},
doi = {10.1145/1568296.1568309},
abstract = {Due to the large number of spelling variants found in historical texts, standard methods of Information Retrieval (IR) fail to produce satisfactory results on historical document collections. In order to improve recall for search engines, modern words used in queries have to be associated with corresponding historical variants found in the documents. In the literature, the use of (1) special matching procedures and (2) lexica for historical language have been suggested as two ways to solve this problem. In the first part of the paper we show how the construction of matching procedures and lexica may benefit from each other, leading the way to a combination of both approaches. A tool is presented where matching rules and a historical lexicon are built in an interleaved way based on corpus analysis. A crucial question considered in the second part of the paper is if matching procedures alone suffice to lift IR on historical texts to a satisfactory level. Since historical language changes over centuries it is not simple to obtain an answer. We present experiments where the performance of matching procedures in text collections from four centuries is studied. After classifying missed vocabulary, we measure precision and recall of the matching procedure for each period. Our results indicate that for earlier periods historical lexica represent an important corrective to matching procedures in IR applications.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {69–76},
numpages = {8},
keywords = {information retrieval, electronic lexica, historical spelling variants},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568310,
author = {Reynaert, Martin},
title = {Parallel Identification of the Spelling Variants in Corpora},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568310},
doi = {10.1145/1568296.1568310},
abstract = {We present a new approach based on anagram hashing to globally handle the typographical variation in large and possibly noisy text collections. Typographical variation is typically handled in a local fashion: given one particular text string some system of retrieving near-neighbours is applied, where near-neighbours are other text strings that differ from the particular string by a given number of characters. The difference in characters between the original string and one of its retrieved near-neighbours we call a particular character confusion. We present a global way of performing this action: given a possible particular character confusion, we identify - in parallel, i.e. in one single operation on anagram-hash derived bit vectors - all the pairs of text strings in the text collection to which the particular confusion applies. The algorithm proposed here is evaluated on about 23,000 English attested typos from the Reuters rcv1 text collection. We further explore its usefulness for unsupervised linking of a historical Dutch word list to its contemporary counterpart.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {77–84},
numpages = {8},
keywords = {spelling variation, typographical, historical, OCR},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568311,
author = {Das, Pradipto and Srihari, Rohini and Mukund, Smruthi},
title = {Discovering Voter Preferences in Blogs Using Mixtures of Topic Models},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568311},
doi = {10.1145/1568296.1568311},
abstract = {In this paper we propose a new approach to capture the inclination towards a certain election candidate from the contents of blogs and to explain why that inclination may be so. The method is based on the availability of "ground truth" speeches from the election candidates that are labeled and also on the collection of noisy blogs which are not labeled in any way. In this unsupervised learning scenario, we used probabilistic topic models to cluster the ground truth documents for each candidate into different underlying latent themes. The same topic models were then applied on the blog collection and the "orientation" of each of the blogs with different themes of the election candidate speeches was performed using KL divergence of the topic distribution over the overlapping vocabularies. We used four models for such theme matching, one with a baseline topic model and the other three by weighting the baseline topic model with the positive, negative and the neutral sentiments of the topics. We then used a collaborative objective function to combine the outcome of candidate preference for the blogs under the four models using an Expectation Maximization algorithm. The novelty of our method is highlighted in its use of unannotated data as well as in the combination of the views of the different "experts" explaining the same phenomenon.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {85–92},
numpages = {8},
keywords = {sentiments, KL divergence, social network, blogs, topic models},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568312,
author = {Giannone, Cristina and Basili, Roberto and Del Vescovo, Chiara and Naggar, Paolo and Moschitti, Alessandro},
title = {Kernel-Based Relation Extraction from Investigative Data},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568312},
doi = {10.1145/1568296.1568312},
abstract = {In a specific process of business intelligence, i.e. investigation on organized crime, empirical language processing technologies can play a crucial role. In the data used on investigative activities, such as police interrogatory or electronic eavesdropping and wiretap, it is customary to find out expressions in non conventional languages as dialects, slangs or coded words. The recognition and storage of complex relations among subjects mentioned in these sources is a very difficult and time consuming task, ultimately based on pools of experts. We discuss here an inductive relation extraction platform that opens the way to much cheaper and consistent workflows. SVMs here are employed to produce a set of possible interpretations for domain relevant concepts. An ontology population process is here realized, where further reasoning can be applied to proof the overall consistency of the extracted information. The empirical investigation presented here shows that accurate results, comparable to the expert teams, can be achieved, and parametrization allows to fine tune the system behavior for fitting the specific domain requirements.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {93–100},
numpages = {8},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568313,
author = {Bratus, Sergey and Rumshisky, Anna and Magar, Rajendra and Thompson, Paul},
title = {Using Domain Knowledge for Ontology-Guided Entity Extraction from Noisy, Unstructured Text Data},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568313},
doi = {10.1145/1568296.1568313},
abstract = {Domain-specific knowledge is often recorded by experts in the form of unstructured text. For example, in the medical domain, clinical notes from electronic health records contain a wealth of information. Similar practices are found in other domains. The challenge we discuss in this paper is how to identify and extract part names from technicians repair notes, a noisy unstructured text data source from General Motors' archives of solved vehicle repair problems, with the goal to develop a robust and dynamic reasoning system to be used as a repair adviser by service technicians.In the present work, we discuss two approaches to this problem. We present an algorithm for ontology-guided entity disambiguation that uses existing knowledge sources such as domain-specific ontologies and other structured data. We illustrate its use in automotive domain, using GM parts ontology and the unit structure of repair manuals text to build context models, which are then used to disambiguate mentions of part-related entities in the text. We also describe extraction of part names with a small amount of annotated data using Hidden Markov Models (HMM) with shrinkage, achieving an f-score of approximately 80%. Next we used linear-chain Conditional Random Fields (CRF) in order to model observation dependencies present in the repair notes. Using CRF did not lead to improved performance, but a slight improvement over the HMM results was obtained by using a weighted combination of the HMM and CRF models.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {101–106},
numpages = {6},
keywords = {information extraction, language models, ontology-guided search, text analysis},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568314,
author = {Dey, Lipika and Haque, S. K. Mirajul},
title = {Studying the Effects of Noisy Text on Text Mining Applications},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568314},
doi = {10.1145/1568296.1568314},
abstract = {Text mining aims at deriving high quality information from text in an automated way. Text mining applications rely on Natural Language Processing (NLP) tools like tagger, parser etc. to locate and retrieve relevant information in an application specific manner. Most of these NLP tools however have been designed to work on clean and grammatically correct text. Presently, many organizations are interested to derive information from informally written text that is generated as a result of human communication through emails, or blog posts, web-based reviews etc. These texts are highly noisy and often found to contain mixture of languages. In this study we present some analysis on how noise introduced due to incorrect English affects the performance of some of the NLP tools and thereafter the text mining applications. The text mining application that we focus on is opinion mining. Opinion mining is the most significant text mining application that has to deal with noisy text generated in an unregulated fashion by users.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {107–114},
numpages = {8},
keywords = {NLP tools, text mining, noisy text},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568315,
author = {Subramaniam, L. Venkata and Roy, Shourya and Faruquie, Tanveer A. and Negi, Sumit},
title = {A Survey of Types of Text Noise and Techniques to Handle Noisy Text},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568315},
doi = {10.1145/1568296.1568315},
abstract = {Often, in the real world noise is ubiquitous in text communications. Text produced by processing signals intended for human use are often noisy for automated computer processing. Automatic speech recognition, optical character recognition and machine translation all introduce processing noise. Also digital text produced in informal settings such as online chat, SMS, emails, message boards, newsgroups, blogs, wikis and web pages contain considerable noise. In this paper, we present a survey of the existing measures for noise in text. We also cover application areas that ingest this noisy text for various tasks like Information Retrieval and Information Extraction.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {115–122},
numpages = {8},
keywords = {text mining, information retrieval, information extraction, noisy text, natural language processing},
location = {Barcelona, Spain},
series = {AND '09}
}

@inproceedings{10.1145/1568296.1568317,
author = {Onkov, Kolyo Z.},
title = {Effect of OCR-Errors on the Transformation of Semi-Structured Text Data into Relational Database},
year = {2009},
isbn = {9781605584966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1568296.1568317},
doi = {10.1145/1568296.1568317},
abstract = {Paper guides and reference books in the fields of Pharmacology, Veterinary and Crops Protection are often presented in the form of semi-structured text data. "Key words", for instance, the names of diseases and drugs, and relationships between them are of a great importance for obtaining the useful information -- advice, instructions, etc. The definition of relationships is significant problem when the aim is to transform relatively big amount semi-structured text data into intelligent computer based system. The paper shortly presents the OCR errors detection and correction in the process of transformation of Bulgarian crops protection reference book into relational database. Finally, this solution leads to substantial change in the form of the data presentation and access. This does not change the essence of the data itself.},
booktitle = {Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data},
pages = {123–124},
numpages = {2},
keywords = {errors classification, database relations, data retrieval},
location = {Barcelona, Spain},
series = {AND '09}
}

