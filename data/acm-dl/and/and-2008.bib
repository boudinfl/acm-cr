@inproceedings{10.1145/1390749.1390750,
author = {Harman, Donna},
title = {Some Thoughts on Failure Analysis for Noisy Data},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390750},
doi = {10.1145/1390749.1390750},
abstract = {A critical piece of evaluation is doing a failure analysis of the experimental results. This is necessary to improve results, but equally important, it is necessary to understand where the problems REALLY are, as opposed to just following "conventional" wisdom. The talk will present some current failure analysis techniques and examine how these techniques could be extended to retrieval from noisy data.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
articleno = {1},
numpages = {1},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390751,
author = {Tait, John I.},
title = {Noise and Information},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390751},
doi = {10.1145/1390749.1390751},
abstract = {In this extended abstract I examine the notion of noise and its application to Information Retrieval and urge the reader to consider noise as an intrinsic property of information, not merely a problem to be eliminated.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
articleno = {2},
numpages = {2},
keywords = {relevance in context, information retrieval, noise},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390752,
author = {Sitbon, Laurianne and Bellot, Patrice},
title = {How to Cope with Questions Typed by Dyslexic Users},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390752},
doi = {10.1145/1390749.1390752},
abstract = {In this paper we propose a way to cope with questions typed by dyslexic users as they are usually a deformation of the intended query that cannot be corrected with classical spellcheckers. We first propose a new model for statistic question answering systems based on a probabilistic information retrieval model and a combination of results. This model allows a multiple weighted terms query as an input. We also introduce a phonology based approach at the sentence level to derive possible intended terms from typed questions. This approach uses the finite state machine framework to go from phonetic hypothesis and spellchecker proposals to hypothesized sentences thanks to a language model. The final weighted queries are obtained thanks to posterior probabilities computation. They are evaluated according to new density and appearance rating measures which adapt recall and precision to non binary data.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {1–8},
numpages = {8},
keywords = {rewriting, dyslexic users, robust probabilistic model, question-answering},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390753,
author = {Lopresti, Daniel},
title = {Optical Character Recognition Errors and Their Effects on Natural Language Processing},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390753},
doi = {10.1145/1390749.1390753},
abstract = {Errors are unavoidable in advanced computer vision applications such as optical character recognition, and the noise induced by these errors presents a serious challenge to down-stream processes that attempt to make use of such data. In this paper, we apply a new paradigm we have proposed for measuring the impact of recognition errors on the stages of a standard text analysis pipeline: sentence boundary detection, tokenization, and part-of-speech tagging. Our methodology formulates error classification as an optimization problem solvable using a hierarchical dynamic programming approach. Errors and their cascading effects are isolated and analyzed as they travel through the pipeline. We present experimental results based on a large collection of scanned pages to study the varying impact depending on the nature of the error and the character(s) involved. The problem of identifying tabular structures that should not be parsed as sentential text is also discussed.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {9–16},
numpages = {8},
keywords = {tokenization, part-of-speech tagging, sentence boundary detection, optical character recognition, performance evaluation},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390754,
author = {Reffle, Ulrich and Gotscharek, Annette and Ringlstetter, Christoph and Schulz, Klaus U.},
title = {Successfully Detecting and Correcting False Friends Using Channel Profiles},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390754},
doi = {10.1145/1390749.1390754},
abstract = {The detection and correction of false friends - also called real-word errors - is a notoriously difficult problem. On realistic data, the break-even point for automatic correction so far could not be reached: the number of additional infelicitous corrections outnumbered the useful corrections. We present a new approach where we first compute a profile of the error channel for the given text. During the correction process, the profile helps to restrict attention to a small set of "suspicious" lexical tokens of the input text where it is "plausible" to assume that the token represents a false friend. Using a conventional word trigram statistics for disambiguation we obtain a correction method that can be successfully applied to unrestricted text. In experiments for OCR documents, we show significant accuracy gains by fully automatic correction of false friends.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {17–22},
numpages = {6},
keywords = {false friends, error dictionaries, error correction},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390755,
author = {Jijkoun, Valentin and Khalid, Mahboob Alam and Marx, Maarten and de Rijke, Maarten},
title = {Named Entity Normalization in User Generated Content},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390755},
doi = {10.1145/1390749.1390755},
abstract = {Named entity recognition is important for semantically oriented retrieval tasks, such as question answering, entity retrieval, biomedical retrieval, trend detection, and event and entity tracking. In many of these tasks it is important to be able to accurately normalize the recognized entities, i.e., to map surface forms to unambiguous references to real world entities. Within the context of structured databases, this task (known as record linkage and data de-duplication) has been a topic of active research for more than five decades. For edited content, such as news articles, the named entity normalization (NEN) task is one that has recently attracted considerable attention. We consider the task in the challenging context of user generated content (UGC), where it forms a key ingredient of tracking and media-analysis systems.A baseline NEN system from the literature (that normalizes surface forms to Wikipedia pages) performs considerably worse on UGC than on edited news: accuracy drops from 80% to 65% for a Dutch language data set and from 94% to 77% for English. We identify several sources of errors: entity recognition errors, multiple ways of referring to the same entity and ambiguous references.To address these issues we propose five improvements to the baseline NEN algorithm, to arrive at a language independent NEN system that achieves overall accuracy scores of 90% on the English data set and 89% on the Dutch data set. We show that each of the improvements contributes to the overall score of our improved NEN algorithm, and conclude with an error analysis on both Dutch and English language UGC. The NEN system is computationally efficient and runs with very modest computational requirements.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {23–30},
numpages = {8},
keywords = {user generated content, named entities, evaluation, wikipedia},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390756,
author = {Ananthanarayanan, Rema and Chenthamarakshan, Vijil and Deshpande, Prasad M and Krishnapuram, Raghuram},
title = {Rule Based Synonyms for Entity Extraction from Noisy Text},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390756},
doi = {10.1145/1390749.1390756},
abstract = {Identification of named entities such as person, organization and product names from text is an important task in information extraction. In many domains, the same entity could be referred to in multiple ways due to variations introduced by different user groups, variations of spellings across regions or cultures, usage of abbreviations, typographical errors and other reasons associated with conventional usage. Identifying a piece of text as a mention of an entity in such noisy data is difficult, even if we have a dictionary of possible entities. Previous approaches treat the synonym problem as part entity disambiguation and use learning-based methods that use the context of the words to identify synonyms. In this paper, we show that existing domain knowledge, encoded as rules, can be used effectively to address the synonym problem to a considerable extent. This makes the disambiguation task simpler, without the need for much training data. We look at a subset of application scenarios in named entity extraction, categorize the possible variations in entity names, and define rules for each category. Using these rules, we generate synonyms for the canonical list and match these synonyms to the actual occurrence in the data sets. In particular, we describe the rule categories that we developed for several named entities and report the results of applying our technique of extracting named entities by generating synonyms for two different domains.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {31–38},
numpages = {8},
keywords = {synonym generation, named entity extraction, product name extraction},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390757,
author = {He, Jiyin and Weerkamp, Wouter and Larson, Martha and de Rijke, Maarten},
title = {Blogger, Stick to Your Story: Modeling Topical Noise in Blogs with Coherence Measures},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390757},
doi = {10.1145/1390749.1390757},
abstract = {Topical noise in blogs arises when bloggers digress from the central topical thrust of their blogs. We introduce a method to explicitly incorporate a model of topical noise into a language modeling approach to the task of blog distillation. Topical noise is integrated into the model using a coherence score, which reflects the tightness of the topical structure of a blog. Tests performed on the TRECBlog06 corpus show that a naive integration of the coherence score as blog prior fails to achieve performance improvements. Instead, we develop a set of more sophisticated models in which the coherence score is weighted by a function of the blog retrieval score. The proposed models help improve effectiveness of our language modeling approach to the blog distillation task.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {39–46},
numpages = {8},
keywords = {coherence measures, blog distillation, language models},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390758,
author = {McArthur, Robert},
title = {Uncovering Deep User Context from Blogs},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390758},
doi = {10.1145/1390749.1390758},
abstract = {People's utterances are fundamentally different to other documents because they are more immediate and less thought through. While this makes them more natural - noisy and unstructured - it provides an unrivalled opportunity to see "inside" the author, to collect some context. The data requires analysis methods that have a relationship to human information processing: socio-cognitively motivated semantic systems. Using HAL, a method validated by cognitive science, the text from a large number of blog entries was analysed to uncover changes in entries author's sense-of-self. Sense-of-self was measured through geometric projection of author's first-person usage onto key indicators of kin and negative emotion words. An example of non-clinical qualitative evaluation affirmed the utility and promise of the technique: that deep personal context can be uncovered from utterances through the appropriate analysis and inference.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {47–54},
numpages = {8},
keywords = {HAL, sense-of-self, semantics, context, socio-cognitive},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390759,
author = {Zhuang, Jinfeng and Hoi, Steven C. H. and Sun, Aixin},
title = {On Profiling Blogs with Representative Entries},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390759},
doi = {10.1145/1390749.1390759},
abstract = {With an explosive growth of blogs, information seeking in blogosphere becomes more and more challenging. One example task is to find the most relevant topical blogs against a given query or an existing blog. Such a task requires concise representation of blogs for effective and efficient searching and matching. In this paper, we investigate a new problem of profiling a blog by choosing a set of m most representative entries from the blog, where m is a predefined number that is application-dependent. With the set of selected representative entries, applications on blogs avoid handling hundreds or even thousands of entries (or posts) associated with each blog, which are updated frequently and often noisy in nature. To guide the process of selecting the most representative entries, we propose three principles, i.e., anomaly, representativeness, and diversity. Based on these principles, a greedy yet very efficient entry selection algorithm is proposed. To evaluate the entry selection algorithms, an extrinsic evaluation methodology from document summarization research is adapted. Specifically, we evaluate the proposed entry selection algorithms by examining their blog classification accuracies. By evaluating on a number of different classification methods, our empirical results showed that comparable classification accuracy could be achieved by using fewer than 20 representative entries for each blog compared to that of engaging all entries.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {55–62},
numpages = {8},
keywords = {entry selection, blog profiling, blog classification},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390760,
author = {Datta, Soumya and Sarkar, Sudeshna},
title = {A Comparative Study of Statistical Features of Language in <i>Blogs-vs-Splogs</i>},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390760},
doi = {10.1145/1390749.1390760},
abstract = {Language usage in Blogs deviate from the language used in traditional corpora largely due to the noise from various causes like spelling errors, grammatical irregularity, overuse of abbreviations and symbolic characters like emoticons. Spam Blogs or Splogs comprise the subset of blogs, which are usually written to target specific audience for marketing promotions and are mostly generated by software that readily imitates Zipfian distribution of words. Therefore it becomes a difficult task to separate splogs from non-splogs using only frequentist distribution of unigrams. In this detailed comparative study we present and highlight several additional statistical features of language, which are hard to imitate and serve as good discriminator between splogs and blogs.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {63–66},
numpages = {4},
keywords = {splogs, Na\"{\i}ve Bayes, svm, blogs},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390761,
author = {Acharyya, Sreangsu and Negi, Sumit and Subramaniam, L. V. and Roy, Shourya},
title = {Unsupervised Learning of Multilingual Short Message Service (SMS) Dialect from Noisy Examples},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390761},
doi = {10.1145/1390749.1390761},
abstract = {Noise in textual data such as those introduced by multi-linguality, misspellings, abbreviations, deletions, phonetic spellings, non standard transliteration, etc pose considerable problems for text-mining. Such corruptions are very common in instant messenger (IM) and short message service (SMS) data and adversely affect off the shelf text mining methods. Most techniques address this problem by supervised methods. But they require labels that are very expensive and time consuming to obtain. While we do not champion unsupervised methods over supervised when quality of results is the supreme and singular concern, we demonstrate that unsupervised methods can provide cost effective results without the need for expensive human intervention to generate parallely labelled corpora. A generative model based unsupervised technique is presented that maps non-standard words to their corresponding conventional frequent form. A Hidden Markov Model (HMM) over subsequencized representation of words is used subject to a parameterization such that the training phase involves clustering over vectors and not the customary dynamic programming over sequences. A principled transformation of maximum likelihood based "central clustering" cost function into a "pairwise similarity" based clustering is proposed. This transformation makes it possible to apply "subsequence kernel" based methods that model delete and insert edit operations well. The novelty of this approach lies in that the expensive (Baum-Welch) iterations required for HMM, can be avoided through a careful factorization of the HMM Loglikelihood and in establishing the connection between information theoretic cost function and the kernel approach of machine learning. Anecdotal evidence of efficacy is provided on public and proprietary data.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {67–74},
numpages = {8},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390762,
author = {J\"{a}rvelin, Antti and Talvensaari, Tuomas and J\"{a}rvelin, Anni},
title = {Data Driven Methods for Improving Mono- and Cross-Lingual IR Performance in Noisy Environments},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390762},
doi = {10.1145/1390749.1390762},
abstract = {In cross-language information retrieval (CLIR), novel or non-standard expressions, technical terminology, or rare proper nouns can be seen as noise when they appear in queries or in the target collection. This kind of vocabulary is often out-of-vocabulary (OOV) for dictionaries that are used to translate queries. In historic document retrieval (HDR), OCR errors and historical spelling variants cause similar problems. In this paper, three data driven approaches to these problems are presented. The two first methods, the transformation rule based translation (TRT) method and the classified s-gram method, operate on string level. With them approximate matches of a query word can be recognized from the target document collection and included into the target query. In the third method, the corpus-based approach, parallel or comparable corpora are employed to derive translation knowledge that can be used to translate OOV words. Besides the overview of the methods, three case studies highlighting their practical applications in CLIR are also presented. The methods are shown to be effective in query translation without dictionaries between closely related languages (TRT and s-grams), OOV word translation (s-grams), and boosting dictionary-based CLIR performance by way of OOV word translation (corpus based methods).},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {75–82},
numpages = {8},
keywords = {TRT, corpus based methods, cross-language information retrieval, noise, s-grams, OOV words},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390763,
author = {Dey, Lipika and Haque, S K Mirajul},
title = {Opinion Mining from Noisy Text Data},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390763},
doi = {10.1145/1390749.1390763},
abstract = {The proliferation of Internet has not only generated huge volumes of unstructured information in the form of web documents, but a large amount of text is also generated in the form of emails, blogs, and feedbacks etc. The data generated from online communication acts as potential gold mines for discovering knowledge. Text analytics has matured and is being successfully employed to mine important information from unstructured text documents. Most of these techniques use Natural Language Processing techniques which assume that the underlying text is clean and correct. Statistical techniques, though not as accurate as linguistic mechanisms, are also employed for the purpose to overcome the dependence on clean text. The chief bottleneck for designing statistical mechanisms is however its dependence on appropriately annotated training data. None of these methodologies are suitable for mining information from online communication text data due to the fact that they are often noisy. These texts are informally written. They suffer from spelling mistakes, grammatical errors, improper punctuation and irrational capitalization. This paper focuses on opinion extraction from noisy text data. It is aimed at extracting and consolidating opinions of customers from blogs and feedbacks, at multiple levels of granularity. Ours is a hybrid approach, in which we initially employ a semi-supervised method to learn domain knowledge from a training repository which contains both noisy and clean text. Thereafter we employ localized linguistic techniques to extract opinion expressions from noisy text. We have developed a system based on this approach, which provides the user with a platform to analyze opinion expressions extracted from a repository.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {83–90},
numpages = {8},
keywords = {wordnet, noisy text, opinion mining, customer feedbacks},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390764,
author = {Arora, Rachit and Ravindran, Balaraman},
title = {Latent Dirichlet Allocation Based Multi-Document Summarization},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390764},
doi = {10.1145/1390749.1390764},
abstract = {Extraction based Multi-Document Summarization Algorithms consist of choosing sentences from the documents using some weighting mechanism and combining them into a summary. In this article we use Latent Dirichlet Allocation to capture the events being covered by the documents and form the summary with sentences representing these different events. Our approach is distinguished from existing approaches in that we use mixture models to capture the topics and pick up the sentences without paying attention to the details of grammar and structure of the documents. Finally we present the evaluation of the algorithms on the DUC 2002 Corpus multi-document summarization tasks using the ROUGE evaluator to evaluate the summaries. Compared to DUC 2002 winners, our algorithms gave significantly better ROUGE-1 recall measures.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {91–97},
numpages = {7},
keywords = {latent dirichlet allocation, multi-document summarization},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390765,
author = {Pandey, Amaresh Kumar and Siddiqui, Tanveer J},
title = {An Unsupervised Hindi Stemmer with Heuristic Improvements},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390765},
doi = {10.1145/1390749.1390765},
abstract = {Stemmers are used to convert inflected words into their root or stem. Stem does not necessarily correspond to linguistic root of a word. Stemming improve performance by reducing morphologically variants into same words. This paper presents an approach is to develop unsupervised Hindi stemmer. This paper focus on the development of an unsupervised stemmer for Hindi and evaluation of approach using manually segmented words. We evaluate our approach on 1000-1000 words randomly extracted words (only) from Hindi WordNet1 data base. The training data has been constructed by extracting 106403 words extracted from EMILLE2 corpus. The observed accuracy was found to be 89.9% after applying some heuristic measures. The F-score was 94.96%. As the algorithm does not require any language specific information, it can be applied to other Indian languages as well. We also evaluate the effect of stemmer in terms of reducing size of index for Hindi information retrieval task. The results have been compared with light weight stemmer [10] and UMass stemmer [17]. Test run shows that our stemmer outperforms both the stemmer.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {99–105},
numpages = {7},
keywords = {heuristics and optimal word segment, unsupervised morphological analyzer, Hindi},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390766,
author = {Bhardwaj, Anurag and Farooq, Faisal and Cao, Huaigu and Govindaraju, Venu},
title = {Topic Based Language Models for OCR Correction},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390766},
doi = {10.1145/1390749.1390766},
abstract = {Despite several decades of research in document analysis, recognition of unconstrained handwritten documents is still considered a challenging task. Previous research in this area has shown that word recognizers produce reasonably clean output when used with a restricted lexicon. But in absence of such a restricted lexicon, the output of an unconstrained handwritten word recognizer is noisy. The objective of this research is to process noisy recognizer output and eliminate spurious recognition choices using a topic based language model. We construct a topic based language model for every document using a training data which is manually categorized. A topic categorization sub-system based on Maximum Entropy model is also trained which is used to generate the topic distribution of a test document. A given test word image is processed by the recognizer and its word recognition likelihood is refined by incorporating topic distribution of the document and topic based language model probability. The proposed method is evaluated on a publicly available IAM dataset and experimental results show significant improvement in the word recognition accuracy from 32% to 40% over a test set consisting of 4033 word images extracted from 70 handwritten document images.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {107–112},
numpages = {6},
keywords = {language models, dynamic lexicon, document analysis, OCR correction, topic categorization},
location = {Singapore},
series = {AND '08}
}

@inproceedings{10.1145/1390749.1390767,
author = {Al-Shammari, Eiman and Lin, Jessica},
title = {A Novel Arabic Lemmatization Algorithm},
year = {2008},
isbn = {9781605581965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390749.1390767},
doi = {10.1145/1390749.1390767},
abstract = {Tokenization is a fundamental step in processing textual data preceding the tasks of information retrieval, text mining, and natural language processing. Tokenization is a language-dependent approach, including normalization, stop words removal, lemmatization and stemming.Both stemming and lemmatization share a common goal of reducing a word to its base. However, lemmatization is more robust than stemming as it often involves usage of vocabulary and morphological analysis, as opposed to simply removing the suffix of the word. In this work, we introduce a novel lemmatization algorithm for the Arabic Language.The new lemmatizer proposed here is a part of a comprehensive Arabic tokenization system, with a stop words list exceeding 2200 Arabic words. Currently, there are two Arabic leading stemmers: the root-based stemmer and the light stemmer. We hypothesize that lemmatization would be more effective than stemming in mining Arabic text. We investigate the impact of our new lemmatizer on unsupervised data mining techniques in comparison to the leading Arabic stemmers. We conclude that lemmatization is a better word normalization method than stemming for Arabic text.},
booktitle = {Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data},
pages = {113–118},
numpages = {6},
keywords = {text mining, lemmatization, tokenization, stemming, Arabic},
location = {Singapore},
series = {AND '08}
}

