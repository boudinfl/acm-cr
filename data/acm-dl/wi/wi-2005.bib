@inproceedings{10.1109/WI.2005.163,
title = {Welcome Message from Conference Chairs and Program Chair},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.163},
doi = {10.1109/WI.2005.163},
abstract = {mess-gen},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.17–xxii},
numpages = {6},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.164,
title = {WI'05 and IAT'05 Conference Organization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.164},
doi = {10.1109/WI.2005.164},
abstract = {list-confcom},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.23–xxiv},
numpages = {2},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.118,
title = {Program Committees},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.118},
doi = {10.1109/WI.2005.118},
abstract = {list-confcom},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.25–xxxi},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.165,
title = {WI'05 Non-PC Reviewers},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.165},
doi = {10.1109/WI.2005.165},
abstract = {list-ref},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.32},
numpages = {1},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.110,
author = {Buntine, Wray and Aberer, Karl and Podnar, Ivana and Rajman, Martin},
title = {Opportunities from Open Source Search},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.110},
doi = {10.1109/WI.2005.110},
abstract = {Internet search has a strong business model that permits a free service to users, so it is difficult to see why, if at all, there should be open source offerings as well. This paper first discusses open source search, and a rationale for the computer science community at large to get involved. Because there is no shortage of core open source components for at least some of the tasks involved, the Alvis Consortium is building infrastructure for open source search engines using peer-to-peer and subject specific technology as its core, based on this rationale. We view open source search as a rich future playground in which information extraction and retrieval components can be used and intelligent agents can operate.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {2–8},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.89,
author = {Doherty, Patrick},
title = {Knowledge Representation and Unmanned Aerial Vehicles},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.89},
doi = {10.1109/WI.2005.89},
abstract = {Knowledge representation technologies play a fundamental role in any autonomous system that includes deliberative capability and that internalizes models of its internal and external environments. Integrating both high- and low-end autonomous functionality seamlessly in autonomous architectures is currently one of the major open problems in robotics research. UAVs offer especially diflcult challenges in comparison with ground robotic systems due to the ofen tight time constraints and safety considerations that must be taken into account. This article provides an overview of some of the knowledge representation technologies and deliberative capabilities developed for a fully deployed autonomous unmanned aerial vehicle system to meet some of these challenges.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {9–16},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.22,
author = {Jennings, Nick},
title = {Agreement Technologies},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.22},
doi = {10.1109/WI.2005.22},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {17},
numpages = {1},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.24,
author = {Langley, Pat},
title = {An Adaptive Architecture for Physical Agents},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.24},
doi = {10.1109/WI.2005.24},
abstract = {In this paper we describe ICARUS, an adaptive architecture for intelligent physical agents. We contrast the framework\'{y}s assumptions with those of earlier architectures, taking examples from an in-city driving task to illustrate our points. Key differences include primacy of perception and action over problem solving, separate memories for categories and skills, a hierarchical organization on both memories, strong correspondence between long-term and short-term structures, and cumulative learning of skill hierarchies. We support claims for ICARUS\'{y} generality by reporting our experience with driving and three other domains. In closing, we discuss limitations of the current architecture and propose extensions that would remedy them.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.121,
author = {Lieberman, Henry and Kumar, Ashwani},
title = {Providing Expert Advice by Analogy for On-Line Help},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.121},
doi = {10.1109/WI.2005.121},
abstract = {One of the principal problems of online help is the mismatch between the specialized knowledge and technical vocabulary of experts who are providing the help, and the relative na\"{\i}vet\'{e} of novices, who usually are often not in a position to understand solutions expressed by the expert in their own terms. Most of the interfaces are plagued by recurrent key problems: 1) elicitation - how to ask questions that enable the helper to make decisions, and at the same time, are understandable to the novice, and 2) explanation - how to explain rationale behind expert decisions in terms that the user can understand. One of the best ways to do this is for the expert to provide analogies in terms of Commonsense knowledge, which provide metaphors that help novices learn problem-solving skills. SuggestDesk is a system that acts as an advisor to an online technical support person. It uses a large Commonsense knowledge base to search for analogies between known technical problem-solution pairs, and situations and events in everyday life that can be used to explain them.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {26–32},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.62,
author = {Schuster, Peter},
title = {Evolution in Simple Systems and the Emergence of Complexity},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.62},
doi = {10.1109/WI.2005.62},
abstract = {Evolution, understood as the powerful interplay of reproduction, variation, and selection, represents an excellent tool for optimization in populations of simple and complex entities even under conditions where only limited information is available. The principle underlying this optimization heuristic was discovered and formulated already by Charles Darwin. In the nineteen-sixties evolutionary optimization became an intensively investigated topic. Experimental studies on evolution in the test-tube [1], mathematical analysis based on chemical reaction kinetics [2] as well as development of computer models and extensive simulations [3] provided detailed insight into the process on the molecular level: A repertoire of variants, called the molecular quasi-species, is created through error-prone reproduction and selection chooses among these variants those which have the highest reproductive success. A striking feature of the Darwinian optimization heuristic is its universal applicability. It is operative in ensembles of very simple systems like nucleic acid molecules and it is similarly successful in populations of highly complex entities, for example animal or human societies. The explanation of the success is straightforward: Selection at the population level is based exclusively on the mean numbers of fertile descendants in forthcoming generations, and therefore all mechanistic details of reproduction and variation are irrelevant for the survival of variants.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {33–37},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.38,
author = {Vadrevu, Srinivas and Nagarajan, Saravanakumar and Gelgi, Fatih and Davulcu, Hasan},
title = {Automated Metadata and Instance Extraction from News Web Sites},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.38},
doi = {10.1109/WI.2005.38},
abstract = {Over the past few years World Wide Web has established as a vital resource for news. With the continuous growth in the number of available news Web sites and the diversity in their presentation of content, there is an increasing need to organize the news related information on the Web and keep track of it. In this paper, we present automated techniques for extracting metadata instance information by organizing and mining a set of news Web sites. We develop algorithms that detect and utilize HTML regularities in the Web documents to turn them into hierarchical semantic structures encoded as XML. The tree-mining algorithms that we present identify key domain concepts and their taxonomical relationships. We also extract semi-structured concept instances annotated with their labels whenever they are available. We report experimental evaluation for the news domain to demonstrate the efficacy of our algorithms.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {38–41},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.6,
author = {Su, Yila and Liu, Jiming and Zhong, Ning and Liu, Chunnian},
title = {A Method of Distributed Problem Solving on the Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.6},
doi = {10.1109/WI.2005.6},
abstract = {One of the key research challenges that we will face in developing the Wisdom Web is to make it capable of seamlessly offering solutions to users in dealing with their real-world problems. In order to make this possible, individual contents or services should be developed and written following the syntax and semantics of a pre-defined method for distributed problem solving on the Web. In this paper, we propose a new method for solving problems in a large-scale distributed Web environment. We will also give an illustrative example in this paper to show the service process of the new method.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {42–45},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.128,
author = {Jin, Xiaolong and Liu, Jiming},
title = {Resource Optimization in Heterogeneous Web Environments},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.128},
doi = {10.1109/WI.2005.128},
abstract = {This paper addresses the distributed resource optimization issue in heterogeneous Web environments, where both resource nodes and service requests may be heterogeneous. Specifically, this paper presents an agent-based mechanism, where agents are employed to carry service requests. Agents are equipped with three behaviors, namely, least-loaded move, less-loaded move, and random move, to search for appropriate resource nodes. Every time, agents probalilistically choose a behavior to perform. As a whole, the multiagent system can accomplish the objective of load balancing and resource optimization. Through experiments on a computing platform, called SSADRO, we validate the effectiveness of the proposed mechanism. As compared to our previously proposed load balancing mechanism in [6], the one in this paper can address dynamic load balancing in heterogeneous environments.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {46–49},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.17,
author = {Zou, Hongbo and Jin, Hai and Han, Zongfen and Tie, Jing and Shi, Xuanhua},
title = {A Virtual-Service-Domain Based Bidding Algorithm for Resource Discovery in Computational Grid},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.17},
doi = {10.1109/WI.2005.17},
abstract = {Resource discovery is a basic service in grid computing : gives a description of resources desired and finds the available one to match the description. In computational grid, how to discover resources efficiently has become a crucial factor to evaluate the performance in the whole system. In this paper, we present a bid-based resource discovery algorithm, which converts a resource request into a bidding letter and sends it to a group of physical services owned by the same virtual service to call for bidding. All resources receiving bidding letter make offers to bid according to our algorithm. Job manager selects the best one to response client request. To evaluate the performance of our method, we compare our system with the centralized and peer-to-peer resource discovery approaches. The analysis results show that our system reduces average response time of jobs, leverages the cost of the resource discovery, and improves the system scalability.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {50–53},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.76,
author = {Tang, Yi and Zhang, Liankuan},
title = {INFO: An Improving Strategy for Searching the Small World Networks},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.76},
doi = {10.1109/WI.2005.76},
abstract = {Many real-world network systems demonstrate the small world phenomenon. The applicability of these systems depends on efficient decentralized techniques to search and retrieve data. In this paper, we propose an INquiry-based FOrwarding (INFO) strategy to search the small world networks. When a node initiates a search with the INFO strategy, it first sends an inquiry message to all its neighbors. This message requires each neighbor to recommend a candidate node which is the closest node to the destination in its own neighborhood. After receiving the neighbors\'{y} replies, the source will pick the closest one to the destination among the candidates and forward the search task to that node. The INFO strategy can decide search directions more globally than the traditional neighborhood-based greedy strategy. Experiments show that this strategy can find a path with shorter length to reach the destination.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {54–57},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.149,
author = {Cuzzocrea, Alfredo},
title = {Towards a Semantics-Based Framework for KD- and IR-Style Resource Querying on XML-Based P2P Information Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.149},
doi = {10.1109/WI.2005.149},
abstract = {A semantics-based framework for KD- and IR-style resource querying on XML-based P2P Information Systems is described in this paper. Particularly, we present and discuss in detail the XML data model and the knowledge representation model of such a framework, which are both based on the amenity of adding semantics to data. As main result, we obtain a collection of techniques that allows us to efficiently process XML-formatted knowledge in P2P IS.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {58–61},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.104,
author = {Nickles, Matthias and Cobos, Ruth and Weiss, Gerhard and Froehner, Tina},
title = {Multi-Source Knowledge Bases and Ontologies with Multiple Individual and Social Viewpoints},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.104},
doi = {10.1109/WI.2005.104},
abstract = {In open environments like the Web, and open Multiagent and Peer2Peer systems, consent among the autonomous, self-interested knowledge sources and users very often cannot be established, and the estimation of trustability and truthfulness of knowledge sources may not be possible. Moreover, competing viewpoints and their communicative contexts even provide valuable meta-knowledge about the intentions of the participants and their social relationships. As a foundational approach to semantically heterogeneous knowledge perspectives, we introduce a formal framework for the computational representation and integration of multi-source knowledge, which makes explicit heterogeneous viewpoints, and conflicting opinions and their social contexts, and allows for the rating, generalization and optional fusion of knowledge by social choice.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {62–66},
numpages = {5},
keywords = {Ontology Merging, Semantic Web, Knowledge Provenance, Knowledge and Ontology Aggregation and Ranking},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.4,
author = {Xie, Nengfu and Cao, Cungen and Guo, Hong Yu},
title = {A Knowledge Fusion Model for Web Information},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.4},
doi = {10.1109/WI.2005.4},
abstract = {The Web was designed as an information space by Tim Berners-Lee, with the goal not only that it should be available for human reading, but also that machine would be able to participate in and help users to communicate with each other. Web Information integration can only free them from the tedious task of finding the relevant data sources. But the most existing systems do not really integrate information. Deeper information integration, or knowledge jkion (KF), involves not only delivering the information available via the links to user, but also analyzing, and merging the information results coming fiom information sources by solving the result consistencies, removing duplicates, etc. In the paper, we give a formal model for knowledge fusion for XML data and the KF-based information access architecture.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {67–72},
numpages = {6},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.49,
author = {Zhdanova, Anna V. and Krummenacher, Reto and Henke, Jan and Fensel, Dieter},
title = {Community-Driven Ontology Management: DERI Case Study},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.49},
doi = {10.1109/WI.2005.49},
abstract = {We introduce the concept of community-driven ontology management and demonstrate the added value to conventional ontology management of being community-driven. Further, we present an implementation of an infrastructure supporting community-driven ontology management. The implemented infrastructure was deployed as a part of the intranet at DERI - Digital Enterprise Research Institute, and the community's response and behavior were observed. The results obtained prove feasibility and advantages of community-driven ontology management.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {73–79},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.53,
author = {Narayan, B. L. and Pal, Sankar K.},
title = {Detecting Sequences and Cycles of Web Pages},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.53},
doi = {10.1109/WI.2005.53},
abstract = {Cycle detection in graphs and digraphs has received wide attention and several algorithms are available for this purpose. While the web may be modeled as a digraph, such algorithms would not be of much use due to both the scale of the web and the number of uninteresting cycles and sequences in it. We propose a novel sequence detection algorithm for web pages, and highlight its importance for search related systems. Here, the sequence found is such that its consecutive elements have the same relation among them. This relation is measured in terms of the positional properties of navigational links, for which we provide a method for identifying navigational links. The proposed methodology does not detect all possible sequences and cycles in the web graph, but just those that were intended by the creators of those web pages. Experimental results confirm the accuracy of the proposed algorithm.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {80–86},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.134,
author = {Nolker, Robert D. and Zhou, Lina},
title = {Social Computing and Weighting to Identify Member Roles in Online Communities},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.134},
doi = {10.1109/WI.2005.134},
abstract = {As more and more people join online communities, the ability to better understand members\'{y} roles becomes critical to preserving and improving the health of those communities. We propose a novel approach to identifying key members and their roles by discovering implicit knowledge from online communities. Viewing an online community as a social network connected by poster-poster relationships, the approach takes advantage of the strengths of social network analysis and weighting schemes from information retrieval in identifying key members. Experimental studies were carried out to empirically evaluate the proposed approach with real-world data collected from a Usenet bulletin board over a one year period. The results showed that the proposed approach can not only identify prominent members whose behaviors are community supportive but also filter chatters whose behaviors are superficial to the online community. The findings have broad implications for online communities by allowing moderators to better support their members and by enabling members to better understand the conversation space.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {87–93},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.99,
author = {Zhu, J. and Goncalves, A. L. and Uren, V. S. and Motta, E. and Pacheco, R.},
title = {Mining Web Data for Competency Management},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.99},
doi = {10.1109/WI.2005.99},
abstract = {We present CORDER (COmmunity Relation Discovery by named Entity Recognition) an un-supervised machine learning algorithm that exploits named entity recognition and co-occurrence data to associate individuals in an organization with their expertise and associates. We discuss the problems associated with evaluating unsupervised learners and report our initial evaluation experiments.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {94–100},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.91,
author = {Zhdanova, Anna V. and Fensel, Dieter},
title = {Limitations of Community Web Portals: A Classmates' Case Study},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.91},
doi = {10.1109/WI.2005.91},
abstract = {We analyze typical web portals supporting communication, data sharing and activities of former classmates. The inflexibility and restrictions imposed on users of such portals are demonstrated to support the thesis that introduction of community-driven ontology management is crucial for full-fledged satisfaction of the user needs on the Semantic Web.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {101–104},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.154,
author = {Aneiros, Maria and Estivill-Castro, Vladimir},
title = {Usability of Real-Time Unconstrained WWW-Co-Browsing for Educational Settings},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.154},
doi = {10.1109/WI.2005.154},
abstract = {The World Wide Web (WWW) and its associated browser-server technologies have become ubiquitous for the home, office and school environment. In the area of education, it has been argued that students can learn new ways of thinking and understanding by working in groups [6]. We have developed a tool that allows a group of users to conduct unconstrained collaborative browsing sessions in real-time over the WWW. We have conducted usability studies with volunteer students from South East Queenland, Australia. Our results show that students learnt to use the tool effectively and enjoyed using it.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {105–111},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.41,
author = {Konagaya, Takeshi and Shintani, Toramatsu and Ozono, Tadachika and Ito, Takayuki and Nishi, Kentaro},
title = {Big Blackboard: On a Large Web Page by Using a Fast Page Loading Method},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.41},
doi = {10.1109/WI.2005.41},
abstract = {Recently, the World Wide Web (WWW) has been attracting much attention as the predominant method of sharing information using multimedia contents. However, when users want to provide multimedia contents on the Web, they must learn how to use authoring software or how to write HTML documents. In this paper, we propose a Web information-sharing system called "Big Blackboard," which is based on a large Web page. The large Web page can act as a metaphor for real-world bulletin boards to realize not only a one-dimensional message board but also a two-dimensional message board on the Web. Any user can easily provide multimedia contents using unlimited layout through a Web browser without needing to know HTML. In the case of displaying on a large Web page, loading Web page contents onto a Web browser may require users to wait a long time. We propose a novel method that reduces transmission cost by only loading the contents within the scope at which a user is looking at an entire Web page. The experimental results demonstrate that our method can effectively reduce user waiting time for loading a Web page.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {112–115},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.65,
author = {Li, Yuan-chao and Wu, Xiao-dong and Zhang, De-gan and Zeng, Guang-ping},
title = {Fuzzy-Neural Theory Applied to Web-Based Proactive Service},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.65},
doi = {10.1109/WI.2005.65},
abstract = {In order to realize proactive service, we design and improve relative fuzzy-neural approaches. Generally, the network can be classified into two. One is that fuzzy logic reasoning is completed by fuzzy weight in neural system. The other is that the input data must be fuzzified in the first or second level, but not weight. We discuss and study the second sort fuzzification in this paper. For proactive decision, fusion method based on fuzzy-neural can make Web-based intelligent system keep advantage of fuzzy logic system and remain adaptive optimum in proactive/attentive service. The correctness and validity of our new approach have been tested.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {116–119},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.3,
author = {Narayan, B. L. and Pal, Sankar K.},
title = {A Fuzzy Web Surfer Model},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.3},
doi = {10.1109/WI.2005.3},
abstract = {A novel web surfer model, where the transitions between web pages are fuzzy quantities, is proposed in this article. Such a model is appropriate when the links between pages are imprecise. The theoretical aspects of modeling the uncertainty associated with links are discussed. The advantages and limitations of the proposed methodology, which is based on the theory of fuzzy Markov chains, are described. Situations where fuzzy web surfer models are appropriate compared to existing models are highlighted.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {120–123},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.139,
author = {Sydow, Marcin},
title = {Studying Dependencies among Web Traffic and Link Analysis Data Using Perceptron},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.139},
doi = {10.1109/WI.2005.139},
abstract = {In this paper we try to experimentally check if it is possible to predict Web traffic data using only link analysis data. It is a natural continuation of [4], where correlations between link analysis and traffic data were measured. A perceptron is applied as an intelligent prediction module. The general conclusion is negative, i.e. dependencies mentioned above are too weak or too complex to be grasped by a perceptron. We also report some succesful results concerning dependencies inside link analysis and inside traffic data. All the experiments are performed on intersection of20-million sample from Polish Web graph and traffic data concerning 3 million Polish URLs.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {124–127},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.152,
author = {Vincent, Dubois and Cecile, Bothorel},
title = {Transitive Reduction for Social Network Analysis and Visualization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.152},
doi = {10.1109/WI.2005.152},
abstract = {In this paper, we will show that transitive reduction can be used in real, large social networks analysis. Transitive reduction is an edge-removing operation on directed graphs, that preserves some important properties and structures of the graph. After a presentation of this tool interest, we will show its properties and how to use it. An application of the process on a real-word large social network (public and famous Enron corpus [3]) built upon interaction data (email boxes) validates this approach.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {128–131},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.14,
author = {Yang, Yuping and Williams, M. Howard and MacKinnon, Lachlan M. and Pooley, Rob},
title = {A Service-Oriented Personalization Mechanism in Pervasive Environments},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.14},
doi = {10.1109/WI.2005.14},
abstract = {An important aspect of a pervasive environment is that it needs to support mobile users - i.e. users whose context changes frequently. In doing so it is important to take account of a user's context in adapting and personalizing services for the user. This paper describes a personalization mechanism that is exploited in the Daidalos project to provide users with value-added enhanced services. It personalizes services across layers including networking and service layers.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {132–135},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.8,
author = {Lampropoulos, Aristomenis S. and Lampropoulou, Paraskevi S. and Tsihrintzis, George A.},
title = {A Middleware System for Web-Based Digital Music Libraries},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.8},
doi = {10.1109/WI.2005.8},
abstract = {We present a middleware system that facilitates Internet users\'{y} access to web-based digital music libraries and allows them to manipulate audio meta-information taking into consideration content and semantic information of music data. Useful relations in the data are automatically extracted through semantic networks (constructed and maintained in the library). Our system is complemented with a query-by-example retrieval subsystem, user relevance feedback facilities, and a new approach for musical genre classification based on the features extracted from signals that correspond to distinct musical instrument sources, as these sources have been identified by a source separation process. The system operation is illustrated in detail.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {136–142},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.130,
author = {Ruta, Michele and Di Noia, Tommaso and Di Sciascio, Eugenio and Donini, Francesco M. and Piscitelli, Giacomo},
title = {Semantic Based Collaborative P2P in Ubiquitous Computing},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.130},
doi = {10.1109/WI.2005.130},
abstract = {We propose a collaborative environment for semantic-enabled mobile devices (e.g. PDAs, cell phones, laptops) in peer to peer scenarios. Within the environment, resource discovery is performed exploiting technologies and techniques for knowledge representation developed for the Semantic Web, which have been adapted to cope with the highly flexible structure of ad-hoc networks in ubiquitous computing. The approach exploits the standard Bluetooth stack, using the original UUID payload, to carry semantically annotated data. The environment is motivated and presented in a museum case study.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {143–149},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.68,
author = {Brezany, Peter and Janciak, Ivan and Tjoa, A Min},
title = {GridMiner: A Fundamental Infrastructure for Building Intelligent Grid Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.68},
doi = {10.1109/WI.2005.68},
abstract = {The Grid is considered as a crucial technology for the future knowledge-based economy and science. The Wisdom Grid project (a joint research effort of the University of Vienna and the Vienna University of Technology) aims, as the first research effort, to cover all aspects of the knowledge life cycle on the Grid - from discovery in Grid data repositories, to processing, sharing and finally reusing of knowledge as input for a new discovery. This paper first outlines the architecture of the Wisdom Grid infrastructure and then focuses on the kernel architecture component calledGrid-Miner, which realizes the knowledge discovery, based on data mining and On-Line Analytical Processing (OLAP) in Grid repositories. A running GridMiner prototype is already available to the scientific community as an Open Service System.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {150–156},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.132,
author = {Lu, Hongen},
title = {Semantic Web Services Discovery and Ranking},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.132},
doi = {10.1109/WI.2005.132},
abstract = {The World Wide Web is evolving from a huge information repository to a service oriented marketplace. Web Service is becoming the next generation of web based application. Due to the dramatic increasing number of available web services, how to locate the right services is a big challenge. In this paper, a new approach is proposed for web services matching within the UDDI registry, based on the matching results a ranking process will significantly reduce the number of recommend services and increase the accuracy. This process is based on semantics in domain ontology. The whole solution provides a novel way to discover and utilize published web services. It is flexible and extensible to accomplish complex web service requests.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {157–160},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.131,
author = {Fukuta, Naoki and Osawa, Tetsuya and Iijima, Tadashi and Yamaguchi, Takahira},
title = {Semantic Service Integration Support for Web Portal},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.131},
doi = {10.1109/WI.2005.131},
abstract = {In this paper, we propose an ontology-based framework that integrates existing web applications into web portals. The framework helps Web portal developers to discover, evaluate, and execute semantically composed services on the Web. Parameter passings on service composition are associated using service ontology. Semi-automated source code generation reduces the cost of system development.We show the proposed technologies are useful for constructing statically composed service integration system for current Web world.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {161–164},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.27,
author = {Liu, Tie-Yan and Wan, Hao and Ma, Wei-Ying},
title = {An Editor Labeling Model for Training Set Expansion in Web Categorization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.27},
doi = {10.1109/WI.2005.27},
abstract = {Automatically classifying web pages is an effective way to manage the massive information on the Web. However, our experiments show that the state-of-the-art text categorization technologies can not achieve a satisfactory classification performance in this task. The major reason is the existence of large proportion of rare categories in Web taxonomies. The failure in such categories is simply because there is not enough information to train reliable classifiers. To tackle this problem, we propose to expand the training set of the rare categories, by simulating the labeling behavior of the human editors of Web directories. Experimental results show that in such a way, we achieved significant (relatively 93%) improvement in classification accuracy, which is highly encouraging for high-performance Web classification.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {165–171},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.75,
author = {Crabtree, Daniel and Gao, Xiaoying and Andreae, Peter},
title = {Improving Web Clustering by Cluster Selection},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.75},
doi = {10.1109/WI.2005.75},
abstract = {Web page clustering is a technology that puts semantically related web pages into groups and is useful for categorizing, organizing, and refining search results. When clustering using only textual information, Suffix Tree Clustering (STC) outperforms other clustering algorithms by making use of phrases and allowing clusters to overlap. One problem of STC and other similar algorithms is how to select a small set of clusters to display to the user from a very large set of generated clusters. The cluster selection method used in STC is flawed in that it does not handle overlapping clusters appropriately. This paper introduces a new cluster scoring function and a new cluster selection algorithm to overcome the problems with overlapping clusters, which are combined with STC to make a new clustering algorithm ESTC. This paper\'{y}s experiments show that ESTC significantly outperforms STC and that even with less data ESTC performs similarly to a commercial clustering search engine.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {172–178},
numpages = {7},
keywords = {cluster selection, web clustering},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.153,
author = {Ting, I-Hsien and Kimble, Chris and Kudenko, Daniel},
title = {UBB Mining: Finding Unexpected Browsing Behaviour in Clickstream Data to Improve a Web Site's Design},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.153},
doi = {10.1109/WI.2005.153},
abstract = {This paper describes a novel web usage mining approach to discover patterns in the navigation of websites known as Unexpected Browsing Behaviours (UBBs). By reviewing these UBBs, a website designer can choose to modify the design of their website or redesign the site completely. UBB mining is based on the Continuous Common Subsequence (CCS), a special instance of Common Subsequence (CS), which is used to define a set of expected routes. The predefined expected routes are then treated as rules and stored in a rule base. By using the predefined route and the UBB mining algorithm, interesting browsing behaviours can be discovered. This paper will introduce the format of the expected route and describe the UBB algorithms. The paper also describes a series of experiments designed to evaluate how well UBB mining algorithms work.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {179–185},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.69,
author = {Stolz, Carsten and Viermetz, Maximilian and Skubacz, Michal and Neuneier, Ralph},
title = {Guidance Performance Indicator " Web Metrics for Information Driven Web Sites},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.69},
doi = {10.1109/WI.2005.69},
abstract = {For the evaluation of web sites a multitude of metrics are available. Apart from general statistical measures, success metrics reflect the degree to which a web site achieves itsdefined objectives. Particularly metrics for e-commerce sites based on transaction analysis are commonly available and well understood. In contrast to transaction based sites, the success of web sites geared toward information delivery is harder to quantify since there is no direct feedback of user intent. User feedback is only directly available on transactional web sites. We introduce a metric to measure the success of an information driven web site in meeting its objective to deliver the desired information in a timely and usable fashion. We propose to assign a value to each click based on the type of transition, duration and semantic distance. These values are then combined into a scoring model describing the success of a web site in meeting its objectives. The resulting metric is introduced as the GPI and its applicability shown on a large corporate web site.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {186–192},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.39,
author = {Huang, Chien-Chung and Lin, Kuan-Ming and Chien, Lee-Feng},
title = {Automatic Training Corpora Acquisition through Web Mining},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.39},
doi = {10.1109/WI.2005.39},
abstract = {Text classification is a task having been extensively studied for decades. However, most previous work pre-assumes the existence of explicitly-labeled corpora. In this study, we focus on the issue of automatic corpora acquisition. We propose an Web-based mining approach to collect necessary corpora, which can be greatly useful to both common users and system designers. Moreover, the proposed technique can also be incorporated with existing classification techniques to further boost classifier performance. It has been shown that the concept of the class can be captured by the class name and its associated terms [10]. In this work, we aim at analyzing Web-retrieved documents to discover the associated terms, which are further utilized to collect more training corpora. Working iteratively, the proposed approach can acquire training corpora of high quality. We give empirical evidence that the classifiers thus created have promising accuracy. In sum, the convenience and efficiency of the proposed approach, along with the new perspective on the issue of corpora acquisition, are the primary contributions of this work.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {193–199},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.13,
author = {Rigutini, Leonardo and Maggini, Marco},
title = {A Semi-Supervised Document Clustering Algorithm Based on EM},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.13},
doi = {10.1109/WI.2005.13},
abstract = {Document clustering is a very hard task in Automatic Text Processing since it requires to extract regular patterns from a document collection without a priori knowledge on the category structure. This task can be difficult also for humans because many different but valid partitions may exist for the same collection. Moreover, the lack of information about categories makes it difficult to apply effective feature selection techniques to reduce the noise in the representation of texts. Despite these intrinsic difficulties, text clustering is an important task for Web search applications in which huge collections or quite long query result lists must be automatically organized. Semi-supervised clustering lies in between automatic categorization and auto-organization. It is assumed that the supervisor is not required to specify a set of classes, but only to provide a set of texts grouped by the criteria to be used to organize the collection. In this paper we present a novel algorithm for clustering text documents which exploits the EM algorithm together with a feature selection technique based on Information Gain. The experimental results show that only very few documents are needed to initialize the clusters and that the algorithm is able to properly extract the regularities hidden in a huge unlabeled collection.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {200–206},
numpages = {7},
keywords = {Semi-supervised Document Clustering, EM, Information Gain},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.151,
author = {Adar, Eytan and Adamic, Lada A.},
title = {Tracking Information Epidemics in Blogspace},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.151},
doi = {10.1109/WI.2005.151},
abstract = {Beyond serving as online diaries, weblogs have evolved into a complex social structure, one which is in many ways ideal for the study of the propagation of information. As weblog authors discover and republish information, we are able to use the existing link structure of blogspace to track its flow. Where the path by which it spreads is ambiguous, we utilize a novel inference scheme that takes advantage of data describing historical, repeating patterns of "infection." Our paper describes this technique as well as a visualization system that allows for the graphical tracking of information flow.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {207–214},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.70,
author = {Amarasiri, Rasika and Alahakoon, Damminda and Smith, Kate and Premaratne, Malin},
title = {HDGSOMr: A High Dimensional Growing Self-Organizing Map Using Randomness for Efficient Web and Text Mining},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.70},
doi = {10.1109/WI.2005.70},
abstract = {Mining of text data from the web has become a necessity in modern days due to the volumes of data available on the web. While searching for information on the web using search engines is popular, to analyze the content on large collections of web pages, feature map techniques are still popular. One of the problems associated with processing large collections of text data from the web using feature map techniques is the time taken to cluster them. This paper presents an algorithm based on a growing variant of the Self Organizing Map called the HDGSOMr. This novel algorithm incorporates randomness into the self-organizing process to produce higher quality clusters within few epochs and utilizing smaller neighborhood sizes resulting in a significant reduction in overall processing time. Details of the HDGSOMr algorithm and results of processing large collections of text data proving the efficiency of the algorithm are also presented.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {215–221},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.80,
author = {Yang, Jianwu and Cheung, William K. and Chen, Xiaoou},
title = {Integrating Element and Term Semantics for Similarity-Based XML Document Clustering},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.80},
doi = {10.1109/WI.2005.80},
abstract = {Structured link vector model (SLVM) is a recently proposed document representation that takes into account both structural and semantic information for measuring XML document similarity. Its formulation includes an element similarity matrix for capturing the semantic similarity between XML elements - the structural components of XML documents. In this paper, instead of applying heuristics to define the similarity matrix, we proposed to learn the matrix using pair-wise similar training data in an iterative manner. In addition, we extended SLVM to SLVM-LSI by incorporating term semantics into SL VM using latent semantic indexing, with the element similarity related properties of the original SLVM preserved. For performance evaluation, we applied SLVM-LSI to similarity-based clustering af two XMZ. datasets and the proposed SLVM-LSI was found to significant(y outpeform the conventional vector space model and the edit-distance based methods. The similarity matrix. obtained as a by-product via the learning, can provide higher-level knowledge about the semantic relationship between the XML elements.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {222–228},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.58,
author = {Lartillot, Olivier},
title = {Efficient Extraction of Closed Motivic Patterns in Multi-Dimensional Symbolic Representations of Music},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.58},
doi = {10.1109/WI.2005.58},
abstract = {An efficient model for discovering repeated patterns in symbolic representations of music is presented. Combinatorial redundancy inherent in the pattern discovery paradigm is commonly filtered using global selective mechanisms, based on pattern frequency and length. We propose an alternate approach founded on the concept of closed pattern and enabling detailed analyses through adaptive selection of most specific descriptions in a multi-dimensional parametric space. A notion of cyclic pattern is introduced, enabling an adapted filtering of another form of combinatorial redundancy caused by successive repetitions of patterns. The use of cyclic patterns implies a necessary chronological scanning of the piece, and the addition of mechanisms formalizing particular Gestalt principles. This study shows therefore that automated analysis of music cannot rely on simple mathematical or statistical approaches, but needs rather complex and detailed modeling of the cognitive system ruling listening processes. The resulting algorithm is able to offer for the first time compact and relevant motivic analyses of simple monodies, and may therefore be applied to automated indexing of symbolic music databases. Numerous additional mechanisms need to be added in order to consider all aspects of music expression, including polyphony and complex musical transformations.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {229–235},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.146,
author = {Martins, Bruno and Silva, Mario J.},
title = {The WebCAT Framework " Automatic Generation of Meta-Data for Web Resources},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.146},
doi = {10.1109/WI.2005.146},
abstract = {Automated methods for resource annotation are a clear necessity, as the success of the SemanticWeb depends on the availability of Web resources with meta-data conforming to known standards and ontologies. This paper describes the WebCAT framework for automatically generating RDF descriptions of Web pages. We present a general view of the system and the algorithms involved, giving an emphasis to typical issues in processing Web data.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {236–242},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.155,
author = {Carbo, Joan Marc and Mor, Enric and Minguillon, Julia},
title = {User Navigational Behavior in E-Learning Virtual Environments},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.155},
doi = {10.1109/WI.2005.155},
abstract = {In this paper we describe the navigational behavior of the students of a e-learning virtual environment, in order to determine whether such navigational patterns are related to the academic performance achieved by the students or not, and which behaviors can be identified as more successful. As an example, a subset of students taking a degree in Computer Science in a completely virtual online university are selected as the matter of study. Three levels of analysis are described: a session level, where students perform a few actions in a single session logged to the virtual campus; a course level, where all single sessions are joined to form a course navigational pattern; and a life-long learning level, where students enroll in several subjects each academic semester. A simple experiment is outlined for the course level to demonstrate the possibilities of such analysis in a virtual e-learning environment. This experiment shows that the information collected in this level is useful for understanding user behavior and the relationship with his or her academic achievements, and that some intuitive ideas about the relevance of specific user actions or particularities can be also better explained.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {243–249},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.40,
author = {Raposo, Juan and Pan, Alberto and Alvarez, Manuel and Hidalgo, Justo},
title = {Automatically Generating Labeled Examples for Web Wrapper Maintenance},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.40},
doi = {10.1109/WI.2005.40},
abstract = {In order to let software programs gain full benefit from semi-structured web sources, wrapper programs must be built to provide a "machine-readable" view over them. A significant problem of this approach is that, since web sources are autonomous, they may experience changes that invalidate the current wrapper. In this paper, we address this problem by introducing novel heuristics and algorithms for automatically maintaining wrappers. In our approach the system collects some query results during normal wrapper operation and, when the source changes, it uses them as input to generate a set of labeled examples for the source which can then be used to induce a new wrapper. Our experiments show that the proposed techniques show high accuracy for a wide range of real-world web data extraction problems.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {250–256},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.47,
author = {Hu, Jia and Zhong, Ning},
title = {Clickstream Log Acquisition with Web Farming},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.47},
doi = {10.1109/WI.2005.47},
abstract = {Collecting customer interaction data on the e-business websites and portals will help to figure out customer behavior and build customer profile, and then perform personalized services. Traditional Web server log is hard to be associated with specific customer and impossible to log the complete actions and movements of customers across web-sites. Collecting clickstream log at the application layer with Web farming technology will help to seamlessly integrate Web usage data with other customer-related data. This model can be developed as a common plugin for most existing e-business websites and portals.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {257–263},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.57,
author = {Alguliev, Rasim M. and Aliguliyev, Ramiz M.},
title = {Effective Summarization Method of Text Documents},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.57},
doi = {10.1109/WI.2005.57},
abstract = {In this paper, we propose text summarization method that creates text summary by definition of the relevance score of each sentence and extracting sentences from the original documents. While summarization this method takes into account weight of each sentence in the document. The essence of the method suggested is in preliminary identification of every sentence in the document with characteristic vector of words, which appear in the document, and calculation of relevance score for each sentence. The relevance score of sentence is determined through its comparison with all the other sentences in the document and with the document title by cosine measure. Prior to application of this method the scope of features is defined and then the weight of each word in the sentence is calculated with account of those features. The weights of features, influencing relevance of words, are determined using genetic algorithms.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {264–271},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.96,
author = {Alhammady, Hamad and Ramamohanarao, Kotagiri},
title = {Mining Emerging Patterns and Classification in Data Streams},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.96},
doi = {10.1109/WI.2005.96},
abstract = {A data stream model has been proposed recently for those data-intensive applications such as financial applications, manufacturing, and others [6]. In this model, data arrives in multiple, continuous, rapid, time-varying data streams. These characteristics make it infeasible for traditional classification and mining techniques to deal with data streams. In this paper, we propose a novel method for mining emerging patterns (EPs) in data streams. Moreover, we show how these EPs can be used to classify data streams. EPs [3] are those itemsets whose supports in one class are significantly higher than their supports in the other classes. The experimental evaluation shows that our proposed method can achieve up to 10% increase in accuracy compared to the other methods.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {272–275},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.12,
author = {Wang, Chao and Lu, Jie and Zhang, Guangquan},
title = {A Semantic Classification Approach for Online Product Reviews},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.12},
doi = {10.1109/WI.2005.12},
abstract = {With the fast growth of e-commerce, product reviews on the Web have become an important information source for customers\'{y} decision making when they plan to buy products online. As the reviews are often too many for customers to go through, how to automatically classify them into different semantic orientations (i.e. recommend/not recommend) has become a research problem. Different from traditional approaches that treat a review as a whole, our approach performs semantic classifications at the sentence level by realizing reviews often contain mixed feelings or opinions. In this approach, a typical feature selection method based on sentence tagging is employed and a na\"{\i}ve bayes classifier is used to create a base classification model, which is then combined with certain heuristic rules for review sentence classification. Experiments show that this approach achieves better results than using general na\"{\i}ve bayes classifiers.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {276–279},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.138,
author = {Crabtree, Daniel and Gao, Xiaoying and Andreae, Peter},
title = {Standardized Evaluation Method for Web Clustering Results},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.138},
doi = {10.1109/WI.2005.138},
abstract = {Web clustering assists users of a search engine by presenting search results as clusters of related pages. Many clustering algorithms with different characteristics have been developed: but the lack of a standardized web clustering evaluation method that can evaluate clusterings with different characteristics has prevented effective comparison of algorithms. The paper solves this by introducing a new structure for defining general ideal clusterings and new measurements for evaluating clusterings with different characteristics by comparing them against the general ideal clustering.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {280–283},
numpages = {4},
keywords = {web clustering, evaluation},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.60,
author = {Rios, Sebastian A. and Velasquez, Juan D. and Vera, Eduardo S. and Yasuda, Hiroshi and Aoki, Terumasa},
title = {Establishing Guidelines on How to Improve the Web Site Content Based on the Identification of Representative Pages},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.60},
doi = {10.1109/WI.2005.60},
abstract = {The internet has become a big battle field where organizations are trying to keep their present clients and to gain new ones. Two important weapons that the organizations have are to make a good web site design and to have a content interesting for the visitors. To improve the web site content many tools have been developed. However, it is hard to figure out how to apply these changes. Furthermore, in complex web sites, this is a non trivial task. We propose a novel approach that helps to improve a web site content using a SOFM and performing a reverse clustering analysis that allows us to gather the most representative web pages from a web site, using this small set of pages as a guideline of how these enhancements should be performed. The effectiveness of the method was tested in a real web site.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {284–288},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.97,
author = {Lim, SeungJin and Ko, Youngrae},
title = {Mining Highly Authoritative Web Resources for One-Stop Learning},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.97},
doi = {10.1109/WI.2005.97},
abstract = {The convenience of the Web equipped with automatic search engines attracts "focused learners" for learning about a new subject of interest. The resources recommended by a search engine are, however, often a collection of links to other resources, or commercial-driven, irrelevant, misleading pages. Subsequently, the learner needs to manually click through numerous pages to find quality resources. This paper proposes an approach to a new problem of mining the most suitable resources for one-stop learning, called "highly authoritative resources." The experimental results using top search results from Google and Yahoo for various subjects show that the proposed algorithm is highly effective both in quality and time.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {289–292},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.87,
author = {Wang, Jiabing and Peng, Hong},
title = {Keyphrases Extraction from Web Document by the Least Squares Support Vector Machine},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.87},
doi = {10.1109/WI.2005.87},
abstract = {Automatic keyphrase extraction from documents is a task with many applications in information retrieval and natural language processing. Previously, Several keyphrase extraction methods have been proposed based on different techniques. In this paper a keyphrase extraction algorithm based on the least squares support vector machine is proposed. In order to determine whether a phrase is a keyphrase or not, the following features of a phrase in a given document are adopted: its TF (term frequency) and IDF (inverted document frequency), whether or not it appears in the title or headings (subheadings) of the given document, and its distribution in the paragraphs of the given document. The algorithm is evaluated by the standard information retrieval metrics of precision and recall and human assessment. Experiment results show that this approach is competitive with other known methods.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {293–296},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.55,
author = {Zhou, Baoyao and Hui, Siu Cheung and Fong, Alvis C. M.},
title = {Discovering and Visualizing Temporal-Based Web Access Behavior},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.55},
doi = {10.1109/WI.2005.55},
abstract = {Discovering and understanding web users\'{y} surfing behavior are essential for the development of successful web monitoring and recommendation systems. In this paper, we propose a web usage mining approach for the automatic discovery and visualization of temporal-based web access behavior of individual users by mining client-side logs. The proposed approach is based on a Web Usage Lattice model which represents a hierarchy of web access activities. To describe such web access activities, we incorporate fuzzy logic to represent real-life temporal concepts such as morning, afternoon and evening, and meaningful web categories such as News, Sports and Chat. Based on the lattice, temporal and association behavior patterns can be extracted and visualized.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {297–300},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.157,
author = {Takama, Yasufumi and Mitsuhashi, Noriaki},
title = {Visual Similarity Comparison for Web Page Retrieval},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.157},
doi = {10.1109/WI.2005.157},
abstract = {A Comparison method for Web pages in terms of visual similarity is proposed. Conventional Web Informationretrieval/gathering systems, such as search engines, extract keywords from HTML source files, based on which the similarity between pages is calculated. The extracted keywords are considered as semantic features representing the contents of Web pages. On the other hand, visual feature of Web pages is as important as semantic feature, because HTML is designed for visualizing a Web page in understandable manner for humans. The proposed method compares the layouts of Web pages based on image processing and graph matching. The experimental results show that the accuracy of layout analysis is 91.6% in average, and the visual similarity calculated by the proposed method is closer to the visual judgment by test subjects than color-based comparison method.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {301–304},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.98,
author = {Li, Yuefeng and Murphy, Ben and Zhong, Ning},
title = {Mining Interesting Topics for Web Information Gathering and Web Personalization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.98},
doi = {10.1109/WI.2005.98},
abstract = {The quality of discovery patterns is crucial for building satisfactory systems of Web text mining. It is no doubt that we can find numerous frequent patterns from Web documents. However, there are many meaningless frequent patterns. This paper presents a novel method to improve the quality of discovered patterns. It generalizes discovered patterns into interesting topics in order to acquire the necessary useful information. The experimental results also verify the proposed method is promising.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {305–308},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.160,
author = {Li, Chun-hung and Kit, Chui-chun},
title = {Web Structure Mining for Usability Analysis},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.160},
doi = {10.1109/WI.2005.160},
abstract = {The interaction between usability and how a web site is structured is a complicated issue. In this paper, we discuss a web structure mining algorithm which allows the automatic extraction of navigational structures in a website without performing hypertext analysis. We perform several usability experiments to correlate the usability of web sites and the structural design of the web site. Experimental results show that the structure mining algorithm gives reasonable prediction about several design issues in web structure. The analysis serves as building block in the complex issue of web usability and structure mining.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {309–312},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.46,
author = {How, Bong Chih and Kulathuramaiyer, Narayanan and Kiong, Wong Ting},
title = {Categorical Term Descriptor: A Proposed Term Weighting Scheme for Feature Selection},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.46},
doi = {10.1109/WI.2005.46},
abstract = {This paper proposes a term weighing scheme, Categorical Term Descriptor (CTD), for feature selection in automated text categorization. CTD is an adatation of the Term Frequency Inverse Document Frequency (TFIDF). We compared the performance of the proposed method against classical methods such as Correlation Coefficient, Chi-Square and Information Gain using the Multinomial Na\"{\i}ve Bayes and the Support Vector Machine (SVM) classifiers on the Reuters [10] and Reuters [115] variants of Reuters-21578 dataset. Despite its simplicity, CTD has proven to be promising for both local and global feature selection CTD works best for the Reuters [10] as a stable local FS method.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {313–316},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.32,
author = {Cheng, Pu-Jen and Chiao, Hsin-Chen and Pan, Yi-Cheng and Chien, Lee-Feng},
title = {Annotating Text Segments in Documents for Search},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.32},
doi = {10.1109/WI.2005.32},
abstract = {It has been shown that annotating prominent text patterns contained in documents with appropriate types may benefit many applications. Most conventional tools for automatic text annotation extract named entities from texts and annotate them with information about persons, locations, dates and so on. However, this kind of entity type information is often short in length and is mostly limited to a small set of broader categories. In this paper, we try to remedy this problem by presenting an approach to extract global evidences from documents for improved named entity recognition. We also propose an unsupervised, generalized classification approach that collects training data from the Web automatically and classifies text patterns into more refined categories. Experimental results show the feasibility of the proposed approaches for search on the data of the NTCIR-2 information retrieval task.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {317–320},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.127,
author = {Zettsu, Koji and Tanaka, Katsumi},
title = {Referential Context Mining: Discovering Viewpoints from the Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.127},
doi = {10.1109/WI.2005.127},
abstract = {The Web is a vast playground for the propagation of information by individuals. The capability of Web users to leverage the value of Web content, in terms of factors such as usefulness, reputation, or reliability, has emerged as a requirement. The most significant characteristics of the Web are hyperlinks, which enables an author to refer to Web content published by other people. As a result, Web content can refer to other content on the Web in various contexts. The references provide important clues to understanding the roles or reputations of various Web pages according to the viewpoints of third parties. In this paper we propose an approach to mining referential contexts on the Web.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {321–325},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.56,
author = {Li, Hua-Fu and Lee, Suh-Yin and Shan, Man-Kwan},
title = {DSM-TKP: Mining Top-K Path Traversal Patterns over Web Click-Streams},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.56},
doi = {10.1109/WI.2005.56},
abstract = {Online, single-pass mining Web click streams poses some interesting computational issues, such as unbounded length of streaming data, possibly very fast arrival rate, and just one scan over previously arrived click-sequences. In this paper, we propose a new, single-pass algorithm, called DSM-TKP (Data Stream Mining for Top-K Path traversal patterns), for mining top-k path traversal patterns, where k is the desired number of path traversal patterns to be mined. An effective summary data structure called TKP-forest (Top-K Path forest) is used to maintain the essential information about the top-k path traversal patterns of the click-stream so far. Experimental studies show that DSM-TKP algorithm uses stable memory usage and makes only one pass over the streaming data.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {326–329},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.1,
author = {Myat, Nyeint Nyeint and Hla, Khin Haymar Saw},
title = {A Combined Approach of Formal Concept Analysis and Text Mining for Concept Based Document Clustering},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.1},
doi = {10.1109/WI.2005.1},
abstract = {Nowadays, the demand of conceptual document clustering is becoming increase to manage various types of vast amount of information published on the World Wide Web. In this paper, we use Formal Concept Analysis (FCA) method for clustering documents according to their formal contexts. Concept hierarchy of documents is built using the formal concepts of the documents in the document corpus. We use tf.idf (term frequency \texttimes{} inverse document frequency) term weighting model to reduce less useful concepts from these formal concepts and the association and correlation mining techniques to analyze the relationship of terms in the document corpus.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {330–333},
numpages = {4},
keywords = {association, correlation, conceptual document clustering, frequent termsets, formal concept analysis (FCA)},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.88,
author = {Quafafou, M. and Naouali, S. and Nachouki, G.},
title = {Knowledge Datawarehouse: Web Usage OLAP Application},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.88},
doi = {10.1109/WI.2005.88},
abstract = {Generally, OLAP analysis are based on both the observed data and a set of OLAP operators for restructuration and granularity modification. The goal is to discover patterns hidden into data. Unfortunately, this approach is also based on the analyst background. This latter assumes hypothesis according to his background and analyses data consequently: "hypothesis driven analysis". The integration of knowledge into datawarehouse conduce to enriched analysis context where objects and their relations are explicitly represented, handled and visualized. We investigate a deep integration where the basic datawarehouse\'{y}s operators consider both data and knowledge. This paper applies knowledge datawarehouse concept to web usage analysis.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {334–337},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.150,
author = {Tolksdorf, Robert and Bontas, Elena Paslaru and Nixon, Lyndon J. B.},
title = {Towards a Tuplespace-Based Middleware for the Semantic Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.150},
doi = {10.1109/WI.2005.150},
abstract = {The realization of the Semantic Web needs a set of specialized middleware as its infrastructure. In this paper we describe the principles of tuplespace computing, explain why tuplespaces are a suitable middleware for the Semantic Web, envision "Semantic Web Spaces" 1 and outline how our tuplespace platform XMLSpaces can be extended to support Semantic Web technologies, like RDF(S) and OWL.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {338–344},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.63,
author = {Wang, Ting and Maynard, Diana and Peters, Wim and Bontcheva, Kalina and Cunningham, Hamish},
title = {Extracting a Domain Ontology from Linguistic Resource Based on Relatedness Measurements},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.63},
doi = {10.1109/WI.2005.63},
abstract = {Creating domain-specific ontologies is one of the main bottlenecks in the development of the Semantic Web .. Learning an ontology from linguistic resources is helpful to reduce the costs of ontology creation. In this paper, we describe a method to extract the most related concepts from HowNet, a Chinese-English bilingual knowledge dictionary, in order to create a customized ontology for a particular domain. We introduce a new method to measure relatedness (rather than similarity between concepts), which overcomes some of the traditional problems associated with similar concepts being far apart in the hierarchy. Experiments show encouraging results.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {345–351},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.66,
author = {Gandon, Fabien},
title = {Generating Surrogates to Make the Semantic Web Intelligible to End-Users},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.66},
doi = {10.1109/WI.2005.66},
abstract = {The semantic Web is a vision of a Web augmented with formalized knowledge annotating it. Currently, there is a huge gap between the conceptual structures underlying the semantic Web and the final rendering of a user-interface enabling an end-user to peruse or act on part of it. We describe an approach we experimented to automate part of the process of generating representations for concepts mobilized in the semantic Web. We reuse the notion of surrogate from information retrieval and we show that surrogate patterns tend to be close to the patterns of identity conditions used in ontology engineering. From this observation we propose and discuss a mechanism to derive surrogate templates from structures found in ontologies and rules.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {352–358},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.25,
author = {Stojanovic, Nenad},
title = {An Approach for Defining Relevance in the Ontology-Based Information Retrieval},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.25},
doi = {10.1109/WI.2005.25},
abstract = {One of the vital problems in the searching for information is the ranking of the retrieved results, because users make typically very short queries (2-3 terms) and tend to consider only the first ten results. In traditional IR approaches the relevance of the results is determined only by analysing the underlying information repository (content and hyperlink structure), which leads to the weak relevance model. On the other hand, in the ontology-based IR the querying process is supported by an ontology such that other important sources for determining the relevance of results can be considered: the structure of the underlying domain and the characteristics of the searching process. In this paper we present a novel approach for determining relevance in ontology-based searching for information, which exploits the "full potential" of the semantics of such a semantically-based link structure.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {359–365},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.94,
author = {Berkovsky, Shlomo and Eytani, Yaniv and Gal, Avigdor},
title = {Measuring the Relative Performance of Schema Matchers},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.94},
doi = {10.1109/WI.2005.94},
abstract = {Schema matching is a complex process focusing on matching between concepts describing the data in heterogeneous data sources. There is a shift from manual schema matching, done by human experts, to automatic matching, using various heuristics (schema matchers). In this work, we consider the problem of linearly combining the results of a set of schema matchers. We propose the use of machine learning algorithms to learn the optimal weight assignments, given a set of schema matchers. We also suggest the use of genetic algorithms to improve the process efficiency.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {366–371},
numpages = {6},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.28,
author = {Wang, James Z. and Ali, Farha},
title = {An Efficient Ontology Comparison Tool for Semantic Web Applications},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.28},
doi = {10.1109/WI.2005.28},
abstract = {With the growing access to heterogeneous and independent data repositories, determining the semantic difference of two ontologies is critical in information retrieval, information integration and semantic web applications. In this paper, we propose an ontology comparison tool based on a novel senses refinement algorithm, which builds a senses set to accurately represent the semantics of the input ontology. The senses refinement algorithm automatically extracts senses from the electronic lexical database WordNet (locally installed or online), removes unnecessary senses based on the relationship among the entity classes of the ontology, and specifies relations and constraints of the concepts in the refined senses set. The senses refinement converts the measurement of ontology difference into simple set operations based on set theory, thus ensures the efficiency and accuracy of the ontology comparison. Our experimental studies show that the proposed senses refinement algorithm outperforms the naive senses set construction algorithm in terms of efficiency and accuracy},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {372–378},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.43,
author = {Abulaish, Muhammad and Dey, Lipika},
title = {Biological Ontology Enhancement with Fuzzy Relations: A Text-Mining Framework},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.43},
doi = {10.1109/WI.2005.43},
abstract = {Domain ontology can help in information retrieval from documents. But ontology is a pre-defined structure with crisp concept descriptions and inter-concept relations. However, due to the dynamic nature of the document repository, ontology should be upgradeable with information extracted through text mining of documents in the domain. This also necessitates that concepts, their descriptions and inter-concept relations should be associated with a degree of fuzziness that will indicate the support for the extracted knowledge according to the currently available resources. Supports may be revised with more knowledge coming in future. This approach preserves the basic structured knowledge format for storing domain knowledge, but at the same time allows for update of information. In this paper, we have proposed a mechanism which initiates text mining with a set of ontological concepts, and thereafter extracts fuzzy relations through text mining. Membership values of relations are functions of frequency of co-occurrence of concepts and relations. We have worked on the GENIA corpus and shown how fuzzy relations can be further used for guided information extraction from MEDLINE documents.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {379–385},
numpages = {7},
keywords = {Biological Information extraction, Fuzzy ontology, Text mining, Fuzzy relation},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.23,
author = {Yan, Baoshi},
title = {Aligning Class Hierarchies with Grass-Roots Class Alignment},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.23},
doi = {10.1109/WI.2005.23},
abstract = {The performance of an ontology alignment technique largely depends on the amount of information that can be leveraged for the alignment task. On the Semantic Web, end-users may explicitly or implicitly generate ontology alignments during their use of the semantic data. This kind of end-user-generated ontology alignment, which we call grass-roots ontology alignment, is an important source of information that is yet to be taken into account by current ontology alignment techniques. Grass-roots ontology alignment, often generated as a side effect of other data manipulations, could be user-specific, task-specific, approximate, or even contradictory. This paper reports our work on reusing grass-roots class alignment for aligning class hierarchies. A grass-roots class alignment, though approximate, still reveals some facts about relationships between different classes. We formalize facts about class relationships that can be inferred from an alignment under different cases. We then apply forward-chaining inference to the facts knowledge base to infer more facts. The facts KB is then leveraged for ontology alignment purposes. To deal with uncertainty and inconsistency, each fact is associated with an evidence that tells how the fact is obtained. The evidences are used to select better-supported facts in case of inconsistency.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {386–392},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.73,
author = {Adamku, Gergely and Stuckenschmidt, Heiner},
title = {Implementation and Evaluation of a Distributed RDF Storage and Retrieval System},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.73},
doi = {10.1109/WI.2005.73},
abstract = {It is widely agreed that the Semantic Web will be build on RDF. Therefore the success of the Semantic Web also depends on the availability of a scalable and reliable infrastructure for storing and accessing RDF data. A number of storage and retrieval systems have been developed recently, but despite the inherently distributed nature of the Semantic Web most of these systems do not support distributed storage and retrieval of data. In this paper we report our experiences with implementing a distributed storage and query infrastructure on top of an existing RDF infrastructure. We present the system architecture and discuss performance issues based on a set of experiments with the infrastructure. We conclude with an identification of remaining performance bottlenecks and point to further improvements.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {393–396},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.90,
author = {Yang, Liu and Li, Guojie and Shi, Zhongzhi},
title = {Learning from Ontologies for Common Meaningful Structures},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.90},
doi = {10.1109/WI.2005.90},
abstract = {We put forward a hypothesis that there exist common meaningful structures among ontologies whose domains are analogous to each other. The initial motivation of our hypothesis is to make full use of the structural information in existing ontologies, in order to benefit the domain of ontology. To verify the hypothesis we give a precise definition of the candidate of the common meaningful structure called MICISO (Maximum Isomorphic Common Induced Sub-Ontology). Based on the hypothesis and the definition we present a novel data mining problem called MICISO mining, whose aim is learning from ontologies to find out MICISOs and further recommend the common meaningful structures. We also provide an algorithm for MICISO mining, based on which we have developed a practical tool for mining and checking such structures. With the tool, the algorithm is implemented with quite a few pairs of existing ontologies, and the interesting meaningful results support our hypothesis. Thus we consider that the hypothesis is preliminarily verified. We suppose that our work will spark a novel promising thinking for the domain of ontology - to study existing ontologies for useful things.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {397–400},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.78,
author = {Labsky, Martin and Svatek, Vojtech and Svab, Ondrej and Praks, Pavel and Kratky, Michal and Snasel, Vaclav},
title = {Information Extraction from HTML Product Catalogues: From Source Code and Images to RDF},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.78},
doi = {10.1109/WI.2005.78},
abstract = {We describe an application of information extraction from company websites focusing on product offers. A statistical approach to text analysis is used in conjunction with different ways of image classification. Ontological knowledge is used to group the extracted items into structured objects. The results are stored in an RDF repository and made available for structured search.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {401–404},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.20,
author = {Willmott, Steven and Pena, Felix Oscar Fernandez and Merida-Campos, Carlos and Constantinescu, Ion and Dale, Jonathan and Cabanillas, David},
title = {Adapting Agent Communication Languages for Semantic Web Service Inter-Communication},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.20},
doi = {10.1109/WI.2005.20},
abstract = {The integration of Semantic Web and Web Services technologies promises to be one of the most promising new areas for development of Intelligent Web Applications. One challenging area where these technologies meet is in explicit definitions of meaning for the messages exchanged between Web Services - in other words, semantic definitions of the meanings ofdata/commands exchanged in the execution of a Web Services based application. While current approaches such as OWL-S tackle these elements in service groundings by mapping processes to function calls with specific arguments, Agent Communication Languages could provide a potentially richer alternative. The work presented here shows how this could be done by mapping the existing Agent Communication Language (FIPA-ACL, FIPA-SL and associated standards developed by the Foundation for Intelligent Physical Agents) into OWL based representations which may then be readily used in a Web Services environment.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {405–408},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.133,
author = {Hakkarainen, Sari E. and Kofod-Petersen, Anders and Aranda, Carlos Buil},
title = {Situated Support for Choice of Representation},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.133},
doi = {10.1109/WI.2005.133},
abstract = {As more and more companies are augmenting their data to include semantics it is imperative that the choices made when choosing the modelling language are well founded in knowledge about the language and the domain in question. This work demonstrates how the Semiotic Quality Framework can facilitate the choice of the most suited language for a real world application. Computational and situated features are introduced as an extension to the framework.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {409–412},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.34,
author = {Abrahams, Brooke and Dai, Wei},
title = {Architecture for Automated Annotation and Ontology Based Querying of Semantic Web Resources},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.34},
doi = {10.1109/WI.2005.34},
abstract = {The semantic Web provides the foundation for semantic architecture to support the transparent exchange of information and knowledge among collaborating e-business organizations. Recent advances in semantic Web based technologies offer means for organizations to exchange knowledge in a meaningful way [1]. In spite of these developments, major challenges remain for developers of semantic Web applications, such as the availability of semantic Web content [2], and ontology based information retrieval [3]. In this paper, an architecture aimed at addressing these issues is presented. An easy to use annotation tool is deployed, providing a convenient mechanism for Web site owners to mark up their Web pages with RDF metadata. Search and coordination activities are carried out by a system of multiagents designed for such environments. The architecture is demonstrated in the Accommodation services domain of the Australian Tourism Industry.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {413–417},
numpages = {5},
keywords = {Ontology, Semantic Web, Annotation, RDF, Multiagent Querying, OWL},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.135,
author = {Dunkel, Jurgen and Bruns, Ralf},
title = {Software Architecture of Advisory Systems Using Agent and Semantic Web Technologies},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.135},
doi = {10.1109/WI.2005.135},
abstract = {In this paper, we present the software architecture of a new generation of advisory systems using Intelligent Agent and Semantic Web technologies. Software agents act as clients and advisors to implement negotiation processes in a consultancy situation. The domain knowledge is modeled with the OWL ontology language. Using an inference engine the agents reason on base of their knowledge to make decisions or proposals. The agent knowledge consists of private data and publicly accessible knowledge spread over different web sites. As in a real consultancy an agent only reveals sensitive private data, if it is indispensable for finding a solution. Depending on the actual consultancy situation each agent dynamically expands its knowledge base by accessing OWL knowledge sources from the Internet.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {418–421},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.122,
author = {Yoo, Seung Yeol and Hoffmann, Achim},
title = {Pseudo-Relevance Feedback in Web Information Retrieval Using Segments' Subjective Importance Values},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.122},
doi = {10.1109/WI.2005.122},
abstract = {To make web search more effective, we address the problem of articulating a user\'{y}s information needs more effectively. This is done in an iterative way, by allowing the user to provide relevance feedback regarding individual segments of retrieved Web-pages. Previously applied methods are limited to discovering \'{y}general importance values of segments\'{y} (based on the authors\'{y} \'{y}objective views\'{y} i.e., main topics) rather than \'{y}subjective importance values of segments\'{y} (based on a user\'{y}s \'{y}subjective view\'{y} i.e., personal information needs). In this paper, a user\'{y}s interests are incrementally identified by allowing the user to iteratively select relevant keywords or phrases from a set of system-recommended candidate-keywords and candidate-phrases (i.e., pseudo-relevance feedback). It makes it possible to discover \'{y}subjective importance values of segments\'{y} that can be dynamically changed by the user by indicating their interests regarding retrieved Web-pages. The important segments, selected by the user, provide higher precision of pseudo-relevance feedback for further Web Information Retrieval purposes.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {422–425},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.15,
author = {Khan, Javed I. and Hardas, Manas and Ma, Yongbin},
title = {A Study of Problem Difficulty Evaluation for Semantic Network Ontology Based Intelligent Courseware Sharing},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.15},
doi = {10.1109/WI.2005.15},
abstract = {Testing and evaluation is an integral part of the learning process. Educators have often tried to device methods for design of test-ware. Intelligent design and compilation of test-ware is a very interesting problem with immense applications. This research aims at automating the process of intelligent design of test-ware by providing qualitative assessment of questions. In this attempt, we provide some synthetic parameters for the evaluation of question in its concept space. The parameters are tested in some real world scenarios and intuitive inferences are deduced predicting the performance of the parameters. It is observed that the difficulty of a question is often a function of the concepts it tests. Concept knowledge can be represented in the form of linked concepts in semantic nets, the link representing the relationships between the concepts. If this directed graph is known, complexity of a question can be computed by synthetic means.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {426–429},
numpages = {4},
keywords = {semantic web, test-ware digital library, automatic composition},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.92,
author = {Rogozan, Delia and Paquette, Gilbert},
title = {Managing Ontology Changes on the Semantic Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.92},
doi = {10.1109/WI.2005.92},
abstract = {Although the ontology evolution plays a key role in the Semantic Web, methods and tools to support it are missing. Thus, this paper proposes a component-based framework for managing ontology changes. The main functionalities of the OntoAnalyzer framework are: (1) to track changes and to formalize them using a language that we propose for representing ontology changes and (2) to identify changes a posteriori to ontology evolution and to analyze their effect on the ontology-based annotation of resources.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {430–433},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.64,
author = {Revel, Arnaud},
title = {From Robots to Web-Agents: Building Cognitive Software Agents for Web-Information Retrieval by Taking Inspiration from Experience in Robotics},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.64},
doi = {10.1109/WI.2005.64},
abstract = {In this article, web-search mobile agents inspired by ethology and robotics are presented. 2 strategies are compared: 1st uses pheromones to mark the route to a given goal; 2nd involves a cognitive map to internalize a representation of the web. The implementation on a set of FIPAOS platforms is given and unintuitive results concerning agent mobility are given.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {434–437},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.93,
author = {Nachouki, Gilles and Quafafou, Mohamed and Chastang, Marie-Pierre},
title = {MDSManager: A System Based on Multidatasource Approach for Data Integration},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.93},
doi = {10.1109/WI.2005.93},
abstract = {In this paper, we show the design of MDSManager a system based on a multidatasource approach for data integration. MDSManager uses a multidatasource language called EXQ (Extended XQuery). EXQ is designed in order to access and interconnect multiple conflicting static data sources (included Databases, XML, HTML), and/or active data sources (included distinct services like Java Classes, C programs, Web Services etc.).},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {438–441},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.51,
author = {Blanchet, Warren and Elio, Renee and Stroulia, Eleni},
title = {Conversation Errors in Web Service Coordination: Run-Time Detection and Repair},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.51},
doi = {10.1109/WI.2005.51},
abstract = {Organizations that own web services participating in a workflow composition may evolve their components independently. Service coordination can fail when previously legal messages between independently changing, distributed components become illegal because their respective workflow models are no longer synchronized. This paper presents an intelligent-agent framework that wraps a web service in a conversation layer and a simple workflow-adaptation function. The conversation layer implements protocols and consults globally shared, declarative policy specifications to resolve interaction failures. The framework allows agents to resolves various model mismatches that cause interaction errors, including changes to required preconditions, partners, and expected message ordering. Implications of this distributed approach to web service coordination are also discussed.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {442–449},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.84,
author = {Du, Zongxia and Huai, Jinpeng and Liu, Yunhao and Hu, Chunming and Lei, Lei},
title = {IPR: Automated Interaction Process Reconciliation},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.84},
doi = {10.1109/WI.2005.84},
abstract = {Inter-organizational business processes usually require complex and time-consuming interactions between partners than simple interactions supported by WSDL. Automated reconciliation is essential to enable dynamic inter-organizational business collaboration. To the best of our knowledge, however, there is not a practical automated reconciliation algorithm available. In this paper, we propose a practical automated reconciliation algorithm, called IPR (Interaction Process Reconciliation) based on Petri Net, which is able to effectively facilitate dynamic interactions among trading partners in a peer-to-peer fashion. We implement a prototype IPR server in our lab, and evaluate our design by comprehensive experiments. Results show that IPR significantly outperforms existing approaches in terms of matching success rate, response time, and matching efficiency.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {450–456},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.59,
author = {Chirichiello, Antonella and Salaun, Gwen},
title = {Encoding Abstract Descriptions into Executable Web Services: Towards a Formal Development},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.59},
doi = {10.1109/WI.2005.59},
abstract = {It is now widely accepted that formal methods are helpful for many issues raised in the web services area. In this paper, we advocate the use of process algebra as a first step in the design and development of executable web services. From such formal descriptions, reasoning tools can be used to validate their correct execution. We define some guidelines to encode abstract specifications of services-to-be written using these calculi into executable web services. As a back-end language, we consider the standard orchestration language BPEL. We illustrate our approach through the development of an e-business application.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {457–463},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.54,
author = {Paurobally, Shamimabi and Jennings, Nicholas R.},
title = {Developing Agent Web Service Agreements},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.54},
doi = {10.1109/WI.2005.54},
abstract = {Web services have emerged as a new paradigm that supports loosely-coupled distributed systems in service discovery and service execution. Next generation web services will evolve from performing static invocations to engaging in flexible interactions and negotiations for dynamic resource procurement. To this end, this paper applies an agent-oriented based approach over a recent web service language, WS-Agreement, in order to facilitate conversations of sufficient expressiveness between adaptive and autonomous services. We discuss how such agent web service agreements can be implemented over IBM\'{y}s Emerging Technologies Toolkit (ETTK) that itself includes an implementation of the WS-Agreement specification.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {464–470},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.81,
author = {Savarimuthu, Bastin Tony Roy and Purvis, Maryam and Purvis, Martin and Cranefield, Stephen},
title = {Integrating Web Services with Agent Based Workflow Management System (WfMS)},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.81},
doi = {10.1109/WI.2005.81},
abstract = {Rapid changes in the business environment call for more flexible and adaptive workflow systems. Researchers have proposed that Workflow Management Systems (WfMSs) comprising multiple agents can provide these capabilities. We have developed a multi-agent based workflow system, JBees, which supports distributed process models and the adaptability of executing processes. Modern workflow systems should also have the flexibility to integrate available web services as they are updated. In this paper we discuss how our agent-based architecture can be used to bind and access web services in the context of executing a workflow process model. We use an example from the diamond processing industry to show how our agent architecture can be used to integrate web services with WfMSs.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {471–474},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.109,
author = {Huang, He and Shi, Zhongzhi and Qiu, Lirong and Cheng, Yong},
title = {Ontology-Driven Knowledge Management on the Grid},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.109},
doi = {10.1109/WI.2005.109},
abstract = {The combination of large data set size, geographic distribution of resources and users, and sophisticated applications on data and information needs robust infrastructures. In this paper, Knowledge Management Sphere (KMSphere) is proposed and developed to explore important aspects of service-oriented and ontology-driven knowledge management on the grid. The main idea of KMSphere is to integrate ontologies with a service-oriented grid, build a knowledge space on top of databases, and then organize, utilize, and manage the knowledge resources in that space.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {475–478},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.102,
author = {Lawley, Richard and Luck, Michael and Moreau, Luc},
title = {Modelling and Simulating Chained Negotiation to Enable Sharing of Notifications},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.102},
doi = {10.1109/WI.2005.102},
abstract = {Notification services (NSs) are middleware components providing asynchronous message delivery between publishers and consumers. Multiple interconnected NSs form a distributed NS, with each NS routing notifications between publishers and consumers at different locations, enabling consumers to share subscriptions, reducing the number of messages sent. Consumers can specify Quality of Service (QoS) levels when subscribing to a NS, using negotiation to find QoS levels acceptable to both parties. However, if consumers specify sufficiently different QoS levels,notifications cannot be shared and new subscriptions must be made. Chained negotiation can be used to negotiate QoS levels through intermediate NSs, enabling the reuse of existing subscriptions for additional consumers. In this paper, we present a chained negotiation engine, evaluating its performance and behaviour, showing that it enables negotiation over QoS while still sharing notifications, and that it provides better results for a consumer by negotiation directly with the publisher.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {479–482},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.50,
author = {Shi, Yuliang and Zhang, Liang and Liu, Fangfang and Lin, Lili and Shi, Baile},
title = {Compatibility Analysis of Web Services},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.50},
doi = {10.1109/WI.2005.50},
abstract = {The compatibility analysis is absolutely necessary for guaranteeing the correct composition of web services, no matter what styles the composition takes, statically or dynamically. In this paper, we provide a formalization of web services behavior using the approach of automata. With this understanding, we propose a definition of role among web services interactions. As a result, we can check whether two or more web services are compatible in collaboration or not.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {483–486},
numpages = {4},
keywords = {Automata, Compatibility, Web services},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.156,
author = {Moulin, Claude and Sbodio, Marco},
title = {Using Ontological Concepts for Web Service Composition},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.156},
doi = {10.1109/WI.2005.156},
abstract = {his paper describes an approach for a composition of web services based on their semantic descriptions. The process section of OWL-S service descriptions is built with references to ontology concepts which represent service input and output data types. We present a engine that receives a request containing a concept (OC) corresponding to a service output and a set of concepts (ICs) corresponding to a service inputs. The engine produces a sequence of services whose first element has ICs as inputs and whose last element has OC as output. The result of the composition is described as a BPEL process.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {487–490},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.123,
author = {Willmott, Steven and Ronsdorf, Heiko and Krempels, Karl Heinz},
title = {Publish and Search versus Registries for Semantic Web Service Discovery},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.123},
doi = {10.1109/WI.2005.123},
abstract = {As Service Descriptions formalisms increase in maturity, significant issues remain regarding how the description instances will actually be published, shared and discovered. By way of discussion and description of a prototype Service Description Search service, this paper discusses the distinction between the registry based approaches currently underway for service discovery and more open "publish and search" metaphors used by the current generation web.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {491–494},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.145,
author = {van Splunter, Sander and van Langen, Pieter H. G. and Brazier, Frances M. T.},
title = {The Role of Local Knowledge in Complex Web Service Reconfiguration},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.145},
doi = {10.1109/WI.2005.145},
abstract = {As the number of web services in repositories on the World Wide Web increases so will the number of complex configurations of web services. However, as the World Wide Web is dynamic, web services will come and go, temporarily or for good. As a result, complex web service configurations will need to be reconfigured on demand. To this purpose, complex web service configurations need to include local knowledge about (1) the function, structure and behaviour of each component in a configuration, and (2) the dependencies between components at each level of composition. Templates are proposed as a means to represent such knowledge. To illustrate the process of reconfiguration, an example is given of reconfiguration of a complex web service, for which a template is used specifying both types of local knowledge.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {495–499},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.45,
author = {Li, Keqiu and Tajima, Keishi and Shen, Hong},
title = {Cache Replacement for Transcoding Proxy Caching},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.45},
doi = {10.1109/WI.2005.45},
abstract = {In this paper, we address the problem of cache replacement for transcoding proxy caching. First, an efficient cache replacement algorithm is proposed. Our algorithm considers both the aggregate effect of caching multiple versions of the same multimedia object and cache consistency. Second, a complexity analysis is presented to show the efficiency of our algorithm. Finally, some preliminary simulation experiments are conducted to compare the performance of our algorithm with some existing algorithms. The results show that our algorithm outperforms others in terms of the various performance metrics.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {500–507},
numpages = {8},
keywords = {proxy caching, cache replacement, multimedia, Transcoding, Internet},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.52,
author = {Wang, James Z. and Bhulawala, Vipul},
title = {Design and Implementation of a P2P Cooperative Proxy Cache System},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.52},
doi = {10.1109/WI.2005.52},
abstract = {In this paper, we design and implement a P2P cooperative proxy caching system based on a novel P2P cooperative proxy caching scheme. To effectively locate the cached web documents, a TTL-based routing protocol is proposed to manage the query and response messages in the P2P cooperative proxy cache system. Furthermore, we design a predict query-route algorithm to improve the TTL-based routing protocol by adding extra information in the query message packets. Our performance studies demonstrate that the proposed message routing protocols significantly improve the performance of the P2P cooperative proxy cache system, in terms of cache hit ratio, byte hit ratio, user request latency, and the number of query messages generated in the proxy cache system, compared to the flooding based message routing protocol.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {508–514},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.161,
author = {Liu, Tie-Yan and Ma, Wei-Ying},
title = {Webpage Importance Analysis Using Conditional Markov Random Walk},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.161},
doi = {10.1109/WI.2005.161},
abstract = {In this paper, we propose a novel method to calculate the webpage importance based on a conditional Markov random walk model. The main assumption in this model is that given the hyperlinks in a webpage, users are not really randomly clicking one of them. Instead, many factors may bias their behaviors, for example, the anchor text, the content relevance and the previous experiences when visiting the website that a destination pages belongs to. As one of the results, the user might tend to visit those pages in high-quality websites with higher probability. To implement this idea, we reformulate the Web graph to be a two-layer structure, and the webpage importance is calculated by conditional random walk in this new Web graph. Experiments on the topic distillation task of TREC 2003 Web track showed that our new method can achieve about 18% improvement on mean average precision (MAP) and 16% on precision at 10 (P@10) over the PageRank algorithm.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {515–521},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.166,
author = {Padmanabhan, Divya and Desikan, Prasanna and Srivastava, Jaideep and Riaz, Kashif},
title = {WICER: A Weighted Inter-Cluster Edge Ranking for Clustered Graphs},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.166},
doi = {10.1109/WI.2005.166},
abstract = {Several algorithms based on link analysis have been developed to measure the importance of nodes on a graph such as pages on the World Wide Web. PageRank and HITS are the most popular ranking algorithms to rank the nodes of any directed graph. But, both these algorithms assign equal importance to all the edges and nodes, ignoring the semantically rich information from nodes and edges. Therefore, in the case of a graph containing natural clusters, these algorithms do not differentiate between inter-cluster edges and intra-cluster edges. Based on this parameter, we propose a WeightedInter-Cluster Edge Ranking for clustered graphs that weighs edges (based on whether it is an inter-cluster or an intra-cluster edge) and nodes (based on the number of clusters it connects). We introduce a parameter 'α' which can be adjusted depending on the bias desired in a clustered graph. Our experiments were two fold. We implemented our algorithm to relationship set representing legal entities and documents and the results indicate the significance of the weighted edge approach. We also generated biased and random walks to quantitatively study the performance.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {522–528},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.29,
author = {Rigutini, Leonardo and Maggini, Marco and Liu, Bing},
title = {An EM Based Training Algorithm for Cross-Language Text Categorization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.29},
doi = {10.1109/WI.2005.29},
abstract = {Due to the globalization on the Web, many companies and institutions need to efficiently organize and search repositories containing multilingual documents. The management of these heterogeneous text collections increases the costs significantly because experts of different languages are required to organize these collections. Cross-Language Text Categorization can provide techniques to extend existing automatic classification systems in one language to new languages without requiring additional intervention of human experts. In this paper we propose a learning algorithm based on the EM scheme which can be used to train text classifiers in a multilingual environment. In particular, in the proposed approach, we assume that a predefined category set and a collection of labeled training data is available for a given language L1. A classifier for a different language L2 is trained by translating the available labeled training set for L1 to L2 and by using an additional set of unlabeled documents from L2. This technique allows us to extract correct statistical properties of the language L2 which are not completely available in automatically translated examples, because of the different characteristics of language L1 and of the approximation of the translation process. Our experimental results show that the performance of the proposed method is very promising when applied on a test document set extracted from newsgroups in English and Italian.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {529–535},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.148,
author = {Fujii, Atsushi and Ishikawa, Tetsuya},
title = {Toward the Automatic Compilation of Multimedia Encyclopedias: Associating Images with Term Descriptions on the Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.148},
doi = {10.1109/WI.2005.148},
abstract = {To generate content for multimedia encyclopedias, we propose a method for searching theWeb, seeking images associated with a specific word sense. We use text in an HTML file that links to an image as a pseudo-caption for the image, enabling text-based indexing and retrieval.We use term descriptions in a Web search site called "CYCLONE" as queries and match images and texts based on word senses. We show the effectiveness of our method experimentally.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {536–542},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.21,
author = {Yu, Philip S. and Li, Xin and Liu, Bing},
title = {Adding the Temporal Dimension to Search " A Case Study in Publication Search},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.21},
doi = {10.1109/WI.2005.21},
abstract = {The most well known search techniques are perhaps the PageRank and HITS algorithms. In this paper we argue that these algorithms miss an important dimension, the temporal dimension. Quality pages in the past may not be quality pages now or in the future. These techniques favor older pages because these pages have many in-links accumulated over time. New pages, which may be of high quality, have few or no in-links and are left behind. Research publication search has the same problem. If we use the PageRank or HITS algorithm, those older or classic papers will be ranked high due to the large number of citations that they received in the past. This paper studies the temporal dimension of search in the context of research publication. A number of methods are proposed to deal with the problem based on analyzing the behavior history and the source of each publication. These methods are evaluated empirically. Our results show that they are highly effective.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {543–549},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.9,
author = {Fouss, Francois and Pirotte, Alain and Saerens, Marco},
title = {A Novel Way of Computing Similarities between Nodes of a Graph, with Application to Collaborative Recommendation},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.9},
doi = {10.1109/WI.2005.9},
abstract = {This work presents a new perspective on characterizing the similarity between elements of a database or, more generally, nodes of a weighted, undirected, graph. It is based on a Markov-chain model of random walk through the database. The suggested quantities, representing dissimilarities (or similarities) between any two elements, have the nice property of decreasing (increasing) when the number of paths connecting those elements increases and when the "length" of any path decreases. The model is evaluated on a collaborative recommendation task where suggestions are made about which movies people should watch based upon what they watched in the past. The model, which nicely fits into the so-called "statistical relational learning" framework as well as the "link analysis" paradigm, could also be used to compute document or word similarities, and, more generally, could be applied to other database or web mining tasks.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {550–556},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.103,
author = {Perkio, Jukka and Tuulos, Ville and Buntine, Wray and Tirri, Henry},
title = {Multi-Faceted Information Retrieval System for Large Scale Email Archives},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.103},
doi = {10.1109/WI.2005.103},
abstract = {We profile a system for search and analysis of large-scale email archives. The system builds around four facets: Content-based search engine, statistical topic model, automatically inferred social networks and time-series analysis. The facets correspond to the types of information available in email data. The presented system allows chaining or combining the facets flexibly. Results of one facet may be used as input to another, yielding remarkable combinatorial power. In information retrieval point of view, the system provides support for exploration, approximate textual searches and data visualization. We present some experimental results based on a large real-world email corpus.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {557–564},
numpages = {8},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.106,
author = {Rocha, Luis M. and Simas, Tiago and Rechtsteiner, Andreas and Di Giacomo, Mariella and Luce, Richard},
title = {MyLibrary@LANL: Proximity and Semi-Metric Networks for a Collaborative and Recommender Web Service},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.106},
doi = {10.1109/WI.2005.106},
abstract = {We describe a network approach to building recommendation systems for a Web service. We employ two different types of weighted graphs in our analysis and development: Proximity graphs, a type of Fuzzy Graphs based on a co-occurrence probability, and semi-metric distance graphs, which do not observe the triangle inequality of Euclidean distances. Both types of graphs are used to develop intelligent recommendation and collaboration systems for the MyLibrary@LANL web service, a user-centered front-end to the Los Alamos National Laboratory\'{y}s digital library collections and Web resources.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {565–571},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.72,
author = {Dai, Ying and Cai, Dawei},
title = {Imagery-Based Digital Collection Retrieval on Web Using Compact Perception Features},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.72},
doi = {10.1109/WI.2005.72},
abstract = {In this paper, with the objective to retrieving digital collections more intuitively and flexible, we proposed an approach of image-based digital collection retrieval on web based on compact perception features. For this, the eigen and difference SGLD (Space Gray Level Dependence) matrices were used to extract the features of images. The associations of the extracted features with the human imagery, which can be described by the semantic, color, and structural characteristics, were analyzed. On this basis, the system of flexible image retrieval with human like performance was implemented on web, which retrieved images by the flexible combination of query-by-sample, query-by-perception words, or query-by-impression words, and re-ranked retrieved images according to the adjustment of individual\'{y}s similarity criteria threshold. The user satisfaction-based evaluation illustrated the good performance of the proposed system},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {572–576},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.100,
author = {Huang, Yin-Fu and Hsu, Jhao-Min},
title = {Mining Web Logs to Improve Hit Ratios of Prefetching and Caching},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.100},
doi = {10.1109/WI.2005.100},
abstract = {In the Internet, proxy servers play the key roles between users and web sites, which could reduce the response time of user requests and save network bandwidth. Basically, an efficient buffer manager should be built in a proxy server to cache frequently accessed documents in the buffer, thereby achieving better response time. In the paper, we developed an access sequence miner to mine popular surfing 2-sequences with their conditional probabilities from the proxy log, and stored them in the rule table. Then, according to buffer contents and the rule table, a prediction-based buffer manager also developed here will make appropriate actions such as document caching, document prefetching, and even cache/prefetch buffer size adjusting to achieve better buffer utilization. Through the simulation, we found that our approach has much better performance than the other ones, in the quantitative measures such as hit ratios and byte hit ratios of accessed documents.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {577–580},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.162,
author = {Voutsakis, Epimenides and Petrakis, Euripides G. M. and Milios, Evangelos},
title = {Weighted Link Analysis for Logo and Trademark Image Retrieval on the Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.162},
doi = {10.1109/WI.2005.162},
abstract = {Image retrieval on the Web requires that important (authoritative) images satisfying the query selection criteria are assigned higher ranking over other relevant images. PicASHOW [5] achieves this goal using link information alone. This work proposes WPicASHOW (Weighted PicASHOW) a weighted scheme for co-citation analysis that incorporates within the link analysis method of PicASHOW the text and image content of the queries and of the Web pages. WPicASHOW is implemented and integrated into a fully automated Web retrieval system for logo and trademark images.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {581–585},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.16,
author = {Delort, Jean-Yves},
title = {A User-Centered Approach for Evaluating Query Expansion Methods},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.16},
doi = {10.1109/WI.2005.16},
abstract = {Search engines are powerful tools to find information on the Web. However, they commonly return many irrelevant documents when the users\'{y} queries are not specific enough. To refine the scope of their searches, refinement terms are sometimes recommended to the users by query expansion systems (QES). A recent wide-scale survey has shown that users seldom include these terms in their queries. In this article, we propose a new user-centric approach for evaluating QES. The purpose of our approach is to assess the suggestive power of the suggested terms. Several existing QES are compared following the proposed criteria.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {586–589},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.167,
author = {Jan, Yi-Wei and Tsay, Jyh-Jong and Wu, Bo-Liang},
title = {WISE: A Visual Tool for Automatic Extraction of Objects from World Wide Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.167},
doi = {10.1109/WI.2005.167},
abstract = {Automatic identification and extraction of interesting objects instead of just related Web pages is useful to users of the World Wide Web. For example, an instructor teaching a Data Mining course may be interested in finding all text books, lecture notes, slides, homeworks and term projects from Web pages of all Data Mining and their related courses. In this paper, we present a tool WISE for extraction of objects from web pages, which are of interest. In WISE, objects in HTML pages are defined as nodes in an extension of DOM trees. Extraction of objects is then formulated as the problem of classification of tree nodes. We develop a mechanism for object specifications, and a specific-to-general (bottom-up) search algorithm for learning object specifications. Experiment shows that WISE works well for extracting objects such as, for example, extracting items in paper lists, and extracting lists of lecture notes in Algorithms courses.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {590–593},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.83,
author = {Ralha, Celia Ghedini and Ralha, Jose Carlos Loureiro and Costa, Charles Antonio Nascimento and da Rocha, Augusto Arcoverde},
title = {Intelligent Web Information Mapping with HyperMap},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.83},
doi = {10.1109/WI.2005.83},
abstract = {This paper addresses some particular issues related to the difficult task of automatic mapping of Web information to help the user to find interesting and unexpected information on the Web. The approach puts together qualitative and quantitative reasoning. The qualitative reasoning is done through a spatial mereotopological calculus since we metaphorically see Web sites as space regions. As a result, documents/sites are grouped together into classes according to the spatial relation they satisfy. Quantitative reasoning is done through Information Retrieval techniques allowing users to choose a document based on a similarity measure between documents/sites. Experiments have been undertaken in two directions: to find an intuitive model to display the Web map through the development of a system called HyperMap and to discover interesting and unexpected information from the spatial Web maps metaphor.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {594–597},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.79,
author = {Bai, Jing and Nie, Jian-Yun and Cao, Guihong},
title = {Integrating Compound Terms in Bayesian Text Classification},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.79},
doi = {10.1109/WI.2005.79},
abstract = {Text classification usually assumed a word-based document representation. In this paper, we propose a new approach to integrate compound terms in Bayesian text classification. Compound terms are used as complementary features to single words. An acute problem is to consider their dependence with the component words. In this paper, we propose to use smoothing techniques to combine both compound term and word representations. Experiments have been conducted on two corpora. Our results show that this approach can slightly but steadily improve the classification performance on both test corpora.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {598–601},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.95,
author = {Prime-Claverie, Camille and Beigbeder, Michel and Lafouge, Thierry},
title = {Metadata Propagation in the Web Using Co-Citations},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.95},
doi = {10.1109/WI.2005.95},
abstract = {Given the large heterogeneity of the World Wide Web, using metadata on the search engines side seems to be a useful track for information retrieval. Though, because a manual qualification at the Web scale is not accessible, this track is little followed. We propose a semi-automatic method for propagating metadata. In a first step, homegeneous corpus are extracted. We used in our study the following properties: the authority type, the site type, the information type, and the page type. This first step is realized by a clusterization which uses a similarity measure based on the co-citation frequency between pages. Given the cluster hierarchy, the second step selects a reduced number of documents to be manually qualified and propagates the given metadata values to the other documents belonging to the same cluster. A qualitative evaluation and a preliminary study about the scalability of this method are presented.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {602–605},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.119,
author = {Sung, Li-Chun and Kuo, Chin-Hwa and Chen, Meng Chang and Sun, Yeali},
title = {Progressive Analysis Scheme for Web Document Classification},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.119},
doi = {10.1109/WI.2005.119},
abstract = {In this paper, a web document classification scheme, Progressive Analysis Scheme (PAS) is proposed to efficiently and effectively classify HTML web documents. When an author writes a web document, HTML tags are used to visually emphasize the texts related to main concepts. The design of PAS is to catch the authoring convention in terms of the contributions of nested HTML tags to document classification. During the learning phase, PAS provides an enhanced tag sequence model to resolve the sample lacking problem in learning the classification contributions of HTML tag sequences. While in classification phase, PAS decomposes a web document into regions based on the DOM tag-tree, and analyzes the regions in the descending order of their classification contributions. PAS also provides a mechanism called emphasis degree adjustment to defer the processing of noisy region during classification. The simulation results shows that PAS has better performance than full-text (e.g. SVM) and sequential classifier.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {606–609},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.61,
author = {Wong, Wilson and Sahib, Shahrin and Goh, Ong-Sing},
title = {Evaluation of Response Quality for Heterogeneous Question Answering Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.61},
doi = {10.1109/WI.2005.61},
abstract = {The research in this paper makes explicit why existing measures for response quality evaluation is not suitable for the ever-evolving field of question answering and following that, a short-term solution for evaluating response quality of heterogeneous systems is put forward. To demonstrate the challenges in evaluating systems of different nature, this research presents a black-box approach using a classification scheme and scoring mechanism to assess and rank three example systems.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {610–613},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.112,
author = {Massa, Paolo and Hayes, Conor},
title = {Page-ReRank: Using Trusted Links to Re-Rank Authority},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.112},
doi = {10.1109/WI.2005.112},
abstract = {Search engines like Google.com use the the link structure of the Web to determine whether web pages are authoritative sources of information. However, the linking mechanism provided by HTML does not allow the web author to express different types of links, such as positive or negative endorsements of page content. As a consequence, search engine algorithms cannot discriminate between sites that are highly linked and sites that are highly trusted. We demonstrate our claim by running PageRank on a real world data set containing positive and negative links. We conclude that simple semantic extensions to the link mechanism would provide a richer semantic network from which to mine more precise Web Intelligence.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {614–617},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.74,
author = {Suryavansh, Bhushan Shankar and Shiri, Nematollaah and Mudur, Sudhir P.},
title = {Improving the Effectiveness of Model Based Recommender Systems for Highly Sparse and Noisy Web Usage Data},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.74},
doi = {10.1109/WI.2005.74},
abstract = {A number of approaches which use model-based collaborative filtering (CF) for scalability in building recommendation systems in web personalization have poor accuracy due to the fact that web usage data is often sparse and noisy. Clustering, mining association rules, and sequence pattern discovely have been used to determine the access behavior model. Making use of some of the characteristics of the modeling process can provide significant improvements to recommendation effectiveness. In an earlier work, we introduced a fuzzy hybrid CF technique which inherits the advantages of both memoly-based and model-based CF. In this paper, using relational fuzzy subtractive clustering as the first level modeling and then mining association rules within individual clusters, we propose a two level model-based technique, which is scalable and is an enhancement over association rule based recommender systems. Our results from comprehensive experiments using a large real life web usage data and performance comparisons with memory-based and model-based approaches help substantiate this claim.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {618–621},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.114,
author = {Speretta, Micro and Gauch, Susan},
title = {Personalized Search Based on User Search Histories},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.114},
doi = {10.1109/WI.2005.114},
abstract = {User profiles, descriptions of user interests, can be used by search engines to provide personalized search results. Many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). Both these techniques require participation of the user to install the proxy server or the bot. In this study, we explore the use of a less-invasive means of gathering user information for personalized search. In particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. By implementing a wrapper around the Google [10] search engine, we were able to collect information about individual user search activities. In particular, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. User profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. These profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. Our study found that user profiles based on queries were as effective as those based on snippets. We also found that our personalized re-ranking resulted in a 34% improvement in the rank-order of the user-selected results.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {622–628},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.116,
author = {Brank, Janez and Frayling, Natasa Milic and Frayling, Anthony and Smyth, Gavin},
title = {Predictive Algorithms for Browser Support of Habitual User Activities on the Web},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.116},
doi = {10.1109/WI.2005.116},
abstract = {Routine user activities on the Web result in the revisitation of Web sites and pages. Standard browser applications provide limited support for this type of habitual behaviour. They typically expose lists of visited URLs that are automatically recorded by the system or manually created by the user, such as bookmarks. Studies have shown that these approaches are not successful in supporting routine user activities. Informed by our user research we designed a browser feature that automatically exposes candidate URLs for revisitation by the user. In this paper we describe and evaluate the algorithms that we use to model the user\'{y}s habitual behaviour. We demonstrate how a structured navigation history model facilitates the discovery of relevant usage patterns and supports predictive algorithms that are applicable to relatively short personal navigation histories.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {629–635},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.147,
author = {Halvey, Martin and Keane, Mark T. and Smyth, Barry},
title = {Time Based Segmentation of Log Data for User Navigation Prediction in Personalization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.147},
doi = {10.1109/WI.2005.147},
abstract = {There are many systems that attempt to predict user navigation on the Internet through the use of past behavior, preferences and environmental factors. We believe that many of these models have shortcomings, in that they do not take into account that users may have many different sets of preferences, specifically, we investigate time as an environmental factor in making predictions about user navigation. We present a method for segmenting log files in order to learn time dependent models to predict user navigation patterns and show the benefits of these models over traditional methods. An analysis is carried out on a sample of usage logs for Wireless Application Protocol (WAP) browsing, and the results of this analysis verify our hypothesis.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {636–640},
numpages = {5},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.136,
author = {Tao, Dacheng and Maybank, Steve and Hu, Weiming and Li, Xuelong},
title = {Stable Third-Order Tensor Representation for Color Image Classification},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.136},
doi = {10.1109/WI.2005.136},
abstract = {General tensors can represent colour images more naturally than conventional features; however the general tensors\'{y} stability properties are not reported and remain to be a key problem. In this paper, we use the tensor minimax probability (TMPM) to prove that the tensor representation is stable. The proof is based on the random subspace method through a large number of experiments.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {641–644},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.137,
author = {Bilasco, Ioan Marius and Gensel, Jerome and Villanova-Oliver, Marlene},
title = {STAMP: Adaptable Templates for Synchronized Multimedia Presentations},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.137},
doi = {10.1109/WI.2005.137},
abstract = {This paper addresses the adaptation of dynamic and synchronized multimedia presentations built by querying XML compatible data sources. We provide WIS designers with facilities for describing presentations whose content is not known at design time in terms of quantity, but only after the execution of queries. Our approach relies on the definition of a template. A template consists of a model that aims at automatically adapting the multimedia content of a presentation to both the user\'{y}s profile and the characteristics of her/his access device. We show here how a template is built and how adaptations of the presentation are performed when the quantity of information and/or the material capabilities of the access devices (e.g. display size), do not match the template\'{y}s spatiotemporal specifications.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {645–648},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.105,
author = {Zhao, Yan and Yao, Yiyu and Zhong, Ning},
title = {Multilevel Web Personalization},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.105},
doi = {10.1109/WI.2005.105},
abstract = {Web personalization is one of the major concerns of Web intelligence. It is noticed that the two components of Web, users and services, can be understood from multiple views in forms of hierarchies, and each hierarchy is organized by a multilevel structure. A unified model is proposed for including all personalization styles. The unified model with multilevel hierarchy structures add new understandings and insights into the Web personalization issues. More concise and precise recommendations can then be studied and pursued based on this multilevel model.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {649–652},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.125,
author = {Kuo, Yen-Hung and Chen, Juei-Nan and Jeng, Yu-Lin and Huang, Yueh-Min},
title = {Real-Time Learning Behavior Mining for e-Learning},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.125},
doi = {10.1109/WI.2005.125},
abstract = {Over the last years, we have witnessed an explosive growth of e-learning. More and more learning contents have been published and shared over the Internet. Therefore, how to progress an efficient learning process becomes a critical issue. This paper proposes a sequential mining algorithm to analyze learning behaviors for discovering frequent sequential patterns. By these patterns, we can provide suggestions for learners to select their interest learning contents. Different to other sequential mining algorithms, this study provides an incrementally method to analyze learning sequencing. More specifically, the mining algorithm in this paper can provide real-time analysis, and then report to learners for selecting learning contents more easily.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {653–656},
numpages = {4},
keywords = {data mining, real-time analysis, sequential mining, e-learning, frequent pattern},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.158,
author = {Hoeber, Orland and Yang, Xue-Dong and Yao, Yiyu},
title = {Visualization Support for Interactive Query Refinement},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.158},
doi = {10.1109/WI.2005.158},
abstract = {It has been well documented that web searchers havedifficulties crafting queries to fulfill their information needs. In this work, we use a concept knowledge base generated from the ACM Computing Classification System to generate a query space that represents the query terms in relation to the concepts they describe and the other terms that are related to these concepts. A visual representation of this query space allows the user to interpret the relationships between their query terms and the query space. Interactive queryrefinement within this visual representation takes advantage of the user\'{y}s visual information processing abilities, and allows the user to choose terms that accurately represent their information need. A preview of the search results from Google provides the user with an indication of the current state of their query refinement process. This work allows the user to take an active role in the information retrieval process, supporting the fundamental shift from information retrieval systems to information retrieval support systems.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {657–665},
numpages = {9},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.67,
author = {Scarselli, Franco and Yong, Sweah Liang and Gori, Marco and Hagenbuchner, Markus and Tsoi, Ah Chung and Maggini, Marco},
title = {Graph Neural Networks for Ranking Web Pages},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.67},
doi = {10.1109/WI.2005.67},
abstract = {An artificial neural network model, capable of processing general types of graph structured data, has recently been proposed. This paper applies the new model to the computation of customised page ranks problem in the World Wide Web. The class of customised page ranks that can be implemented in this way is very general and easy because the neural network model is learned by examples. Some preliminary experimental findings show that the model generalizes well over unseen web pages, and hence, may be suitable for the task of page rank computation on a large web graph.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {666–672},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.7,
author = {Ngo, Chi Lang and Nguyen, Hung Son},
title = {A Method of Web Search Result Clustering Based on Rough Sets},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.7},
doi = {10.1109/WI.2005.7},
abstract = {Due to the enormous size of the web and low precision of user queries, finding the right information from the web can be difficult if not impossible. One approach that tries to solve this problem is using clustering techniques for grouping similar document together in order to facilitate presentation of results in more compact form and enable thematic browsing of the results set. The main problem of many web search result (snippet) clustering algorithm is based on the poor vector representation of snippets. In this paper, we present a method of snippet representation enrichment using Tolerance Rough Set model. We applied the proposed method to construct a rough set based search result clustering algorithm and compared it with other recent methods.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {673–679},
numpages = {7},
keywords = {snippet, rough sets, clustering},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.30,
author = {Yang, Yong and Wang, Guoyin},
title = {An Evaluation Model for Web-Based Learning Support Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.30},
doi = {10.1109/WI.2005.30},
abstract = {With the development of Web technology and it is applied in education, Web-based Learning Support Systems (WLSSs) have been adopted all over the world. This paper presents a formal evaluation model for WLSSs based on the method of fuzzy integrated evaluation. We concentrate on the commonly factors and elements that ensure and contribute to the learning course. The model is comprehensive and objective, so that it can be used to evaluate all WLSSs. Based on the proposed model, some WLSSs are evaluated and compared with each other. Some further research issues are also discussed.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {680–683},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.77,
author = {Xing, Wenpu and Ghorbani, Ali A.},
title = {Information Domain Modeling for Adaptive Web Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.77},
doi = {10.1109/WI.2005.77},
abstract = {This paper presents a Domain Modeling System, which builds a domain model framework for adaptive Web systems. It records concepts and the relationships among them and represents them as a concept network. To speed up run time searches, the system finds all related concepts offline and in advance by calculating the optimal paths between all pairs of concepts. In addition, a new algorithm, Rich Maximal Frequent Sequence algorithm, is introduced in the system for discovering frequent sequence patterns among concepts. For the purpose of evaluation, the system is applied to an adaptive web system. The experiments demonstrate that the adaptive Web system is improved in the performance of accurate page recommendations and quick responses.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {684–687},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.36,
author = {Richard, Bruno and Tchounikine, Pierre},
title = {Assisting Analysts in the Construction of Model Based Tracking Recommender Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.36},
doi = {10.1109/WI.2005.36},
abstract = {Model based tracking is an approach that allows the construction of recommender systems based on the comparison of the user\'{y}s navigation with predefined prototypic models of navigation. This approach is well adapted for complex Websites and allows overcoming a certain number of drawbacks from classical approaches. However, it requires an additional task: analysts must build the reference models and their associated tips. In this paper we present an approach and tools to support the analysts in this task.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {688–691},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.115,
author = {Preda, Mircea and Popescu, Dan},
title = {Personalized Web Recommendations: Supporting Epistemic Information about End-Users},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.115},
doi = {10.1109/WI.2005.115},
abstract = {The online recommendations are a popular presence in the Web sites world due to their potential to increase the customers\'{y} satisfaction. The ability to represent epistemic information about the clients\'{y} beliefs is important to understand their needs. This paper presents a recommender system based on reinforcement learning. The system represents concepts presented on a Web site by epistemic logical programs and uses a similarity measure between programs in order to facilitate generalization. A prototype of this system and experiments are presented.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {692–695},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.140,
author = {Rahman, Shapiee Abd and Bhalla, Subhash},
title = {Supporting Spatial Data Queries for Mobile Services},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.140},
doi = {10.1109/WI.2005.140},
abstract = {In the last few years, many GIS applications have appeared on the Web giving users access to both spatial and non-spatial (attribute) data. Web access to GIS resources needs to address the possibility of access via handheld devices such as mobile phones, PDAs or pocket computers. Currently, only a few research studies are available on high-level user interface for spatial queries on such devices. Thus, an important research problem is to design user-friendly interfaces for mobile Web GIS. Query-By-Example (QBE) is a user-friendly language developed for a Relational Database Management System (RDBMS). Many RDBMSs have included support for spatial extensions. In this paper, we make an effort to create extensions for QBE to support spatial queries on mobile devices. It is called Spatial mQBE. We show that Spatial mQBE is a simple and user-friendly interface. The main strengths of this interface are its simplicity to express a query and its expressive power.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {696–699},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.48,
author = {Tsumoto, Shusaku and Hirano, Shoji and Abe, Hidenao and Nakakuni, Hideaki and Hanada, Eisuke},
title = {Clinical Decision Support Based on Mobile Telecommunication Systems},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.48},
doi = {10.1109/WI.2005.48},
abstract = {In this paper, we focus on the application of knowledge engineering techniques to medical mobile communication network, where the web intelligence technologies are used for an efficient interface of medical expert system. Then, the system was put on the internet to provide an intelligent decision support in telemedicine and is now being evaluated by region medical home doctors. The results show that such an internet-based medical decision support enables home doctors to take a quick action to the applied domain.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {700–703},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.44,
author = {Aswath, Dipti and Ahmed, Syed Toufeeq and D'cunha, James and Davulcu, Hasan},
title = {Boosting Item Keyword Search with Spreading Activation},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.44},
doi = {10.1109/WI.2005.44},
abstract = {Most keyword search engines return directly matching keyword phrases. However, publishers cannot anticipate all possible ways in which users would search for the items in their documents. In fact, many times, there may be no direct keyword match between a keyword search phrase and descriptions of relevant items that are perfect matches for the search. We present an automated, high precision-based information retrieval solution to boost item find-ability by bridging the semantic gap between item information and popular keyword search phrases. Our solution achieves an average of 80% F-measure for various boosted matches for keyword search phrases in various categories.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {704–707},
numpages = {4},
keywords = {Data Mining, Spreading Activation, Information Retrieval, Web Data, Information Extraction, E-commerce, Support Vector Machines},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.111,
author = {Chang, Wei-Chun and Wu, Ching-Seh and Chang, Chun},
title = {Optimizing Dynamic Web Service Component Composition by Using Evolutionary Algorithms},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.111},
doi = {10.1109/WI.2005.111},
abstract = {An evolutionary process designed to optimize dynamic composition of web service components is proposed. The solution proposed in this paper applies evolutionary computing techniques to automatically select optimal combinations of web service components from available component repositories. Use of the process is illustrated with a computational simulation for optimal component selection.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {708–711},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.107,
author = {Hu, Wei and Hu, Weiming},
title = {Network-Based Intrusion Detection Using Adaboost Algorithm},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.107},
doi = {10.1109/WI.2005.107},
abstract = {Intrusion detection on the internet is a heated research field in computer science, where much work has been done during the past two decades. In this paper; we build a network-based intrusion detection system using Adaboost, a prevailing machine learning algorithm. The experiments demonstrate that our system can achieve an especially low false positive rate while keeping a preferable detection rate, and its computational complexity is extremely low, which is a very attractive property in practice.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {712–717},
numpages = {6},
keywords = {Intrusion detection, Computational complexity, AdaBoost, Network-based IDS},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.124,
author = {Keselj, Vlado and Jutla, Dawn},
title = {QTIP: Multi-Agent NLP and Privacy Architecture for Information Retrieval in Usable Web Privacy Software},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.124},
doi = {10.1109/WI.2005.124},
abstract = {We present a generic natural language processing (NLP) architecture, acronym QTIL, based on a system of cooperating multiple agents (Q/A, T, I, and L agents) which can be used in any information system incorporating Internet Information Retrieval. We then introduce a hybrid multi-agent system (MAS) architecture, acronym QTIP, for the privacy domain through integrating the PeCAN (Personal Context Agent Networking) and QTIL MAS architectures. There are two areas where NLP is used: in the user-MAS interaction and in the process of resource indexing and matching. These two areas map to the Q/Aagent and to the I-agents. We propose using a lightweight Head-driven Phrase Structure Grammar (HPSG) natural language method for the Q architectural layers and qualitatively justify its applicability. We provide an example of employing the HPSG formalism for Information Retrieval using natural language capability via Privacy Web Services in one instantiation of the QTIP architecture. Independent preliminary results for HPSG on the Q level show that our approaches for enhancing the usability of PET tools are promising.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {718–724},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.117,
author = {Polat, Huseyin and Du, Wenliang},
title = {Privacy-Preserving Top-N Recommendation on Horizontally Partitioned Data},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.117},
doi = {10.1109/WI.2005.117},
abstract = {Collaborative filtering techniques are widely used by many E-commerce sites for recommendation purposes. Such techniques help customers by suggesting products to purchase using other users\'{y} preferences. Today\'{y}s top-N recommendation schemes are based on market basket data, which shows whether a customer bought an item or not. Data collected for recommendation purposes might be split between different parties. To provide better referrals and increase mutual advantages, such parties might want to share data. Due to privacy concerns, however, they do not want to disclose data. This paper presents a scheme for binary ratings-based top-N recommendation on horizontally partitioned data, in which two parties own disjoint sets of users\'{y} ratings for the same items while preserving data owners\'{y} privacy. If data owners want to produce referrals using the combined data while preserving their privacy, we propose a scheme to provide accurate top-N recommendations without exposing data owners\'{y} privacy. We conducted various experiments to evaluate our scheme and analyzed how different factors affect the performance using the experiment results.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {725–731},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.113,
author = {Laperrousaz, Christelle and Leroux, Pascal and Teutsch, Philippe},
title = {Perception of Individual Activities in a Group Activity through Qualitative Information about the Group Dynamics},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.113},
doi = {10.1109/WI.2005.113},
abstract = {This paper presents the difficulties that the tutor encounters when he carries out the follow-up of a distance collective activity, and particularly those related to the perception of the individual work in a collective activity. From a study of scenarios of distance collective activity found in the literature, we make some proposals in order to facilitate the tutor\'{y}s perception of the learner\'{y}s activity in the group activity, in particular, to facilitate the perception of the group dynamics with qualitative information. We have elaborated a model of follow-up of learners involved in a collective activity which we present in this paper, especially, the functionality of perception of the group dynamics. We present the tools of perception of the group dynamics we have implanted.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {732–738},
numpages = {7},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.26,
author = {Sanchez-Nielsen, Elena and Chavez-Gutierrez, Francisco},
title = {An E-Services Architecture for Legislative Assemblies},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.26},
doi = {10.1109/WI.2005.26},
abstract = {Current and geographically distributed legislative institutions demand exchange of information between related entities. However, existing legislative applications are composed of autonomous and heterogeneous components. In this paper, we describe an e-Services model for Spanish legislative distributed assemblies. Our approach addresses the following issues: (1) a scalar and interoperable framework that allows incorporating new institutions ande-Government services in a dynamic way, (2) a flexible security mechanism and (3) maintaining the autonomy, accessibility and multilingualism among all the participant institutions.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {739–742},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.129,
author = {Li, You and Liu, Dong-Bo and Zhang, Wei-Ming},
title = {Schema Matching Using Neural Network},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.129},
doi = {10.1109/WI.2005.129},
abstract = {Schema matching plays a key role in data integration, data warehouse and e-business. This paper introduces a schema matching method SMDD based on neural network. By analyzing the characteristics of data distribution, it automatically fulfills the task of schema matching. It can be used independently or as a supplement of other schema matching methods. SMDD can improve the accuracy of schema matching from the point of view of data contents.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {743–746},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.126,
author = {Guo, Xuetao and Lu, Jie},
title = {Recommending Trade Exhibitions by Integrating Semantic Information with Collaborative Filtering},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.126},
doi = {10.1109/WI.2005.126},
abstract = {Recommender systems have gained successfully applications for the past ten years, particular in E-commerce domain. However, existing recommendation approaches can not effectively deal with recommendation issue of one-and-only items occurred in government-to-business services, e.g. recommendation of trade exhibitions. Thus, in this study, we propose a novel approach by integrating semantic information with the traditional item-based collaborative filtering, and attempt to help the businesses choose the right trade exhibitions at the right time. The outcome of this study will have tremendous significance in overcoming the \'{y}new item\'{y} problem of existing recommendation approaches.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {747–750},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.35,
author = {Boella, Guido and Hulstijn, Joris and van der Torre, Leendert},
title = {Argument Games for Interactive Access Control},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.35},
doi = {10.1109/WI.2005.35},
abstract = {We are interested in interactive access control to web services in virtual organizations. We discuss argument games in which the set of credentials requested by the service provider to access a service is established by means of an interaction between a client acting as a proponent and a server acting as an opponent.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {751–754},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.42,
author = {Lo, Shuchuan},
title = {Binary Prediction Based on Weighted Sequential Mining Method},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.42},
doi = {10.1109/WI.2005.42},
abstract = {This paper presents a weighted-binary-sequential method to predict the status of customer patronage for the next day. Most of the research using association rules to mine sequential data focus on the algorithms and computing efficiency of pattern or rule generation. But few of them consider the time value of the sequential data. It is desirable to weight recent observations more heavily than remote observations in the analysis of time-series data. In this paper, we address a time-weighted concept on association algorithm to mine the binary-time-series data. The weighted binary sequence algorithm will give more weight on the recent data in finding the longest frequent patterns from binary-time-series data. There are two weighting methods; dynamic-length weighting and fixed-length weighting. Both algorithms are compared to the un-weighted algorithm to show how time value influences the prediction accuracy. Some performance results with a real-life website application given in this paper show that time-weighted sequential algorithms are generally superior to un-weighted sequential algorithm.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {755–761},
numpages = {7},
keywords = {Binary-time-series data, Association rules, Pattern, CRM, Sequential Mining},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.108,
author = {Dong, Aijuan and Li, Honglin},
title = {Ontology- Based Information Integration in Virtual Learning Environment},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.108},
doi = {10.1109/WI.2005.108},
abstract = {A good virtual learning environment should deliver relevant learning materials to learners at the most appropriate time and locations to facilitate learners\'{y} acquisition of knowledge and skills. In this paper, we propose ontology-based information integration in virtual learning environment using ontology and web services. Relevant concepts extracted from domain ontology provide ontology-based browsing space that allows users to browse and select relevant terms of interest and increases the degree of relevancy. By using web services to integrate learning materials from heterogeneous public domain data sources, applications do not need to know the internal structure and working of public domain data sources, and reuse existing applications and recourses. We use Gene Ontology, PubMed eUtils and Google Web APIs to demonstrate our idea. The implementation involves techniques in image and video processing, database management, programming, and multimedia learning materials presentation.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {762–765},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.2,
author = {Liu, Peng and Hu, Jian-bin and Chen, Zhong},
title = {A Formal Language for Access Control Policies in Distributed Environment},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.2},
doi = {10.1109/WI.2005.2},
abstract = {Although several access control policies have been proposed for securing access to resources, they focused on security of distributed environments that were rather static. Nowadays distributed environment becomes open and dynamic. In this paper, we propose a formal language for access control policies in open and dynamic environment. The language is based on description logic program and generalized courteous logic program supporting classical negation, prioritized conflict handling and mutual exclusion constraints. The language allows the specification of positive and negative authorization, privilege delegation and revocation, prioritized conflict resolution and mutual authorization exclusions.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {766–769},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.33,
author = {Garcia, Ana Cristina Bicharra and Ciuffo, Leandro Neumann},
title = {Applying the HYRIWYG Incentive Mechanism in a Recommender System},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.33},
doi = {10.1109/WI.2005.33},
abstract = {The use of Recommender Systems (RS) has become common in several e-commerce websites. Collaborative Filtering is one of the most popular RS techniques. It infers an user's predilections based on similarity with other users. However, to work correctly, the RS depends on the evaluator's participation with truly opinions. The HYRIWYG mechanism proposes an incentive model to motivate users to contribute with a high number of opinions and, more important, guaranteeing the truthfulness of the information. In this paper, we present HYRIWYG mechanism and discuss it using an empirical study in the movie evaluation domain. Initial results highlight the great potential benefits of using this kind of incentive mechanism.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {770–773},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.11,
author = {Wu, Hao-Tian and Cheung, Yiu-ming},
title = {A Reversible Data Hiding Approach to Mesh Authentication},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.11},
doi = {10.1109/WI.2005.11},
abstract = {This paper proposes a reversible data hiding method to authenticate 3D meshes by modulating the distances from the mesh faces to the mesh centroid to embed a fragile watermark. It keeps the modulation information in the watermarked mesh so that the reversibility of the embedding process is achieved. Since the embedded watermark is sensitive to geometrical and topological processing, unauthorized modifications on the watermarked mesh can be therefore detected by retrieving and comparing the embedded watermark with the original one. Furthermore, as long as the watermarked mesh is intact, the original mesh can be recovered using some priori knowledge.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {774–777},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.19,
author = {Balbiani, Philippe},
title = {Access Control with Uncertain Surveillance},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.19},
doi = {10.1109/WI.2005.19},
abstract = {We present a variant of the access control matrix model obtained by incorporating uncertain surveillance saying that "if subject s performs illegally action a on object o then the degree of likelihood that it will remain unpunished is equal to x". We then turn to the question whether the expressive power of the matrix model grows when enriching access control with uncertain surveillance. In connection with this enriched model, we also discuss the solvable and unsolvable cases of the major theme of computer security, namely the safety problem for access control matrices.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {778–781},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.71,
author = {Saerens, Marco and Fouss, Francois},
title = {HITS is Principal Components Analysis},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.71},
doi = {10.1109/WI.2005.71},
abstract = {In this work, we show that Kleinberg\'{y}s hubs and authorities model (HITS) is simply Principal Components Analysis (PCA; maybe the most widely used multivariate statistical analysis method), albeit without centering, applied to the adjacency matrix of the graph of web pages. We further show that a variant of HITS, SALSA, is closely related to correspondence analysis, another standard multivariate statistical analysis method. In addition to provide a clear statistical interpretation for HITS, this result suggests to rely on existing work already published in the multivariate statistical analysis litterature (extensions of PCA or correspondence analysis) in order to analyse or design new web pages scoring procedures.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {782–785},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.86,
author = {Li, Jing and Tao, Dacheng and Hu, Weiming and Li, Xuelong},
title = {Kernel Principle Component Analysis in Pixels Clustering},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.86},
doi = {10.1109/WI.2005.86},
abstract = {We propose two new methods in the nonlinear kernel feature space for pixel clustering based on the traditional KMeans and Gaussian Mixture Model (GMM). Unlike the previous work on the kernel machines, we give out a new perspective on the new developed kernel machines. That is, kernel principle component analysis (KPCA) combined with the KMeans and the GMM are kernel KMeans (KKMeans) and kernel GMM (KGMM), respectively. In this paper, we prove the new perspective on KKMeans and give out a clear statement on the KGMM as well. Based on this new perspectives, we can implement the KKMeans and the KGMM conveniently. At the end of the paper, we utilize these new algorithms on the problem of the colour image segmentation. Based on a series of experimental results on Corel Colour Images, we find that the KKMeans and KGMM can outperform the traditional KMeans and GMM consistently, respectively.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {786–789},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.10,
author = {Grilheres, Bruno and Beauce, Christophe and Canu, Stephane and Brunessaux, Stephan},
title = {A Platform for Semantic Annotations and Ontology Population Using Conditional Random Fields},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.10},
doi = {10.1109/WI.2005.10},
abstract = {Ontologies are widely used for organising and sharing knowledge. But elaborating these resources is a heavy and time-consuming task. This paper is two-fold: it describes EADS DCS text-mining platform, in particular its service to annotate documents with semantic tags and it presents its extension for incremental learning of ontologies. Domain experts are assisted in the ontology population task by recent machine learning techniques (i.e. Conditional Random Fields). Comparisons are made between annotations from the ontology and from a trained CRF model, so as to detect candidate instances. An iterative process controlled by the experts results in knowledge discovery and constitution of an accurate ontology.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {790–793},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.159,
author = {Desmarais, Michel C.},
title = {Web Log Session Analyzer: Integrating Parsing and Logic Programming into a Data Mart Architecture},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.159},
doi = {10.1109/WI.2005.159},
abstract = {Navigation and interaction patterns of Web users can be relatively complex, especially for sites with interactive applications that support user sessions and profiles. We describe such a case for an interactive virtual garment dressing room. The application is distributed over many web sites, supports personnalization and user profiles, and the notion of a multi-site user session. It has its own data logging system that generates approximately 5GB of complex data per month. The analysis of those logs requires more sophisticated processing than is typically done using a relational language. Even the use of procedural languages and DBMS can prove tedious and inefficient. We show an approach to the analysis of complex log data based on a parallel stream processing architecture and the use of specialized languages, namely a grammatical parser and a logic programming module, that offers an efficient, flexible, and powerful solution.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {794–797},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.85,
author = {Pan, Alberto and Raposo, Juan and Alvarez, Manuel and Montoto, Paula and Losada, Jose and Hidalgo, Justo},
title = {ITPilot: A Toolkit for Industrial-Strength Web Data Extraction},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.85},
doi = {10.1109/WI.2005.85},
abstract = {In recent years, many research systems have been proposed to perform data extraction and automation tasks on Web sources. Since most of today\'{y}s Web sources are "human-readable" but not "machine-readable", these systems must address a number of difficult challenges, such as dealing with complex navigation sequences, extracting data from HTML pages and reacting to source changes. Denodo Corporation has developed ITPilot, an industrial-strength solution that allows complex "wrappers" for Web sources to be graphically generated and automatically maintained. This paper presents the architecture and the basic ideas "behind the scenes" in ITPilot.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {798–801},
numpages = {4},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.5,
author = {Pavlin, Gregor and de Oude, Patrick and Nunnink, Jan},
title = {A MAS Approach to Fusion of Heterogeneous Information},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.5},
doi = {10.1109/WI.2005.5},
abstract = {Distributed Perception Networks (DPN) are a MAS approach to large scale fusion of heterogeneous and noisy information. DPN agents can establish meaningful information filtering channels between the relevant information sources and the decision makers. Through specification of high level concepts, DPN agent organizations generate distributed Bayesian Networks, which provide mappings between the observed symptoms and the hypotheses relevant to the decision making. In addition, DPNs support robust distributed inference as well as decentralized probabilistic resource allocation.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {802–804},
numpages = {3},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.18,
author = {Sislak, David and Rehak, Martin and Pechoucek, Michal},
title = {A-Globe: Multi-Agent Platform with Advanced Simulation and Visualization Support},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.18},
doi = {10.1109/WI.2005.18},
abstract = {A-globe [3, 1] is a simulation oriented multi-agent platform featuring agent migration, communication inaccessibility simulation and high scalability with moderate hardware requirements. Using dedicated simulation messaging together with 2D and 3D visualization support, large agent systems can be engineered, tested and visualized on a single machine. A-globe agents are fully fledged JAVA agents, each with its own independent thread, that can autonomously migrate between platforms running on different hosts. Thanks to the separation of simulation and agent code, deployment of agents to embedded devices is straight-forward. Platform is not natively FIPA-compliant, as the interoperability was sacrificed to support the scalability and efficiency.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {805–806},
numpages = {2},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.101,
author = {Speretta, Micro and Gauch, Susan},
title = {Misearch},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.101},
doi = {10.1109/WI.2005.101},
abstract = {User profiles, descriptions of use interests, can be used by search engines to provide personalized search results. Many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). Both these techniques require participation of the user to install the proxy server or bot. In a previous study [1], we explored the use of a less-invasive means of gathering user information for personalized search. In particular, we built user profiles based on activity at the search site itself and studied the use of these profiles to provide personalized search results. By implementing a wrapper around the Google search engine, we were able to collect information about individual user search activities. More specifically, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. User profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. These profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. Our study found that user profiles based on queries were as effective as those based on snippets. We also found that our personalized re-ranking resulted in a 34% improvement in the rank-order of the user-selected results. In this demo we present misearch (http://moby.ittc.ku.edu/~micro/demo), a meta-search engine that implemented the algorithm analyzed in the previous study.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {807–808},
numpages = {2},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.82,
author = {Abrol, Mani and Doshi, Bhavin and Kanihan, Jim and Kumar, Amit and Liu, Jinhui and Mao, Jianchang},
title = {Intelligent Taxonomy Management Tools for Enterprise Content},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.82},
doi = {10.1109/WI.2005.82},
abstract = {The taxonomy is the most popular way for organizing a large volume of content. A taxonomy, which is typically a hierarchical representation of categories, provides a navigation structure for exploring and understanding the underlying corpus without sifting through a huge volume of documents. Creating and maintaining taxonomies with a large volume of documents remains a daunting task facing many enterprises. Content organization process typically involves four basic steps: (i) creating taxonomies, (ii) building classification models, (iii) populating taxonomies with documents, and (iv) deploying populated taxonomies in enterprise portals. Each step in the process may have unique requirements that determine what techniques and tools are suitable for the tasks. In this paper, we present a comprehensive suite of tools developed by Verity Inc. for accurately, collaboratively, and efficiently organizing enterprise content.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {809–811},
numpages = {3},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.120,
author = {Nakatsuji, Makoto and Miyoshi, Yu and Kimura, Tatsuyuki},
title = {Proposal and Verification of Flexible Interface Mapping Technique for Automatic System Cooperation Based on Semantics},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.120},
doi = {10.1109/WI.2005.120},
abstract = {These days, many companies are executing their business aims based on decentralized cooperation of software components which work on various systems over a network. However, messages and processes between systems are designed individually in each operations division. Therefore, the system development for adjusting interfaces is expensive, so the companies cannot introduce their services in a dynamic business environment. To resolve such problems, we propose interface modeling technique and message mapping technique which model the relationship between the message formats and semantics on the formats by using Web Ontology Language (OWL) and execute the mapping between the message formats by using semantics. We developed the user interactive message mapping tool and evaluated our proposed methods based on the interfacespecifications of real network management systems.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {812–813},
numpages = {2},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.31,
author = {del Castillo, M. D. and Serrano, J. I.},
title = {An Interactive Hybrid System for Identifying and Filtering Unsolicited Email},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.31},
doi = {10.1109/WI.2005.31},
abstract = {This paper presents a system for automatically detecting and filtering unsolicited electronic messages. The underlying filtering method is based on email origin and content. A heuristic knowledge base formed by spam words is extracted from labelled emails by a finite state automata. The processing of three parts of every email by a single Bayesian filter and the integration of the every part classification allows to achieve a maximum performance goal. The system is dynamic and interactive and evolves from the evolution of spam by incremental machine learning.},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {814–815},
numpages = {2},
series = {WI '05}
}

@inproceedings{10.1109/WI.2005.37,
title = {Author Index},
year = {2005},
isbn = {076952415X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2005.37},
doi = {10.1109/WI.2005.37},
booktitle = {Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {816–819},
numpages = {4},
series = {WI '05}
}

