@inproceedings{10.5555/1331740.1331756,
title = {2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology - Cover},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {c1},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.3,
title = {2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology - Title},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.3},
doi = {10.1109/WI.2007.3},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {i–iii},
numpages = {3},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.1,
title = {2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology - Copyright},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.1},
doi = {10.1109/WI.2007.1},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {iv},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.113,
title = {Welcome Message from Conference Chairs and Program Chair},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.113},
doi = {10.1109/WI.2007.113},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xvi–xviii},
numpages = {3},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.114,
title = {WI'07 Conference Organization},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.114},
doi = {10.1109/WI.2007.114},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xix–xx},
numpages = {2},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.116,
title = {WI'07 Program Committee Members},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.116},
doi = {10.1109/WI.2007.116},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxi–xxiii},
numpages = {3},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.115,
title = {WI'07 Non-PC Reviewers},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.115},
doi = {10.1109/WI.2007.115},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxiv},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.35,
author = {Karp, Richard M.},
title = {Computer Science as a Lens on the Sciences},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.35},
doi = {10.1109/WI.2007.35},
abstract = {This talk will trace the growing influence of fundamental ideas from computer science on the nature of research in a number of scientific fields. There is a growing awareness that information processing lies at the heart of the processes studied in fields as diverse as quantum mechanics, statistical physics, nanotechnology, neuroscience, linguistics, economics and sociology. Increasingly, mathematical models in these fields are expressed in algorithmic languages and describe algorithmic processes. The speaker will briefly describe connections between quantum computing and the foundations of quantum mechanics, and between statistical mechanics and phase transitions in computation. He will indicate how the growth of the Web has created new phenomena to be investigated by sociologists and economists. He will then focus in greater detail on computational molecular biology, where the view of living cells as complex information processing systems has become the dominant paradigm, and will discuss specific algorithmic problems arising in the sequencing of genomes, the comparative analysis of the resulting genomic sequences,the modeling of networks of interacting proteins, and the associations between genetic variation and disease.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxvi},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.96,
author = {Fensel, Professor Dieter},
title = {ServiceWeb 3.0},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.96},
doi = {10.1109/WI.2007.96},
abstract = {Computer science is entering a new generation. The previous generation was based on abstracting from hardware. The emerging generation comes from abstracting from software and sees all resources as services in a service-oriented architecture (SOA). In a world of services, it is the service that counts for a customer and not the software or hardware components that implement the service. Service-oriented architectures are rapidly becoming the dominant computing paradigm. However, current SOA solutions are still restricted in their application context to in-house solution of companies. A service web will have billions of services. While service orientation is widely acknowledged for its potential to revolutionize the world of computing by abstracting form underlying hardware and software layers, that success depends on resolving fundamental challenges that SOA does not address currently. The mission of Service Web 3.0 is to provide solutions to integration and search that will enable the Service Oriented Architecture (SOA) revolution on a worldwide scale. Hereby we must focus on three major areas where we need to extend current approaches towards service orientation:},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxvii},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.40,
author = {Nishida, Professor Toyoaki},
title = {Conversational Informatics and Human-Centered Web Intelligence},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.40},
doi = {10.1109/WI.2007.40},
abstract = {Conversation is the most natural communication means for people to communicate with each other. I believe that conversation plays a critical role in realizing a paradigm of human-centered web intelligence in which web intelligence engines are grounded on the human society. We are currently building a computational framework for circulating information in a conversational fashion, using information packages called conversation quanta that encapsulate conversational scenes. Technologies are being developed for acquiring conversation quanta on the spot, accumulating them in a visually recognizable form, and reusing them in a situated fashion. Conversational Informatics constitutes the theoretical foundation for measurement, analysis, and modeling of conversation. I will overview recent results in Conversational Informatics that will help achieve our vision. I will also discuss our approach in the context of Social Intelligence Design aimed at the understanding and augmentation of social intelligence for collective problem solving and learning.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxviii},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.65,
author = {Shoham, Professor Yoav},
title = {How Relevant is Game Theory to Intelligent Agent Technology?},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.65},
doi = {10.1109/WI.2007.65},
abstract = {At this point, restricted to rather specialized areas, and even there must be taken with a grain of salt. But at the same time you can't afford not to know it; there is currently no better underpinning for understanding multiagent systems. I will elaborate using experience in both academia and industry.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxix},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.100,
author = {Santos Jr., Professor Eugene},
title = {The Challenge of Cultural Modeling for Inferring Intentions and Behavior},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.100},
doi = {10.1109/WI.2007.100},
abstract = {Accounting for social, cultural, and political factors must form the basis for understanding decision-making, actions, and reactions of individuals, thus driving their behaviors and intentions. Clearly, the individual is not wholly defined by just personal social, cultural, and political beliefs but also functions within a group of individuals. Within these groups (or organizations), they assimilate a potentially wide variety of different social factors, which may or may not differ from their own. Also, the group itself can vary in degrees of complexity, styles of interaction, and so forth, resulting in highly dynamic and emergent modes of behaviors. Even more difficult, this also includes taking into account the values, attitudes, and beliefs of the local population/environment that the individual/group is situated within. Without all these factors, we cannot expect to effectively understand, analyze, or predict the behaviors and intentions of others which grows ever more critical as our society continues to globalize and especially in today's conflicts and catastrophes. Thus, the need for a comprehensive modeling framework is evident as our only real hope of addressing such complexity. However, to date, only small isolated groups of pertinent behavioral factors have been studied, while there is little or no work towards developing a general unified and comprehensive approach that is also computational. The major challenges we face can be summed up in the following questions: 1. For prediction and explanation of intent and behavior, how does one computationally model individual or organizations and their emergent interactions with others, in various situations? 2. How does one organize and build the necessary social, cultural, political, behavioral, etc. knowledge- base? 3. How do you avoid brittleness and overspecialization? How do you construct these models efficiently and effectively, and dynamically evolve such models over time based on changing cultural and social factors? 4. How do you validate your models? In this talk, we will explore these challenges and focus on addressing pragmatic and computational issues in such modeling and examine some existing real-world efforts, current solutions, and openquestions.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxx},
numpages = {1},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.62,
author = {Yao, Professor Yiyu},
title = {Granular Computing for Web Intelligence and Brain Informatics},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.62},
doi = {10.1109/WI.2007.62},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {xxxi–xxxiv},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.95,
author = {Rauch, Jan and Simunek, Milan},
title = {Semantic Web Presentation of Analytical Reports from Data Mining - Preliminary Considerations},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.95},
doi = {10.1109/WI.2007.95},
abstract = {Project SEWEBAR concerning presentation of analytical reports from data mining through semantic web is introduced. Related local and global analytical reports are mentioned. An example of local analytical report is given and problems of indexing local reports are shortly discussed.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {3–7},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.34,
author = {Spangler, Scott and Chen, Ying and Proctor, Larry and Lelescu, Ana and Behal, Amit and He, Bin and Griffin, Thomas D. and Liu, Anna and Wade, Brad and Davis, Trevor},
title = {COBRA - Mining Web for Corporate Brand and Reputation Analysis},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.34},
doi = {10.1109/WI.2007.34},
abstract = {Corporations are extremely sensitive to issues such as brand stewardship and product reputation. Traditional brand image and reputation tracking is limited to news wires and contact centres analysis. However, with the emergence of web, Consumer Generated Media (CGM), such as blogs, news forums, message boards, and web pages/sites, is rapidly becoming the "voice of the people". This paper describes a COBRA (COrporate Brand and Reputation Analysis) solution that mines a wide range of CGM contents for brand and reputation analysis. The solution contains a flexible ETL (Extract, Transform, and Load) engine that processes diverse sets of structured and unstructured information, a suite of analytical capabilities that mines CGM content to extract semantic entities and insights out of the data, and an alerting mechanism that utilizes the analytics results to accurately generate brand and reputation alerts. We use a real-world case study to demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {11–17},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.88,
author = {Agichtein, Eugene and Burges, Chris and Brill, Eric},
title = {Question Answering over Implicitly Structured Web Content},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.88},
doi = {10.1109/WI.2007.88},
abstract = {Implicitly structured content on the Web such as HTML tables and lists can be extremely valuable for web search, question answering, and information retrieval, as the implicit structure in a page often reflects the underlying semantics of the data. Unfortunately, exploiting this information presents significant challenges due to the immense amount of implicitly structured content on the web, lack of schema information, and unknown source quality. We present TQA, a web-scale system for automatic question answering that is often able to find answers to real natural language questions from the implicitly structured content on the web. Our experiments over more than 200 million structures extracted from a partial web crawl demonstrate the promise of our approach.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.51,
author = {Shehata, Shady and Karray, Fakhri and Kamel, Mohamed},
title = {Enhancing Search Engine Quality Using Concept-Based Text Retrieval},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.51},
doi = {10.1109/WI.2007.51},
abstract = {Most of the common techniques in text retrieval are based on the statistical analysis of a term either as a word or a phrase. Statistical analysis of a term frequency captures the importance of the term within a document only. Thus, to achieve a more accurate analysis, the underlying representation should indicate terms that capture the semantics of text. In this case, the representation can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based representation, called Conceptual Ontological Graph (COG), where a concept can be either a word or a phrase and totally dependent on the sentence semantics, is introduced. The aim of the proposed representation is to extract the most important terms in a sentence and a document with respect to the meaning of the text. The COG representation analyzes each term at both the sentence and the document levels. This is different from the classical approach of analyzing terms at the document level. First, the proposed representation denotes the terms which contribute to the sentence semantics. Then, each term is chosen based on its position within the COG representation. Lastly, the selected terms are associated to their documents as features for the purpose of indexing before text retrieval. The COG representation can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the key concepts that represent the sentence meaning. Large sets of experiments using the proposed COG representation on different datasets in text retrieval are conducted. Experimental results demonstrate the substantial enhancement of the text retrieval quality using the COG representation over the traditional techniques. The evaluation of results relies on two quality measures, the bpref and P(10). Both the quality measures improved when the newly developed COG representation is used to enhance the quality of the text retrieval results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {26–32},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.16,
author = {Yang, Jie and Matsuo, Yutaka and Ishizuka, Mitsuru},
title = {An Augmented Tagging Scheme with Triple Tagging and Collective Filtering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.16},
doi = {10.1109/WI.2007.16},
abstract = {Collaborative tagging is increasingly drawing attentions. However the keyword based tagging scheme has its limitations and it can be observed that tagging society are seeking and using new tagging patterns. This paper proposes a subject-predicate-object scheme for users to triple tag web resources. We first introduce the triple tag model and discuss its relations with existing tag schema and RDF model. Then a filter-based framework that supports the query of triple tags is proposed. The implementation and a case study in the comparison shopping domain are exhibited.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {35–38},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.13,
author = {Velardi, Paola and Cucchiarelli, Alessandro and Petit, Michael},
title = {A Semantically Enriched Competency Management System to Support the Analysis of a Web-Based Research Network},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.13},
doi = {10.1109/WI.2007.13},
abstract = {While it is generally acknowledged that domain ontologies can significantly improve knowledge management systems (KMS) within organizations and among distributed web communities, we have little evidence of operational ontology-based KMS and their practical utility in real settings in the literature. We describe here the INTEROP KMap, a fully implemented, semantically indexed, competency management system, used to facilitate research collaboration and coordination of a Network of Excellence (NoE) on Enterprise Interoperability. Since the main highlighted advantages of ontologies are improved information access and interoperability, our aim in this paper is to give experimental support to these claims. We provide a summary description and usage data on the KMap, as well as experiments to quantify the added value of semantic search wrt traditional document ranking measures.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {41–47},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.30,
author = {Lin, Yu-Ru and Sundaram, Hari and Chi, Yun and Tatemura, Junichi and Tseng, Belle L.},
title = {Blog Community Discovery and Evolution Based on Mutual Awareness Expansion},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.30},
doi = {10.1109/WI.2007.30},
abstract = {There are information needs involving costly decisions that cannot be efficiently satisfied through conventional web search engines. Alternately, community centric search can provide multiple viewpoints to facilitate decision making. We propose to discover and model the temporal dynamics of thematic communities based on mutual awareness, where the awareness arises due to observable blogger actions and the expansion of mutual awareness leads to community formation. Given a query, we construct a directed action graph that is time-dependent, and weighted with respect to the query. We model the process of mutual awareness expansion using a random walk process and extract communities based on the model. We propose an interaction space based representation to quantify community dynamics. Each community is represented as a vector in the interaction space and its evolution is determined by a novel interaction correlation method. We have conducted experiments with a real-world blog dataset and have promising results for detection as well as insightful results for community evolution.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {48–56},
numpages = {9},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.39,
author = {De Choudhury, Munmun and Sundaram, Hari and John, Ajita and Seligmann, Doree Duncan},
title = {Contextual Prediction of Communication Flow in Social Networks},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.39},
doi = {10.1109/WI.2007.39},
abstract = {The paper develops a novel computational framework for predicting communication flow in social networks based on several contextual features. The problem is important because prediction of communication flow can impact timely sharing of specific information across a wide array of communities. We determine the intent to communicate and communication delay between users based on several contextual features in a social network corresponding to (a) neighborhood context, (b) topic context and (c) recipient context. The intent to communicate and communication delay are modeled as regression problems which are efficiently estimated using Support Vector Regression. We predict the intent and the delay, on an interval of time using past communication data. We have excellent prediction results on a real-world dataset from MySpace.com with an accuracy of 13-16%. We show that the intent to communicate is more significantly influenced by contextual factors compared to the delay.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {57–65},
numpages = {9},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.44,
author = {Mendes Rodrigues, Eduarda and Milic-Frayling, Natasa and Fortuna, Blaz},
title = {Detection of Web Subsites: Concepts, Algorithms, and Evaluation Issues},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.44},
doi = {10.1109/WI.2007.44},
abstract = {Web sites are often organized into several regions, each dedicated to a specific topic or serving a particular function. From a user's perspective, these regions typically form coherent sets of pages characterized by a distinct navigation structure and page layout-we refer to them as subsites. In this paper we propose to characterize Web site structure as a collection of subsites and devise a method for detecting subsites and entry points for subsite navigation. In our approach we use a new model for representing Web site structure called Link Structure Graph (LSG). The LSG captures a complete hyperlink structure of a Web site and models link associations reflected in the page layout. We analyze a sample of Web sites and compare the LSG based approach to commonly used statistics for Web graph analysis. We demonstrate that LSG approach reveals site properties that are beyond the reach of standard site models. Furthermore, we devise a method for evaluating the performance of subsite detection algorithms and provide evaluation guidelines.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {66–73},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.59,
author = {Fu, Yupeng and Xiang, Rongjing and Liu, Yiqun and Zhang, Min and Ma, Shaoping},
title = {Finding Experts Using Social Network Analysis},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.59},
doi = {10.1109/WI.2007.59},
abstract = {Searching an organization's document repositories for experts is a frequently occurred problem in intranet information management. A common method for finding experts in an organization is to use social networks -people are not isolated but connected by various kinds of associations. In organizations, people explicitly send email to one another thus social networks are likely to be contained in the patterns of communication. Moreover, in some web pages, the relationship among people is also recorded. In our approach we propose several strategies in discovering the associations among people from emails and web pages. Based on the social networks, we proposed an expertise propagation algorithm: from a ranked list of candidates according to their probability of being expert for a certain topic, we select a small set of the top ones as seed, and then use the social networks among the candidates to discover other potential experts. The experiments on TREC enterprise track show significant performance improvement with the algorithm.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {77–80},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331798,
author = {Cheng, Xueqi and Ren, Fuxin and Cao, Xianbin and Ma, Jing},
title = {How Contents Influence Clustering Features in the Web},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In World Wide Web, contents of web documents play important roles in the evolution process because of their effects on linking preference. A majority of topological properties are content-related, and among them the clustering features are sensitive to contents of Web documents. In this paper, we first observe the impacts of content similarity on web links by introducing a metric called Linkage Probability. Then we investigate how contents influence the formation mechanism of the most basic cluster, triangle, with a metric named Triangularization Probability. Experimental results indicate that content similarity has a positive function in the process of cluster formation in theWeb. Theoretical analysis predicts the contents influence on the clustering features in the Web very well.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {81–84},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.71,
author = {Murata, Tsuyoshi and Moriyasu, Sakiko},
title = {Link Prediction of Social Networks Based on Weighted Proximity Measures},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.71},
doi = {10.1109/WI.2007.71},
abstract = {Question-Answering Bulletin Boards (QABB), such as Yahoo! Answers and Windows Live QnA, are gaining popularity recently. Communications on QABB connect users, and the overall connections can be regarded as a social network. If the evolution of social networks can be predicted, it is quite useful for encouraging communications among users. This paper describes an improved method for predicting links based on weighted proximity measures of social networks. The method is based on an assumption that proximities between nodes can be estimated better by using both graph proximity measures and the weights of existing links in a social network. In order to show the effectiveness of our method, the data of Yahoo! Chiebukuro (Japanese Yahoo! Answers) are used for our experiments. The results show that our method outperforms previous approaches, especially when target social networks are sufficiently dense.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {85–88},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.69,
author = {Goussevskaia, Olga and Kuhn, Michael and Wattenhofer, Roger},
title = {Layers and Hierarchies in Real Virtual Networks},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.69},
doi = {10.1109/WI.2007.69},
abstract = {The virtual world is comprised of data items related to each other in a variety of contexts. Often such relations can be represented as graphs that evolve over time. Examples include social networks, co-authorship graphs, and the world-wide-web. Attempts to model these graphs have introduced the notions of hierarchies and layers, which correspond to taxonomies of the underlying objects, and reasons for object relations, respectively. In this paper we explore these concepts in the process of mining such naturallygrown networks. Based on two sample graphs, we present some evidence that the current models well fit real world networks and provide concrete applications of these findings. In particular, we show how hierarchies can be used for greedy routing and how separation of layers can be used as a preprocessing step to implement a location estimation application.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {89–94},
numpages = {6},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.109,
author = {Viermetz, Maximilian and Skubacz, Michal},
title = {Using Topic Discovery to Segment Large Communication Graphs for Social Network Analysis},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.109},
doi = {10.1109/WI.2007.109},
abstract = {The application of social network analysis to graphs found in the World Wide Web and the Internet has received increasing attention in recent years. Networks as diverse as those generated by e-mail communication, instant messaging, link structure in the Internet as well as citation and collaboration networks have all been treated with this method. So far these analyses solely utilize graph structure. There is, however, another source of information available in messaging corpora, namely content. We propose to apply the field of content analysis to the process of social network analysis. By extracting relevant and cohesive sub-networks from massive graphs, we obtain information on the actors contained in such sub-networks to a much firmer degree than before.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {95–99},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.29,
author = {Du, Nan and Wu, Bin and Wang, Bai},
title = {Backbone Discovery in Social Networks},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.29},
doi = {10.1109/WI.2007.29},
abstract = {Recent years have seen a thriving development of the World Wide Web as the most visible social media which enables people to share opinions, experiences and expertise with each other across the world. People now get involved in many different social networks simultaneously, which are often large intricate web of connections among the massive entities they are made of. As a result, the challenge of collecting and analyzing large-scale data among social members has left most basic questions about the global composition and function of such networks largely unresolved: What is the essential organization of a social network? who are the influential individuals whose voice is echoed by others? To address these questions, this paper presents an algorithm called sketcher to discover and describe the overall backbone of a specific network. Experimental results on the American College Football, Scientific Collaboration, and Telecommunications Call networks show that sketcher can extract the essential composition of a social network both efficiently and intuitively.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {100–103},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.28,
author = {Tayebi, Mohammad A. and Hashemi, S. Mehdi and Mohades, Ali},
title = {B2Rank: An Algorithm for Ranking Blogs Based on Behavioral Features},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.28},
doi = {10.1109/WI.2007.28},
abstract = {Blogs have become one of most important parts of web but we do not have so efficient search engines for them. One reason is differences between regular web pages and blog pages and inefficiency of conventional web pages ranking algorithms for blogs ranking. There are some works in this field but users' behavioral features have not considered yet. In this paper we present a new blogs ranking algorithm called B2Rank based on these features.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {104–107},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.78,
author = {Ma, Jing and Cao, Xianbin and Guo, Yuanping and Cheng, Xueqi},
title = {Modeling the Evolution of Web Using Vertex Content Similarity},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.78},
doi = {10.1109/WI.2007.78},
abstract = {In the evolution process of World Wide Web, contents of web pages play important roles because of their direct effect on linking preference. In this paper, we propose a model which combines vertex connectivity and content similarity in a proportional manner. Analytical solutions indicate that our model exhibits a power-law degree distribution with variable exponent determined by the weight of content similarity. Distribution of content similarity on connected vertex pairs shows content similar web pages trend to be linked together. Simulation results show our model yields remarkably agreements of both degree and content similarity distributions with real network.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {108–111},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.43,
author = {Falkowski, Tanja and Barth, Anja and Spiliopoulou, Myra},
title = {DENGRAPH: A Density-Based Community Detection Algorithm},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.43},
doi = {10.1109/WI.2007.43},
abstract = {Detecting densely connected subgroups in graphs such as communities in social networks is of interest in many research fields. Several methods have been developed to find communities but most of them have a high time complexity and are thus not applicable for large networks. Inspired by the clustering algorithm incremental DBSCAN we propose a density-based graph clustering algorithm DENGRAPH that is designed to deal with large dynamic datasets with noise and present first experimental results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {112–115},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.6,
author = {Chikhi, Nacim Fateh and Rothenburger, Bernard and Aussenac-Gilles, Nathalie},
title = {A Comparison of Dimensionality Reduction Techniques for Web Structure Mining},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.6},
doi = {10.1109/WI.2007.6},
abstract = {In many domains, dimensionality reduction techniques have been shown to be very effective for elucidating the underlying semantics of data. Thus, in this paper we investigate the use of various dimensionality reduction techniques (DRTs) to extract the implicit structures hidden in the web hyperlink connectivity. We apply and compare four DRTs, namely, Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), Independent Component Analysis (ICA) and Random Projection (RP). Experiments conducted on three datasets allow us to assert the following: NMF outperforms PCA and ICA in terms of stability and interpretability of the discovered structures; the wellknown WebKb dataset used in a large number of works about the analysis of the hyperlink connectivity seems to be not adapted for this task and we suggest rather to use the recent Wikipedia dataset which is better suited.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {116–119},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.117,
author = {Huang, H. Howie and Grimshaw, Andrew S. and Karpovich, John F.},
title = {You Can't Always Get What You Want: Achieving Differentiated Service Levels with Pricing Agents in a Storage Grid},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.117},
doi = {10.1109/WI.2007.117},
abstract = {We have designed a new storage grid called Storage@desk to harness unused storage available on desktop machines and turn it into a useful resource for clients. Given the complexity of managing clientspecific QoS requirements, and the dynamism inherent in supply and demand for resources, even a highly experienced system administrator cannot effectively manage resource allocation. In this paper, we present a market-based resource allocation model where pricing agents help resource providers adjust the prices as demand fluctuates. With derivative-following pricing, an agent requires no knowledge of competitors or consumers, which reduces communication overheads and avoids bottlenecks in the system. Individual clients need a variety of service levels and are in competition in scarce resources. Under the budget constraints, the consumers can't always get what they want. The budgets serve as an incentive for the consumers to react to the price signals. We simulate our model using real world trace data and the results show that, using this model, the system allows the consumers to achieve QoS goals under sufficient budgets and degrade in accordance with relative budget amounts.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {123–131},
numpages = {9},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.79,
author = {Patel, Yash and Darlington, John},
title = {Novel Stochastic Profitable Techniques For Brokers In A Web-Service Based Grid Market},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.79},
doi = {10.1109/WI.2007.79},
abstract = {Web service-oriented Grid is becoming a standard for achieving loosely coupled distributed computing. Grid services could easily be specified with web-service based interfaces. In this paper we first envisage a realistic Grid market with players such as end-users, brokers and service providers participating co-operatively with an aim to meet requirements and earn profit. End-users wish to use functionality of Grid services by paying the minimum possible price or price confined within a specified budget, brokers aim to maximise profit whilst establishing a SLA (Service Level Agreement) and satisfying end-user needs and at the same time resisting the volatility of service execution time and availability. Service providers aim to develop price models based on end-user or broker demands that will maximise their profit. In this paper we focus on developing stochastic approaches to end-user workflow scheduling that provides QoS guarantees by establishing a SLA. We also develop a novel 2-stage stochastic programming technique that aims at establishing a SLA with end-users regarding satisfying their workflow QoS requirements. We develop a scheduling (workload allocation) technique based on linear programming that embeds the negotiated workflow QoS into the program and model Grid services as generalised queues. This technique is shown to outperform existing scheduling techniques that don't rely on real-time performance information.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {132–140},
numpages = {9},
keywords = {Workflow, Broker, QoS, Stochastic Optimisation, Web Service},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.42,
author = {Sam, Yacine and Boucelma, Omar},
title = {Customizable Web Services Description, Discovery and Composition: An Attribute Based Formalism},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.42},
doi = {10.1109/WI.2007.42},
abstract = {We present in this article a theoretical framework for customizable Web services description, discovery and composition based on an attributive formalism.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {143–146},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.45,
author = {Sun, Yang and Zhuang, Ziming and Councill, Isaac G. and Giles, C. Lee},
title = {Determining Bias to Search Engines from Robots.Txt},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.45},
doi = {10.1109/WI.2007.45},
abstract = {Search engines largely rely on robots (i.e., crawlers or spiders) to collect information from the Web. Such crawling activities can be regulated from the server side by deploying the Robots Exclusion Protocol in a file called robots.txt. Ethical robots will follow the rules specified in robots.txt. Websites can explicitly specify an access preference for each robot by name. Such biases may lead to a "rich get richer" situation, in which a few popular search engines ultimately dominate the Web because they have preferred access to resources that are inaccessible to others. This issue is seldom addressed, although the robots.txt convention has become a de facto standard for robot regulation and search engines have become an indispensable tool for information access. We propose a metric to evaluate the degree of bias to which specific robots are subjected. We have investigated 7,593 websites covering education, government, news, and business domains, and collected 2,925 distinct robots.txt files. Results of content and statistical analysis of the data confirm that the robots of popular search engines and information portals, such as Google, Yahoo, and MSN, are generally favored by most of the websites we have sampled. The results also show a strong correlation between the search engine market share and the bias toward particular search engine robots.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {149–155},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.76,
author = {Lau, Raymond Y. K. and Li, Yuefeng and Xu, Yue},
title = {Mining Fuzzy Domain Ontology from Textual Databases},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.76},
doi = {10.1109/WI.2007.76},
abstract = {Ontology plays an essential role in the formalization of common information (e.g., products, services, relationships of businesses) for effective human-computer interactions. However, engineering of these ontologies turns out to be very labor intensive and time consuming. Although some text mining methods have been proposed for automatic or semi-automatic discovery of crisp ontologies, the robustness, accuracy, and computational efficiency of these methods need to be improved to support large scale ontology construction for real-world applications. This paper illustrates a novel fuzzy domain ontology mining algorithm for supporting real-world ontology engineering. In particular, contextual information of the knowledge sources is exploited for the extraction of high quality domain ontologies and the uncertainty embedded in the knowledge sources is modeled based on the notion of fuzzy sets. Empirical studies have confirmed that the proposed method can discover high quality fuzzy domain ontology which leads to significant improvement in information retrieval performance.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {156–162},
numpages = {7},
keywords = {Text Mining, Semantic Web., Fuzzy Domain Ontology, Fuzzy Sets},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.106,
author = {Zhou, Xujuan and Li, Yuefeng and Bruza, Peter and Wu, Sheng-Tang and Xu, Yue and Lau, Raymond Y. K.},
title = {Using Information Filtering in Web Data Mining Process},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.106},
doi = {10.1109/WI.2007.106},
abstract = {The amount of Web information is growing rapidly, improving the efficiency and accuracy of Web information retrieval is uphill battle. There are two fundamental issues regarding the effectiveness of Web information gathering: information mismatch and overload. To tackle these difficult issues, an integrated information filtering and sophisticated data processing model has been presented in this paper. In the first phase of the proposed scheme, an information filter that based on user search intents was incorporated in Web search process to quickly filter out irrelevant data. In the second data processing phase, a pattern taxonomy model (PTM) was carried out using the reduced data. PTM rationalizes the data relevance by applying data mining techniques that involves more rigorous computations. Several experiments have been conducted and the results show that more effective and efficient access Web information has been achieved using the new scheme.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {163–169},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.41,
author = {Jacquemont, Stephanie and Jacquenet, Francois and Sebban, Marc},
title = {Correct Your Text with Google},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.41},
doi = {10.1109/WI.2007.41},
abstract = {With the increasing amount of text files that are produced nowadays, spell checkers have become essential tools for everyday tasks of millions of end users. Among the years, several tools have been designed that show decent performances. Of course, grammatical checkers may improve corrections of texts, nevertheless, this requires large resources. We think that basic spell checking may be improved (a step towards) using the Web as a corpus and taking into account the context of words that are identified as potential misspellings. We propose to use the Google search engine and some machine learning techniques, in order to design a flexible and dynamic spell checker that may evolve among the time with new linguistic features.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {170–176},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.57,
author = {Fissaha Adafre, Sisay and Jijkoun, Valentin and de Rijke, Maarten},
title = {Fact Discovery in Wikipedia},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.57},
doi = {10.1109/WI.2007.57},
abstract = {We address the task of extracting focused salient information items, relevant and important for a given topic, from a large encyclopedic resource. Specifically, for a given topic (a Wikipedia article) we identify snippets from other articles in Wikipedia that contain important information for the topic of the original article, without duplicates. We compare several methods for addressing the task, and find that a mixture of content-based, link-based, and layout-based features outperforms other methods, especially in combination with the use of so-called reference corpora that capture the key properties of entities of a common type.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {177–183},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.60,
author = {van Zwol, Roelof},
title = {Flickr: Who is Looking?},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.60},
doi = {10.1109/WI.2007.60},
abstract = {This article presents a characterization of user behavior on Flickr, a popular on-line photo sharing service that allows users to store, search, sort and share their photos. Based on a sub-set of photos being uploaded during a 10 day window, we track the interest of users in those photos over a period of 50 days. In particular we investigate the user behavior on temporal, social, and spatial dimensions. Results show that the users are able to discover new photos within hours after being uploaded and that 50% of the photo views are generated within the first two days. The social networking behavior of users, and photo pooling are identified as the two major indicators related to a photo's popularity. Finally we show that the geographic distribution is more focussed around a geographic location for the infrequently viewed photos, than for the photos that attract a large number of views.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {184–190},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.27,
author = {Yan, Ping and Zhang, Zhu and Garcia, Ray},
title = {Automatic Website Comprehensibility Evaluation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.27},
doi = {10.1109/WI.2007.27},
abstract = {The Web provides easy access to a vast amount of informational content to the average person, who may often be interested in selecting websites that best match their learning objectives and comprehensibility level. Web content is generally not tagged for easy determination of its instructional appropriateness and comprehensibility level. Our research develops an analytical model, using a group of website features, to automatically determine the comprehensibility level of a website. These features, selected from a large pool of website features quantitatively measured, are statistically shown to be significantly correlated to website comprehensibility based on empirical studies. The automatically inferred comprehensibility index may be used to assist the average person, interested in using web content for self-directed learning, to find content suited to their comprehension level and filter out content which may have low potential instructional value.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {191–197},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.68,
author = {Bolelli, Levent and Ertekin, Seyda and Zhou, Ding and Giles, C. Lee},
title = {K-SVMeans: A Hybrid Clustering Algorithm for Multi-Type Interrelated Datasets},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.68},
doi = {10.1109/WI.2007.68},
abstract = {Identification of distinct clusters of documents in text collections has traditionally been addressed by making the assumption that the data instances can only be represented by homogeneous and uniform features. Many real-world data, on the other hand, comprise of multiple types of heterogeneous interrelated components, such as web pages and hyperlinks, online scientific publications and authors and publication venues to name a few. In this paper, we present KSVMeans, a clustering algorithm for multi-type interrelated datasets that integrates the well known K-Means clustering with the highly popular Support Vector Machines. The experimental results on authorship analysis of two real world web-based datasets show that K-SVMeans can successfully discover topical clusters of documents and achieve better clustering solutions than homogeneous data clustering.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {198–204},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.31,
author = {Fujimura, Shigeru and Fujimura, KO and Okuda, Hidenori},
title = {Blogosonomy: Autotagging Any Text Using Bloggers' Knowledge},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.31},
doi = {10.1109/WI.2007.31},
abstract = {There are at least three barriers to utilizing blog tags in classification or navigation: 40% of entries are not (from our observations) tagged, there are many orthographic or synonymous tag variations, and not all tags are informative.We propose a method of multi-autotagging, based on k-NN, which is a case-based classijication method. Our method also has the functions of merging tags with the same meaning and identifying informative tags. For realizing these functions, we propose the term weighting method named residual document frequency(RDF); it can score the similarity between tags. Experiments show the effectiveness of our methods. Our autotagging system is generic and can assign tag(s) to any text as well as blog entries although the training data is collected from the blogosophere.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {205–212},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.111,
author = {Hofgesang, Peter I.},
title = {Web Personalisation Through Incremental Individual Profiling and Support-Based User Segmentation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.111},
doi = {10.1109/WI.2007.111},
abstract = {Online personalised "my*" services are gaining popularity due to a growing customer need for information filtering and customisation. However, current systems mostly rely on some general usage and customer interaction in selecting components from prespecified blocks of content. The demand is great for high-quality unsupervised services on the customer side and for enabling techniques on the vendor side. Furthermore, individual profiles and, thus, personalised content should reflect changing individual behaviour. How do we efficiently build and maintain up-to-date personalised services for a large number of individuals? A compact and efficient, incrementally updatable representation of individual profiles is crucial. In addition, methods are required for efficient comparison of such profiles. Here we propose a methodology for building up-to-date personalised services. Individual profiles are represented as space-efficient prefix trees that are inherently easy to update incrementally. To measure the similarity of profiles, and also for the purpose of segmentation, we define a support-based metric that exploits the advantages of the tree-based structure. We evaluate our method on anonymised web data of 10,000 customers of an investment bank collected over 1.5 years.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {213–220},
numpages = {8},
keywords = {user profiling, clustering., Incremental web mining},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.97,
author = {Rojas, Carlos and Nasraoui, Olfa},
title = {Summarizing Evolving Data Streams Using Dynamic Prefix Trees},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.97},
doi = {10.1109/WI.2007.97},
abstract = {In stream data mining it is important to use the most recent data to cope with the evolving nature of the underlying patterns. Simply keeping the most recent records offers no flexibility about which data is kept, and does not exploit even minimal redundancies in the data (a first step towards pattern discovery). This paper focuses in how to construct and maintain efficiently (in one pass) a compact summary for data such as web logs and text streams. The resulting structure is a prefix tree, with ordering criterion that changes with time, such as an activity time stamp or attribute frequency. A detailed analysis of the factors that affect its performance is carried out, including empirical evaluations using the well known 20 Newsgroups data set. Guidelines for forgetting and tree pruning are also provided. Finally, we use this data structure to discover evolving topics from the 20 Newsgroups.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {221–227},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.86,
author = {Junejo, Khurum Nazir and Karim, Asim},
title = {PSSF: A Novel Statistical Approach for Personalized Service-Side Spam Filtering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.86},
doi = {10.1109/WI.2007.86},
abstract = {The volume of spam e-mails has grown rapidly in the last two years resulting in increasing costs to users, network operators, and e-mail service providers (ESPs). E-mail users demand accurate spam filtering with minimum effort from their side. Since the distribution of spam and non-spam e-mails is often different for different users a single filter trained on a general corpus is not optimal for all users. The question asked by ESPs is: How do you build robust and scalable automatic personalized spam filters? We address this question by presenting PSSF, a novel statistical approach for personalized service-side spam filtering. PSSF builds a discriminative classifier from a statistical model of spam and non-spam e-mails. A classifier is first built on a general training corpus that is then adapted in one or more passes of soft labeling and classifier rebuilding over each user's unlabeled e-mails. The statistical model captures the distribution of tokens in spam and non-spam e-mails. This model is robust in the sense that its size can be reduced significantly without degrading filtering performance. We evaluate PSSF on two datasets. The results demonstrate the superior performance and scalability of PSSF in comparison with other published results on the same datasets.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {228–234},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.77,
author = {Hosseini, Mehdi and Abolhassani, Hassan},
title = {Mining Search Engine Query Log for Evaluating Content and Structure of a Web Site},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.77},
doi = {10.1109/WI.2007.77},
abstract = {Mining search engine query log is a new method for evaluating web site link structure and information architecture. In this paper we propose a new query-URL co-clustering for a web site useful to evaluate information architecture and link structure. Firstly, all queries and clicked URLs corresponding to particular web site are collected from a query log as bipartite graph, one side for queries and the other side for URLs. Then a new content free clustering is applied to cluster queries and URLs concurrently. Afterwards, based on information entropy, clusters of URLs and queries will be used for evaluating link structure and information architecture respectively. Data sets of different web sites have been extracted from a huge query log to evaluate our method, and experiments show promising result.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {235–241},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.38,
author = {Ziegler, Cai-Nicolas and Skubacz, Michal},
title = {Content Extraction from News Pages Using Particle Swarm Optimization on Linguistic and Structural Features},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.38},
doi = {10.1109/WI.2007.38},
abstract = {Today's Web pages are commonly made up of more than merely one cohesive block of information. For instance, news pages from popular media channels such as Financial Times or Washington Post consist of no more than 30%-50% of textual news, next to advertisements, link lists to related articles, disclaimer information, and so forth. However, for many search-oriented applications such as the detection of relevant pages for an in-focus topic, dissecting the actual textual content from surrounding page clutter is an essential task, so as to maintain appropriate levels of document retrieval accuracy. We present a novel approach that extracts real content from news Web pages in an unsupervised fashion. Our method is based on distilling linguistic and structural features from text blocks in HTML pages, having a Particle Swarm Optimizer (PSO) learn feature thresholds for optimal classification performance. Empirical evaluations and benchmarks show that our approach works very well when applied to several hundreds of news pages from popular media in 5 languages.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {242–249},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.84,
author = {Yang, Yu-Jiu and Hu, Bao-Gang},
title = {Pairwise Constraints-Guided Non-Negative Matrix Factorization for Document Clustering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.84},
doi = {10.1109/WI.2007.84},
abstract = {Nonnegative Matrix Factorization (NMF) has been proven to be effective in text mining. However, since NMF is a well-known unsupervised components analysis technique, the existing NMF method can not deal with prior constraints, which are beneficial to clustering or classification tasks. In this paper, we address the text clustering problem via a novel strategy, called Pairwise Constraintsguided Non-negative Matrix Factorization (PCNMF for short). Differing from the traditional NMF method, the proposed method can capture the available abundance prior constraints in original space, which result in more effective for clustering or information retrieval. Therefore, PCNMF enforces the discriminative capability in the reduced space. Utilizing the appropriate transformation, PCNMF represents as a new optimization problem, which can be efficiently solved by an iterative approach. The cluster membership of each document can be easily determined as the standard NMF. Empirical studies based on Benchmark document corpus demonstrate appealing results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {250–256},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.32,
author = {Wang, Bo and Wang, Houfeng},
title = {Bootstrapping Both Product Properties and Opinion Words from Chinese Reviews with Cross-Training},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.32},
doi = {10.1109/WI.2007.32},
abstract = {We investigate the problem of identifying both product properties and opinion words for sentences in a unified process when only a much small labeled corpus is available. Naive Bayesian method is used in this process. Specifically, considering the fact that product properties and opinion words usually co-occur with high frequency in product review articles, a crosstraining method is proposed to bootstrap both of them, in which the two sub-tasks are boosted by each other iteratively. Experiment results show that with a much small labeled corpus cross-training could produce both product properties and opinion words which are very close to what Naive Bayesian Classifiers could do with a large labeled corpus..},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {259–262},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.54,
author = {Lin, Zhenjiang and Lyu, Michael R. and King, Irwin},
title = {Extending Link-Based Algorithms for Similar Web Pages with Neighborhood Structure},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.54},
doi = {10.1109/WI.2007.54},
abstract = {The problem of fnding similar pages to a given web page arises in many web applications such as search engine. In this paper, we focus on the link-based similarity measures which compute web page similarity solely from the hyperlinks of the Web. We first propose a simple model called the Extended Neighborhood Structure (ENS), which defines a bi-directional (in-link and out-link) and multi-hop neighborhood structure. Based on the ENS model, several existing similarity measures are extended. Preliminary experimental results show that the accuracy of the extended algorithms are signifcantly improved.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {263–266},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.110,
author = {Oh, Jong-Hoon and Isahara, Hitoshi},
title = {Validating Transliteration Hypotheses Using the Web: Web Counts vs. Web Mining},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.110},
doi = {10.1109/WI.2007.110},
abstract = {We describe a novel approach for validating transliteration hypotheses based on a Web mining technique. We implemented a machine transliteration system and generated Chinese, Japanese, and Korean transliteration hypotheses for given English words. Then, we mined the Web for features relevant to validating transliteration hypotheses. Finally we validated transliteration hypotheses using machine learning algorithms learned with the mined features. Comparing Web counts with our Web mining technique, our proposed method consistently performed better than systems based on Web counts, regardless of the language.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {267–270},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.46,
author = {Jung, Yuchul and Choi, Yoonjung and Myaeng, Sung-Hyon},
title = {Determining Mood for a Blog by Combining Multiple Sources of Evidence},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.46},
doi = {10.1109/WI.2007.46},
abstract = {Mood classification for blogs is useful in helping user-to-agent interaction for a variety of applications involving the web, such as user modeling, recommendation systems, and user interface fields. It is challenging at the same time because of the diversity of the characteristics of bloggers, their experiences, and the way moods are expressed. As an attempt to handle the diversity, we combine multiple sources of evidence for a mood type. Support Vector Machine based Mood Classifier (SVMMC) is integrated with Mood Flow Analyzer (MFA) that incorporates commonsense knowledge obtained from the general public (i.e. ConceptNet), the Affective Norms English Words (ANEW) list, and mood transitions. In combining the two different approaches, we employ a statistically weighted voting scheme based on the Support Vector Machine (SVM). For evaluation, we have built a mood corpus consisting of manually annotated blogs, which amounts to over 4000 blogs. Our proposed method outperforms SVMMC by 5.68% in precision. The improvement is attributed to the strategy of choosing more trustable classification results in an interleaving fashion between the SVMMC and our MFA.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {271–274},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.50,
author = {Yang, Changhua and Lin, Kevin Hsin-Yih and Chen, Hsin-Hsi},
title = {Emotion Classification Using Web Blog Corpora},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.50},
doi = {10.1109/WI.2007.50},
abstract = {In this paper, we investigate the emotion classification of web blog corpora using support vector machine (SVM) and conditional random field (CRF) machine learning techniques. The emotion classifiers are trained at the sentence level and applied to the document level. Our methods also determine an emotion category by taking the context of a sentence into account. Experiments show that CRF classifiers outperform SVM classifiers. When applying emotion classification to a blog at the document level, the emotion of the last sentence in a document plays an important role in determining the overall emotion.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {275–278},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.15,
author = {Duan, Qiguo and Miao, Duoqian and Wang, Ruizhi and Chen, Min},
title = {An Approach to Web Page Classification Based on Granules},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.15},
doi = {10.1109/WI.2007.15},
abstract = {Granular computing is a new conceptual and computing paradigm of information processing, the idea of which is the use of granules for problem solving at different granularities. Web page classification is an important research direction for Web mining. In this paper, we propose an approach to Web page classification based on granules. Some concepts are defined firstly. Then, a Web page classification framework based on granules is built and an automatic classification algorithm is proposed. Experiment results demonstrate that the proposed approach is promising and effective.1},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {279–282},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.23,
author = {Qian, Ting and Qiu, Lin},
title = {Automatic Correction of Idiomatic Usage in English UsingWeb Search},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.23},
doi = {10.1109/WI.2007.23},
abstract = {Non-native English speakers often have problems determining the exact form of an idiomatic expression while they have some vague idea about the key words in them. In this paper, we describe a system called Webtionary that allows users to consult idiomatic usage by entering a questionable expression. Webtionary uses web search to find candidate corrections and suggests expressions that are commonly used in writing and semantically-related to the user query. Evaluation results show that Webtionary significantly outperforms direct web search in providing useful suggestions.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {283–286},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.56,
author = {Li, Qing and Candan, K. Selcuk and Qi, Yan},
title = {Extracting Relevant Snippets FromWeb Documents through Language Model Based Text Segmentation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.56},
doi = {10.1109/WI.2007.56},
abstract = {Extracting a query-oriented snippet (or passage) and highlighting the relevant information in long document can help reduce the result navigation cost of end users. While the traditional approach of highlighting matching keywords helps when the search is keyword oriented, finding appropriate snippets to represent matches to more complex queries requires novel techniques that can help characterize the relevance of various parts of a document to the given query, succinctly. In this paper, we present a languagemodel based method for accurately detecting the most relevant passages of a given document. Unlike previous works in passage retrieval which focus on searching relevance nodes for filtering of preoccupied passages, we focus on query-informed segmentation for snippet extraction. The algorithms presented in this paper are currently being deployed in OASIS, a system to help reduce the navigational load of blind users in accessing Web-based digital libraries.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {287–290},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.58,
author = {Chang, Hung-Chi and Wang, Jenq-Haur and Chiu, Chih-Yi},
title = {Finding Event-Relevant Content from the Web Using a Near-Duplicate Detection Approach},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.58},
doi = {10.1109/WI.2007.58},
abstract = {In online resources, such as news and weblogs, authors often extract articles, embed content, and comment on existing articles related to a popular event. Therefore, it is useful if authors can check whether two or more articles share common parts for further analysis, such as cocitation analysis and search result improvement. If articles do have parts in common, we say the content of such articles is event-relevant. Conventional text classification methods classify a complete document into categories, but they cannot represent the semantics precisely or extract meaningful event-relevant content. To resolve these problems, we propose a near-duplicate detection approach for finding event-relevant content in Web documents. The efficiency of the approach and the proposed duplicate set generation algorithms make it suitable for identifying event-relevant content. The experiment results demonstrate the potential of the proposed approach for use in weblogs.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {291–294},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.85,
author = {Yang, Kai-Hsiang and Chung, Jen-Ming and Ho, Jan-Ming},
title = {PLF: A Publication List Web Page Finder for Researchers},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.85},
doi = {10.1109/WI.2007.85},
abstract = {Finding and keeping track of other researchers' publication lists is an essential activity for every researcher, because they often contain citations not found elsewhere and may provide access to information, such as slides and talks, which can help other researchers keep abreast of state-ofthe-art knowledge and technology. There are many different ways to generate publication list web pages, and a researcher may have several different versions of a publication list on the Web because he holds different positions. So it is difficult to find the correct publication list web page from the top results retrieved from search engines, especially when we only know the name of the researcher. Very few works have addressed the problem. In this paper, we propose a system called the "Publication List Web Page Finder" (PLF), which can automatically find the publication list web pages for a given researcher's name. The PLF system is an automatic and language-independent system, and its main idea is that publication list web pages often contain many citations about a specific researcher, so the system uses those citations as clues to find out publication list web pages. Our experimental results show that the PLF system outperforms other approaches, especially when a researcher has multiple publication list web pages.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {295–298},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.63,
author = {Herbert, Joseph P. and Yao, JingTao},
title = {Growing Hierarchical Self-Organizing Maps for Web Mining},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.63},
doi = {10.1109/WI.2007.63},
abstract = {Many information retrieval and machine learning methods have not evolved in order to be applied to the Web. Two main problems in applying some machine learning techniques for Web mining are the dynamic and ever-changing nature of Web data and the sheer size of possible dimensions that this data could portray. One such technique, self-organizing maps (SOMs), have been enhanced to deal with these two problems individually. The growing hierarchical self-organizing map can adapt to the dynamic data present on the Web by changing its topology according to the amount of change in input size. In addition, it reduces local dimensionality by splitting features into levels. We extend this model by including bidirectional update propagation over the levels of the hierarchy. We demonstrate the effectiveness of the new approach with a Web-based news coverage example.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {299–302},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.87,
author = {Chen, Po Chih and Moh, Teng-Sheng},
title = {Quality Improvement of Clustering Engine in the Internet Based on Correlation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.87},
doi = {10.1109/WI.2007.87},
abstract = {Search engines give so many results that a user cannot handle them all in a short period of time. Many approaches have been proposed to alleviate this problem. For example, some approaches try to add personalized features and some try to group the results into different categories. The latter one is called a clustering engine, which is the emphasis of this paper. It first reviews several existing approaches such as STC, SHOC, LINGO and SnakeT. It then gives a new approach called HICSEC (Hierarchical Clustering Search Engine with Correlation) to improve the accuracy of clustering by the Correlation calculated with the Singular Value Decomposition.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {303–306},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.8,
author = {Ghorbani, Ali A. and Xu, Xiaowen},
title = {A Fuzzy Markov Model Approach for Predicting User Navigation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.8},
doi = {10.1109/WI.2007.8},
abstract = {User navigation is an interesting aspect in Web usage mining. Analysis of this issue can be of great benefit in discovering users' behavior. This paper presents a fuzzy approach for predicting users' navigation paths using the Markov chain model. A standard Markov model can be used to predict the ID of the next page. However, our proposed approach can predict not only users' next requests for pages, but also the time-duration to be spent on the requests. The experimental results show that our method is highly accurate (average 77.9%) in session prediction. Even though the standard methods also perform well (average 78.9%), our proposed approach},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {307–311},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.107,
author = {Mehlitz, Martin and Kunegis, J\'{e}rome and Albayrak, Sahin},
title = {Using Novel IR Measures to Learn Optimal Cluster Structures for Web Information Retrieval},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.107},
doi = {10.1109/WI.2007.107},
abstract = {The Internet is a vast resource of information. Unfortunately, finding and accessing this information is often a very cumbersome task even with existing information platforms. Searching on the WWW suffers from the fact that almost every word is ambiguous to a certain degree in the information-rich environment of the Internet. Clustering search results is a way to solve this problem. This paper demonstrates how to employ novel Information Retrieval measures to derive optimal parametrizations for a cluster algorithm.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {312–316},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.80,
author = {Barouni-Ebrahimi, M. and Ghorbani, Ali A.},
title = {On Query Completion in Web Search Engines Based on Query Stream Mining},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.80},
doi = {10.1109/WI.2007.80},
abstract = {In this paper, YourEye, the real-time phrase recommender is introduced that suggests the related frequent phrases to the incomplete user query. The frequent phrases are extracted from within previous queries based on a new frequency rate metric suitable for query stream mining. The advantages of YourEye compared to Google Suggest, a service powered by Google for phrase suggestion, is described. The experimental results also confirm the significant benefit of monitoring phrases instead of queries. The number of the monitored elements significantly reduces that results in smaller memory consumption as well as better performance.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {317–320},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.26,
author = {Makrehchi, Masoud and Kamel, Mohamed S.},
title = {Automatic Taxonomy Extraction Using Google and Term Dependency},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.26},
doi = {10.1109/WI.2007.26},
abstract = {An automatic taxonomy extraction algorithm is proposed. Given a set of terms or terminology related to a subject domain, the proposed approach uses Google page count to estimate the dependency links between the terms. A taxonomic link is an asymmetric relation between two concepts. In order to extract these directed links, neither mutual information nor normalized Google distance can be employed. Using the new measure of information theoretic inclusion index, term dependency matrix, which represents the pair-wise dependencies, is obtained. Next, using a proposed algorithm, the dependency matrix is converted into an adjacency matrix, representing the taxonomy tree. In order to evaluate the performance of the proposed approach, it is applied to several domains for taxonomy extraction.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {321–325},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.91,
author = {Arevian, Garen},
title = {Recurrent Neural Networks for Robust Real-World Text Classification},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.91},
doi = {10.1109/WI.2007.91},
abstract = {This paper explores the application of recurrent neural networks for the task of robust text classification of a real-world benchmarking corpus. There are many well-established approaches which are used for text classification, but they fail to address the challenge from a more multi-disciplinary viewpoint such as natural language processing and artificial intelligence. The results demonstrate that these recurrent neural networks can be a viable addition to the many techniques used in web intelligence for tasks such as context sensitive email classification and web site indexing.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {326–329},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.70,
author = {Ligozat, Anne-Laure and Grau, Brigitte and Vilnat, Anne and Robba, Isabelle and Grappy, Arnaud},
title = {Lexical Validation of Answers in Question Answering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.70},
doi = {10.1109/WI.2007.70},
abstract = {Question answering (QA) aims at retrieving precise information from a large collection of documents, typically the Web. Different techniques can be used to find relevant information, and to compare these techniques, it is important to evaluate question answering systems. The objective of an Answer Validation task is to estimate the correctness of an answer returned by a QA system for a question, according to the text snippet given to support it. In this article, we present a lexical strategy for deciding if the snippets justify the answers, based on our own question answering system. We discuss our results, and show the possible extensions of our strategy.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {330–333},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.33,
author = {Wang, Xia and Vitvar, Tomas and Hauswirth, Manfred and Foxvog, Doug},
title = {Building Application Ontologies from Descriptions of Semantic Web Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.33},
doi = {10.1109/WI.2007.33},
abstract = {Different ontologies used in semantic web services fields raise numerous interoperation and communication problems with respect to service discovery, composition, and execution. The current approaches for ontology mediation often failed due to their lack of sufficient semantic expressiveness and reasoning capability. In this paper1, we present a novel approach allowing ontologies to provide self-contained semantics for service applications. We show how desired application ontologies can be generated using a new merging algorithm for service ontologies. We also show some experimental results and compare them to the output of the PROMPT ontology merging tool.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {337–343},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.81,
author = {Stojanovic, Ljiljana and Stojanovic, Nenad and Ma, Jun},
title = {On the Conceptual Tagging: An Ontology Pruning Use Case},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.81},
doi = {10.1109/WI.2007.81},
abstract = {In this paper we present a method for tagging web pages using more formal conceptual structures than a set of keywords. We have developed a formal Tag model and a method to map a set of keywords on it. Moreover, this model is used as the basis for the conceptual tag refinement, which searches for terms which are conceptually related to the tags that are assigned to an information source. In that way the meaning of the tags can be disambiguated, which supports a better usage of tags for further management of tagged documents. We have developed a software tool, an annotation framework, which realizes this approach. We present results from the first evaluation studies regarding its application for ontology pruning.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {344–350},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.82,
author = {Tao, Xiaohui and Li, Yuefeng and Zhong, Ning and Nayak, Richi},
title = {Ontology Mining for PersonalizedWeb Information Gathering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.82},
doi = {10.1109/WI.2007.82},
abstract = {It is well accepted that ontology is useful for personalized Web information gathering. However, it is challenging to use semantic relations of "kind-of", "part-of", and "related-to" and synthesize commonsense and expert knowledge in a single computational model. In this paper, a personalized ontology model is proposed attempting to answer this challenge. A two-dimensional (Exhaustivity and Specificity) method is also presented to quantitatively analyze these semantic relations in a single framework. The proposals are successfully evaluated by applying the model to a Web information gathering system. The model is a significant contribution to personalized ontology engineering and concept-based Web information gathering in Web Intelligence.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {351–358},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.14,
author = {Yao, Limin and Tang, Jie and Li, Juanzi},
title = {A Unified Approach to Researcher Profiling},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.14},
doi = {10.1109/WI.2007.14},
abstract = {This paper addresses the issue of researcher profiling. By researcher profiling, we mean building a semantic profile for an academic researcher, by identifying and annotating information from the Web. Previously, person profile annotation was often undertaken separately in an ad-hoc fashion. This paper first gives a formalization of the entire problem and proposes a unified approach to perform the task using Conditional Random Fields (CRF). The paper shows that with introduction of a set of tags, most of the annotation tasks can be performed within this approach. Experiments show that significant improvements over the separated method can be obtained, because the subtasks of annotation are interdependent and should be performed together. The method has been applied to expert finding. Experimental results show that the performance of expert finding can be significantly improved by using the profiling method.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {359–366},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.17,
author = {Osman, Taha and Thakker, Dhavalkumar and Schaefer, Gerald and Lakin, Phil},
title = {An Integrative Semantic Framework for Image Annotation and Retrieval},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.17},
doi = {10.1109/WI.2007.17},
abstract = {Most public image retrieval engines utilise free-text search mechanisms, which often return inaccurate matches as they in principle rely on statistical analysis of query keyword recurrence in the image annotation or surrounding text. In this paper we present a semantically-enabled image annotation and retrieval engine that relies on methodically structured ontologies for image annotation, thus allowing for more intelligent reasoning about the image content and subsequently obtaining a more accurate set of results and a richer set of alternatives matchmaking the original query. Our semantic retrieval technology is designed to satisfy the requirements of the commercial image collections market in terms of both accuracy and efficiency of the retrieval process. We also present our efforts in further improving the recall of our retrieval technology by deploying an efficient query expansion technique.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {366–373},
numpages = {8},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.103,
author = {Espinosa Perald, S. and Kaya, A. and Melzer, S. and Moller, R. and Wessel, M.},
title = {Towards a Media Interpretation Framework for the Semantic Web},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.103},
doi = {10.1109/WI.2007.103},
abstract = {We present a formal framework for media interpretation that leverages low-level information extraction to a higher level of abstraction in order to support semantics-based information retrieval for the Semantic Web. The overall goal of the framework is to provide high-level content descriptions of documents for maximizing precision and recall of semantics-based information retrieval.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {374–380},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.104,
author = {Iosif, Elias and Potamianos, Alexandros},
title = {Unsupervised Semantic Similarity Computation UsingWeb Search Engines},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.104},
doi = {10.1109/WI.2007.104},
abstract = {In this paper, we propose two novel web-based metrics for semantic similarity computation between words. Both metrics use a web search engine in order to exploit the retrieved information for the words of interest. The first metric considers only the page counts returned by a search engine, based on the work of [1]. The second downloads a number of the top ranked documents and applies "widecontext" and "narrow-context" metrics. The proposed metrics work automatically, without consulting any human annotated knowledge resource. The metrics are compared with WordNet-based methods. The metrics' performance is evaluated in terms of correlation with respect to the pairs of the commonly used Charles - Miller dataset. The proposed "wide-context" metric achieves 71% correlation, which is the highest score achieved among the fully unsupervised metrics in the literature up to date.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {381–387},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.83,
author = {Wang, Shenghui and Pan, Jeff Z.},
title = {Ontology-Based Integration and Retrieval over Multiple Quantities - What If "Ovate Leaves and Often Blue to Purple Flowers"},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.83},
doi = {10.1109/WI.2007.83},
abstract = {Information integration and retrieval have been important problems for many information systems - it is hard to combine multidimensional and parallel information and make them available for application queries. In our previous work [12], we have shown how to use ontologies to facilitate integrating and querying parallel but single dimensional information. In this paper, we further investigate how to take advantage of ontologies to facilitate integrating parallel information and querying over multiple quantities.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {388–394},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.36,
author = {Wang, James Z. and Taylor, William},
title = {Concept Forest: A New Ontology-Assisted Text Document Similarity Measurement Method},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.36},
doi = {10.1109/WI.2007.36},
abstract = {Although using ontologies to assist information retrieval and text document processing has recently attracted more and more attention, existing ontologybased approaches have not shown advantages over the traditional keywords-based Latent Semantic Indexing (LSI) method. This paper proposes an algorithm to extract a concept forest (CF) from a document with the assistance of a natural language ontology, the WordNet lexical database. Using concept forests to represent the semantics of text documents, the semantic similarities of these documents are then measured as the commonalities of their concept forests. Performance studies of text document clustering based on different document similarity measurement methods show that the CF-based similarity measurement is an effective alternative to the existing keywords-based methods. In particular, this CFbased approach has obvious advantages over the existing keywords-based methods, including LSI, in processing short text documents or in P2P or live news environments where it is impractical to collect the entire document corpus for analysis.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {395–401},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.47,
author = {Zavitsanos, Elias and Paliouras, Georgios and Vouros, George A. and Petridis, Sergios},
title = {Discovering Subsumption Hierarchies of Ontology Concepts from Text Corpora},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.47},
doi = {10.1109/WI.2007.47},
abstract = {This paper proposes a method for learning ontologies given a corpus of text documents. The method identifies concepts in documents and organizes them into a subsumption hierarchy, without presupposing the existence of a seed ontology. The method uncovers latent topics in terms of which document text is being generated. These topics form the concepts of the new ontology. This is done in a language neutral way, using probabilistic space reduction techniques over the original term space of the corpus. Given multiple sets of concepts (latent topics) being discovered, the proposed method constructs a subsumption hierarchy by performing conditional independence tests among pairs of latent topics, given a third one. The paper provides experimental results over the GENIA corpus from the domain of biomedicine.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {402–408},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.49,
author = {Guo, Yuanbo and Heflin, Jeff},
title = {Document-Centric Query Answering for the Semantic Web},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.49},
doi = {10.1109/WI.2007.49},
abstract = {In this paper, we propose document-centric query answering, a novel form of query answering for the Semantic Web. We discuss how we have built a knowledge base system to support the new queries. In particular, we describe the key techniques used in the system in order to address scalability issues. In addition, we show encouraging experimental results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {409–415},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.24,
author = {Speretta, Mirco and Gauch, Susan},
title = {Automatic Ontology Identification for Reuse},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.24},
doi = {10.1109/WI.2007.24},
abstract = {The increasing interest in the Semantic Web is producing a growing number of publicly available domain ontologies. These ontologies are a rich source of information that could be very helpful during the process of engineering other domain ontologies. We present an automatic technique that, given a set of Web documents, selects appropriate domain ontologies from a collection of pre-existing ontologies. We empirically compare an ontology match score that is based on statistical techniques with simple keyword matching algorithms. The algorithms were tested on a set of 183 publicly available ontologies and documents representing ten different domains. Our algorithm was able to select the correct domain ontology as the top ranked ontology 8 out of 10 times.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {419–422},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.108,
author = {Nauman, Mohammad and Khan, Shahbaz},
title = {Using PersonalizedWeb Search for Enhancing Common Sense and Folksonomy Based Intelligent Search Systems},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.108},
doi = {10.1109/WI.2007.108},
abstract = {A large part of the modern web is characterized by usergenerated content categorized using collaborative tagging or folksonomy. It becomes difficult to search for relevant content because of ambiguity in lexical representation of concepts and variances in preferences of users. With more and more services relying on tags for content categorization, it is important that search techniques evolve to better suit the scenario. A promising approach towards solving these problems is to use machine common sense in conjunction with folksonomy. A past attempt to use this approach has shown positive results in finding relevant content but it does not address the issue of noise in search results. In this paper, we use the personalized web search technique of traditional web search systems to address the issue of irrelevant search results in common sense and folksonomy based search systems. In personalized web search, results are reflective of user's preferences, which are decided by search history and categories of interest. We propose modifications to personalized web search technique. Using this modified approach, we extend the basic common sense and folksonomy based search systems to address the issue of noise in search results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {423–426},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.73,
author = {Karali, Isambo},
title = {Logic Programming to Address Issues of the Semantic Web},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.73},
doi = {10.1109/WI.2007.73},
abstract = {The size of the Web and its increase rate made it cumber-some to locate high precision results to a requested piece of information. The Semantic Web provides a framework and a set of technologies enabling an effective machine processable information, aiming at a better computer-computer and human-computer communication. What if, though, we use a both machine processable and human understandable approach which can also apply in existent HTML Web sources? In this work, we investigate the problems being solved with the Semantic Web technologies and how this can be coped with Logic Programming techniques, especially Modular Logic Programming. We discuss issues from the data level, metadata and reasoning. Last but not least, we discuss agents. What is more is that, we claim that these techniques can be applied in the current Web information sources providing formal semantics for some aspects of the traditional Web.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {427–430},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.61,
author = {Wang, Hai H. and Saleh, Ahmed and Payne, Terry and Gibbins, Nick},
title = {Formal Specification of OWL-S with Object-Z: The Static Aspect},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.61},
doi = {10.1109/WI.2007.61},
abstract = {To support standardization and tool support of OWL-S, a formal semantics of the language is highly desirable. In this paper, we present a formal Object-Z semantics of OWL-S. This model not only provides a formal unambiguous model which can be used to develop tools and facilitate future development, but as demonstrated in the paper, can be used to identify and eliminate errors in the current documentation.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {431–434},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.98,
author = {Ghoula, Nizar and Khelif, Khaled and Dieng-Kuntz, Rose},
title = {Supporting Patent Mining by Using Ontology-Based Semantic Annotations},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.98},
doi = {10.1109/WI.2007.98},
abstract = {Semantic web approach seems interesting for supporting content mining of millions of patents accessible through the Web. In this paper, we describe our approach for generating semantic annotations on patents, by relying on the structure and on a semantic representation of patent documents. We use both the structure of the patent documents and their textual contents processed by Natural Language Processing (NLP) tools. This method, primarily aimed at helping biologists use patent information can be generalized to all kinds of domains or of structured documents.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {435–438},
numpages = {4},
keywords = {ontology, Natural Language Processing., semantic annotations, patent mining, semantic web},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.52,
author = {Xin, Xin and Li, Juanzi and Tang, Jie},
title = {Enhancing Semantic Web by Semantic Annotation: Experiences in Building an Automatic Conference Calendar},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.52},
doi = {10.1109/WI.2007.52},
abstract = {In this paper, we describe a Semantic Web application that builds a customizable conference calendar. In contrast to previous works aiming at manually creating a list of upcoming/current and past conferences, in this work we aim at providing a semantic conference calendar which automatically extracts information from the web using semantic annotation. In this system, to build a calendar, the user simply needs to specify what conferences he/she is interested in. The system finds, extracts, and updates the semantic information from the Web. We propose a unified approach for semantic annotation of the conference calendar. We also present evaluations of our approach on real-world data.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {439–442},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.5,
author = {Abdul-Ghafour, Samer and Ghodous, Parisa and Shariat, Behzad and Perna, Eliane},
title = {A Common Design-Features Ontology for Product Data Semantics Interoperability},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.5},
doi = {10.1109/WI.2007.5},
abstract = {In a collaborative design environment, various software tools are utilized to enhance the product development. This entails a meaningful representation and exchange of product data semantics across these different systems. Semantic interoperability of product information refers to enabling the exchange of design intelligence, including construction history, parameters, features, and constraints. This is a crucial difference compared to current standards such as STEP that deliver "dumb" geometry, where no design intent is associated. To enable semantics data exchange, we propose an ontology-based approach, consisting in developing a "Common Design Features Ontology", called CDFO. Interoperability among ontologies is fulfilled by defining several mapping rules. We use a descriptive logic-based language, notably OWL DL to represent formally our ontology.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {443–446},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.55,
author = {Drabent, Wlodzimierz and Wilk, Artur},
title = {Extending XML Query Language Xcerpt by Ontology Queries},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.55},
doi = {10.1109/WI.2007.55},
abstract = {The paper addresses a problem of combining XML querying with ontology reasoning. We present an extension of a rule-based XML query and transformation language Xcerpt. The extension allows to interface an ontology reasoner from Xcerpt programs. In this way querying can employ the ontology information, for instance to filter out semantically irrelevant answers. The approach employs an existing Xcerpt engine and ontology reasoner; no modifications are required. We present the semantics of extended Xcerpt and an implementation algorithm. Communication between Xcerpt programs and ontology reasoner is based on DIG interface.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {447–451},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.53,
author = {Chen, Chuming and Matthews, Manton M.},
title = {Extending Description Logic for Reasoning about Ontology Evolution},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.53},
doi = {10.1109/WI.2007.53},
abstract = {Ontologies play a key role in achieving global automatic information integration and sharing on the Semantic Web. They allow intelligent applications to exchange information through a shared and formal conceptualization of an application domain. Understanding ontology evolution can help both ontology developers and users evaluating the potential consequences of ontology changes and act accordingly. Our contribution is proposing a temporal paradigm for ontology evolution and extending Description Logic with Temporal Logic operators to formally characterize and reason about ontology evolution. We investigate related reasoning problems and algorithm.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {452–456},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.74,
author = {Spiliopoulos, Vassilis and Vouros, George A. and Karkaletsis, Vangelis},
title = {Mapping Ontologies Elements Using Features in a Latent Space},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.74},
doi = {10.1109/WI.2007.74},
abstract = {This paper proposes a method for the mapping of ontologies that, in a greater extent than other approaches, discovers and exploits sets of latent features for approximating the intended meaning of ontology elements. This is done by applying the reverse generative process of the Latent Dirichlet Allocation model. Similarity between element pairs is computed by means of the Kullback-Leibler divergence measure. Experimental results show the potential of the method.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {457–460},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.75,
author = {Liu, Jiahui and Birnbaum, Larry},
title = {Measuring Semantic Similarity between Named Entities by Searching the Web Directory},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.75},
doi = {10.1109/WI.2007.75},
abstract = {The importance of named entities in information retrieval and knowledge management has recently brought interest in characterizing semantic relationships between entities. In this paper, we propose a method for measuring semantic similarity, an important type of semantic relationship, between entities. The method is based on Google Directory, a search interface to the Open Directory Project. Via the search engine, we can locate the web pages relevant to an entity and automatically create a profile of the entity according to the directory assignments of its web pages, which capture various features of the entity. Using their profiles, the semantic similarity between entities can be measured in different dimensions. We apply the semantic similarity measurement to two knowledge acquisition tasks: thesaurus construction of entities and fine grained categorization of entities. Our experiments demonstrate that the proposed method works effectively in these two tasks.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {461–465},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.10,
author = {Thanh Le, Bach and Dieng-Kuntz, Rose},
title = {A Graph-Based Algorithm for Alignment of OWL Ontologies},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.10},
doi = {10.1109/WI.2007.10},
abstract = {This paper presents ASCO3 algorithm, aimed at finding mappings between entities of two ontologies represented in OWL DL/Lite. ASCO3 algorithm searches the maximal common subgraph of two graphs representing two ontologies, and relies on an algorithm of search of the maximal clique of their association graph.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {466–469},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.89,
author = {Cherfi, Hacene and Corby, Olivier and Masia-Tissot, Cyril},
title = {RDF(S) and SPARQL Expressiveness in Engineering Design Patterns},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.89},
doi = {10.1109/WI.2007.89},
abstract = {In engineering product design, using RDF(S) Semantic Web (SW) language, the number of instances to handle is usually large. We want to easily put additional information on property values and metadata over selected instances. Moreover, we want to use standard SPARQL in order to query the RDF graph. We have designed solutions to address a family of problems related to instance property values in engineering design patterns (order, metadata, etc.).},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {470–473},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.90,
author = {Corby, Olivier and Faron-Zucker, Catherine},
title = {RDF/SPARQL Design Pattern for Contextual Metadata},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.90},
doi = {10.1109/WI.2007.90},
abstract = {The basic principle of the Semantic Web carried by the RDF data model is that a collection of RDF statements coexist all together and are universally true. However some case study imply contextual relevancy and truth. The SPARQL query language is provided with patterns enabling to choose the RDF dataset against which a query is executed. This is a first step to handle contextual metadata. Based on it, we present in this paper a design pattern to handle contextual metadata hierarchically organized. This is done by means of a subStateOf property reifying RDF entailment between contextual graphs. The subStateOf relation is modelled within RDF and therefore context hierarchies can be described and queried by means of SPARQL queries. We propose a slight syntactic extension to SPARQL to facilitate the query of context hierarchies, together with rewriting rules to return to standard SPARQL.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {470–473},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.94,
author = {Chen, Liming and Roberts, Craig},
title = {Semantic Tagging for Large-Scale Content Management},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.94},
doi = {10.1109/WI.2007.94},
abstract = {This paper introduces a novel approach to large scale content management based on the synergy and mashup of the Semantic Web and collaborative tagging technologies. We propose a generic conceptual architecture for community oriented semantic tagging and discusses the functionality and interplay of its key components. The approach has been applied to a real world application - an Open Online Publishing System (OOPS) to enable individuals to achieve flexible online publishing, open accesses and effective discovery. A prototype OOPS system and underpinning technological infrastructure have been developed to demonstrate the approach.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {478–481},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.21,
author = {Wetzker, Robert and Alpcan, Tansu and Bauckhage, Christian and Umbrath, Winfried and Albayrak, Sahin},
title = {An Unsupervised Hierarchical Approach to Document Categorization},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.21},
doi = {10.1109/WI.2007.21},
abstract = {We propose a hierarchical approach to document categorization that requires no pre-configuration and maps the semantic document space to a predefined taxonomy. The utilization of search engines to train a hierarchical classifier makes our approach more flexible than existing solutions which rely on (human) labeled data and are bound to a specific domain. We show that the structural information given by the taxonomy allows for a context aware construction of search queries and leads to higher tagging accuracy. We test our approach on different benchmark datasets and evaluate its performance on the single- and multi-tag assignment tasks. The experimental results show that our solution is as accurate as supervised classifiers for web page classification and still performs well when categorizing domain specific documents.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {482–486},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.99,
author = {Neshati, Mahmood and Alijamaat, Ali and Abolhassani, Hassan and Rahimi, Afshin and Hoseini, Mehdi},
title = {Taxonomy Learning Using Compound Similarity Measure},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.99},
doi = {10.1109/WI.2007.99},
abstract = {Taxonomy learning is one of the major steps in ontology learning process. Manual construction of taxonomies is a time-consuming and cumbersome task. Recently many researchers have focused on automatic taxonomy learning, but still quality of generated taxonomies is not satisfactory. In this paper we have proposed a new compound similarity measure. This measure is based on both knowledge poor and knowledge rich approaches to find word similarity. We also used Machine Learning Technique (Neural Network model) for combination of several similarity methods. We have compared our method with simple syntactic similarity measure. Our measure considerably improves the precision and recall of automatic generated taxonomies.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {487–490},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.20,
author = {Krummenacher, Reto and Simperl, Elena and Fensel, Dieter},
title = {An Ontology-Driven Approach To Reflective Middleware},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.20},
doi = {10.1109/WI.2007.20},
abstract = {Recent work in the field of middleware technology proposes semantic spaces as a tool for coping with the scalability, heterogeneity and dynamism issues arising in large scale distributed environments. Reflective middleware moreover offers answers to the needs for adaptivity and selfdetermination of systems where mobility and ubiquity add to such environments. Based on experiences with traditional middleware we argue that ontology-driven management is a major advancement for semantic spaces and provides the fundamental means for reflection. By means of ontologies, and ontology-based reasoning services we can implement automatic adaptation of the middleware's functionality to environmental changes and user desires.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {493–499},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.101,
author = {Hartmann, Bj\"{o}rn-Oliver and Bohm, Klemens and Khachatryan, Andranik and Schosser, Stephan},
title = {The Dangers of Poorly Connected Peers in Structured P2P Networks and a Solution Based on Incentives},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.101},
doi = {10.1109/WI.2007.101},
abstract = {This paper analyzes structured P2P systems where peers choose both their interaction mode, i.e., how they process incoming queries, and additional contacts in the network autonomously. Since additional contacts incur additional costs, a new kind of free riding behavior, namely having only few contacts, comes into the fray. We refer to it as deliberately poor connectedness (dpc). In this paper, we show that dpc is dominant in many situations. This leads to networks with a low degree of connectivity and a higher overall forwarding load than necessary. We then propose an incentive mechanism against dpc and demonstrate its effectiveness using a formal analysis and experiments.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {500–508},
numpages = {9},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.105,
author = {Shi, Wei and Wu, Jian and Li, Ying and Wu, Zhaohui and Wang, Bo},
title = {Using Improved FOAF to Enhance BPEL-Extracted RBAC Capability},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.105},
doi = {10.1109/WI.2007.105},
abstract = {BPEL can automate orchestrations for crossorganizational web services; however, it meets a serious challenge from modeling human-intensive business activities, especially from addressing access control for human coordination considering complex interpersonal relationship in modern business. This paper analyzes the importance of human-intensive processes and introduces several additional types of BPEL constructs, then discusses RBAC Model extracted from BPEL process, finally uses improved FOAF to enhance RBAC Model in BPEL. The goal of our work is to enhance human coordination capability in BPEL-based business processes by using RBAC model and improved FOAF.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {511–514},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.102,
author = {Bo, Yang and Zheng, Qin and Fan, Yu and Jun, Qin},
title = {The Soundness and Completeness Proof of Agent Intention in AgentSpeak},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.102},
doi = {10.1109/WI.2007.102},
abstract = {Autonomy is one of the characteristics that agent has which distinguish agent systems from the other conceptualisations within Computer Science. To prove the validity of intention execution in AgentSpeak, according to the agent's goal, we construct a model-theoretic semantics of AgentSpeak and an informal interpretation of agent program. Then we give an equivalence theorem of intention execution for AgentSpeak that the sequence of actions produced by an agent written in AgentSpeak is equivalent with the intention produced by the model that satisfies the belief set and plan set of agent.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {515–518},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.72,
author = {Mitra, Saayan and Basu, Samik and Kumar, Ratnesh},
title = {Local and On-the-Fly Choreography-Based Web Service Composition},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.72},
doi = {10.1109/WI.2007.72},
abstract = {We present a goal-directed, local and on-the-fly algorithm for verifying the existence and synthesizing a choreographer forWeb service composition. We use i/o-automata to represent services, the desired functionality of the composition, and a choreographer to achieve the desired service by composing the existing ones. Choreographer existence and synthesis are typically performed by identifying all possible compositions realizable from the existing services and verifying whether one such composition conforms to the desired required functionality. Such a technique is subject to state-space explosion. In light of this, we have developed a tabled-logic programming technique which generates and explores compositions in a goal-directed fashion to prove/disprove the existence of choreographer and to infer whether the desired functionality is realizable. We present a prototype implementation and show the practical applicability of our technique using a variety of composition problems with the corresponding computational savings in terms of number of states and transitions explored.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {521–527},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.9,
author = {Agarwal, Sudhir},
title = {A Goal Specification Language for Automated Discovery and Composition of Web Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.9},
doi = {10.1109/WI.2007.9},
abstract = {In order to find suitableWeb services from a large collection of Web services, automatic support is needed to filter out Web services relevant according to some criteria specified by the user. In real business scenarios constraints on the types of input and output parameters are often not sufficient. Rather one wishes to specify constraints on relationships of input and output parameters, interaction pattern and non-functional properties ofWeb services. Therefore, there is a need for a more expressive goal specification language. Current goal specification techniques for matchmaking and composition of Web services either lack expressivity to support real business scenarios or formal semantics to enable development of automatic algorithms. In this paper, we present a goal specification language that allow specifying constraints on functional and nonfunctional properties of Web services. The language is a novel combination of an expressive temporal logic \`{\i}- calculus and an expressive description logic SHIQ(D).},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {528–534},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.92,
author = {De Cock, Martine and Chung, Sam and Hafeez, Omar},
title = {Selection OfWeb Services with Imprecise QoS Constraints},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.92},
doi = {10.1109/WI.2007.92},
abstract = {When several functionally equivalent web services are available to perform the same task, their Quality of Service (QoS) characteristics such as performance and reliability become important in the selection process. Consumers that specify their QoS requirements too strictly however, risk not finding any web services meeting their demands. Therefore, in this paper we allow QoS constraints to be described imprecisely as fuzzy sets. We compare the effectiveness of an intelligent web service selection algorithm that takes these imprecise QoS constraints into account with a baseline algorithm acting on precise QoS values.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {535–541},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.11,
author = {Zhao, Wenbing},
title = {A Lightweight Fault Tolerance Framework for Web Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.11},
doi = {10.1109/WI.2007.11},
abstract = {In this paper, we present the design and implementation of a lightweight fault tolerance framework for Web services. With our framework, a Web service can be rendered fault tolerant by replicating it across several nodes. A consensusbased algorithm is used to ensure total ordering of the requests to the replicated Web service, and to ensure consistent membership view among the replicas. The framework is built by extending an open-source implementation of the WS-ReliableMessaging specification, and all reliable message exchanges in our framework conform to the specification. As such, our framework does not depend on any proprietary messaging and transport protocols, which is consistent with the Web services design principles. Our performance evaluation shows that our implementation is nearly optimal and the framework incurs only moderate runtime overhead.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {542–548},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.18,
author = {Mohabey, Megha and Narahari, Y. and Mallick, Sudeep and Suresh, P. and Subrahmanya, S. V.},
title = {An Intelligent Procurement Marketplace ForWeb Services Composition},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.18},
doi = {10.1109/WI.2007.18},
abstract = {This paper presents an intelligent procurement marketplace for finding the best mix of web services to dynamically compose the business process desired by a web service requester. We develop a combinatorial auction approach that leads to an integer programming formulation for the web services composition problem. The model takes into account the Quality of Service (QoS) and Service Level Agreements (SLA) for differentiating among multiple service providers who are capable of fulfilling a functionality. An important feature of the model is interface aware composition.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {551–554},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.112,
author = {Nayak, Richi and Lee, Bryan},
title = {Web Service Discovery with Additional Semantics and Clustering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.112},
doi = {10.1109/WI.2007.112},
abstract = {Due to the lack of semantic descriptions of the Web services, the search results returned by the service registries are effectively inadequate. This paper presents the Semantic Web services Clustering (SWSC) method that extends the semantic representation of services and groups the similar Web services in order to improve the service discovery. The empirical analysis shows the improvement in service discovery with the use of SWSC.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {555–558},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.66,
author = {Gupta, Chaitali and Bhowmik, Rajdeep and Head, Michael R. and Govindaraju, Madhusudhan and Meng, Weiyi},
title = {Improving Performance of Web Services Query Matchmaking with Automated Knowledge Acquisition},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.66},
doi = {10.1109/WI.2007.66},
abstract = {There is a critical need to design and develop tools that abstract away the fundamental complexity of XML-based Web services specifications and toolkits, and provide an elegant, intuitive, simple, and powerful query-based invocation system to end users. Web services based tools and standards have been designed to facilitate seamless integration and development for application developers. As a result, current implementations require the end user to have intimate knowledge of Web services and related toolkits, and users often play an informed role in the overall Web services execution process. We employ a self-learning mechanism and a set of algorithms and optimizations to match user queries with corresponding operations in Web services. Our system uses Semantic Web concepts and Ontologies in the process of automating Web services matchmaking. We present performance analysis of our system and quantify the exact gains in precision and recall due to the knowledge acquisition algorithms.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {559–563},
numpages = {5},
keywords = {Matchmaking, Ontology, Information Extraction, Semantic Web, Web services},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.25,
author = {Huan, LI and Zheng, QIN and Fan, YU and Jun, Qin and Bo, YANG},
title = {Automatic Semantic Web Service Composition via Agent Intention Execution in AgentSpeak},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.25},
doi = {10.1109/WI.2007.25},
abstract = {AI planning is the main stream method for automatic semantic web service composition (SWSC) research. However, planning based SWSC method can only return service composition upon user requirement description and lacks flexibility to deal with environment change. Deliberate agent architecture, such as BDI agent, is hopeful to make SWSC more intelligent. In this paper, we propose an automatic SWSC enabling method for AgentSpeak agent. Firstly, conversion algorithm from OWL-S web service description to agent's plan set (OWLS2APS) is presented. Target service is converted to agent's goal and related services are converted into agent's plan set. Then, SWSC is automatically performed through agent's intention formation. Agent invokes web service according to service sequence converted back from its intention. Agent can behave rationally with rules or ask for human intervention when SWSC or service invocation is not feasible. At last, a case study on enterprise credit rating service composition is presented to illustrate the method.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {564–567},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.12,
author = {Wang, Hongbing and Liu, Hui and Wang, Chen and Hung, Patrick},
title = {A New Approach to Describe Web Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.12},
doi = {10.1109/WI.2007.12},
abstract = {This paper is based on the theory of Finite State Automata (FSA's), models a web service as a FSA, extends WSDL for conceptually describing the behaviors of Web services, and introduces the concept of Temporal Logic of Actions (short for TLA) to describe and specify the behavior of a service in a formal way.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {568–571},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.19,
author = {Yang, Kai and Steele, Robert},
title = {An Ontology Mediated Web Service Aggregation Hub},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.19},
doi = {10.1109/WI.2007.19},
abstract = {This paper introduces the concept of service aggregation at the data level. Through the development of an Ontology mediated Web Service Aggregation Hub, we are aiming to create a general platform to enable data level composition among web services which have common capabilities. The contributions of this paper include (i) the proposal of the Ontology Mediated Web Service Aggregation Hub architecture. (ii) the definition of an application ontology for service operation modeling and classification. (iii) the utilization of ontology for enabling dynamic service invocation and result aggregation.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {572–576},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.67,
author = {Chou, Yi-Ting and Chuang, Shui-Lung and Wang, Xuanhui},
title = {Instant Web Retrieval for Instance-Attribute Queries},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.67},
doi = {10.1109/WI.2007.67},
abstract = {As the Web becomes the major information source of our daily activities, tools for finding various information on it are indispensable. This paper addresses theWeb retrieval of instance-attribute information, e.g., the contact addresses and research interests (attributes) of faculty and students (instances). This kind of information need is very common but cannot be directly supported by current keywordmatching-based search engines. People commonly use a two-phase search: First, locate the candidate pages, e.g., a faculty page, and then search within them for the desired information, e.g., contact information. Based on the stimulation of such human search behavior, we design a retrieval engine, upon general search engines, to help find the instance-attribute information from the Web. The experiment on several faculty members has shown the feasibility of the approach.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {579–585},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.37,
author = {Bautin, Mikhail and Skiena, Steven},
title = {Concordance-Based Entity-Oriented Search},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.37},
doi = {10.1109/WI.2007.37},
abstract = {We consider the problem of finding the relevant named entities in response to a search query over a given text corpus. Entity search can readily be used to augment conventional web search engines for a variety of applications. To assess the significance of entity search, we analyzed the AOL dataset of 36 million web search queries with respect to two different sets of entities: namely (a) 2.3 million distinct entities extracted from a news text corpus and (b) 2.9 million Wikipedia article titles. The results clearly indicate that search engines should be aware of entities, for under various criteria of matching between 18-39% of all web search queries can be recognized as specifically searching for entities, while 73-87% of all queries contain entities. Our entity search engine creates a concordance document for each entity, consisting of all the sentences in the corpus containing that entity. We then index and search these documents using open-source search software. This gives a ranked list of entities as the result of search. Visit http://www.textmap.com for a demonstration of our entity search engine over a large news corpus. We evaluate our system by comparing the results of each query to the list of entities that have highest statistical juxtaposition scores with the queried entity. Juxtaposition score is a measure of how strongly two entities are related in terms of a probabilistic upper bound. The results show excellent performance, particularly over well-characterized classes of entities such as people.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {586–592},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.7,
author = {Kao, Hung-Yu and Lin, Seng-Feng},
title = {A Fast PageRank Convergence Method Based on the Cluster Prediction},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.7},
doi = {10.1109/WI.2007.7},
abstract = {In recent years, search engines have already played the key roles among Web applications, and link analysis algorithms are the major methods to measure the important values of Web pages. These algorithms employ the conventional flat Web graph built by Web pages and link relations of Web pages to obtain the relative importance of Web objects. Previous researches have observed that PageRank-like link analysis algorithms have a bias against newly created Web pages. A new ranking algorithm called Page Quality was then proposed to solve this issue. Page Quality predicates future ranking values by the difference rate between the current ranking value and the previous ranking value. In this paper, we propose a new algorithm called DRank to diminish the bias of PageRank-like link analysis algorithms, and attain the better performance than Page Quality. In this algorithm, we model Web graph as a three-layer graph which includes Host Graph, Directory Graph and Page Graph by using the hierarchical structure of URLs and the structure of link relation of Web pages. We calculate the importance of Hosts, Directories and Pages by weighted graph we built and then the clustering distribution of PageRank values of pages within directories is observed. We can then predicate the more accurate values of page importance to diminish the bias of newly created pages by the clustering characteristic of PageRank. Experiment results show that DRank algorithm works well on predicating future ranking values of pages and outperform Page Quality.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {593–599},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331769,
author = {Thomas, Christopher and Sheth, Amit P.},
title = {Semantic Convergence of Wikipedia Articles},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Social networking, distributed problem solving and human computation have gained high visibility. Wikipedia is a well established service that incorporates aspects of these three fields of research. For this reason it is a good object of study for determining quality of solutions in a social setting that is open, completely distributed, bottom up and not peer reviewed by certified experts. In particular, this paper aims at identifying semantic convergence of Wikipedia articles; the notion that the content of an article stays stable regardless of continuing edits. This could lead to an automatic recommendation of good article tags but also add to the usability of Wikipedia as a Web Service and to its reliability for information extraction. The methods used and the results obtained in this research can be generalized to other communities that iteratively produce textual content.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {600–606},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.48,
author = {Neubauer, Nicolas and Scheel, Christian and Albayrak, Sahin and Obermayer, Klaus},
title = {Distance Measures in Query Space: How Strongly to Use Feedback From Past Queries},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.48},
doi = {10.1109/WI.2007.48},
abstract = {Feedback on past queries is a valuable resource for improving retrieval performance on new queries. We introduce a modular approach to incorporating feedback information into given retrieval architectures. We propose to fusion the original ranking with those returned by rerankers, each of which trained on feedback given for a distinct, single query. Here, we examine the basic case of improving a query's original ranking qtest by only using one reranker: the one trained on feedback on the "closest" query qtrain. We examine the use of various distance measures between queries to first identify qtrain and then determine the best linear combination of the original and the reranker's ratings, that is: to find out which feedback to learn from, and how strongly to use it. We show the cosine distance between the term vectors of the two queries, each enriched by representations of the top N originally returned documents, to reliably answer both questions. The fusion performs equally well or better than a) always using only the original ranker or the reranker, b) selecting a hard distance threshold to decide between the two, or c) fusioning results with a ratio that is globally optimized, but fixed across all tested queries.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {607–613},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.129,
author = {Mostafa Al Masum, Shaikh and Prendinger, Helmut and Ishizuka, Mitsuru},
title = {Emotion Sensitive News Agent: An Approach Towards User Centric Emotion Sensing from the News},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.129},
doi = {10.1109/WI.2007.129},
abstract = {This paper describes a character-based system called "Emotion Sensitive News Agent" (ESNA). ESNA is been developed as a news aggregator to fetch news from different news sources chosen by a user, and to categorize the themes of the news into eight emotion types. A small user study indicates that the system is conceived as intelligent and interesting as an affective interface. ESNA exemplifies a recent research agenda that aims at recognizing affective information conveyed through texts. News is an interesting application domain where user may have marked attitudes to certain events or entities reported about. Different approaches have already been employed to "sense" emotion from text. The novelty of our approach is twofold: affective information conveyed through text is analyzed (1) by considering the cognitive and appraisal structure of emotions, and (2) by taking into account user preferences.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {614–620},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.128,
author = {Zhang, Zhiyong and Nasraoui, Olfa},
title = {Efficient Hybrid Web Recommendations Based on Markov Clickstream Models and Implicit Search},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.128},
doi = {10.1109/WI.2007.128},
abstract = {In this paper, we present novel methods that combine (1) MarkovModels and (2) web page content search techniques to generate web navigation recommendations. For clickstream modeling, both first-order and second-orderMarkov Models were studied and a compact storage format for Markov transition matrices was used. For content-based search, a search engine was used to obtain similar-content pages for recommendation to compensate for the sparsity of the Markov model and thus improve coverage. Experiments were conducted on real web clickstream logs, and confirmed the efficiency of the proposed methods.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {621–627},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.134,
author = {Lee, Hyun Chul and Liu, Haifeng and Miller, Ren\'{e}e J.},
title = {Geographically-Sensitive Link Analysis},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.134},
doi = {10.1109/WI.2007.134},
abstract = {Many web pages and resources are primarily relevant to certain geographic locations. For example, in many queries web pages on restaurants, hotels, or movie theaters are mostly relevant to those users who are in geographic proximity to these locations. Moreover, as the number of queries with a local component increases, searching for web pages which are relevant to geographic locations is becoming increasingly important. The performance of geographically-oriented search is greatly affected by how we use geographic information to rank web pages. In this paper, we study the issue of ranking web pages using geographically-sensitive link analysis algorithms. More precisely, we study the question of whether geographic information can improve search performance. We propose several geographically-sensitive link analysis algorithms which exploit the geographic linkage between pages. We empirically analyze the performance of our algorithms.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {628–634},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.132,
author = {Zareh Bidoki, Ali Mohammad and Yazdani, Nasser and Ghodsnia, Pedram},
title = {FICA: A Fast Intelligent Crawling Algorithm},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.132},
doi = {10.1109/WI.2007.132},
abstract = {Due to the proliferation and highly dynamic nature of the web, an efficient crawling and ranking algorithm for retrieving the most important pages has remained as a challenging issue. Several algorithms like PageRank [13] and OPIC [1] have been proposed. Unfortunately, they have high time complexity. In this paper, an intelligent crawling algorithm based on reinforcement learning, called FICA is proposed that models a real surfing user. The priority for crawling pages is based on a concept which we name as logarithmic distance. FICA is easy to implement and its time complexity is O(E*logV) where V and E are the number of nodes and edges in the web graph respectively. Comparison of the FICA with other proposed algorithms shows that FICA outperforms them in discovering highly important pages. Furthermore, FICA computes the importance (ranking) of each page during the crawling process. Thus, we can also use FICA as a ranking method for computation of page importance. We have used UK's web graph for our experiments.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {635–641},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.136,
author = {Su, Xiaoyuan and Greiner, Russell and Khoshgoftaar, Taghi M. and Zhu, Xingquan},
title = {Hybrid Collaborative Filtering Algorithms Using a Mixture of Experts},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.136},
doi = {10.1109/WI.2007.136},
abstract = {Collaborative filtering (CF) is one of the most successful approaches for recommendation. In this paper, we propose two hybrid CF algorithms, sequential mixture CF and joint mixture CF, each combining advice from multiple experts for effective recommendation. These proposed hybrid CF models work particularly well in the common situation when data are very sparse. By combining multiple experts to form a mixture CF, our systems are able to cope with sparse data to obtain satisfactory performance. Empirical studies show that our algorithms outperform their peers, such as memory-based, pure model-based, pure content-based CF algorithms, and the contentboosted CF (a representative hybrid CF algorithm), especially when the underlying data are very sparse.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {645–649},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.131,
author = {Hoeber, Orland},
title = {ExploringWeb Search Results by Visually Specifying Utility Functions},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.131},
doi = {10.1109/WI.2007.131},
abstract = {In general, Web search engines perform well for users whose information needs are well-defined. When searchers can provide specific terms that adequately describe the information they are seeking, the top search results are commonly very relevant. However, when users wish to explore a topic, little assistance is provided by Web search engines to help users in finding the information they seek. In this paper, a system that supports exploratory search through the visual specification of utility functions is presented. Users are able to recognize potentially relevant terms using a term frequency histogram, and can indicate their preferences for these in a visual manner. The search results are re-sorted based on the corresponding utility function; colour coding allows users to easily locate the selected terms within the search results list.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {650–654},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.142,
author = {Chan, Michael and Chi-fai Chan, Stephen and Wing-ki Leung, Cane},
title = {Online Search Scope Reconstruction by Connectivity Inference},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.142},
doi = {10.1109/WI.2007.142},
abstract = {To cope with the continuing growth of the web, improvements should be made to the current brute-force techniques commonly used by robot-driven search engines. We propose a model that strikes a balance between robot and directorybased search engines by expanding the search scope of conventional directories to automatically include related categories. Our model makes use of a knowledge-rich and wellstructured corpus to infer relationships between documents and topic categories. We show that the hyperlink structure of Wikipedia articles can be effectively exploited to identify relations among topic categories. Our experiments show the average recall rate and precision rate achieved are 91% and between 85% and 215% of Google's respectively.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {655–658},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.150,
author = {Tan, Songbo and Wang, Yuefen and Cheng, Xueqi},
title = {Text Feature Ranking Based on Rough-Set Theory},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.150},
doi = {10.1109/WI.2007.150},
abstract = {With the aim to reduce the dimensionality without sacrificing classification performance, the author gains insights from attribute reduction based on discernibility matrix in rough-set theory and proposes two text feature selection algorithms, i.e., DB1 and DB2. The experimental results indicate that DB2 not only yields much higher accuracy than Information Gain when the number of features is smaller than 6000, but also incurs much smaller CPU time than Information Gain.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {659–662},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.126,
author = {Zhu, Mingliang and Hu, Weiming and Li, Xi and Wu, Ou},
title = {Customizable Instance-Driven Webpage Filtering Based on Semi-Supervised Learning},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.126},
doi = {10.1109/WI.2007.126},
abstract = {The World Wide Web has been growing rapidly in recent years, along with increasing needs for contentbased webpage filtering. But most existing filtering systems cannot easily satisfy the personalized filtering demands from different users at the same time. In this paper, a customizable instance-driven webpage filtering strategy is proposed. For different users, different webpage filters are produced by our system through mining the certain webpage classes they focus on. A semi-supervised learning (SSL) approach is applied for obtaining a precise description of the webpage class which a user wants to filter based on the small sized user instance set he or she provided. Subsequently, a feature selection step is performed and a Bayes classifier is created over the enlarged training set. Experimental results show the great stability and high performance of our proposed method, and it outperforms existing methods.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {663–666},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.140,
author = {Lin, Ling and Zhou, Lizhu},
title = {Leveraging Webpage Classification for Data Object Recognition},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.140},
doi = {10.1109/WI.2007.140},
abstract = {Data-rich webpages are providing an increasingly important data source for web applications. While the problem of data object recognition is intensively discussed, it is mostly addressed as a separated process from the frontier task of relevant webpage identification. In this paper, we propose a method to leverage the classification result of data-rich webpages for efficient and scalable data object recognition. A novel context information is proposed, which can be inferred from the webpage classification and exploited in the bottom-up data object recognition. Experimental results show that the context information brings a 19% improvement in the running efficiency of the bottomup data object recognition.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {667–670},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.130,
author = {Al-Saffar, Sinan and Heileman, Gregory},
title = {Experimental Bounds on the Usefulness of Personalized and Topic-Sensitive PageRank},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.130},
doi = {10.1109/WI.2007.130},
abstract = {PageRank is an algorithm used by several search engines to rank web documents according to their assumed relevance and popularity deduced from theWeb's link structure. PageRank determines a global ordering of candidate search results according to each page's popularity as determined by the number and importance of pages linking to these results. Personalized and topic-sensitive PageRank are variants of the algorithm that return a local ranking based on each user's preferences as biased by a set of pages they trust or topics they prefer. In this paper we compare personalized and topic-sensitive local PageRanks to the global PageRank showing experimentally how similar or dissimilar results of personalization can be to the original global rank results and to other personalizations. Our approach is to examine a snapshot of the Web and determine how advantageous personalization can be in the best and worst cases and how it performs at various values of the damping factor in the PageRank formula.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {671–675},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.120,
author = {Khan, Javed I. and Shaikh, Sajid S.},
title = {A Multi-Scenario Reputation Estimation Framework and Its Resilience Study against Various Forms of Attacks},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.120},
doi = {10.1109/WI.2007.120},
abstract = {Online transactional activities that involve establishment of trust between participating individual seem to require a reputation function for the reputation estimation framework (REF). They are often vulnerable to various kinds of attacks. Also it seems we do not evaluate reputation in the same way in all situations. Using Occam's razor we propose a generalized set-theoretic reputation function with customizable components that can be changed to meet the reputation requirements in wide variety of reputation assessment scenarios. Further we identify several canonical classes of the functions. The resilience of the framework is then analyzed by subjecting it to various reputation attacks such as gang attacks, vendetta and Dr Jekyll &amp; Mr. Hyde.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {676–682},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.137,
author = {He, Kuan-Yu and Chang, Yao-Sheng and Lu, Wen-Hsiang},
title = {Improving Identification of Latent User Goals through Search-Result Snippet Classification},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.137},
doi = {10.1109/WI.2007.137},
abstract = {In this paper, we propose an enhanced approach to improving our previous method which employs syntactic structures (verb-object pairs) to identify latent user goals. Our new approach employs a supervised-learning method to learn hint verbs and considers URL information and title information to classify snippets into three coarse categories, which are resource-seeking, informational, and navigational. Also, we propose three different models to identify three different categories of specific latent user goals from the classified snippets.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {683–686},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.145,
author = {Guo, Yong Zhen and Ramamohanarao, Kotagiri and Park, Laurence A. F.},
title = {Personalized PageRank for Web Page Prediction Based on Access Time-Length and Frequency},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.145},
doi = {10.1109/WI.2007.145},
abstract = {Web page prefetching techniques are used to address the access latency problem of the Internet. To perform successful prefetching, we must be able to predict the next set of pages that will be accessed by users. The PageRank algorithm used by Google is able to compute the popularity of a set of Web pages based on their link structure. In this paper, a novel PageRank-like algorithm is proposed for conducting Web page prediction. Two biasing factors are adopted to personalize PageRank, so that it favors the pages that are more important to users. One factor is the length of time spent on visiting a page and the other is the frequency that a page was visited. The experiments conducted show that using these two factors simultaneously to bias PageRank results in more accurate Web page prediction},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {687–690},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331803,
author = {Crabtree, Daniel and Andreae, Peter and Gao, Xiaoying},
title = {Understanding Query Aspects with Applications to Interactive Query Expansion},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {For many hard queries, users spend a lot of time refining their queries to find relevant documents. Many methods help by suggesting refinements, but it is hard for users to choose the best refinement, as the best refinements are often quite obscure. This paper presents Qasp, an approach that overcomes the limitations of other refinement approaches by using query aspects to find different refinements of ambiguous queries. Qasp clusters the refinements so that descriptive refinements occur together with more obscure and potentially better performing refinements, thereby explaining the effect of refinements to the user. Experiments are presented that show Qasp significantly increases the precision of hard queries. The experiments also show that Qasp's clustering method does find meaningful groups of refinements that help users choose good refinements, which would otherwise be overlooked.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {691–695},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.135,
author = {He, Daqing and Brusiloviksy, Peter and Grady, Jonathan and Li, Qi and Ahn, Jae-wook},
title = {How Up-to-Date Should It Be? The Value of Instant Profiling and Adaptation in Information Filtering},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.135},
doi = {10.1109/WI.2007.135},
abstract = {In profile-based or content-based adaptive systems, one of the open research questions is how frequently the user's profile and the list of recommended items should be updated. Different systems tend to choose one of the two extremes. Some systems do it once per session (thus called between-session update strategy), whereas some others update whenever there is feedback (called instant update strategy). This paper presents our attempt to assess the value of keeping the list of recommended items up-to-date in the context of task-based information exploration. We conducted controlled studies involving human users performing realistic tasks using two systems that have the same adaptive filtering engine but with the above two different update strategies. Our results show that the between-session strategy helped to find better quality information, and received better subjects' responses about its usefulness and usability. However, it prolonged the selection of useful passages, whereas the instant update strategy helped subjects to obtain almost all of their selected passages (&gt; 98%) within the first 5 minutes. Based on the results, we hypothesize that the best strategy for updating might be a hybrid between the two update strategies, where both adaptability and stability can be achieved.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {699–705},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.133,
author = {Ahn, Jae-wook and Brusilovsky, Peter},
title = {From User Query to User Model and Back: Adaptive Relevance-Based Visualization for Information Foraging},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.133},
doi = {10.1109/WI.2007.133},
abstract = {Adaptive information filtering is a promising tool for both casual Web news readers and professional intelligence analysts. Adaptive filtering augments the traditional query- or profile-based rankings provided by search engines. An interesting research challenge in this context is to offer users more control over the rankings by letting them mediate between the two extremes -- query- and profile-based rankings. To address this challenge, we developed an adaptive relevance-based visual exploration tool based on the VIBE (Visual Information Browsing Environment) visualization approach, which was previously developed at our School. This paper presents the rationale and functionality of this visual exploration tool and reports the results of its preliminary evaluation.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {706–712},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.122,
author = {Ren, Yonglin and Qin, Mian and Ren, Weilin},
title = {A Web Intelligent System Based on Measuring the Effects of Bother},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.122},
doi = {10.1109/WI.2007.122},
abstract = {The influence of bother measures the extent to which a user would be bothered or annoyed by any interaction with the agent, especially when the user interacts over the Internet. This paper discusses the model of bother cost and its application in web services that wish to obtain personal information from those users. Thereby, we propose a Comprehensive and Hierarchical Bother Cost Model (CHBCM) which extends traditional bother cost by including Web-specific factors for Web interactions. Simulation experiments show that the agent that uses a bother cost model has more benefit to the user and modeling bother cost is indeed worthwhile. Hence, Web intelligent agents that contain CHBCM can try to reduce unnecessary interruptions while obtaining the maximum amount of information from users.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {715–718},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.148,
author = {Mrabet, Yassine and Khelif, Khaled and Dieng-Kuntz, Rose},
title = {Recognising Professional-Activity Groups and Web Usage Mining for Web Browsing Personalisation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.148},
doi = {10.1109/WI.2007.148},
abstract = {Web usage mining can play an important role in supporting the navigation on the future Web. In fact detection of common or professional profiles allows browsers and web sites to personalise the user session and to recommend specific resources to the interested people. Semantic web approach seems interesting for this task. We propose in this paper a generic approach for profile detection relying on semantic web technologies. It takes advantages from ontologies, semantic annotations on web resources and inference engines.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {719–722},
numpages = {4},
keywords = {profile learning, annotations, semantic web browsing., ontologies},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.118,
author = {Tetchueng, Jean-Louis and Garlatti, Serge and Laube, Sylvain},
title = {A Didactic-Based Model of Scenarios for Designing an Adaptive and Context-Aware Learning System},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.118},
doi = {10.1109/WI.2007.118},
abstract = {Nowadays, technology-enhanced learning systems must have the ability to deal with the context and to allow dynamic adaptation based on pedagogical theories and knowledge models. The main issue is to design a generic scenario to deal with the broadest range of learning situations. From a generic scenario, the learning system will compute on the fly a scenario adapted to the current learner and its situation. Our main contribution is a semantic and didactic-based model of scenarios to design an adaptive and contextaware learning system. The scenario model is acquired from: i) the know-how and real practices of teachers ii) the theory in didactic anthropology of knowledge of Chevallard [1]; iii) a hierarchical task model.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {723–726},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.139,
author = {Yao, Yiyu and Zeng, Yi and Zhong, Ning and Huang, Xiangji},
title = {Knowledge Retrieval (KR)},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.139},
doi = {10.1109/WI.2007.139},
abstract = {With the ever-increasing growth of data and information, finding the right knowledge becomes a real challenge and an urgent task. Traditional data and information retrieval systems that support the current web are no longer adequate for knowledge seeking tasks. Knowledge retrieval systems will be the next generation of retrieval system serving those purposes. Basic issues of knowledge retrieval systems are examined and a conceptual framework of such systems is proposed. Theories and Technologies such as theory of knowledge, machine learning and knowledge discovery, psychology, logic and inference, linguistics, etc. are briefly mentioned for the implementation of knowledge retrieval systems. Two applications of knowledge retrieval in rough sets and biomedical domains are presented.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {729–735},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.124,
author = {Gelgi, Fatih and Davulcu, Hasan},
title = {Baum-Welch Style EM Approach on Simple Bayesian Models ForWeb Data Annotation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.124},
doi = {10.1109/WI.2007.124},
abstract = {In this paper, our focus will be on weakly annotated data (WAD) which is typically generated by a (semi) automated information extraction system from theWeb documents. The extracted information has a certain level of accuracy which can be surpassed by using statistical models that are capable of contextual reasoning such as Bayesian models. Our contribution is an EM algorithm that operates on simple Bayesian models to re-annotate WAD. EM estimates the parameters, i.e., the prior and conditional probabilities by iterating Bayesian model on the given Web data. In the expectation step, Bayesian classifier is trained from current annotations, and in the maximization step, the roles of all the labels are re-annotated to find the best fitting annotation with the current model then the probabilities are re-adjusted from the new annotations. Our experiments show that EM increases the Web data annotation accuracies up to 8%. We use Baum-Welch methodology in our EM approach.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {736–742},
numpages = {7},
keywords = {Expectation-Maximization, Bayesian Models., Baum-Welch, Weakly annotated data},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331817,
author = {Fujima, Jun and Yoshihara, Shohei and Tanaka, Yuzuru},
title = {Web Application Orchestration Using Excel},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Form-based Web applications described in HTML can be easily used by end-users. In order to enable end-users to define a series of tasks by combining multiple Web resources, it is necessary to provide an orchestration environment for Web applications. A spreadsheet is one of the most popular applications for office workers. It provides an enduser programming environment. In this paper, we propose a spreadsheet-based environment for end-users to orchestrate multiple Web applications. First, we provide a method for embedding various Web resources in spreadsheet cells as visual components in order to reuse them on the spreadsheet. Second, we propose an access method for embedded components using the special function in the formula language. Our approach enables users to define the complex coordination of multiple Web applications on the spreadsheet using the formula language.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {743–749},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.155,
author = {Lo, Anthony and Kianmehr, Keivan and Kaya, Mehmet and Ozyer, Tansel and Alhajj, Reda},
title = {Wrapping VRXQuery with Self-Adaptive Fuzzy Capabilities},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.155},
doi = {10.1109/WI.2007.155},
abstract = {This paper addresses the development of a plug and run wrapper to incorporate fuzziness into VRXQuery, the querying facility of VIREX which is a user-friendly system for transforming and querying relational data as XML. Our basic argument is not to force the underlying XML data to incorporate fuzziness. Rather, fuzziness is smoothly supported in a novel plug and run manner via a wrapper. Either the user specifies the membership functions for the elements/attributes to be queried as fuzzy, or multi-objective genetic algorithm is used to automatically decide on and optimize the membership functions. The interface of VIREX has been expanded to allow specifying queries with fuzziness. Then, queries expressed in VRXQuery empowered with fuzziness are translated into corresponding XQuery code, which is run on the underlying XML and the returned result is translated into a fuzzy representation; translation into SQL is also possible. The user is given the choice to display the result either as coloredtext or in graphical format.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {750–756},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.141,
author = {Jembere, E and Adigun, M. O. and Xulu, S. S.},
title = {Mining Context-Based User Preferences for m-Services Applications},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.141},
doi = {10.1109/WI.2007.141},
abstract = {Human Computer Interaction (HCI) challenges in mobile computing can be addressed by tailoring access and use of mobile services to user preferences. Our investigation of existent approaches to personalisation in context-aware computing found that user preferences are assumed to be static across different context descriptions, whilst in reality some user preferences are transient and vary with the change in context. Furthermore, existent preference models do not give an intuitive interpretation of a preference and lack user expressiveness. To tackle these issues, this paper presents a user preference model and mining framework for a context-aware m-services environment based on an intuitive quantitative preference measure and a strict partial order preference representation. Experimental evaluation of the user preference mining framework in a simulated m-Commerce environment showed that it is very promising. The preference mining algorithms were found to scale well with increases in the volumes of data.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {757–763},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.138,
author = {Wu, Chen and Chang, Elizabeth},
title = {Intelligent Web Services Selection Based on AHP and Wiki},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.138},
doi = {10.1109/WI.2007.138},
abstract = {Web Service selection is an essential element in Service-Oriented Computing. How to wisely select appropriate Web services for the benefits of service consumers is a key issue in service discovery. In this paper, we approach QoS-based service selection using a decision making model - the Analytic Hierarchy Process (AHP). In our solution, both subjective and objective criteria are supported by the AHP engine in a context-specific manner. We also provide a flexible Wiki platform to collaboratively form the initial QoS model within a service community. The software prototype is evaluated against the system scalability.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {767–770},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331827,
author = {Sasajima, Munehiko and Kitamura, Yoshinobu and Naganuma, Takefumi and Kurakake, Shoji and Mizoguchi, Riichiro},
title = {OOPS: User Modeling Method for Task Oriented Mobile Internet Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Growth in the mobile services industry has remarkably increased in the number of mobile services provided, and present methods of service provision have proven insufficient to guide users efficiently to the services they need. To solve this problem, a Task-oriented menu, which enables users to search for services by "what they want to do" instead of by "name of category", has been proposed. Construction of such a Task-oriented menu is based on a task ontology modeling method which supports the description of user activity such as task execution and the solving of obstacles encountered during the task. This paper discusses a task ontology-based modeling method which supports the description of users' activity and related knowledge such as how to solve problems that occurs on the users and prevention method for accidents. Models described by our method contribute to checking, designing and improving mobile internet services},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {771–775},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.5555/1331740.1331828,
author = {Oishi, Tetsuya and Kuramoto, Shunsuke and Nagata, Hiroto and Mine, Tsunenori and Hasegawa, Ryuzo and Fujita, Hiroshi and Koshimura, Miyuki},
title = {User-Schedule-Based Web Page Recommendation},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In web retrieval, it is often the case that the results given by search engines are not just what we want. To solve this problem there have been many studies on improving queries to be submitted to search engines. However, they are still insufficient due to lack of consideration on time information. To remedy this we propose a user-schedule-based web page recommendation method. The method makes use of a user's schedule to make the recommendation suite to hidher requests. In addition, an algorithm for extracting related words is introduced as a key technique in this method. Some preliminary experiments show very promising results in recommending web pages relevant to users requests. We also confirmed that this algorithm outperforms a method using mutual information.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {776–779},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.146,
author = {Eckhardt, A. and Horvath, T. and Vojtas, P.},
title = {PHASES: A User Profile Learning Approach for Web Search},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.146},
doi = {10.1109/WI.2007.146},
abstract = {Web search heuristics based on Fagin's threshold algorithm assume we have the user profile in the form of particular attribute ordering and a fuzzy aggregation function representing the user combining function. Having these, there are sufficient algorithms for searching top-k answers. Finding particular attribute ordering and aggregation for a user still remains a problem. In this short paper our main contribution is a proof of concept of a new iterative process of acquisition of user preferences and attribute ordering .},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {780–783},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.125,
author = {Klaisubun, Piyanuch and Kajondecha, Phichit and Ishikawa, Takashi},
title = {Behavior Patterns of Information Discovery in Social Bookmarking Service},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.125},
doi = {10.1109/WI.2007.125},
abstract = {The paper describes an analysis of users' behaviors in discovering useful information resources by using social bookmarking service. The aims of the analysis are to evaluate the effectiveness of social bookmarking service for information discovery and to propose improvements in its navigational function. We analyzed users' behaviors of information discovery in an experimental social bookmarking service and evaluated the effectiveness of social navigation for information discovery. The results of the analysis show users frequently select tags to focus topics to search and prefer to browse others' libraries to find related information around the focused topics.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {784–787},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.147,
author = {Bao, Jie and Slutzki, Giora and Honavar, Vasant},
title = {Privacy-Preserving Reasoning on the SemanticWeb},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.147},
doi = {10.1109/WI.2007.147},
abstract = {Many semantic web applications require selective sharing of ontologies between autonomous entities due to copyright, privacy or security concerns. In such cases, an agent might want to hide a part of its ontology while sharing the rest. However, prohibiting any use of the hidden part of the ontology in answering queries from other agents may be overly restrictive. We provide a framework for privacypreserving reasoning in which an agent can safely answer queries against its knowledge base using inferences based on both the hidden and visible part of the knowledge base, without revealing the hidden knowledge. We show an application of this framework in the widely used special case of hierarchical ontologies.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {791–797},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.144,
author = {Nurmi, Petteri},
title = {Perseus -- A Personalized Reputation System},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.144},
doi = {10.1109/WI.2007.144},
abstract = {We propose Perseus, a personalized reputation system. In Perseus, reputations comprise of three aspects: how much I personally trust another individual, how trustworthy others think the individual is, and how much I trust the opinions of others. Perseus is adaptive in the sense that user feedback is used to modify the way the different aspects are considered. We also present simulation experiments, which indicate that Perseus is robust and able to survive under extreme conditions of misbehavior. In addition, Perseus encourages individuals to rate the other party and give fair ratings. We also compare Perseus against other well-known reputation systems.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {798–804},
numpages = {7},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.149,
author = {Chung, Wingyan and Leung, Ada},
title = {Supporting Web Searching of Business Intelligence with Information Visualization},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.149},
doi = {10.1109/WI.2007.149},
abstract = {In this research, we proposed and validated an approach to using information visualization to augment search engines in supporting the analysis of business stakeholder information on the Web. We report in this paper findings from a preliminary evaluation comparing a visualization prototype with a traditional method of stakeholder analysis (Web browsing and searching). We found that the prototype achieved a higher perceived usefulness and perceived analysis effectiveness and was perceived favorably in expediting the subjects' decision making and in helping them understand the analysis results. Overall, the proposed approach was found to augment traditional methods of analyzing business stakeholders. We discuss implications to researchers and practitioners and future directions.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {807–811},
numpages = {5},
keywords = {Web searching, information visualization, Business intelligence, experiment., systems, search engine, business stakeholder analysis},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.123,
author = {Niu, Li and Lu, Jie and Chew, Eng and Zhang, Guangquan},
title = {An Exploratory Cognitive Business Intelligence System},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.123},
doi = {10.1109/WI.2007.123},
abstract = {An exploratory study of web-based cognitive business intelligence systems (CBIS) is presented in this paper. The underpinning concepts and theories are situation awareness, mental model, and naturalistic decision making (NDM). The CBIS is an extension of the traditional business intelligence system with cognitive orientation. It focuses on developing, enriching, and utilizing the executive's situation awareness, mental models, and other past experience during human-computer interaction, which drives the decision process to approach a naturalistic decision.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {812–815},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.127,
author = {Wang, Long and Meinel, Christoph},
title = {Detecting the Changes OfWeb Students' Learning Interest},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.127},
doi = {10.1109/WI.2007.127},
abstract = {In this paper, we discover the changes of students' learning interest from their usage data in web-based learning environment. Due to the effects on each other of the changes in web students and web lectures, we seek a method that integrates the changes in both sides to measure the changes of learning interest. We implement our work on our webbased learning environment: tele-TASK. The mined results help teachers to know their students clearly and adjust their teaching schedules efficiently.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {816–819},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.154,
author = {Quasthoff, Matthias and Sack, Harald and Meinel, Christoph},
title = {Why HTTPS Is Not Enough -- A Signature-Based Architecture for Trusted Content on the Social Web},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.154},
doi = {10.1109/WI.2007.154},
abstract = {Easy to use, interactive web applications accumulating data from heterogeneous sources represent a recent trend on the World Wide Web, referred to as the Social Web. There however, security standards are often disregarded in favor of interface design or brand new features. This prevents the new services from gaining ground in the enterprise, in medical or e-government environments. We propose the deployment of XML Digital Signatures on web content and demonstrate how an architecture enabling for various security properties would look like. The solution proposed will benefit from the research on security engineering in Service-Oriented Architectures and thus allows for an in-depth analysis on the results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {820–824},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.121,
author = {Hu, Jia and Zhong, Ning},
title = {A Multilevel Integration Approach for Developing E-Finance Portals: Challenges and Perspectives},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.121},
doi = {10.1109/WI.2007.121},
abstract = {E-finance industry is rapidly transforming and evolving toward more dynamic, flexible and intelligent solutions. The proposed e-finance portal provides an integrated enterprise platform for retail banking services as well as for other financial services. In the meantime, the proposed multilevel solution will keep monitoring and analyzing the huge volume of dynamic information flowing through the portal. In this way, the portal is able to find the useful knowledge for refining business processes and providing personalized services, and detect hidden financial problems as well.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {825–828},
numpages = {4},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.119,
author = {Yan, Zhuang and Fong, Simon},
title = {A Model of B2B Negotiation Using Knowledge},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.119},
doi = {10.1109/WI.2007.119},
abstract = {Knowledge incorporation is one challenge in e-Commerce automated negotiation. In this paper, we describe a model of B2B negotiation using knowledge. We classify the types of knowledge namely general knowledge and negotiation knowledge, in the negotiation process. A methodology that uses Knowledge Bead (KB) and meta-KB as knowledge representation that would be suitable for the design of automated negotiation systems is discussed. An experimental prototype demonstrates that by incorporating knowledge into automated negotiation yields improved results.},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {829–832},
numpages = {4},
keywords = {Agents, Keywords: B2B Negotiation, Knowledge},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.151,
author = {Razmerita, Liana and Bjorn-Andersen, Niels},
title = {Towards Ubiquitous E-Custom Services},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.151},
doi = {10.1109/WI.2007.151},
abstract = {In this paper we present an advanced e-custom system pilot designed to address strategic goals for future custom systems. Future custom systems will support simplified paperless trade procedures, prevent potential security threats and counterfeit tax related fraud while at the same time insure interoperability with other e-custom systems within and outside Europe. The focus is placed on the advantages of use of novel technologies for the implementation of advanced e-custom systems. In particular we highlight the use of service oriented architecture (SOA), web services and TREC (Tamper Resistant Embedded Controller) device in an integrated framework named EPCIS (Electronic Product Code for Information Systems). Among the advantages of the presented solution are: the ubiquitous access to the location of goods through its supply chain, the provision of evidence for import/export, the notification through alerts in case of exceptions (such as deviation from the planned trajectory, abnormal conditions for containers, etc.)},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {833–837},
numpages = {5},
series = {WI '07}
}

@inproceedings{10.1109/WI.2007.22,
title = {Author Index},
year = {2007},
isbn = {0769530265},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2007.22},
doi = {10.1109/WI.2007.22},
booktitle = {Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {838–841},
numpages = {4},
series = {WI '07}
}

