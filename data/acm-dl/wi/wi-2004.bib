@inproceedings{10.5555/1025132.1026280,
title = {Message from the WI'04 and IAT'04 Conference Chairs and Program Chair},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.14–xvii},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026281,
title = {WI'04 and IAT'04 Conference Organizers},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.18–xix},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026282,
title = {WI'04 and IAT'04 Program Committee Leadership},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.20–xx},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026283,
title = {WI'04 and IAT'04 Program Committee Members},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.21–xxvi},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026284,
title = {WI'04 Non-Program Committee Reviewers},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.27–xxvii},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026285,
title = {WI'04 and IAT'04 Tutorials and Workshops},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {.28–xxviii},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026286,
author = {McCarthy, John},
title = {The Web - Early Visions, Present Reality, the Grander Future},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {3},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026287,
author = {Kesselman, Carl},
title = {Applications of Intelligent Agent Technology to The Grid},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {4},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026288,
author = {Lesser, Victor},
title = {Scaling up Multi-Agent Systems through Organizational Structuring},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {5},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026289,
author = {Mitchell, Tom M.},
title = {Intelligent Workstation Agents and Unstructured Workstation Data},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {6},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026290,
author = {Wu, Xindong},
title = {Data Mining: Artificial Intelligence in Data Analysis},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {7},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026291,
author = {Faltings, Boi},
title = {Incentive-Compatible Social Choice},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many situations present a social choice problem where different self-interested agents have to agree on joint, co-ordinated decisions.For example, power companies have to agree on how to use the power grid, and airlines have to agree on how to schedule takeoffs and landings. Mechanisms for social choice are called incentive-compatible when cooperative behavior is optimal for all parties.The most well-known examples of incentive-compatible mechanisms are auctions.However, the party that receives the auction revenue has an incentive to manipulate the outcome to increase the revenue.For example, a power gird operator has an interest to reduce capacity and drive up prices.Conversely, if it provides sufficient capacity to every user it derives no revenue to cover its costs. We present a novel mechanism for social choice that is incentive-compatible without generating a payment surplus. We give several examples of applications where it solves the social choice problem without unwanted incentives, and provides significantly better overall utility than any other known mechanism.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {8–14},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026292,
author = {Cao, Longbign and Luo, Chao and Luo, Dan and Zhang, Chengqi},
title = {Integration of Business Intelligence Based on Three-Level Ontology Services},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Usually, integration of business intelligence (BI) from realistic telecom enterprise is by packing data warehouse (DW), OLAP, data mining and reporting from different vendors together.As a result, BI system users are transferred to a reporting system with reports, data models, dimensions and measures predefined by system designers.As a result of survey, 85% of DW projects failed to meet their intended objectives.In this paper, we investigate how to integrate BI packages into an adaptive and flexible knowledge portal by constructing an internal link and communication channel from top-level business concepts to underlying enterprise information systems (EIS).An approach of three-level ontology services is developed, which implements unified naming, directory and transport of ontology services, and ontology mapping and query parsing among conceptual view, analytical view and physical view from user interfaces through DW to EIS.Experiments on top of real telecom EIS shows that our solution for integrating BI presents much stronger power to support operational decision making more user-friendly and adaptively compared with those simply combining BI products presently available together.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {17–23},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026294,
author = {Tian, Zhihong and Fang, Binxing and Yun, Xiaochun},
title = {Defending Against Flash Crowds and Malicious Traffic Attacks with An Auction-Based Method},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Flash crowd events (FCEs) and malicious traffic including DDoS and worm attacks present a real threat to the stability of Web services. In this paper, we design a practical defense system that can provide some needed relief from the two types of events and protect the availability of Web services. A novel method of dynamic bandwidth arbitration using Generalized Vickrey auction based on microeconomics is proposed. By adopting this approach, not only the availability of Web services is improved but also the total utility of users can be maximized. Initial simulations have shown that this mechanism is promising direction to control both FCEs and malicious traffic. The presentation in this paper is a first step towards a more rigorous evaluation.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {24–28},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026295,
author = {Qiu, Mei Kang and Zhang, Kang and Huang, Maolin},
title = {An Empirical Study of Web Interface Design on Small Display Devices},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper reports an empirical study that explores the problem of finding a highly-efficient, user-friendly interface design method on small display devices. We compared three models using our PDA interface simulator: presentation optimization method, semantic conversion method, and zooming method. A controlled experiment has been carried out to identify the pros and cons of each method. The results show that of the three interface methods, the zooming method is slightly better than the semantic conversion method, while they both outperform the optimizing presentation method.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {29–35},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026296,
author = {Stojanovic, Nenad and Studer, Rudi and Stojanovic, Ljiljana},
title = {An Approach for Step-By-Step Query Refinement in the Ontology-Based Information Retrieval},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present a comprehensive approach for the refinement of ontology-based queries, which is based on incrementally (step-by-step) and interactively tailoring a query to the current information needs of a user, whereas these needs are implicitly and on-line elicited by analysing the user's behaviour during the searching process. The gap between a user's need and his query is quantified by measuring several types of query ambiguities. Consequently, in the refinement process a user is provided with a ranked list of refinements, which leads to a decrease of some of these ambiguities. Moreover, by exploiting the ontology background, the approach supports finding "similar" results that can help a user to satisfy his information need.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {36–43},
numpages = {8},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026297,
author = {Loke, Seng W.},
title = {Logic Programming for Context-Aware Pervasive Computing: Language Support, Characterizing Situations, and Integration with the Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We characterize situations as constraints on sensor readings expressed in rules. We also introduce an extension of Prolog which we call LogicCAP for programming context-aware applications, where situations are first-class entities. The operator "in-situation" in the language captures a common form of reasoning in context-aware applications, which is to ask if an entity is in a given situation. We show the usefulness of our approach via programming idioms, including defining relations among situations and integration with the Web.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {44–50},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026298,
author = {Hsieh, S. M. and Huang, S. J. and Hsu, C. C. and Chang, H. C.},
title = {Personal Document Recommendation System Based on Data Mining Techniques},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most existing recommendation systems may not be very effective due to the lack of the adequate knowledge of user's behavior or interests.Some systems are not] efficient enough because of the huge on-line computational demand.In this paper, we propose a personal documents recommendation system that effectively filters the on-line news on WWW for each individual user.The proposed system recommends useful news by employing the profiling techniques of modified content- and collaborative-based filtering.In order to reduce the on-line computation and improve the recommendation quality, we design a tree-based data mining algorithm that treats users' behavior and interest as input and filter the news documents efficiently.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {51–57},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026299,
author = {Atlas, Mourad and Zhang, Yan-Qing},
title = {Fuzzy Neural Agents for Online NBA Scouting},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Internet is the focus of the intelligent agent applications. The current status of intelligent agent for the World Wide Web is still in its infancy, but like the Web itself, is evolving constantly. Several agents are being designed and implemented for a variety of tasks in diverse range of applications: managing e-mail, navigating and retrieving information from the Internet, online shopping, electronic business, monitoring stock prices or currency exchanges, etc. In this paper, we present and describe an intelligent service agent that assists an NBA scouting agent in his/her work. The particularity of our agent is not only it retrieves relevant information about NBA players from the Internet for the scouting agent but also it derives metadata from that information, which consist of player performance evaluation and player statistics prediction.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {58–63},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026300,
author = {Xu, Qingyang and Zuo, Wanli},
title = {Extracting Precise Link Context Using NLP Parsing Technique},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Link context has been exploited extensively ever since the advent of the World Wide Web, but the approach to extracting precise link context has not been fully explored and many state-of-the-art extraction methods are based on simplistic heuristics and require ad-hoc parameters. In this paper, we propose a novel two-step extraction model, which aims to systematically derive link context of quality as high as anchor text. In the macroscopic analysis step, a systematic web page structure analysis is performed to locate the content cohesive text region and potential relevant header or header like tags. In the microscopic extraction step, an English parser is used to extract the relevant sentence fragments in the text region and the nearest heading text is encompassed if the need arises. Preliminary experimental results proved our approach's effectiveness.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {64–69},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026301,
author = {Wakaki, Toshiko and Itakura, Hiroyuki and Tamura, Masaki},
title = {Rough Set-Aided Feature Selection for Automatic Web-Page Classification},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recently Web-pages on the World Wide Web are explosively increasing, and it is now required for portal sites such as Yahoo! service having directory-style search engines to classify Web-pages into many categories automatically. This paper investigates how rough settheory can help select relevant features for Web-page classification. Our experimental results show that the combination of the rough set-aided feature selection method and the Support Vector Machine with a linear kernel is quite useful in practice to classify Web-pages into many categories because not only the performance gives acceptable accuracy but also the high dimensionality reduction is achieved without depending on arbitrary thresholds for feature selection.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {70–76},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026302,
author = {Hu, Xiaohua and Lin, T. Y. and Song, Il-Yeol and Lin, Xia and Yoo, Illhoi and Lechner, Mark and Song, Min},
title = {Ontology-Based Scalable and Portable Information Extraction System to Extract Biological Knowledge from Huge Collection of Biomedical Web Documents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Automated discovery and extraction of biological knowledge from biomedical web documents has become essential because of the enormous amount of biomedical literature published each year. In this paper we present an ontology-based scalable and portable information extraction system to automatically extract biological knowledge from huge collection of online biomedical web documents. Our method integrates ontology-based semantic tagging, information extraction and data mining together, automatically learns the patterns based on a few user seed tuples, and then extract new tuples from the biomedical web documents based on the discovered patterns. A novel system SPIE (Scalable and Portable Information Extraction) is implemented and tested on the PuBMed to find the chromatin protein-protein interaction and the experimental results indicate our approach is very effective in extracting biological knowledge from huge collection of biomedical web documents.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {77–83},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026303,
author = {Lei, Hansheng and Govindaraju, Venu},
title = {Matching and Retrieving Sequential Patterns Under Regression},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Sequential pattern matching and retrieving is of real value.For example, finding stocks in the NASDAQ market whose closing prices are always about $β higher than or β times as that of a given company.The probelm reduces to linear pattern retrieval: given query X, find all sequence Y from database S so that Y = β + β with confidence C. In this paper, we novelly introduce SLR (Simple Linear Regression) model [5,7] to solve this problem.We extend 1-dimensional R^2 to ER^2 for multi-dimensional sequence matching, such as on-line handwritten signature.In addition, we develop SLR+FFT pruning techniques based on SLR to speed up retrieval without incurring any false dismissal. Experimental results show that the pruning ratio of SLR+FFT is efficient (can be above 99%).Experiments on real stocks discovered many interesting patterns.Preliminary test on on-line signature recognition using ER^2 as similarity measure also shows high accuracy.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {84–90},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026304,
author = {Lowe, David and Kong, Xiaoying},
title = {NavOptim Coding: Supporting Website Navigation Optimisation Using Effort Minimisation},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web applications have rapidly become critical to the interaction that organisations have with their external stakeholders. A major factor in the effectiveness of this interaction is the ease with which navigation within the application can occur, and especially the extent to which users can locate information and functionality which they are seeking. Effective design is however complicated by the multiple purposes and users which Web applications typically support. Despite the fact that this implies that navigation design is inherently an optimisation problem, few optimisation techniques have been applied in this domain - with most design techniques being based on intuition, general heuristics, or experimental refinement. In this paper we discuss this problem, and propose a navigation representation which can become the basis for optimisation techniques.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {91–97},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026305,
author = {Kong, Danxia},
title = {One Dynamic Pricing Strategy in Agent Economy Using Neural Network Based on Online Learning},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper examines seller strategies for dynamic pricing in a market for which a seller has finite time horizon to sell its inventory. A dynamic pricing strategy is developed using neural network based on online learning (called SDNN strategy). The SDNN strategy takes in account the dynamics and resulting uncertainties of the market place. The experiments show that the SDNN strategy exhibits superior performance to the other candidate dynamic pricing strategies which of similar computational simplicity and lack of assumptions about the market place.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {98–102},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026306,
author = {Zhao, Yanchang and Zhang, Chengqi and Shen, Yi-Dong},
title = {Clustering High-Dimensional Data with Low-Order Neighbors},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Density-based and grid-based clustering are two main clustering approaches. The former is famous for its capability of discovering clusters of various shapes and eliminating noises, while the latter is well known for its high speed. Combination of the two approaches seems to provide better clustering results. To the best of our knowledge, however, all existing algorithms that combine density-based clustering and grid-based clustering take cells as atomic units, in the sense that either all objects in a cell belong to a cluster or no object in the cell belong to any cluster. This requires the cells to be small enough to ensure the fine resolution of results. In high-dimensional spaces, however, the number of cells can be very large when cells are small, which would make the clustering process extremely costly. On the other hand, the number of neighbors of a cell grows exponentially with the dimensionality of datasets, which makes the complexity increase further. In this paper, we present a new approach that takes objects (or points) as the atomic units, so that the restriction of cell size can be relaxed without degrading the resolution of clustering results. In addition, a concept of ith-order neighbors is introduced to avoid considering the exponential number of neighboring cells. By considering only low-order neighbors, our algorithm is very efficient while losing only a little bit of accuracy. Experiments on synthetic and public data show that our algorithm can cluster high-dimensional data effectively and efficiently.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {103–109},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026307,
author = {Grefenstette, Gregory and Qu, Yan and Evans, David A.},
title = {Mining the Web to Create a Language Model for Mapping between English Names and Phrases and Japanese},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The Web provides the largest, exploitable collection of language use. If we can mine the Web to build abstract models of language use, these models may have many applications. Here we present one example of using the implicit intelligence of language use to solve an important problem for machine translation programs and cross-lingual applications. This problem involves the translation of words written in katakana characters in Japanese. In this paper, we describe techniques of discovering katakana transliteration of English names and of finding English translations of multiword katakana sequences using implicit language models of English and Japanese found on the Web. These techniques were evaluated against human-constructed English-katakana glosses.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {110–116},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026308,
author = {Huang, Shen and Xue, Gui-Rong and Zhang, Ben-Yu and Chen, Zheng and Yu, Yong and Ma, Wei-Ying},
title = {TSSP: A Reinforcement Algorithm to Find Related Papers},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Content analysis and citation analysis are two common methods in recommending system. Compared with content analysis, citation analysis can discover more implicitly related papers. However, the citation-based methods may introduce more noise in citation graph and cause topic drift. Some work combine content with citation to improve similarity measurement. The problem is that the two features are not used to reinforce each other to get better result. To solve the problem, we propose a new algorithm, Topic Sensitive Similarity Propagation (TSSP), to effectively integrate content similarity into similarity propagation. TSSP has two parts: citation context based propagation and iterative reinforcement. First, citation contexts provide clues for which papers are topic related to and filter out less irrelevant citations. Second, iteratively integrating content and citation similarity enable them to reinforce each other during the propagation. The experimental results of a user study show TSSP outperforms other algorithms in almost all cases.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {117–123},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026309,
author = {Tang, Na and Vemuri, V. Rao},
title = {Web-Based Knowledge Acquisition to Impute Missing Values for Classification},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Machine learning is the science of building predictors from data while accounting for the predictor's accuracy on future data. Many machine learning classifiers can make accurate predictions when the data is complete. In the presence of insufficient data, statistical methods can be applied to fill in a few missing items. But these methods rely only on the available data to calculate the missing values and perform poorly if the percentage of missing values exceeds a threshold. An alternative is to fill in the missing data by an automated knowledge discovery process via mining the WWW. This novel procedure is applied by first restoring missing information and next learning the parameters of the classifier from the restored data. Using a Bayesian network as a classifier, the parameters, i.e., the probabilities associated with the causal relationships in the network, are deduced using the knowledge mined from the WWW in conjunction with the data available on hand. The method, when tested with heart disease data sets from the UC Irvine Machine Learning Repository [UCI repository of machine learning databases], gave satisfactory results.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {124–130},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026310,
author = {Jiang, Shuqiang and Huang, Tiejun and Gao, Wen},
title = {An Ontology-Based Approach to Retrieve Digitized Art Images},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Although much progress has been made, current low-level based visual information retrieval technology does not allow users to formulate queries through high-level semantics. More and more digitized art images appear on the Internet, and techniques need to be established on how to organize and retrieve them. In this work, a framework for retrieving art images using an ontology-based method is introduced. The proposed ontology describes images in various aspects. Non-objectionable semantics are first introduced, and how to express these semantics is given. Concepts in the ontology could be automatically derived. The retrieval scheme makes users more naturally find visual information and experimental implementation demonstrates good potential on retrieving art images in a human-centered manner.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {131–137},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026311,
author = {Huang, Weihong and Webster, David},
title = {Enabling Context-Aware Agents to Understand Semantic Resources on The WWWand The Semantic Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Six years after Tim Berners-Lee and his colleagues drew the vision of the Semantic Web (SW) in 1998, the SW is very likely to take off in the near future based on a set of specifications such as the Resource Description Framework (RDF) and the Web Ontology Language (OWL). As a natural result, people will see the coexistence of the WWW and the SW for a certain long time. Under this situation, how to bridge the gap between the WWW and the SW will become an inevitable issue. In this paper, we present a context-aware approach on enabling agents to understand semantic resources on the two webs. The basic idea of this approach is to structure user-centred contextual information to facilitate agent-based (inter)operations on the network. Based on this idea, we design a Knowledge Interoperation Reference Model (KIRM) to address the interoperation issue at the global level. To demonstrate how agents understand semantic resources in a context-aware manner in real practices, we develop a news aggregation system based on RDF Site Summary / Really Simple Syndication (RSS) using agents. Considering the fact that RSS format set is a combination of XML and RDF, the success of agent understanding of RSS content under specific semantic contexts shows the possibility of extending and applying the context-aware approach in other semantic-based applications on both the WWW and the SW in the future.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {138–144},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026313,
author = {Smyth, Barry and McGinty, Lorraine and Reilly, James and McCarthy, Kevin},
title = {Compound Critiques for Conversational Recommender Systems},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recommender systems bring together ideas from information retrieval and filtering, user profiling, adaptive interfaces and machine learning in an attempt to offer users more personalized and responsive search systems. Conversational recommenders guide a user through a sequence of iterations, suggesting specific items, and using feedback from users to refine their suggestions in subsequent iterations. Different recommender systems look for different types of feedback from users. In this paper we examine the role of critiquing, a form of feedback in which the user indicates a preference over a particular feature of a recommended item. For example, when shopping for a PC a user might indicate that they like the current suggestion but they are looking for something "cheaper"; "cheaper" is a critique over the price feature of the PC case. Sometimes it is useful to critique multiple features simultaneously (compound critiques). In this paper we describe how a recommender can automatically discover useful compound critiques during the recommendation session and how these critiques can be used to improve recommendation efficiency.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {145–151},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026312,
author = {Tian, M. and Gramm, A. and Ritter, H. and Schiller, J.},
title = {Efficient Selection and Monitoring of QoS-Aware Web Services with the WS-QoS Framework},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {While the concept of UDDI supports the automatic discovery of services implementing a common public tModel interface, there have been only few attempts to find a standardized form to describe the quality of service (QoS) properties of Web services. We introduce our approach "Web service QoS (WS-QoS)" that enables an efficient, dynamic, and QoS-aware selection and monitoring of Web services. The prototype of our approach is implemented with the .NET technology, including the following components: a WS-QoS Editor for the specification of QoS properties, a WS-QoS Requirement Manager for retrieving QoS requirements specified by client applications, a Web service broker for an efficient and QoS-aware selection of Web service offers, and a WS-QoS Monitor for checking the compliance of the service offers.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {152–158},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026314,
author = {Butz, C. J. and Hua, S. and Maguire, R. B.},
title = {A Web-Based Intelligent Tutoring System for Computer Programming},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web Intelligence is a direction for scientific research that explores practical applications of Artificial Intelligence to the next generation of Web-empowered systems. In this paper, we present a Web-based intelligent tutoring system for computer programming. The decision making process conducted in our intelligent system is guided by Bayesian networks, which are a formal framework for uncertainty management in Artificial Intelligence based on probability theory. Whereas many tutoring systems are static HTML Web pages of a class textbook or lecture notes, our intelligent system can help a student navigate through the online course materials, recommend learning goals, and generate appropriate reading sequences.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {159–165},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026315,
author = {Chua, Stephanie and Kulathuramaiyer, Narayanan},
title = {Semantic Feature Selection Using WordNet},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The web has caused an explosion of documents, requiring the need for an automated text categorization system. This paper explores the notion of semantic feature selection by employing WordNet [Introduction to WordNet: An On-line Lexical Database], a lexical database. The proposed semantic approach employs noun synonyms and word senses for feature selection to select terms that are semantically representative of a category of documents. The categorical sense disambiguation extends the use of WordNet, which has been typically used for text retrieval and word sense disambiguation [A WordNet-based Algorithm for Word Sense Disambiguation]. Our experiments on the Reuters-21578 dataset have shown that automated semantic feature selection is able to perform better than well known statistical feature selection methods, Information Gain and Chi-Square as a feature selection method.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {166–172},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026316,
author = {Wang, Sun-Chong and Yu, Ching-Yun and Liu, Kuang-Ping and Li, Sai-Ping},
title = {A Web-Based Political Exchange for Election Outcome Predictions},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Prices form in a free market by the interaction of supply and demand. In a highly competitive industry, prices fluctuate. An exchange market provides an arena for investors to exercise speculative opportunities based on their perception of price movements. We describe an online exchange where political futures contracts are traded. In the design, the liquidation value of a share of political futures contract is determined by the percentage of votes received by a candidate in the election. Such a political exchange was run during Taiwan's general election campaign in March, 2004. A feature of the experimentation is that money is fictitious in the trading. We devised ways to introduce incentives. We report predictions by the exchange that outperform conventional polls.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {173–178},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026317,
author = {Ji, Junzhong and Liu, Chunnian and Yan, Jing and Zhong, Ning},
title = {Bayesian Networks Structure Learning and Its Application to Personalized Recommendation in a B2C Portal},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web Intelligence (WI) is a new and active research field in current AI and IT. Personalized recommendation in an intelligent B2C portal is an important research topic in WI. In this paper, we first investigate the architecture of a B2C portal from the viewpoint of conceptual levels of WI. Aiming at data mining of knowledge-level in a B2C portal, we present a new improved learning algorithm of Bayesian Networks, which consists of two major contributions, namely, making the best of lower order Conditional Independence (CI) tests and accelerating search process by means of sort order for parent nodes. By a number of experiments on ALARM datasets, we find that the proposed algorithm is both more efficient and effective than others. We have applied this algorithm to a commodity recommendation system in a B2C portal. Our experimental results demonstrate that the recommendation method based on a Customer Shopping Model (CSM) produced by the new algorithm outperforms some traditional ones in rates of coverage and precision.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {179–184},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026318,
author = {Kim, Byeong Man and Li, Qing},
title = {Probabilistic Model Estimation for Collaborative Filtering Based on Items Attributes},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the development of e-commerce and the proliferation of easily accessible information, recommender systems have become a popular technique to prune large information spaces so that users are directed toward those items that best meet their needs and preferences. While many collaborative recommender systems (CRS) have succeeded in capturing the similarity among users or items based on ratings to provide good recommendation, there are still some challenges for them to be a more efficient RS. In this paper, we address three problems in CRS (user bias, non-transitive association, and new item problem) and provide a new item-based probabilistic model approach in order to solve the addressed problems in hopes of achieving better performance. In this probabilistic model, items are classified into groups and predictions are made for users considering the Gaussian distribution of user ratings. Experiments on a real-word data set illustrate that our proposed approach is comparable with others.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {185–191},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026319,
author = {Gao, Xiangzhu and Murugesan, San and Lo, Bruce},
title = {Multi-Dimensional Evaluation of Information Retrieval Results},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Evaluation of information retrieval (IR) results is an important and difficult activity. Available evaluation methods are commonly based on the classical recall and precision measures. While these methods provide an evaluation of average system performance, they are not able to identify influence of other factors such as retrieval task and system user. Detailed information is hidden behind the average and no indication can be obtained from the evaluation for further improvement. Therefore, there is a need of new methods for detailed evaluation. In this paper, we propose a multi-dimensional approach to the evaluation of IR results so that effects of both an IR system and the environment where the system performs on retrieval results can be examined. It aids in the identification of problems of the system and the environment with respect to retrieval effectiveness, and assists in making improvement for IR.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {192–198},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026320,
author = {Baraglia, Ranieri and Silvestri, Fabrizio},
title = {An Online Recommender System for Large Web Sites},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we propose a WUM recommender system, called SUGGEST 3.0, that dynamically generates links to pages that have not yet been visited by a user and might be of his potential interest. Differently from the recommender systems proposed so far, SUGGEST 3.0 does not make use of any off-line component, and is able to manage Web sites made up of pages dynamically generated. To this purpose SUGGEST 3.0 incrementally builds and maintains historical information by means of an incremental graph partitioning algorithm, requiring no off-line component. The main innovation proposed here is a novel strategy that can be used to manage large Web sites. Experiments, conducted in order to evaluate SUGGEST 3.0 performance, demonstrated that our system is able to anticipate users' requests that will be made farther in the future, introducing a limited overhead on the Web server activity.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {199–205},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026321,
author = {Tuulos, Ville H. and Tirri, Henry},
title = {Combining Topic Models and Social Networks for Chat Data Mining},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Informal chat-room conversations have intrinsically different properties from regular static document collections. Noise, concise expressions and dynamic, changing and interleaving nature of discussions make chat data ill-suited for analysis with an off-the-shelf text mining method. On the other hand, interactive human communication has some implicit features which may be used to enhance the results. In our research we infer social network structures from the chat data by using a few basic heuristics. We then present some preliminary results showing that the inferred social graph may be used to enhance topic identification of a chat room when combined with a state-of-the-art topic and classification models. For validation purposes we then compare the performance effects of using this social information in a topic classification task.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {206–213},
numpages = {8},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026322,
author = {Jurca, Radu and Faltings, Boi},
title = {Eliciting Truthful Feedback for Binary Reputation Mechanisms},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Reputation mechanisms offer an efficient way of building the necessary level of trust in electronic markets. Feedback about an agent's past behavior can be aggregated into a measure of reputation, and used by other agents for taking trust decisions. Unfortunately, true feedback cannot be automatically assumed. In the absence of Trusted Third Parties, the mechanism has to make it rational for agents to truthfully share reputation information. In this paper we describe two mechanisms that can be used in decentralized environments for eliciting true feedback. The mechanisms are accompanied by examples inspired by real scenarios.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {214–220},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026323,
author = {Brunner, Brunner and Berrien, Isabelle},
title = {Using Semantic Graphs in Clustering Process: Enhance Information Level},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we particularly focused our attention on how to enhance expressivity of ontologies when used as organized space values in a catalogue request process. Using the Wishbone engine [Method for a data-driven adjustement of queries over a database and system for implementing the method], we describe our new contribution by using a semantic graph instead of a tree for the solution set clustering step. We perform a dynamic transformation in order to enhance the intension meaning, and implement a linear programming clustering algorithm. We also expose a rating system in order to evaluate our clustering process quality.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {221–227},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026324,
author = {Buntine, Wray and Lofstrom, Jaakko and Perkio, Jukka and Perttu, Sami and Poroshin, Vladimir and Silander, Tomi and Tirri, Henry and Tuominen, Antti and Tuulos, Ville},
title = {A Scalable Topic-Based Open Source Search Engine},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Site-based or topic-specific search engines work with mixed success because of the general difficulty of the information retrieval task, and the lack of good link information to allow authorities to be identified. We are advocating an open source approach to the problem due to its scope and need for software components. We have adopted a topic-based search engine because it represents the next generation of capability. This paper outlines our scalable system for site-based or topic-specific search, and demonstrates the developing system on a small 250,000 document collection of EU and UN web pages.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {228–234},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026325,
author = {Yao, J. T. and Zhang, M.},
title = {A Fast Tree Pattern Matching Algorithm for XML Query},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Finding all distinct matchings of the query tree pattern is the core operation of XML query evaluation. The existing methods for tree pattern matching are decomposition-matching-merging processes, which may produce large useless intermediate result or require repeated matching of some sub-patterns. We propose a fast tree pattern matching algorithm called TreeMatch to directly find all distinct matchings of a query tree pattern. The only requirement for the data source is that the matching elements of the non-leaf pattern nodes do not contain sub-elements with the same tag. The TreeMatch does not produce any intermediate results and the final results are compactly encoded in stacks, from which the explicit representation can be produced efficiently.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {235–241},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026326,
author = {Wu, Sheng-Tang and Li, Yuefeng and Xu, Yue and Pham, Binh and Chen, Phoebe},
title = {Automatic Pattern-Taxonomy Extraction for Web Mining},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose a model for discovering frequent sequential patterns, phrases, which can be used as profile descriptors of documents. It is indubitable that we can obtain numerous phrases using data mining algorithms. However, it is difficult to use these phrases effectively for answering what users want. Therefore, we present a pattern taxonomy extraction model which performs the task of extracting descriptive frequent sequential patterns by pruning the meaningless ones. The model then is extended and tested by applying it to the information filtering system. The results of the experiment show that pattern-based methods outperform the keyword-based methods. The results also indicate that removal of meaningless patterns not only reduces the cost of computation but also improves the effectiveness of the system.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {242–248},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026327,
author = {Benassi, Roberta and Bergamaschi, Sonia and Vincini, Maurizio},
title = {TUCUXI: The InTelligent Hunter Agent for Concept Understanding and LeXical ChaIning},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present TUCUXI, an intelligent hunter agent that replaces traditional keywords-based queries on the Web with a user-provided domain ontology, where meanings to be searched are not ambiguous. TUCUXI judges the relevance of the retrieved pages by matching the domain ontology against a simplified, but semantically rich, document representation (Map of Meanings). The Map of Meanings extraction involves the Lexical Chaining technique, from the Natural Language Processing (NLP) research field.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {249–255},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026328,
author = {Li, Yuefeng and Zhong, Ning},
title = {Capturing Evolving Patterns for Ontology-Based Web Mining},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {An ontology-based Web mining model tends to extract an ontology from user feedback and use it to search the right data from the Web to answer what users want.It is indubitable that we can obtain numerous discovered patterns using a Web mining model.However, some discovered patterns might include uncertainties when we extract them.Also user profiles are changeable. Therefore, the difficult issue is how to use and maintain the discovered patterns.This paper presents a theoretical framework for this issue, which consists of automatic ontology extraction, reasoning on the ontology and capturing evolving patterns.The experimental results show that all objectives we expect for the theoretical framework are achievable.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {256–263},
numpages = {8},
keywords = {data reasoning, data mining, pattern evolution, Ontology-based Web mining, ontology learning},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026329,
author = {Lau, Raymond Y. K. and Valdal, Eivind},
title = {Discovering Negotiation Knowledge for a Probabilistic Negotiation Web Service in E-Business},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Negotiation has long been identified as one of the key processes in e-Business. Classical negotiation models have limited use in e-Business because these models often assume that complete information about the negotiation spaces is available and the computational efficiency is negligible. This paper proposes a practical negotiation system which is developed based on Bayesian learning and engineered as a Web service. The knowledge acquisition bottle-neck presented in previous Bayesian negotiation approaches is resolved by a novel method of inducing domain knowledge based on negotiation history. Our preliminary experiment shows that the proposed probabilistic negotiation method is more effective than a non-learning negotiation model and the extra computational time incurs in the probabilistic model is negligible.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {264–270},
numpages = {7},
keywords = {Automated Negotiation, Web Services, e-Business, Bayesian Learning},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026330,
author = {Chen, L. and Cox, S. J. and Tao, F. and Shadbolt, N. R. and Puleston, C. and Goble, C.},
title = {Empowering Resource Providers to Build the Semantic Grid},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The future success of Grid-enabled e-Science depends on the availability of semantic/knowledge-rich resources on the Grid, i.e., the so-called semantic Grid. This requires not only novel knowledge modelling and representation formalisms but also a shift of knowledge acquisition and population from a limited number of specialised knowledge engineers to resource providers. To this end we have developed a lightweight ontology-enabled tool, "Function Annotator", to support resource providers in capturing and publishing resource semantics and knowledge. Function Annotator takes a different line to most knowledge acquisition tools in that it is designed for use by resource providers, probably in the absence of a knowledge engineer. Its aim is to facilitate large scale knowledge population on the Grid. Function Annotator is built on the state-of-the-art of semantic web technologies, such as ontologies, OWL, instance store and DL-based reasoning, thus ensuring flexibility and scalability on the Grid. This paper describes the tool's role in a Grid-oriented resource lifecycle, its underlying technologies and implementation. It also illustrates the usage of the tool in the context of engineering design search and optimisation.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {271–277},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026331,
author = {Hang, Xiaoshu and Dai, Honghua},
title = {An Immune Network Approach for Web Document Clustering},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The human immune system provides inspiration for solving a wide range of innovative problems.In this paper, we propse an immune network based approach for web document clustering.All the immune cells in the network competitively recognize the antigens (web documents) which are presented to the network one by one.The interaction between immune cells and an antigen leads to an augment of the network through the clonal selection and somatic mutation of the stimulated immune cells, while the interaction among immune cells results in a network compression.The structure of the immune network is well maintained by learning and self-regularity. We use a public web document data set to test the effectiveness of our method and compare it with other approaches.The experimental results demonstrate that the most striking advantage of immune-based data clustering is its adaptation in dynamic environment and the capability of finding new clusters automatically.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {278–284},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026332,
author = {Mika, Peter},
title = {Social Networks and the Semantic Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A formal, web-based representation of social networks is both a necessity in terms of infrastructure as well as a prominent application for the Semantic Web. In this paper we present three advances in exploiting the opportunity of semantically-enriched network data: (1) an ontology for the representation of social networks and relationships (2) a hybrid system for online data acquisition that combines traditional web mining techniques with the collection of Semantic Web data (2) a case study highlighting some of the possible analysis of this data using methods from Social Network Analysis, the branch of sociology concerned with relational data.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {285–291},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026333,
author = {Liu, Peng and Chen, Zhong},
title = {An Access Control Model for Web Services in Business Process},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Business process describes a set of services that span enterprise boundaries and are provided by enterprises that see each other as partners. Web services is widely accepted and adopted to construct business process. Web services are built in exposed environment and open to security threats. When a web service contained in a business process is authorized to illegal users, it will cause economic loss of the service provider. Although there exist some standards for security of Web services and access control for services in distributed systems are well studied, there is a lack of comprehensive approach in access control for web services, especially in business process. In this paper, an extended RBAC model, called WS-RBAC, is proposed to secure web services in business process. The model takes web services in business process as protected objects and extends the classical RBAC model. Next, The software architecture of WS-RABC is presented. This paper also presents how to specify business process in the model and the authorization constraints of WS-RBAC based on WS-Policy.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {292–298},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026334,
author = {Sun, Jian-Tao and Zhang, Ben-Yu and Chen, Zheng and Lu, Yu-Chang and Shi, Chun-Yi and Ma, Wei-Ying},
title = {GE-CKO: A Method to Optimize Composite Kernels for Web Page Classification},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most of current researches on Web page classification focus on leveraging heterogeneous features such as plain text, hyperlinks and anchor texts in an effective and efficient way. Composite kernel method is one topic of interest among them. It first selects a bunch of initial kernels, each of which is determined separately by a certain type of features. Then a classifier is trained based on a linear combination of these kernels. In this paper, we propose an effective way to optimize the linear combination of kernels. We proved that this problem is equivalent to solving a generalized eigenvalue problem. And the weight vector of the kernels is the eigenvector associated with the largest eigen-value. A support vector machine (SVM) classifier is then trained based on this optimized combination of kernels. Our experiment on the WebKB dataset has shown the effectiveness of our proposed method.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {299–305},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026335,
author = {Constantinescu, Ion and Faltings, Boi and Binder, Walter},
title = {Type-Based Composition of Information Services in Large Scale Environments},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Service matchmaking and composition has recently drawn increasing attention in the research community. Most existing algorithms construct chains of services based on exact matches of input/output types. However, this does not work when the available services only cover a part of the range of the input type. We present an algorithm that also allows partial matches and composes them using switches that decide on the required service at runtime based on the actual data type. We report experiments on randomly generated composition problems that show that using partial matches can decrease the failure rate of the integration algorithm using only complete matches by up to 7 times with no increase in the number of directory accesses required. This shows that composition with partial matches is an essential and useful element of web service composition.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {306–312},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026336,
author = {Hu, Bo},
title = {Fusing Reasoning Services with Formal Concept Analysis},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Description Logic (DL)-based concept modelling formalisms provide a powerful means to ontology reasoning. However, the inevitable "trade-off" between the expressive power and the computational complexity prevents DLs from widely being applied to model problem domains featured by heterogeneous knowledge.This paper envisages a dynamic approach locating, composing and fusing DL-based inferential engines with constraint reasoning systems without modifying underlying inference algorithms form both sides.The fusion process is facilitated by a scheme driven by the Formal Concept Analysis (FCA) that based on a request, composes engines dumped in a virtual Engine Pool. A simple ontology containing both abstract and concrete knowledge is used to demonstrate the working theory of the FCA-driven reasoning engine composition and the applicability of the inference fusion framework.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {313–319},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026337,
author = {Marchiori, Massimo},
title = {Towards a People's Web: Metalog},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper introduces Metalog, a query/logical system designed to allow reasoning on the Web. Metalog tries to start filling in the so-called people axis, where the Web is tailored for the people, and not just for the machine. Besides allowing the formulation of declarative logical rules, Metalog's distinctive feature is to lower the entry access level, by employing a Pseudo Natural Language (PNL) interface, which is particularly easy to understand. This allows almost everybody to use Metalog, even without any particular expertize in the field. This capability, together with other advanced solutions that enhance customization and user friendliness, are key components for a wide adoption of intelligent semantic web technologies.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {320–326},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026338,
author = {Chuang, Shui-Lung and Hsu, Jane Yung-jen},
title = {Tree-Structured Template Generation for Web Pages},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As the web becomes an increasingly important source of information, tools for modeling, searching, and extracting information from Web pages are indispensable. By modeling the structure of a Web page defined by its markup tags, one can easily extract target information using structural templates. This paper introduces the Tree Template Automatic Generator (TTAG) that learns tree-structured templates from training Web pages. TTAG was applied to both query-based and frequently updated Web sites, and produced effective templates from a small number of examples. The experiments show that TTAG is a powerful extraction tool for semi-structured information sources.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {327–333},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026339,
author = {Luan, Xiaocheng and Peng, Yun and Finin, Timothy},
title = {Quantitative Agent Service Matching},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The ultimate goal of service matching is to find the service provider(s) that would perform tasks of given description with the best overall degree of satisfaction. However, service description matching solves only part of the problem. Agents that match a given request may vary greatly in their actual capabilities to perform the tasks, and an agent may have strong and weak areas. In this work, we take a quantitative approach in which performance rating is considered an integral part of an agent's capability model and service distribution is taken into account in determining the degree of match. With the dynamic refinement of the agent capability model, the broker captures an agent's performance levels as well as its strong and weak areas. An experimental system has been designed and implemented within the OWL/OWL-S framework and the results statistics show significant advantage over other major levels of brokers.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {334–340},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026340,
author = {Wang, Yao and Vassileva, Julita},
title = {Trust-Based Community Formation in Peer-to-Peer File Sharing Networks},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Decentralized peer-to-peer (P2P) networks can benefit from forming interest-based communities that can provide peers with information about the resources shared in the community and collectively computed rating of their quality as well as about the agents in the community and their reputation. We propose a mechanism for forming communities in a P2P system for sharing academic papers. The mechanism requires each agent to compute its trust in the agents with whom it interacts. A simulation shows that such communities can benefit peers.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {341–348},
numpages = {8},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026341,
author = {Habegger, Benjamin and Quafafou, Mohamed},
title = {Building Web Information Extraction Tasks},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most recent research in the field of information extraction from the Web has concentrated on the task of extracting the underlying content of a set of similarly structured web pages. However in order to build real-world web information extraction applications this is not sufficient. Indeed, building such applications requires fully automating the access to web sources. This does not just involve the extraction of the data from web pages. There is a need to set up the necessary infrastructure allowing to query a source, retrieve the result pages, extract the results from these pages and filter out the unwanted results. In this paper we show how such an infrastructure can be set up. We propose to build a web information extraction application by decomposing it into sub-tasks and describing it in an XML based language named WetDL. Each of the sub-tasks consists in applying a web information extraction specific operation onto its input, one of these operators being the application of an extractor. By connecting such operations together it is possible to simply define complex applications. This is shown in the paper by applying this approach to real-world information extraction tasks such as extracting DVD listings from Amazon. com, extracting addresses from online telephone directories superpages.com, etc.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {349–355},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026342,
author = {Hung, Shao-Shin and Kuo, Ting-Chia and Liu, Damon Shing-Min},
title = {An Efficient Mining and Clustering Algorithm for Interactive Walk-Through Traversal Patterns},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many algorithms are suggested for improving the performance of a walkthrough system which contains large-scale VRML models.Since massive objects are stored in the storage systems, and may be scattered, this situation increases the search time to access the objects.However, traditional walkthrough system never considers the problem of how to reduce access times of objects in the storage systems.The quality of the walkthrough system needs to be improved in order to meet the user's demand. In this paper, we present an efficient mining method to improve the efficiency of object accesses.Meanwhile, clustering methodology is particularly appropriate for the exploration of interrelationships among objects to reduce the access time.In other words, prediction and accuracy are our major concerns for improving the system performance.Also, we introduce the relationship measures among transactions, views and objects.Based on these relationship measures, the clustering algorithm will determine how to cluster and the optimal physical organization of those VRML objects on disks.Besides, we suggest two clustering criteria - intra-pattern similarity matrix and inter-pattern frequency table.Our experimental evaluation on the walkthrough data set shows that our algorithm doesn't only significantly cut down the access time, but also enhance the accuracy of data prefetch.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {356–362},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026343,
author = {Maggini, M. and Rigutini, L. and Turchi, M.},
title = {Pseudo-Supervised Clustering for Text Documents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Effective solutions for Web search engines can take advantage of algorithms for the automatic organization of documents into homogeneous clusters. Unfortunately, document clustering is not an easy task especially when the documents share a common set of topics, like in vertical search engines. In this paper we propose two clustering algorithms which can be tuned by the feedback of an expert. The feedback is used to choose an appropriate basis for the representation of documents, while the clustering is performed in the projected space. The algorithms are evaluated on a dataset containing papers from computer science conferences. The results show that an appropriate choice of the representation basis can yield better performance with respect to the original vector space model.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {363–369},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026344,
author = {Cho, W. C. and Richards, D.},
title = {Improvement of Precision and Recall for Information Retrieval in a Narrow Domain: Reuse of Concepts by Formal Concept Analysis},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the exponential growth of the World Wide Web (WWW), it has become the most popular place to gather information. However the size of the WWW makes it difficult for people to locate relevant information. About 85% of all Web users use search engines of some kind for this purpose. However, existing search engines often do not return relevant information. The main focus of this paper is to improve search performance by using keywords and web pages which have been previously used or visited by other users. The Formal Concept Analysis (FCA) method has been adapted to maintain a concept map of keywords. This paper shows how both precision and recall has improved the specific-domain area, in which users can share the same knowledge.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {370–376},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026345,
author = {Haq, Mohammad Aminul and Matsumoto, Mitsuji},
title = {MAMI: Mobile Agent Based System for Mobile Internet},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Accessing the Internet on the move via wireless link requires some provisioning in the network and the terminal part so that we can overcome the difficulties of wireless link, mobility, and device capability limitations. In this paper we propose a mobile agent based system for wireless mobile Internet. We use static agents (terminal and provider) at the two ends of wireless link. We represent the user by a mobile agent on the provider side, mobilize this agent when necessary. We show when, how and what to mobilize. We show that our agent based system has some desirable properties which are effective in compensating the drawbacks of wireless environment, mobile device limitations and TCP, HTTP protocol limitations in wireless environment. We describe the system architecture and operation scenario. Benefits of using agent in this scenario are discussed and we show how different optimization techniques can easily be incorporated in such a system due to its flexibility.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {377–383},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026346,
author = {Yih, Wen-tau and Chang, Po-hao and Kim, Wooyoung},
title = {Mining Online Deal Forums for Hot Deals},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Online deal forums are public places where participants share with each other news and information regarding "deals" such as sales promotion events by online stores. The large number of messages in the forums and their inherent uncertainty make it difficult for even seasoned users to identify useful deal information from the forums. We develop an intelligent deal alert service which assists ordinary Web surfers to find useful deals by mining online deal forums. It periodically crawls relevant deal forums to collect fresh message posts and responses, and evaluate them using a form of probabilistic text classification. Users may be notified of new, "potentially" useful deal messages via emails or they may browse them using their favorite Web browser. We train and evaluate the service using deal posts and responses collected from actual deal forums in the Web. The preliminary evaluation results show that the service is quite effective in reducing the time to find useful deals.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {384–390},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026347,
author = {Mohapatra, Roshni and Rajaraman, Kanagasabai and Yuan, Sung Sam},
title = {Efficient Wrapper Reinduction from Dynamic Web Sources},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper investigates wrapper induction from web sites whose layout may change over time. We formulate the reinduction as an incremental learning problem and identify that wrapper induction from an incomplete label is a key problem to be solved. We propose a novel algorithm for incrementally inducing LR wrappers and show that this algorithm asymptotically identifies the correct wrapper as the number of tuples is increased. This property is used to propose a LR wrapper reinduction algorithm. This algorithm requires examples to be provided exactly once and there-after the algorithm can detect the layout changes and reinduce wrappers automatically. In experimental studies, we observe that the reinduction algorithm is able to achieve near perfect performance.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {391–397},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026348,
author = {Song, Guanglei and Zhang, Kang and Wong, Raymond K. and Kong, Jun},
title = {Management of Web Data Models Based on Graph Transformation},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Based on the Reserved Graph Grammar (RGG), this paper presents a unified framework to manage model-based information on the Web in a hierarchical structure. The framework allows models, schemas, and data instances to be represented explicitly and uniformly. The uniform representation of the framework also enables simple user-defined graph transformation rules for different Web data models to translate schemas and data instances between different formats. In addition, the framework implements a set of prototype tools for users to identify meta-primitives at the meta-model level, to define a model or schema by specifying a set of graph grammar rules and to draw the structure of data instances. These features promote a wide scope of Web-related applications, such as information exchange between different organizations, and integration of data coming from heterogeneous information sources.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {398–404},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026349,
author = {Tsui, Kwok Ching and Liu, Jiming and Shi, Jinglun},
title = {Anycast-Based Cooperative Proxy Caching: Preliminary Results},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The World Wide Web is one of the most popular applications currently running on theInternet, and its size is growing exponentially. Web caching is an important technique that aims to reduce network traffic, server load, and user-perceived retrieval delays by replicating popular contents on proxy servers. Anycast is a new network service widely used for providing auto-configuration and load-balancing. We present an anycast-based cooperative proxy algorithm (ACPA) that brings a server ienearestle to a client to improve overall performance by selective allowing duplicates and migrate objects to other proxies based on request pattern. Analytical and experimental results show that the algorithm outperforms a hash-based algorithm in response time and hop counts.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {405–411},
numpages = {7},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026350,
author = {Cheung, William K. and Liu, Jiming and Tsang, Kevin H. and Wong, Raymond K.},
title = {Dynamic Resource Selection For Service Composition in The Grid},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {While numerous efforts have focused on service composition in the Grid environment, service selection among similar services from multiple providers has not been addressed. In particular, all service composition work done so far are based on a given selection of services under a well set environment. As a result, uncertainty (e.g., server load, network traffic, computation time of the services due to changing memory and other unexpected conditions) under a real, dynamic environment has never been considered. This paper prototypes the service selection under a Grid environment and proposes an uncertainty framework to address the issue. Experimental results show that our considerations are valid and our preliminary solution works well in our Globus Grid network.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {412–418},
numpages = {7},
keywords = {uncertainty management, Services composition, ontology, Grid services},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026351,
author = {Cuzzocrea, Alfredo},
title = {Knowledge on the Web: Making Web Services Knowledge-Aware},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Knowledge personalization is currently the most investigated issue in the context of service-oriented systems on the Web. Knowledge representation and management are the critical issues for knowledge personalization, and actually are currently being widely investigated, mainly due to the explosion of data modeling technologies such as XML and XML Schema. Despite some progress, a widely approved standard for delivering knowledge is still missing. In this paper we propose a new approach for representing, managing, and delivering knowledge on the Web and the correspondent framework, called Distributed Knowledge Networks (DKN), that implements it. We also provide a reference architecture for DKN and some experimental results about knowledge personalization.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {419–426},
numpages = {8},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026352,
author = {Sala, Michel and Pompidor, Pierre and Herin, Daniele and Isoird, Gael},
title = {Help a Teacher in the Conception of a Course by Using Semantic Information},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper aims to help a teacher to improve its course by using semantic information.For it, we supply to the teacher a means to evaluate his on-line course with regard to the student browsing..So, we can reveal various types of problems and propose a referential or a conceptual revision of the course.The learning phases, and so knowledge acquired with by students are improved.Course is modelled by a global ontology and we study the student logs to obtain semantic information about their browsing.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {429–432},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026353,
author = {Yang, Wenchuan and Zhu, Wujie and Liu, Yang and De, Yan},
title = {The Prototype Research of a Web-Based DSS Intelligent Agent Over Data Warehouse},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In the data analysis processing based on the statistic data warehouse, we divided the hierarchy of Web-based Decision Support System Intelligent Agent into different layers, which were named method, model and application intelligent agent layer.This paper introduces the definition and function for each layer in Statistic data warehouse, also gives the theory and practice in it.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {433–436},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026354,
author = {Xue, Liang and Guan, Tao and Feng, Boqin},
title = {XML-Based Meta-Retrieval of Networked Data Sources},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The XML-based meta-retrieval described in this paper is an integrated retrieval over multiple academic data sources on the premise that each data source has one web interface visible in the Internet environment. The goal of this model is to present an integrated retrieval interface in the Internet, to offer transparent accessing by formulating new queries suitable for heterogeneous data sources based on three adaptation rules, and to return the consolidated data results retrieved from multiple data sources. Technologies like eXtensible Markup Language(XML) and eXtensible Stylesheet Language Family(XSL) are used to facilitate the fulfillment of the goal. Experiment results prove that the model solve the heterogeneity of the data sources and provide transparent accessing for the end users.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {437–440},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026355,
author = {Lingras, Pawan and Hogo, Mofreh and Snorek, Miroslav},
title = {Temporal Cluster Migration Matrices for Web Usage Mining},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Time can play a crucial role in the analysis of web usage. Temporal data mining has been an active area of research. However, there is little work on the analysis of cluster memberships over time. Typical clustering operations in web mining involve finding natural groupings of web resources or web users. Changes in clusters can provide important clues about the changing nature of the usage of a web site, as well as changing loyalties of web users. This paper addresses two different types of temporal changes in cluster analysis. The changes in cluster compositions over time and changes in cluster memberships of individual web users. The paper also proposes the concept of temporal cluster migration matrices (TCMM). The proposed matrices are shown to be useful for analyzing the changing nature ofa web site as well as changing patronages of individual web users. TCMM can be used as a visualization technique to study results obtained from temporal data mining, which can be more complex because of the additional time dimension.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {441–444},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026356,
author = {Zhou, Lina and Zhang, Dongsong},
title = {Building a Misinformation Ontology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The importance of misinformation research has received wide recognition. However, one major challenge facing the research community is the lack of misinformation data and the difficulty in managing such data. Motivated by the role of ontology in information sharing and reuse, we propose a misinformation ontology and its representation based on the analysis of the characteristics of misinformation. The application of misinformation ontology is illustrated with a case study. Finally, we discuss implications of the research result.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {445–448},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026357,
author = {Mei, Qibin and Wang, Chengbo and Mei, Xiaolan},
title = {Description Mathematical Approach to Validation for Grid Service Matching Function},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In grid computing, we believe that keywords and ontologies can not always be defined or interpreted precisely enough to achieve semantic agreement in a truly distributed, heterogeneous computing environment. In this paper, we present the functional validation concept in grid computing, analyze the possible validation situations and apply basic machine learning theory such as PAC learning and Chernoff bounds to explore the relationship between sample size and confidence in service semantics.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {449–452},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026358,
author = {Peng, Chengyuan and Vuorimaa, Petri},
title = {Text Adaptation for Mobile Digital Teletext},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Small and varying screen sizes of mobile devices pose a big problem for digital Teletext service to display its content. It is difficult to display all the text information on a small screen, where page scroll or transparent page to live video is not practical. This paper presents an adaptive text extraction method which can automatically extract key information from original text and keep semantic meanings as close as possible. We combine both statistical methods and coarse coding algorithm from neural science to shorten long text sentences in terms of generalization. The experiment results show that the methods are efficient and effective.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {453–456},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026359,
author = {Li, Keqiu and Shen, Hong},
title = {An Improved GreedyDual Cache Document Replacement Algorithm},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web caching is an important technique for reducing web traffic, user access latency, and server load and cache replacement plays an important role in the functionality of web caching. In this paper we propose an improved GreedyDual (GD) cache document replacement algorithm, which considers update frequency as a factor in its utility function. We use both trace data and statistical data to simulate our proposed algorithm. The experimental results show that our improved GD algorithm can outperform the existing GD algorithm over the performance metrics considered.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {457–460},
numpages = {4},
keywords = {Web caching, simulation, GreedyDual, cache replacement, algorithm},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026360,
author = {Zhang, Fan and Zhang, Hongbin},
title = {Image Watermarking Capacity Analysis Using Neural Network},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Image watermarking capacity research is to study how much information can be hidden in an image. In watermarking schemes, watermarking can be viewed as a form of communications. Almost all previous works on watermarking capacity are based on information theory, using Shannon formula to calculate the capacity of watermarking. This paper presents a blind watermarking algorithm using Hopfield neural network, and analyzes watermarking capacity based on neural network for the first time. Result shows that the attraction basin of associative memory decides watermarking capacity.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {461–464},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026361,
author = {Bangyong, Liang and Jie, Tang and Juanzi, Li and Kehong, Wang},
title = {Using DAML+OIL to Enhance Search Semantic},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current web search mostly relies on the keywords in the web pages. This method lacks of semantics ii many ways. For example, a search for a person by the person's name means to find the web pages that contain the text of the name. On the contrary, search semantic is to find the information about the person in the real world. It is hard to achieve this goal in current content based web search engines because text is not useful during inference. The semantic web brings semantic to current web with formalized knowledge and data that computers can process. Therefore search can be benefit from the inference which is supported by ontology. In this paper, we propose a novel method to enhance the search semantic using ontology language DAML+OIL. Experiment shows preferable results comparing to the traditional search. The conclusion and future work will also be discussed in this paper.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {465–468},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026362,
author = {Mao-yuan, Zhang and Zheng-ding, Lu},
title = {A Fuzzy Classification Based on Feature Selection for Web Pages},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {An automatic web page classification is needed for web information extraction, but the number of keywords of web pages is so giant that many classifications are not speedy or capable of self-learning. In this paper, a fuzzy classification method for web pages, which is based on fuzzy learning and parallel feature selection, is proposed. Fuzzy learning of parameter c{ik} is adopted to increase the accuracy, while parallel feature selection based on weighted similarity is used not only to decrease the dimension of the features but also to let parameter c{ik} need no learning. The weights of features are deducted in theory, and to speed up the calculation of weights, a parallel sum algorithm of the matrix is proposed.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {469–472},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026363,
author = {Thawani, Amit and Gopalan, Srividya and Sridhar, V.},
title = {Web-Based Context Aware Information Retrieval in Contact Centers},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One of the important challenges in today's contact center solutions is to increase the speed at which the agents can find information to respond to customer queries. In this paper, we study the issues relevant to information retrieval by contact center agents and propose a solution to address those issues. Our proposed solution is towards: (a) defining user specific, agent specific, and business specific contexts for contact center; (b) using simple mechanisms initially to derive current context of a query and pre-fetch the information and appropriately present the pre-fetched information to the agent; (c) derive contact center specific analyzed context based on past information about customer, agent and business scenarios to understand the context of the query; and (d) using the current and analyzed contexts to retrieve information.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {473–476},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026364,
author = {Stojanovic, Nenad},
title = {A Logic-Based Approach for Query Refinement},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present a novel approach for the refinement of relational queries that enables so-called step-by-step refinement of a user's query. The approach is based on discovering causal relationships between queries regarding the inclusion relation between the answers of these queries. We define a formal model for these query-answering pairs and use methods from inductive logic programming for the efficient calculation of a (lattice) order between them. The approach is very suitable for modelling information retrieval tasks based on the database repositories, like searching product catalogues.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {477–480},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026365,
author = {Huayong, Liu},
title = {Content-Based TV Sports Video Retrieval Based on Audio-Visual Features and Text Information},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose content-based video retrieval, which is a kind of retrieval by its semantical contents. Because video data is composed of multimodal information streams such as visual, auditory and textual streams, we describe a strategy of using multimodal analysis for automatic parsing sports video. The paper first defines the basic structure of sports video database system, and then introduces a new approach that integrates visual streams analysis, speech recognition, speech signal processing and text extraction to realize video retrieval. The experimental results for TV sports video of football games indicate that multimodal analysis is effective for video retrieval by quickly browsing tree-like video clips or inputting keywords within predefined domain.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {481–484},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026366,
author = {Ratinov, L. and Gudes, E.},
title = {Abbreviation Expansion in Schema Matching and Web Integration},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Schema matching is a problem of finding correspondences, particularly equivalence relationships across schemas. The problem has a particular significance in integrating web repositories, as distributed databases over the web becomes increasingly popular. Most of the existing prototypes use schema level lexical information for schema matching. However, most of them perform rather poorly on real-world problems due to the abundance of abbreviations in real-world schemas. For example, none of the lexical matchers we tested would recommend a mapping of 'cnum' to 'cid', while 'customer number' and 'customer ID' are matching entities. In this work we propose a method for abbreviation expansion in schemas that facilitates lexical schema matching.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {485–489},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026367,
author = {Wu, Xiaobing},
title = {Knowledge Representation and Inductive Learning with XML},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a novel knowledge representation method and learning system for XML documents. The traditional machine learning methods which use attribute-value languages are not suitable for representing XML documents due to their complex structures. In this paper, we propose a decision-tree algorithm for XML learning, which is based on a rich representation language for structured data and driven by precision/recall heuristic.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {491–494},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026368,
author = {Silvestri, Fabrizio and Puppin, Diego and Laforenza, Domenico and Orlando, Salvatore},
title = {A Search Architecture for Grid Software Components},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Today, the development of Grid applications is considered a nightmare, due to lack of grid programming environments, standards, off-the-shelf software components, and so on.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {495–498},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026369,
author = {Tang, Menglei and Zhang, Yan-Qing and Zhang, Gang},
title = {Type-2 Fuzzy Web Shopping Agents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we create an online shopping system "hotstore.com" using type-2 fuzzy reasoning. The type-2 fuzzy logic agent has 5 components: fuzzifier, fuzzy rule, fuzzy inference, type reducer and defuzzifier. The antecendent and consequent in the fuzzy rule use type-2 fuzzy sets and the type reducer is used to map a type-2 fuzzy set into a type-1 fuzzy set. All the merchandise information is stored in the database. Based on the merchandise shopping history, a manager can rate the merchandise data using type-2 fuzzy data mining technology. Implementation techniques include ASP.NET + SQL 2000 + IIS 2000, VB, ASP.NET, HTML, and CSS.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {499–503},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026370,
author = {Shi, Lei and Gu, ZhiMin and Wei, Lin and Shi, Yun},
title = {Popularity-Based Selective Markov Model},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web prefetching is a promising solution used to reduce user's latency and improve the QOS.This paper presents a popularity-based selective Markov prefetching model for predicting the forthcoming Web pages.We make use of teh Zipf's law to model the Web objects' popularity.An experimental evaluation of the prefetching mechanism is presented using real server logs.Our trace-driven simulation results show that the popularity-based selective.Markov prefetching model can achieve a good hit ratio with reducing the traffic load to some degree.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {504–507},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026371,
author = {Yuan, Soe-Tsyr and Sun, Jerry},
title = {Ontology-Based Structured Cosine Similarity in Speech Document Summarization},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Development of algorithms for automated text categorization in massive text document sets is an important research area of data mining and knowledge discovery. Most of the text-clustering methods were grounded in the term-based measurement of distance or similarity, ignoring the structure of terms in documents. In this paper we present a novel method named Structured Cosine Similarity that furnishes document clustering with a new way of modeling on document summarization, considering the structure of terms in documents in order to improve the quality of speech document clustering.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {508–513},
numpages = {6},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026372,
author = {Baethies, Sebastian and Gaertner, Christian and Spanihel, Siglinde and Tsatsas, Dimitrios},
title = {Design of an Emotional Search in an Existing Product Platform Www.Stylepark.Com},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we describe the interactive effect of multi-medially presented picture worlds. A product database, present on the Internet since 2001, is to be extended by an additional user interface. An experimental web design draft based on emotional reactions is presented. Have a stroll, let yourself go, select and refine your selections. Decision-making processes are based on real-world experiences. Select furniture as if you were strolling through the city center. All products available in the database of www.stylepark.com are associated with attributes which are encoded in XML. Thus, products can be searched for and found in multiple selection ranges. The new user interface allows for a greater sphere of activity within the search by attaching emotional and symbolic attributes and values to existing products. Starting point is a specific motivation of the user. The motivation matrix contains the motives of the use in its structure.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {514–518},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026373,
author = {Linn, Craig},
title = {A Metric Framework for Quantifying Semantic Reliability in Shared Ontology Environments},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Modern web based systems frequently exchange semantically rich messages in order to carry out their tasks. Such systems include Web Services, multi-agent systems, and the semantic web in general. For such message communication to be successful the terms used in messages must be interpreted according to agreed conventions on term meaning and hence usage. Ontologies can provide a common vocabulary to facilitate such meaning exchange. However semiotic constraints and application shortcomings, may still result in less than perfect semantic exchanges between applications. Where the variability of interpretation of messages is high (i.e. inconsistent or incomplete interpretation) we may have a semantically unreliable system. To date semantic reliability has only been examined in qualitative terms. This paper proposes a metric framework based on ontology test bed results to provide quantitative indices for likely semantic reliability. Such numeric indices are important, for as web applications assume more intelligent and complex tasks the potential for semantic variance in interactions only increases, and this must be carefully managed for next generation web systems to interact reliably. In order to manage anything it is invariably essential to be able to measure it; and the proposed quantitative indices provide an appropriate and viable approach for such measurement.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {519–523},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026374,
author = {Chen, Bee-Chung and Hsiang, Jieh},
title = {A Logical Framework of Knowledge Retrieval with Fuzziness},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Fuzziness is an intrinsic property of knowledge retrieval. For instance, the relationship similarity is fuzzy by nature. The degree of truth of some transitive relations naturally degrades as transitivity is applied repeatedly. In this paper we propose a formal model of knowledge retrieval based on Goguen Style Fuzzy Horn Logic, a subset of Goguen's fuzzy logic system [The Logic of Inexact Concepts]. This model provides a mathematical (logical) foundation for knowledge retrieval that naturally incorporates fuzziness. By using only a subset of Goguen's logic, we have a simple but useful and efficient reasoning framework.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {524–528},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026376,
author = {Fumin, Bao and Aiguo, Li and Zheng, Qin},
title = {Photo Time-Stamp Recognition Based on Particle Swarm Optimization},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Time-stamp in image shows the time when an image was taken, and which is important information for retrieval. An automatic photo time-stamp recognition approach is proposed in this paper. The proposed method consists of three steps: 1) A photo is roughly segmented to determine which area contains the photo time-stamp. Four detecting areas are chosen in this step, which probably include the photo-stamp, and then their R component and G component are margin-detected using Sobel operator to determine whether the photo time-stamp is in them; 2) The area which contains time-stamp of the photo is finely segmented to locate each character; 3) Particle Swarm Optimization (PSO) algorithm is used to do template matching and optimization recognition to complete the photo time-stamp recognition. This approach needs no prior knowledge and off-line learning processes. Experimental results demonstrate that the proposed approach has high recognition accuracy.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {529–532},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026375,
author = {Yu, Shui and Zhou, Wanlei},
title = {A Novel Middleware Based Web Database Model},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose a novel model for web-based database systems based on the multicast and anycast' protocols.In the model, we design a middleware, castway, which locates between the database server and the Web server.Every castway in a distributed system operates as a multicast node and an anycast node independently, respectively.The proposed mechanism can balance the workload among the distributed database servers, and offers the "best" server to serve for a query.Three algorithms are employed for the model: the requirement-based probing algorithm for anycast routing, the atomic multicast update algorithm for database synchronization, and the job deviation algorithm for system workload balance.The simulations and experiments show that the proposed model works very well.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {533–536},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026377,
author = {Liu, Shaohua and Wei, Jun and Ma, Yinglong and Liu, Yu},
title = {Web Service Cooperation Ideology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As the Internet environment becomes more and more dynamic, open and mutable, future software have to be more autonomic, reactive, adaptive, cooperative, and evolvable. To meet the need, we introduce emerging service cooperation middleware providing such infrastructure support. Derived from Chinese ancient five-elements ideology, a similar service cooperation philosophy is developed. Complying the idea, we develop a workflow system, PI, supporting Process Intelligence. We believe that the service cooperation will become a feasible solution to the future complex environment.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {537–540},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026378,
author = {Wang, Xuren and Xu, Rongsheng and Wang, Wansen},
title = {Rough Set Theory: Application in Electronic Commerce Data Mining},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As Electronic Commerce become more and more prevalent to people, this paper discusses application of rough set theory in Electronic Commerce (EC) data mining. The EC sites collect large data every day. The data includes valuable information about customers, products and so on. The large amount of data can be mined to find the unknown knowledge or trends hiding in the data, which can be used by the sites to arrange their products according to the buyers' preference and take appropriate selling policies. This paper employs rough set theory that can perform feature selection to obtain patterns of customers and products.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {541–544},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026379,
author = {Bomhardt, Christian},
title = {NewsRec, a SVM-Driven Personal Recommendation System for News Websites},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Fast absorption of information is a necessity for modern information workers. In the short-lived news area, information is a perishable good. While online news websites can speed up the publication of current events compared to traditional newspapers, reading can be more exhausting as online readers have to navigate through websites by clicking on abstracts or headlines before viewing the underlying article. Online shops use personalization methods in order to improve product selection. So far, most types of personalization are offered by website owners and are therefore bound to a specific website. This work presents NewsRec, a client side personal recommendation system for news websites, that supports information workers during their usage of online news websites. Design aspects are discussed and empirical results are shown.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {545–548},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026380,
author = {Abel, Abel and Lenne, Dominique and Moulin, Claude and Benayache, Ahcene},
title = {Using Two Ontologies to Index E-Learning Resources},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As each university owns several web sites dedicated to face-to-face lectures support or distance learning training, courses can no more be considered as global presentations of theory elements. Organizations involving different actors, contribute to produce documents and information and doing so must share knowledge. We present in this paper a way to index documents inside a memory adapted to e-learning situations. This indexing mainly requires concepts representing the pedagogical notions of the domain of training. It requires also some concepts that are typical of learning activities.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {549–552},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026381,
author = {Viamonte, Maria Joao and Ramos, Carlos and Rodrigues, Fatima and Cardoso, Cardoso},
title = {Simulating the Behaviour of Electronic MarketPlaces with an Agent-Based Approach},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We forecast a future in which the global economy and the Internet will host a large number of interacting software agents. Most of them will be economically motivated, and will negotiate a variety of goods and services. It is therefore important to consider the economic incentives and behaviours of economic software agents, and to use all available means to anticipate their collective interactions. This paper addresses this concern by presenting a multi-agent market simulator designed for analysing agent market strategies based on a complete understanding of buyer and seller behaviours, preference models and pricing algorithms. The results of the negotiations between agents will be analysed by data mining tools in order to extract rules that will give the agents feedback to improve their strategies.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026382,
author = {Mikroyannidis, Alexander and Theodoulidis, Babis},
title = {A Theoretical Framework and an Implementation Architecture for Self Adaptive Web Sites},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Serious difficulties in navigation through the web are very often encountered by users, especially in structurally complicated web sites. The main reason behind this is the lack of adaptation of a web site to its visitors' needs. Constant reorganization of the information offered by a web site is required, based upon the users' online behaviour. In the context of the present paper, a framework for self-adaptive web sites is presented. Furthermore, an architecture implementing this framework is introduced. It employs web usage mining as well as text mining methodologies with the intention of improving the web site's structure and ontology.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {558–561},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026383,
author = {Doulkeridis, Christos and Vazirgiannis, Michalis},
title = {Querying and Updating a Context-Aware Service Directory in Mobile Environments},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Several interesting research directions materialize through the convergence of mobile computing and service-oriented computing. As mobile devices keep getting smaller, cheaper and more sophisticated, their use is becoming a commodity. We envision future scenarios that involve mobile devices acting not only as requestors, but as providers of data as well. In order to hide the heterogeneous nature of web data, service-oriented architectures are adopted. Nevertheless, existing service discovery mechanisms usually focus on exact or semantic matching of static attributes, thus ignoring contextual parameters. We argue that context for mobile web services plays an important role in service discovery by increasing the precision and efficiency of the search. We explain our notion of context regarding mobile services and describe query evaluation, updating and merging of context-aware service directories.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {562–565},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026384,
author = {Bai, Qingyuan and Hong, Jun and McTear, Michael F. and Wang, Hui},
title = {Bucket-Based Query Rewriting with Disjunctive Data Source},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many algorithms for query rewriting using views have been proposed.Most of them are used for rewriting conjunctive queries with conjunctive views only.Only one inverse rule-based rewriting algorithm has been presented to deal with query rewriting with disjunctive views, in which a set of disjunctive inverse rules are created and used to generate the query rewritings.However, there has no been bucket-based algorithm for this issue.In this paper we apply the ideas of the previous bucket-based algorithms to deal with query rewriting using views in the presence of disjunctions in view definitions.We create a set of buckets over either a conjunctive view or a disjunctive view.Specially, when a bucket is over a disjunctive view, we present a method to remove the tuples that are in the view but not required by a given query.The rewritings obtained by our algorithm are contained in the original query.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {566–569},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026385,
author = {Zhu, Cheng and Liu, Zhong and Zhang, Weiming and Xiao, Weidong and Huang, Jincai},
title = {An Efficient Decentralized Grid Service Discovery Approach Based on Service Ontology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents an efficient decentralized Grid service discovery approach based on service ontology. It uses two techniques to improve efficiency. First, Grid information nodes are organized into community overlays of different service categories defined in service ontology. A distributed hash table (DHT) based upper layer network is constructed to provide efficient navigation between communities. Second, a simple and lightweight greedy search based service location (GSBSL) method is introduced to identify service providers with high QoS efficiently within communities. Simulation results show that, the efficiency is improved compared with existing decentralized Grid service discovery approaches, and the overhead is acceptable and controllable.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {570–573},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026386,
author = {Yang, Dong-Sheng and Liu, Zhong and Lu, Yin-Long and Zhang, Wei-Ming},
title = {Task Allocating Among Group of Agents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {An effective task allocating is important for MAS to complete its missions through efficient cooperation among agents.A new approach is advanced in this paper, which is based on the multidimensional dynamic list scheduling (MDLS) algorithms.And the results from the two algorithms are compared.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {574–578},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026387,
author = {Meng, Tao and Yan, Hongfei and Wang, Jimin and Li, Xiaoming},
title = {The Evolution of Link-Attributes for Pages and Its Implications on Web Crawling},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {It is important for an incremental crawler to know how web pages evolve and the relation between their changing frequencies and the link-attributes such as indegrees. This paper proposes a model for incremental crawling and performs an experiment to verify the correlation between them, by monitoring the evolution of all the link-attributes of the web pages within one website. Particularly, we look deeply into one special kind of page named Index-pages. From the experiment, we can make four conclusions: (1) Pages which have bigger indegrees, outdegrees or PageRank values change more often, and these link-attributes all approximately obey a power-law distribution. (2) The link-attributes of pages seldom change though the pages change themselves. (3) A small proportion of the pages link to most of the vertexes in the web graph. (4) The Index-pages link to sizeable new pages in a website. These conclusions can be used to greatly enhance the performance of an incremental crawler, which is the foremost component for general search engines and web information stores.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {578–581},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026388,
author = {Zhou, Qing and Zheng, ZeQI},
title = {An Intelligent Query Expansion of Searching Related Text Information by Keywords},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we represent a new idea of searching related text information on web. The syntactical characters of related keywords and texts are described detail, which do not involve semantics of the keywords. So computers can find related keywords and texts without understanding their meanings. Based on these, an intelligent expansion of query of searching related text information is proposed. Algorithms for such an expansion are provided. The algorithms are effective. The results of experiments of this method we made on internet before show that the algorithms in the paper are powerful and satisfying. The paper concludes with a comprehensive analysis on the results of the experiments.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {582–585},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026389,
author = {Chan, Jason and Koprinska, Irena and Poon, Josiah},
title = {Co-Training with a Single Natural Feature Set Applied to Email Classification},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {When dealing with information overload from the Internet, such as the classification of Web pages and the filtering of email spam, a new technique called co-training has been shown to be a promising approach to help build more accurate classifiers. Co-training allows classifiers to learn with fewer labelled documents by taking advantage of the more abundant unclassified documents. However, conventional co-training requires the dataset to be described by two disjoint and natural feature sets that are sufficiently redundant. In many practical situations, it is not intuitively obvious how to obtain two natural feature sets. This paper shows that when only a single natural feature set is used, the performance of co-training is beneficial in the application of email classification.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {586–589},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026390,
author = {Phan, Xuan Hieu and Horiguchi, Susumu and Ho, Tu Bao},
title = {PEWeb: Product Extraction from the Web Based on Entropy Estimation},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Mining product descriptions (PDs) from e-commercial web sites is an important task in information extraction from the Web. In this paper, we propose an efficient technique for this task. The technique first discovers the set of PDs based on the measure of entropy at each internal node in the HTML tag tree. Afterwards, a set of association rules based on heuristic features is employed to filter the output and therefore enhance the precision. The experimental results of PEWeb system show that the proposed method outperforms existing automatic techniques remarkably.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {590–593},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026391,
author = {Balfe, Evelyn and Smyth, Barry},
title = {Query Mining for Community Based Web Search},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present an innovative approach to personalized Web search that exploits the search behaviour of a community of users to re-rank future result-lists according to the implied preferences of this group. Evaluation results demonstrate the precision and recall benefits of our collaborative search technique and we show how personalization can be achieved without the need for individual user profiling.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {594–598},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026392,
author = {How, Bong Chih and Narayanan, K.},
title = {An Empirical Study of Feature Selection for Text Categorization Based on Term Weightage},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes a local feature selection (FS) measure namely, Categorical Descriptor Term (CTD) for text categorization. It is derived based on classic term weighting scheme, TFIDF. The method explicitly chooses feature set for each category by only selecting set of terms from relevant category. Although past literatures have suggested that the use of features from irrelevant categories can improve the measure of text categorization, we believe that by incorporating only relevant feature can be highly effective. The experimental comparison is carried out between CTD and five well-known feature selection measures: Information Gain, Chi-Square, Correlation Coefficient, Odd Ratio and GSS Coefficient. The results also show that our proposed method can perform comparatively well with other FS measures, especially on collection with highly overlapped topics.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {599–602},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026393,
author = {Xu, Jianjun and Zhu, Qian and Li, Juanzi and Zhang, Po and Wang, Kehong},
title = {Modeling and Implementation of Unified Semantic Web Platform},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {More and more infrastructure software for semantic web has emerged with the popularity of Semantic Web. Now Semantic Web applications are calling for different infrastructure software to support essential ontology operations, such as ontology persistence, ontology consistency, ontology query, ontology management, reasoning and so on.This paper brings forward the vision about integration of ontology operations based on the unified semantic web software platform.It introduces a model in which varied operations of ontology are added, updated and deleted dynamically.Furthermore, a USWP ("Unified Semantic Web Platform") is implemented according to this model.As an ontology platform using the standard of RDF and RQL, the key characteristic of the USWP is that the platform has a "total solution" for the semantic web applications built on an expansible, flexible, scalable and open architecture.Third part's ontology operation modules can be deployed into USWP with a wrapper under the standard of OOU ("Ontology Operation Unit") and can share data flow with other modules using DOOD ("Dynamical Operatin Ontology Domain") dynamically.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {603–606},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026449,
author = {Gu, Jinguang and Chen, Heping and Yang, Lingxian and Zhang, Lin},
title = {OBSA: Ontology-Based Semantic Information Processing Architecture},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Integrating with the respective advantages of XML Schema and Ontology, this paper puts forward a semantic information processing architecture-OBSA to solve the problem of heterogeneity of information sources and uncertainty of semantic. It introduces an F-Logic based semantic information presentation mechanism, presents a design of an ontology-based semantic representation language and a mapping algorithm converting Ontology to XML DTD/Schema, and an adapter architecture for accessing distributed and heterogeneous information.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {607–610},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026394,
author = {Li, Tianchao and Bollinger, Toni and Breuer, Nikolaus and Wehle, Hans-Dieter},
title = {Grid-Based Data Mining in Real-Life Business Scenario},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a Grid-based distributed and parallel data mining system targeting a real-life application scenario typical in the business realm - franchise supermarket basket analysis. Following a layered design of three tiers, this system enables parallel association rule mining on a farm of Grid servers, offers a standard service interface for custom applications, and provides a friendly user portal. The work presented in this paper reveals specific requirements for applying Grid-based data mining in the business realm, which is helpful for the design and implementation of a generic Grid-based data mining system.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {611–614},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026395,
author = {Vargas-Vera, Maria and Celjuska, David},
title = {Event Recognition on News Stories and Semi-Automatic Population of an Ontology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes a system which recognizes events on news stories. Our system classifies stories and populates a hand-crafted ontology with new instances of classes defined in it. Currently, our system recognizes events which can be classified as belonging to a single category and it also recognizes overlapping events within one article (more than one event is recognized). In each case, the system provides a confidence value associated to the suggested classification. Our system uses Information Extraction and Machine Learning technologies. The system was tested using a corpus of 200 news articles from an archive of electronic news stories describing the academic life of the Knowledge Media (KMi). In particular, these news stories describe events such as a project award, publications, visits, etc.)},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {615–618},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026396,
author = {Weigang, Li and Pinheiro Dib, Marcos V. and Cardoso, Daniel Amaral},
title = {Grid Service Agents for Real Time Traffic Synchronization},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Grid Service Agents for Real Time Traffic Synchronization is proposed in this research. The paper presents Air Traffic Flow Management (ATFM) problem and its synchronization property. For such a complex problem, using grid computing with multi-agent coordination and negotiation techniques to improve ATFM computational efficiency is the main objective of actual even further research. To demonstrate the developed model - Air Traffic Flow Management in Grid Computing (ATFMGC), the grid architecture, the basic components and relationships among them are described. At the same time, the function of agents, their knowledge representation and inference processes are also discussed. As an example, a tactical planning case study related with some Brazilian airports is illustrated.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {619–623},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026397,
author = {Haarslev, Volker and Lu, Ying and Shiri, Nematollah},
title = {ONTOXPL - Intelligent Exploration of OWL Ontologies},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The OWL ontology explorer ONTOXPL is based on the web server tomcat. Standard HTML browsers can be used to interact with ONTOXPL. It is intended to complement existing ontology editors and does not offer any editing support. ONTOXPL uses the OWL DL reasoner RACER via its extensive query interface in order to support the intelligent exploration of OWL ontologies.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {624–627},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026398,
author = {Wagner, Gerd and Antoniou, Grigoris and Tabet, Said and Boley, Harold},
title = {The Abstract Syntax of RuleML - Towards a General Web Rule Language Framework},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper discusses the approach taken by the Rule Markup Language (RuleML) Initiative towards a general Web rule language framework and relates it to the MDA and UML by the Object Management Group (OMG). It also presents the abstract syntax of RuleML 0.85 as a MOF/UML model and considers the possibility to integrate RuleML with OCL and Action Semantics.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {628–631},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026399,
author = {Chirita, Paul-. Alexandru and Olmedilla, Daniel and Nejdl, Wolfgang},
title = {Finding Related Pages Using the Link Structure of the WWW},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Most of the current algorithms for finding related pages are exclusively based on text corpora of the WWW or incorporate only authority or hub values of pages. In this paper, we present HubFinder, a new fast algorithm for finding related pages exploring the link structure of the Web graph. Its criterion for filtering output pages is "pluggable", depending on the user's interests, and may vary from global page ranks to text content, etc. We also introduce HubRank, a new ranking algorithm which gives a more complete view of page "importance" by biasing the authority measure of PageRank towards hub values of pages. Finally, we present an evaluation of these algorithms in order to prove their qualities experimentally.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {632–635},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026400,
author = {Velasquez, Juan D. and Yasuda, Hiroshi and Aoki, Terumasa},
title = {Web Site Structure and Content Recommendations},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web sites have become necessary channels for effective marketing and efficient operation for almost every company. To maintain and update a site is, however, a non-trivial task due to the complexity of the underlying business and the dynamic visitor behavior. Tools for the reconfiguration of web sites offer help for such tasks. We propose a methodology for the reconfiguration of web structure and content. Its application to the web site of a Chilean bank shows its benefits.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {636–639},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026401,
author = {Jung, Seikyung and Kim, Juntae and Herlocker, Jonathan L.},
title = {Applying Collaborative Filtering for Efficient Document Search},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the SERF (System for Electronic Recommendation Filtering) which is a collaborative filtering system that recommends context-sensitive, high-quality information sources for document search. Collaborative filtering systems remove the limitation of traditional content-based search by using individual's ratings to evaluate and recommend information sources. SERF uses collaborative filtering algorithms to predict the relevance and quality of each document with respect to each particular user and their specific information need. In our system, users specify their need in the form of a natural language query, and are provided with recommended documents based on ratings by other users with similar questions. Preliminary experiments show that the collaborative filtering recommendations increase the efficiency of the document search process. We also discuss some key challenges of designing a collaborative filtering system for document search.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {640–643},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026402,
author = {Chou, Tung-Hsiang and Chien, Chih-Cheng and Yeh, Ting-Yuan},
title = {Using the Intelligent Agent on the Telecom Electronic Commerce Website},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In recent years, the electronic commerce provided by Internet and Web is about to establish itself as a significant economic factor worldwide.For this reason, corporations have been seeking to develop a system to assist business process.More of all electronic commerce websites aim to improve and realize business process.In addition to the telecom corporation also started to provide their services on the interneet, but sometimes their services cannot communicate with the backend systems.Hence, we consider an intelligent agent approachto solve above problem.Our intelligent agent is designed by web service technology and communicated with each backend system.The methodology is also applied to a real case that involves receiving user's request to accomplish the telecomm company's services.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {644–647},
numpages = {4},
keywords = {intelligent agent, backend system, telecom corporation, e-commerce, web service},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026403,
author = {Dong, Lei and Watters, Carolyn},
title = {Improving Efficiency and Relevance Ranking in Information Retrieval},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The increasing amount of data available on the Internet has made it more important to create efficient IR (Information Retrieval) systems than ever before. However, the execution efficiency of IR tasks under a given scheme is rarely discussed in research. This paper addresses this particular issue by proposing a method that reduces the complexity in both similarity computation and ranking procedures. With the algorithms proposed, the similarity computation may be completed in linear time and the ranking may take near linear time to be finished. Also, it has the potential to improve relevancy analysis in the ranking procedure.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {648–651},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026404,
author = {Zhang, Rui and Watters, Carolyn and Duffy, Jack},
title = {Examining Table Variations on Small Screen Devices},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We are concerned with users who have already used data on a larger screen and have migrated to a smaller device and wish to continue working with that data. Earlier studies concentrated on the dynamic transformation of text content, lists, and forms embedded in web pages for access on a range of devices from desktop to handheld. In this paper, we focus on the display and manipulation of large tables on small mobile devices. We report on a project to design and evaluate a transformational model for large tables onto small screens that is both effective and efficient.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {652–655},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026405,
author = {Rao, Wenbi and Li, Wei},
title = {Design of An Open and Secure Ubiquitous Computing System},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many difficulties such as scalability and security problems are addressed in this paper, and some concluded principles are given to guide the design and implementation of a large-scale open and secure ubiquitous computing system.A servicetoolkit (aCAST) consisting of a set of software components and detector services have been developed, and a Context Aware Call Forwarding system has also been developed as a proof-of-concept test bed using aCAST. The off-the-shelf devices, such as Bluetooth mobile phones and PDAs, is used as user devices and the Session Initiation Protocol (SIP) is adopted as an open communication protocol.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {656–659},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026406,
author = {Wu, Yonghui},
title = {Normalization Design of XML Database Schema for Eliminating Redundant Schemas and Satisfying Lossless Join},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Normalization design of XML database schema is to produce a set of XML schemas or DTDs that can well represent data dependencies and eliminate redundancies. In the current researches on normalization design of XML database schema, redundancies in XML database schema are not studied specially and classified, and normalization design algorithms are only converting an initial schema into one in one of normal forms proposed in these researches. The paper defines hierarchical schema representing XML database schema and corresponding normal forms - first normal form (INF) and second normal form (2NF) for XML database schema, and presents the algorithm eliminating redundant schemas and normalization design algorithm for 2NF.In XML database schema in 1NF, the set of full and embedded MVDs are implied by the given set of MVDs. XML database schema in 2NF satifies properties for 1NF, eliminates reduant schemas, and satifies lossless join property.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {660–663},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026407,
author = {Perkio, Jukka and Buntine, Wray and Perttu, Sami},
title = {Exploring Independent Trends in a Topic-Based Search Engine},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Topic-based search engines are an alternative to simple keyword search engines that are common in today's intranets. The temporal behaviour of the topics in a topic model based search engine can be used for trend analysis, which is an important research goal on its own. We apply topic modelling to an online financial newspaper data and show that some of the trends in the topics are consistent with common understanding.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {664–668},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026408,
author = {Ye, Shiren and Chua, Tat-Seng},
title = {Detecting and Partitioning Data Objects in Complex Web Pages},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents an automated approach to detect and partition data objects or product description from complex Web pages. First, we derive the common page structure by comparing similar pages, and then identify data region covering the descriptions of data objects. Second, we partition the nodes belonging to different data objects in the data region and construct the self-explainable XML output files. The experiments indicate that our technique is effective.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {669–672},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026409,
author = {Murata, Tsuyoshi},
title = {Discovery of User Communities from Web Audience Measurement Data},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As the research of Web structure mining, several attempts have been made for discovering group of related Web pages (Web communities) such as Kumar's trawling and Flake's method. There are groups of users who watch such related Web pages, and discovering such groups (user communities) is important for clarifying the behaviors of the users of similar tastes. Moreover, it is expected that the characteristics of user communities in the Web correspond to that in real human societies. A method for discovering user communities is described in this paper. Client-level log data (Web audience measurement data) is used as the data of users' Web watching behaviors. Maximal complete bipartite graphs are searched from the graph obtained from the log data without analyzing the contents of Web pages. Experimental results show that our method succeeds in discovering many interesting user communities with labels that characterize the communities.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {673–676},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026410,
author = {Han, Jianchao and Cercone, Nick and Hu, Xiaohua},
title = {A Weighted Freshness Metric for Maintaining Search Engine Local Repository},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Current search engines maintain a local repository to improve the search efficiency. A crawler is used to periodically poll the remote web pages to update the contents of the local repository. Due to the resource limitations, some local pages may be stale. To maintain the high freshness of the repository, the crawler is expected to revisit remote web pages in optimized order and frequency. The intuitive metric of freshness of the local repository is defined as the fraction of up-to-date web pages in the repository, which is merely based on the repository content, and does not, unfortunately, reflect the perspective of the search engine users, e.g., how often is a web page queried? We propose a novel weighted metric of the repository freshness with the importance of web pages being the weights. This metric not only takes into account the local web pages themselves but also the perspectives of the search engine users. We study the repository synchronization policy under this new metric, compare this metric with others, analyze its features, and discuss how the web page importance is determined.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {677–680},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026411,
author = {Still, Kaisa and Isomursu, Minna and Still, Johanna and Isomursu, Pekka},
title = {The Role of Technology in Interest-Based Communities: Interactivity, Immersion and Connectivity},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper explores the creation and distribution of knowledge in communities that have emerged through a shared interest or goal. The focus is on the technology used for supporting knowledge creation and distribution. We examine the problem area through three case studies: birdwatchers, virtual stables and ice-hockey fans. As a result, we present an analysis of issues concerning knowledge creation and distribution from the viewpoints of interactivity, immersion, and connectivity.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {681–685},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026412,
author = {Yang, Jie and Wang, Lei and Zhang, Song and Sui, Xin and Zhang, Ning and Xu, Zhuoqun},
title = {Building Domain Ontology Based on Web Data and Generic Ontology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The automatic or semi-automatic construction of ontology has become a research topic of interest in recent years. This paper describes a mechanism for constructing domain specific ontologies automatically based on web data and generic ontology.Firstly, we employ the hierarchical agglomerative clustering(HAC) algorithm, clustering web pages hierarchically and resulting in a binary tree.Then an algorithm is proposed, which selectd from the binary tree the significative nodes as topics implying concepts of domain interests.Lastly, the Chinese generic ontology, HowNet, is introduced to evolve the topics (together with their hierarchical structures) into domain ontology.We experiment our method in the field of computer hardware based on web pages collected from Chinese BtoC web sites.An in-depth discussion on the experiment results is also given.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {686–689},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026413,
author = {Boella, Guido and Torre, Leendert van der},
title = {Local vs Global Policies and Centralized vs Decentralized Control in Virtual Communities of Agents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We are interested in the design of policies for virtual communities of agents based on the grid infrastructure. In a virtual community agents can play both the role of resource consumers and the role of resource providers, and they remain in control of their resources. We argue that this requirement creates a distinction between two dimensions: global vs local and centralized and decentralized control by means of policies. The providers should be enabled to specify their local policies on their own resources, but their policies should be consistent with the global policies. At the same time, some aspects of the decentralized control should be delegated to specialized providers; this delegation requires a distinction between the authorization to access a resource and a permission to do so.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {690–693},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026414,
author = {Zheng, Yanlin and Ogata, Hiroaki and Li, Luyi and Yano, Yoneo},
title = {Using Knowledge Awareness Support Learning Services Providing in E-Learning Environment},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The distribution of all kinds of resources in e-learning environment requires more conscious learning services for learner's personal or collaborative learning.Satisfying personalized needs on knowledge contents and recommending the most potential knowledge collaborators for effective collaboration are regarded as the two most significant learning services.This paper proposes to use Knowledge Awareness (KA) to support learning services providing focused on Personalization and Collaboration in e-learning space.This paper presents a three-dimensional KA model, illustrates a KA-supported learning services providing mechanism, and also provides a simple case study.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {694–697},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026415,
author = {An, Jiyuan and Chen, Yi-Ping Phoebe},
title = {Concept Learning of Text Documents},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Concept learning of text documents can be viewed as the problem of acquiring the definition of a general category of documents. To definite the category of a text document, the Conjunctive of keywords is usually be used. These keywords should be fewer and comprehensible. A na\"{\i}ve method is enumerating all combinations of keywords to extract suitable ones. However, because of the enormous number of keyword combinations, it is impossible to extract the most relevant keywords to describe the categories of documents by enumerating all possible combinations of keywords. Many heuristic methods are proposed, such as GA-base, immune based algorithm. In this work, we introduce pruning power technique and propose a robust enumeration-based concept learning algorithm. Experimental results show that the rules produce by our approach has more comprehensible and simplicity than by other methods.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {698–701},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026416,
author = {Zhang, Ning and Zhang, Song and Wang, Lei and Yang, Jie and Xu, Zhuoqun},
title = {Offer Group Generation and Delayed Processing in Multi-Issue Negotiation},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we describe a mechanism for automated negotiation in which artificial agents can deal with a multi-issue negotiation in a one-to-many way based on some strategies. The negotiation process are governed dynamically by some strategies that emphasize on the diverse processing methods under the considering of different commodity attributes, including dynamic delayed evaluation and response to the received offers, generation of offer groupwith the same utility. Our analysis shows that the mechanism ensures that bargaining agents could reach a better agreement automatically. We also implement a prototype system to verify the advantages of our mechanism.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {702–705},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026417,
author = {Pimentao, Joao Paulo and Sousa, Pedro A. C. and Amaral, Pedro and Steiger-Garcao, Adolfo},
title = {A Multi-Agent System's Approach to Communication Security in the Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the Split and Merge method, an approach to provide secure communication over the Internet using software agents. Instead of relying exclusively on the traditional approaches of ciphering the message at the source and then sending it over a communication channel, the Split and Merge method focuses on denying access to the message itself. This is achieved by splitting the message in parts and sending them to the destination through different routes, in an ad-hoc network of agents. The method is detailed in the paper and the current implementation using multi agent systems is presented. The solutions found to deal with error detection and correction (loss of message fragments, message integrity and node misbehavior) are then discussed and further research directions are presented.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {706–710},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026419,
author = {Silaghi, Marius Calin},
title = {Meeting Scheduling Guaranteeing n/2-Privacy and Resistant to Statistical Analysis (Applicable to Any DisCSP)},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Distributed problems raise privacy issues. The user would like to specify securely his constraints (desires, availability, money) on his computer once. The computer is expected to compute and communicate for searching an acceptable solution while maintaining the privacy of the user. Even without computers infested with spy viruses that capture the interaction with the user, most agent based approaches reveal parts of one agent's secret data to its partners in distributed computations [Using privacy loss to guide decisions in distributed CSP search]. Some cryptographic multi-party computation protocols [Completeness theorems for non-cryptographic fault-tolerant distributed computating] succeed to avoid leaking secrets at the computation of some functions with private inputs. They have been applied to find the set of all solutions for the meeting scheduling problem [On securely scheduling a meeting]. However, nobody yet succeeded to apply those techniques for finding a random solution to the meeting scheduling problem. Note that revealing all solutions, when you only need a single one, leaks a lot of data about when others are, or are not, available. Some answers were proposed in our previous approaches to distributed constraint problems [Solving a distributed CSP with cryptographic multi-party computations, without revealing constraints and without involving trusted servers]. They guarantee that no agent can infer with certitudea secret from the identity of the solution of the problem (other than the acceptance of the solution), but guarantee nothing about inference of probabilistic information about secrets. Our new technique answers this problem, too.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {711–715},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026418,
author = {Oikonomopoulou, Diamanto and Rigou, Maria and Sirmakessis, Spiros and Tsakalidis, Athanasios},
title = {Full-Coverage Web Prediction Based on Web Usage Mining and Site Topology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Understanding and modeling user online behavior, as well as predicting future requests remain an open challenge for researchers, analysts and marketers. In this paper, we propose an efficient prediction schema based on the extraction of sequential navigation patterns from server log files, combined with web site topology. Traversed paths are monitored, internally recorded and cleaned before being completed with cashed page views. After session and episode identification follows the construction of n-grams. Prediction is based upon a 5 + n-gram schema with all lower level n-grams participating, a procedure that resembles the construction of an All 5th-order Markov Model. The schema achieves full coverage while maintaining competitive prediction precision.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {716–719},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026420,
author = {Habegger, Benjamin and Quafafou, Mohamed},
title = {Context Generalization for Information Extraction from the Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many online data sources, such as product catalogs, on-line directories, etc. are available on the web. Extracting information from such sources is a hard task since these sources are designed to be presented to human users. Many researchers have tackled the problem of building wrappers for such sources. The state of the art approach is to use machine learning techniques based on fully labeled example pages. In this paper we propose and study an approach based on example instances. This allows the user to build a wrapper using only a handful of examples of the whole source allowing to take into account structural differences. The patterns obtained allow to extract the instances of the relation described by the examples and contained in the same data source.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {720–723},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026421,
author = {Pu, Pearl and Faltings, Boi and Torrens, Marc},
title = {Effective Interaction Principles for Online Product Search Environments},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {To find products in online environments, people increasingly rely on computerized search tools.The performance of such tools depends crucially on an accurate model of their users' preferences.Obtaining such models requires an adequate interaction model and system guidance.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {724–727},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026422,
author = {Callaghan, M. J. and El-Gueddari, M. and Harkin, J. and McGinnity, T. M. and Maguire, L. P.},
title = {Distributed Architecture for Adaptive Intelligent Environments},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Increasingly web based distance education engineering courses are on offer, augmented by the provision of remote experimentation laboratories facilitating distant access to campus based physical resources. The design and implementation of effective and usable remote experimentation facilities poses unique challenges given the inherent complexities of the learning environment and the constraints imposed by the delivery medium. Developments in recent years have addressed many of these issues. However autonomous learning environments by their very nature offer minimal educator assistance and from a students perspective it is inevitable that at some stage of the experimental process, context specific help will be required. This paper seeks to address this issue in the context of remote experimentation for embedded systems and presents a distributed communications application implemented using a web services/.NET Remoting framework for an adaptive intelligent learning environment.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {728–731},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026423,
author = {Liu, Hongyu and Milios, Evangelos and Janssen, Jeannette},
title = {Focused Crawling by Learning HMM from User's Topic-Specific Browsing},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A focused crawler is designed to traverse the Web to gather documents on a specific topic. It is not an easy task to predict which links lead to good pages. In this paper, we present a new approach for prediction of the important links to relevant pages based on a learned user model. In particular, we first collect pages that a user visits during a learning session, where the user browses the Web and specifically marks which pages she is interested in. We then examine the semantic content of these pages to construct a concept graph, which is used to learn the dominant content and link structure leading to target pages using a Hidden Markov Model (HMM). Experiments show that with learned HMM from a user's browsing, the crawling performs better than Best-First strategy.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {732},
numpages = {1},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026424,
author = {Basili, Roberto and Vindigni, Michele and Zanzotto, Fabio Massimo},
title = {Understanding the Web through Its Language},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A tight integration between ontological and linguistic knowledge is critical within the information processes of the Semantic Web. In Information Extraction, ontologies should include knowledge components neglected in domain conceptualizations generally used for other tasks. In this paper, we analyze such critical information in the light of existing applications. Accordingly, a methodology for semi-automatic development of an IE ontology integrating pre-existing domain and lexical knowledge is presented. The proposed ontological framework supports the discovery of new relations among known concepts by means of text processing, but also induction of new conceptual information.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {736–739},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026425,
author = {Wu, Lei and Sahraoui, Houari},
title = {Supporting Web Collaboration for Cooperative Software Development},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the rapid growth of web technology, more and more software development projects use web collaborations to facilitate the development process. However, web collaboration activity is a complex orchestration. It involves many people work together without the barrier of time and space difference. Therefore, how to efficiently monitor and control web collaboration activity becomes a critical issue in a web-based collaborative software development project. In this paper, we present a novel approach to tackle this difficult problem by means of monitoring collaboration task progress. In addition, we also provide solutions to automate the dynamic control of cooperation, thus to improve the web collaboration performance.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {740–743},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026426,
author = {Wang, J. R. and Parameswaran, N.},
title = {Intelligent Streaming Video Data over the Web},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Delivering video over the Web has posted a big problem. Video files are extremely big and need hours or even tens of hours to download. The available bandwidth varies dynamically. The conventional proactive buffering approach used by current video players cannot fully satisfy user needs. This paper presents a system designed to allow efficient retrieving, browsing and real-time playing of videos through the Internet using a web browser. The scheme is domain-specific.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {744–747},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026427,
author = {Lam, Chak-Man and Zhang, Xiao-Feng and Cheung, William K.},
title = {Mining Local Data Sources For Learning Global Cluster Models},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Distributed data mining has been a topic getting more important nowadays as there are many cases where physically sharing of data is probibited, e.g., due to huge data volume or data privacy. In this paper, we are interested in learning a global cluster model by exploring data in distributed sources. A methodology based on periodic model exchange and merge is proposed and applied to hyperlinked Web pages analysis. In addition, we have tested a number of variations of the basic idea, including putting more emphasis on the privacy concern and testing the effect of having different numbers of distributed sources. Experimental results show that the proposed distributed learning scheme is effective with accuracy close to the case with all the data physically shared for the learning.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {748–751},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026428,
author = {Buehrer, Daniel J. and Chun-Yao, Wang},
title = {Using a Class Algebra Ontology To Define Conversions between OWL/SQL/Java Beans},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes the xml definition of class algebra which is available at http://xbean.cs.ccu.edu.tw/~dan/classAlgebra.xml . A sample user-defined ontology document and instance document are available at http://xbean.cs.ccu.edu.tw/~dan/userOntology.xml and http://xbean.cs.ccu.edu.tw/~dan/userInstances.xml . The class algebra ontology is very similar to the OWL ontology, but it uses its own definition of pointers for non-partOf relations. This simplifies the underlying theory as well as the syntax. The same syntax is used to define the ontology (i.e. the schema) as well as instance documents. The class algebra ontology has sufficient information to enable conversion between class algebra instance documents, SQL tables, and Java persistent objects, all of which can be queried by class algebra queries and updated by class algebra assignment operators, RMI calls, or SOAP method calls. The state-space graph of all possible orderings of class algebra operators is searchable by efficient constraint-based search techniques. Operators include guarded class algebra assignments as well as traditional SOAP or RMI method calls.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {752–754},
numpages = {3},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026429,
author = {Baldini, Nicola and Gori, Marco and Maggini, Marco},
title = {MumbleSearch Extraction of High Quality Web Information for SME},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Although search engines are playing a crucial role for the retrieval of information from the Web, they cannot guarantee the quality required for most relevant business activities as well as for many top-level research projects. In this paper we present MumbleSearch, a Web Content Monitor which is especially conceived to extract and organize topic-based information with emphasis on quality requirements. We present the architecture of the software platform and its deployment for a real-world application, involving Italian Small and Medium Enterprises (SME).},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {757–760},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026430,
author = {Liu, Yang and Huang, Xiangji and An, Aijun and Promhouse, Gary},
title = {Clustering Web Surfers with Probabilistic Models in a Real Application},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The problem of modeling and predicting a Web surfer's browsing patterns has gained increasing attention in recent years. In this paper we present our experience in clustering Web surfers using a mixture of Markov models with a real application of Livelink log data. We propose different techniques to improve the clustering performance, and evaluate the techniques through experiments.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {761–765},
numpages = {5},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026431,
author = {Noah, Shahrul Azman and Zakaria, Lailatulqadri and Alhadi, Arifah Che and Sembok, Tengku Mohd Tengku and Saad, Saidah},
title = {Towards Building Semantic Rich Model for Web Documents Using Domain Ontology},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Accessing and extracting semantic meanings from web documents is crucial for the realization of Semantic Web. While the web offers the flexibility of making information easily available, it is considerably hard to find a fruitful way to describe, classify and present this information with rich semantic content. Therefore, the semantic information content of web documents need to be specified in order to make the tangled information more accessible to search engines and other applications. In this paper we propose an approach meant to assist in constructing semantic document models using natural language analysis technique and a domain specific ontology.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {769–770},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026432,
author = {Tan, Saravadee Sae and Hoon, Gan Keng and Kong, Tang Enya and Lin, Cheong Sook and Lin, Chan Siew and Ying, Foo Wen},
title = {MICE: Aggregating and Classifying Meta Search Results into Self-Customized Categories},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Having broad coverage of search results returned by various search sources, combining and organizing these results in a meaningful way has become a common issue in the field of information retrieval. In this demo paper, we describe our meta search system, MICE, that is able to aggregate and classify search results based on user-customized categories. Categories help user to focus on search results, with respect to the categories concept customized by the user.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {771–772},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026433,
author = {Callaghan, M. J. and El-Gueddari, M. and Harkin, J. and McGinnity, T. M. and Maguire, L. P.},
title = {Intelligent Remote Experimentation},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Constant innovation and product evolution in the area of embedded systems necessitates educational institutions and other training providers to continuingly reassess the content and delivery of engineering curricula. Increasingly web based distance education courses are on offer, augmented by the provision of remote experimentation laboratories facilitating distant access to campus based physical resources. The design and implementation of effective and usable remote experimentation facilities poses unique challenges given the inherent complexities of the learning environment and the constraints imposed by the delivery medium. Developments in recent years have addressed many of these issues. However autonomous learning environments by their very nature offer minimal educator assistance. This paper seeks to address this issue in the context of remote experimentation for embedded systems and demonstrates a distributed communications application implemented using a web services/.NET Remoting framework for an adaptive intelligent learning environment for remote experimentation.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {773–774},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026434,
author = {Huang, Jiajin and Zhong, Ning and Liu, Chunnian and Yao, Y. Y. and Qiu, Dejun and Ou, Chuangxin},
title = {TMS: Targeted Marketing System Based on Market Value Functions},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {TMS (Targeted Marketing System) is an integrated system and toolkit for profit-driven and cost-effective marketing. The system consists of three components: a Web-based user interface, a market value inference engine, and a presentation and evaluation module. It supports marketing decision making for a company or an organization by combining results from information retrieval, data mining, information theory, and utility theory.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {775–776},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026435,
author = {Hu, Jia and Zhong, Ning},
title = {Organizing Dynamic Multi-Level Workflows on Multi-Layer Grids for Developing e-Business Portals},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Developing an intelligent e-Business portal needs various WI technologies. In this demonstration, we present a conceptual model with dynamic multi-level workflows corresponding to a multi-layer Grid architecture for multi-aspect analysis in building an e-Business portal on the Wisdom Web, and for dynamically organizing status-based business processes that governs enterprise application integration.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {777–778},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026436,
author = {Milani, Alfredo and Suriani, Silvia},
title = {ADAN: Adaptive Newspapers Based on Evolutionary Programming},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This work presents AdaN an online adaptive newspaper system based on evolutionary programming. Online adaptive newspapers provide flexible services and news to a mass of clients/users for maximizing some system goals, they dynamically adapt the form and the content of the newspaper while the population of clients evolve over time. The techniques of online evolutionary programming (or online GAs) used in ADAN exploits the online clients response behavior as a fitness function in order to produce the next generation of services. The principle implemented in online GAs, "the application environment is the fitness", allow to model highly evolutionary domains where both services providers and clients change and evolve over time. The flexibility and the adaptive behavior of this approach seems to be very relevant and promising for applications characterized by highly dynamical features such as in the web domain (online newspapers, e-markets, websites and advertising engines).},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {779–780},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026437,
author = {Ichalkaranje, Nikhil},
title = {Intelligent Agents and Their Applications WIC Australia Centre Report 2002-2004},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper attempts to encapsulate the activities for the year 2002-2004 of the Web Intelligence Consortium (WIC) Australia Centre, an affiliated Centre of the WIC international organization.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {783–784},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026438,
author = {Liu, Chunnian},
title = {A Report of Web Intelligence Research at the WIC Beijing Center},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents some Web Intelligence related systems developed at the WIC Beijing Research Center, such as Web-Based Intelligent Tutoring System, Recommended System in e-Business and Web-Text Mining System. In addition, we also introduce our other related work including Constraint Inductive Logic Programming (CILP), Genetic Inductive Logic Programming (GILP), association rules mining and Bayesian network learning.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {785–786},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026293,
author = {Yao, Yiyu and Yao, JingTao and Butz, Cory and Lingras, Pawan and Jutla, Dawn},
title = {Web-Based Support Systems (WSS): A Report of the WIC Canada Research Centre},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {787–788},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026439,
author = {Morizet-Mahoudeaux, Pierre},
title = {WIC France Research Center Activity Report},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The research activity of the Heudiasyc laboratory in the domains related to WIC concerns four projects : (i) intelligent agents, (ii) e-learning, topic maps, and semantic Web, (iii) Web topology, and (iv) digital music preservation.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {789–790},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.1109/WI.2004.1,
author = {Liu, Jiming and Cheung, William K.},
title = {A Driving Force for E-Transformation - The Centre for e-Transformation Research / WIC Hong Kong Centre},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2004.1},
doi = {10.1109/WI.2004.1},
abstract = {The Centre for e-Transformation Research (CTR), also an affiliated Centre of Web Intelligence Consortium (WIC), is established under the Science Faculty of Hong Kong Baptist University, currently funded by Hong Kong Research Grant Council Central Allocation and FRG Strategic Research Grant, Hong Kong Baptist University for developing an Area of Strength in e-transformation research, making high impact to various sectors of the society, from e-business, e-learning, to e-government, to name a few.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {791–792},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026441,
author = {Narayan, B. L. and Pal, Sankar K.},
title = {A Report of Activities at the WIC-India Research Center},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The research activities of the WIC-India Research Center include topics like improving the performance of search engines, link and neighborhood analysis, as well as, surfer modeling for ranking and categorization of web pages, and query answering. In solving several web mining tasks, both statistical and soft computing tools are employed. Besides, some international collaborations and establishing a center of excellence are going on.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {793–794},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026442,
author = {Zhong, Ning},
title = {Using WI Technologies to Develop Intelligent Portals - Research Activities at the WIC Japan Center -},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The objective of the WIC Japan Research Centre is to carry out basic research concerning certain aspects of Web Intelligence (WI). Our research activities focus on investigating WI technologies for developing various portals that enable intelligence for e-science, e-business, e-government, and e-learning, as well as deal with the scalability and complexity of real world, efficiently and effectively. We observe that developing intelligent portals is one of the most sophisticated applications, which needs to be supported by WI technologies. Research work that has been carried out can be categorized and described as follows.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {795–796},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026443,
author = {Choi, Joongmin},
title = {Research Activities at the WIC Korea Center},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The WIC Korea Center is organized with the goal of achieving collaboration among Korean researchers in various fields related to Web intelligence including Web information retrieval and extraction, Semantic Web, artificial intelligence, ubiquitous computing, and data mining.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {797–798},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026444,
author = {Favela, Jesus and Montes, Manuel and Chavez, Edgar},
title = {Web Intelligence in Mexico},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The Mexico Research Centre of the Web Intelligence Consortium was established in 2003 motivated by Mexico being selected as the host of the 2nd Atlantic Web Intelligence Conference. It currently has 18 members including faculty and doctoral students from 7 different institutions. The WIC-Mexico includes groups working in the areas of Intelligent Web Information Retrieval, Web Mining and Farming, Knowledge Management, and Agents in Ubiquitous Computing.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {799–800},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026445,
author = {Szczepaniak, Piotr S.},
title = {Web Intelligence Research - Activity within the Polish Center},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this report, the activity area of the "WIC-Poland Research Center" is presented. The Center is established in the Institute of Computer Science of the Technical University of Lodz, and the research work of its members is focused on the intelligent exploration of the Web.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {801–804},
numpages = {4},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026446,
author = {Herrero, Pilar and Perez, Maria S. and Menasalvas, Ernestina and Segovia, Javier},
title = {A Report of Activities at the WIC-Spain Research Centre},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {805–806},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026447,
author = {Wang, Frank Zhigang and Jiang, Sheng and Yip, Yau Jim},
title = {Web Intelligence Research Activities in the UK},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The Web Intelligence Consortium (WIC) UK Centre has been founded in 2003 as an affiliated Centre of the WIC international organization.Since its foundation, the WIC UK Centre has devoted itself to developing and disseminating Web Intelligence (WI) technologies with a focus on Web Services powered by Grid Computing.The Centre has become a well organized and active group of strong academics and industrial practitioners, playing an important role in the international WI research.A number of activities have been carried out and planned by the WIC UK Centre.},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {807–808},
numpages = {2},
series = {WI '04}
}

@inproceedings{10.5555/1025132.1026448,
title = {Author Index},
year = {2004},
isbn = {0769521002},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2004 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {809–812},
numpages = {4},
series = {WI '04}
}

