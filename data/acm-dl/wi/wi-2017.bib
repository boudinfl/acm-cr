@inproceedings{10.1145/3106426.3109448,
author = {Sheth, Amit and Perera, Sujan and Wijeratne, Sanjaya and Thirunarayan, Krishnaprasad},
title = {Knowledge Will Propel Machine Understanding of Content: Extrapolating from Current Examples},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109448},
doi = {10.1145/3106426.3109448},
abstract = {Machine Learning has been a big success story during the AI resurgence. One particular stand out success relates to learning from a massive amount of data. In spite of early assertions of the unreasonable effectiveness of data, there is increasing recognition for utilizing knowledge whenever it is available or can be created purposefully. In this paper, we discuss the indispensable role of knowledge for deeper understanding of content where (i) large amounts of training data are unavailable, (ii) the objects to be recognized are complex, (e.g., implicit entities and highly subjective content), and (iii) applications need to use complementary or related data in multiple modalities/media. What brings us to the cusp of rapid progress is our ability to (a) create relevant and reliable knowledge and (b) carefully exploit knowledge to enhance ML/NLP techniques. Using diverse examples, we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data and continued incorporation of knowledge in learning techniques.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1–9},
numpages = {9},
keywords = {implicit entity recognition, personalized digital health, knowledge-enhanced machine learning, knowledge-enhanced NLP, emoji sense disambiguation, understanding complex text, multimodal exploitation, knowledge-driven deep content understanding, semantic-cognitive-perceptual computing, machine intelligence},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106455,
author = {Fushimi, Takayasu and Satoh, Tetsuji},
title = {Constructing and Visualizing Topic Forests for Text Streams},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106455},
doi = {10.1145/3106426.3106455},
abstract = {A great deal of such texts as news and blog articles, web pages, and scientific literature are posted on the web as time goes by, and are generally called time-series documents or text streams. For each document, some strongly or weakly relevant texts exist. Although such relevance is represented as citations among scientific literatures, trackback among blog articles, hyperlinks among Wikipedia articles or web pages and so on, the relevance among news articles is not always clearly specified. One easy way to build a similarity network is by calculating the similarity among news articles and making links among similar articles; however, adding information about the posted times of articles to a similarity network is difficult. To overcome this problem, we propose a framework that consists of two parts: 1) tree structures called Topic Forests and 2) their visualization. Topic Forests are constructed by semantically and temporally linking cohesive texts while preserving their posted order. We provide effective access for users to text streams by embedding Topic Forests over the polar coordinates with a technique called Polar Coordinate Embedding. From experimental evaluations using the actual text streams of news articles, we confirm that Topic Forests semantically and temporally maintain cohesiveness, and Polar Coordinate Embedding achieves effective accessibility.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {10–17},
numpages = {8},
keywords = {tree structure, visualization, text stream},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106427,
author = {Fabian, Benjamin and Ermakova, Tatiana and Lentz, Tino},
title = {Large-Scale Readability Analysis of Privacy Policies},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106427},
doi = {10.1145/3106426.3106427},
abstract = {Online privacy policies notify users of a Website how their personal information is collected, processed and stored. Against the background of rising privacy concerns, privacy policies seem to represent an influential instrument for increasing customer trust and loyalty. However, in practice, consumers seem to actually read privacy policies only in rare cases, possibly reflecting the common assumption stating that policies are hard to comprehend. By designing and implementing an automated extraction and readability analysis toolset that embodies a diversity of established readability measures, we present the first large-scale study that provides current empirical evidence on the readability of nearly 50,000 privacy policies of popular English-speaking Websites. The results empirically confirm that on average, current privacy policies are still hard to read. Furthermore, this study presents new theoretical insights for readability research, in particular, to what extent practical readability measures are correlated. Specifically, it shows the redundancy of several well-established readability metrics such as SMOG, RIX, LIX, GFI, FKG, ARI, and FRES, thus easing future choice making processes and comparisons between readability studies, as well as calling for research towards a readability measures framework. Moreover, a more sophisticated privacy policy extractor and analyzer as well as a solid policy text corpus for further research are provided.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {18–25},
numpages = {8},
keywords = {readability, user experience, privacy},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106452,
author = {Neto, Joaquim A. M. and Yokoyama, Kazuki M. and Becker, Karin},
title = {Studying Toxic Behavior Influence and Player Chat in an Online Video Game},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106452},
doi = {10.1145/3106426.3106452},
abstract = {Many online collaborative games, e-sports in particular, heavily rely on teamwork. However, players can act in an antisocial way during the match, creating dissent into the match. This kind of behavior is referred to as toxic. We aim to discover the influence brought by toxic behavior in a popular e-sport, League of Legends, through the study of communication patterns of players during the match. We discovered that different communication patterns exist, and that they are directly related to player performance and level of toxic behavior. We also propose metrics to analyze players' performance and the toxic contamination level, which measures the negative impacts of the toxic behavior. Our analysis contributes to shed light on how players behave in an online game, and opens ways to provide a better ambience on the online video game community.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {26–33},
numpages = {8},
keywords = {online games, league of legends, text mining, toxic behavior},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106498,
author = {Mauro, Noemi and Ardissono, Liliana and Savoca, Adriano},
title = {Concept-Aware Geographic Information Retrieval},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106498},
doi = {10.1145/3106426.3106498},
abstract = {Textual queries are largely employed in information retrieval to let users specify search goals in a natural way. However, differences in user and system terminologies can challenge the identification of the user's information needs, and thus the generation of relevant results. We argue that the explicit management of ontological knowledge, and of the meaning of concepts (by integrating linguistic and encyclopaedic knowledge in the system ontology), can improve the analysis of search queries, because it enables a flexible identification of the topics the user is searching for, regardless of the adopted vocabulary. This paper proposes an information retrieval support model based on semantic concept identification. Starting from the recognition of the ontology concepts that the search query refers to, this model exploits the qualifiers specified in the query to select information items on the basis of possibly fine-grained features. Moreover, it supports query expansion and reformulation by suggesting the exploration of semantically similar concepts, as well as of concepts related to those referred in the query through thematic relations. A test on a data-set collected using the OnToMap Participatory GIS has shown that this approach provides accurate results.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {34–41},
numpages = {8},
keywords = {information search, linked data, participatory GIS, ontologies},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106499,
author = {Goel, Deepti and Chaudhury, Santanu and Ghosh, Hiranmay},
title = {An IoT Approach for Context-Aware Smart Traffic Management Using Ontology},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106499},
doi = {10.1145/3106426.3106499},
abstract = {This paper exhibits a novel context-aware service framework for IoT based Smart Traffic Management using ontology to regulate smooth traffic flow in smart cities by analyzing real-time traffic environment. The proposed approach makes smarter use of transport networks to achieve objectives related to performance of transport system. This requires efficient traffic planning measures which relate to the actions designed to adjust the demand and capacity of the network in time and space by use of IoT technologies. The adoption of sensors and IoT devices in Smart Traffic System helps to capture the user's preferences and context information which can be in the form of travel time, weather conditions or real-life driving patterns. We have employed multimedia ontology to derive higher level descriptions of traffic conditions and vehicles from perceptual observation of traffic information which provides important grounds for our proposed IoT framework. The multimedia ontology encoded in Multimedia Web Ontology Language(MOWL) helps to define classes, properties, and structure of a possible traffic environment to provide insights across the transportation network. MOWL supports Dynamic Bayesian networks (DBN) to deal with time-series data and uncertainties linked with context observations which fits the definition of an intelligent IoT system. Thus, our proposed smart traffic framework aggregates information corresponding to traffic domain such as traffic videos captured using CCTV cameras and allows automatic prediction of dynamically changing situations which helps to make traffic authorities more responsive. We have illustrated use of our approach by utilizing contextual information, to assess real-time congestion situation on roads thus allowing to visualize planning services. Once the congestion situation is predicted, alternate congestion free routes which are in accordance with the coveted criteria are suggested that can be propagated through text-messages or e-mails to the users.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {42–49},
numpages = {8},
keywords = {IoT, context aware service, multimedia ontology, smart traffic planning, dynamic bayesian network(DBN)},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106513,
author = {Ishioka, Tsunenori and Kameda, Masayuki},
title = {Overwritable Automated Japanese Short-Answer Scoring and Support System},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106513},
doi = {10.1145/3106426.3106513},
abstract = {We have developed an automated Japanese short-answer scoring and support machine for new National Center written test exams. Our approach is based on the fact that accurate recognizing textual entailment and/or synonymy has been almost impossible for several years. The system generates automated scores on the basis of evaluation criteria or rubrics, and human raters revise them. The system determines semantic similarity between the model answers and the actual written answers as well as a certain degree of semantic identity and implication. Owing to the need for the scoring results to be classified at multiple levels, we use random forests to utilize many predictors effectively rather than use support vector machines. An experimental prototype operates as a web system on a Linux computer. We compared human scores with the automated scores for a case in which 3--6 allotment points were placed in 8 categories of a social studies test as a trial examination. The differences between the scores were within one point for 70--90 percent of the data when high semantic judgment was not needed.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {50–56},
numpages = {7},
keywords = {writing test, recognizing textual entailment, random forests, automated scoring, machine learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106428,
author = {Georgala, Kleanthi and Hoffmann, Michael and Ngomo, Axel-Cyrille Ngonga},
title = {An Evaluation of Models for Runtime Approximation in Link Discovery},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106428},
doi = {10.1145/3106426.3106428},
abstract = {Time-efficient link discovery is of central importance to implement the vision of the Semantic Web. Some of the most rapid Link Discovery approaches rely internally on planning to execute link specifications. In newer works, linear models have been used to estimate the runtime of the fastest planners. However, no other category of models has been studied for this purpose so far. In this paper, we study non-linear runtime estimation functions for runtime estimation. In particular, we study exponential and mixed models for the estimation of the runtimes of planners. To this end, we evaluate three different models for runtime on six datasets using 500 link specifications. We show that exponential and mixed models achieve better fits when trained but are only to be preferred in some cases. Our evaluation also shows that the use of better runtime approximation models has a positive impact on the overall execution of link specifications.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {57–64},
numpages = {8},
keywords = {link specifications, link discovery, taylor series, runtime approximation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106454,
author = {Bouzeghoub, Amel and Jabbour, Said and Ma, Yue and Raddaoui, Badran},
title = {Handling Conflicts in Uncertain Ontologies Using Deductive Argumentation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106454},
doi = {10.1145/3106426.3106454},
abstract = {Ontologies can represent knowledge in a structured and formally well-understood way, which is crucial for information sharing. However, in practice, it is often difficult to have an error-free ontology. Conflicts can occur due to modeling errors or ontology merging and evolution. Moreover, uncertainty can happen because of modeling choices or the lack of confidence for a constructed ontology. Argumentation frameworks for knowledge bases reasoning and management have received extensive interests in the field of Artificial Intelligence in recent years. In this paper, we propose a unified framework to handle conflicts in uncertain ontologies with the use of deductive argumentation. Different from existing approaches, we introduce a stronger notion of conflict that covers both inconsistency and incoherence, where the latter is a special contradiction that can occur in an ontology. The unified approach spreads uncertainty degrees throughout argumentation trees and the enriched argument structure leads us to two novel inference relations. We then present a method to compute (counter)-arguments as well as argumentation trees in the context of uncertain ontologies based on the developments of three notions called minimal conflicting subontologies, maximal nonconflicting subontologies, and prudent justifications.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {65–72},
numpages = {8},
keywords = {argumentation theories, ontologies, incoherence, inconsistency},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106463,
author = {Nishioka, Chifumi and Scherp, Ansgar},
title = {Keeping Linked Open Data Caches Up-to-Date by Predicting the Life-Time of RDF Triples},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106463},
doi = {10.1145/3106426.3106463},
abstract = {Many Linked Open Data applications require fresh copies of RDF data at their local repositories. Since RDF documents constantly change and those changes are not automatically propagated to the LOD applications, it is important to regularly visit the RDF documents to refresh the local copies and keep them up-to-date. For this purpose, crawling strategies determine which RDF documents should be preferentially fetched. Traditional crawling strategies rely only on how an RDF document has been modified in the past. In contrast, we predict on the triple level whether a change will occur in the future. We use the weekly snapshots of the DyLDO dataset as well as the monthly snapshots of the Wikidata dataset. First, we conduct an in-depth analysis of the life span of triples in RDF documents. Through the analysis, we identify which triples are stable and which are ephemeral. We introduce different features based on the triples and apply a simple but effective linear regression model. Second, we propose a novel crawling strategy based on the linear regression model. We conduct two experimental setups where we vary the amount of available bandwidth as well as iteratively observe the quality of the local copies over time. The results demonstrate that the novel crawling strategy outperforms the state of the art in both setups.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {73–80},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106465,
author = {Ristoski, Petar and Faralli, Stefano and Ponzetto, Simone Paolo and Paulheim, Heiko},
title = {Large-Scale Taxonomy Induction Using Entity and Word Embeddings},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106465},
doi = {10.1145/3106426.3106465},
abstract = {Taxonomies are an important ingredient of knowledge organization, and serve as a backbone for more sophisticated knowledge representations in intelligent systems, such as formal ontologies. However, building taxonomies manually is a costly endeavor, and hence, automatic methods for taxonomy induction are a good alternative to build large-scale taxonomies. In this paper, we propose TIEmb, an approach for automatic unsupervised class subsumption axiom extraction from knowledge bases using entity and text embeddings. We apply the approach on the WebIsA database, a database of subsumption relations extracted from the large portion of the World Wide Web, to extract class hierarchies in the Person and Place domain.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {81–87},
numpages = {7},
keywords = {text embeddings, ontology induction, entity embeddings},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106495,
author = {Trinh, Tuan-Dat and Aryan, Peb R. and Do, Ba-Lam and Ekaputra, Fajar J. and Kiesling, Elmar and Rauber, Andreas and Wetz, Peter and Tjoa, A Min},
title = {Linked Data Processing Provenance: Towards Transparent and Reusable Linked Data Integration},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106495},
doi = {10.1145/3106426.3106495},
abstract = {The growth of Linked Data has created a promising environment for data exploration and a growing number of tools allow users to interactively integrate data from various sources. Eliciting the reliability of the results of such ad-hoc integration processes, consistently recreating those results, and identifying changes upon re-execution, however, can be difficult. Automated process provenance trail creation can provide major benefits in this context, because (i) it enables users to trace the contribution of individual sources and processing steps to the final outcome and judge whether the result can be trusted; (ii) it ensures repeatability and raises the trustworthiness of results; (iii) it ideally enables reconstruction of Linked Data integration processes from the provenance information embedded in the final result. In this paper, we present a provenance model that facilitates automatic generation of semantic provenance information for generic Linked Data integration processes. We implement the generic model in a collaborative mashup environment and evaluate it by means of an example application. We find that the model provides a solid foundation for verifiability and contributes towards making Linked Data integration processes more open, transparent, and reusable, which is crucial in domains where the origin of data is essential, such as, for instance, statistical analyses, scientific research, and data journalism.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {88–96},
numpages = {9},
keywords = {provenance, linked data, data processing},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106484,
author = {Gao, Chuancong and Wang, Jiannan and Pei, Jian and Li, Rui and Chang, Yi},
title = {Preference-Driven Similarity Join},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106484},
doi = {10.1145/3106426.3106484},
abstract = {Similarity join, which can find similar objects (e.g., products, names, addresses) across different sources, is powerful in dealing with variety in big data, especially web data. Threshold-driven similarity join, which has been extensively studied in the past, assumes that a user is able to specify a similarity threshold, and then focuses on how to efficiently return the object pairs whose similarities pass the threshold. We argue that the assumption about a well set similarity threshold may not be valid for two reasons. The optimal thresholds for different similarity join tasks may vary a lot. Moreover, the end-to-end time spent on similarity join is likely to be dominated by a back-and-forth threshold-tuning process.In response, we propose preference-driven similarity join. The key idea is to provide several result set preferences, rather than a range of thresholds, for a user to choose from. Intuitively, a result set preference can be considered as an objective function to capture a user's preference on a similarity join result. Once a preference is chosen, we automatically compute the similarity join result optimizing the preference objective. As the proof of concept, we devise two useful preferences and propose a novel preference-driven similarity join framework coupled with effective optimization techniques. Our approaches are evaluated on four real-world web datasets from a diverse range of application scenarios. The experiments show that preference-driven similarity join can achieve high-quality results without a tedious threshold-tuning process.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {97–105},
numpages = {9},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106497,
author = {Valdestilhas, Andr\'{e} and Soru, Tommaso and Ngomo, Axel-Cyrille Ngonga},
title = {CEDAL: Time-Efficient Detection of Erroneous Links in Large-Scale Link Repositories},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106497},
doi = {10.1145/3106426.3106497},
abstract = {More than 500 million facts on the Linked Data Web are statements across knowledge bases. These links are of crucial importance for the Linked Data Web as they make a large number of tasks possible, including cross-ontology, question answering and federated queries. However, a large number of these links are erroneous and can thus lead to these applications producing absurd results. We present a time-efficient and complete approach for the detection of erroneous links for properties that are transitive. To this end, we make use of the semantics of URIs on the Data Web and combine it with an efficient graph partitioning algorithm. We then apply our algorithm to the LinkLion repository and show that we can analyze 19,200,114 links in 4.6 minutes. Our results show that at least 13% of the owl :sameAs links we considered are erroneous. In addition, our analysis of the provenance of links allows discovering agents and knowledge bases that commonly display poor linking. Our algorithm can be easily executed in parallel and on a GPU. We show that these implementations are up to two orders of magnitude faster than classical reasoners and a non-parallel implementation.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {106–113},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106529,
author = {Nema, Waleed and Tang, Yinshan},
title = {Consensus-Based Ranking of Wikipedia Topics},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106529},
doi = {10.1145/3106426.3106529},
abstract = {To improve the effectiveness of users' information seeking experience in interactive web search we hypothesize how people might be influenced when making relevance judgment decisions by introducing the <u>C</u>onsensus <u>T</u>heory <u>&amp;</u> Relevance Judgment <u>M</u>odel (CT&amp;M). This is combined with a practical path to assess the extent of difference between suggestions of current search engines versus user expectations. A user-centered, evidence-based, phenomenology approach is used to improve on Google PageRank (GPR) in two ways. The first by biasing GPR's equal navigation probability assumption using (f)actual usage stats as implicit user consensus which leads to the StatsRank (SR) algorithm. Secondly, we aggregate users' explicit ranking to derive Consensus Rank (CR) which is shown to predict individual user ranking significantly better than GPR and meta-search of modern search engines Google and Yahoo/Bing real-time.CT&amp;M contextualizes CR, SR, and a live open online web experiment, called <u>The Ranking Game</u>, which is based on the August-2016 English Wikipedia corpus (12.7 million pages) and Page View Statistics for May to July 2016. Limiting this work to Wikipedia makes GPR topic-based since any Wikipedia page is focused on one topic. TREC's pooling is used to merge top 20 results from major search engines and present an alphabetized list for users' explicit ranking via drag and drop. The same platform captures implicit data for future research and can be used for controlled experiments.Our contributions are: CT&amp;M, SR, CR, and the open online user feedback web experiment research platform.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {114–124},
numpages = {11},
keywords = {consensus ranking, google pagerank, explicit relevance feedback, information seeking, wikipedia, information retrieval, interactive web search, implicit feedback},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106514,
author = {Ermilov, Timofey and Moussallem, Diego and Usbeck, Ricardo and Ngomo, Axel-Cyrille Ngonga},
title = {GENESIS: A Generic RDF Data Access Interface},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106514},
doi = {10.1145/3106426.3106514},
abstract = {The availability of billions of facts represented in RDF on the Web provides novel opportunities for data discovery and access. In particular, keyword search and question answering approaches enable even lay people to access this data. However, the interpretation of the results of these systems, as well as the navigation through these results, remains challenging. In this paper, we present Genesis, a generic RDF data access interface. Genesis can be deployed on top of any knowledge base and search engine with minimal effort and allows for the representation of RDF data in a layperson-friendly way. This is facilitated by the modular architecture for reusable components underlying our framework. Currently, these include a generic search back-end, together with corresponding interactive user interface components based on a service for similar and related entities as well as verbalization services to bridge between RDF and natural language.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {125–131},
numpages = {7},
keywords = {knowledge extraction, semantic web, summarization, semantic user interface, similarity},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106496,
author = {Duarte, Julio Cesar and Cavalcanti, Maria Claudia Reis and de Souza Costa, Igor and Esteves, Diego},
title = {An Interoperable Service for the Provenance of Machine Learning Experiments},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106496},
doi = {10.1145/3106426.3106496},
abstract = {Nowadays, despite the fact that Machine Learning (ML) experiments can be easily built using several ML frameworks, as the demand for practical solutions for several kinds of scientific problems is always increasing, organizing its results and the different algorithms' setups used, in order to be able to reproduce them, is a long known problem without an easy solution. Motivated by the need of a high level of interoperability and data provenance with respect to ML experiments, this work presents a generic solution using a web-service application that interacts with the MEX vocabulary, a lightweight solution for archiving and querying ML experiments. By using this solution, researchers can share their setups and results, in a interoperable format that describes all the steps needed to reproduce their research. Although the solution presented in this work could be implemented in any programming language, we chose Java to build the web-service and also we chose to present experiments with Python's Scikit-learn ML Framework, using Decorators and Code Reflection, that demonstrates the simplicity of incorporating data provenance in such a high level, simplifying the experiment logging process.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {132–138},
numpages = {7},
keywords = {interoperability, mex, data provenance, reproducible research, machine learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106530,
author = {Esteves, Diego and Moussallem, Diego and Soru, Tommaso and Neto, Ciro Baron and Lehmann, Jens and Ngomo, Axel-Cyrille Ngonga and Duarte, Julio Cesar},
title = {LOG4MEX: A Library to Export Machine Learning Experiments},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106530},
doi = {10.1145/3106426.3106530},
abstract = {A choice of the best computational solution for a particular task is increasingly reliant on experimentation. Even though experiments are often described through text, tables, and figures, their descriptions are often incomplete or confusing. Thus, researchers often have to perform lengthy web searches for reproducing and understanding the results. In order to minimize this gap, vocabularies and ontologies have been proposed for representing data mining and machine learning (ML) experiments. However, we still lack proper tools to export properly these metadata. To this end, we present an open-source library dubbed LOG4MEX which aims at supporting the scientific community to fulfill this gap.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {139–145},
numpages = {7},
keywords = {LOG4MEX, provenance, logging, metadata, ontology, machine learning experiments, software architecture},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106538,
author = {Gl\"{o}ckner, Michael and Ludwig, Andr\'{e}},
title = {Ontological Structuring of Logistics Services},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106538},
doi = {10.1145/3106426.3106538},
abstract = {The paradigm of cloud logistics is essentially built upon the virtualization of logistics resources from different logistics service providers. The virtualized resources are pooled and can subsequently be combined and encapsulated within customer-specific modular logistics services. The pooling within bigger logistics networks leads to a high quantity of different available logistics resources and services. Domain-specific structuring with the concept of the logistics service map helps to retrieve specific requested services from that quantity. The structuring of resources and services is a challenging task based on the semantic gap of differing wordings, descriptions used by different providers. The developed ontology design pattern for domain-specific structuring of logistics services can help to close the semantic gap as well as to enable the concept of the logistics service map. Structuring data and information (of services) from different providers can be made available, linked and interchanged easily within the network. Digitalized collaboration is supported and the disruptive paradigm of cloud logistics is enabled.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {146–153},
numpages = {8},
keywords = {ontology design pattern, logistics, service map, domain structuring, cloud logistics},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106503,
author = {Vennesland, Audun},
title = {Matcher Composition for Identification of Subsumption Relations in Ontology Matching},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106503},
doi = {10.1145/3106426.3106503},
abstract = {Ontology matching is the process of identifying alignment between ontologies with the objective to facilitate interoperability and knowledge integration. A limitation of state of the art ontology matching systems is that the produced alignments usually only contain equivalence relations, while other relations, such as subsumption relations, are often neglected. Furthermore, an ontology matching system is normally composed of a set of differently tuned matching algorithms, but their appropriateness and order of execution typically require human judgement. The COMPOSE framework identifies subsumption relations automatically using a three-stage matcher composition process. These three processes are ontology analysis, matcher selection, and matcher combination. Within this framework we integrate existing techniques for all three processes with novel ones and evaluate the feasibility of the framework in an experiment involving six acknowledged ontologies.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {154–161},
numpages = {8},
keywords = {ontology matching, matcher composition, complex ontology relations},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106493,
author = {Hartman, Ryan and Faustino, Josemar and Pinheiro, Diego and Menezes, Ronaldo},
title = {Assessing the Suitability of Network Community Detection to Available Meta-Data Using Rank Stability},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106493},
doi = {10.1145/3106426.3106493},
abstract = {In the last two decades, we have witnessed the widespread use of structural analysis of data. The area, generally called Network Science, concentrates on understanding complex phenomena by looking for properties that emerge from the relationships between the pieces of data instead of the traditional mining of the data itself. A commonly used structural analysis in networks consists of finding subgraphs whose density of connections within the subgraph surpasses that of outside connections; called Community Detection. Many techniques have been proposed to find communities as well as benchmarks to evaluate the algorithms ability to find these substructures. Until recently, the literature has mostly neglected the fact that these communities often represent common characteristic of the elements in the community. For instance, in a social network, communities could represent: people who follow the same particular sport, people from the same classroom, authors working in the same field of study, to name a few. The problem here is one of community detection selection as a function of the ground truth provided by available meta-data. In this work, we propose the use of rank stability (entropy of ranks) to assess communities identified using different techniques from the perspective of meta-data. We validate our approach using a large-scale data set of on-line social interactions across multiple community detection techniques.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {162–169},
numpages = {8},
keywords = {meta-data, community detection, rank stability},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106480,
author = {Batista, Nat\'{e}rcia A. and Brand\~{a}o, Michele A. and Alves, Gabriela B. and da Silva, Ana Paula Couto and Moro, Mirella M.},
title = {Collaboration Strength Metrics and Analyses on GitHub},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106480},
doi = {10.1145/3106426.3106480},
abstract = {We perform social analyses over an important community: the open code collaboration network. Specifically, we study the correlation among features that measure the strength of social coding collaboration on GitHub - a Web-based source code repository that can be modeled as a social coding network. We also make publicly available a curated dataset called GitSED, GitHub Socially Enhanced Dataset. Our results have many practical applications such as to improve the recommendation of developers, the evaluation of team formation and existing analysis algorithms.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {170–178},
numpages = {9},
keywords = {tie strength, online cooperative work, social network analysis},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106460,
author = {Zhang, Yang and Ni, Minyue and Han, Weili and Pang, Jun},
title = {Does #like4like Indeed Provoke More Likes?},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106460},
doi = {10.1145/3106426.3106460},
abstract = {Hashtags, created by social network users, have gained a huge popularity in recent years. As a kind of metatag for organizing information, hashtags in online social networks, especially in Instagram, have greatly facilitated users' interactions. In recent years, academia starts to use hashtags to reshape our understandings on how users interact with each other. #like4like is one of the most popular hashtags in Instagram with more than 290 million photos appended with it, when a publisher uses #like4like in one photo, it means that he will like back photos of those who like this photo. Different from other hashtags, #like4like implies an interaction between a photo's publisher and a user who likes this photo, and both of them aim to attract likes in Instagram. In this paper, we study whether #like4like indeed serves the purpose it is created for, i.e., will #like4like provoke more likes? We first perform a general analysis of #like4like with 1.8 million photos collected from Instagram, and discover that its quantity has dramatically increased by 1,300 times from 2012 to 2016. Then, we study whether #like4like will attract likes for photo publishers; results show that it is not #like4like but actually photo contents attract more likes, and the lifespan of a #like4like photo is quite limited. In the end, we study whether users who like #like4like photos will receive likes from #like4like publishers. However, results show that more than 90% of the publishers do not keep their promises, i.e., they will not like back others who like their #like4like photos; and for those who keep their promises, the photos which they like back are often randomly selected.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {179–186},
numpages = {8},
keywords = {social networks, social media behavior, hashtags, data mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106451,
author = {Uchida, K. and Toriumi, F. and Sakaki, T.},
title = {Evaluation of Retweet Clustering Method Classification Method Using Retweets on Twitter without Text Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106451},
doi = {10.1145/3106426.3106451},
abstract = {Burst phenomena, which frequently occur on social media, are caused by such social events as flaming on the internet, elections, and natural disasters. To understand people's thoughts and feelings, we must classify their opinions from burst phenomena. Therefore, classification methods that categorize tweets are critical. However, since most classification methods focus on text mining, they cannot group tweets by topics because each tweet has poor linguistic similarities. We used a non-text-based classification method proposed by Baba et al. that groups tweets by topics, even if they have poor linguistic similarities, and verified its validity by comparing it with a text-based classification method in two different evaluations: qualitative and quantitative. In the qualitative evaluation part, we did a questionnaire survey and validated the suitability of the topic clusters created using both the non-and text-based methods. Since evaluating the similarity of every pair of tweets in each topic is difficult, we evaluated the similarity between sampled pairs in the survey and acquired more appropriate topic clustering results using the non-text-based method than the text-based method. In the quantitative evaluation part, we focused on the robustness of each method against data reduction. Many approaches analyze social media data, especially because collecting data from social media is comparatively easy. However, since collecting the whole data of burst phenomena is very costly due to the vast amounts of available social media data, robustness against data reduction is an important index to evaluate classification methods. With the non-text-based method, over 55% of the pairs of tweets in the same cluster were also included in the same cluster even when the data were reduced to 10% in all three of our example cases. In this paper, as a source we focus on Twitter, one of the most popular microblogging services. Using clustering to conduct detailed case analyses, we scrutinized three burst cases that include natural disasters and flaming on the internet and found that a non-text-based method more effectively classified tweets in burst phenomena than a text-based method.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {187–194},
numpages = {8},
keywords = {robustness, non-text-based method, burst phenomena, network clustering, Twitter},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106489,
author = {Castellini, Jacopo and Poggioni, Valentina and Sorbi, Giulia},
title = {Fake Twitter Followers Detection by Denoising Autoencoder},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106489},
doi = {10.1145/3106426.3106489},
abstract = {Gaining followers on the Twitter platform has become a rapid way to increase one's credibility on this social network, that in the last few years has become a launch pad for new trends and to influence people opinions. So, many people have begun to buy fake followers on underground markets appositely created to sold them. Therefore, identifying fake followers profiles is useful to maintain the balance between real influential people on the network and people who simply exploited this mechanism. This work presents a model based on artificial neural networks able to detect fake Twitter profiles. In particular, a denoising autoencoder has been implemented as anomaly detector trained with a semi-supervised learning approach. The model has been tested on a benchmark already used in literature and results are presented.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {195–202},
numpages = {8},
keywords = {autoencoder, artificial neural networks, semi-supervised learning, fake Twitter followers, anomaly detection, denoising autoencoder, one class mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106438,
author = {Farasat, Moeen and Scripps, Jerry},
title = {LCHI: Multiple, Overlapping Local Communities},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106438},
doi = {10.1145/3106426.3106438},
abstract = {Local community finding algorithms are helpful for finding communities around a seed node especially when the network is large and a global method is too slow. Most local methods find only a single community or are required to be run several times over different seed nodes to create multiple communities. In this paper, we present a new algorithm, LCHI that finds multiple, overlapping communities around a single node. Examples and analyses are presented support the effectiveness of LCHI.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {203–210},
numpages = {8},
keywords = {community finding, network science},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106467,
author = {Gadek, Guillaume and Pauchet, Alexandre and Malandain, Nicolas and Khelif, Khaled and Vercouter, Laurent and Brunessaux, St\'{e}phan},
title = {Measures for Topical Cohesion of User Communities on Twitter},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106467},
doi = {10.1145/3106426.3106467},
abstract = {Nowadays, Online Social Networks (OSN) are commonly used by groups of users to communicate. Members of a family, colleagues, fans of a brand, political groups: the demand for a precise identification of these groups is increasing from brand monitoring, business intelligence and e-reputation management.However, a gap can be observed between the communities detected by many data analytics algorithms on OSN, and effective groups existing in real life: the detected communities often lack of meaning and internal semantic cohesion. Most of existing literature on OSN either focuses on the community detection problem in graphs without considering the topic of the messages exchanged, or concentrates exclusively on the messages without taking into account the social links.In this article, we support the hypothesis that communities extracted on OSN should be topically coherent. We therefore propose a model to represent the interaction between users on Twitter, the reference on micro-blogging OSN, and metrics to evaluate the topical cohesion of the detected communities. As an evaluation, we measure the topical cohesion of the groups of users detected by a baseline community detection algorithm, using two measures inspired from the classification domain, and one measure inspired from the NLP domain.A detailed analysis is performed on a big tweet dataset, from which a user graph is built. Introduced measures are compared with statistics to better picture the experiment, and yield interesting insights on a social and textual corpus.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {211–218},
numpages = {8},
keywords = {topical cohesion measure of communities, community detection, online social network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106469,
author = {Aumayr, Erik and Hayes, Conor},
title = {The Path to Success: A Study of User Behaviour and Success Criteria in Online Communities},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106469},
doi = {10.1145/3106426.3106469},
abstract = {Maintaining online communities is vital in order to increase and retain their economic and social value. That is why community managers look to gauge the success of their communities by measuring a variety of user behaviour, such as member activity, turnover and interaction. However, such communities vary widely in their purpose, implementation and user demographics, and although many success indicators have been proposed in the literature, we will show that there is no one-fits-all approach to community success: Different success criteria depend on different user behaviour. To demonstrate this, we put together a set of user behaviour features, including many that have been used in the literature as indicators of success, and then we define and predict community success in three different types of online communities: Questions &amp; Answers (Q&amp;A), Healthcare and Emotional Support (Life &amp; Health), and Encyclopaedic Knowledge Creation. The results show that it is feasible to relate community success to specific user behaviour with an accuracy of 0.67--0.93 F1 score and 0.77--1.0 AUC.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {219–226},
numpages = {8},
keywords = {online communities, community success, user behaviour},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106474,
author = {Spillane, Brendan and Lawless, S\'{e}amus and Wade, Vincent},
title = {Perception of Bias: The Impact of User Characteristics, Website Design and Technical Features},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106474},
doi = {10.1145/3106426.3106474},
abstract = {Bias, whether real or perceived by the user, is inherent in news media. In this paper, we demonstrate that user characteristics, the design and common technical features of news websites impact users' perception of bias. A complex bias evaluation process was conducted using crowdsourced participants on webpages from nine popular news websites. Each webpage was subject to one of eight distortions which removed individual features of the design. Along with the control, a 9x9 experiment was conducted with participants asked to rate their perception of positive or negative bias in the design of the webpages. This tested the impact that removing each feature had on the user's perception of bias. Significant differences were found between how participants rated some distorted webpages and their respective controls. The category of news website was also found to influence the perception of bias. Furthermore, certain groups of users were found to have a predilection for rating certain categories of websites as more or less biased and were influenced by particular features of the design.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {227–236},
numpages = {10},
keywords = {news website design, bias, adaptive interfaces},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106450,
author = {Timilsina, Mohan and Davis, Brian and Taylor, Mike and Hayes, Conor},
title = {Predicting Citations from Mainstream News, Weblogs and Discussion Forums},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106450},
doi = {10.1145/3106426.3106450},
abstract = {The growth in the alternative digital publishing is widening the breadth of scholarly impact beyond the conventional bibliometric community. Thus, research is becoming more reachable both inside and outside of academic institutions and are found to be shared, downloaded and discussed in social media. In this study, we linked the scientific articles found in mainstream news, weblogs and Stack Overflow to the citation database of peer-reviewed literature called Scopus. We then explored how standard graph-based influence metrics can be used to measure the social impact of scientific articles. We also proposed the variant of Katz centrality metrics called EgoMet score to measure the local importance of scientific articles in its ego network. Later we evaluated these computed graph-based influence metrics by predicting absolute citations. Our results of the prediction model describe 34% variance to predict citations from blogs and mainstream news and 44% variance to predict citations from Stack Overflow.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {237–244},
numpages = {8},
keywords = {centrality, altmetrics, impact, graphs, prediction},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106446,
author = {Kang, Qiyu and Tay, Wee Peng},
title = {Sequential Multi-Class Labeling in Crowdsourcing: A Ulam-Renyi Game Approach},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106446},
doi = {10.1145/3106426.3106446},
abstract = {We consider a crowdsourcing platform where workers are posed questions by a crowdsourcer, who then uses their responses to determine the hidden state of a multi-class labeling problem. Workers may be unreliable, therefore by designing the questions using error correction coding approaches, the crowdsourcer can achieve a more reliable overall result. We propose to perform sequential questioning in which workers are asked q-ary questions sequentially, and questions are determined based on the workers' previous responses. We propose an optimization framework to determine the best q and questioning strategy to use, subject to a crowdsourcer budget constraint. For a fixed q, this problem is equivalent to finding an optimal questioning strategy to a q-ary Ulam-R\'{e}nyi game, which is in general intractable. We propose a heuristic to find a suboptimal strategy, and demonstrate through simulations that our solution outperforms another error correction coding strategy that does not utilize previous workers' responses. Simulations also suggest that q can in general be chosen to be much smaller than the number of classes in the multi-class labeling problem.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {245–251},
numpages = {7},
keywords = {multi-class labeling, cooperative work, ulam-r\'{e}nyi game, question design, crowdsourcing},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106488,
author = {Belk, Marios and Pamboris, Andreas and Fidas, Christos and Katsini, Christina and Avouris, Nikolaos and Samaras, George},
title = {Sweet-Spotting Security and Usability for Intelligent Graphical Authentication Mechanisms},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106488},
doi = {10.1145/3106426.3106488},
abstract = {This paper investigates the trade-off between security and usability in recognition-based graphical authentication mechanisms. Through a user study (N=103) based on a real usage scenario, it draws insights about the security strength and memorability of a chosen password with respect to the amount of images presented to users during sign-up. In particular, it reveals the users' predisposition in following predictable patterns when selecting graphical passwords, and its effect on practical security strength. It also demonstrates that a "sweet-spot" exists between security and usability in graphical authentication approaches on the basis of adjusting accordingly the image grid size presented to users when creating passwords. The results of the study can be leveraged by researchers and practitioners engaged in designing intelligent graphical authentication user interfaces for striking an appropriate balance between security and usability.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {252–259},
numpages = {8},
keywords = {usability, security, eye-tracking, recognition-based graphical authentication, user study},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106494,
author = {Xu, Ke and Cai, Yi and Min, Huaqing and Zheng, Xushen and Xie, Haoran and Wong, Tak-Lam},
title = {UIS-LDA: A User Recommendation Based on Social Connections and Interests of Users in Uni-Directional Social Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106494},
doi = {10.1145/3106426.3106494},
abstract = {The rapid growth of population has posed a challenge to people for discovering new followees in uni-directional social networks. Intuitively, a user's adoption of others as followees may motivated by her interest as well as social connection. Therefore, it is worth-while to consider both factors at the same time for better recommendations. Previous recommender works on implicit follow or not feedbacks become unqualified, mainly because of the coarse users' preferences inferring, which cannot distinguish whether one follows the other is based on her social connection or individual interest. In this paper, we present a new user recommendation method which is capable of recommending candidate followees who have similar interest and closer social connection relevant to a target user. As its core, a novel topic model namely UIS-LDA is designed to jointly model a user's preferences with respect to the set of latent interest topics and social topics. The experiments using Twitter dataset proves that our proposed method effective in improving the Precision, Conversion Rate F1 score and NDCG.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {260–265},
numpages = {6},
keywords = {uni-directional social networks, topic modeling, user recommendation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106472,
author = {Messias, Johnnatan and Vikatos, Pantelis and Benevenuto, Fabr\'{\i}cio},
title = {White, Man, and Highly Followed: Gender and Race Inequalities in Twitter},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106472},
doi = {10.1145/3106426.3106472},
abstract = {Social media is considered a democratic space in which people connect and interact with each other regardless of their gender, race, or any other demographic factor. Despite numerous efforts that explore demographic factors in social media, it is still unclear whether social media perpetuates old inequalities from the offline world. In this paper, we attempt to identify gender and race of Twitter users located in U.S. using advanced image processing algorithms from Face++. Then, we investigate how different demographic groups (i.e. male/female, Asian/Black/White) connect with other. We quantify to what extent one group follow and interact with each other and the extent to which these connections and interactions reflect in inequalities in Twitter. Our analysis shows that users identified as White and male tend to attain higher positions in Twitter, in terms of the number of followers and number of times in user's lists. We hope our effort can stimulate the development of new theories of demographic information in the online space.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {266–274},
numpages = {9},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106510,
author = {Dondio, Pierpaolo and Usher, James},
title = {Analysing the Behaviour of Online Investors in Times of Geopolitical Distress: A Case Study on War Stocks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106510},
doi = {10.1145/3106426.3106510},
abstract = {In this paper we analysed how the behavior of an online financial community changed in times of geopolitical crises. In particular, we studied the behaviour and communication patterns of online investors before and after a military geopolitical event. We selected a set of 23 key-events belonging to the 2003 US-led invasion of Iraq, the Arab Spring and the first period of the Ukraine crisis, and we restricted our study to a set of eight so called war stocks. We studied the resilience of the community to information shocks by comparing the community composition, its sentiment and users' communication networks before and after an event at different time intervals. We found how community reaction is governed by ordered patterns. Experimental evidence suggested how in the after-math of an event the community did not lose its information sharing functionality. Communication networks showed a higher in-degree Gini index, connectivity and a rich-club effect. Discussions developed around central users acting as hubs. These backbone users were present both before and after an event, their sentiment were less volatile than other users, and they were previously recognized as local experts of a specific stock. As a further evidence of community resilience, the equilibrium of all the indicators analysed were restored after two weeks.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {275–283},
numpages = {9},
keywords = {content analysis, web mining, social media, behavioral finance},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106516,
author = {Liu, Yang and Gu, Zhonglei and Ko, Tobey H. and Liu, Jiming},
title = {Brand Key Asset Discovery via Cluster-Wise Biased Discriminant Projection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106516},
doi = {10.1145/3106426.3106516},
abstract = {Accurate and effective discovery of a brand's key assets, namely, Key Opinion Leaders (KOLs) and potential customers, plays an essential role in marketing campaigns. In a massive online social network, brands are challenged with identifying a small portion of key assets over an enormous volume of irrelevant users, making the problem a highly imbalanced one. Moreover, having to deal with social media data that are usually high-dimensional, the task of brand key asset discovery can be immensely expensive yet inaccurate if the information are not processed efficiently to extract representative features from the original space prior to the learning process. To address the above issues, we propose a novel method dubbed Cluster-wise Biased Discriminant Projection (CBDP) to uncover the compact and informative features from users' data for brand key asset discovery. CBDP conducts a two-layer learning procedure. In the first layer, a Discriminant Clustering (DC) scheme is developed to partition the original dataset into clusters with maximum discriminant capacity. In the second layer, a Biased Discriminant Projection (BDP) algorithm is proposed and performed on each cluster to map the high-dimensional data to the low-dimensional subspace, where the discriminant information of classes with high importance/preference is preserved. A unified mapping function of CBDP is finally established by integrating these two layers. Experiments on both synthetic examples and a real-world brand key asset dataset validate the effectiveness of the proposed method.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {284–290},
numpages = {7},
keywords = {supervised feature extraction, brand key asset discovery, cluster-wise biased discriminant projection},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106531,
author = {Ramesh, Arti and Rodriguez, Mario and Getoor, Lise},
title = {Multi-Relational Influence Models for Online Professional Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106531},
doi = {10.1145/3106426.3106531},
abstract = {Professional networks are a specialized class of social networks that are particularly aimed at forming and strengthening professional connections and have become a vital component of professional success and growth. In this paper, we present a holistic model to jointly represent different heterogenous relationships between pairs of individuals, user actions and their respective propagations to characterize influence in online professional networks. Previous work on influence in social networks typically only consider a single action type in characterizing influence. Our model is capable of representing and combining different kinds of information users assimilate in the network and compute pairwise values of influence taking the different types of actions into account. We evaluate our models on data from the largest professional network, LinkedIn and show the effectiveness of the inferred influence scores in predicting user actions. We further demonstrate that modeling different user actions, node features, and edge relationships between users leads to around 20% increase in precision at top k in predicting user actions, when compared to the current state-of-the-art model.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {291–298},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106515,
author = {Hirota, Soichiro and Sasano, Ryohei and Takamura, Hiroya and Okumura, Manabu},
title = {Real-Time Tweet Selection for TV News Programs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106515},
doi = {10.1145/3106426.3106515},
abstract = {We present an automated, real-time tweet selection system for TV news programs. The system collects tweets related to a TV news program and chooses an appropriate tweet every 10 seconds for display. The selection procedure consists of two steps: assessing the importance of each tweet, and assessing the difference between the tweet and previously selected tweets. The previously selected tweets are taken into consideration for the purpose of maintaining the diversity of the entire set of selected tweets. We conducted experiments with a real TV news program and showed that the system developed in this study can select appropriate tweets.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {299–305},
numpages = {7},
keywords = {summarization, social media, microblogging},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106536,
author = {Brand\~{a}o, Michele A. and de Melo, Pedro O. S. Vaz and Moro, Mirella M.},
title = {Tie Strength Dynamics over Temporal Co-Authorship Social Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106536},
doi = {10.1145/3106426.3106536},
abstract = {In co-authorship social networks, nodes are authors linked by co-authorship interactions. As time is a relevant aspect of such interactions, concepts and metrics designed to static networks have to be adapted to temporal networks. Tie strength is one of those concepts. Here, we verify if current tie strength definitions are valid for temporal networks by analyzing the strength of ties dynamism over temporal co-authorship networks. Surprisingly, our results show that most ties, even the strong ones, tend to perish over time. Thus, most co-authorships are symbiotic without positive concerns. Also, real co-authorship social networks from different research areas have more weak and random ties than strong and bridge ties.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {306–313},
numpages = {8},
keywords = {tie strength, temporal social networks analyses, data and graph mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106544,
author = {Veillon, Lise-Marie and Bourgne, Gauvain and Soldano, Henry},
title = {Waves: A Model of Collective Learning},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106544},
doi = {10.1145/3106426.3106544},
abstract = {Collective learning considers how agents, in a community sharing a learning purpose, may benefit from exchanging hypotheses and observations to learn efficiently as a community as well as individuals. The community forms a communication network and each agent has access to observations. We address the question of a protocol, i.e. a set of agent's behaviours, which guarantees the hypotheses retained by the agents take into account all the observations in the community. We present and investigate the protocol WAVES which displays such a guarantee in a turn-based scenario: at the beginning of each turn, agents collect new observations and interact until they all reach this consistency guarantee. We investigate and experiment WAVES on various network topologies and various experimental parameters. We present results on learning efficiency, in terms of computation and communication costs, as well as results on learning quality, in terms of predictive accuracy for a given number of observations collected by the community.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {314–321},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106526,
author = {Wang, Wei and Miao, Chunyan and Hao, Shuji},
title = {Zero-Shot Human Activity Recognition via Nonlinear Compatibility Based Method},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106526},
doi = {10.1145/3106426.3106526},
abstract = {Human activity recognition aims to recognize human activities from sensor readings. Most of existing methods in this area can only recognize activities contained in training dataset. However, in practical applications, previously unseen activities are often encountered. In this paper, we propose a new zero-shot learning method to solve the problem of recognizing previously unseen activities. The proposed method learns a nonlinear compatibility function between feature space instances and semantic space prototypes. With this function, testing instances are classified to unseen activities with highest compatibility scores. To evaluate the effectiveness of the proposed method, we conduct extensive experiments on three public datasets. Experimental results show that our proposed method consistently outperforms state-of-the-art methods in human activity recognition problems.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {322–330},
numpages = {9},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106485,
author = {Najjar, Amro and Mualla, Yazan and Boissier, Olivier and Picard, Gauthier},
title = {AQUAMan: QoE-Driven Cost-Aware Mechanism for SaaS Acceptability Rate Adaptation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106485},
doi = {10.1145/3106426.3106485},
abstract = {As more interactive and multimedia-rich applications are migrating to the cloud, end-user satisfaction and her Quality of Experience (QoE) will become a determinant factor to secure success for any Software as a Service (SaaS) provider. Yet, in order to survive in this competitive market, SaaS providers also need to maximize their Quality of Business (QoBiz) and minimize costs paid to cloud providers. However, most of the existing works in the literature adopt a provider-centric approach where the end-user preferences are overlooked. In this article, we propose the AQUAMan mechanism that gives the provider a fine-grained QoE-driven control over the service acceptability rate while taking into account both end-users' satisfaction and provider's QoBiz. The proposed solution is implemented using a multi-agent simulation environment. The results show that the SaaS provider is capable of attaining the predefined acceptability rate while respecting the imposed average cost per user. Furthermore, the results help the SaaS provider identify the limits of the adaptation mechanism and estimate the best average cost to be invested per user.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {331–339},
numpages = {9},
keywords = {SaaS, QoE, user satisfaction, multi-agent negotiation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106486,
author = {Vanegas-Hernandez, Meili and da Costa Pereira, C\'{e}lia and Moreno, Diego and Fusco, Giovanni and Tettamanzi, Andrea G. B. and Riveill, Michel and Hern\'{a}ndez, Jos\'{e} Tiberio},
title = {A New Urban Segregation-Growth Coupled Model Using a Belief-Desire-Intention Possibilistic Framework},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106486},
doi = {10.1145/3106426.3106486},
abstract = {We study the feasibility of using Belief, Desire and Intention agents for modeling the phenomena of urban growth and segregation. Uncertainty, typical of real world situations is modeled using possibility theory. We have also implemented a simple visualization tool whose aim is to track the changes in the model. Some preliminary experiments suggest that such an approach might allow a decision-maker to dynamically track the changes in the model. Besides, it is also possible to interact with the different steps of the simulation via the model. Our proposal makes it possible to simulate the interactions between cognitive agents in an economical environment while taking the spatial context into account.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {340–347},
numpages = {8},
keywords = {BDI agent model, urban segregation, agent-based modeling, geosimulation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106445,
author = {Samadidana, Saeid and Mailler, Roger},
title = {Solving DCSP Problems in Highly Degraded Communication Environments},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106445},
doi = {10.1145/3106426.3106445},
abstract = {Although there have been tremendous gains in network communication reliability, many real world applications of distributed systems still face message loss, limitations, delay, and corruption. Yet despite this fact, most Distributed Constraint Satisfaction (DCSP) protocols assume that communication is perfect (messages that are sent will be received) although not ideal (not in a timely manner). As a result, many protocols are designed to exploit this assumption and are severely impacted when applied to real world conditions.This study compares the performance of several leading DCSP protocols including the Distributed Stochastic Algorithm (DSA), Distributed Breakout Algorithm (DBA), Max-Gain Message (MGM) and Distributed Probabilistic Protocol (DPP) to analyse their behaviour in communication degraded environments. The analysis begins by comparing the performance of all of the protocols in a perfect communication environment. We then use a simulated communication degraded environment where messages are probabilistically lost. Finally, we compare their performance by limiting the communication rate, which introduces delay.We show that DBA, once modified with a message timeout, is quite resistant to high message loss while DPP and DSA converge slower onto worse solutions. Our results also show that the setting of timeout value for DBA and MGM is an important factor in the convergence of these algorithms. Under conditions of message delay, DPP and DSA are less affected than DBA and MGM. Overall, DPP and DSA cause considerably less network load.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {348–355},
numpages = {8},
keywords = {robustness, communication, probability, constraint satisfaction, dynamic},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106518,
author = {Calvaresi, Davide and Marinoni, Mauro and Sturm, Arnon and Schumacher, Michael and Buttazzo, Giorgio},
title = {The Challenge of Real-Time Multi-Agent Systems for Enabling IoT and CPS},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106518},
doi = {10.1145/3106426.3106518},
abstract = {Techniques originating from the Internet of Things (IoT) and Cyber-Physical Systems (CPS) areas have extensively been applied to develop intelligent and pervasive systems such as assistive monitoring, feedback in telerehabilitation, energy management, and negotiation. Those application domains particularly include three major characteristics: intelligence, autonomy and real-time behavior. Multi-Agent Systems (MAS) are one of the major technological paradigms that are used to implement such systems. However, they mainly address the first two characteristics, but miss to comply with strict timing constraints. The timing compliance is crucial for safety-critical applications operating in domains such as healthcare and automotive. The main reasons for this lack of real-time satisfiability in MAS originate from current theories, standards, and technological implementations. In particular, internal agent schedulers, communication middlewares, and negotiation protocols have been identified as co-factors inhibiting the real-time compliance. This paper provides an analysis of such MAS components and pave the road for achieving the MAS compliance with strict timing constraints, thus fostering reliability and predictability.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {356–364},
numpages = {9},
keywords = {CPS, MAS negotiation timing compliant, real-time systems, IoT, multi-agent systems, real-time multi-agent systems},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106533,
author = {Drias, Yassine and Pasi, Gabriella},
title = {A Collaborative Approach to Web Information Foraging Based on Multi-Agent Systems},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106533},
doi = {10.1145/3106426.3106533},
abstract = {In this paper the task of Information Foraging (IF) is considered as a useful paradigm to address Exploratory Search. In the context of IF, a Web navigation strategy is introduced and formalized, and a multi-agent based model is proposed to exploit a collaborative approach to Information Foraging. A system based on this model has been developed, and its evaluations on the ACM and DBLP repositories are reported. Two datasets with different sizes were considered to show the effectiveness and the efficiency of the developed system. Furthermore, comparative evaluations were conducted in order to compare our approach with classical information access approaches. The results are promising and show the ability of the proposed Web Information Foraging system to find relevant Web pages in a very short time.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {365–371},
numpages = {7},
keywords = {web information foraging, collaborative search, multi-agent systems},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106509,
author = {Belchior, Mairon and da Silva, Viviane Torres},
title = {Detection of Normative Conflict That Depends on Execution Order of Runtime Events in Multi-Agent Systems},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106509},
doi = {10.1145/3106426.3106509},
abstract = {Norms in multi-agent systems are used as a mechanism to regulate the behavior of autonomous and heterogeneous agents and to maintain the social order of the society of agents. Norms describe actions that must be performed, actions that can be performed and actions that cannot be performed by a given entity in a certain situation. One of the challenges in designing and managing systems governed by norms is that they can conflict with another. Two norms are in conflict when the fulfillment of one causes the violation of the other. When that happens, whatever the agent does or refrains from doing will lead to a social constraint being broken. Several researches have been proposed mechanisms to detect conflicts between norms. However, there is a kind of normative conflict not investigated yet in the design phase, here called runtime conflicts, that can only be detected if we know information about the runtime execution of the system. This paper presents two approaches based on execution scenarios to detect normative conflicts that depends on execution order of runtime events in multi-agent systems. In the first approach, the system designer are able to provide examples of execution scenarios and evaluate the conflicts that may arise if those scenarios would be executed in the system. In the second approach, the conflict checker identifies potential normative conflicts by switching the position order of the runtime events referred in the norm conditions.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {372–380},
numpages = {9},
keywords = {norms, normative conflict, SWRL, multi-agent systems, OWL},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106511,
author = {Koohborfardhaghighi, Somayeh and Romero, Juan Pablo and Maliphol, Sira and Liu, YuLin and Altmann, J\"{o}rn},
title = {How Bounded Rationality of Individuals in Social Interactions Impacts Evolutionary Dynamics of Cooperation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106511},
doi = {10.1145/3106426.3106511},
abstract = {In this study, we explore the emergence of cooperative behavior in the prisoner's dilemma evolutionary game. In particular, we investigate the effect of bounded rationality of individuals on the networking topology (i.e., the individuals' personal networks). For this, we highlight the evolutionary dynamics of cooperation on top of different graph topologies with respect to their baseline properties such as average shortest path length and clustering coefficient. In addition, we test the effect of a new variable, called memory of interactions, on the changes in behavior and decision-making of the players as well as the networking outcome. For this purpose, we use agent-based modeling, which allows studying how changes in the environment or changes of properties of networked actors affect the evolutionary dynamics of cooperation among them. The results of our analysis confirm that the networking topology and the memory duration are important in affecting the emergence of cooperative behavior of players. They also impact the total utility that can be obtained from playing the Prisoner's Dilemma evolutionary game. Although the Prisoner's Dilemma game simulations tend towards full cooperation, if they are run over graph topologies with short average shortest path lengths and low clustering coefficients, the number of steps needed to reach equilibrium increases. This new result provides an understanding of the interactions of actors in a game.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {381–388},
numpages = {8},
keywords = {prisoner's dilemma game, agent-based modeling and simulation, average shortest path length, clustering coefficient, cooperative games},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106471,
author = {Wan, Zhijiang and Zhong, Ning and Chen, Jianhui and Zhou, Haiyan and Yang, Jie and Yan, Jianzuo},
title = {A Depressive Mood Status Quantitative Reasoning Method Based on Portable EEG and Self-Rating Scale},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106471},
doi = {10.1145/3106426.3106471},
abstract = {In order to actualize the quantitative reasoning function into the portable brain and mental health-monitoring system under WaaS architecture, this study proposes a method named quantitative reasoning for depressive mood status based on portable EEG and self-rating scale data. 5 inpatients were recruited to join the experiment, from which the portable EEG data and clinical self-rating scale data of 2 weeks were collected. The principal component analysis method is adopted to process the self-rating data. The regression analysis based on random forest algorithm is used to generate the quantitative reasoning model for acquiring reasoning rules. In order to further implement the quantitative reasoning function, the Protege and Jena are adopted to build data ontologies and actualize an automatic reasoning function for objectively quantifying the depressive mood status respectively. The effectiveness of reasoning rules are validated, the preliminary results show that the expected quantitative value outputted from the quantitative reasoning model is highly correlated (the absolute value of correlation coefficient ≥0.7, P-value ≤0.05) with the actual self-rating scale data.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {389–395},
numpages = {7},
keywords = {WaaS, depression quantitative analysis, ontology technology, reasoning and annotation, regression analysis, rulemaking},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106434,
author = {Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque},
title = {Intelligent Decision Support for Data Purchase},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106434},
doi = {10.1145/3106426.3106434},
abstract = {The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called "gut feeling" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {396–402},
numpages = {7},
keywords = {personalization, decision support, data purchase, computational intelligence},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106441,
author = {Carpineto, Claudio and Romano, Giovanni},
title = {Learning to Detect and Measure Fake Ecommerce Websites in Search-Engine Results},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106441},
doi = {10.1145/3106426.3106441},
abstract = {When searching for a brand name in search engines, it is very likely to come across websites that sell fake brand's products. In this paper, we study how to tackle and measure this problem automatically. Our solution consists of a pipeline with two learning stages. We first detect the ecommerce websites (including shopbots) present in the list of search results and then discriminate between legitimate and fake ecommerce websites. We identify suitable learning features for each stage and show through a prototype system termed RI.SI.CO. that this approach is feasible, fast, and highly effective. Experimenting with one goods sector, we found that RI.SI.CO. achieved better classification accuracy than that of non-expert humans. We next show that the information extracted by our method can be used to generate sector-level 'counterfeiting charts' that allow us to analyze and compare the counterfeit risk associated with different brands in a same sector. We also show that the risk of coming across counterfeit websites is affected by the particular web search engine and type of search query used by shoppers. Our research offers new insights and some very practical and useful means for analyzing and measuring counterfeit ecommerce websites in search-engine results, thus enabling targeted anti-counterfeiting actions.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {403–410},
numpages = {8},
keywords = {trustworthiness assessment of eshops, cybercrime measurement, online counterfeit goods, website classification, spam detection in web search results},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106545,
author = {Janjua, Naeem Khalid and Hussain, Omar Khadeer and Chang, Elizabeth and Islam, Syed Mohammed Shamsul},
title = {Conjoint Utilization of Structured and Unstructured Information for Planning Interleaving Deliberation in Supply Chains},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106545},
doi = {10.1145/3106426.3106545},
abstract = {Effective business planning requires seamless access and intelligent analysis of information in its totality to allow the business planner to gain enhanced critical business insights for decision support. Current business planning tools provide insights from structured business data (i.e. sales forecasts, customers and products data, inventory details) only and fail to take into account unstructured complementary information residing in contracts, reports, user's comments, emails etc. In this article, a planning support system is designed and developed that empower business planners to develop and revise business plans utilizing both structured data and unstructured information conjointly. This planning system activity model comprises of two steps. Firstly, a business planner develops a candidate plan using planning template. Secondly, the candidate plan is put forward to collaborating partners for its revision interleaving deliberation. Planning interleaving deliberation activity in the proposed framework enables collaborating planners to challenge both a decision and the thinking that underpins the decision in the candidate plan. The planning system is modeled using situation calculus and is validated through a prototype development.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {411–418},
numpages = {8},
keywords = {planning, argumentation, unstructured information, ontology},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106532,
author = {Yosef, David Ben and Naamani-Dery, Lihi and Obraztsova, Svetlana and Rabinovich, Zinovi and Bannikova, Marina},
title = {Haste Makes Waste: A Case to Favour Voting Bots},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106532},
doi = {10.1145/3106426.3106532},
abstract = {Voting is a common way to reach a group decision. When possible, voters will attempt to vote strategically, in order to optimize their satisfaction from the outcome. Previous research has modelled how rational voter agents (bots) vote to maximize their personal utility in an iterative voting process that has a deadline (a timeout). However, it remains an open question whether human beings behave rationally when faced with the same settings. The focus of this paper is therefore to examine how the deadline factor affects manipulative behavior in real-world scenarios were humans are required to reach a decision before a deadline. An On-line platform was built to enable voting games by all types of users: agents (bots), humans, and mixed games with both humans and agents. We compare the results of human behavior and bot behavior and conclude that it might be wise to allow bots to make (certain) decisions on our behalf.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {419–425},
numpages = {7},
keywords = {deadline, voting, consensus, collective decision making, social choice},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106502,
author = {Iitsuka, Shuhei and Kawakami, Kazuya and Hagiwara, Seigen and Kawakami, Takayoshi and Hamada, Takayuki and Matsuo, Yutaka},
title = {Inferring Win-Lose Product Network from User Behavior},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106502},
doi = {10.1145/3106426.3106502},
abstract = {Various data mining techniques to extract product relations have been examined, especially in the context of building intelligent recommender systems. Most such techniques, however, specifically examine co-occurrences of browsed or purchased products on e-commerce websites, which provide little or no useful information related to the direct relation of superiority or the factor which forms that superiority. For marketers and product managers, understanding the competitive advantages of a given product is important to consolidate their product differentiation strategies.As described in this paper, we propose a win-lose relation, a new product relation analysis method that retrieves the superiority relation between competitive products in terms of product attractiveness. Our proposed method uses the difference between user browsing and purchasing behaviors, assuming that a purchased product is superior to products that are browsed but not purchased. We also propose superiority factor analysis to examine keywords that represent the superiority factor by mining product reviews. We evaluate our methods using an actual dataset from Zexy, the largest wedding portal website in Japan. Our experimental evaluation revealed that our proposed method can estimate actual user preferences observed from a user study using only log data. Results also show that our proposed method raises the accuracy of superiority factor extraction by around 17% by considering the win-lose relation of products.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {426–433},
numpages = {8},
keywords = {review mining, e-commerce, product network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106505,
author = {Nakamura, Tatsuya and Tominaga, Tomu and Watanabe, Miki and Thammasan, Nattapong and Urai, Kenji and Nakamura, Yutaka and Hosoda, Kazufumi and Hara, Takahiro and Hijikata, Yoshinori},
title = {Investigation on Dynamics of Group Decision Making with Collaborative Web Search},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106505},
doi = {10.1145/3106426.3106505},
abstract = {In this paper, we present results of investigation on the dynamics of group decision making - how people discuss and make a decision-with collaborative web search. Prior works proposed systems that support group decision making with web search but have not examined the influence of discussion behaviors especially on the satisfaction levels with the final conclusion. In this study, we conducted a set of experiments to observe discussion behaviors and the consequent satisfaction with the conclusion using our experimental system and a set of questionnaires. The task for each participant was to make a decision on a restaurant. Our primary results revealed (1) the similar activities across all groups at the beginning and the end of the group discussion, (2) a lack of correspondence between the satisfaction with the conclusion and the time spent to reach the conclusion, and (3) the presumption that a member who actively engaged in the activities that were visible for the other members was likely to be voted as a leader in the group discussion beyond the discussion. Finally, we discussed how to implement intelligent systems that aid group decision making.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {434–441},
numpages = {8},
keywords = {collaborative web search, dynamics, group decision making},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106453,
author = {Liu, Xi and Tan, Pang-Ning and Liu, Lei and Simske, Steven J.},
title = {Automated Classification of EEG Signals for Predicting Students' Cognitive State during Learning},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106453},
doi = {10.1145/3106426.3106453},
abstract = {For distance learning applications, inferring the cognitive states of students, particularly, their concentration and comprehension levels during instruction, is important to assess their learning efficacy. In this paper, we investigated the feasibility of using EEG recordings generated from an off-the-shelf, wearable device to automatically classify the cognitive states of students as they were asked to perform a series of reading and question answering tasks. We showed that the EEG data can effectively predict whether a student is attentive or distracted as well as the student's reading speed, which is an important measure of reading fluency. However, the EEG signals alone are insufficient to predict how well the students can correctly answer questions related to the reading materials as there were other confounding factors, such as the students' background knowledge, that must be taken into consideration. We also showed that the accuracy in predicting the different cognitive states depends on the choice of classifier used (global, local, or multi-task learning). For example, the concentration level of a student can be accurately predicted using a local model whereas a global model that incorporates side information about the student's background knowledge is more effective at predicting whether the student will correctly answer questions about the materials they read.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {442–450},
numpages = {9},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106429,
author = {Seyfi, Majid and Nayak, Richi and Xu, Yue and Geva, Shlomo},
title = {Efficient Mining of Discriminative Itemsets},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106429},
doi = {10.1145/3106426.3106429},
abstract = {Discriminative itemsets can be more useful than frequent itemsets as the former identifies the frequent itemsets in one dataset with much higher frequencies than the same itemsets in other datasets. The discriminative itemsets can distinguish the target dataset from all others. The discriminative itemsets are a small subset of frequent itemsets. The efficient mining of discriminative itemsets is a challenging problem, since the Apriori property of frequent itemsets is not applicable, and the designed algorithms must deal with the exponential number of itemset combinations in more than one dataset. In this paper, a novel algorithm, called DISSparse, is proposed for efficient mining of discriminative itemsets. Two determinative heuristics are proposed for limiting the mining of discriminative itemsets to the potential discriminative itemsets. Our experiments show the efficient time and space usage of the proposed algorithm in the large and complex datasets.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {451–459},
numpages = {9},
keywords = {discriminative itemsets, data mining, prefix-tree},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106447,
author = {Zhang, Denghui and Li, Manling and Jia, Yantao and Wang, Yuanzhuo and Cheng, Xueqi},
title = {Efficient Parallel Translating Embedding for Knowledge Graphs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106447},
doi = {10.1145/3106426.3106447},
abstract = {Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great difficulty in practical applications. In this paper, we propose an efficient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [19], and a more efficient variant TransE- AdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {460–468},
numpages = {9},
keywords = {knowledge graph embedding, translation-based, parallel},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106477,
author = {Liu, Fanzhen and Chen, Zhengpeng and Cui, Yali and Liu, Chen and Li, Xianghua and Gao, Chao},
title = {A Hybrid Evolutionary Algorithm for Community Detection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106477},
doi = {10.1145/3106426.3106477},
abstract = {Evolutionary algorithm belongs to the behaviorism which is one of major approaches to artificial intelligence. Community detection is one of the important applications of the evolutionary algorithm. Detecting the community structure, an essential property for complex networks, can help us understand the inherent functions of real systems. It has been proved that genetic algorithm (GA) is feasible for community detection, and yet existing GA-based community detection algorithms still need improving in terms of their robustness and accuracy. A Physarum-based network model (PNM) with an intelligence of recognizing inter-community edges based on a kind of multi-headed slime mold, has been proposed in the phase of GA's initialization for optimization. In this paper, integrated with PNM after three operators of GA during the process of community detection, a novel genetic algorithm, called P-GACD, is proposed to improve the efficiency of GA for community detection. In addition, some experiments are implemented in five real-world networks to evaluate the performance of P-GACD. The results reveal that P-GACD shows an advantage in terms of the robustness and accuracy, contrasted with the existing algorithms.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {469–475},
numpages = {7},
keywords = {physarum, complex networks, genetic algorithm, community detection},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106436,
author = {Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth},
title = {An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106436},
doi = {10.1145/3106426.3106436},
abstract = {The big data research topic has grown rapidly for the past decade due to the advent of the "data deluge". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {476–482},
numpages = {7},
keywords = {crowdsourcing management, crowd workers, statistical quality control, multiple choice HIT},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106458,
author = {Otsuka, Tomoaki and Sugawara, Toshiharu},
title = {Robust Spread of Cooperation by Expectation-of-Cooperation Strategy with Simple Labeling Method},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106458},
doi = {10.1145/3106426.3106458},
abstract = {This paper proposes an interaction strategy called the extended expectation-of-cooperation (EEoC) that is intended to spread cooperative activities in prisoner's dilemma situations over an entire agent network. Recently developed computer and communications applications run on the network and interact with each other as delegates of the owners, so they often encounter social dilemma situations. To improve social efficiency, they are required to cooperate, but one-sided cooperation is meaningless and loses some payoff due to a rip-off by defecting agents. The concept of EEoC is that when agents encounter mutual cooperation, they continue to cooperate a few times with the desire to see the emergence of cooperative behavior in their neighbors. EEoC is easy to implement in computer systems. We experimentally show that EEoC can effectively spread cooperative activities in dilemma situations in complete, Erd\"{o}s-R\'{e}nyi, and regular networks. We also clarify the robustness against defecting agents and the limitation of the EEoC strategy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {483–490},
numpages = {8},
keywords = {cooperation, prisoner's dilemma, agent network, reinforcement learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106430,
author = {Lee, Jun and Kim, Kyoung-Sook and Kwon, YongJin and Ogawa, Hirotaka},
title = {Understanding Human Perceptual Experience in Unstructured Data on the Web},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106430},
doi = {10.1145/3106426.3106430},
abstract = {Computing for human experience has become more important for understanding all of aspects of any interaction of human beings in the cyber, physical, and social environments. In particular, artificial intelligent technologies based on big data enable to understand natural language, enhance day to day human experience, and make a better decision. In this paper, we propose a method to classify unstructured text data on the Web into the five types of sensation features: sight (ophthalmoception), hearing (audioception), touch (tactioception), smell (olfacception), and taste (gustaoception). Even though sensation is the first process of human experience against the environments, the study of sensation information extraction is neglected due to lack of sensory expression and knowledge comparing with the sentimental analysis or opinion mining. We first define the sensation measurement that is assigned to each feature. Then, we identify which sensation feature has a strong influence on human perceptual experience in a specific topic of corpus. Finally, we evaluate our method by comparing with several baselines in terms of the accuracy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {491–498},
numpages = {8},
keywords = {word sense disambiguation, text mining, sensation information, human sensory knowledge, classification},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106537,
author = {Bordogna, Gloria and Ciriello, Daniele E. and Psaila, Giuseppe},
title = {A Flexible Framework to Cross-Analyze Heterogeneous Multi-Source Geo-Referenced Information: The J-CO-QL Proposal and Its Implementation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106537},
doi = {10.1145/3106426.3106537},
abstract = {The need for cross-analyzing JSON objects representing heterogeneous geo-referenced information coming from multiple sources, such as open data published on the Web by public administrations and crowd-sourced posts and images from social networks, is becoming common for studying, predicting and planning social dynamics. Nevertheless, although NoSQL databases have emerged as a de facto standard means to store JSON objects, a query language that can be easily used by not-programmers to manipulate and correlate such data is still missing. Furthermore, when the information is geo-referenced, we also need both spatial analysis and mapping facilities.In the paper, we motivate the need for a novel flexible framework, named J-CO, that provides a query language, named J-CO-QL, enabling novel declarative (spatial) queries for JSON objects. We will illustrate the basic concepts of the proposal and the possible use of its spatial and non-spatial operators for cross-analyzing open data and crowd-sourced information. This framework is powered by a plug-in for QGIS that can be used to write and execute queries on MongoDB databases.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {499–508},
numpages = {10},
keywords = {collections of JSONbjects, geo-tagged data sets, powerful spatial operators, query language for geographical analysis},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106519,
author = {Fan, Tuan-Fang and Liau, Churn-Jung},
title = {A Logic for Reasoning about Evidence and Belief},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106519},
doi = {10.1145/3106426.3106519},
abstract = {In agent-based systems, an agent generally forms her belief based on evidence from multiple sources, such as messages from other agents or perception of the external environment. In this paper, we present a logic for reasoning about evidence and belief. Our framework not only takes advantage of the source-tracking capability of justification logic, but also allows the distinction between the actual observation and simply potential admissibility of evidence. We present the axiomatization for the basic logic and its dynamic extension, investigate its properties, and use a running example to show its applicability to information fusion for autonomous agents.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {509–516},
numpages = {8},
keywords = {evidential reasoning, second-order propositional modal logic, dynamic epistemic logic, reasoning about belief, logical omniscience, justification logic},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106524,
author = {Wang, Juntao and Guo, Wenshuo and Szeto, Kwok Yip},
title = {Optimization of Financial Network Stability by Genetic Algorithm},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106524},
doi = {10.1145/3106426.3106524},
abstract = {Directed network with flow dynamics is an important topic in realistic complex systems such as banking systems. We study the stability of these financial networks using a homogeneous and a more general inhomogeneous model. The nodes of the networks are banks and they are under four different kinds of disturbance that may lead to bankruptcy, which is defined by the condition that the capital value of the bank is below a critical value, such as zero. The stability of this network is measured by the minimum value of capital of the nodes after a fixed time of money flow in the network. The smaller the minimum value, the higher the probability of the first bankruptcy. By means of genetic algorithm with networks as the chromosome, the edges between nodes as genes, we can perform evolutionary computation using rewiring of edges as the genetic operators to search for a network topology with high stability. Our analytical results on flow dynamics are based on the linear relation between net change and PageRank centrality under small damping factor. Our numerical analysis shows that genetic algorithm can be very efficient in finding network with given connectivity that is stable against four categories of disturbance, namely under internal (and external) perturbation that is cumulative or noncumulative. Our results show that a banking network with each bank with similar relative wealth (thus small Gini coefficient in wealth distribution) is more stable than one with highly unequal wealth distribution. We also find our optimization method works for delaying the second bankruptcy after the first. These results may be useful in practical applications for the prevention of the first failure, as well as cascade failures in real complex systems.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {517–524},
numpages = {8},
keywords = {genetic algorithm, stability, network, optimization},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106457,
author = {Wang, Wenbo and Chen, Lu and Chen, Keke and Thirunarayan, Krishnaprasad and Sheth, Amit P.},
title = {Adaptive Training Instance Selection for Cross-Domain Emotion Identification},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106457},
doi = {10.1145/3106426.3106457},
abstract = {This paper exploits a large number of self-labeled emotion tweets as the training data from the source domain to improve emotion identification in target domains (i.e., blogs and fairy tales), where there is a short supply of labeled data. Due to the noisy and ambiguous nature of self-labeled emotion training data, the existing domain adaptation methods that typically depend on high-quality labeled source-domain data do not work satisfactorily. This paper describes an adaptive source-domain training instance selection method to address the problem of noisy source-domain training data. The proposed approach can effectively identify the most informative training examples based on three carefully designed measures: consistency, diversity, and similarity. It uses an iterative method that consists of the following steps in each iteration: selecting informative samples from the source domain with the informativeness measures, merging with the target-domain training data, evaluating the performance of learned classifier for the target domain, and updating the informativeness measures for the next iteration. It stops until no new training instance is selected or in a designated number of iterations. Experiments show that our approach performs effectively for cross-domain emotion identification and consistently outperforms baseline approaches across four domains.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {525–532},
numpages = {8},
keywords = {instance selection, cross-domain emotion identification},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106459,
author = {Zhou, Xujuan and Tao, Xiaohui and Rahman, Md Mostafijur and Zhang, Ji},
title = {Coupling Topic Modelling in Opinion Mining for Social Media Analysis},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106459},
doi = {10.1145/3106426.3106459},
abstract = {Many of social media platforms such as Facebook and Twitter make it easy for everyone to share their thoughts on literally anything. Topic and opinion detection in social media facilitates the identification of emerging societal trends, analysis of public reactions to policies and business products. In this paper, we proposed a new method that combines the opining mining and context-based topic modelling to analyse public opinions on social media data. Context based topic modelling is used to categorise data in groups and discover hidden communities in data group. The unwanted data group discovered by the topic model then will be discarded. A lexicon based opinion mining method will be applied to the remaining data groups to spot out the public sentiment about the entities. A set of Tweets data on Australian Federal Election 2010 was used in our experiments. Our experimental results demonstrate that, with the help of topic modelling, our social media analysis model is accurate and effective.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {533–540},
numpages = {8},
keywords = {social media analysis, topic modelling, online social networks, opinion mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106476,
author = {Dasgupta, Tirthankar and Naskar, Abir and Saha, Rupsa and Dey, Lipika},
title = {CrimeProfiler: Crime Information Extraction and Visualization from News Media},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106476},
doi = {10.1145/3106426.3106476},
abstract = {News articles from different sources regularly report crime incidents that contain details of crime, information about accused entities, details of the investigation process and finally details of judgement. In this paper, we have proposed natural language processing techniques for extraction and curation of crime-related information from digitally published News articles. We have leveraged computational linguistics based methods to analyse crime related News documents to extract different crime related entities and events. This includes name of the criminal, name of the victim, nature of crime, geographic location, date and time, and action taken against the criminal. We have also proposed a semi-supervised learning technique to learn different categories of crime events from the News documents. This helps in continuous evolution of the crime dictionaries. Thus the proposed methods are not restricted to detecting known crimes only but contribute actively towards maintaining an updated crime dictionary. We have done experiments with a collection of 3000 crime-reporting News articles. The end-product of our experiments is a crime-register that contains details of crime committed across geographies and time. This register can be further utilized for analytical and reporting purposes.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {541–549},
numpages = {9},
keywords = {entity extraction, text classification, crime profiling, entity resolution, crime ontology},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106482,
author = {Verma, Ishan and Dey, Lipika and Meisheri, Hardik},
title = {Detecting, Quantifying and Accessing Impact of News Events on Indian Stock Indices},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106482},
doi = {10.1145/3106426.3106482},
abstract = {The impact of different types of events reported in News articles on stock market is a widely accepted phenomenon. Market analysts rely heavily on technology to combine data from different sources and generate appropriate insights for predicting stock movements. With plethora of sources reporting news on plentitude of events happening across the world, a combination of text mining techniques and predictive technologies can play a significant role in this arena. In this paper we have presented methodologies to identify and quantify the presence of different types of information that can affect the market from a multitude of web sources, and finally use the information for predicting stock movement direction. We propose the use of PESTEL factors to categorize market-impacting information. We have analyzed large volumes of past available data using Granger causality to understand how these categories impact the market. We propose a paragraph-vector based information classification mechanism. We also present Long-Short term memory Network (LSTM) based prediction model to investigate the prediction capabilities of the information components. The proposed system outperforms state of the art linear SVM on data from different stock indices.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {550–557},
numpages = {8},
keywords = {news event detection, stock prediction, granger causality},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106449,
author = {Petrovski, Petar and Bizer, Christian},
title = {Extracting Attribute-Value Pairs from Product Specifications on the Web},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106449},
doi = {10.1145/3106426.3106449},
abstract = {Comparison shopping portals integrate product offers from large numbers of e-shops in order to support consumers in their buying decisions. Product offers often consist of a title and a free-text product description, both describing product attributes that are considered relevant by the specific vendor. In addition, product offers might contain structured or semi-structured product specifications in the form of HTML tables and HTML lists. As product specifications often cover more product attributes than free-text descriptions, being able to extract attribute-value pairs from these specifications is a critical prerequisite for achieving good results in tasks such as product matching, product categorisation, faceted product search, and product recommendation.In this paper, we present an approach for extracting attribute-value pairs from product specifications on the Web. We use supervised learning to classify the HTML tables and HTML lists within a web page as product specification or not. In order to extract attribute-value pairs from the HTML fragments identified by the specification detector, we again use supervised learning to classify columns as attribute column or value column. Compared to DEXTER, the current state-of-the-art approach for extracting attribute-value pairs from product specifications, we introduce several new features for specification detection and support the extraction of attribute-value pairs from specifications having more than two columns. This allows us to improve the F-score up to 10% for extracting attribute-value pairs from tables and up to 3% for lists. In addition, we report the results of using duplicate-based schema matching to align the product attribute schemata of 32 different e-shops. This experiment confirms the suitability of duplicate-based schema matching for product data integration.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {558–565},
numpages = {8},
keywords = {web tables, feature extraction, product data, schema matching},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106473,
author = {Yen, An-Zi and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Fusing Domain-Specific Data with General Data for in-Domain Applications},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106473},
doi = {10.1145/3106426.3106473},
abstract = {This paper analyzes the lexical semantics of domain-specific terms based on various pre-trained specific domain and general domain word vectors, and addresses the semantic drift between domains. To capture lexical semantics in the specific domain, we propose a bridge mechanism to introduce domain-specific data into general data, and re-train word vectors. We find that even a small-scale fusion can result in the similar lexical semantics learned by using the large-scale domain-specific dataset. Experiments on sentiment analysis and outlier detection show that application of word embedding by the fusion dataset has the better performance than applications of word embeddings by pure large domain-specific and pure large general datasets. The simple, but effective methodology facilitates the domain adaptation of distributed word representations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {566–572},
numpages = {7},
keywords = {sentiment analysis, outlier detection, cross-domain data fusion},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106483,
author = {Fazil, Mohd and Abulaish, Muhammad},
title = {Identifying Active, Reactive, and Inactive Targets of Socialbots in Twitter},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106483},
doi = {10.1145/3106426.3106483},
abstract = {Online social networks are facing serious threats due to presence of human-behaviour imitating malicious bots (aka socialbots) that are successful mainly due to existence of their duped followers. In this paper, we propose an approach to categorize Twitter users into three groups - active, reactive, and inactive targets, based on their interaction behaviour with socialbots. Active users are those who themselves follow socialbots without being followed by them, reactive users respond to the following socialbots by following them back, whereas inactive users do not show any interest against the following requests from anonymous socialbots. The proposed approach is modelled as both binary and ternary classification problem, wherein users' profile is generated using static and dynamic components representing their identical and behavioural aspects. Three different classification techniques viz Naive Bayes, Reduced Error Pruned Decision Tree, and Random Forest are used over a dataset of 749 users collected through live experiment, and a thorough analyses of the identified users categories is presented, wherein it is found that active and reactive users keep on frequently updating their tweets containing advertising related contents. Finally, feature ranking algorithms are used to rank identified features to analyse their discriminative power, and it is found that following rate and follower rate are the most dominating features.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {573–580},
numpages = {8},
keywords = {socialbot identification, Twitter data analysis, social network analysis, user profiling, socialbot characterization},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106435,
author = {Romero, Simone and Becker, Karin},
title = {Improving the Classification of Events in Tweets Using Semantic Enrichment},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106435},
doi = {10.1145/3106426.3106435},
abstract = {Contextual enrichment using external sources has been proposed as a means to deal with the poor textual contents of tweets for event classification. Related work performs contextual enrichment according to specific assumptions about the events. Furthermore, enrichment adds a significant amount of extra features, most of them with no discriminative contribution to the event classification task. In this paper, we propose an enrichment framework targeted at the classification of events in general, of which the key elements are: a) external enrichment using related web pages for extending the conceptual features contained within the tweets; b) semantic enrichment using the DBpedia to add related semantic features, and c) a pruning technique that selects the semantic features with discriminative potential. We compared the proposed approach against two distinct baselines based on textual features only and word embeddings, using seven different event datasets. Our experiments reveal that the proposed framework supports the classification of distinct event types, outperforming the textual baseline in 63.5% of the cases, and the word embeddings baseline in 96.5% of the cases.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {581–588},
numpages = {8},
keywords = {event classification, semantic web, contextual enrichment},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106468,
author = {Xu, Yu and Zhou, Dong and Lawless, S\'{e}amus},
title = {Inferring Your Expertise from Twitter: Combining Multiple Types of User Activity},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106468},
doi = {10.1145/3106426.3106468},
abstract = {Understanding the expertise of users in social networking sites like Twitter is a key component for many applications such as user recommendation and talent seeking. A range of interactions between users on Twitter can provide important information that implicitly reflects a user's expertise. This paper proposes a learning model that tries to infer a user's topical expertise from Twitter using information such as tweets posted by the user and the characteristics of their followers. The model takes various types of user-related data from Twitter as input and considers their inference consistency in the process of learning. It aims to deliver accurate and effective inference results, even in cases where some types of data are missing for a user, e.g. the user has yet to post any tweets. The experiments reported in the paper were conducted on a large-scale Twitter dataset. Experimental results show that our model outperforms several baseline approaches and outperforms approaches which use only a single type of user data for inference.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {589–598},
numpages = {10},
keywords = {inference model, Twitter, user expertise},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106443,
author = {Shabunina, Ekaterina and Pasi, Gabriella},
title = {Information Evolution Modeling and Tracking in Social Media},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106443},
doi = {10.1145/3106426.3106443},
abstract = {Nowadays, User Generated Content is the main source of real time news and opinions on the world happenings. Social Media, which serves as an environment for the creation and spreading of User Generated Content, is, therefore, representative of our culture and constitutes a potential treasury of knowledge. In this paper we propose a fully automatic approach for modeling and tracking the information evolution in Social Media. In particular, we propose to model a Social Media stream as a text graph. A graph degeneracy technique is used to identify the temporal sequence of the core units of information streams represented by graphs. Furthermore, as the major novelty of this work, we propose a set of measures to track and evaluate the evolution of information in time. An experimental evaluation on the crawled datasets from one of the most popular Social Media platforms proves the validity and applicability of the proposed approach.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {599–606},
numpages = {8},
keywords = {information evolution, information propagation, meme, text graph, graph degeneracy},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106461,
author = {Pothirattanachaikul, Suppanut and Yamamoto, Takehiro and Fujita, Sumio and Tajima, Akira and Tanaka, Katsumi},
title = {Mining Alternative Actions from Community Q&amp;A Corpus for Task-Oriented Web Search},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106461},
doi = {10.1145/3106426.3106461},
abstract = {Web searchers often use a Web search engine to find a way or means to achieve his/her goal. For example, a user intending to solve his/her sleeping problem, the query "sleeping pills" may be used. However, there may be another solution to achieve the same goal, such as "have a cup of hot milk" or "stroll before bedtime." The problem is that the user may not be aware that these solutions exist. Thus, he/she will probably choose to take a sleeping pill without considering these solutions. In this study, we define and tackle the alternative action mining problem. In particular, we attempt to develop a method for mining alternative actions for a given query. We define alternative actions as actions which share the same goal and define the alternative action mining problem as similar in the search result diversification. To tackle the problem, we propose leveraging a community Q&amp;A (cQA) corpus for mining alternative actions. We propose a method to compute how well two actions can be alternative actions by using a question-answer structure in a cQA corpus. Our method builds a question-action bipartite graph and recursively computes how well two actions can be alternative actions. We conducted experiments to investigate the effectiveness of our method using two newly built test collections, each containing 50 queries. The experimental results indicated that our proposed method outperformed the query suggestion methods provided by the commercial search engines in terms of D#-nDCG.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {607–614},
numpages = {8},
keywords = {community Q&amp;A, task-oriented search, query suggestion},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106487,
author = {Mallela, Deepa and Ahlers, Dirk and Pera, Maria Soledad},
title = {Mining Twitter Features for Event Summarization and Rating},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106487},
doi = {10.1145/3106426.3106487},
abstract = {We present CEST, a generic method for detection and rich summarization of events occurring in a city. CEST exploits Twitter metadata, does not need prior information on events, and is event category and structure agnostic. We developed CEST to process unstructured documents and take advantage of shorthand notations, hashtags, keywords, geographical and temporal data, as well as sentiment within tweets to both detect and summarize arbitrary events without prior knowledge. We also introduce a novel strategy that analyzes sentiment and tweeting behavior over time to create a qualitative score that captures events' overall appeal to attendees.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {615–622},
numpages = {8},
keywords = {spatio-temporal analysis, summarization, event, Twitter},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106470,
author = {Dong, Guangchang and Chen, Jianhui and Wang, Haiyuan and Zhong, Ning},
title = {A Narrow-Domain Entity Recognition Method Based on Domain Relevance Measurement and Context Information},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106470},
doi = {10.1145/3106426.3106470},
abstract = {Entity recognition is the basis of text mining. With the further development of knowledge-driven applications, types of target entities are increasingly subdivided. The lack of corpus and the limited number of entity have been the main challenges of entity recognition. Based on this observation, this paper proposes a weak-supervision method for recognizing entities from a specifically narrow domain by fusing domain relevance measurement and context information. The experimental result shows that the proposed method has high efficiency and accuracy without manual participation.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {623–628},
numpages = {6},
keywords = {entity recognition, weak supervision, context information, domain relevance measurement},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106437,
author = {Zhao, Kui and Li, Bangpeng and Peng, Zilun and Bu, Jiajun and Wang, Can},
title = {Navigation Objects Extraction for Better Content Structure Understanding},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106437},
doi = {10.1145/3106426.3106437},
abstract = {Existing works for extracting navigation objects from webpages focus on navigation menus, so as to reveal the information architecture of the site. However, web 2.0 sites such as social networks, e-commerce portals etc. are making the understanding of the content structure in a web site increasingly difficult. Dynamic and personalized elements such as top stories, recommended list in a webpage are vital to the understanding of the dynamic nature of web 2.0 sites. To better understand the content structure in web 2.0 sites, in this paper we propose a new extraction method for navigation objects in a webpage. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-specific navigation lists. Since the navigation objects in a webpage naturally come in blocks, we first cluster hyperlinks into different blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density. Then we identify navigation objects from those blocks using the SVM classifier with novel features such as anchor text lengths etc. Experiments on real-world data sets with webpages from various domains and styles verified the effectiveness of our method.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {629–636},
numpages = {8},
keywords = {information extraction, web structure mining, navigation objects},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106433,
author = {Ko, Man-Chun and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {Paid Review and Paid Writer Detection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106433},
doi = {10.1145/3106426.3106433},
abstract = {There has been a surge in opinion-sharing in the public domain. Some opinions greatly influence our decisions, e.g., the choice of purchase. Malicious parties or individuals exploit social media by generating fake reviews for opinion manipulation. This paper aims to investigate the phenomenon of online paid restaurant reviews by bloggers. Our research provides an insight into some characteristics of paid reviews and their authors. We then explore a set of features based on our observations and detect paid reviews and paid bloggers using supervised machine learning techniques. Experimental results show the effectiveness of our approach.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {637–645},
numpages = {9},
keywords = {fake review, blogging, paid review, opinion spam},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106490,
author = {Wijeratne, Sanjaya and Balasuriya, Lakshika and Sheth, Amit and Doran, Derek},
title = {A Semantics-Based Measure of Emoji Similarity},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106490},
doi = {10.1145/3106426.3106490},
abstract = {Emoji have grown to become one of the most important forms of communication on the web. With its widespread use, measuring the similarity of emoji has become an important problem for contemporary text processing since it lies at the heart of sentiment analysis, search, and interface design tasks. This paper presents a comprehensive analysis of the semantic similarity of emoji through embedding models that are learned over machine-readable emoji meanings in the EmojiNet knowledge base. Using emoji descriptions, emoji sense labels and emoji sense definitions, and with different training corpora obtained from Twitter and Google News, we develop and test multiple embedding models to measure emoji similarity. To evaluate our work, we create a new dataset called EmoSim508, which assigns human-annotated semantic similarity scores to a set of 508 carefully selected emoji pairs. After validation with EmoSim508, we present a real-world use-case of our emoji embedding models using a sentiment analysis task and show that our models outperform the previous best-performing emoji embedding model on this task. The EmoSim508 dataset and our emoji embedding models are publicly released with this paper and can be downloaded from http://emojinet.knoesis.org/.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {646–653},
numpages = {8},
keywords = {semantic similarity, emoji analysis and search, emoji similarity},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106440,
author = {Alharbi, Abdullah Semran and Li, Yuefeng and Xu, Yue},
title = {Topical Term Weighting Based on Extended Random Sets for Relevance Feature Selection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106440},
doi = {10.1145/3106426.3106440},
abstract = {It is challenging to discover relevant features from long documents that describe user information needs due to the nature of text where synonymy, polysemy noise, and high dimensionality are inherited problems. Traditional feature selection methods could not effectively deal with these problems, because they assume that documents describe one topic only. Topic-based techniques, such as Latent Dirichlet Allocation (LDA), relax this assumption. They have been developed on the basis that a document can exhibit multiple hidden topics. However, LDA does not show encouraging results in selecting relevant features, because LDA calculates the weight of terms based on their local documents and does not generalise it globally at the collection level. So as to address this problem, we propose an innovative and effective extended random set model to generalise LDA weight for local document terms. The model is used as a weighting scheme for topical terms. It can assign a more discriminately accurate weight to these terms based on their appearance in LDA topics and relevant documents. The experimental results, based on the standard RCV1 dataset, TREC topics, and five standard performance measures, show that the proposed model significantly outperforms eight state-of-the-art baseline models in information filtering.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {654–661},
numpages = {8},
keywords = {extended random set, feature selection, latent dirichlet allocation, term weighting, text mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106542,
author = {Leung, Carson K. and Jiang, Fan and Pazdor, Adam G. M.},
title = {Bitwise Parallel Association Rule Mining for Web Page Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106542},
doi = {10.1145/3106426.3106542},
abstract = {For many real-life web applications, web surfers would like to get recommendation on which collections of web pages that would be interested to them or that they should follow. In order to discover this information and make recommendation, data mining---and specially, association rule mining or web mining---is in demand. Since its introduction, association rule mining has drawn attention of many researchers. Consequently, many association rule mining algorithms have been proposed for finding interesting relationships---in the form of association rules---among frequently occurring patterns. These algorithms include level-wise Apriori-based algorithms, tree-based algorithms, hyperlinked array structure based algorithms, and vertical mining algorithms. While these algorithms are popular, they suffer from some drawbacks. Moreover, as we are living in the era of big data, high volumes of a wide variety of valuable data of different veracity collected at a high velocity post another challenges to data science and big data analytics. To deal with these big data while avoiding the drawbacks of existing algorithms, we present a bitwise parallel association rule mining system for web mining and recommendation in this paper. Evaluation results show the effectiveness and practicality of our parallel algorithm---which discovers popular pages on the web, which in turn gives the web surfers recommendation of web pages that might be interested to them---in real-life web applications.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {662–669},
numpages = {8},
keywords = {knowledge discovery, web mining, frequent patterns, data mining, web intelligence, association rules},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106540,
author = {Silva, Gustavo R. Lacerda and de Oliveira, Lucas Machado and de Medeiros, Rafael Ribeiro and Goussevskaia, Olga and Benevenuto, Fabr\'{\i}cio},
title = {Characterizing Internet Radio Stations at Scale},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106540},
doi = {10.1145/3106426.3106540},
abstract = {In this paper we build and characterize a large-scale dataset of internet radio streams. More than 25 million snapshots of more than 75 thousand different radio stations were collected from the SHOUTcast service between December 2016 and April 2017. We characterized several attributes of the dataset, such as audience and music genre distributions among radio stations, advertisement and seasonal content dynamics, as well as bit rates and media formats of the radio streams. Finally, we analyzed to which extent these features affect audience size. We hope these and the other findings of our study may provide valuable information for content personalization and better advertisement placement in internet radio streams.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {670–677},
numpages = {8},
keywords = {internet radio characterization, web content mining, radio popularity, multimedia data mining, internet radio streams},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106534,
author = {Bahrami, Ramazan Ali and Gulati, Jayati and Abulaish, Muhammad},
title = {Efficient Processing of SPARQL Queries over GraphFrames},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106534},
doi = {10.1145/3106426.3106534},
abstract = {With the advent of huge data management systems storing voluminous data, there arises a need to develop efficient data analytics techniques for knowledge discovery at different levels of granularity. Resource Description Framework (RDF), mainly developed for Semantic Web, is presumably a good option when considering graph databases dealing with huge real-world data. RDF models information in the form of triples <subject, predicate, object>, and is considered as a useful tool to store graph data (aka linked data) where each edge can be stored as a triple. Due to existence of huge amount of linked data, mostly in the form of graphs, graph mining has been successful in attracting researchers from different research fields for efficient handling (storage, indexing, retrieval, etc.) of graph data. As a result, various APIs like GraphX and GraphFrames are developed to facilitate relational queries over graph data. Though GraphX is older than GraphFrames and processing SPARQL queries over GraphX has been explored by some researchers, to the best of our knowledge, SPARQL query processing over GraphFrames has not been explored yet. In this paper, we present an initial study on query-specific search space pruning and query optimization approach to process SPARQL queries over GraphFrames in an efficient manner. The experimental results, in terms of low response time for query execution, are encouraging, and give way to invest more research efforts in this direction.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {678–685},
numpages = {8},
keywords = {graph mining, graphframes, GraphX, SPARQL query processing, linked data mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106546,
author = {Rahman, Md Mostafizur and Takasu, Atsuhiro},
title = {Entity Oriented Action Recommendations for Actionable Knowledge Graph Generation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106546},
doi = {10.1145/3106426.3106546},
abstract = {Popular search engines have recently utilized the power of knowledge graphs (KGs) to provide specific answers to queries in a direct way. Search engine result pages (SERPs) are expected to provide facts in response to queries that satisfy semantic meaning. This encourages researchers to propose more influential knowledge graph generation techniques. To achieve and advance the technologies related to actionable knowledge graph presentation, creating action recommendations (ARs) is an essential step and a relatively new research direction to nurture research on generating KGs that are optimized for facilitating an entity's actions. An action represents the physical or mental activity of an entity. For example, for the entity "Donald J. Trump", typical potential actions could be "won the US presidential election" or "targets US journalists". In this paper, we describe the generation of relevant action recommendations based on entity instance and entity type. We propose two models that employ different approaches. Our first model exploits semisupervised learning and we introduce entity context vector (ECV) as an entity's distinguishing features for capturing the context of entities to reveal the similarity between entities, grounded on the prominent word2vec model. The second model is a probabilistic approach based on the Naive Bayes Theorem. We extensively evaluate our proposed models. Our first model significantly outperforms probabilistic and supervised learning-based models.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {686–693},
numpages = {8},
keywords = {actionable knowledge graph, action recommendations},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106520,
author = {Hirokawa, Sachio and Suzuki, Takahiko and Mine, Tsunenori},
title = {Machine Learning is Better than Human to Satisfy Decision by Majority},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106520},
doi = {10.1145/3106426.3106520},
abstract = {Government 2.0 activities have become very attractive and popular these days. Using platforms to support the activities, anyone can anytime report issues or complaints in a city with their photographs and geographical information on the Web, and share them with other people. Since a variety of reports are posted, officials in the city management section have to check the importance of each report and sort out their priorities to the reports. However, it is not easy task to judge the importance of the reports. When several officials work on the task, the agreement rate of their judgments is not always high. Even if the task is done by only one official, his/her judgment sometimes varies on a similar report. To remedy this low agreement rate problem of human judgments, we propose a method of detecting signs of danger or unsafe problems described in citizens' reports. The proposed method uses a machine learning technique with word feature selection. Experimental results clearly explain the low agreement rate of human judgments, and illustrate that the proposed machine learning method has much higher performance than human judgments.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {694–701},
numpages = {8},
keywords = {classification, machine learning, support vector machine, feature selection, government 2.0},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106504,
author = {Yokobayashi, Ryohei and Miura, Takao},
title = {Modeling Random Projection for Tensor Objects},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106504},
doi = {10.1145/3106426.3106504},
abstract = {In this investigation, we discuss high order data structure (called tensor) for efficient information retrieval and show especially how well reduction techniques of dimensionality goes while preserving Euclid distance between information. High order data structure requires much amount of space. One of the effective approaches comes from dimensionality reduction such as Latent Semantic Indexing (LSI) and Random Projection (RP) which allows us to reduce complexity of time and space dramatically. The reduction techniques can be applied to high order data structure. Here we examine High Order Random Projection (HORP) which provides us with efficient information retrieval keeping feasible dimensionality reduction.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {702–708},
numpages = {7},
keywords = {dimensionality reduction, tensor, HORP, random projection},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106517,
author = {dos Santos, Henrique D. P. and Woloszyn, Vinicius and Vieira, Renata},
title = {Portuguese Personal Story Analysis and Detection in Blogs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106517},
doi = {10.1145/3106426.3106517},
abstract = {Diary-like content expressing authors personal experiences and sentiments over a variety of topics is generated every day and made available on the Internet. This rich content can be used for psychological analysis and knowledge discovery regarding human related issues in several ways. This paper presents the creation of a Brazilian Portuguese corpus, using blog posts, for personal stories analyses and detection. We present an analysis of psycholinguistic categories across personal story and non-story posts, discussing their similarities and differences. We also study the use of these psycholinguistic categories as classifying features. Then we describe the evaluation of several machine learning approaches and the process of applying them to identify personal stories on the basis of our dataset. Finally, we investigate the main topic-related polarity of personal narratives posts.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {709–715},
numpages = {7},
keywords = {social media, psycholinguistic, personal story, natural language processing, corpus},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106521,
author = {Keshi, Ikuo and Suzuki, Yu and Yoshino, Koichiro and Nakamura, Satoshi},
title = {Semantically Readable Distributed Representation Learning for Social Media Mining},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106521},
doi = {10.1145/3106426.3106521},
abstract = {The problem with distributed representations generated by neural networks is that the meaning of the features is difficult to understand. We propose a new method that gives a specific meaning to each node of a hidden layer by introducing a manually created word semantic vector dictionary into the initial weights and by using paragraph vector models. Our experimental results demonstrated that weights obtained based on learning and weights based on the dictionary are more strongly correlated in a closed test and more weakly correlated in an open test, compared with the results of a control test. Additionally, we found that the learned vector are better than the performance of the existing paragraph vector in the evaluation of the sentiment analysis task. Finally, we determined the readability of document embedding in a user test. The definition of readability in this paper is that people can understand the meaning of large weighted features of distributed representations. A total of 52.4% of the top five weighted hidden nodes were related to tweets where one of the paragraph vector models learned the document embedding. Because each hidden node maintains a specific meaning, the proposed method succeeds in improving readability.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {716–722},
numpages = {7},
keywords = {paragraph vector, semantic lexicon, semantic vector, sentiment analysis, word2vec, Twitter, distributed representation learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106525,
author = {Al-Dhelaan, Mohammed and Al-Suhaim, Abeer},
title = {Sentiment Diversification for Short Review Summarization},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106525},
doi = {10.1145/3106426.3106525},
abstract = {With the abundance of reviews published on the Web about a given product, consumers are looking for ways to view major opinions that can be presented in a quick and succinct way. Reviews contain many different opinions, making the ability to show a diversified review summary that focus on coverage and diversity a major goal. Most review summarization work focuses on showing salient reviews as a summary which might ignore diversity in summaries. In this paper, we present a graph-based algorithm that is capable of producing extractive summaries that are both diversified from a sentiment point of view and topically well-covered. First, we use statistical measures to find topical words. Then we split the dataset based on the sentiment class of the reviews and perform the ranking on each sentiment graph. When compared with different baselines, our approach scores best in most ROUGE metrics. Specifically, our approach shows improvements of 3.9% in ROUGE-1 and 1.8% in ROUGE-L in comparison with the best competing baseline.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {723–729},
numpages = {7},
keywords = {text summarization, sentiment-aware summarization, review summarization, opinion summarization},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106541,
author = {Cort\'{e}s, V\'{\i}ctor D. and Vel\'{a}squez, Juan D. and Ib\'{a}\~{n}ez, Carlos F.},
title = {Twitter for Marijuana Infodemiology},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106541},
doi = {10.1145/3106426.3106541},
abstract = {Today online social networks seem to be good tools to quickly monitor what is going on with the population, since they provide environments where users can freely share large amounts of information related to their own lives. Due to well known limitations of surveys, this novel kind of data can be used to get additional real time insights from people to understand their actual behavior related to drug use. The aim of this work is to make use of text messages (tweets) and relationships between Chilean Twitter users to predict marijuana use among them. To do this we collected Twitter accounts using a location-based criteria, and built a set of features based on tweets they made and ego centric network metrics. To get tweet-based features, tweets were filtered using marijuana-related keywords and a set of 1000 tweets were manually labeled to train algorithms capable of predicting marijuana use in tweets. In addition, a sentiment classifier of tweets was developed using the TASS corpus. Then, we made a survey to get real marijuana use labels related to accounts and these labels were used to train supervised machine learning algorithms. The marijuana use per user classifier had precision, recall and F-measure results close to 0.7, implying significant predictive power of the selected variables. We obtained a model capable of predicting marijuana use of Twitter users and estimating their opinion about marijuana. This information can be used as an efficient (fast and low cost) tool for marijuana surveillance, and support decision making about drug policies.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {730–736},
numpages = {7},
keywords = {text mining, marijuana, web content mining, social network analysis, opinion mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106543,
author = {VanDam, Courtland and Tang, Jiliang and Tan, Pang-Ning},
title = {Understanding Compromised Accounts on Twitter},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106543},
doi = {10.1145/3106426.3106543},
abstract = {Social media has become a valuable tool for hackers to disseminate misinformation through compromised accounts. A compromised account is an account accessed by a third party without the user's knowledge. Previous studies have found 13% of online adults experienced their social media accounts compromised. Since compromised accounts can have a significant adverse impact on the social media sites, this has led to the growing research on detecting compromised accounts. However, previous works are limited as they either focus on the detection of hacked accounts for spamming and phishing activities or utilize only twitter content information. In this paper, we performed a systematic study on compromised accounts in Twitter by identifying who compromise the accounts; what information they share, and what patterns their tweets present. Our findings suggest that the accounts can be compromised by two different types of hackers and the content they post tend to follow several common themes. We also showed that, in addition to the text content of the tweets, there are other meta-information that can be exploited to help improve the detection of compromised accounts.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {737–744},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106431,
author = {Geurts, Tomas and Frasincar, Flavius},
title = {Addressing the Cold User Problem for Model-Based Recommender Systems},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106431},
doi = {10.1145/3106426.3106431},
abstract = {Customers of a webshop are often presented large assortments, which can lead to customers struggling finding their desired product(s), an issue known as choice overload. In order to overcome this issue, recommender systems are used in webshops to provide personalized product recommendations to customers. Though, recommender systems using matrix factorization are not able to provide recommendations to new customers (i.e., cold users). To facilitate recommendations to cold users we investigate multiple active learning strategies, and subsequently evaluate which active learning strategy is able to optimally elicit the preferences from the cold users. Our model is empirically validated using a dataset from the webshop of de Bijenkorf, a Dutch department store. We find that the overall best-performing active learning strategy is PopGini, an active learning strategy which combines the popularity of an item with its Gini impurity score.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {745–752},
numpages = {8},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106466,
author = {Zheng, Yong},
title = {Context Suggestion: Empirical Evaluations vs User Studies},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106466},
doi = {10.1145/3106426.3106466},
abstract = {Recommender System has been successfully applied to assist user's decision making by providing a list of recommended items. Context-aware recommender system additionally incorporates contexts (such as time and location) into the system to improve the recommendation performance. The development of context-aware recommender systems brings a new opportunity - context suggestion which refers to the task of recommending appropriate contexts to the users to improve user experience. In this paper, we explore the question whether user's contextual ratings can be reused to produce context suggestions. We propose two evaluation mechanisms for context suggestion, and empirically compare direct context predictions and indirect context suggestions based on a movie data that was collected from user studies. The experimental results reveal that indirect context suggestion works better than the direct context prediction, and tensor factorization is the best approach to produce context suggestions in our movie data.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {753–760},
numpages = {8},
keywords = {recommender systems, context-aware, context suggestion, context},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106439,
author = {Johnson, Joseph and Ng, Yiu-Kai},
title = {Enhancing Long Tail Item Recommendations Using Tripartite Graphs and Markov Process},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106439},
doi = {10.1145/3106426.3106439},
abstract = {Given that the Internet and sophisticated transportation networks have made an increasingly huge number of products and services available to the public, consumers are unable to identify, much less evaluate the usefulness of, such goods accessible to them. Modern recommendation systems filter out products of lesser utility to the customer, showcasing those items of higher preference to the user. While current state-of-the-art recommendation systems perform fairly well, they generally do better at recommending the popular subset of all products available rather than matching consumers with the vast amount of niche products in what has been termed the "Long Tail". In their seminal work, "Challenging the Long Tail Recommendation", Yin et al. make an eloquent argument that the long tail is where organizations can create the most value for their consumers. They also argue that existing recommender systems operate fundamentally different for long tail products than for mainstream goods. While matrix factorization, nearest-neighbors, and clustering work well for the "head" market, the long tail is better represented by a graph, specifically a bipartite graph that connects a set of users to a set of goods. In this paper, we discuss the algorithms presented by Yin et al., as well as a set of similar algorithms proposed by Shang et al., which traverse the bipartite graphs through a random walker in order to identify similar users and products. We build on elements from each work, as well as elements from a Markov process, to facilitate the random walker's traversal of tripartitle graphs into the long tail regions. This method specifically constructs paths into regions of the long tail that are favorable to users.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {761–768},
numpages = {8},
keywords = {Markov process, long tail recommendation, tripartite graphs},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106462,
author = {Imrattanatrai, Wiradee and Kato, Makoto P. and Tanaka, Katsumi},
title = {Entity Search by Leveraging Attributive Terms in Sentential Queries over RDF Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106462},
doi = {10.1145/3106426.3106462},
abstract = {This paper proposes methods of finding a ranked list of entities using RDF data for a given sentential query (e.g. "Cars 3", "Toy Story 4", or "The Incredibles 2" for the query "upcoming animated films pixar") by leveraging different types of modifiers in the query through identifying corresponding properties (e.g. released and movie type for the modifiers "upcoming" and "animated", respectively). While major search engines provide the entity search functionality that returns a list of entities based on users' queries, entities are neither presented for a wide variety of search queries, nor in the order that users expect. To enhance the efficiency of entity search, we propose two entity ranking methods. Our first proposed method is a Web-based entity ranking that directly finds highly relevant entities from Web search results returned in response to the query as a whole, and propagates the estimated relevance to the other entities. The second proposed method is a property-based entity ranking that ranks entities based on properties corresponding to modifier terms in the query. To this end, we propose a novel method that identifies a set of relevant properties based on the combination of the frequency of property values containing the modifier, co-occurrence of the modifier and property names, and difference in property value distributions of entities in the search results for a query. The experimental results showed that our proposed property identification method could predict more relevant properties than using each criterion separately. Moreover, we achieved the best performance for returning a ranked list of relevant entities when using both of the Web-based and property-based entity ranking methods.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {769–776},
numpages = {8},
keywords = {property identification, knowledge base, entity ranking},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106479,
author = {Amami, Maha and Faiz, Rim and Stella, Fabio and Pasi, Gabriella},
title = {A Graph Based Approach to Scientific Paper Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106479},
doi = {10.1145/3106426.3106479},
abstract = {When looking for recently published scientific papers, a researcher usually focuses on the topics related to her/his scientific interests. The task of a recommender system is to provide a list of unseen papers that match these topics. The core idea of this paper is to leverage the latent topics of interest in the publications of the researchers, and to take advantage of the social structure of the researchers (relations among researchers in the same field) as reliable sources of knowledge to improve the recommendation effectiveness. In particular, we introduce a hybrid approach to the task of scientific papers recommendation, which combines content analysis based on probabilistic topic modeling and ideas from collaborative filtering based on a relevance-based language model. We conducted an experimental study on DBLP, which demonstrates that our approach is promising.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {777–782},
numpages = {6},
keywords = {scientific paper recommendation, language modeling, hybrid approaches, LDA},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106478,
author = {Jing, Xia and Tang, Jie},
title = {Guess You like: Course Recommendation in MOOCs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106478},
doi = {10.1145/3106426.3106478},
abstract = {Recommending courses to online students is a fundamental and also challenging issue in MOOCs. Not exactly like recommendation in traditional online systems, students who enrolled the same course may have very different purposes and with very different backgrounds. For example, one may want to study "data mining" after studying the course of "big data analytics" because the former is a prerequisite course of the latter, while some other may choose "data mining" simply because of curiosity.Employing the complete data from XuetangX1, one of the largest MOOCs in China, we conduct a systematic investigation on the problem of student behavior modeling for course recommendation. We design a content-aware algorithm framework using content based users' access behaviors to extract user-specific latent information to represent students' interest profile. We also leverage the demographics and course prerequisite relation to better reveal users' potential choice. Finally, we develop a course recommendation algorithm based on the user interest, demographic profiles and course prerequisite relation using collaborative filtering strategy. Experiment results demonstrate that the proposed algorithm performs much better than several baselines (over 2X by MRR). We have deployed the recommendation algorithm onto the platform XuetangX as a new feature, which significantly helps improve the course recommendation performance (+24.6% by click rate) comparing with the recommendation strategy previously used in the system.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {783–789},
numpages = {7},
keywords = {course recommendation, MOOCs, personalization},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106475,
author = {Wu, Qiong and Liu, Siyuan and Miao, Chunyan},
title = {Modeling Uncertainty Driven Curiosity for Social Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106475},
doi = {10.1145/3106426.3106475},
abstract = {Most of the current recommender systems focus on estimating user preferences. However, a person's interest in an item is not determined by his/her preference alone. Psychological research has shown that curiosity is a critical motivation relating to a person's interests and driving explorative behaviours. Motivated as above, we aim to model user curiosity in social recommendation context. In this work, we model uncertainty driven curiosity, wherein uncertainty is a well acknowledged factor that stimulates human curiosity. We model user uncertainty based on two well-known theories of uncertainty, i.e., Shannon entropy and Damster-Shafter theory. Then, we rank items by consolidating both user preference and user uncertainty using weighted Borda count. The proposed model is evaluated with two large-scale real world datasets, Douban and Flixster. The experimental results highlight that uncertainty driven curiosity has a positive impact on personalized ranking, by remarkably improving recommendation precision and diversity.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {790–798},
numpages = {9},
keywords = {diversity, uncertainty, coverage, precision, curiosity, social recommendation},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106481,
author = {Chaa, Messaoud and Nouali, Omar and Bellot, Patrice},
title = {New Technique to Deal with Verbose Queries in Social Book Search},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106481},
doi = {10.1145/3106426.3106481},
abstract = {Verbose query reduction and query term weighting are automatic techniques to deal with verbose queries. The objective is either to assign an appropriate weight to query terms according to their importance in the topic, or outright remove unsuitable terms from the query and keep only the suitable terms to the topic and user's need. These techniques improve performance and provide good results for ad hoc information retrieval. In this paper we propose a new approach to deal with long verbose queries in Social Information Retrieval (SIR) by taking Social Book Search as an example. In this approach, a new statistical measure was introduced to reduce and weight terms of verbose queries. Next, we expand the query by exploiting the similar books mentioned by users in their queries. We find that the proposed approach improves significantly the results.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {799–806},
numpages = {8},
keywords = {Tf.Iqf, query term weighting, quey expansion, social book search, stop-word list, verbose query reduction},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106442,
author = {Ghosh, Krishnendu and Bhowmick, Plaban Kumar and Goyal, Pawan},
title = {Using Re-Ranking to Boost Deep Learning Based Community Question Retrieval},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106442},
doi = {10.1145/3106426.3106442},
abstract = {The current study presents a two-stage question retrieval approach which, in the first phase, retrieves similar questions for a given query using a deep learning based approach and in the second phase, re-ranks initially retrieved questions on the basis of inter-question similarities. The suggested deep learning based approach is trained using several surface features of texts and the associated weights are pre-trained using a deep generative model for better initialization. The proposed retrieval model outperforms standard baseline question retrieval approaches. The proposed re-ranking approach performs inference over a similarity graph constructed with the initially retrieved questions and re-ranks the questions based on their similarity with other relevant questions. Suggested re-ranking approach significantly improves the precision for the retrieval task.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {807–814},
numpages = {8},
keywords = {question retrieval, re-ranking, community question answering},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106535,
author = {Zheng, Yong},
title = {Affective Prediction by Collaborative Chains in Movie Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106535},
doi = {10.1145/3106426.3106535},
abstract = {Recommender systems have been successfully applied to alleviate the information overload and assist user's decision makings. Emotional states have been demonstrated as effective factors in recommender systems. However, how to collect or predict a user's emotional state becomes one of the challenges to build affective recommender systems. In this paper, we explore and compare different solutions to predict emotions to be applied in the recommendation process. More specifically, we propose an approach named as collaborative chains. It predicts emotional states in a collaborative way and additionally takes correlations among emotions into consideration. Our experimental results based on a movie rating data demonstrate the effectiveness of affective prediction by collaborative chains in movie recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {815–822},
numpages = {8},
keywords = {recommender systems, emotion, context-aware, affective computing, collaborative chains},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106527,
author = {Kataoka, Daisuke and Kato, Makoto P. and Yamamoto, Takehiro and Ohshima, Hiroaki and Tanaka, Katsumi},
title = {Context-Aware Relevance Feedback over SNS Graph Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106527},
doi = {10.1145/3106426.3106527},
abstract = {This study proposes a method for retrieving and ranking posts from social network services(SNSs) by specifying and providing feedback on the context of posts. Current search systems for SNS posts cannot handle user intent with regard to the context of posts to be retrieved, mainly owing to the incompleteness of SNS posts, i.e., they do not contain the users' contexts (e.g., situations or preferences) of users posting messages. Hence, we propose a search method that accepts two kinds of queries, namely, content queries and context queries, and that updates these queries based on the user feedback with special attention to the contexts of posts. Our search method considers the whole SNS dataset as a graph and the nodes surrounding each post as its context; to find relevant posts in terms of content and context, our method propagates user feedback via this graph. Our experimental results based on a Twitter test collection revealed that our proposed method showed improved retrieval performance as compared with conventional SNS retrieval and relevance feedback. In addition, we could detect the optimal parameters for feedback propagating.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {823–830},
numpages = {8},
keywords = {query expansion, relevance feedback, context},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106528,
author = {Oppokhonov, Shokirkhon and Park, Seyoung and Ampomah, Isaac K. E.},
title = {Current Location-Based next POI Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106528},
doi = {10.1145/3106426.3106528},
abstract = {Availability of large volume of community contributed location data enables a lot of location providing services and these services have attracted many industries and academic researchers by its importance. In this paper we propose the new recommender system that recommends the new POI for next hours. First we find the users with similar check-in sequences and depict their check-in sequences as a directed graph, then find the users current location. To recommend the new POI recommendation for next hour we refer to the directed graph we have created. Our algorithm considers both the temporal factor i.e., recommendation time, and the spatial(distance) at the same time. We conduct an experiment on random data collected from Foursquare and Gowalla. Experiment results show that our proposed model outperforms the collaborative-filtering based state-of-the-art recommender techniques.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {831–836},
numpages = {6},
keywords = {sequential check-ins, location-based social networks, point-of-interest, directed graph},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106547,
author = {Liu, Ying and Yang, Jiajun},
title = {A Novel Learning-to-Rank Based Hybrid Method for Book Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106547},
doi = {10.1145/3106426.3106547},
abstract = {Recommendation system is able to recommend items that are likely to be preferred by the user. Hybrid recommender systems combine the advantages of the collaborative filtering and content-based filtering for improved recommendation. Hybrid recommendation methods use as many significant factors as possible to generate recommendation, which is practically very functional in real scenarios. However, such method has not been applied to book recommendation yet. Thus, in this paper, we propose a set of novel features which can be categorized into three types: latent features, derived features and content features. These features can be combined to form a new hybrid feature vector containing rating information and content information. Then, we adopted learning-to-rank to use the proposed feature vector as the input for book recommendation. Collaborative Ranking (CR) and Probabilistic Matrix Factorization (PMF) are compared with our proposed method. The experimental results show that the proposed method outperforms CR and PMF. It shows that, on NDCG@1, PMF achieves 0.713818, CR achieves 0.690072 vs. our method achieves 0.742689 which is 4.04% over PMF and 7.62% over CR.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {837–842},
numpages = {6},
keywords = {ranking-based recommendation, recommendation system, hybrid recommendation, collaborative filtering},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106522,
author = {Sommer, Felix and Lecron, Fabian and Fouss, Fran\c{c}ois},
title = {Recommender Systems: The Case of Repeated Interaction in Matrix Factorization},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106522},
doi = {10.1145/3106426.3106522},
abstract = {This work presents a new matrix factorization recommender system approach, that takes repeated interaction into account. We analyze if and how users' repeated interaction behavior---such as repeat purchases---can be integrated into a recommender system. We develop a method that takes advantage of this additional data dimension that is studied in many other fields to derive useful conclusions. Furthermore, we empirically test our method on real-life retailer data and on the Last.fm dataset. We compare our algorithm with popular matrix factorization approaches. Results indicate that our method manages to slightly outperform the existing methods.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {843–847},
numpages = {5},
keywords = {repeated interaction, recommender systems, matrix factorization, singular value decomposition},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106492,
author = {Labbaci, Hamza and Medjahed, Brahim and Binzagr, Faisal and Aklouf, Youcef},
title = {A Deep Learning Approach for Web Service Interactions},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106492},
doi = {10.1145/3106426.3106492},
abstract = {Predicting Web service interactions such as composition and substitution provides support for developers during mashup design. In this paper, we propose a deep-learning approach for predicting compositions and substitutions. To the best of our knowledge, this work is the first to adopt deep learning for interactions prediction. We use stacked autoencoders to learn latent service features. A deep feed forward neural network leverages the learned features and the history of previous interactions to predict new ones. We conducted extensive experiments on real-world Web services to illustrate the performance of our approach. We show that the use of deep learning achieves a high accuracy level and outperforms existing models such as multi-layer perceptron and support vector machine.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {848–854},
numpages = {7},
keywords = {deep learning, composition, prediction, substitution, web services},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106501,
author = {Ashikawa, Masayuki and Kawamura, Takahiro and Ohsuga, Akihiko},
title = {Crowdsourcing Worker Development Based on Probabilistic Task Network},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106501},
doi = {10.1145/3106426.3106501},
abstract = {Crowdsourcing platforms provide an attractive solution for processing numerous tasks at low cost. However, insufficient quality control remains a major concern. In the present study, we propose a grade-based training method for workers. Our training method utilizes probabilistic networks to estimate correlations between tasks based on workers' records for 18.5 million tasks and then allocates pre-learning tasks to the workers to raise the accuracy of target tasks according to the task correlations. In an experiment, the method automatically allocated 31 pre-learning task categories for 9 target task categories, and after the training of the pre-learning tasks, we confirmed that the accuracy of the target tasks was raised by 7.8 points on average. We thus confirmed that the task correlations can be estimated using a large amount of worker records, and that these are useful for the grade-based training of low-quality workers.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {855–862},
numpages = {8},
keywords = {education, crowdsourcing, bayesian network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106464,
author = {Fontanarava, Julien and Pasi, Gabriella and Viviani, Marco},
title = {An Ensemble Method for the Credibility Assessment of User-Generated Content},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106464},
doi = {10.1145/3106426.3106464},
abstract = {The Social Web supports and fosters social interactions by means of different social media, which allow the spread of the so called User-Generated Content (UGC). In this context, characterized by the absence of trusted third parties that verify the reliability of the sources and the believability of the content generated, the issue of assessing the credibility of the information diffused by means of social media is receiving increasing attention. In the literature, this issue has been mainly tackled as a classification problem; information is categorized into genuine and fake, usually by implementing or applying classifiers that consider multiple kinds of features (mainly textual and non-textual) to be evaluated in terms of credibility.In this article, unlike prior research, textual features are considered separately with respect to other kinds of features during the classification process. In particular, an Ensemble Method that combines the results produced by two text classifiers and the ones returned by another classifier acting on non-textual features is proposed. This allows to have better results with respect to the use of a single classifier on multiple features together. The effectiveness of the Ensemble Method has been assessed in the context of review sites, by means of a labeled dataset gathered from the Yelp.com site, where on-line reviews are already classified as recommended and not recommended.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {863–868},
numpages = {6},
keywords = {classification, social media, credibility, language models, text mining, social web, ensemble learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106448,
author = {Sizov, Sergej},
title = {Mining Ordinal Data under Human Response Uncertainty},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106448},
doi = {10.1145/3106426.3106448},
abstract = {Analysis and interpretation of collective feedback on ordinal scales is an important issue for several disciplines, including social sciences, recommender systems research, marketing, political science, and many others. A "reasonable" model is expected to provide an "explanation" of collective user behaviour. Many existing data mining approaches employ for this purpose probabilistic models, based on distributions and mixtures from a certain parametric family.In real life, users meet their decisions with considerable uncertainty. Its assessment and use in probabilistic models for better interpretation of collective feedback is the key concern of this paper.In doing so, we introduce approaches for gathering individual uncertainty, and discuss their viability and limitations. Consequently, we enrich state of the art response mining models (especially focused on discovery of latent user groups) with uncertainty knowledge, and demonstrate resulting advantages in systematic experiments with real users.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {869–876},
numpages = {8},
keywords = {probabilistic models, collective feedback, user uncertainty, ordinal scales},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106444,
author = {Woloszyn, Vinicius and dos Santos, Henrique D. P. and Wives, Leandro Krug and Becker, Karin},
title = {MRR: An Unsupervised Algorithm to Rank Reviews by Relevance},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106444},
doi = {10.1145/3106426.3106444},
abstract = {The automatic detection of relevant reviews plays a major role in tasks such as opinion summarization, opinion-based recommendation, and opinion retrieval. Supervised approaches for ranking reviews by relevance rely on the existence of a significant, domain-dependent training data set. In this work, we propose MRR (Most Relevant Reviews), a new unsupervised algorithm that identifies relevant revisions based on the concept of graph centrality. The intuition behind MRR is that central reviews highlight aspects of a product that many other reviews frequently mention, with similar opinions, as expressed in terms of ratings. MRR constructs a graph where nodes represent reviews, which are connected by edges when a minimum similarity between a pair of reviews is observed, and then employs PageRank to compute the centrality. The minimum similarity is graph-specific, and takes into account how reviews are written in specific domains. The similarity function does not require extensive pre-processing, thus reducing the computational cost. Using reviews from books and electronics products, our approach has outperformed the two unsupervised baselines and shown a comparable performance with two supervised regression models in a specific setting. MRR has also achieved a significantly superior run-time performance in a comparison with the unsupervised baselines.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {877–883},
numpages = {7},
keywords = {opinion retrieval, relevant reviews, unsupervised algorithm},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106456,
author = {Chen, Hao and Mckeever, Susan and Delany, Sarah Jane},
title = {Presenting a Labelled Dataset for Real-Time Detection of Abusive User Posts},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106456},
doi = {10.1145/3106426.3106456},
abstract = {Social media sites facilitate users in posting their own personal comments online. Most support free format user posting, with close to real-time publishing speeds. However, online posts generated by a public user audience carry the risk of containing inappropriate, potentially abusive content. To detect such content, the straightforward approach is to filter against blacklists of profane terms. However, this lexicon filtering approach is prone to problems around word variations and lack of context. Although recent methods inspired by machine learning have boosted detection accuracies, the lack of gold standard labelled datasets limits the development of this approach. In this work, we present a dataset of user comments, using crowdsourcing for labelling. Since abusive content can be ambiguous and subjective to the individual reader, we propose an aggregated mechanism for assessing different opinions from different labellers. In addition, instead of the typical binary categories of abusive or not, we introduce a third class of 'undecided' to capture the real life scenario of instances that are neither blatantly abusive nor clearly harmless. We have performed preliminary experiments on this dataset using best practice techniques in text classification. Finally, we have evaluated the detection performance of various feature groups, namely syntactic, semantic and context-based features. Results show these features can increase our classifier performance by 18% in detection of abusive content.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {884–890},
numpages = {7},
keywords = {abusive detection, feature selection, labelling strategy, machine learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106506,
author = {Sizov, Sergej},
title = {Comparative Assessment of Rating Prediction Techniques under Response Uncertainty},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106506},
doi = {10.1145/3106426.3106506},
abstract = {An objective assessment of collaborative filtering techniques and recommender systems requires application of suitable predictive accuracy metrics. In real life, individuals meet their decisions with considerable uncertainty. This raises the question to what extent the comparison between observed and predicted user responses can be seen as an evident proof of systematic quality differences. In this paper, we accordingly justify underlying assumptions of quality assessment, introduce an appropriate uncertainty-aware evaluation strategy for recommender comparisons, and demonstrate its feasibility and consistency in experiments with real users.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {891–898},
numpages = {8},
keywords = {recommender systems, quality assessment, human uncertainty},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106539,
author = {Nielek, Radoslaw and Lutosta\'{n}ska, Marta and Kope\'{c}, Wies\l{}aw and Wierzbicki, Adam},
title = {Turned 70? It is Time to Start Editing Wikipedia},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106539},
doi = {10.1145/3106426.3106539},
abstract = {Success of Wikipedia would not be possible without the contributions of millions of anonymous Internet users who edit articles, correct mistakes, add links or pictures. At the same time Wikipedia editors are currently overworked and there is always more tasks waiting to be completed than people willing to volunteer. The paper explores the possibility of involving the elderly in the Wikipedia editing process. Older adults were asked to complete various tasks on Wikipedia. Based on the observations made during these activities as well as in-depth interviews, a list of recommendation has been crafted. It turned out that older adults are willing to contribute to Wikiepdia but substantial changes have to be made in the Wikipedia editor.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {899–906},
numpages = {8},
keywords = {Wiki, older adults, content creation, participatory design, Wikipedia},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3106491,
author = {Bhatt, Shreyansh and Minnery, Brandon and Nadella, Srikanth and Bullemer, Beth and Shalin, Valerie and Sheth, Amit},
title = {Enhancing Crowd Wisdom Using Measures of Diversity Computed from Social Media Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106491},
doi = {10.1145/3106426.3106491},
abstract = {"Wisdom of Crowds" (WoC) refers to a form of collective intelligence in which the aggregate judgment of a group of individuals is, in most instances, superior to that of any one group member. For a crowd to be wise, its members must possess diverse knowledge and viewpoints. Such diversity leads to uncorrelated judgment errors that cancel out in aggregate. Yet despite the fact that diversity is known to be an essential ingredient in WoC, little research aims to measure and exploit diversity in human social systems for the purpose of maximizing crowd intelligence. Here we quantify the diversity of a group of individuals through semantic analysis of their social media (Twitter) communications. Focusing on the domain of fantasy sports, we show that virtual crowds of fantasy team owners selected based on the diversity of their tweet content can outperform both non-diverse and randomly sampled crowds. Our results suggest a new approach for intelligent crowd assembly in which measures of diversity extracted from online social media communications can guide the selection of crowd members. These results have implications for numerous domains that utilize aggregated judgments - from consumer reviews, to econometrics, to geopolitical forecasting and intelligence analysis.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {907–913},
numpages = {7},
keywords = {diversity, collective intelligence, semantic analysis, fantasy sports, social media, wisdom of crowds, Twitter},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109418,
author = {Hern\'{a}ndez-Garc\'{\i}a, A. and Fern\'{a}ndez-Mart\'{\i}nez, F. and D\'{\i}az-de-Mar\'{\i}a, F.},
title = {Emotion and Attention: Predicting Electrodermal Activity through Video Visual Descriptors},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109418},
doi = {10.1145/3106426.3109418},
abstract = {This paper contributes to the field of affective video content analysis through the novel employment of electrodermal activity (EDA) measurements as ground truth for machine learning algorithms. The variation of the electrical properties of the skin, known as EDA, is a psychophysiological indicator widely used in medicine, psychology and neuroscience which can be considered a somatic marker of the emotional and attentional reaction of subjects towards stimuli. One of its main advantages is that the recorded information is not biased by the cognitive process of giving an opinion or a score to characterize the subjective perception. In this work, we predict the levels of emotion and attention, derived from EDA records, by means of a small set of low-level visual descriptors computed from the video stimuli. Linear regression experiments show that our descriptors predict significantly well the sum of emotion and attention levels, reaching a coefficient of determination R2 = 0.25. This result sets a promising path for further research on the prediction of emotion and attention from videos using EDA.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {914–923},
numpages = {10},
keywords = {emotion, attention, electrodermal activity, visual descriptors, affective video content analysis},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109421,
author = {Franzoni, Valentina and Milani, Alfredo and Vallverd\'{u}, Jordi},
title = {Emotional Affordances in Human-Machine Interactive Planning and Negotiation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109421},
doi = {10.1145/3106426.3109421},
abstract = {Emotional affordances represent a recently introduced concept which model all the mechanisms used to collect/transmit emotional meaning in the context of human machine interaction. In this work, we introduce and formally define the cognitive role of emotional affordances in a collaboration human-machine dialogue as tools for triggering or recognizing planning-based activities of delegation, goal negotiation, state acquisition, plan prioritization, taking place with the interaction partner. The presented formal model is grounded in an emergency scenario where reacting to emotional affordances or transmitting an emotional content is instrumental to reach the goal of an effective collaborative response. The implementation issues of generation and recognition of emotional affordance are also discussed.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {924–930},
numpages = {7},
keywords = {affective computing, human-machine interaction, emotion recognition, interactive planning, emotional affordance},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109422,
author = {Franzoni, Valentina and Poggioni, Valentina},
title = {Emotional Book Classification from Book Blurbs},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109422},
doi = {10.1145/3106426.3109422},
abstract = {Knowing and predicting opinions of people is considered a strategic added value, interpreting the qualia i.e., the subjective nature of emotional content. The aim of this work is to study the feasibility of an emotion recognition and automated classification of books according to emotional tags, by means of a lexical and semantic analysis of book blurbs. A supervised learning approach is used to determine if a correlation exists between the characteristics of a book blurb and emotional icons associated to the book by users. In this paper the underlying idea of the system is presented, the preprocessing and features extraction phases are described and experimental results on the social network Zazie and its mood tags are discussed.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {931–938},
numpages = {8},
keywords = {sentiment analysis, book classification, emotion recognition, machine learning, automated classification},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109423,
author = {Torres, Juan M. Mayor and Stepanov, Evgeny A.},
title = {Enhanced Face/Audio Emotion Recognition: Video and Instance Level Classification Using ConvNets and Restricted Boltzmann Machines},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109423},
doi = {10.1145/3106426.3109423},
abstract = {Face-based and audio-based emotion recognition modalities have been studied profusely obtaining successful classification rates for arousal/valence levels and multiple emotion categories settings. However, recent studies only focus their attention on classifying discrete emotion categories with a single image representation and/or a single set of audio feature descriptors. Face-based emotion recognition systems use a single image channel representations such as principal-components-analysis whitening, isotropic smoothing, or ZCA whitening. Similarly, audio emotion recognition systems use a standardized set of audio descriptors, including only averaged Mel-Frequency Cepstral coefficients. Both approaches imply the inclusion of decision-fusion modalities to compensate the limited feature separability and achieve high classification rates. In this paper, we propose two new methodologies for enhancing face-based and audio-based emotion recognition based on a single classifier decision and using the EU Emotion Stimulus dataset: (1) A combination of a Convolutional Neural Networks for frame-level feature extraction with a k-Nearest Neighbors classifier for the subsequent frame-level aggregation and video-level classification, and (2) a shallow Restricted Boltzmann Machine network for arousal/valence classification.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {939–946},
numpages = {8},
keywords = {face emotion, audio emotion, ConvNets, RBM, EU emotion stimulus},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109420,
author = {Franzoni, Valentina and Li, Yuanxi and Mengoni, Paolo},
title = {A Path-Based Model for Emotion Abstraction on Facebook Using Sentiment Analysis and Taxonomy Knowledge},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109420},
doi = {10.1145/3106426.3109420},
abstract = {Each term in a short text can potentially convey emotional meaning. Facebook comments and shared posts often convey human biases, which play a pivotal role in information spreading and content consumption. Such bias is at the basis of human-generated content, and capable of conveying contexts which shape the opinion of users through the social media flow of information. Starting from the observation that a separation in topic clusters, i.e. sub-contexts, spontaneously occur if evaluated by human common sense, this work introduces a process for automated extraction of sub-context in Facebook. Basing on emotional abstraction and valence, the automated extraction is exploited through a class of path-based semantic similarity measures and sentiment analysis. Experimental results are obtained using validated clustering techniques on such features, on the domain of information security, over a sample of over 9 million page users. An additional expert evaluation of clusters in tag clouds confirms that the proposed automated algorithm for emotional abstraction clusters Facebook comments compatibly with human common sense. The baseline methods rely on the robust notion of collective concept similarity.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {947–952},
numpages = {6},
keywords = {word similarity, knowledge discovery, data mining, sentiment analysis, collective knowledge, emotional abstraction, semantic distance, artificial intelligence},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109417,
author = {Franzoni, Valentina and Milani, Alfredo and Biondi, Giulio},
title = {SEMO: A Semantic Model for Emotion Recognition in Web Objects},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109417},
doi = {10.1145/3106426.3109417},
abstract = {In this work, we present SEMO, a Semantic Model for Emotion Recognition, which enables users to detect and quantify the emotional load related to basic emotions hidden in short, emotionally rich sentences (e.g. news titles, tweets, captions). The idea of assessing the semantic similarity of concepts by looking at the occurrences and co-occurrences of terms describing them in pages indexed by a search engine can be directly extended to emotions, and to the words expressing them in different languages. The emotional content associated to a particular emotion for a term can thus be estimated using web-based similarity measures, e.g. Confidence, PMI, NGD and PMING, aggregating the distance computed by a model of emotions, e.g. Ekman, Plutchik and Lovheim. Emotions are ranked based on their similarity to the analyzed text, describing each sentence through a vector of values of emotion load, which form the Vector Space Model for the chosen emotion model and similarity measures. The model is tested comparing experimental results to a ground truth in literature. SEMO takes care of both the phases of data collection and data analysis, to produce knowledge to be used in application domains such as social robots, recommender systems, and human-machine interactive systems.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {953–958},
numpages = {6},
keywords = {affective data, sentiment analysis, web document retrieval, artificial intelligence, semantic similarity measures, emotion recognition},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109428,
author = {Shi, Yong and Li, Peijia and Niu, Lingfeng},
title = {Augmented SVM with Ordinal Partitioning for Text Classification},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109428},
doi = {10.1145/3106426.3109428},
abstract = {Ordinal regression has received increasing interest in the past years. It aims to classify patterns by an ordinal scale. With the the explosive growth of data, the method of SVM with ordinal partitioning called SVMOP highlights its advantages due to its convenience of dealing with large scale data. However, the method of SVMOP for ordinal regression has not been exploited much. As we know, the costs should be different when dealing with mislabeled samples and how to use them plays a dominant role in model building. However, L2-loss which could enlarge the cost sensitivity has not been applied into SVM ordinal partition yet. In this paper, we propose the method of SVMOP with L2-loss for ordinal regression. Numerical results show that our approach outperforms the method of SVMOP with L1-loss and other ordianl regression models.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {959–962},
numpages = {4},
keywords = {ordinal regression, cost-sensitive, ordinal partitioning, L2-loss, text classification},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109426,
author = {Jin, Beibei and Yang, Jianing and Huang, Xiangsheng and Khan, Dawar},
title = {Deep Deformable Q-Network: An Extension of Deep Q-Network},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109426},
doi = {10.1145/3106426.3109426},
abstract = {The performance of Deep Reinforcement Learning (DRL) algorithms is usually constrained by instability and variability. In this work, we present an extension of Deep Q-Network (DQN) called Deep Deformable Q-Network which is based on deformable convolution mechanisms. The new algorithm can readily be built on existing models and can be easily trained end-to-end by standard back-propagation. Extensive experiments on the Atari games validate the feasibility and effectiveness of the proposed Deep Deformable Q-Network.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {963–966},
numpages = {4},
keywords = {deep learning, reinforcement learning, deformable convolution layer, deep Q-Network},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109425,
author = {Lai, Lin},
title = {A New Approach for CNYX Prediction Based on SSA and Random Forest},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109425},
doi = {10.1145/3106426.3109425},
abstract = {Over different time horizons, the RMB Effective Exchange Rate Index (CNYX) has certain features, which are conducive for us to analyze the intrinsic features of the series and improve accuracy of the prediction. Based on the SSA and the research subject selected from the closing price of the CNYX from Jan.1, 2016 and the Apr.27, 2017, this paper breaks down the original series into the trend term, market fluctuation term and noise term, and then we introduce these characteristic series into the Random Forest, SVR and ANN to compare their predictive effect respectively and we found that Random Forest outperforms SVR and ANN for the accuracy of predictive value.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {967–970},
numpages = {4},
keywords = {CNYX, combination prediction method, random forest, singular spectrum analysis},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109431,
author = {Nawaz, Falak and Hussain, Omar Khadeer and Janjua, Naeem and Chang, Elizabeth},
title = {A Proactive Event-Driven Approach for Dynamic QoS Compliance in Cloud of Things},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109431},
doi = {10.1145/3106426.3109431},
abstract = {Cloud-of-things service providers use various descriptions languages to describe Quality of Service (QoS) attributes. However, existing modelling approaches provide support for modelling static QoS attributes only and lack features to model and reason with dynamic QoS attributes such as response time and availability. This paper presents an event-based approach for monitoring dynamic QoS values and their compliance by modelling the behavior of QoS attributes using an Event Calculus (EC) based framework. The logic based reasoning is then performed to proactively identify the possible QoS violations in future.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {971–975},
numpages = {5},
keywords = {service degradation, service level agreement, quality of service, cloud of things, compliance},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109424,
author = {Robinson, Jace and Doran, Derek},
title = {Seasonality in Dynamic Stochastic Block Models},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109424},
doi = {10.1145/3106426.3109424},
abstract = {Sociotechnological and geospatial processes exhibit time varying structure that make insight discovery challenging. This paper proposes a new statistical model for such systems, modeled as dynamic networks, to address this challenge. It assumes that vertices fall into one of k types and that the probability of edge formation at a particular time depends on the types of the incident nodes and the current time. The time dependencies are driven by unique seasonal processes, which many systems exhibit (e.g., predictable spikes in geospatial or web traffic each day). The paper defines the model as a generative process and an inference procedure to recover the seasonal processes from data when they are unknown. Evaluation with synthetic dynamic networks show the recovery of the latent seasonal processes that drive its formation.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {976–979},
numpages = {4},
keywords = {kalman filter, dynamic networks, structural time series, state space model},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109427,
author = {Tang, Jingjing and Tian, Yingjie and Wu, Guoqiang and Li, Dewei},
title = {Stochastic Gradient Descent for Large-Scale Linear Nonparallel SVM},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109427},
doi = {10.1145/3106426.3109427},
abstract = {In recent years, nonparallel support vector machine (NPSVM) is proposed as a nonparallel hyperplane classifier with superior performance than standard SVM and existing nonparallel classifiers such as the twin support vector machine (TWSVM). With the perfect theoretical underpinnings and great practical success, NPSVM has been used to dealing with the classification tasks on different scales. Tackling large-scale classification problem is a challenge yet significant work. Although large-scale linear NPSVM model has already been efficiently solved by the dual coordinate descent (DCD) algorithm or alternating direction method of multipliers (ADMM), we present a new strategy to solve the primal form of linear NPSVM different from existing work in this paper. Our algorithm is designed in the framework of the stochastic gradient descent (SGD), which is well suited to large-scale problem. Experiments are conducted on five large-scale data sets to confirm the effectiveness of our method.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {980–983},
numpages = {4},
keywords = {stochastic gradient descent, nonparallel support vector machine, large-scale},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109429,
author = {Wang, Bo and Shi, Yong},
title = {A Study on Error Correction of Multiple Criteria and Multiple Constraint Levels Linear Programming Based Classification},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109429},
doi = {10.1145/3106426.3109429},
abstract = {In credit card client classification problem, reducing misclassified rate is regarded as a key issue. Unfortunately, existing machine learning methods cannot be successfully applied to this problem. This paper introduces a classification model based on multiple criteria and multiple constraint levels linear programming (MC2LP), which equips two intervals of cutoff in the model. Two parallel hyperplanes are employed to indicate the relative positions between the points and hyperplanes. Then, we discuss the correctness of new model in error correction. Matrix representations of relevant models are also offered. Finally, compared to original MCLP, known MC2LP, Logistic Regression (LR) and Support Vector Machine (SVM), the propsed model shows superiority in solving two types of error related data mining problem.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {984–987},
numpages = {4},
keywords = {error correction, multiple criteria decision making, classification},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3120963,
author = {Zitouni, Hanane and Meshoul, Souham and Taouche, Kamel},
title = {Improving Content Based Recommender Systems Using Linked Data Cloud and FOAF Vocabulary},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3120963},
doi = {10.1145/3106426.3120963},
abstract = {With the deluge of data published on the web, it becomes even more difficult for a user to get access to the relevant information based on his preferences. In order to accurately predict the preference a user would give to an item, recommender systems should use an effective information filtering engine. This task can be achieved using content based filtering (CBF) or collaborative filtering or a hybrid approach. This work describes an approach to CBF that aims to deal with the issues of unstructured data and new user on which existing approaches perform poorly. The basic feature of the proposed approach is to incorporate linked data cloud into the information filtering process using a semantic space vector model. FOAF vocabulary is used to define a new distance measure between users based on their FOAF profiles. Unstructured items representations are enhanced by additional attributes extracted from Linked data cloud which alleviates the burden to analyze the content of these items and therefore reduces the computational cost. We report on some promising experiments of the proposed approach performed on MovieLens data sets.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {988–992},
numpages = {5},
keywords = {web intelligence, web of data, FOAF, vector space model, content based filtering, linked data cloud},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3117762,
author = {Wittwer, Matthias and Reinhold, Olaf and Alt, Rainer},
title = {Capturing Customer Context from Social Media: Mapping Social Media API and CRM Profile Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3117762},
doi = {10.1145/3106426.3117762},
abstract = {The evolution of the social web opens a new channel that allows bidirectional electronic interactions directly with customers in real-time. By accessing social media content via application programming interfaces (API), businesses may enrich their information on customers, which are usually represented in customer profiles. However, these profiles are often incomplete since additional meaningful data on the customer's context are missing. Based on this idea, this research in progress paper describes first ideas on how data from social media, which are available through API, may be matched with customer profiles via a customer context model.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {993–997},
numpages = {5},
keywords = {social media, customer context, social CRM},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3117761,
author = {Cirqueira, Douglas and Pinheiro, M\'{a}rcia and Braga, Tha\'{\i}s and Jacob, Antonio and Reinhold, Olaf and Alt, Rainer and Santana, \'{A}damo},
title = {Improving Relationship Management in Universities with Sentiment Analysis and Topic Modeling of Social Media Channels: Learnings from UFPA},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3117761},
doi = {10.1145/3106426.3117761},
abstract = {Online Social networking (OSN) platforms such as Facebook, daily have a massive number of users and content being created. Users of such services have the power to share opinions and influence others. This creates an interesting scenario, where brands and institutions can have a digital presence to interact directly with their target audience. Universities around the world are also using those platforms for reaching out their students and staff, acting as a new channel for services and public relations. Social Customer Relationship Management (SCRM) principles can be applied in this scenario, for universities, and help them on the management of this relationship with their public. Thus, this work aims to apply SCRM methodology for a university Facebook fan page, through techniques of Sentiment Analysis (SA) and topic modeling (TM) using Latent Dirichlet Allocation, in order to find topics people are complimenting or complaining about in their comments. The main goal is to improve SCRM processes through the results and insights provided by those techniques. The final discussion with the communications professionals from UFPA university has revealed the experiments and insights provided in this work are valuable for their social media (SM) management workflow.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {998–1005},
numpages = {8},
keywords = {LDA, sentiment analysis, SCRM, social CRM, topic modeling},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3117760,
author = {Silva, Wendel and Santana, \'{A}damo and Lobato, F\'{a}bio and Pinheiro, M\'{a}rcia},
title = {A Methodology for Community Detection in Twitter},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3117760},
doi = {10.1145/3106426.3117760},
abstract = {The microblogging service Twitter is one of the world's most popular online social networks and assembles a huge amount of data produced by interactions between users. A careful analysis of this data allows identifying groups of users who share similar traits, opinions, and preferences. We call community detection the process of user group identification, which grants valuable insights not available upfront. In order to extract useful knowledge from Twitter data many methodologies have been proposed, which define the attributes to be used in community detection problems by manual and empirical criteria - oftentimes guided by the aimed type of community and what the researcher attaches importance to. However, such approach cannot be generalized because it is well known that the task of finding out an appropriate set of attributes leans on context, domain, and data set. Aiming to the advance of community detection domain, reduce computational cost and improve the quality of related researches, this paper proposes a standard methodology for community detection in Twitter using feature selection methods. Results of the present research directly affect the way community detection methodologies have been applied to Twitter and quality of outcomes produced.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1006–1009},
numpages = {4},
keywords = {feature selection, Twitter, methodology, community detection},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109430,
author = {Koohborfardhaghighi, Somayeh and Altmann, J\"{o}rn and Tserpes, Konstantinos},
title = {Social Analytics Framework for Intelligent Information Systems Based on a Complex Adaptive Systems Approach},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109430},
doi = {10.1145/3106426.3109430},
abstract = {An employee profile record within a human resource management department includes information about the employee's past activities within the enterprise. These profile records are valuable sources of information for any enterprise. Using this information requires an intelligent enterprise information system. In this study, we emphasize the importance of having detailed analyses on the employees' knowledge base within an enterprise by applying dynamic social impact theory. We argue that the richer the knowledge base within an enterprise with respect to its human and social capital is, the more it can empower its employees to be creative and innovative during group works. We propose a framework for effectively modeling the ever-changing knowledge bases of big enterprises for delivering optimal and automated team composition techniques. Our discussions cover the complete pipeline from data management and knowledge modeling, via graph analysis, to decision support services.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1010–1017},
numpages = {8},
keywords = {decision support systems, social capital, heterogeneous information networks, dynamic social impact theory},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109037,
author = {Su, Yuhan and Jin, Zhongming and Chen, Ying and Sun, Xinghai and Yang, Yaming and Qiao, Fangzheng and Xia, Fen and Xu, Wei},
title = {Improving Click-through Rate Prediction Accuracy in Online Advertising by Transfer Learning},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109037},
doi = {10.1145/3106426.3109037},
abstract = {As the main revenue source of Internet companies, online advertising is always a significant topic, where click-through rate (CTR) prediction plays a central role. In online advertising systems, there are often many advertisement products. Due to the competition in the bidding mechanism, some advertising products may get lots of data to train the CTR prediction model while some may lack high-quality data. However, to predict accurate CTR, a large amount of data is needed. Therefore, transfer knowledge from the large product (source) to the small product (target) is necessary. We propose a transfer learning method that iteratively updates the data weights to selectively combine source data with target data for training. To efficiently process huge advertisement data, we design a sampling strategy based on the gradient information, and implement the algorithm with a MapReduce-like machine learning framework. We do experiments on real advertisement datasets. The results show that our approach improves the accuracy of CTR prediction compared to the supervised learning method.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1018–1025},
numpages = {8},
keywords = {transfer learning, CTR prediction, online advertising},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109035,
author = {Marrara, Stefania and Pasi, Gabriella and Viviani, Marco and Cesarini, Mirko and Mercorio, Fabio and Mezzanzanica, Mario and Pappagallo, Marco},
title = {A Language Modelling Approach for Discovering Novel Labour Market Occupations from the Web},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109035},
doi = {10.1145/3106426.3109035},
abstract = {This article presents an approach for the identification of potential new occupations, i.e., professions, not yet codified by the international standard taxonomy ISCO. This work is framed within the research activities of the WoLMIS project, developed by the University of Milano-Bicocca for the CEDEFOP European Agency, which classifies on-line job offers according to the ISCO taxonomy by using machine learning techniques.The proposed approach is based on text analysis, in particular on the use of language models, and provides two main contributions in the labour market context. First, it can support labour market experts in identifying new potential occupations and the process of updating the ISCO taxonomy. Second, language models are an effective way to identify the most similar occupations to a given one (either new or already coded in the taxonomy) in terms of skills and competencies. The proposed approach has been tested on a dataset of English job vacancies, obtaining promising results.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1026–1034},
numpages = {9},
keywords = {text analysis, language models, labour market},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109038,
author = {Zhang, Lili and Xie, Ying and Liu, Guoliang},
title = {A Sentiment-Change-Driven Event Discovery System},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109038},
doi = {10.1145/3106426.3109038},
abstract = {We present a system that automatically discovers important events that have significantly driven people's sentiment changes towards a target using Twitter data (i.e. tweets). This system can also provide the time, importance, and description of events that are associated with people's sentiment changes. In this system, a sentiment classifier is used as the sensor to detect the time points of those changes. It is also used as the filter to effectively eliminate a considerable amount of noisy information and select the most informative tweets to be further analyzed for event descriptions. Discovered events are described from the following aspects, 1) the most important tweets ranked by tweet-based TextRank algorithm, 2) the topics generated by the nonnegative matrix factorization, and 3) the most important keywords generated by word-based TextRank algorithm. Compared with traditional event discovery techniques, the experimental results show that this system can effectively discover important patterns from tweets and unveil 3Ws of an event (i.e. what happens, when it happens, what its effect is), which provides good reference on understanding behavior changes and making strageties. Furthermore, the system was applied to analyze people's sentiment changes towards the two candidates during the 2016 U.S. presidential election. It can also be applied in other scenarios where people's attitude plays an important role like the brand influence marketing and financial investment markets.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1035–1041},
numpages = {7},
keywords = {sentiment classification, topic modeling, text summarization, event discovery},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109436,
author = {Gulla, Jon Atle and Zhang, Lemei and Liu, Peng and \"{O}zg\"{o}bek, \"{O}zlem and Su, Xiaomeng},
title = {The Adressa Dataset for News Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109436},
doi = {10.1145/3106426.3109436},
abstract = {Datasets for recommender systems are few and often inadequate for the contextualized nature of news recommendation. News recommender systems are both time- and location-dependent, make use of implicit signals, and often include both collaborative and content-based components. In this paper we introduce the Adressa compact news dataset, which supports all these aspects of news recommendation. The dataset comes in two versions, the large 20M dataset of 10 weeks' traffic on Adresseavisen's news portal, and the small 2M dataset of only one week's traffic. We explain the structure of the dataset and discuss how it can be used in advanced news recommender systems.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1042–1048},
numpages = {7},
keywords = {machine learning, datasets, recommender systems},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109434,
author = {Ollikainen, Ville and Niemi, Valtteri},
title = {Evaluating the Performance and Privacy of a Token-Based Collaborative Recommender},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109434},
doi = {10.1145/3106426.3109434},
abstract = {The rapid expansion of available online services has raised concerns about user privacy. In the online world, only a minority of users is actually aware where their data is stored and the policies, how the data may be eventually used. However, at the same time consumers expect more quality from online services, demanding personalized services that fit their individual needs, preferences and values. One approach for service personalization is to use collaborative recommenders. From the privacy perspective, mainstream collaborative recommenders present an inherent security risk, since they are based on memorizing user-item transactions. In this paper, we will study a recently developed token-based method (sometimes referred as an acronym "upcv") which creates privacy-protecting abstraction that is based on collections of randomly generated tokens. These collections are capable of providing information for collaborative recommendations without maintaining any transactional history. This paper presents quality evaluation of item-to-item recommendations using the token-based collaborative recommender, utilizing ISBN agencies of Book-Crossing dataset (BX) books at the data set. This paper will also discuss challenges related to BX. Privacy issues are evaluated with a specific emphasis on the concept of deniability.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1049–1053},
numpages = {5},
keywords = {token, collaborative recommender, privacy, book-crossing, deniability, upcv},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109435,
author = {Mohallick, Itishree and \"{O}zg\"{o}bek, \"{O}zlem},
title = {Exploring Privacy Concerns in News Recommender Systems},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109435},
doi = {10.1145/3106426.3109435},
abstract = {With the increasing ubiquity of access to online news sources, the news recommender systems are becoming widely popular in recent days. However, providing interesting news for each user is a challenging task in highly-dynamic news domain. Many news aggregator sites such as Google News suggest its users to provide sign in to the system for getting user-specific (relevant) news articles. For more generic news recommendation, the system collects user click history and page access pattern implicitly. Often the users are not sure about the usage of the collected and consolidated data by the recommender systems which they usually trade for receiving the news recommendation. Privacy of user identity, user behavior in terms of page access patterns contributes to the overall privacy risks in the news domain. This review paper discusses the current state-of-the-art of privacy risks and existing privacy preserving approaches in the news domain from user perspective.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1054–1061},
numpages = {8},
keywords = {privacy, recommender systems, news recommender systems},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109433,
author = {Lommatzsch, Andreas and Kille, Benjamin and Albayrak, Sahin},
title = {Incorporating Context and Trends in News Recommender Systems},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109433},
doi = {10.1145/3106426.3109433},
abstract = {In our fast changing world, data streams move into the focus. In this paper, we study recommender systems for news portals. Compared with traditional recommender scenarios based on static data sets, the short life cycle of news items and the dynamics in users' preferences are major challenges when developing news recommender systems. This motivates us to research methods facilitating the inclusion of context and trends into news recommender systems. We explain specific requirements for news recommender system and discuss approaches incorporating trends and temporal user habits in order to improve news recommender system. A detailed data analysis motivates our approach. In addition, we discuss experiences of applying news recommendation algorithms online. The evaluation shows that approaches come with specific strengths and weaknesses. Consequently, publishers should select the recommendation strategy with the specific requirements in mind.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1062–1068},
numpages = {7},
keywords = {time-aware recommender models, trends, context, news recommendation, temporal recommender},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109432,
author = {Shashkin, Pavel and Karpov, Nikolay},
title = {Learning to Rank for Personalized News Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109432},
doi = {10.1145/3106426.3109432},
abstract = {Improving user experience through personalized recommendations is crucial to organizing the abundance of data on news websites. Modeling user preferences based on implicit feedback has recently gained lots of attention, partly due to growing volume of web generated click stream data. Matrix factorization learned with stochastic gradient descent has successfully been adopted to approximate various ranking objectives. The aim of this paper is to test the performance of learning to rank approaches on the real-world dataset and apply some simple heuristics to consider temporal dynamics present in news domain. Our model is based on WARP loss with changes to classic factorization model.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1069–1071},
numpages = {3},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109042,
author = {Fietkau, J.},
title = {The Case for Including Senior Citizens in the Playable City},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109042},
doi = {10.1145/3106426.3109042},
abstract = {The topic of "playable cities" has recently emerged as a variation on smart cities, focusing on ways to make urban spaces more playfully interactive and fun by incorporating digital technology. Existing work in this field has largely focused on explorative design and case studies. As of yet, there are barely any design guidelines specific to the context. In this paper, we motivate the need for urban interaction designers to consider the restrictions of senior citizens, give a broad overview over interaction design recommendations for older adults as relevant for urban spaces, examine selected published "playable city" case studies for their suitability regarding this population group, and propose some preliminary design guidelines for future work.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1072–1075},
numpages = {4},
keywords = {accessibility, seniors, urban interaction, playable cities, joy of use},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109041,
author = {Nielek, Radoslaw and Ciastek, Miroslaw and Kope\'{c}, Wies\l{}aw},
title = {Emotions Make Cities Live: Towards Mapping Emotions of Older Adults on Urban Space},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109041},
doi = {10.1145/3106426.3109041},
abstract = {Understanding of interaction between people and urban spaces is crucial for inclusive decision making process. Smartphones and social media can be a rich source of behavioral and declarative data about urban space, but it threatens to exclude voice of older adults. The platform proposed in the paper attempts to address this issue. A universal tagging mechanism based on the Pluchik Wheel of Emotion is proposed. Usability of the platform was tested and prospect studies are proposed.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1076–1079},
numpages = {4},
keywords = {older adults, social inclusion, social design, social participation, crowdsourcing},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109039,
author = {Koch, Michael and K\"{o}tteritzsch, Anna and Fietkau, Julian},
title = {Information Radiators: Using Large Screens and Small Devices to Support Awareness in Urban Space},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109039},
doi = {10.1145/3106426.3109039},
abstract = {Information radiators are ubiquitous stationary installations that radiate information that is likely to improve awareness of passers-by in semi-public environments like organization floors. In this paper, we present the idea of using several kinds of information radiators for enhancing urban participation of seniors - by providing awareness for supporting the planning and execution of activities in public environments. We motivate the idea and discuss interaction design as well as HCI challenges to be addressed in future work.1},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1080–1084},
numpages = {5},
keywords = {information radiator, benefits, seniors, public screen, large screen, pervasive display, awareness},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109040,
author = {Kope\'{c}, Wies\l{}aw and Skorupska, Kinga and Jaskulska, Anna and Abramczuk, Katarzyna and Nielek, Radoslaw and Wierzbicki, Adam},
title = {LivingLab PJAIT: Towards Better Urban Participation of Seniors},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109040},
doi = {10.1145/3106426.3109040},
abstract = {In this paper we provide a brief summary of development LivingLab PJAIT as an attempt to establish a comprehensive and sustainable ICT-based solution for empowerment of elderly communities towards better urban participation of seniors. We report on our various endeavors for better involvement and participation of older adults in urban life by lowering ICT barriers, encouraging social inclusion, intergenerational interaction, physical activity and engaging older adults in the process of development of ICT solutions. We report on a model and assumptions of the LivingLab PJAIT as well as a number of activities created and implemented for LivingLab participants: from ICT courses, both traditional and e-learning, through on-line crowdsourcing tasks, to blended activities of different forms and complexity. We also provide conclusions on the lessons learned in the process and some future plans, including solutions for better senior urban participation and citizen science.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1085–1092},
numpages = {8},
keywords = {older adults, intergenerational interaction, social design, crowdsourcing, living lab, participatory design, social inclusion},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109445,
author = {Zhou, Xujuan and Soar, Jeffrey and Gururajan, Raj and Wu, Zhangguang and Wu, Weimin},
title = {A Cross - Layer Optimization of Video Transmission Based on Packet Loss Rate in 802.11e Wireless Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109445},
doi = {10.1145/3106426.3109445},
abstract = {Although the smaller quantization parameter has smaller coding distortion of message source, the overall distortion rate at the receiver does not necessarily decrease as its quantization parameter decreases. The reason is that a smaller quantization parameter often means a long transmission queue, and a long transmission queue means a bigger loss rate and channel distortion. In this paper, we propose a packet loss-distortion driven cross-layer optimization of video transmission for H.264 video applications in 802.11e wireless networks. Firstly, we analyzed the relationship between quantization parameter and quantization distortion and built an estimation model of transmission distortion. Then the total distortions in the received station are estimated according to the packet loss rate of different video data partition. Secondly, a selection algorithm of optimal quantization parameter based on the total distortion is presented. Our experimental results demonstrate that, at certain loss rates, the proposed method not only outperforms the up-bottom cross-layer optimization with various queue priorities for video data partitions, but also outperforms the bottom-up cross-layer with an adaptive quantization step selection both in terms of received-end destination and video traffic.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1093–1099},
numpages = {7},
keywords = {wireless network, distortion rate, cross-layer optimization, video transmission, packet loss rate},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3115866,
author = {Wlodarczak, Peter and Ally, Mustafa and Soar, Jeffrey},
title = {Data Mining in IoT: Data Analysis for a New Paradigm on the Internet},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3115866},
doi = {10.1145/3106426.3115866},
abstract = {This paper provides an overview on Data Mining (DM) technologies for the Internet of Things (IoT). IoT has become an active area of research, since IoT promises among other to improve quality of live and safety in Smart Cities, to make resource supply and waste management more efficient, and optimize traffic. DM is highly domain specific and depends on what is being mined for. For instance, if IoT is used to optimize traffic in a Smart City to reduce traffic jams and to find parking spaces quicker, different types of data needs to be collected and analysed from an eHealth solution, where IoT is used in a Smart Home to monitor the well being of patients or elderly people. IoT connects things that can collect numeric data from smart sensors, streaming data from cameras or route information on maps. Depending on the type of data, different techniques need to be adopted to analyse them. Also, many IoT applications analyse data from different devices and correlate them to make predictions about possible machine failures in production sites or looming emergency situations in Smart Buildings in a home security application. DM techniques need to handle the heterogeneity of IoT data, the large volumes of data and the speed at which they are produced. This paper explores the state of the art DM techniques for IoT.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1100–1103},
numpages = {4},
keywords = {predictive analytics, data mining, smart city, internet of things, machine learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3117759,
author = {Aleithe, M. and Skowron, P. and Franczyk, B. and Sommer, B.},
title = {Data Modeling of Smart Urban Object Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3117759},
doi = {10.1145/3106426.3117759},
abstract = {In the digital age, where research is data-driven, understanding all involved fields of research becomes more and more important. Understanding various data sources within interdisciplinary research and beyond domain boundaries is a significant core competency. All participants should have a same-level understanding of significant information, which can be created from various data sources. Based on this fact, the paper at hand demonstrates a modeling approach for the generation of a unified data model in terms of smart urban objects. These smart objects are represented by interconnected data structures which is a prime example in context of Internet of Things. Further, an implementation of the graph database Neo4J and a correlated visualization of intuitive structuring of data sources beyond domain boundaries will be demonstrated.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1104–1109},
numpages = {6},
keywords = {smart urban object network, data model architecture, spatio temporal object graph, graph database},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3115589,
author = {Marivate, Vukosi and Moorosi, Nyalleng},
title = {Employment Relations: A Data Driven Analysis of Job Markets Using Online Job Boards and Online Professional Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3115589},
doi = {10.1145/3106426.3115589},
abstract = {Data from online job boards and online professional networks present an opportunity to understand job markets as well as how professionals transition from one job/career to another. We propose a data driven approach to begin to understand a slice of the South African job market. We do this by analysing data from career websites as well as a South African online professional networks. Our goals are to be able to group jobs given their descriptions, characterise career paths as well as to have some building blocks to be able to extract job position hierarchies given a description.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1110–1113},
numpages = {4},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3115865,
author = {Sankaran, Prema and Bheeman, Sankaran and Priya, K. Hari and Zhou, Xujuan and Gururajan, Raj},
title = {Factors Impacting Employee Engagement on Enterprise Social Media},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3115865},
doi = {10.1145/3106426.3115865},
abstract = {The emergence of knowledge-based economies has emphasised the importance of interactive knowledge management technologies, which have manifested themselves in the form of social networking tools. Organization's ability to leverage and manage the relevant knowledge is a sustainable strategic tool. This research focus on the ways in which social technologies facilitate knowledge sharing in the workplace. Findings uncovers key drivers of three dimensions of knowledge management, individual, organization and technology and suggest to connect them along with a knowledge process architecture for leveraging knowledge.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1114–1121},
numpages = {8},
keywords = {enterprise social media, social media, Twitter, employee engagement},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109447,
author = {Wang, Jinhua and Zhang, Mingxi and He, Zhenying and Wang, Wei},
title = {Partial Sums-Based P-Rank Computation in Information Networks},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109447},
doi = {10.1145/3106426.3109447},
abstract = {P-Rank is a simple and captivating link-based similarity measure that extends SimRank by exploiting both in- and out-links for similarity computation. However, the existing work of P-Rank computation is expensive in terms of time and space cost and cannot efficiently support similarity computation in large information networks. For tackling this problem, in this paper, we propose an optimization technique for fast P-Rank computation in information networks by adopting the spiritual of partial sums. We write P-Rank equation based on partial sums and further approximate this equation by setting a threshold for ignoring the small similarity scores during iterative similarity computation. An optimized similarity computation algorithm is developed, which reduces the computation cost by skipping the similarity scores smaller than the give threshold during accumulation operations. And the accuracy loss estimation under the threshold is given through extensive mathematical analysis. Extensive experiments demonstrate the effectiveness and efficiency of our proposed approach through comparing with the straightforward P-Rank computation algorithm.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1122–1130},
numpages = {9},
keywords = {information network, partial sums, similarity computation, P-Rank},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109444,
author = {Mohammed, Atheer Abdullah and Gururajan, Raj and Hafeez-Baig, Abdul},
title = {Primarily Investigating into the Relationship between Talent Management and Knowledge Management in Business Environment},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109444},
doi = {10.1145/3106426.3109444},
abstract = {Purpose: This paper aims to address concepts, visions and gaps in literature in the field of the relationship between talent management and knowledge management in business organisations. It also aims to develop a model for the relationship between talent management processes and knowledge management processes. This is because there are practical benefits for business organisations focused on developing knowledge and talents. Design/methodology/approach: This study takes on a detailed literature review of the relationship between talent and knowledge management in business organisations. Conclusion: The key conclusion for this research is that more research is required to examine further the relationship between talent management processes and knowledge management processes in the business environment. It also concludes that it is worth noting scholars' serious interest in attraction, development, and talent retention processes.Conversely, it also viewed that the majority of researchers in the field of knowledge management have focused on creation, application, knowledge storage, transfer and acquisition of knowledge. This research provides a comprehensive review of further research in selecting the common processes associated with talent management processes and knowledge management processes.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1131–1137},
numpages = {7},
keywords = {talent management, knowledge management, business organisations},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109052,
author = {Albukhitan, Saeed and Helmy, Tarek and Alnazer, Ahmed},
title = {Arabic Ontology Learning Using Deep Learning},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109052},
doi = {10.1145/3106426.3109052},
abstract = {Ontology, the backbone of Semantic Web, is defined as the formal specification of conceptual hierarchy with relationships between concepts. Ontology Learning (OL) is a process to create an ontology from text automatically or semi-automatically. OL is an important topic in the Semantic Web field in the last two decades but it is still not mature in Arabic not like Latin languages. Currently, there is a limited support for using knowledge from Arabic literature automatically in semantically-enabled systems. Deep Learning (DL), an artificial neural networks learning based application, has proved a good improvement in multiple areas including text mining. By using DL, it is possible to have word embedding as distributed word representations from textual data. The application of DL to aid Arabic ontology development remains largely unexplored. This paper investigates the performance of implementing DL with Arabic ontology learning tasks using major models such as Continuous Bag of Words (CBOW) and Skip-gram. Initial performance results are promising as an effective application of Arabic ontology learning.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1138–1142},
numpages = {5},
keywords = {arabic ontology, ontology learning, deep learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109047,
author = {Qi, XingLiang and Zhou, Yang and Fu, XianJun and Wang, ZhenGuo},
title = {Construction of the Diagnosis and Treatment Process of Dermatosis Based on Data-Driven Approach},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109047},
doi = {10.1145/3106426.3109047},
abstract = {In this study, we selected the dermatology data, constructed a dictionary of dermatology semantic classification, and established a corpus of dermatology literature. Based on the context calculation model we took the case of erythema skin disease as the example, identified the semantic meaning of the dermatology data, and formed the concept attribute of dermatology knowledge, constructed the dermatology ontology. We established a priori database and verified each other with the running data to correct the data error and improve the data definition. In this study, we used the natural language to describe the characteristic information of skin diseases, and carried out the semantic searching. Finally, we constructed the mathematical model of the diagnosis and treatment of dermatology based on the corpus, the ontology and the priori database. The purpose is to supply the gap of unbalance of doctors' personal level of the dermatosis diagnosis and treatment, improve the level and accuracy of clinical diagnosis and treatment of dermatosis, and to achieve effective drive of medical literature data for clinical diagnosis and treatment.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1143–1146},
numpages = {4},
keywords = {ontology, clinical diagnosis and treatment, dermatology, corpus, natural language processing},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109054,
author = {Kem, Oudom and Balbo, Flavien and Zimmermann, Antoine},
title = {Multi-Goal Pathfinding in Ubiquitous Environments: Modeling and Exploiting Knowledge to Satisfy Goals},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109054},
doi = {10.1145/3106426.3109054},
abstract = {Multi-goal pathfinding (MGPF) is a problem of searching for a path between a start and a destination allowing a set of goals to be satisfied. We address MGPF in ubiquitous environments that accommodate cyber, physical and social (CPS) entities from smart objects to sensors and to humans. Given a MGPF problem in a pervasive environment, our approach aims at exploiting data from various resources including CPS entities located in the environment and external resources such as the Web to solve the problem. In this paper, we present a knowledge model for describing a ubiquitous environment integrating its spatial dimension, CPS entities it contains and its relevant resources. A global view of the approach is provided. We address particularly one of the challenges in MGPF, namely goal satisfaction problem, which consists of identifying through which entities a goal can be satisfied. Towards this aim, we design an ontology to formally model CPS entities, goals and their relations. We describe a method to exploit modeled knowledge in order to solve the goal satisfaction problem.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1147–1150},
numpages = {4},
keywords = {multi-goal pathfinding, ontology, ubiquitous environment},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109050,
author = {He, Long and Qiu, Likun},
title = {Ontology of Human Relation Extraction Based on Dependency Syntax Rules},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109050},
doi = {10.1145/3106426.3109050},
abstract = {This paper proposed a novel scheme for extracting character relation from unstructured text based on dependency grammar rules. First of all, we took the Three Kingdoms characters as our research object, then selected articles containing target relationships and thus constructed a corpus consisting of 1000 sentences. Secondly, We analyzed the corpus and developed a set of dependent grammar rules for relation extraction. Finally, we proposed a system, which makes it possible for computers to automatically extract and identify character relationships.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1151–1157},
numpages = {7},
keywords = {character relation, relative words, dependency grammar},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109045,
author = {Li, Yunxuan and Ji, Weiyun and Xu, Dekuan},
title = {Quantitative Style Analysis of Mo Yan and Zhang Wei's Novels},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109045},
doi = {10.1145/3106426.3109045},
abstract = {Selecting 24 novels written by Mo Yan and Zhang Wei as corpus, This paper analyzed the stylistic features of Mo Yan and Zhang Wei's novels from the perspective of quantitative style. Features include the pauses in sentences, the relevance of context, the type/token ratio, the frequency of the word string, high-frequency words and text clustering. Through statistic analysis, it is found that Mo Yan and Zhang Wei's works have much in common, which are both very oral, creative and can use all kinds of linguistic materials. However, they are different from one another in the usage of sentence patterns and of words and in the attention of social life. Compared with Mo Yan's language features, Zhang Wei's is more changeable.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1158–1161},
numpages = {4},
keywords = {clustering, quantitative analysis, statistics, Mo Yan, folk, language features, Zhang Wei},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109046,
author = {Cheng, Yonghong and Huang, Yi},
title = {Research and Development of Domain Dictionary Construction System},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109046},
doi = {10.1145/3106426.3109046},
abstract = {With the rapid development of science and technology, every day there are a large number of data in various forms produced on the Internet. Domain vocabularies embody the core knowledge of a subject field, and they play important role in parsing, tagging and understanding domain literature, as well as grasping the development of the subject field status with theoretical and practical significance. Currently, most of these dictionaries are constructed manually by domain experts, and there is an urgent need to develop a professional dictionary construction system to facilitate the construction and editing process. This paper investigates the functions of several dictionary systems home and abroad, designs the overall framework and component modules, and builds a dictionary construction system. The implementation and application of the system show that it can effectively support the professional dictionary construction process.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1162–1165},
numpages = {4},
keywords = {system implementation, domain dictionary, dictionary compilation, dictionary construction},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109048,
author = {Zhu, Wenwen and Guo, Zhijun and Cheng, Yonghong},
title = {Research on Design and Implementation of Data Exchange System},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109048},
doi = {10.1145/3106426.3109048},
abstract = {With the rapid development of computer technology, the barriers of communication among different systems caused by system heterogeneity or data structure have been broken down. However, the demands for personalized content for accuracy in resource exchange and delivery are becoming increasingly high. The structures of existing literature resources like papers, patents and books, with different formats and structures, leading to lots of problems in content delivery and inheritance. Thus, based on the XML technology, we design and develop the data exchange system. This system supports the mapping and integration of different structures of literature resource, and parsing resources at the same time, so that users can upload and verify the XML schema files according to their individual demands for data exchange.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1166–1170},
numpages = {5},
keywords = {XML schema, data exchange, data exchange system},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109053,
author = {Zhang, Yu and Saberi, Morteza and Chang, Elizabeth},
title = {Semantic-Based Lightweight Ontology Learning Framework: A Case Study of Intrusion Detection Ontology},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109053},
doi = {10.1145/3106426.3109053},
abstract = {Building ontology for wireless network intrusion detection is an emerging method for the purpose of achieving high accuracy, comprehensive coverage, self-organization and flexibility for network security. In this paper, we leverage the power of Natural Language Processing (NLP) and Crowdsourcing for this purpose by constructing lightweight semi-automatic ontology learning framework which aims at developing a semantic-based solution-oriented intrusion detection knowledge map using documents from Scopus. Our proposed framework uses NLP as its automatic component and Crowdsourcing is applied for the semi part. The main intention of applying both NLP and Crowdsourcing is to develop a semi-automatic ontology learning method in which NLP is used to extract and connect useful concepts while in uncertain cases human power is leveraged for verification. This heuristic method shows a theoretical contribution in terms of lightweight and timesaving ontology learning model as well as practical value by providing solutions for detecting different types of intrusions.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1171–1177},
numpages = {7},
keywords = {ontology learning, natural language processing, intrusion detection, crowdsourcing},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109049,
author = {Wang, Mengjiao and Jiang, Shuang and Qiu, Likun and He, Long},
title = {A Study of Anaphora Resolution in the Novel <i>Life</i>},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109049},
doi = {10.1145/3106426.3109049},
abstract = {Automatic anaphora resolution is useful for many natural language process tasks, including automatic summarization, information extraction and machine translation. This paper took the novel Life as the original corpus, and then annotated the anaphora relations in this corpus. Based on this corpus, we analyzed the distribution of different types of anaphora phenomena, and then developed a set of rules for automatic anaphora resolution, which might be useful for future automatic processing.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1178–1183},
numpages = {6},
keywords = {antecedent, named entity, anaphora resolution},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109412,
author = {Akashiba, Shunsuke and Nishimoto, Chihiro and Takahashi, Naoya and Morita, Takeshi and Kukihara, Reiji and Kuwayama, Misae and Yamaguchi, Takahira},
title = {Development of Applications for Teaching Assistant Robots with Teachers in PRINTEPS},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109412},
doi = {10.1145/3106426.3109412},
abstract = {PRINTEPS is currently being developed as a total intelligent application, which has sub systems for knowledge-based reasoning, speech dialogue, image sensing, motion planning, and machine learning, in order to support end users on easily developing intelligent applications for human-machine collaboration. In this paper, a lesson application for collaborative teaching among a robot, laptop PC, sensor, teachers and students was developed with PRINTEPS. The implementation lesson was performed in a science class for six grade elementary students.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1184–1190},
numpages = {7},
keywords = {ontology, rule base, PRINTEPS},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109414,
author = {Adachi, Takuya and Fukuta, Naoki},
title = {A Mapping-Enhanced Linked Data Inspection and Querying Support System Using Dynamic Ontology Matching},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109414},
doi = {10.1145/3106426.3109414},
abstract = {Supporting heterogeneous ontologies is an important issue on retrieving from and linking to linked data stored in various SPARQL endpoints via SPARQL queries. There are several approaches to support the coding process of a SPARQL query for users who are unfamiliar to code it. On the use of some ontology mapping-based support approaches on SPARQL-based query systems, we often assume that the users already have appropriate weighted ontology mappings for the ontologies used in the query. In this paper, we present ontology mapping inspection mechanisms for mapping-enhanced SPARQL queries to widely retrieve various data from Linked Open Data (LOD). Our dynamic ontology mapping adaptation technique complements the used incomplete ontology mappings by dynamically detecting and adding missing mappings to include the correspondences between entities of terms in heterogeneous ontologies.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1191–1194},
numpages = {4},
keywords = {query processing, ontology matching, SPARQL},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109415,
author = {Oishi, Sho and Fukuta, Naoki},
title = {MstdnDeck: An Agent-Based Protection of Cyber-Bullying on Distributedly Managed Linked Microbloggings},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109415},
doi = {10.1145/3106426.3109415},
abstract = {In this paper, to resolve some issues on personal assistance that are working on some social network services, we present a platform that allows agents to analyze some associated informations to make effective protraction and prevention of cyber-bullying. We also present a prototype implementation of our platform that allows agents handle and analyze contexts on the Mastodon-based social networks. On the current implementation, a personal assistant agent can run on a same browser that opened web site of the social networking service.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1195–1198},
numpages = {4},
keywords = {mastodon, cyber-bullying, reasoning},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109413,
author = {Nakamura, Kodai and Morita, Takeshi and Yamaguchi, Takahira},
title = {PRINTEPS for Development Integrated Intelligent Applications and Application to Robot Teahouse},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109413},
doi = {10.1145/3106426.3109413},
abstract = {We are developing PRactical INTEligent aPplicationS (PRINTEPS), which is a user-centric platform to develop integrated intelligent applications only by combining four types of modules such as knowledge-based reasoning, speech dialog, image sensing and motion management. PRINTEPS supports end users to participate in AI applications design (user participation design) and to develop applications easily. This paper introduces the architecture and applications of PRINTEPS for robot teahouse robot where multiple people and robots cooperate.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1199–1206},
numpages = {8},
keywords = {stream reasoning, ontology, PRINTEPS},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109416,
author = {Ohbe, Tatsuya and Ozono, Tadachika and Shintani, Toramatsu},
title = {A Sentiment Polarity Classifier for Regional Event Reputation Analysis},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109416},
doi = {10.1145/3106426.3109416},
abstract = {It is important to analyze the reputation or demands for a regional event, such as a school festival. In our work, we use sentiment polarity classification in order to coordinate regional event reputation. We proposed sentiment polarity classification based on bag-of-words models in the previous works. To get over the traditional models, we proposed several classifier models based on deep learning models. As the application, we also described the overview of a system supports to analyze regional event reputation and an example of regional event analysis using our system. In this paper, we described how to improve the performance of the sentiment polarity classification using deep learning models. We compared the performance of four models in terms of the classification accuracy and the training speed. We found the Convolutional Neural Networks based model, three words convolutions, was the best model among the four models.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1207–1213},
numpages = {7},
keywords = {sentiment polarity classification, sentiment visualization, convolutional neural networks, recurrent neural networks},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3110323,
author = {Asadabadi, Mehdi Rajabi and Saberi, Morteza and Chang, Elizabeth},
title = {A Fuzzy Game Based Framework to Address Ambiguities in Performance Based Contracting},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3110323},
doi = {10.1145/3106426.3110323},
abstract = {Avoiding ambiguity and fuzziness in the determination of the requirements is a crucial factor in the success of Performance Based Contracting (PBC). To date, there is a research gap because insufficient studies have been undertaken to address this significant issue in the pro-curement process. Previous studies that have been con-ducted on requirement specification and elicitation are limited to software engineering. This study investigates this issue in the procurement process and proposes an integrated framework using Natural Language Pro-cessing (NLP), game theory and fuzzy logic. This re-search contributes to contract theory by opening a new line of research which paves the way for leveraging arti-ficial intelligence techniques in automated or semi-automated contract monitoring.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1214–1217},
numpages = {4},
keywords = {NLP, contracting issues, game theory, ambiguity},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3110324,
author = {Clarke, Rebekah Storan},
title = {Intelligent Client-Side Personalisation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3110324},
doi = {10.1145/3106426.3110324},
abstract = {This research will explore the notion that personalisation can be performed on client-side web caches with no significant impact on performance or accuracy.Web caching is used to reduce bandwidth usage, server load, and perceived lag by website users. However, once a page is cached personalisation of that page cannot be updated despite new data which may be gathered.The intelligent architecture proposed in this work will enable personalisation of these caches as well as realising the privacy benefits of performing personalisation on the client-side. The effectiveness of the pre-fetching strategy is improved through insights from web usage mining, analysing users' behavioural data to predict their next click. In this research the predictive system will take the form of a Propensity Model, monitoring implicit user actions such as clicks, mouse movement and scrolling. To ensure high performance is maintained the architecture must be capable of determining which pages to cache and when personalisation is required based on the user's behaviour.The research will follow a case-study based approach, evaluating the architecture performance and predictive accuracy across three webpage types. Evaluation of the content-service interactions will examine how the predictive accuracy and service response time are impacted by the frequency of communication with both the propensity prediction service and a personalisation service.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1218–1221},
numpages = {4},
keywords = {client-side personalisation, interaction modelling, click prediction, propensity modelling, caching},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3110325,
author = {Jones, Jim and de Siqueira Braga, Diego and Tertuliano, Kleber and Kauppinen, Tomi},
title = {MusicOWL: The Music Score Ontology},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3110325},
doi = {10.1145/3106426.3110325},
abstract = {Managing and analyzing music scores content is a known issue for digital libraries. Despite their inarguable complexity, these contents are often ignored by digital libraries, where music scores are mostly treated as simple digital images. This paper reports on the development and application of MusicOWL, an ontology to formally describe music scores contents for western music. Unlike other efforts on the subject, MusicOWL provides a comprehensive vocabulary for annotating music scores, which covers important music-related information, such as melodies, dynamics, and tonalities. We also report on the Linked Music Score dataset creation, including all necessary steps from scanned music scores to RDF triples, and how this linked music score data enables users to search for music scores by using handy queries, thus translating the needs of many music professionals, data curators and online learners. Additionally, we introduce and analyze the search engine WWU Music Score Portal, which uses the approach proposed in this work to improve music scores discovery and support online learning of music.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1222–1229},
numpages = {8},
keywords = {music scores, linked data, digital humanities, OWL},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3110322,
author = {Manrique, Rub\'{e}n},
title = {Towards Automatic Learning Content Sequence via Linked Open Data},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3110322},
doi = {10.1145/3106426.3110322},
abstract = {The paradigm of lifelong learning supported by technology is redefining the way we learn as well as the way we search and consume the ever growing corpus of information available in the Web to acquire knowledge on a particular subject. This research addresses the problem of finding and organizing learning content to support self-directed learners in achieving a learning goal through the search, selection and sequencing of Web content that might or might not have been conceived as learning resources. We plan to build an automatic process driven by the knowledge available in datasets belonging to the Linked Open Initiative and open non-structured information such as courses syllabi and books table of contents. Our proposed service have two main components: (i) a graph of interrelated learning concepts from which is possible infer what concepts must be addressed first before others in the learning process (prerequisite relationships), and (ii) a component for the creation of learning resources sequences based on a learning goal and a learner profile.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1230–1233},
numpages = {4},
keywords = {user modeling, linked open data, personalization},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109441,
author = {Piazza, Alexander and Kr\"{o}ckel, Pavlina and Bodendorf, Freimut},
title = {Emotions and Fashion Recommendations: Evaluating the Predictive Power of Affective Information for the Prediction of Fashion Product Preferences in Cold-Start Scenarios},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109441},
doi = {10.1145/3106426.3109441},
abstract = {Emotions have a significant impact on the purchasing process. Due to novel affective computing approaches, affective information of users can be acquired in implicit and therefore non-intrusive manner. Recent research in the field of recommender systems indicates that the incorporation of affective user information in the prediction model has a positive impact on the recommender systems accuracy. Existing research mainly focused on product recommendations in the movie anfd music domain. Our paper investigates the impact of affective emotions on fashion products, which is one of the largest consumer industries. We integrate the users' mood and their emotion in the prediction model, and the results are compared to the baseline model using rating data only. For this, we generate a dataset with 337 participants, 64 products, and 10816 ratings. We determine the mood information using the PANAS questionnaire, and the emotion by using the SAM self-assessment method. The affective information is integrated leveraging Factorization Machines. The evaluation of the offline experiments reveals that in new item cold-start scenarios the mood information has a positive impact on the prediction accuracy, whereas the emotion information has a negative impact.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1234–1240},
numpages = {7},
keywords = {affective recommender systems, fashion recommendation, context-aware recommendation, new item cold-start, factorization machines},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109437,
author = {Li, Zhaoqiang and Huang, Jiajin and Zhong, Ning},
title = {Exploiting User and Item Embedding in Latent Factor Models for Recommendations},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109437},
doi = {10.1145/3106426.3109437},
abstract = {Matrix factorization (MF) models and their extensions are widely used in modern recommender systems. MF models decompose the observed user-item interaction matrix into user and item latent factors. In this paper, we propose mixture models which combine the technology of MF and the embedding. We show that some of these models significantly improve the performance over the state-of-the-art models on two real-world datasets, and explain how the mixture models improve the quality of recommendations.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1241–1245},
numpages = {5},
keywords = {recommendations, latent factor models, user and item embedding},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109440,
author = {Manrique, Rub\'{e}n and Mari\~{n}o, Olga},
title = {How Does the Size of a Document Affect Linked Open Data User Modeling Strategies?},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109440},
doi = {10.1145/3106426.3109440},
abstract = {Semantic user modeling techniques use a representation based on concepts that are linked to a Knowledge Base (KB). Current research uses Linked Open Data (LOD) because of its comprehensive interlinked datasets, which allow excellent cross-domain modeling capabilities. LOD semantic user profiles have been employed in the context of Social Networks, which require user's posts as input. Less attention has been paid to other domains for which input documents differ from short and concise posts. In this paper, we perform a comparative study of different LOD semantic user modeling techniques by taking different types of documents as input: short, medium, and long texts. We selected recommending academic documents based on modeling the user's research interests as the evaluation scenario. Academic documents' titles, abstracts, and the body of text were used, respectively, for short, medium, and long documents. Our results showed that expansion strategies work best for short and medium documents while filtering strategies are more appropriate when the whole document is used as input. Finally, we explored diverse alternatives if documents did not include a summary or abstract, and we concluded that, in this case, the two best alternatives are a filtering strategy over the whole text and the use of TextRank algorithm to build a set of key sentences to be used as input of an expansion strategy.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1246–1252},
numpages = {7},
keywords = {user modeling, personalization, linked open data},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3109439,
author = {Kuai, Hongzhi and Yan, Jianzhuo and Chen, Jianhui and Yu, Yongchuan and Wang, Haiyuan and Zhong, Ning},
title = {A Knowledge-Driven Approach for Personalized Literature Recommendation Based on Deep Semantic Discrimination},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109439},
doi = {10.1145/3106426.3109439},
abstract = {The query and selection of scientific literatures are knowledge driven. Researchers regard public literature resources as target knowledge sources and use their own domain knowledge to explore in them. However, existing knowledge-driven methods of literature recommendation mainly focus on morphological matching and cannot effectively resolve polysemous phenomenon brought by "knowledge overload". Based on this observation, this paper presents a knowledge-driven approach for personalized literature recommendation. Domain ontology, synonyms and knowledge labels are integrated into a multidimensional domain knowledge map for modeling user knowledge requirements and literature contents based on deep semantic discrimination. The personalized recommendation is achieved by calculating knowledge distances between users and literatures. Experimental results on a real data set of PubMed show that the recommended relevance of the current method is 67%, better than other methods.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1253–1259},
numpages = {7},
keywords = {literature recommendation, concept-based pseudo-relevance feedback, deep semantic discrimination},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3106426.3110321,
author = {Sottocornola, Gabriele and Stella, Fabio and Zanker, Markus and Canonaco, Francesco},
title = {Towards a Deep Learning Model for Hybrid Recommendation},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3110321},
doi = {10.1145/3106426.3110321},
abstract = {The deep learning wave is propagating through many research areas and communities. In the last years it quickly propagated to Recommendation Systems, a research area which aims to recommend items to users. Indeed, many deep learning models and architectures have been proposed for Recommendation Systems to improve collaborative filtering and content based algorithms. In this paper we propose a hybrid recommendation system combining user ratings and natural language text processing to solve the 0/1 recommendation problem. In particular, we describe a deep learning architecture combining two information sources, namely natural language text and user rating. Natural language text is used to learn a user-specific content-based classifier, while user ratings are used to develop user-adaptive collaborative filtering recommendations. We perform numerical experiments on MovieLens 1M and reach first preliminary, but promising results, showing the proposed architecture has the potential to combine content-based and collaborative filtering recommendation mechanisms using a deep learning supervisor.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1260–1264},
numpages = {5},
keywords = {deep learning, recommendation systems, hybrid recommendation system},
location = {Leipzig, Germany},
series = {WI '17}
}

