@inproceedings{10.5555/946251.947118,
title = {Welcome Message},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {.12},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947117,
title = {WI'03 and IAT'03 Organizing Committee},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {.15},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947120,
title = {WI'03 and IAT'03 Program Committees},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {.17},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947119,
title = {WI'03 Non-Program Committee Reviewers},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {.21},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947037,
author = {Carter, Jonathan and Ghorbani, Ali A.},
title = {Value Centric Trust in Multiagent Systems},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This work focuses on the design and implementation of a new model of trust based on the formalizations of reputation, self-esteem, and similarity within an agent. In this work we universalize reputation through the use of values found within all Multiagent Systems. The following values are manifested within Multiagent Systems: responsibility, honesty, independence, obedience, ambition, helpfulness, capability, knowledgability, and cost-efficiency. Manifestations of these values lead to a more universalized approach to formalizing reputation.This new model of trust is examined within the context of an e-commerce framework. It is analyzed with respect to stability, scalability, accuracy in attaining e-commerce objectives, and general effectiveness in discouraging untrustworthy behavior. Based on the experiments, the model is scalable and stable dependent upon the agent population of buyers and sellers. It achieves its primary objective of discouraging untrustworthy behavior as measured through the acceleration of Gross Domestic Product growth over time.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {3},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947030,
author = {Miao, Jinghao},
title = {Graph Structures in Paragraph-Linked Repositories},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The study of automatically generated link networks in hypertext repositories yields valuable insights into the algorithms that produced them. In this investigation of these link networks, the phenomena of centrality and its opposite, dispersion, are identified as issues to be dealt with. Different algorithms produce link networks with different degrees of centrality. We investigate this using the MultiBrowser system as a testbed and applying these algorithms to repositories consisting of documents downloaded based on search engine return lists. Once automatically generated hyperlinks are added to this repository, making documents accessible from otherdocuments in the repository, the centrality properties of the linked repository can be investigated.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {10},
numpages = {1},
keywords = {information foraging, direct display, clustering, N-Grams, MultiBrowser, Clan Graph, multidisplay},
series = {WI '03}
}

@inproceedings{10.5555/946251.946987,
author = {Aneiros, Maria and Estivill-Castro, Vladimir},
title = {Foundations of Unconstrained Collaborative Web Browsing with Awareness},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Research has focused signi.cantly in enabling the Web (WWW) for Computer Supported Cooperative Work (CSCW). However, sur.ng, the most common use in the WEB, remains an individual, rather than group activity. Previous attempts to provide collaborative browsing capability constrain some users to the command of a selected user who controls the browsers of others. We adapt the technology of unconstrained distributed collaborative editors to develop unconstrained collaborative Web browsing. However, the effective collaboration is dependent on the awareness of context and group activity. We develop the history mechanisms for our solution to provide 4 types of awareness commonly discussed in the literature of CSCW.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {18},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946993,
author = {Yuan, Soe-Tsyr and Lin, Kwei-Jay},
title = {WISEâ€”Building Simple Intelligence into Web Services},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web Services are self contained and self described modular applications that can be published, discovered and employed on the Web. Many standard protocols supporting web services have been adopted and more are being proposed. In this paper, we study the issues on providing intelligent web services. We propose the enhancement of web service functionalities by deploying software agents on both server side and/or client side. Our goal of designing the WISE web service architecture is to provide a working middle ground between the current web service standards and the Semantic Web architecture. The WISE softwareagent architecture for web services is presented. We discuss the design issues of WISE. We also present the QoS management protocol and algorithm that can be used by WISE servers.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {26},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947003,
author = {Li, Qing and Kim, Byeong Man},
title = {Clustering Approach for Hybrid Recommender System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recommender system is a kind of web intelligence techniques to make a daily information filtering for people. In this work1, Clustering techniques have been applied to the item-based collaborative filtering framework to solve the cold start problem. It also suggests a way to integrate the content information into the collaborative filtering. Extensive experiments have been conducted on MovieLens data to analyze the characteristics of our technique. The results show that our approach contributes to the improvement of prediction quality of the item-based collaborative filtering, especially for the cold start problem.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {33},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947022,
author = {Sim, Kwang Mong},
title = {Towards Holistic Web-Based Information Retrieval: An Agent-Based Approach},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents an agent-based system for bolstering holistic information retrieval via the WWW. In Ellis' holistic model of information seeking behaviors, the information seeking activities include: selection of sources, browsing and differentiating, monitoring as well as extraction. Through the use of a query processing agent (QPA), information filtering agents (IFAs) and information monitoring agents (IMAs), these activities can be automated. By establishing sub-class relations among (key)words the query processing agent (QPA) expands a query with a list of sub-queries to select appropriate URLs. Using three relevance metrics: word relations, frequency and nearness of keywords, the IFA is used to determine the relevance of a page. Additionally, IMAs can be used to track changes in the content of selected pages, paragraphs or tables in websites. Empirical results demonstrated that the QPA can find appropriate number of websites, and IFAs are effective in filtering relevant information. As part of an on-going work, an Information Extraction Agent is currently beingdesigned and developed.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {39},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947034,
author = {Chen, Chih-Ming},
title = {Incremental Personalized Web Page Mining Utilizing Self-Organizing HCMAC Neural Network},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In recent years, information has grown rapidly, especially on the World Wide Web. Also volume of information found by search engines tends to be large, and these documents are not tailored to a user's actual needs and interests. Thus, to offer the personalized service thatincludes only user interested information become increasingly important. Web mining techniques have proven themselves as a very useful tool for mining information of interests on the Web. However, past pioneers' studies have indicated that the main challenges in Web mining are in terms of handling high-dimensional data, achieving incremental learning (or incremental mining), providing scalable mining and parallel and distributed mining algorithms. This study presents a novel self-organizing Hierarchical CMAC (HCMAC) neural network composed of two-dimensional Weighted Grey CMACs (WGCMAC) capable of handling both higher dimensional classification problems and self-organizing memory structure according to the distribution of training patterns. Moreover, a learning algorithm that can learn incrementally from new added data without forgetting prior knowledge is proposed to train the self-organizingHCMAC neural network. It can be applied to incrementally learn user profiles from user feedback for identifying personalized Web pages. A benchmark dataset of Web pages ratings that contains four topics of user profiles is used to demonstrate the effectiveness of the proposed method. Experimental results show that the selforganizing HCMAC neural network has a good incrementally learning ability and can overcome the problem of enormous memory requirement in the conventional CMAC while it is applied to solve the higher dimensional classification problems. Furthermore, experiments also confirm that the self-organizing HCMACneural network has a better forecasting ability to identify user interesting Web pages than other well-known classifiers do.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {47},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947033,
author = {Lin, Shou-de and Knoblock, Craig A.},
title = {Exploiting a Search Engine to Develop More Flexible Web Agents},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the rapid growth of the World Wide Web, more and more people rely on the online services to acquire and integrate information. However, it is time consuming to find the online services that are perfectly suited for a given task. First, the users might not have enoughinformation to fill in the required input fields for querying an online service. Second, the online service might generate only partial information. Third, the user might only find the inverse version of the desired service. In this paper we propose a framework to develop flexible webagents that handle these imperfect situations. In this framework we exploit a search engine as a general information discovery tool to assist finding and pruning information. To demonstrate this framework, we implemented two web agents: the Internet inverse geocoder and the address lookup module.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {54},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947009,
author = {Arjona, J. L. and Corchuelo, R. and Toro, M.},
title = {A Knowledge Extraction Process Specification for Today's Non-Semantic Web},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The semantic web shall enable web agents an efficient, precise, and comprehensive extraction of knowledge. Nevertheless, this new web is not likely to be adopted in the immediate future.In this article, we present a specification of a new framework in order to extract knowledge from today's dynamics non-semantic web. Our proposal is novel in that it associates semantics with the information extracted, which improves agent interoperability; it can also deal with changes to the structure of a web page, which improves adaptability; furthermore, it achieves to delegate the knowledge extraction procedure to specialist agents, easing software development and promoting software reuse and maintainability.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {61},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947026,
author = {Ji, Junzhong and Sha, Zhiqiang and Liu, Chunnian and Zhong, Ning},
title = {Online Recommendation Based on Customer Shopping Model in E-Commerce},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As e-commerce developing rapidly, it is becoming a research focus about how to capture or find customer's behavior patterns and realize commerce intelligence by use of Web mining technology. Recommendation system in electronic commerce is one of the successful applications that are based on such mechanism. In this paper, we present a new framework in recommendation system by finding customer model from business data. This framework formalizes the recommending process as knowledge representation of the customer shopping information and uncertainty knowledge inference process. In our approach, we firstly build a customer model based on Bayesian network by learning from customer shopping history data, then we present a recommendation algorithm based on probability inference in combination with the last shopping action of the customer, which can effectively and in real time generate a recommendation set of commodity.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {68},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946994,
author = {Constantinescu, Ion and Faltings, Boi},
title = {Efficient Matchmaking and Directory Services},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {It has been widely recognised that matchmaking is an important component for environments populated with heterogeneous services. Several researchers have developed powerful techniques for the matchmaking problem in general. There are also specific representation of service capabilities such as DAML-S which provide a more specific framework for matchmaking.Most approaches to matchmaking have assumed a sequential search for a service with matching capabilities. This may become intractable when the number of available services gets large. In this paper, we consider how matchmaking can be developed into service directories that can be searched and maintained efficiently. Our main contribution is to show how matchmaking with DAML-S specifications can be integrated with efficient methods forsearching and maintaining balanced directory trees. We also report on experimental results using an implementation based on generalised search trees.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {75},
numpages = {1},
keywords = {directories, matchmaking, Service description, service discovery, indexing},
series = {WI '03}
}

@inproceedings{10.5555/946251.947010,
author = {Silva, Nuno and Rocha, Jo\~{a}o},
title = {Semantic Web Complex Ontology Mapping},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Ontology Mapping is the process whereby two ontologies are semantically related at conceptual level and the source ontology instances are transformed into target ontology entities according to those semantic relations. Ontology mapping faces new challenges in the context of Semantic Web, especially concerning heterogeneity, dynamics, distribution and limitations on representation technology. This paper introduces a new methodology and transformation process based on the notion of Service, which represents system transformation capabilities. MAFRA Toolkit is a specific implementation of MAFRA-Mapping FRAmework, where these new methodology and transformation process are being validated. MAFRA Toolkit is being applied in the European project Harmonise, which aims to provide solutions for (semi-) automatic interoperability between major operators in tourism e-business, MAFRA plays a major role in the specification, representation and reconciliation phases of the sematic mapping within the scope of the Harmonise technology.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {82},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947002,
author = {Khan, Javed I. and Tao, Qingping},
title = {Exploiting Webspace Organization for Accelerating Web Prefetching},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The paper explores how the structure of Webspace can be used for accelerated Web prefetch. We have conducted experiments based on a novel hyperspace aware prefetch proxy and have studied the prefetch performance on several dominant hyperspace patterns. The study assesses the system's responsiveness and background loads for various user interaction duration, surfing and prefetch sequences. The results show that Webspace awareness can help in improving prefetch performance. This study also provides an interesting insight toward a framework where the professional content developers can gain more control towards authoring prefetch friendly collection for increased site responsiveness.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {89},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947004,
author = {Li, Yuefeng and Zhong, Ning},
title = {Ontology-Based Web Mining Model: Representations of User Profiles},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web mining is used to search the right information from the Web to meet user information needs. Acquiring correct user profiles is difficult, since users may be unsure of their interests and may not wish to invest a great deal of effort in creating such a profile. Our aim is to present a foundation for representations of user profiles on ontology for designing efficient Web mining models. In this paper, we assume the user concept can be constructed from some primary ones; hence, we use "part-of" relation to describe the relationship between classes. We also present set-valued relevance functions on such ontology to unravel the relationships between facts and the existing classes. A numerical interpretation is also presented for the set-valued relation functions.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {96},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947016,
author = {King, John and Li, Yuefeng},
title = {Web Based Collection Selection Using Singular Value Decomposition},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As the number of electronic data collections available on the internet increases, so does the difficulty of finding the right collection for a given query. Often the first time user will be overwhelmed by the array of options available, and will waste time hunting through pages of collection names, followed by time reading results pages after doing an ad-hoc search. Automatic collection selection methods try to solve this problem by suggesting the best subset of collections to search based on a query. This is of importance to fields containing large number of electronic collections which undergo frequent change, and collections that cannotbe fully indexed using traditional methods such as spiders. This paper presents a solution to this problems of selecting the best collections and reducing the number of collections needing to be searched. Preliminary tests of the system, conducted onWeb search engines, suggest that this will solve much of the Web based Collection selection problem.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {104},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947008,
author = {Khosla, Rajiv and Goonesekera, Tharanga},
title = {An Online Multi-Agent E-Sales Recruitment System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {E-recruitment area has emerged as an important e-business function in the past few years. This paper describes a novel erecruitment multi-agent application for recruitment and benchmarking of salespersons. Most existing approaches to recruitment rely on the interview process and or on psychometric techniques. Both these approaches have had limited success. The paper describes a multiagent e-sales recruitment system (e-SRS), which integrates a selling behavioral model with expert systems and soft computing techniques like fuzzy-K-means for predicting the selling behavior profile of a sales candidate.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {111},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947023,
author = {Negm, Khaled E. A.},
title = {CARP Compliant Proxy Enforcer Frame Work},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One of the important aspects of network management is the proxy usage. Nowadays it is a common practice to force the user to connect to some network services via the proxy. The disadvantage of this method is that it requires from the user an additional network knowledge and manual client configuration. The problem can be solved by developing a system that willenforce proxy usage and should remain completely transparent to user. In addition to that, the enforcing should reduce network traffic, increase the speed and thus increase performance.In this research we present a frame work of a proxy system that is able to process the requests of a number of different application level protocols. The system deals with program redirection of HTTP protocol requests, but the same scheme can be applied to implement the enforcer for other protocols too. The system is implemented on Linux Red Hat platform and can run on new distribution of host operating systems that implements IP firewall (ipfw). This system also posses a different dimension of implementing and enforcing security policy among enterprises. This could be achieved by stopping any proxy bypass event processed by clients to browse for prohibited sites during the working hours according to certain companies' security policies. The system can be installed on host operating system to provide support to all clients independent of their platforms.The system is tested in a private network that simulates the real traffic environment with 10 proxy servers, 50 hosts that serve 250,000 clients connected via different network topologies, technologies and services. The system showed feasibility and efficiency for improving network performance between 17-23 % which is a fairly successful result.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {118},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946999,
author = {Neumann, G\"{u}nter and Xu, Feiyu},
title = {Mining Answers in German Web Pages},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present a novel method for mining textual answers in German Web pages using semi-structured NL questions and Google for initial document retrieval. We exploit the redundancy on the Web by weighting all identified named entities (NEs) found in the relevantdocument set based on their occurrences and distributions. The ranked NEs are used as our primary anchors for document indexing, paragraph selection, and answer identification. The latter is dependent on two factors: the overlap of terms at different levels (e.g., tokens and named entities) between queries and sentences, and the relevance of identified NEs corresponding to the expected answer type. The set of answer candidates is further subdivided into ranked equivalent classes from which the final answer is selected. The system has been evaluated using question-answer pairs extracted from a popular German quiz book.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {125},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947031,
author = {Tadrus, Samih and Bai, Li},
title = {A QoS Network Routing Algorithm Using Multiple Pheromone Tables},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Quality-of-Services routing algorithms that utilize probes are criticized for not being able to achieve optimal path selection for real-time flows. This is because probe-based algorithms do not have a global view of the network. This paper introduces a new probe-based routing algorithm for packet-switched networks that supports both best-effort and real-time flows. Unlike other probe-based algorithms, probes are routed via routing tables to achieve optimalperformance. Simulations were carried out on QColony and other QoS probe-based routing systems under various network traffic loads and irregular network topologies. Simulation results, concerning resource utilization and connection request success rates, show that QColony provides good performance under heavy loads with failure conditions especially for large networks. The paper also introduces a novel approach for traffic protection in case of network failure conditions.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {132},
numpages = {1},
keywords = {Swarm intelligence, Network routing},
series = {WI '03}
}

@inproceedings{10.5555/946251.947032,
author = {Bonino, Dario and Corno, Fulvio and Squillero, Giovanni},
title = {A Real-Time Evolutionary Algorithm for Web Prediction},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {As an increasing number of users access information on the World Wide Web, there is a opportunity to improve well known strategies for web caching, prefetching, dynamic user modeling and dynamic site customization in order to obtain better subjective performance and satisfaction in web surfing. In this paper, we propose a new method to exploit user navigational path behavior to predict, in real-time, future requests. Predicting user next requests is useful not only for document caching/prefetching, it is also suitable for quick dynamic portal adaptation to user behavior. Real-time user adaptation prevents the use of statistical techniques on web logs, and we propose the adoption of a predictive user modelbased on finite state machines together with an evolutionary algorithm that evolves a population of FSMs for achieving a good prediction rate.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {139},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946991,
author = {Carchiolo, Vincenza and Longheu, Alessandro and Malgeri, Michele},
title = {Improving WEB Usability by Categorizing Information},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Modern browsers allow users to search and navigate the vast amount of web data, but the significant problem of extracting desired information from such data still remains, mainly due to the lack of an explicit structure both in web pages and sites. In this paper, we present an approach to web structuring in which both web pages and sites are considered. In particular, we analyze the structure and semantics, aiming at highlight (possibly hidden) structural andsemantic organization, therefore building an explicit logical schema which improves web usability (browsing, searching) and designing.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {146},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946990,
author = {van Splunter, S. and Sabou, M. and Brazier, F. M. T. and Richards, D.},
title = {Configuring Web Services, Using Structuring and Techniques from Agent Configuration},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper explores the use of an Agent Factory for the composition of web services. Previous work proposed a structuring approach for automated reconfiguration of agents by an Agent Factory. The question is whether the same approach can be applied to web service composition, i.e. whether DAML-S descriptions of web services offer enough structure for automated configuration by the Agent Factory. An example trace of the Agent Factory for configuration of DAML-S web services illustrates this approach.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {153},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947039,
author = {Boella, Guido and van der Torre, Leendert},
title = {Local Policies for the Control of Virtual Communities},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we study the rational balance between local and global policies in web based distributed systems. We use a logical framework for multiagent systems to model obligations and permissions composing policies. In particular, a qualitative decision theory allows agents to trade off the decision of respecting a norm against the consequences of not respecting it: the possibility that they are considered violators and thus sanctioned. Global policies refer not to the existence of a local norm but to the fact that it is enforced by the local authority by recognizing and sanctioning violations.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {161},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947040,
author = {Robles, V. and Larra\~{n}aga, P. and Menasalvas, E. and P\'{e}rez, M. S. and Herves, V.},
title = {Improvement of Na\"{\i}ve Bayes Collaborative Filtering Using Interval Estimation},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recommender systems emerged to help users choose among the large amount of options that e-commerce sites offer. Collaborative filtering is one of the most successful recommender techniques. In this paper we propose an approach to collaborative filtering based on the simple Bayesian classifier. We propose a method of increasing the efficiency of na\"{\i}ve bayes by applying a new semi na\"{\i}ve Bayes approach based on interval estimation. To evaluate our algorithm we use a database of Microsoft Anonymous Web Data from the UCI repository. Our empirical results show that our proposed interval based na\"{\i}ve Bayes approach outperforms typical na\"{\i}ve bayes1.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {168},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947005,
author = {Basili, Roberto and Vindigni, Michele and Zanzotto, Fabio Massimo},
title = {Integrating Ontological and Linguistic Knowledge for Conceptual Information Extraction},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Text understanding makes strong assumptions about the conceptualisation of the underlying knowledge domain. This mediates between the accomplishment of the specific task at the one hand and the knowledge expressed in the target text fragments at the other. However, building domain conceptualisations from scratch is a very complex and time-consuming task. Traditionally, the re-use of available domain resources, although not constituting always the best, has been applied as an accurate and cost effective solution.In this paper, we investigate the possibility of exploiting sources of domain knowledge (e.g. a subject reference system) to build a linguistically motivated domain concept hierarchy. The limitation connected with the use of domain taxonomies as ontological resources will be firstly discussed in the specific light of IE, i.e. for supporting linguistic inference. We then define a method for integrating the taxonomical domain knowledge and a general-purpose lexical knowledge base, like WordNet. A case study, i.e. the integration of the MeSH, Medical Subject Headings, and Word-Net, will be then presented as a proof of the effectiveness and accuracy of the overall approach.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {175},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947035,
author = {Somefun, D. J. A. and J.A. La Poutr\'{e}, J. A.},
title = {Bundling and Pricing for Information Brokerage: Customer Satisfaction as a Means to Profit Optimization},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Traditionally, the study of on-line dynamic pricing and bundling strategies for information goods is motivated by the value-extracting or profit-generating potential of these strategies. In this paper we discuss the relatively overlooked potential of these strategies to on-line learn more about customers' preferences. Based on this enhanced customer knowledge an information broker can by tailoring the brokerage services more to the demand of the various customer groups persuade customers to engage in repeated transactions (i.e., generate customer lock-in). To illustrate the discussion, we show by means of a basic consumer model how, with the use of on-line dynamic bundling and pricing algorithms, customer lock-in can occur. The lock-in occurs because the algorithms can both find appropriate prices and (from the customers' perspective) the most interesting bundles. In the conducted computer experiments we use an advanced genetic algorithm with a niching method to learn the most interesting bundles efficiently and effectively. brokerage; recommender systems.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {182},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947038,
author = {Matsuo, Yutaka and Tomobe, Hironori and Hasida, K\^{o}iti and Ishizuka, Mitsuru},
title = {Mining Social Network of Conference Participants from the Web},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In a ubiquitous computing environment, it is desirable to provide a user with information depending on a user's situation, such as time, location, user behavior, and social context. At conventions, such as academic conferences and exhibitions, where participants must register in advance, the social context of participants can be extracted from the Web using their names and af.liations without asking the participants many questions. In this paper, we attempt to extract the social network of participants from the Web, where a node represents a participant and an edge represents the relationship of two participants. Each edge is added using the number of pages retrieved by a search engine which include both participants names. Moreover, each edge has a label such as "co-authors" and "members of the sameproject" by applying classi.cation rules to the page content. We show an example of the extracted network and make a preliminary evaluation. This network can be used in manyinformation services, such as finding an appropriate introducer or negotiater, and who one should talk to in order to efficiently expand his/her network.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {190},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947018,
author = {Cair\'{o}, Osvaldo and Olarte, Juan Gabriel and Rivera-Illingworth, Fernando},
title = {A Negotiation Strategy for Electronic Trade Using Intelligent Agents},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Information systems with an intelligent or knowledge component are now prevalent and include knowledge-based systems, intelligent agents, and knowledge management systems. These systems are capable of explain their reasoning or justify their behavior. Empirical studies, mainly with knowledge-based systems, are reviewed and linked to a theoretical and practical base. The present paper has two main objectives: a) to present a negotiation strategy that allows the interaction between an intelligent agent and a human consumer. This kind of negotiation is adapted to the Latin American market and idiosyncrasy where an appropriate tool to perform automated negotiations over Web does not exist. b) To include animations in order to show an agent that represents an actual person. This incorporation aims to reduce the impact and gap created by the new technology. The agent presented can find an optimal path to achieve its goal using its mental states and libraries designed for the business roles.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {194},
numpages = {1},
keywords = {Knowledge-base, Web Intelligence, Intelligent Agents, Negotiation, e-commerce},
series = {WI '03}
}

@inproceedings{10.5555/946251.947014,
author = {Kruengkrai, Canasai and Jaruskulchai, Chuleerat},
title = {Generic Text Summarization Using Local and Global Properties of Sentences},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the proliferation of text data on the World-Wide Web, the development of methods for automatically summarizing these data becomes more important. In this paper, we propose a practical approach for extracting the most relevant sentences from the original document to form a summary. The idea of our approach is to exploit both the local and global properties of sentences. The local property can be considered as clusters of significant words within eachsentence, while the global property can be though of as relations of all sentences in the document. These two properties are combined to get a single measure re.ecting the informativeness of sentences. Experimental results show that our approach compares favorably to a commercial text summarizer.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {201},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947001,
author = {Niu, Yonghe and Zheng, Tong and Chen, Jiyang and Goebel, Randy},
title = {WebKIV: Visualizing Structure and Navigation ForWeb Mining Applications},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A significant part of the web mining problem is simply in understanding the value of any mining method. For example, the value of web mining to improve user navigation is even more challenging if one can't visualize the differences over a large collection of web pages or a significant structure within the existing web.We present WebKIV, a tool we've developed to help us visualize our own results in web mining. WebKIV combines strategies from several other web visualization tools, to providea single method of visualizing web structure, and the results of web mining on that structure.We summarize the value of web visualization tools along the dimensions of scale (can one visualize small and large structures), navigation dynamics (can one visualize navigation dynamically or statically), and cumulative usage (can one distinguish individual and aggregate web usage). We then show how WebKIV provides a way of visualizing the results of web mining in a way that distinguishes properties along all three of these dimensions.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {207},
numpages = {1},
keywords = {web structure, visualization, web mining, web navigation},
series = {WI '03}
}

@inproceedings{10.5555/946251.946995,
author = {Tang, Tiffany Ya and Winoto, Pinata and Chan, Keith C. C.},
title = {On the Temporal Analysis for Improved Hybrid Recommendations},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recommender systems address the issue of information overload by providing personalized recommendations towards a target user based upon a history of his/her likes and dislikes. Collaborative filtering and content-based methods are two most commonly used approaches in most recommender systems. Although each of them has both advantages and disadvantages in providing high quality recommendations, a hybrid recommnedation mechanism incorporating components from both of the methods would yield satisfactory results in many situations. Unfortunately, most hybrid approaches have focused on the contents of items but the temporal feature of them, which is the theme of our study here. In particular, we argue, in this paper in the context of movie recommendation, that movies' production year, which reflects the situational environment where the movies were filmed, might affect the values of the movies being recommended, and in turn significantly affect target users' future preferences. We called it the temporal effects of the items on the performance of the recommender systems. We perform some experiments on the famous MovieLens data sets, and significant results were obtained from our experiments. We believe that the temporal features of items can be exploited to not only scale down the huge amount of data set, especially for web-based recommender system, but also allow us to quickly select high quality candidate sets to make more accurate recommendations.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {214},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946997,
author = {Hernandez, Nicolas and Grau, Brigitte},
title = {Topic and Meta-Descriptor Extraction for Text Description},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present a dynamic summarization method that combines text segmentation and topical and rhetorical text description to rapidly skim through texts. The speed reading is made thanks to presentation to the user of relevant expressions from the text, labelled with topical and argumentative specification. In particular, we describe a method to automatically acquire meta-descriptive lexicon from a scientific corpus without specific knowledge resources requirement.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {221},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947036,
author = {Tian, YongHong and Huang, TieJun and Gao, Wen and Cheng, Jun and Kang, PingBo},
title = {Two-Phase Web Site Classification Based on Hidden Markov Tree Models},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the exponential growth of both the amount and diversity of the information that the web encompasses, automatic classification of topic-specific web sites is highly desirable. In this paper we propose a novel approach for web site classification based on the content, structure and context information of web sites. In our approach, the site structure is represented as a two-layered tree in which each page is modeled as a DOM (Document Object Model) tree and a site tree is used to hierarchically link all pages within the site. Two context models are presented to capture the topic dependences in the site. Then the Hidden Markov Tree (HMT) model is utilized as the statistical model of the site tree and the DOM tree, and an HMT-based classifier is presented for their classification. Moreover, for reducing the download size of web sites but still keeping high classification accuracy, an entropy-based approach is introduced todynamically prune the site trees. On these bases, we employ the two-phase classification system for classifying web sites through a fine-to-coarse recursion. The experiments show our approach is able to offer high accuracy and efficient process performance.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {227},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946998,
author = {Feng, Yazhong and Zhuang, Yueting and Pan, Yunhe},
title = {Music Information Retrieval by Detecting Mood via Computational Media Aesthetics},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {It is well known that music can convey emotion and modulate mood, to retrieval music by mood is sometimes the exclusive manner people select music to enjoy. This paper concentrates on music retrieval by detecting mood. Mood detection is implemented on the viewpoint of Computational Media Aesthetics, that is, by analyzing two music dimensions, tempo and articulation, in the procedure of making music, we derive four categories of mood, happiness, anger, sadness and fear. Concretely, with regard to music in the format of raw audio, after tempo is detected using a multiple-agent approach, a feature called relative tempo is calculated, and after the mean and standard deviation of the feature called average silence ratio in the presented computational articulation model are calculated, a simple BP neural network classifier is trained to detect mood. Users retrieval music by browsing the 3D visualization of feature space associated with specific mood. This paper reports the experimental result on a test corpus of 353 pieces of popular music with various genres.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {235},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947006,
author = {Lin, Wei-Hao and Jin, Rong and Hauptmann, Alexander},
title = {Web Image Retrieval Re-Ranking with Relevance Model},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web image retrieval is a challenging task that requires efforts from image processing, link structure analysis, and web text retrieval. Since content-based image retrieval is still considered very difficult, most current large-scale web image search engines exploit text and link structure to "understand" the content of the web images. However, local text information, such as caption, filenames and adjacent text, is not always reliable and informative. Therefore,global information should be taken into account when a web image retrieval system makes relevance judgment. In this paper, we propose a re-ranking method to improve web image retrieval by reordering the images retrieved from an image search engine. The re-ranking process is based on a relevance model, which is a probabilistic model that evaluates the relevance of the HTML document linking to the image, and assigns a probability of relevance. The experiment results showed that the re-ranked image retrieval achieved better performance than original web image retrieval, suggesting the effectiveness of the re-ranking method. The relevance model is learned from the Internet without preparing any training data and independent of the underlying algorithm of the image search engines. The re-ranking process should be applicable to any image search engines with little effort.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {242},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947015,
author = {Di Iorio, Ernesto and Diligenti, Michelangelo and Gori, Marco and Maggini, Marco and Pucci, Augusto},
title = {Detecting Near-Replicas on the Web by Content and Hyperlink Analysis},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The presence of near-replicas of documents is very common on the Web. Documents may be replicated completely or partially for different reasons (versions, mirrors, etc.), or the same resource can be associated to different URLs (dynamically generated pages, etc.). Whilst replication can improve information accessibility by the users, the presence of near-replicated documents can hinder the effectiveness of search engines (for example, decreasing the coverage). We propose a method to detect similar pages, in particular replicas and near-replicas, which is based on a pair of signatures. The first signature is obtained by a random projection of the bag-of-words vector representing the page contents. The second signature is computed by a recursive equation which exploits the connectivity among the Webpages to code the context of each page. The accuracy of the proposed approach is analyzed and validated by experimental results which show that on the given dataset near-replicas can be detected with a precision-recall of 93%.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {249},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947041,
author = {Lau, Raymond Y. K.},
title = {Context Sensitive Text Mining and Belief Revision for Adaptive Information Retrieval},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Autonomous information agents alleviate the information overload problem on the Internet. The AGM belief revision framework provides a rigorous foundation to develop adaptive information agents. The expressive power of the belief revision logic allow a user's information preferences and contextual knowledge of a retrieval situation to be captured and reasoned about within a single logical framework. Contextual knowledge for information retrieval can be acquired via context sensitive text mining. THis paper illustrates a novel approach of integrating the proposed text mining method into the belief revision based adaptive information agents to improve the agents' learning autonomy and prediction power.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {256},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946996,
author = {Valavanis, Efstratios and Ververidis, Christopher and Vazirgianis, Michalis and Polyzos, George C. and Nrv\r{a}g, Kjetil},
title = {MobiShare: Sharing Context-Dependent Data and Services from Mobile Sources},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The rapid advances in wireless communications technology and mobile computing have enabled personal mobile devices that we use in everyday life to become information and services providers by complementing or replacing fixed-location hosts connected to the wireline network. Such mobile resources can be highly important for other moving users, creating significant opportunities for many interesting and novel applications. The MobiShare architecture outlined in this paper provides the infrastructure for ubiquitous mobile access andmechanisms for publishing, discovering and accessing heterogeneous mobile resources in a large area, taking into account the context of both sources and requestors. Any wireless communication technology could be used between a device and the system. Furthermore, the use of XML-related languages and protocols for describing and exchanging metadata gives the system a uniform and easily adaptable interface, allowing a variety of devices to use it. The overall approach is data-centric and service-oriented, implying that all the devices are treated as producers or requestors of data wrapped as information services.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {263},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947013,
author = {Cheung, Kwok-Wai and Sun, Yuxiang},
title = {Mining Web Site's Clusters from Link Topology and Site Hierarchy},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Foraging information in large and complex web sites simply using keyword search usually results in unpleasant experience due to the overloaded search results. To support more effective information search, some descriptive abstractions of the web sites (e.g., sitemaps) are mostly needed. However, their creation and maintenance normally requires recurrent manual effort due to the fast-changing web contents. In this paper, we extend the HITS algorithm and integrate hyperlink topology and web site hierarchy to identify a hierarchy of web page clusters as the abstraction of a web site. As the algorithm is based on HITS, each identified cluster follows the bipartite graph structure, with an authority and hub pair as the cluster summary. The effectiveness of the algorithm has been evaluated using three differentweb sites (containing ~ 6000-14000 web pages) with promising results. Detailed interpretation of the experimental results as well as qualitative comparison with other related works are also included.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {271},
numpages = {1},
keywords = {information search and retrieval, HITS algorithm, Cluster mining, web site analysis},
series = {WI '03}
}

@inproceedings{10.5555/946251.947024,
author = {Richard, Bruno and Tchounikine, Pierre and Jacoboni, Pierre},
title = {An Architecture to Support Navigation and Propose Tips within a Dedicated Website},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present an approach for constructing recommender systems that support a user in his navigation through a given Website. This approach is based on a proxy-like architecture that intercepts the user's requests and the Website answers, analyzes them in respect to an explicit model of the Website and some prototypical uses of it and dynamically generates, in a separate window, a tip and/or an access to additional functionalities related to the Website objective. The paper is illustrated by examples from the construction of a recommender system for a Website proposed by the French Ministry of Research and Education.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {278},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947019,
author = {Rasseneur, Doroth\'{e}e and Jacoboni, Pierre and Tchounikine, Pierre},
title = {Enhancing a Web-Based Distance-Learning Curriculum with Dedicated Tools},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The work presented in this paper aims at supporting students involved in a Web-based distance-learning curriculum. For such a purpose, the Saafir system proposes students with different tools that support them in their appropriation of this curriculum, e.g. visualization of the curriculum from different points of view or construction of individual projects by putting differentitems of the curriculum into relation. In order to allow using Saafir with already existing distance-learning Websites, the system is designed as an epiphyte system, that can be associated to an existing Website without modifying it. It is founded on an ontology-based model of the curriculum. We present the importance of supporting students in their definition of individual projects, the epiphyte approach we use, the modeling of the curriculum and the different support tools proposed by Saafir.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {285},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947025,
author = {Linn, Craig},
title = {Semantic Reliability in Distributed Systems: Ontology Issues and System Engineering},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Key to the successful operation of any distributed computing system is that messages between systems be interpreted in such a way that the sender's desired effect is achieved. To achieve this effect the meaning of a message as interpreted by the receiver must align with the meaning intended by the sender. If it does not then we have semantic mismatch or variance, which if severe can result in system failure. A system with little semantic variance is a semantically reliable system. While relevant to all heterogeneous distributed systems, semantic reliability is particularly relevant to those systems that seek to utilise the ideas embodied in the notion of the semantic web. These systems include: web services, heterogeneous multi-agent systems, and autonomic computing. In such non trivial systems achieving semantic reliability is a complex and multi-facetted challenge. This paper first considers classic notions of reliability in hardware and software engineering. The problem ofmeaning exchange is then considered at length from a semiotic standpoint. It is argued that while the use of ontologies certainly effectively mitigates the problem, ontologies alone cannot be the complete solution and human dimensions must also be considered. By drawing on sources from a range of foundation disciplines (reliability theory, semiotics, linguistics, philosophy, logic, and ontology practice), some of the deeper and more intractable semantic issues associated with meaning exchange are exposed and explored. From this base, semantic reliability's relationship with classic reliability theory is considered, and several systems engineering techniques are identified that have potential to significantly improve semantic reliability, and hence overall system reliability.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {292},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947000,
author = {Chen, Yi and Revesz, Peter},
title = {Querying Spatiotemporal XML Using DataFoX},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We describe DataFoX, which is a new query language for XML documents and extends Datalog with support for trees as the domain of the variables. We also introduce for DataFoX a layer algebra, which supports data heterogeneity at the language level, and several algebra-based evaluation techniques.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {301},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947028,
author = {Narayan, B. L. and Murthy, C. A. and Pal, Sankar K.},
title = {Topic Continuity for Web Document Categorization and Ranking},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {PageRank is primarily based on link structure analysis. Recently, it has been shown that content information can be utilized to improve link analysis. We propose a novel algorithm that harnesses the information contained in the history of a surfer to determine his topic of interest when he is on a given page. As the history is unavailable until query time, we guess it probabilistically so that the operations can be performed of.ine. This leads to a better web page categorization and, thereby, to a better ranking of web pages.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {310},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947012,
author = {Conte, Rosaria and Paolucci, Mario},
title = {Social Cognitive Factors of Unfair Ratings in Reputation Reporting Systems},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we will concentrate on the potential of reputation for optimising electronic transactions, liable to fraud and cheating. No surprise, electronic auctions have been the first arena for the implementation of computerised reputation systems. Economists expect that the probability of cooperation is positively affected by online reputation reporting systems (e.g., eBay), but electronic communities present several opportunities and forms of misbehaving also at the level of reputation. In eBay, rated transactions are about 50% of the total and positive ratings exceed negative ones. This evidence is interpreted in terms of a social cognitive model of reputation, allowing formulating testable hypotheses about the different performances of different reputational systems.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {316},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947020,
author = {Ragab, Khaled and Kaji, Naohiro and Mori, Kinji},
title = {Service-Oriented Autonomous Decentralized Community Communication Technique for a Complex Adaptive Information System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In rapidly changing large-scale information system, users' requirements are changing constantly. To address the extreme dynamism in the large-scale information system, we have proposed the Autonomous Community Information System (ACIS). It is a decentralized bilateral-hierarchy architecture that forms a community of individual end-users (community members) having the same interests and demands in somewhere, at specified time. ACIS allows the community members to mutually cooperate and share information without loading up any single node excessively. In this paper, an autonomous decentralized community communication technique is proposed to assure a flexible, scalable and multilateral communication among the community members. The main ideas behind this communication technique are: content-code communication (community service-based) for flexible information service provision/utilization and multilateral benefits communication for scalable and productive cooperation among members. All members communicate productively for the satisfaction of all the community members. The scalability of the system's response time regardless of the number of the community members has been shown by simulation. Thus, the autonomous decentralized community communication technique reveals significant results of the response time with continuous increasing in the total number of members.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {323},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947007,
author = {Su, Xiaomeng and Matskin, Mihhail and Rao, Jinghai},
title = {Implementing Explanation Ontology for Agent System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The overall issue addressed in this paper is to improve semantic interoperability among and across agent systems. We propose to use explanation as a way to approach that aim. The explanation process is expressed in terms of an explanation ontology shared by the agents who participate to the explanation session. The explanation ontology is defined in a way general enough to support a variety of explanation mechanisms. The paper describes the explanation ontology and provides a working through example illustrating how the proposedgeneric ontology can be used to develop specific explanation mechanism. Finally, the ontology is being integrated into a running agent platform Agora to demonstrate the practical usefulness of the approach.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {330},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947021,
author = {Narahashi, Masaki and Suzuki, Einoshin},
title = {Detecting Hostile Accesses through Incremental Subspace Clustering},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose an incremental subspace clustering method for flexibly detecting hostile accesses to a Web site. Typical log data for Web accesses are huge, contain irrelevant information, and exhibit dynamic characteristics. We overcome these difficulties through data squashing, subspace clustering, and an incremental algorithm. We have improved, by modifying its data squashing functionality, our subspace clustering method SUBCCOM so that it can exploit previous results. Experimental evaluation confirms superiority of our I-SUBCCOM in terms of precision, recall, and computation time.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {337},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946989,
author = {Ye, Shiren and Chua, Tat-seng and Kei, Jeremy R.},
title = {Querying and Clustering Web Pages about Persons and Organizations},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One of the most frequent Web surfing tasks is to search for names of persons and organizations. Such names are often not distinctive, commonly occurring, and nonunique. Thus, a single name may be mapped to several entities. The paper describes a methodology to cluster the Web pages returned by the search engine so that pages belonging to different entities are clustered into different groups. The algorithm uses a combination of named entities, link-based and structure-based information as features to partition the document set into direct and indirect pages using a decision model. It then uses the distinct direct pages as seeds to cluster the document set into different clusters. The algorithm has been found to beeffective for Web-based applications.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {344},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947011,
author = {Liu, Weiru and Liao, Zhining and Hong, Jun and Liao, Zhifang},
title = {Determining Remote System Contention States in Query Processing over the Internet},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In the environment of data integration over the Internet, three major factors affect the cost of a query: network congestion situation, server contention states (workload), and data/query complexity. In this paper, we concentrate on system contention states. For a remote data source, we first determine the total number of contention states of the system through applying clustering techniques to the costs of sample queries. We then develop a set of cost formulae for each of the contention states using a multiple regression process. Finally, we estimate the system's current contention state when a query is issued and using either a time slides method or a statistical method depending on the information we have about the system. Our method can accurately predict the system contention state so that the effect of the contention states on the cost of queries can be estimated precisely.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {351},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947017,
author = {Kwan, VivienWai-Man and Lau, Francis Chi-Moon and Wang, Cho-Li},
title = {Functionality Adaptation: A Context-Aware Service Code Adaptation for Pervasive Computing Environments},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Pervasive computing has attracted a lot of attention in recent years. There are now proxy servers that are specially designed for pervasive computing. To enable content viewing in small devices, different kinds of content adaptation techniques have been used (such as distillation and transcoding) to adapt web contents in content-rich servers to resource-constrained devices. Adaptation of web contents has been widely discussed, but little attention was paid to the adaptation of services (or service code), which is equally important for computing anytime, anywhere, and on any device. In this paper, we present an approach to adaptation of service code which is proxy-based and context-aware, called "functionality adaptation". The main difficulty of such an adaptation is to estimate the resource usage required for an execution, which varies with the input size and is available only at run-time. We propose a conservative solution. A simple prototype has been implemented toevaluate our adaptation approach.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {358},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946992,
author = {Bianchini, M. and Gori, M. and Scarselli, F.},
title = {PageRank and Web Communities},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The definition of the ordering of the Web pages, returned on a given query, is a crucial topic, which gives rise to the notion of Web visibility. A fundamental contribution towards the conception of appropriate ordering criteria has been given by means of the introduction of PageRank, which takes into account only the hyper-linked structure of the Web, regardless of the content of the pages. In this paper, we introduce a circuit analysis which allows us to understand the distribution of PageRank, and show some basic results for understanding the way it migrates amongst communities. In particular, we highlight some topological properties which suggest methods for the promotion of Web communities. These results confirm the importance and the effectiveness of PageRank for discovering relevant information but, at the same time, point out its vulnerability to spamming.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {365},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946986,
author = {Wang, Yao and Vassileva, Julita},
title = {Bayesian Network-Based Trust Model},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose a Bayesian network-based trust model. Since trust is multi-faceted, even in the same context, agents still need to develop differentiated trust in different aspects of other agents' behaviors. The agent's needs are different in different situations. Depending on the situation, an agent may need to consider its trust in a specific aspect of another agent's capability or in a combination of multiple aspects. Bayesian networks provide a flexible method to present differentiated trust and combine different aspects of trust. A Bayesian network-based trust model is presented for a file sharing peer-to-peer application.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {372},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947027,
author = {Ding, Li and Zhou, Lina and Finin, Timothy},
title = {Trust Based Knowledge Outsourcing  for Semantic Web Agents},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The Semantic Web enables intelligent agents to "outsource" knowledge, extending and enhancing their limited knowledge bases. An open question is how agents can efficiently and effectively access the vast knowledge on the inherently open and dynamic Semantic Web. Teh problem is not that of finding a source for desired information, but deciding which among many possibly inconsistent sources is most reliable. We propose an approach to agent knowledge outsourcing inspired by the use trust in human society. Trust is a type of social knowledge and encodes evaluations about which agents can be taken as a reliable sources of information or services. We focus on two important practical issues: learning trust and justifying trust. An agent can learn trust relationships by reasoning about its direct interactions with other agents and about public or private reputation information, i.e., the aggregate trust evaluations of other agents. We use the term trust justification to describe the process in which an agent integrates the beliefs of other agents, trust information, and its own beliefs to update its trust model. We describe the results of simulation experiments of the use and evolution of trust in multi-agent systems. Our experiments demonstrate that the use of explicit trust knowledge can significantly improve knowledge outsourcing performance. We also describe a collaborative trust justification technique that focuses on reducing search complexity, handling inconsistent knowledge, and avoiding error propagation.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {379},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.946988,
author = {Stojanovic, Nenad},
title = {Information-Need Driven Query Refinement},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present a framework for query refinement that is driven by users' information needs. Since a query just approximates a user's need, we analyze the ambiguities of that query with respect to the vocabulary used for querying and the information repository. The goal is to determine the refinements that can help the user to express his original need moreclearly. Particularly, a user is provided with the queries that are "nearby" the given query and a "compass" for determining the ways of changing the ambiguities of the query. The neighborhood of a query is defined regarding the result of that query. The formal concept analysis is used for its efficient calculation. We present an evaluation study that shows the benefits of our approach.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {388},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947029,
author = {Sinka, Mark P. and Corne, David W.},
title = {Towards Modernised and Web-Specific Stoplists for Web Document Analysis},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Research areas such as text classification and document clustering underpin many issues in web intelligence. A fundamental tool in document clustering is a list of stop' words (stoplist) that is used to identify frequent words that are unlikely to assist in classification and are hence removed during pre-processing. Current stoplists are outdated both in light of fluctuations in word usage, and innocent of web-specific' stop words, hence questioning their applicability in web-based tasks. We explore this by developing new word-entropy based stoplists: one derived from random web pages, and one derived from the BankSearch dataset. We evaluate these against other stoplists using accuracies obtained from unsupervised clustering experiments. We find that existing stoplists perform well, but are sometimes outperformed by our new stoplists, especially on hard classification tasks.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {396},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947070,
author = {Lu, Jianjiang and Xu, Baowen and Yang, Hongji},
title = {Matrix Dimensionality Reduction for Mining Web Logs},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web-based logs contain potentially useful data with which designers can assess the usability and effectiveness of their choices. Clustering techniques have been used to automatically discover typical user profiles from Web access logs recently. But it is a challenging problem todesign effective similarity measure between the session vectors which are usually high dimensional and sparse. Non-negative matrix factorisation approaches are applied to dimensionality reduction of the session-URL matrix, and the spherical k-means algorithm is used to partition the projecting vectors of the user session vectors into several clusters. Two methods for discovering typical user session profiles from the clusters are presented last. Theresults of experiment show that our algorithms can mine interesting user profiles effectively.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {405},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947079,
author = {Curran, Kevin and Murphy, Cliona and Annesley, Stephen},
title = {Web Intelligence in Information Retrieval},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web Intelligence is a fascinating area in the very early stages of research and development. It combines the interaction of the human mind and artificial intelligence with networks and technology. How will the next generation Web mature? With the imminent growth of web intelligence what expectations do users have? Users will expect more from the Web than for it to merely pass raw data between people via search engines. This paper attempts to define and summarise the concept of web Intelligence, highlight the key elements of Web Intelligence, and explore the topic of web information retrieval with particular focus on multimedia/information retrieval and intelligent agents.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {409},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947044,
author = {Soh, Ben and Joy, Aaron},
title = {A Novel Web Security Evaluation Model for a One-Time-Password System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One-time passwords (OTPs) have the advantage over regular passwords in that they protect legitimate users from replay attacks by generating a different password for each time of authentication. There are two variables that play a major role in creating a secure OTP; they are the passphrase length and the number of times the one-time password should be hashed. It is already a known fact that the larger the passphrase length the better is the security an OTP can offer. However, there is still a lack of quantitative analysis carried out to study how optimal web security can be achieved. To this end, we propose a novel web security evaluation model that can be used to measure the strength of a one-time password.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {413},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947065,
author = {Wang, Yu and Zhou, Lizhu},
title = {A Hybrid Method for Web Data Extraction},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web data extraction refers to the technology that helps people find wanted information from the Web. In this paper, we first classify existing data extraction algorithms into two classes: Top-Down and Bottom-Up, and then analyze their strengths and weaknesses in terms of extraction accuracy. On the basis of this analysis, we present a hybrid algorithm: Bi-Direction Data Extraction (BiDDE for short), which takes the full strengths of both Top-Down and Bottom-Up algorithms and yet avoid their weaknesses. The experimental results show that BiDDE has not only higher accuracy than Top-Down algorithm and Bottom-Up algorithm, but satisfactory performance.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {417},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947046,
author = {Zhang, Ruofei and Zhang, Zhongfei (Mark) and Yao, Jian},
title = {A Unified Fuzzy Feature Indexing Scheme for Region Based Online Image Querying},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes a novel indexing and retrieval methodology integrating color, texture and shape information for content-based image retrieval in online image databases. This methodology, called PicSearcher, applies unsupervised image segmentation to partition an image into a set of regions, then fuzzy color histogram as well as fuzzy texture and shape properties of each region is calculated to be part of their signatures. The fuzzification procedures resolve the recognition uncertainty stemming from color quantization and humanperception of colors. At the same time, this unified fuzzy scheme incorporates the segmentation-related uncertainties into the retrieval algorithm. Then an adaptive and effective measure for the overall similarity between images is developed by integrating properties of all the regions in the image. An implemented prototype system of PicSearcher has demonstrateda promising retrieval performance for an online test database containing 10,000 general-purpose color images, as compared with its peer systems in the literature.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {421},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947066,
author = {Ishioka, Tsunenori},
title = {Evaluation of Criteria for Information Retrieval},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We investigate van Rijsbergen's F-measure, break-even point, and 11-point averaged precision, all of which can be translated into 1-dimensional scalar quantity from the precisionand the recall. These investigations can be done by comparing to tetrachoric (four-fold) correlation coefficient and phi (four-fold point) coefficient, which are often used as the index of statistical association in a 2 2 contingency table. The results show that when a fallout rate is less than 0.1, (1) the F1 measure has similar properties of the phi coefficient, (2) the break-even point is almost equivalent to a phi coefficient, and (3) the 11-point averaged precision should be a measure which is larger than a phi coefficient and has a value smaller than a tetrachoric correlation coefficient.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {425},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947091,
author = {Kobayashi, A. and Fujioka, H.},
title = {Personalizing a Web Site for Cellular Phones},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We propose a new methodology of personalizing a Web site with access control, to be adapted for cellular phones. We realize it by following three steps. First, we classify all Web pages into either link pages, which are intended to link to another pages, or data pages, which is intended to offer service. We grant access privileges to particular data pages. Second, we eliminate redundant links by calculating the shortest path from a home page to the data pages. Third, we remove the waste link pages from the Web site, by merging link pages based on the similarity between them. The resulting personalized Web site makes it easier for a user to access the data pages offering his requested service.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {432},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947062,
author = {Kouremenos, S. and Vrettos, S. and Stafylopatis, A.},
title = {An Intelligent Agent-Mediated Web Trading Environment},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes an intelligent agent-mediated trading environment for business-to-consumer transactions. The generic architecture of the system enables it to be used with semi-structured product ontologies. The buying behavior model consists of thefollowing fundamental trading agent-mediated stages: Product Brokering, Merchant Brokering and Negotiation Brokering. The Product and Merchant Brokering stages involve three agents: the Indexing Agent and the Retrieval Agent, that collaborate through machine learning, textclassification techniques and fuzzy logic to match potential buyers with products and merchants, and the Profile Agent that creates the buyer's profile using his/her relevance feedback. Finally, the Negotiation stage includes two transaction agents: the Buyer NegotiatorAgent and the Merchant Negotiator Agent, that participate in an auction between the buyers and the suggested'' merchant acting according to the transaction strategies of the buyers and merchant respectively.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {436},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947058,
author = {Huang, Jinghua and Wang, Jing and Chang, Tao and Zhao, Chunjun and Wang, Liao},
title = {A Comparative Framework for EB Systems Development Methodologies},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {EB systems development methodology is one of active researches in the field of software engineering, information systems and EB. This paper, firstly, introduces the existing EB systems development methodologies briefly. Then, we put forward the comparative framework to be as criteria to compare methodologies. Finally, we compare the development methodologies by the framework. The conclusion of the comparison can be used to design a new development methodology and improve existing EB systems development methodologies.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {442},
numpages = {1},
keywords = {EB systems, systems development methodology, Web-based information systems},
series = {WI '03}
}

@inproceedings{10.5555/946251.947094,
author = {Yang, Zhanmin and Mu, Chundi},
title = {The Formalization of Argumentation and Its Application in Network Computing},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The new generation of Network Computing seeks for organic integration of information resources and computing resources in the Internet, whereas most important of all is to deal with the open information in the Internet which acts as an Open Information System. In thepaper the intension and extension of a concept as well as its dynamics, some basic definitions, operations and theorems is first studied, then a new formalization method of argumentation is presented, which emphasizes concept dynamics and argumentation phases. Based on these works, an argumentation system is developed.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {446},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947099,
author = {Hogo, Mofreh and \v{S}norek, Miroslav and Lingras, Pawan},
title = {Temporal Web Usage Mining},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Temporal web usage mining involves application of data mining techniques on temporal web usage data to discover temporal patterns, which describe the temporal behavior of web users. Cluster and associations in web usage mining do not necessarily have crisp boundaries. This paper introduces the temporal web usage mining of web users on one educational web site, using the adapted Kohonen SOM based on rough set properties [1].},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {450},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947104,
author = {Ralha, C\'{e}lia Ghedini and Ralha, Jos\'{e} Carlos Loureiro},
title = {Intelligent Mapping of Hyperspace},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper addresses some particular issues related to the difficult task of automatically structuring information available in ill-structured environments, through a distributed hypermedia system like the Web. We present an original approach to this problem, which coordinates different aspects of automatic computation of relations between nodes in hyperspace, through dynamic linking, using intelligent mapping of the domain material, by the application of spatial reasoning. We present a multi-purpose framework to dynamically structure information in the Web and a proof-of-concept prototype, dubbed Hypermap, implemented to build the spatial cognitive maps of the hyperspace, inspired by the human cognitive mapping process.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {454},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947086,
author = {Huang, Gai-Tai and Yao, Hsiu-Hsen},
title = {A System for Chinese Question Answering},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Traditional Chinese text retrieval systems return a ranked list of documents in response to a user's request. While a ranked list of documents can be an appropriate response for the user, frequently it is not. Usually it would be better for the system to provide the answer itself instead of requiring the user to search for the answer in a set of documents. Since Chinese text retrieval has just been developed lately, and due to various specific characteristics of Chinese language, the approaches of its retrieval are quite different from those studies and researches proposed to deal with Western language. Thus, we have developed an architecture that augments existing search engines so that they support Chinese natural language question answering. In this paper we describe a new approach to build Chinese question answeringsystem. Which we believe to be the first general-purpose, fully-automated Chinese question-answering system available on the web. Our system performs quite well given the simplicity of the techniques being utilized. Experimental results show that question answeringaccuracy can be greatly improved by analyzing more and more matching ERE relation data lists. Simple ERE relation data extraction techniques work well in our system making it efficient to use with many backend retrieval engines.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {458},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947087,
author = {Li, Xiaolin and Xu, Zhiwei and Liu, Xingwu and Yang, Ning},
title = {Community-Based Model and Access Control for Information Grid},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {It is a challenge to integrate and share resources securely across multiple autonomous administrative domains environment. This paper introduces the Community-Based Model of Vega Enterprise Information Grid and its operation mechanism. The individuals and/or institutes forming different communities can not only implement diverse policies and autonomous management of resource sharing without any impact on the other communities, but also achieve a globally shared goal. Based on the model, the access control technique, including framework and uniform formalizing, is presented in detail. The static or dynamic role binding is used for entitling user's request for accessing resources within or across communities, which ensures a globally unified view for user. A prototype describes how these techniques are useful in building an enterprise information grid. The evaluation and future continuing works are presented in the conclusion.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {462},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947054,
author = {Koo, Sang Ok and Lim, Soo Yeon and Lee, Sang Jo},
title = {Building an Ontology Based on Hub Words for Information Retrieval},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper suggests a semi-automatic ontology construction method based on hub words. To do this, we define the words that are related to many other words as hub words. We determine the hub words by the statistical analysis of documents. Additionally, we discuss the base ontology construction process based on hub words and automatic ontology extension method. The proposed ontology can be used like an index file for traditional document retrieval and can offer more semantic information than simple index files.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {466},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947057,
author = {Huang, Jiajin and Liu, Chunnian and Ou, Chuangxin and Yao, Y. Y. and Zhong, Ning},
title = {Attribute Reduction of Rough Sets in Mining Market Value Functions},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The linear model of market value functions is a new method for direct marketing. Just like other methods in direct marketing, attribute reduction is very important to deal with large databases. In the paper, we apply the algorithm of attribute reduction, which is based on the combination of rough set theory with the Boosting algorithm, to the linear model of market value functions. Experimental results compared with the ELSA/ANN model show that the proposed algorithms can be used effectively in the linear model of market value functions.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {470},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947083,
author = {Takeuchi, Haruhiko and Kitajima, Muneo and Urokobara, Haruhiko},
title = {Using Psychological Word Database in Web Search},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose a new approach for indexing web contents. The essence of our approach is that we use a psycho-linguistic word database for calculating the index of web pages. Since there is no existing database designed for this purpose, we started our study by creating a word database. We will show that the database can be effectively used for estimating the reading levels of specific web pages. We will also show that this approach can be used to reflect user profiles in web searches.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {474},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947107,
author = {He, Ge and Xu, Zhiwei},
title = {Design and Implementation of a Web-Based Computational Grid Portal},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Grid Portal are emerging as convenient mechanisms for providing the friendly access interface to grid resource, consistent accessing pattern and easy usage of the grid services, to solve the complexity in using grid computing resources. In this paper, we describe our experience in building a web based computational grid portal, named WebCom. In the design and implementation, several features the grid portal needed such as security, system transparency, ease of use, flexibility, adaptability, and completeness are taken into consideration. Finally, we test the WebCom in the campus computational grid testbed of the Xian Jiaotong Univ. and evaluate and compare the WebCom with other grid portal projects.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {478},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947085,
author = {Revel, Arnaud},
title = {Web-Agents Inspired by Ethology: A Population of "Ant"-Like Agents to Help Finding User-Oriented Information},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this article, we present a web-search ant-agent inspired by ethology and robotics. We detail its implementation on a set of FIPAOS platforms and show useful results in route finding and re-routing. Finally, we discuss its interest and drawbacks in comparison with classical searchengines and give perspectives to overcome them.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {482},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947111,
author = {Kim, Yuna and Kim, Jong},
title = {Web Prefetching Using Display-Based Prediction},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Since the amount of network traffic has rapidly increased with the WWW expansion, users have experienced a long latency when retrieving Web pages. To solve the latency problem, we propose a client-side prefetching mechanism, which reflects changes on Web document structures and utilizes information from the entrance pages of frequently visited Web sites. It starts by constructing link graphs by gathering usage information of visited Web sites, andpredicts the next document to be referenced based on the overall displayed documents in the Web browser. We also manage entrance pages not to be easily replaced from the cache. Our simulation results show that it has a remarkably improved performance: an increased cache hit ratio (by 48%) and a high prefetching effect (by 297%), with a slightly increased network overhead compared to similar previous schemes.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {486},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947052,
author = {Viamonte, Maria Jo\~{a}o and Ramos, Carlos and Rodrigues, F\'{a}tima and Cardoso, Jos\'{e} Carlos},
title = {A Simulation-Based Approach for Testing Market Strategies in Electronic MarketPlaces},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the increasing importance of Electronic Commerce across the Internet it is becoming increasingly evident that in a few years the Internet will host large numbers of interacting software agents. A vast number of them will be economically motivated, and will exchange avariety of goods and services. It is therefore important to consider the economic incentives and behaviours of economic software agents, and to use every available means to anticipate their collective interactions. This paper address this concern by presenting a market simulator designed for analysing agent market strategies based on a complete understanding of Buyer and Seller behaviours, preference model and pricing algorithms.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {490},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947064,
author = {Gao, Xiaoying and Zhang, Mengjie and Andreae, Peter},
title = {Learning Information Extraction Patterns from Tabular Web Pages without Manual Labelling},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes a domain independent approach to automatically constructing information extraction patterns for semi-structured web pages. The approach was tested onthree corpora containing a series of tabular web sites from different domains and achieved a success rate of at least 80%. A signi.cant strength of the system is that it can infer extraction patterns from a single training page and does not require any manual labeling of the training page.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {495},
numpages = {1},
keywords = {Machine learning, semi-structured data, automatic pattern generation, wrapper},
series = {WI '03}
}

@inproceedings{10.5555/946251.947106,
author = {Shin, Seung-Won and Kim, Ki-Young and Jang, Jong-Su},
title = {LRU Based Small Latency First Replacement (SLFR) Algorithm for the Proxy Cache},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Today, many replacement algorithms have been proposed to improve the performance of Web caching. Most suggested algorithms replace documents through calculating the networkcost by several parameters. These algorithms require many parameters and need a long time to select the document for replacement. In this paper, we introduce new algorithm, called LRU based Small Latency First Replacement (LRU-SLFR), which combines LRU policy with real latency to achieve the best overall performance. LRU-SLFR algorithm is an extension of LRU policy with real network latency and access-count. We make the linked-list as the LRU policy and make groups by our algorithms. If proxy must replace a certain document, proxy that uses our algorithm replaces the document that takes the smallest time to load in the Same Conditional Group Window.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {499},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947097,
author = {Sandhu, Kamaljeet and Corbitt, Brian},
title = {User Attributes in Web-Based Electronic Service Adoption Model (E-SAM)},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This study examines the user attributes of web-based electronic services. It aims to develop a web-based electronic service model (e-sam), test its effectiveness, and from it develop an understanding about user interaction with websites. In the process constructs are developed to measure user attributes in using web-based e-services. This paper reports a survey which captured 403 responses on user attributes towards a specific university website. This paper concludes by suggesting a conceptual framework for a web-based e-service adoption model (E-SAM).},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {503},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947069,
author = {Gong, Leiguang and Riecken, Doug},
title = {Constraining Model-Based Reasoning Using Contexts},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Web-based customer service has become a norm of business practice with increasing emphasis on modeling customer needs and providing them with targeted or personalized service solutions in a timely fashion. Almost all the commercial web service systems adopt some kind of simple customer segmentation models and shallow pattern matching or rule-based techniques for high performance. The models built based on these techniquesthough very efficient have a fundamental limitation in their ability to capture and explain the reasoning in the process of determining and selecting appropriate services or products. However, using deep models (e.g. semantic networks), though desirable for their expressive power, may require significantly more computational resources (e.g. time) for reasoning. This can compromise the system performance. This paper reports on a new approach that represents and uses contextual information in semantic net-based models to constrain and prune potentially very large search space, which results in much improved performance in terms of speed and selectivity as evidenced by the evaluation results.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {507},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947059,
author = {Vel\'{a}squez, Juan D. and Yasuda, Hiroshi and Aoki, Terumasa and Weber, Richard},
title = {Using the KDD Process to Support Web Site Reconfigurations},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The continuous improvement of a web site's content, can be the key to attract new customers or maintain the existing ones. A way to obtain such improvement, is to study the behavior of a user while browsing in the web. For the analysis of this behavior two variables are of particular interest: the pages visited during a user session and the time spent in each one of them.The respective web log files contain part of this data. These files, however, can contain a huge number of registers where large part of them possibly do not contain relevant information.This is one of the reasons why finding initially unknown and useful relations in web log registers is a complex task, which can be performed applying the process of KnowledgeDiscovery in Databases (KDD).In this work, we propose a methodology for web mining based on a Data Mart model. We applied this methodology analyzing log files from a certain web site. The respective results, gave very important insights regarding visitors behavior and preferences. This knowledge has been used in the web site's reconfiguration.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {511},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947096,
author = {Lin, Tsong-Wuu and Chou, Yun-Feng},
title = {A Comparative Study of Zernike Moments},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Effective image retrieval by content requires that visual image properties are used instead of textual labels to properly index pictorial data. Shape is one of the primary low-level image features. Many shape representations had been proposed. The Zernike moment descriptor is the most suitable for shape similar-based retrieval in terms of computation complexity, compact representation, robustness, and retrieval performance. In this paper, we study the first 36 Zernike moments and find the dependence relations between them. A new compact representation is proposed to replace the old one. It is not only saving storage capacity but also reducing the execution time of index generation.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {516},
numpages = {1},
keywords = {shape representation, CBIR, correlation analysis, Zernike moments},
series = {WI '03}
}

@inproceedings{10.5555/946251.947108,
author = {Hu, Yongli and Yin, Baocai and Kong, Dehui},
title = {A New Facial Feature Extraction Method Based on Linear Combination Model},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A new facial feature extraction method is proposed in this paper. Based on linear combination model, the method locates feature points in facial images precisely. The model uses the knowledge of prototypic faces to interpret novel faces. To get the knowledge, the prototypesare labeled manually on the feature points. Generally, the construction of the linear combination model depends on pixel-wise alignments of prototypes, and the alignments are computed by an optical flow algorithm or bootstrapping algorithm which is a full-scale optimization and not includes local information such as facial feature points. To combine local facial feature with the linear combination model, a restrained optical flow algorithm is proposed to compute the pixel-wise alignments. With the information of labeled feature points, the model matches the input facial images and extracts the feature points automatically. Implementing the feature extraction method on the MPI face database, the experimental results show that the method has good performance.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {520},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947102,
author = {Kim, Choonho and Kim, Juntae},
title = {A Recommendation Algorithm Using Multi-Level Association Rules},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Recommendation systems predict user's preference to suggest items. Collaborative filtering is the most popular method in implementing a recommendation system. The collaborative filtering method computes similarities between users based on each user's known preference, and recommends the items preferred by similar users. Although the collaborative filtering method generally shows good performance, it suffers from two major problems - data sparseness and scalability. In this paper, we present a model-based recommendation algorithm that uses multi-level association rules to alleviate those problems. In this algorithm, we build a model forpreference prediction by using association rule mining. Multi-level association rules are used to compute preferences for items. The experimental results show that applying multi-level association rules is effective, and performance of the algorithm is improved compared with the collaborative filtering method in terms of the recall and the computation time.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {524},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947081,
author = {Duan, Lijuan and Gao, Wen},
title = {IISM: An Image Internal Semantic Model for Image Database Based on Relevance Feedback},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, a semantic model IISM (image internal semantic model) is introduced. Unlike other semantic extracting methods, IISM extracts the semantic information not by image segmentation and image understanding, but by analyzing relevance feedback image retrieval results. For relevance feedback image retrieval system, the images relevant to query are pointed as positive example, otherwise the images irrelevant to query are pointed as negative examples. It is assumed that these positive examples are related in semantic content. IISM computes comprehensive pair-wise mutual information for all images through analyzing the results of relevance feedback image retrieval. An association with a high mutual informationmeans that one image is semantically associated with another. Semantic retrieval and clustering is carried out based on these association relationships.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {528},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947055,
author = {Khoussainov, Rinat and Kushmerick, Nicholas},
title = {Performance Management in Competitive Distributed Web Search},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Distributed heterogeneous search environments are an emerging phenomenon in Web search. They can be viewed as a federation of independently controlled metasearchers and many specialised search engines. Specialised search engines provide focused search services in a specific domain (e.g. a particular topic). Metasearchers help to process user queries effectively and ef.ciently by distributing them only to the most suitable search engines for each query. Compared to the traditional search engines like Google or AltaVista, specialised search engines (together) provide access to arguably much larger volumes of high-quality information resources, frequently called the "deep" or "invisible" Web [10]. We envisage that such heterogeneous environments will become more popular and influential.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {532},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947090,
author = {Rouane, Khalid and Frasson, Claude and Kaltenbach, Marc},
title = {A Framework for an Advanced Reading Support in the Digital Library Age},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The promise of digital libraries is an easy and rapid access to digital documents using information technology. However, in an educational context, helping the students access the right document is not sufficient. Systems must be able to help during the reading phase as well. This article presents the LEKC framework (Learning by Explicit Knowledge Construction) [14][15] which aims to take advantage of the natural annotation activity to increase the reader's engagement with the document and enhance his understanding, by externalizing and making explicit some of the implicit knowledge construction processes taking place.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {537},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947056,
author = {Nagino, Norikatsu and Yamada, Seiji},
title = {Future View: Web Navigation Based on Learning User's Browsing Patterns},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper, we propose a Future View system that assists user's usual Web browsing. The Future View will prefetch Web pages based on user's browsing strategies and present them to a user in order to assist Web browsing. To learn user's browsing patterns, the Future View uses two types of learning classifier systems: a content-based classifier system for contents change patterns and an action-based classifier system for user's action patterns. The resultsof learning is applied to crawling by Web robot, and gathered Web pages are presented to a user through a Web browser. We experimentally show effectiveness of navigation using the Future View.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {541},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947077,
author = {Yamakami, Toshihiko},
title = {Towards Know-When Technology in the Mobile Information Space: Long-Term User Trace Log Analysis in the Mobile Internet},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The mobile Internet is one of the most promising application domains in the computer communications. The mobile handsets close the gap between end users and the computercommunications using the 24-hour 365-day availability. The emerging success in Japan has demonstrated the potential capabilities for the next generation Internet platform. The close relationship to the end user reveals a new research area of the user behavior analysis. Using the user transaction logs for over a year, the author tries to build up a new knowledge domain: know-when knowledge. A methodology to capture the user behavior change over a span of time is important to manage the subscription-based mobile Internet services. Using the know-when knowledge acquisition, the author explores the possible cues for the long-term analysis of user behavior patterns using the long-term transaction logs. Considering the behavior dynamism of the mobile Internet users, it is important to explore the know-when technologies for the future mobile Internet marketing.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {545},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947098,
author = {Pacey, D. and Dempster, E. and Williams, M. H. and Cawsey, A. and Marwick, D. and MacKinnon, L.},
title = {A Toolkit for Creating Personalized Presentations},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the explosion in the availability of information on-line, users are finding it increasingly difficult to track down the specific material that they require. Users are therefore becoming increasingly dependent on intelligent services to provide information that is dynamically selected and presented according to their preferences. However, development of these personalized services is not trivial. Significant effort is required in terms of engineering the underlying knowledge that is used by a service to determine which information might be relevant to a particular user and how to present it. A Toolkit that reduces the complexity involved in the creation and maintenance of such services is discussed in this paper.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {550},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947071,
author = {Ceglowski, Maciej and Coburn, Aaron and Cuadrado, John L.},
title = {An Automated Management Tool for Unstructured Data},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The rapidly growing quantity of online data has created a need for automated, content-based categorization and search tools. The authors describe an open-source, Web-based archive management which uses latent semantic indexing, coupled with vector clustering techniques, to provide users with a fully searchable and automatically categorized interface to a data collection. The default English document parser included in the project uses part-of-speech tagging and recursive maximal noun phrase extraction to create a more effective term list for LSI than traditional stop list techniques.The archive interface supports multiple user views of the data collection. Advanced search features are implemented through relevance feedback, and do not require users to learn a query syntax.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {554},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947112,
author = {Teles, Wesley Martins and Weigang, Li and Ralha, C\'{e}lia Ghedini},
title = {AntWebâ€”The Adaptive Web Server Based on the Ants' Behavior},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the AntWeb system, developed under the research area of Web Intelligence (WI). Our approach to AntWeb application is inspired by the ant colonies foragingbehavior, to adaptively mark the most significant links, by means of the shortest route to arrive to target pages. We consider the web users as artificial ants, and use the ant theory as a metaphor to guide user's activity in the Web site. In this paper, we describe the ant's theory in which AntWeb is based on. We also present the AntWeb system, its implementation and a case study with some experiments. The database in AntWeb stores a vast amount of information related to the users' visit to Web sites, which can be useful for further Web mining.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {558},
numpages = {1},
keywords = {ant's behavior, pheromone, Web Intelligence, adaptive Web server, AntWeb system},
series = {WI '03}
}

@inproceedings{10.5555/946251.947073,
author = {Menasalvas, E. and Pardo, B. and Millan, S. and Hochsztain, E. and Pe\~{n}a, J. M.},
title = {Expected Value of User Sessions: Limitations to the Non-Semantic Approach},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The amazing evolution of e-commerce and the fierce competitive environment it has produced have encouraged commercial firms to apply intelligent methods to take advantage of competitors by gathering and analyzing information collected from consumer web sessions. Knowledge about user objectives and session goals can be discovered from the information collected regarding user activities, as tracked by web clicks. Most current approaches to customer behaviour analysis study the user session by examining only web page accesses. To find out about navigators behaviour is crucial to web sites sponsors attempting to evaluate the performance of their sites. Nevertheless, knowing the current navigation patterns is not always enough. Very often it is also necessary to measure sessions value according to business goals perspectives. This paper presents two different measures to include business goals inside click stream analysis. Each of the alternatives is discussed and evaluated in terms of how company's objectives and expectations are taken into account as well as how this approach could be achieved.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {562},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947100,
author = {Korhonen, Jarmo and Pajunen, Lasse and Puustj\"{a}rvi, Juha},
title = {Automatic Composition of Web Service Workflows Using a Semantic Agent},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents a way to automatically compose web service workflow that uses component web services. The web services workflows are described using our transactional workflow ontology. The workflow ontology can be used to describe both component web service workflows and master web service workflows. We have also implemented a workflow engine that runs the workflow instances. Here we analyze using our ontology and some workflow instances with a reasoning agent to automatically find a composed workflow that fulfills all given constraints. The result from the inference is a workflow instance that can be executed using our workflow engine. We see that this kind of dynamic composition is needed in dynamic heterogeneous environments with loosely coupled web services.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {566},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947078,
author = {Yao, J. T. and Yao, Y. Y.},
title = {Web-Based Information Retrieval Support Systems: Building Research Tools for Scientists in the New Information Age},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The concept of Web-based Information Retrieval Support Systems (WIRSS) is introduced. The needs for WIRSS are shown by a detailed case study of existing research article indexing and citation analysis systems, such as Current Content, DBLP, Science Citation Index and CiteSeer. The objective of WIRSS is to build new and effective research tools for scientists to access, explore and use information on the Web, which may lead to improved research productivity and quality.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {570},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947063,
author = {Hammami, Mohamed and Chahir, Youssef and Chen, Liming},
title = {WebGuard: Web Based Adult Content Detection and Filtering System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes a Web filtering system "WebGuard", which aims to automatically detect and filter adult content on the Web. WebGuard uses Web crawler to extract relevant data from the Web, combines the textual content, the image content, and the URL name of a Web page to construct its feature vector. WebGuard uses data mining techniques to classify URLs into two classes: suspect URLs and normal URLs. The suspect URLs are stored in a database, which is constantly and automatically updated in order to reflect the highly dynamic evolution of the Web. When working, WebGuard simply captures a user's URL, matches it with the suspect URLs stored in the database and takes an appropriate action - filtering or blocking - according to the result of the analysis. Our preliminary results show that it can detect and filter adult content effectively.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {574},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947095,
author = {Khan, Javed I. and He, Yihua},
title = {Embedded Data Indexing for Fast Stream Interception by Internet Appliances},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Interception of a data stream is central to any intelligent and dynamic processing of web information. It is perhaps as fundamental to Internet services' overall architecture as the design of disk scheduling to the conventional machine architecture. In this paper we discuss an IPv6 based indexing protocol that can facilitate random access into multilevel hierarchicallyencoded content streams and provide serious performance boost to web content processing.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {579},
numpages = {1},
keywords = {network appliances, filtering, stream interception, IPV6},
series = {WI '03}
}

@inproceedings{10.5555/946251.947092,
author = {Ding, Chen and Chi, Chi-Hung},
title = {A Generalized Site Ranking Model for Web IR},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Normally, the unit for a ranking model in a web IR system is a web page, which is, sometimes,just an information fragment. A larger unit considering the linkage information may be desired to reduce the cognitive overload for users to identify the complete information from the interconnected web. It is the purpose of this paper to propose a ranking model to measure the relevance of the whole web site. We take some illustrations to show the idea and provideevidences to indicate its effectiveness.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {584},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947109,
author = {Butz, C. J. and Liu, J.},
title = {A Query Processing Algorithm for Hierarchical Markov Networks},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Hierarchical Markov networks (HMNs) were recently proposed as a faithful representation of Bayesian networks. In this paper, we propose a query processing algorithm for HMNs. This method takes one query processing algorithm for a traditional Markov network and extends it to a hierarchy of Markov networks. Experimental results explicitly demonstrate the effectiveness of our approach. The work here will then be useful to any problem utilizing Bayesian networks, such as traditional information retrieval, web search, user profiling, multi-agents and e-commerce.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {588},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947080,
author = {Bianchi-Berthouze, Nadia and Katsumi, Naoto and Yoneyama, Harutaka and Bhalla, Subhash and Izumita, Tomoko},
title = {Supporting the Interaction between User and Web-Based Multimedia Information},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Nowadays, a major activity on the Internet is the retrieval and browsing of multimedia information. Yet, today's search engines are not really up to the task. Users often have to query various search engines and browse many web-sites before finding a satisfactory answer. Once users find such an answer, presentation of the results is very rigid, i.e., not tailored to the users' needs, and it does not enable users to manipulate the data. Thus, this retrieval activity often becomes tedious. To improve user-information interaction, a major issue is to give search-engines the ability to access data semantics. In this study, we propose a framework to address the issue. Our framework combines database and multimedia data mining to endow web-based applications with the ability to let users manipulate the data at different levels of interest. As an experimental test-bed, we implemented a holiday planner that supports users in their search for a hotel. By using database technology, web-based multimedia interaction becomes possible, offering as a side effect more feasible and reliable feedback.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {593},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947110,
author = {Hammouda, Khaled M. and Kamel, Mohamed S.},
title = {Incremental Document Clustering Using Cluster Similarity Histograms},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Clustering of large collections of text documents is a key process in providing a higher level of knowledge about the underlying inherent classification of the documents. Web documents, in particular, are of great interest since managing, accessing, searching, and browsing large repositories of web content requires efficient organization. Incremental clustering algorithms are always preferred to traditional clustering techniques, since they can be applied in a dynamic environment such as the Web. An incremental document clustering algorithm is introduced in this paper, which relies only on pair-wise document similarity information. Clusters are represented using a Cluster Similarity Histogram, a concise statistical representation of the distribution of similarities within each cluster, which provides a measure of cohesiveness. The measure guides the incremental clustering process. Complexity analysis and experimental results are discussed and show that the algorithm requires less computational time than standard methods while achieving a comparable or better clustering quality.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {597},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947114,
author = {Mak, Harry and Koprinska, Irena and Poon, Josiah},
title = {INTIMATE: A Web-Based Movie Recommender Using Text Categorization},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents INTIMATE, a web-based movie recommender that makes suggestions by using text categorization to learn from movie synopses The performance of various feature representations, feature selectors, feature weighting mechanisms and classifiers is evaluated and discussed. INTIMATE was also compared with a feature-based movie recommender. The results show that the text-based approach outperforms the feature-based if the ratio of the number of user ratings to the vocabulary size is high.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {602},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947103,
author = {Tian, Fengzhan and Zhang, Hongwei and Lu, Yuchang},
title = {Research on Modeling with Dynamic Bayesian Networks},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {For simplicity of calculation, Dynamic Bayesian Networks (DBNs) make assumptions that their evolvement follows Markov process and the transition probabilities in the evolvement are time-invariant. While this is not the case in many real complex systems. For the purpose of modeling these complex systems with DBNs, the paper attempts to add hidden variables to the evolutional process so as to build Markov models and expands the EM-EA algorithm to the DBNs learning in the presence of hidden variables. Moreover, as for the time-variant transition probabilities, the paper estimates the sufficient statistics of posterior time slices using polynomial fitting algorithm, and then learns the time-variant transition probabilities with both current sufficient statistics and estimated sufficient statistics. The theoretical analysis demonstrates the validity of the methods of the paper. The future work is to do experimentalanalysis with real complex systems.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {606},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947093,
author = {Bullot, Hadrien and Gupta, S. K. and Mohania, M. K.},
title = {A Data-Mining Approach for Optimizing Performance of an Incremental Crawler},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Crawlers visit the Web to maintain a local repository of web pages up to date. In this paper we introduce another perspective to build an effective incremental crawler. Based on previous work in this field, we study how we can improve the performance of a crawler using data-mining. The information collected from the users can help the crawler to know which are the popular pages and to revisit them as soon as possible.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {610},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947089,
author = {Tomaz, Ricardo Ferraz and Labidi, Sofiane},
title = {Increasing Matchmaking Semantics in Intelligent Commerce System},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This Work is part of the ICS (Intelligent Commerce System) project whose aim is to design and implement an effective Intelligent B2B Commerce System. This paper presents a practical application of Semantic Web and Web Services concepts where a matching procedure toassociate potential buyers and suppliers in an B2B system is shown and the use of patterns and tools in the development of Semantic Web's structure is proposed as an alternative for its implementation.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {616},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947075,
author = {Yao, Zhongmei and Choi, Ben},
title = {Bidirectional Hierarchical Clustering for Web Mining},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we propose a new bidirectional hierarchical clustering system for addressing challenges of web mining. The key feature of our approach is that it aims to maximize the intra-cluster similarity in the bottom-up cluster-merging phase and it ensures to minimize the inter-cluster similarity in the top-down refinement phase. This two-pass approach achieves better clustering than existing one-pass approaches. We also propose a new cluster-merging criterion for allowing more than two clusters to be merged in each step and a new measure of similarity for taking into consideration not only the inter-connectivity between clusters but alsothe internal connectivity within the clusters. These result in reducing the average complexity for creating the final hierarchical structure of clusters from O(n2) to O(n). The hierarchical structure represents a semantic structure between concepts of clusters and is directly applicable to the future of semantic net.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {620},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947076,
author = {Yang, Hsin-Chang and Lee, Chung-Hong},
title = {A Text Mining Approach on Automatic Generation of Web Directories and Hierarchies},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {There are enormous amount of web pages in the world. Retrieval of required information from the WWW is thus an arduous task. Different models for retrieving web pages have been used by the WWW community. One of the most widely used model is by traversing a predefined web directory hierarchy to reach a user's goal. The web directories are compiled or classified folders of web pages and are usually organized into a hierarchical structure. The classificationof web pages into proper directories and the organization of directory hierarchies are generally performed by human experts. In this work, we provide a method to apply a kind of text mining techniques on a set of web pages to automatically create web directories and organize them into hierarchies. The method is based on the self-organizing map learning algorithm and requires no human intervention during the construction of web directories and hierarchies. Theexperiments show that our method can produce comprehensible and reasonable web directories and hierarchies.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {625},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947061,
author = {Tsumoto, Shusaku},
title = {Web Based Medical Decision Support System for Neurological Diseases},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In early 1980s, many medical expert system were developed with knowledgebase which are acquired from medical experts, and their performance was almost as good as domain experts. However, they were not frequently used mainly due to the poor user interface and the lack in learning new knowledge. However, the solutions of these two problems have been introduced since 1990s. For the latter problem, machine learning methods have provided several solutions, and for the former problem, the rapid progress of web technologies enables us to implement a good user interface. Furthermore, recent advances in computer resources strengthen these two solusions. In this paper, we focus on the interface problem. The recent advances in web technologies were used for an ef.cient interface of medical expert system. Then, the system was put on the internet to provide an intelligent decision support in telemedicine and is now being evaluated by region medical home doctors.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {629},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947047,
author = {Leblet, Jimmy and Quafafou, Mohamed},
title = {A New Method for Query Generation Applied to Learning Text Classifiers},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this article, we introduce a new method for query generation. This method uses only a logical approach and does not need a statistical process or a natural language processing.The main interest of this new method is the abstraction. In a second part, we discuss a method for learning a text classifier and query generation for this classi.er. The two problems are resolved in a complementary approach using our query generation method and SVM as text classifiers. We use this approach for studying words polysemy. Our method generates queries in order to retrieve documents about a specific sense of the word and in the same timelearning the associated text classifier. Our method have good results.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {633},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947074,
author = {Arnt, Andrew and Zilberstein, Shlomo},
title = {Learning to Perform Moderation in Online Forums},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Online discussion forums are a valuable resource for people looking to find information, discuss ideas, and get advice on the Internet. Unfortunately, many forums have too much activity and information available, resulting in information overload. Moderation systems are implemented in some forums as a way to handle this problem, but due to sparsity issues, they are often not sufficient. In this paper we describe a novel method for learning from past moderations to develop a classifier that can perform automated moderation and thus address the sparsity problem. Additionally, we discuss the possibility of training a moderating classifieron a moderated forum and then applying it to an otherwise unmoderated forum.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {637},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947053,
author = {Felden, Carsten and Chamoni, Peter},
title = {Web Farming and Data Warehousing for Energy Tradefloors},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The recent liberalisation of the German energy market forced the energy industry to develop and install new information systems to support agents on the energy trading floors in their analytical tasks. Besides classical approaches of building a data warehouse to give insight into the time series to understand market and pricing mechanisms it is crucial to provide a variety of external data from the web. Weather information as well as political news or market rumors are relevant to give the right interpretation to the variables of a volatile energy market. Starting from a multidimensional data model and a collection of buy and sell transactions a data warehouse is built that gives analytical support to the agents. Following the idea of web farming we harvest the web, match the external information sources after a filtering and evaluation process to the data warehouse objects and present this qualified information on a user interface where market values are correlated with those external sources over the time axis.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {642},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947072,
author = {Yu, Wen-der and Lai, Chien-chung},
title = {WICE: A Web-Based Intelligent Cost Estimator for Real-Time Decision Support},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Real-time response to construction cost estimation request is crucial for firms to survive and grow in the construction industry. However, no existing construction cost estimating system fulfills this need thus far. This paper describes a joint effort by the academia and the industrial partner on developing such a system. Advanced web-based intelligence techniques are employed in the proposed system including WWW, neuro-fuzzy system, and data mining. The industrial partner is in charge providing knowledge sources, including expert judgments and historical data, for conceptual cost estimation. Web-based Intelligent Cost Estimator (WICE) is resulted. The testing results show that the proposed system provides not only a globally accessible and promptly responding means for cost estimation, but also an effective and reliable tool for real-time decision-making.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {646},
numpages = {1},
keywords = {Neuro-fuzzy system, Intelligent web-agent, Conceptual cost estimation},
series = {WI '03}
}

@inproceedings{10.5555/946251.947115,
author = {Zhu, Yingying and Zhou, Dongru},
title = {Video Browsing and Retrieval Based on Multimodal Integration},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The rapid growth of multimedia data requires more effective content-based video browsing and retrieval. In this paper, we present a system developed for video browsing and retrieval based on multimedia integration. First, a basic structure of the system is defined. Second, arobust scene segmentation method is presented, which analyzes audio and visual information and accounts for their inter-relations and coincidence to semantically identify video scenes. We then extract text from key frames with video OCR technique and extract text transcriptions by speech recognition to classify video scenes and form the full-text indices. Finally, naturallanguage understanding technique is used to automatically classify video scenes on the basis of the texts obtained from close caption, video OCR process and speech recognition. In this way, we have developed the content-based video database system which integrates multimodality to browse and retrieve video data. The experimental results show that multimodal integration is effective for video scene segmentation. Our system built on the idea of multimodal integration makes content-based browsing and retrieval of video data, key-frame-based video abstract and search by keywords practical.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {650},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947045,
author = {Ardissono, L. and Goy, A. and Petrone, G. and Sch\"{a}fer, R. and Holland, M. and Friedrich, G. and Russ, C.},
title = {Intelligent User Interfaces for Web-Based Configuration Systems},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We present a model for the integration of Intelligent User Interfaces and Configuration techniques. This model enhances the capabilities of on-line stores by supporting the development of con.guration systems that assist customers in a personalised way, while they selects the features of the products/services to be configured.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {654},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947082,
author = {Wu, Zonghuan and Raghavan, Vijay and Qian, Hua and Rama, Vuyyuru and Meng, Weiyi and He, Hai and Yu, Clement},
title = {Towards Automatic Incorporation of Search Engines into a Large-Scale Metasearch Engine},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A metasearch engine supports unified access to multiple component search engines. To build a very large-scale metasearch engine that can access up to hundreds of thousands of component search engines, one major challenge is to incorporate large numbers of autonomous search engines in a highly effective manner. To solve this problem, we propose automatic search engine discovery, automatic search engine connection, and automatic search engine result extraction techniques. Experiments indicate that these techniques are highly effective and efficient.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {658},
numpages = {1},
keywords = {Web Data Extraction, Metasearch Engine},
series = {WI '03}
}

@inproceedings{10.5555/946251.947067,
author = {Yu, M. and Taleb-Bendiab, A. and Reilly, D. and Omar, Wail},
title = {Ubiquitous Service Interoperation through Polyarchical Middleware},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Next generation software applications will be required to run on globally distributed heterogeneous assemblies of disparate resources including: emerging computing grids. Such application calls for seamless integration and interoperation between varieties of service standards and architectures developed and deployed using current service middleware standards and architectures. Whilst such middleware adequately provides different APIs, programming models for distributed components and services integration and interoperation at both design and runtime. There is still need for additional middleware services including thesupport of runtime multi-standard services invocation regardless of the components/service standards and type of middleware used. Based on an ongoing research focusing on self-adaptive software for adaptive middleware, this paper will describe a proposed on-demand(runtime) service invocation mechanism, and the associated service interoperation mechanism.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {662},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947049,
author = {Ohmukai, Ikki and Takeda, Hideaki},
title = {Social Scheduler: A Proposal of Collaborative Personal Task Management},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes a collaborative approach for personal task management which is modeled as an integration of alliance and human-in-the-loop model. Alliance model is based on information sharing and collaboration of several persons. They disclose their task condition and maintain to be updatable by their friends. To avoid privacy issues we propose emergent group discovery algorithm to control the level of disclosure. Human-in-the-loop model consistsof three sub-systems to support decision-making activities. Visualizer indicates the attributes associated with each task such as the deadline, the subjective priority, and the workload, which are determined by the user. Optimizer generates executable schedules from these tasks by active scheduler and multi-objective genetic algorithm. Recommender evaluates these alternatives by analytic hierarchy process. We implement client/server system called Social Scheduler on cell-phones environment. We remark the advantages of our approach with an experiment.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {666},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947088,
author = {Nyongesa, Henry O. and Shicheng, Tian and Maleki-Dizaji, S. and Huang, Shr-Ting and Siddiqi, J.},
title = {Adaptive Web Interface Design Using Fuzzy Logic},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The paper presents the development of a web portal to support on-line shopping, using fuzzy logic to enable adaptability in order to improve users' shopping experience. The adaptive interface is designed at the level of presentation of product category, through capturing and inferring dynamic user behaviour. Personalised information such as past shopping records and current shopping list is maintained. Evaluation regarding functionality and usability is performed and the results, in general, show that using a fuzzy approach for adaptation can viably be incorporated into Web interfaces.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {671},
numpages = {1},
keywords = {Web Intelligence, E-commerce, Online Shopping, Adaptive Interfaces, Fuzzy Logic},
series = {WI '03}
}

@inproceedings{10.5555/946251.947084,
author = {Lin, Yuan and Zhanhuai, Li},
title = {The Concept of Attribute Dimension and Corresponding Operations},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Member attribute is used to describe the property of dimension members. It is not fully understood or well defined by OLAP research community. This paper focuses on a special kind of member attributes that could also be used as dimensions called attribute dimensions. To facilitate this kind of multidimensional data modeling from real-world applications, the classic multidimensional data structure is extended and a group of algebraic operations are introduced to formulate corresponding multidimensional queries. In this extended model, theattribute dimension is regarded as a special view' and can be stated either statically or dynamically based on member attribute. With this approach, both ROLAP and MOLAP can benefit from storage saving and reduced processing time. Compared with current OLAP products and research papers, built-in integrity restraint on member attribute and multidimensional data set makes this extended model unique.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {675},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947050,
author = {Callaghan, M. J. and Harkin, J. and McGinnity, T. M. and Maguire, L. P.},
title = {Adaptive Intelligent Environment for Remote Experimentation},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The use of laboratory experiments is a critically important aspect of engineering education where experience has shown that a complementary approach combining theoretical and practical exercises is vital for effective learning. Increasingly, teaching institutions are offering web based remote access to distant laboratories as part of an overall e-learning strategy. The design and implementation of effective and usable remote experimentation facilities poses unique challenges given the inherent complexities of the learning environment and the constraints imposed by the delivery medium. Developments in recent years have addressed many of these issues. However autonomous learning environments by their very nature offer minimal educator assistance and from a students perspective it is inevitable that at some stage of the experimental process, context specific help will be required. This paper seeks to address this issue in the context of remote experimentation for Embedded Systems and presents an adaptive intelligent learning environment with intelligent user help.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {680},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947051,
author = {Dobsa, Jasminka and Basic, Bojana Dalbelo},
title = {Concept Decomposition by Fuzzy K-Means Algorithm},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The method of Latent Semantic Indexing (LSI) is an information retrieval technique using a low-rank singular value decomposition (SVD) of the term-document matrix. Although the LSI method has empirical success, it suffers from the lack of interpretation for the low-rank approximation and, consequently, the lack of controls for accomplishing specific tasks in information retrieval. A method introduced by Dhillon and Modha is an improvement in that direction. It uses centroids of clusters or so called concept decomposition for lowering the rank of the term-document matrix. Our work is focused on improvements of that method using fuzzy kmeans algorithm. Also, we compare the precision of information retrieval for the two above methods.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {684},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947105,
author = {Milic-Frayling, Natasa and Sommerer, Ralph and Rodden, Kerry},
title = {WebScout: Support for Revisitation of Web Pages within a Navigation Session},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {WebScout is a system that creates a personal archive of Web pages seen by the user and a rich record of the user's navigation, including various types of user and system generated annotations. In this paper we explore how this rich archive can be used to provide support foruser navigation, in particular, for revisitation of pages within a navigation session. We describe the WebScout SessionNavigator feature that enhances the current browser functionality by providing both sequential and graph representation of the user navigation. It introduces the concept of a WebTrail which designates a sequence of navigation steps, started by a particular event, such as search, or explicit specification of a URL by typing into the address bar, or executing a link from a bookmark list. We present details of a user study that explores how users perceive and remember their navigation on the Web.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {689},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947060,
author = {Yan, Zhuang and Fong, Simon and Meilin, Shi},
title = {Negotiation Paradigms Based on Knowledge Bead's Methodology},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Online negotiation for today's B2B e-commerce plays a promising role in assisting traders to best fulfill their business deals. But existing uncertain constraints and sophisticated strategies presented in e-trading environment make negotiation a rather complex process. As business intelligence and efficient protocols are essential to successful negotiation, we defined an object-oriented ontology Knowledge Bead (KB) for knowledge representation and enabling agent negotiation in e-trading environment [1]. This paper continues the research work of KB on formulating its theorems and methodology. Some typical negotiation paradigms usingappropriate strategies based on the KB's methodology are presented. In particular, KB's taxonomies and their use in the negotiation process are discussed.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {694},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947068,
author = {Kao, Hung-Yu and Ho, Jan-Ming and Chen, Ming-Syan},
title = {Clustering for Web Information Hierarchy Mining},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Benefiting from the growth of techniques of dynamic page generation, the amount and the complexity of Web pages increase explosively. The structures of Web pages which are dynamically generated by the same templates are thus similar to one another and are usually assembled by a set of fundamental information clusters These neighboring information clusters usually represent the similar semantics and form a larger cluster with the more generalizedinformation. The hierarchical structure generated by information clusters in a bottom-up manner is called the information hierarchy of a page. In this paper, we study the problem of mining the information hierarchies of pages in Web sites to recognize the information distribution of pages within the multi-level, multi-granularity configurations. Explicitly, we propose an information clustering system that applies a top-down information centroid searching algorithm and a multi-granularity centroid converging process on the document object model (DOM) trees of pages to build the information hierarchies of pages. Experiments on several real news Web sites show the high precision and recall rates of the proposed method on determining information clusters of pages and also validate its practical applicability to real Web sites.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {698},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947113,
author = {Clark, James and Koprinska, Irena and Poon, Josiah},
title = {A Neural Network Based Approach to Automated E-Mail Classification},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In this paper we present a neural network based system for automated e-mail filing into folders and anti-spam filtering. The experiments show that it is more accurate than several other techniques. We also investigate the effects of various feature selection, weighting and normalization methods, and also the portability of the anti-spam filter across different users.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {702},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947048,
author = {Maleki-Dizaji, S. and Othman, Z. A. and Nyongesa, H. O. and Siddiqi, J.},
title = {Evolutionary Reinforcement of User Models in an Adaptive Search Engine},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The volume and variety of the Internet information is exponentially grows and therefore causes difficulties for a user to obtain information that accurately matches of the user interested. Several combination techniques are used to achieve the precise goal. This is due, firstly, to the fact that users often do not present queries to information retrieval systems that optimally represent the information they want, and secondly, the measure of a document's relevance ishighly subjective and variable between different users. This paper addresses this problem with an approach that relies on evolutionary user-modelling, in order to retrieve domain-specific information. The paper describes an adaptive information retrieval system that learns user needs from user-provided relevance feedback. The method combines qualitative feedback measures using fuzzy inference, and quantitative feedback using genetic algorithms (GA) fitness measures. In this paper, we utilised the multi-agent design approach for designing an information retrieval system (IRS). The system consists of following combination of complexprocesses: document indexing, learning strategic for relevant feedback and user modelling using genetic algorithm, filtering and ranking the retrieve documents based on the user model. This paper shows the design of the IRS consists of several agents that cooperate with each other and may perform in parallel to achieve the system goal.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {706},
numpages = {1},
keywords = {Multi-Agent System, Fuzzy Inference, Genetic Algorithms, Relevance Feedback, Information Retrieval},
series = {WI '03}
}

@inproceedings{10.5555/946251.947101,
author = {Chen, Zerong and Ghose, Aditya},
title = {Web Agents for Requirements Consistency Management},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Inconsistency handling is an aspect of requirements engineering that has attracted considerable research attention. This paper explores novel ways to applying semantic web technologies to this problem, in the context of a web-based agent-mediated environment for distributed requirements engineering.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {710},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947043,
author = {Goyal, Ram Dayal and Mukherjee, Joydeb},
title = {Decreasing Saw-Tooth Priority (DSTP) Based Product Data Classifier},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the advancement of web technology, huge information about products is available online in B2B Market. Quite often the available product data is only of short description. This short description product data is needed to be classi.ed to speci.c categories of a desired schema,before it can be used. Most of the classi.ers available presently require training and do not provide expected results when the data is of small description. Also, almost none of these consider the variation in the importance across the terms (words/phrases) along the input (sentences). For classification of such data, our decreasing-saw-tooth-priority based data classifier takes care of such variation. It makes use of information provided in schema only, hence does not require any training. Our classifier has been applied on such type of real life data taken from various domains. Results shown in the paper underscores the superiority of our classifier in terms of its accuracy as compared to others.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {717},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947042,
author = {Matsuda, Toshio and Nakamura, Kazushige and Sakamoto, Norihiko},
title = {An Efficient Internet Crawling and Filtering System for the Nationwide Tendering Information Retrieval},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {With the growth of internet, the Central Government and local governments have begun to publish matters concerning the prospect of orders for public works, the announcement of tendering and the contracting information on their web sites. However, it is time consuming and painful for bidders such as constructors and manufacturers to periodically search the above information that matches their needs. Recently, there are various search engines, e.g.Google and Yahoo!, but those general search engines are not effective for the purpose of retrieving the above information quickly enough because of their crawling interval and coverage. Then we developed a system to automate the process of gathering such information, filtering for users' needs and delivering as the tendering and contracting information database. This paper describes the concept of the system as well as the key techniques to realize it; (1) to efficiently retrieve only relevant web pages, and (2) filtering to match users' needs.},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {722},
numpages = {1},
series = {WI '03}
}

@inproceedings{10.5555/946251.947116,
title = {Author Index},
year = {2003},
isbn = {0769519326},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence},
pages = {727},
numpages = {1},
series = {WI '03}
}

