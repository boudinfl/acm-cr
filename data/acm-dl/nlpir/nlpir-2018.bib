@inproceedings{10.1145/3278293.3278297,
author = {Benitez, Ian P. and Sison, Ariel M. and Medina, Ruji P.},
title = {Implementation of GA-Based Feature Selection in the Classification and Mapping of Disaster-Related Tweets},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278297},
doi = {10.1145/3278293.3278297},
abstract = {The extracted features from Twitter messages were transformed into feature vector matrix for which feature selection using an improved Genetic Algorithm was applied. The features selected were used to train and test the classifiers. The evaluation showed the effectiveness of the implemented feature selection method in the dimensionality reduction of the feature space and in increasing the accuracy of Multinomial Naive Bayes. Moreover, a web-based prototype utilizing the model was developed and was used to analyze tweet data pertaining to natural disasters in the Philippines. The prototype exhibited potential to harness the capability of social media as a tool in helping the affected community in times of natural crisis. This work may spark ideas for a more advanced development of IT-based disaster management applications.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {1–6},
numpages = {6},
keywords = {Twitter message classification, Natural disaster event detection, short text mining, GA-based feature selection},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278300,
author = {Phongwattana, Thiptanawat and Chan, Jonathan H.},
title = {A Combination of Text Mining Techniques for Relevant Literature Search and Extractive Summarization},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278300},
doi = {10.1145/3278293.3278300},
abstract = {Over the past few years, the amount of research papers published has dramatically increased. Consequently, researchers spend a lot of time reviewing relevant literature in order to better understand their domain of interest and keep up with new developments. After doing literature reviews in the area of text mining, we found many works proposing the means of sentence representation in machine learning for finding sentence similarity. These include average bag of words, weight average word vectors, bag of n-grams, and matrix-vector operations. However, these techniques are limited in word ordering and semantic analysis. This paper proposes a framework that combines two text mining techniques, paragraph vectors and TextRank, for the selection of relevant research paper and extractive summarization, respectively. Our training corpus includes over 20 million research papers. The aim of this work is to build a supplementary research tool that assists researchers in saving time conducting literature reviews. As the result, we can rank all relevant research papers potentially within the corpus, and utilize the outputs in our literature reviews. Moreover, the tool can extract all potential keywords in a single task as well.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {7–11},
numpages = {5},
keywords = {paragraph vectors, literature review, text mining, keywords extraction, extractive summarization, document similarity, TextRank},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278301,
author = {Eclarin, Bobby A. and Fajardo, Arnel C. and Medina, Ruji P.},
title = {A Novel Feature Hashing With Efficient Collision Resolution for Bag-of-Words Representation of Text Data},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278301},
doi = {10.1145/3278293.3278301},
abstract = {Text Mining is widely used in many areas transforming unstructured text data from all sources such as patients' record, social media network, insurance data, and news, among others into an invaluable source of information. The Bag Of Words (BoW) representation is a means of extracting features from text data for use in modeling. In text classification, a word in a document is assigned a weight according to its frequency and frequency between different documents; therefore, words together with their weights form the BoW. One way to solve the issue of voluminous data is to use the feature hashing method or hashing trick. However, collision is inevitable and might change the result of the whole process of feature generation and selection. Using the vector data structure, the lookup performance is improved while resolving collision and the memory usage is also efficient.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {12–16},
numpages = {5},
keywords = {Feature Hashing, Bag Of Words, Collision Resolution, Text Analytics, Text Mining},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278306,
author = {Matsumoto, Kazuyuki and Yoshida, Minoru and Kita, Kenji},
title = {Classification of Emoji Categories from Tweet Based on Deep Neural Networks},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278306},
doi = {10.1145/3278293.3278306},
abstract = {In this paper, we describe the sentiment analysis method from tweets based on emoji's category. Many of existing study about sentiment analysis focused on the emotional expressions included in sentence. However, because there are various kinds of emotional expressions, such as Internet slang, it cannot be constructed that the fixed emotional expression dictionary. The most of the methods based on corpus and machine learning, its performance is quite depended on the quality of annotation. Therefore, we attempt to use categories which are expressed by emoji as sentiment label instead of manually annotated labels. Our proposed method uses automatically annotated category label by emoji which is annotated to sentence, and train word embedding feature by deep neural networks. As the result of the experiment, our proposed method overcome the simple word feature based method.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {17–25},
numpages = {9},
keywords = {Twitter, Emoji, emoji category, deep neural networks},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278294,
author = {Kubek, Mario M. and Unger, Herwig},
title = {The WebEngine: A Fully Integrated, Decentralised Web Search Engine},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278294},
doi = {10.1145/3278293.3278294},
abstract = {This paper presents a basic, new concept for decentralized web search which addresses major shortcomings of current web search engines. Its methods are characterised by their local working principles, making it possible to employ them on diverse hardware configurations. The concept's implementation in form of an interactive, librarian-inspired peer-to-peer software client, called 'WebEngine', is elaborated on in detail. This software extends and interconnects common web servers creating and forming a decentralised web search system on top of the existing web structure while --for the first time-- combining modern text analysis techniques with novel and efficient search functions as well as approaches for the semantically induced P2P-network construction and its exible management. This way, an alternative, fully integrated and powerful web search engine under the motto 'The Web is its own search engine.' is built making the web searchable without any central authority.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {26–31},
numpages = {6},
keywords = {web search engine, P2P-system, WebEngine, co-occurrence graph, librarian of the web},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278296,
author = {Hayat, Shoaib and Li, Yue and Riaz, Muhammad},
title = {Automatic Recovery of Broken Links Using Information Retrieval Techniques},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278296},
doi = {10.1145/3278293.3278296},
abstract = {World Wide Web is very dynamic in its nature and we experienced changes in web pages every day. Web pages are updated, deleted, created or moved from one domain to another. Due to its dynamic nature often the web users experience broken links. Internet has been suffering from broken links problem despite of its contemporary services. Broken links are frequent problem occurring in web domain. Sometimes the page which was pointing from another page has been disappeared forever or moved to some other location. There are numerous reasons behind broken links. Some of these are permanently deleted Web pages, or modification made in Web pages causes broken links or the link of target page has some errors in code of source page. Researchers proposed several techniques in order to recover the broken links or at least retrieve some relevant pages. Number of sources have been used in research community for broken links recover like URL of target page, Anchor text, surround text near to anchor text and text in the source pages. All these sources of information are useful for retrieving the candidate pages relevant to broken links. System returns a ranked list of highly relevant candidate pages on submitting a query which has been extracted from different sources listed above. Previous work relies on TF (Term Frequency) or DF (Document Frequency) weights for extracting term from anchor text and full text of page containing missing links but not showed good results which cause the problem of retrieving similar pages for multiple broken links. In this paper we investigate the use of term proximity (position) relationship between the terms of anchor text and full text in order to extract relevant (good and bad) terms through classification model. This solves the problem by providing different query terms for multiple broken links and also increases the effectiveness as the terms that are proximity close to each other reveal more relevance.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {32–36},
numpages = {5},
keywords = {Link Integrity, Web Information Retrieval, Recommender System},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278303,
author = {Rasel, Risul Islam and Sultana, Nasrin and Akhter, Sharna and Meesad, Phayung},
title = {Detection of Cyber-Aggressive Comments on Social Media Networks: A Machine Learning and Text Mining Approach},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278303},
doi = {10.1145/3278293.3278303},
abstract = {The spread of aggressive tweets, status and comments on social network are increasing gradually. People are using social media networks as a virtual platform to troll, objurgate, blaspheme and revile one another. These activities are spreading animosity in race-to-race, religion to religion etc. So, these comments should be identified and blocked on social networks. This work focuses on extracting comments from social networks and analyzes those comments whether they convey any blaspheme or revile in meaning. Comments are classified into three distinct classes; offensive, hate speech and neither. Document similarity analyses are done to identify the correlations among the documents. A well defined text pre-processing analysis is done to create an optimized word vector to train the classification model. Finally, the proposed model categorizes the comments into their respective classes with more than 93% accuracy.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {37–41},
numpages = {5},
keywords = {logistic regression, natural language processing, decision tree, support vector machine, machine learning, random forest},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278298,
author = {Fabon, Gil L. and Fajardo, Arnel C. and Medina, Ruji P.},
title = {Automated Teller Machines Location's Information Retrieval Search Engine Using Suffix Tree Clustering Technique},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278298},
doi = {10.1145/3278293.3278298},
abstract = {In this paper, the researcher presented the Automated Teller Machines Location's Information Retrieval Search Engine using Suffix Tree Clustering Technique. This new offering is very helpful to the day to day evolving demands of money transactions of the bank customers, especially during unexpected Automated Teller Machines failures. With an application of Suffix Tree Clustering Technique, the proposed Automated Teller Machines Location Information Retrieval Search Engine is not only limited to produce more efficient, accurate and precise Automated Teller Machines location search results than the current bank existing system. It also provides easier access focused control in information dissemination to provides 24/7 access to the list of ATM location booth with the corresponding ATM information's according to the areas of familiarity of the bank customers. It's also conveying innovation to the bank online services to exploit the provisions of Online and Offline ATM status transparency to the bank customers and avoid the bank customers to other banks high transactions fees. This claim is reinforced by 100% average effectiveness of precision, recall and F-measure experimental results on bank ATM locations data set. In spite of the rapid growth of improving and modernizing the Automated Teller Machines services there is still a lot ideas in fetching new offerings to modified Automated Teller Machines Location's Information Retrieval Search Engine in the country to dig up more accessible ATM location with a timely manner as contribution to the fast-paced era of modernization and technology.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {42–48},
numpages = {7},
keywords = {Precision, F-measure, Recall, Suffix Tree Clustering, ATM Failure, ATM locator, Automated Teller Machine},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278295,
author = {Du, Tingting and Huang, Yunyin and Wu, Xian and Chang, Huiyou},
title = {Multi-Attention Network for Sentiment Analysis},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278295},
doi = {10.1145/3278293.3278295},
abstract = {Sentiment analysis is an active research area in natural language processing. However, most existing methods use extra data such as pre-specified syntactic structure or user preference information. In this work, we propose a multiple attention network (MAN) that learns both word- and phrase-level features for sentiment analysis. MAN uses vector representation of the input sequence as target in the first attention layer to locate the words that contribute to the sentence sentiment. However, although an isolated word may indicate subjectivity, there may be insufficient context to determine sentiment orientation. We argue that the sentence sentiment often requires multiple steps of reasoning. Thus, we apply the second attention layer to explore the phrase information around the keyword. We experiment our method on three benchmark datasets and the results show that our model achieves state-of-the-art performance without any extra data. The visualization of the attention layers illustrates the effectiveness of our attention based model.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {49–54},
numpages = {6},
keywords = {Sentiment Analysis, LSTM, Attention Mechanism},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278299,
author = {Galutira, Edwin F. and Fajardo, Arnel C. and Medina, Ruji P.},
title = {A Novel Learning Rate Decay Function of Kohonen Self-Organizing Maps Using the Exponential Decay Average Rate of Change for Image Clustering},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278299},
doi = {10.1145/3278293.3278299},
abstract = {Clustering requires efficient selection of similarities among sample vectors and true clustering capability of the algorithm. The Kohonen Self-Organizing Maps is the most preferred unsupervised Artificial Neural Network clustering algorithm for high-dimensional or multi-dimensional data. This study introduces a new way of improving the clustering capability of the algorithm by enhancing its learning rate decay function to decrease its learning rate gradually as the training goes on through the use of the Exponential Decay Average Rate of Change. The new function allows the Enhanced Kohonen Self-Organizing Maps algorithm to converge to the minimum producing a more robust clustered datasets. The enhanced algorithm and the conventional algorithm were applied for image clustering, and the EKSOM remarkably outperformed the clustering capability of the KSOM. The introduction of EDARC function paves the way to explore the clustering and classification capability of KSOM further.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {55–59},
numpages = {5},
keywords = {Average Rate of Change, Clustering, Convergence, High-dimensional Data, Learning Rate Decay},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278302,
author = {Zhao, Grace and Zhang, Xiaowen},
title = {Domain-Specific Ontology Concept Extraction and Hierarchy Extension},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278302},
doi = {10.1145/3278293.3278302},
abstract = {The domain-specific vernaculars and notations have been a hurdle to automatic ontology building and augmentation, since most of the ontology learning methods are essentially based on the natural language studies and lexicosyntactic pattern explorations. This paper proposes two robust approaches to ontology hierarchical enhancement, in particular, adding new terms to the ontology graph. We designed our learning models from a computational vantage point, examining the inter-relationship between documents, ontology dictionary terms, and the graph structure of the seed ontology. We then take advantage of late studies of neural networks and machine learning to perform classification over the inter-related data, and insert the new term at the most desirable nodal place on the domain ontology graph.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {60–64},
numpages = {5},
keywords = {Ontology Hierarchy Extension, Ontology Engineering, Domain Knowledge Learning, Supervised Machine Learning},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278304,
author = {Li, XianBo and Ma, ZhiXin},
title = {Computational Pragmatics: A Survey in China and the World},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278304},
doi = {10.1145/3278293.3278304},
abstract = {The definition and scope of computational linguistics are reconstructed to distinguish it with other interdisciplinary disciplines like mathematical pragmatics, formal pragmatics, corpus pragmatics, etc. Meanwhile, the position of computational pragmatics and its relationship with other disciplines are displayed in this paper. Then, we reviewed the research status of computational pragmatics in China and the world, finding out the current study of computational pragmatics is still in its infancy, awaiting further research in four aspects: discipline construction, philosophy and methodology, fundamental theory, and application. Literatures indicate that computational pragmatics plays a vital role in both theory for a lot of disciplines and applications in our life.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {65–69},
numpages = {5},
keywords = {Computer Application, Computational Pragmatics, Discipline Scope, Theoretical Research},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

@inproceedings{10.1145/3278293.3278305,
author = {Truong, Thinh and Dao, An and Nguyen, Long and Dinh, Dien},
title = {Improving Named Entity Recognition of English and Vietnamese Languages Using Bilingual Constraints},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278305},
doi = {10.1145/3278293.3278305},
abstract = {Named entity recognition plays a crucial role in many Natural Language Processing tasks because the semantic information is carried by entities. The recent efforts are trying to reduce the annotation labor because the state-of-the-art Named Entity Recognition systems are still based on supervised machine learning algorithms that require huge amounts of training data. Such training data are difficult and expensive to produce manually. In particular, Vietnamese is a resource-limited language which lacks high-quality named entity annotated corpora. This limitation leads to the low performance of Vietnamese Named Entity Recognition. Therefore, in this paper, thanks to the use of an existing unannotated English-Vietnamese bilingual corpus, we propose an approach to improve Named Entity Recognition systems of both English and Vietnamese languages. Experimental results show an improvement of both English and Vietnamese Named Entity Recognition compared to the strong baseline StanfordNER. In particular, Vietnamese Named Entity Recognition improves significantly by 18.45% in term of F1-score. As for the English side, F1-score improves from 92.44% to 95.05%. Our proposed method can also be generalized to apply to other resource-limited languages.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {70–75},
numpages = {6},
keywords = {Named entity recognition, Bilingual text, Word alignment},
location = {Bangkok, Thailand},
series = {NLPIR 2018}
}

