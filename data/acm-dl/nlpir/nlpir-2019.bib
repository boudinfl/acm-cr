@inproceedings{10.1145/3342827.3342828,
author = {Kuyumcu, Birol and Aksakalli, Cuneyt and Delil, Selman},
title = {An Automated New Approach in Fast Text Classification (FastText): A Case Study for Turkish Text Classification without Pre-Processing},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342828},
doi = {10.1145/3342827.3342828},
abstract = {Any Text Classification (TC) problem need pre-processing steps which may affect the classification accuracy. Especially pre-processing steps need substantial effort particularly in agglutinative languages such as Turkish. In this context, a traditional text categorization problem requires pre-processing steps such as tokenization, stop-word removal, lower-case conversion, stemming and feature dimension reduction. Before classification, one or more of these steps are applied to text and then a classifier is trained to evaluate the corresponding precision. Deep neural network classifiers combined with word embedding is one of the solutions to eliminate the pre-processing prerequisites. Another novel approach is fastText word embedding based classifier which was developed by Facebook. In this study, we evaluate a fastText classifier on TTC-3600 Turkish dataset without using any pre-processing steps and present the performance of the algorithm.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {1–4},
numpages = {4},
keywords = {TTC-3600 dataset, fastText classifier, word embedding, text classification},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342829,
author = {Wang, Xueying},
title = {Using Sentiment Analysis for Comparing Attitudes between Computer Professionals and Laypersons on the Topic of Artificial Intelligence},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342829},
doi = {10.1145/3342827.3342829},
abstract = {Most research in investigating computer professionals and laypersons' attitudes toward artificial intelligence (AI) are limited to online or offline surveys. This paper analyzes computer professionals' and laypersons' attitudes toward AI by using a sentiment lexicon developed by Wilson et al. To explore whether there is a correlation between the occupation categories (computer-related versus non-computer-related occupations) and people's attitudes toward artificial intelligence, I conducted a polarity classification of over 0.6 million tweets containing references to "AI", "artificial intelligence", or both. The result did not provide evidence of a relationship between public attitudes toward AI and the occupation categories. In the end, several future directions in the data collection and the data analysis are discussed.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {5–8},
numpages = {4},
keywords = {Computer Professional, Tweets, Lexicon, Polarity Classification, AI, Scientific Knowledge, Layperson, Public Attitudes},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342834,
author = {Litvinova, Tatiana and Litvinova, Olga and Panicheva, Polina},
title = {Authorship Attribution of Russian Forum Posts with Different Types of N-Gram Features},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342834},
doi = {10.1145/3342827.3342834},
abstract = {Authorship attribution is an important field in online security. Recently there have been numerous successful works in authorship attribution in various European languages. Character n-grams are reported to be the best choice in authorship attribution, as they encode both style and content information. We evaluate different types of character n-gram features in an authorship attribution task in a real-world noisy dataset of Russian forum posts. We also supplement them with a number of new simple n-gram features capturing syntactic and discourse patterns. We perform authorship attribution in a single-topic and a cross-topic setting, as the research question is whether character n-grams capture both style and content information. Our results show that character n-grams are indeed very successful in Russian forum post authorship attribution. However, there is no clear distinction of style and content n-grams, as the same types of n-grams work well for both single-topic and cross-topic settings. In our experiments the generalized simple n-gram features which reveals syntactic and discourse patterns were proved to be also very important in authorship attribution of short informal Russian texts. They represent a different kind of authorship information and are a successful addition to the character n-grams in authorship attribution of forum texts in the Russian language.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {9–14},
numpages = {6},
keywords = {extremist forum, n-gram, authorship attribution, Russian language},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342835,
author = {Mu, Pengyu and He, Jingsha and Zhu, Nafei},
title = {Text Classification of Network Pyramid Scheme Based on Topic Model},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342835},
doi = {10.1145/3342827.3342835},
abstract = {At present, the network pyramid scheme has become a major tumor that hinders social development. In order to curb the propagation of the network pyramid scheme and effectively identify the pyramid scheme text in the network, this study proposes a joint topic model, Paragraph Vector Latent Dirichlet Allocation (PV_LDA), based on the characteristics of high-yield, high rebate, hierarchical salary and text topic diversity described in the text. The model uses the paragraph as the minimum processing unit to generate the topic distribution matrix of "high-interest rate" and "hierarchical salary" from the network pyramid scheme text. The Gibbs sampling is used to derive the "pyramid scheme" topic distribution matrix represented by the two features, which is used for classification processing by the classifier. the classification accuracy rate for the network pyramid scheme text can reach 86.25%. The conclusions show that the topic model proposed in this paper can capture the characteristics of the pyramid scheme more reasonably.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {15–19},
numpages = {5},
keywords = {topic mining, text classification, network pyramid scheme, topic model},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342836,
author = {Chang, Kai-Cheng and Chang, Hsien-Tsung},
title = {Is It Possible to Use Chatbot for the Chinese Word Segmentation?},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342836},
doi = {10.1145/3342827.3342836},
abstract = {A word is the smallest item in Natural Language Processing. However, there is no obvious boundary for Chinese words. How to segment Chinese words always obstructs Chinese researches and applications. Nowadays, a neural network model, Seq2Seq with LSTM, is well-known for translation or chatbot application. In this paper, we try to transform the Chinese word segmentation problem into a translation problem. And we utilized an open-source chatbot to simulate the translation task. In our experimental results, we can produce similar Chinese word segmentation results when we provide training data which is automatically generated from famous Chinese word segmentation services.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {20–24},
numpages = {5},
keywords = {LSTM, Chatbot, Chinese word segmentation, Seq2Seq},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342837,
author = {Song, Xuebo and Srimani, Pradip K. and Wang, James Z.},
title = {HWE: Hybrid Word Embeddings For Text Classification},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342837},
doi = {10.1145/3342827.3342837},
abstract = {Text classification is one of the most important tasks in natural language processing and information retrieval due to the increasing availability of documents in digital form and the ensuing need to access them in flexible ways. By assigning documents to labeled classes, text classification can reduce the search space and expedite the process of retrieving relevant documents. In this paper, we propose a novel text representation method, Hybrid Word Embeddings (HWE), which combines semantic information obtained fromWord- Net and contextual information extracted from text documents to provide concise and accurate representations of text documents. The proposed HWE method can improve the efficiency of deriving word semantics from text by taking advantage of the semantic relationships extracted from WordNet with less training corpus. Experimental study on classification of documents shows that the proposed HWE outperforms existing methods, including Doc2Vec and Word2Vec, in terms of classification accuracy, recall, precision, etc.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {25–29},
numpages = {5},
keywords = {Text classification, WordNet, Word embedding},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342838,
author = {Cao, Dong and Zhang, Dongdong and Chen, HaiBo},
title = {A Novel Task-Oriented Text Corpus in Silent Speech Recognition and Its Natural Language Generation Construction Method},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342838},
doi = {10.1145/3342827.3342838},
abstract = {Millions of people with severe speech disorders around the world may regain their communication capabilities through techniques of silent speech recognition (SSR). Using electroencephalography (EEG) as a biomarker for speech decoding has been popular for SSR. However, the lack of SSR text corpus has impeded the development of this technique. Here, we construct a novel task-oriented text corpus, which is utilized in the field of SSR. In the process of construction, we propose a task-oriented hybrid construction method based on natural language generation (NLG) algorithm. The algorithm focuses on the strategy of data-to-text generation, and has two advantages including linguistic quality and high diversity. These two advantages use template-based method and deep neural networks respectively. In an SSR experiment with the generated text corpus, analysis results show that the performance of our hybrid construction method outperforms the pure method such as template-based natural language generation or neural natural language generation models.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {30–33},
numpages = {4},
keywords = {natural language generation, deep neural networks, silent speech recognition},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342839,
author = {Daoruang, Beesuda and Sintanakul, Krich and Mingkhwan, Anirach},
title = {The Study of Learning Achievement of Learners Classified VARK Learning Style in Blended Learning},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342839},
doi = {10.1145/3342827.3342839},
abstract = {There are many learning methods presented. How could learners know which method is suitable for their learning style? In this paper, we have the objective to classify learning style base on the VARK model using Blended learning method on media creation, learning activities on Multimedia Design and Development subject.The research samples were 47 undergraduates from Information Technology Department who enrolled in the second semester of the academic year 2018 and selected by using the purposive random sampling method.We concluded that teaching/learning methods do not have equally achievement for the different group of learning style. In our case, the performance base plan combined with blended learning is better with the multimodal VAK.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {34–38},
numpages = {5},
keywords = {VARK, Unimodal, Multimodal, blended learning},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342847,
author = {Chao, Guan-Lin and Shen, John Paul and Lane, Ian},
title = {Deep Speaker Embedding for Speaker-Targeted Automatic Speech Recognition},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342847},
doi = {10.1145/3342827.3342847},
abstract = {In this work, we investigate three types of deep speaker embedding as text-independent features for speaker-targeted speech recognition in cocktail party environments. The text-independent speaker embedding is extracted from the target speaker's existing speech segment (i-vector and x-vector) or face image (f-vector), which is concatenated with acoustic features of any new speech utterances as input features. Since the proposed model extracts the speaker embedding of the target speaker once and for all, it is computationally more efficient than many prior approaches which estimate the target speaker's characteristics on the fly. Empirical evaluation shows that using speaker embedding along with acoustic features improves Word Error Rate over the audio-only model, from 65.7% to 29.5%. Among the three types of speaker embedding, x-vector and f-vector show robustness against environment variations while i-vector tends to overfit to the specific speaker and environment condition.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {39–43},
numpages = {5},
keywords = {speaker-targeted speech recognition, robust speaker embeddings, acoustic modeling},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342848,
author = {Rattanajariya, Tanakorn and Piromsopa, Krerk},
title = {Applying Deep Learning in Word Embedding for Making a Diagnosis Prediction Model from Orthopedic Clinical Note},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342848},
doi = {10.1145/3342827.3342848},
abstract = {We propose deep learning in word embedding for making a diagnostic prediction model. One factor that causes uncertainties in diagnostic is the inexperience of physicians. The diagnosis errors lead to incorrect and delay in treatment, waste of time and money. To solve the problem, a differential diagnosis is a critical tool. It is powerful and does not introduce additional work to physician. Our method applied a deep learning tool together with word embedding from existing diagnosis texts in medical system. The model takes the clinical notes from a physician. The note is then used to analyze the possibilities of diseases. The output is sorted by model confidence. We validate our model with True Positive Rate (Recall), False Positive Rate (Precision) and accuracy. Our model achieves a new record of accuracy at 99.95% The highest recall rate is at 86.64% in top first prediction.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {44–48},
numpages = {5},
keywords = {Word embedding, Text classification, Medical record, Machine learning, Diagnosis, Deep learning},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342850,
author = {Sarkar, Anindya and Reddy, Sujeeth and Iyengar, Raghu Sesha},
title = {Zero-Shot Multilingual Sentiment Analysis Using Hierarchical Attentive Network and BERT},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342850},
doi = {10.1145/3342827.3342850},
abstract = {Sentiment analysis is considered an important downstream task in language modelling. We propose Hierarchical Attentive Network using BERT for document sentiment classification. We further showed that importing representation from Multiplicative LSTM model in our architecture results in faster convergence. We then propose a method to build a sentiment classifier for a language in which we have no labelled sentiment data. We exploit the possible semantic invariance across languages in the context of sentiment to achieve this.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {49–56},
numpages = {8},
keywords = {Sentiment analysis, BERT, Multilingual, Hierarchical networks},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342856,
author = {Tsubaki, Hajime},
title = {Analysis of Native and Non-Native Speakers' English Compositions Based on Word-Frequency Distribution and Text Statistics},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342856},
doi = {10.1145/3342827.3342856},
abstract = {In this paper, word-frequency distribution of JACET 8000 basic words and text statistics were researched to compare and analyze differentials of English compositions (essays) written by native speakers and non-native speakers. As for the native speakers' essays, the Guiraud Index in each Level 2-8 to Average sentence length and Automated Readability Index had higher correlation coefficients. Meanwhile, on the non-native speakers' essays, the index values to Sentence count showed moderate correlation coefficients. It was observed that the productivity and readability of the compositions seem to depend on ranges of basic content words which native or non-native writers have acquired and can use in English. To verify the word-frequency distribution as proficiency rating measurement for non-native speakers, the estimation experiment was carried out based on a multiple-regression model using word-frequency distribution of 68 English compositions written by the non-native writers. The estimated scores of the learners showed a correlation score 0.475 to their actual TOEIC scores. These results confirmed the possibility of the word usage statistics for the objective evaluation of L2 (second language) learners' language proficiency.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {57–61},
numpages = {5},
keywords = {L2 writing ability evaluation, Corpus-based language analysis, Text mining},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342830,
author = {Lin, Nay and A, Kudinov Vitaly and Soe, Yan Naing},
title = {Text Compression for Myanmar Information Retrieval},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342830},
doi = {10.1145/3342827.3342830},
abstract = {Myanmar word segmentation is an important task for construction of dictionary file for Myanmar information retrieval and Myanmar text compression. Although Myanmar word segmentation using dictionary and orthography has been existed for Myanmar language, the performance of word segmentation depends on the coverage of the dictionary and training dataset and can cause out of vocabulary (OOV) problem, leading to lower precision and recall in information retrieval. And to compress Myanmar text, words in text needs to be recognized first. In this paper, we propose a new method for Myanmar word segmentation by local statistical dataset without the use of any additional data (e.g., training corpus) and new compressed Myanmar Information Retrieval (MIR) model which used End Tagged Dense Code (ETDC) text compressed method. The experimental results showed that the method can improve evaluation of vocabulary file with precision 75%, recall 87%, F-measure 80% and average compression ratio is 32% of texts for Myanmar language.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {62–67},
numpages = {6},
keywords = {Myanmar information retrieval, Myanmar Natural Language Processing, ETDC, Boyer Moore pattern matching, vocabulary file, indexing, Text Compression},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342832,
author = {Tobing, Berty Chrismartin Lumban and Suhendra, Immanuel Rhesa and Halim, Christian},
title = {Catapa Resume Parser: End to End Indonesian Resume Extraction},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342832},
doi = {10.1145/3342827.3342832},
abstract = {This paper proposes a method to solve the problem of extracting contents from a resume, especially for Indonesian resumes using segmentation method by header followed by models for each corresponding headers. An end to end resume extraction system is created using some heuristic rules and machine learning algorithms to solve the problem. On average, an accuracy of ~91.41% is achieved for personal information entities (name, email, phone, gender, date of birth, and religion), ~68.47% accuracy for job experiences entities (company, job title, start date, and end date), and ~80.85% accuracy for educations entities (institution, major, level, start date, end date, and GPA) out of 221 random resumes using the aforementioned method.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {68–74},
numpages = {7},
keywords = {Information Retrieval, Indonesia, Heuristic, Entity, Unstructured Text, Extraction, Resume, Machine Learning},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342840,
author = {Komatsu, Seiya and Sasayama, Manabu},
title = {Speech Error Detection Depending on Linguistic Units},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342840},
doi = {10.1145/3342827.3342840},
abstract = {In this research, we aim at the construction of a system which detects, points out and corrects speech error (slip of the tongue) of a human speech that occurs in a dialogue system (example: Pepper, Amazon Echo, Google Home) and a human dialogue. In the present dialogue system, even if human makes a speech error, the system cannot recognize it, which could lead to broken communication. So far, we have created a system to detect speech error using deep learning. In this study, we propose a method to augmented training data used for deep learning. The training data is a corpus that collects examples of speech error. At present, the number of training data is insufficient to detect with high accuracy. Therefore, it is necessary to augment the training data. Specifically, the feature of the speech error is examined from an existing speech error corpus, and extended rules are created. The data augmentation of training data is performed by generating dialogue sentence which made the speech error based on the rule. As a result of evaluation experiment, detection accuracy was improved in LSTM model by data augmentation.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {75–79},
numpages = {5},
keywords = {Speech Error, Deep Learning, Corpus, Natural Language Processing, Data Augmentation},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342843,
author = {Shen, Zhengru and Wang, Xi and Spruit, Marco},
title = {Big Data Framework for Scalable and Efficient Biomedical Literature Mining in the Cloud},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342843},
doi = {10.1145/3342827.3342843},
abstract = {The massive size of available biomedical literature requires researchers to utilize novel big data technologies in data storage and analysis. Among them is cloud computing which has become the most popular solution for big data applications in industry. However, many bioinformaticians still rely on expensive and inefficient in-house infrastructure to discover knowledge from biomedical literature. Although some cloud-based solutions were constructed recently, they failed to sufficiently address a few key issues including scalability, flexibility, and reusability. Moreover, no study has taken computational cost into consideration. To fill the gap, we proposed a cloud-based big data framework that enables researchers to perform reproducible and scalable large-scale biomedical literature mining in an efficient and cost-effective way. Additionally, a cloud agnostic platform was constructed and then evaluated on two open access corpora with millions of full-text biomedical articles. The results indicate that our framework supports scalable and efficient large-scale biomedical literature mining.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {80–86},
numpages = {7},
keywords = {big data, text mining, document classification, cloud computing, biomedical literature, topic modeling},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342844,
author = {Chou, Tai-Liang and Hsueh, Yu-Ling},
title = {A Task-Oriented Chatbot Based on LSTM and Reinforcement Learning},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342844},
doi = {10.1145/3342827.3342844},
abstract = {Traditional conversational chatbots usually adopt a retrieved-based model. Developers have to provide a large amount of conversational data and classify those data to different intents. To avoid cumbersome development processes, we propose a method to build a chatbot by a sentence generation model which generates sequence sentences based on the generative adversarial network. The architecture of our model contains a generator that generates a diverse sentence, and a discriminator that judges the sentences between the generated and the raw data. In the generator, we combine the attention model that responses for tracking conversational states with the sequence-to-sequence model using hierarchical long-short term memory to extract sentence information. For the discriminator, we calculate twotypes of rewards to assign low rewards for repeated sentences and high rewards for diverse sentences. Extensive experiments are presented to demonstrate the utility of our model which generates more diverse and information-rich sentences than those of the existing approaches.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {87–91},
numpages = {5},
keywords = {Chatbots, Natural Language Processing, Service Robots, Sentence Generation, Dialog Management, Deep Learning},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342845,
author = {Aljubran, Murtadha},
title = {Evaluation of Pseudo-Relevance Feedback Using Wikipedia},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342845},
doi = {10.1145/3342827.3342845},
abstract = {Users have specific information needs which are expressed in short queries to information retrieval systems. The queries are unstructured, and they tend to be short and ambiguous in most cases. Using the shallow language statistics including probabilistic or language models such as BM25 or Indri respectively can enhance the retrieval system metrics like Mean Average Precision (MAP). However, such methods depend on query terms and their presence in the retrieved document to define relevance. Query expansion is a technique that can be used to overcome this problem by expanding the query with terms from an initial top few relevant documents. The question that we try to answer is whether the quality of the corpus used for expansion produce a significant improvement MAP and precision at top 30 retrieved documents. We show that the quality and the selection criteria of expansion documents are important factors in query expansion performance.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {92–98},
numpages = {7},
keywords = {Information Retrieval (IR), Query Expansion, Mean Average Precision (MAP)},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342849,
author = {Kanika and Chakraverty, Shampa and Chakraborty, Pinaki and Agnihotri, Shikhar and Mohapatra, Soumya and Bansal, Prakriti},
title = {KELDEC: A Recommendation System for Extending Classroom Learning with Visual Environmental Cues},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342849},
doi = {10.1145/3342827.3342849},
abstract = {We develop an innovative personalized recommendation system called KELDEC that links the notes that students take in class with their outdoor experiences captured with camera, to suggest websites that extend their knowledge. Despite the plethora of educational recommendation systems, there is a dearth of effective tools that make evident the practical application of theory in the real world. KELDEC extracts the core learning points from class notes and distinctive labels that describe objects in a picture. It then mines the web to first extract the technical context of the picture, and subsequently culls out websites that establish linkages between notes and the picture. Response to user surveys garnered from students studying Software Engineering in the undergraduate Computer Engineering course reveal that they gain new and practical extension of classroom knowledge.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {99–103},
numpages = {5},
keywords = {Web content mining, Educational recommender system, Personalized mobile learning, Classroom learning points, Image analysis},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342851,
author = {Trang, Nguyen Thi Thu and Bach, Dang Xuan and Tung, Nguyen Xuan},
title = {A Hybrid Method for Vietnamese Text Normalization},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342851},
doi = {10.1145/3342827.3342851},
abstract = {This paper presents a hybrid method for normalizing written text often found on newspapers to its spoken form. To normalize raw text with a number of non-standard words (NSWs), a two-step model is proposed. The first step involves classifying NSWs into different categories using Random Forest. The latter one is to expand them, depending on their NSW types, into pronounceable syllables using a hybrid method. Most of numeric types can be expanded by well-defined rules while most of alphabetic ones must be expanded by a deep learning (i.e. sequence-to-sequence) model and a post adjustment. The experiment on a Vietnamese corpus with proposed NSW categories shows that the most ambiguous cases of the classification model are for abbreviation and read-as-sequence types, hence combined into one category for the latter expansion with more complex model and better context. The classification model gives an enhanced result of 99.20% with the category combination and the feature optimization. In the expansion, the sequence-to-sequence model shows a good result of 96.53% for abbreviations and 96.25% for loanwords with a post-adjustment for some completely wrong cases. This model can predict effectively the expansions of abbreviations in context.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {104–109},
numpages = {6},
keywords = {Text normalization, Automatic Speech Recognition, Speech Synthesis, Deep learning},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342854,
author = {Lam, Khang Nhut and To, Tuan Huynh and Tran, Thong Tri and Kalita, Jugal},
title = {Improving Vietnamese WordNet Using Word Embedding},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342854},
doi = {10.1145/3342827.3342854},
abstract = {This paper presents a simple but effective method to improve the quality of WordNet synsets and extract glosses for synsets. We translate the Princeton WordNet and other intermediate WordNets to a target language using a machine translator, then the correct candidates are selected by applying different ranking methods: occurrence count, cosine similarity between words, cosine similarity between word embeddings and cosine similarity between Doc2Vec of sentences. Our approaches may be applicable to build WordNets in any language which has some bilingual dictionaries and at least a monolingual corpus in the target language.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {110–114},
numpages = {5},
keywords = {word embedding, WordNet, gloss, synset},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342855,
author = {Promrit, Nuttachot and Waijanya, Sajjaporn and Thaweesith, Kran},
title = {The Evaluation of Thai Poem's Content Consistency Using Siamese Network},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342855},
doi = {10.1145/3342827.3342855},
abstract = {Many research describes Textual Entailment model for compare pair of the sentence but two sentences in term of the poem content consistency are not the same. The content consistency is very important for storytelling in Thai poem composing. In this article, we propose the model and result of The evaluation of Thai poem's content consistency using The Siamese Network 3 models comprise 1) Merge Vector Model 2) Siamese Absolute Different Model and 3) Siamese Dot Vector Model compare with the Basic CNN model. The training data is Thai poem 14,173 pair (batt) and validation data is Thai poem 3,544 pair. All models learn by apply one shot learning technic. The accuracy of Siamese Absolute Different Model near 100%. The macro average of F1-score shows 99.27%. The Area Under Curve shows 0.997 near the perfect value.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {115–120},
numpages = {6},
keywords = {poem content consistency, klon-suphap thai poem, Siamese network, consistency classification},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342831,
author = {Hidayatullah, Ahmad Fathan and Kurniawan, Wisnu and Ratnasari, Chanifah Indah},
title = {Topic Modeling on Indonesian Online Shop Chat},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342831},
doi = {10.1145/3342827.3342831},
abstract = {This paper aims to discover topics from an Indonesian online shop chat. Moreover, we employed Latent Dirichlet Allocation to find out what kind of topics that are often discussed and conversation trends between buyers and customer service. Several tasks were performed, such as, collecting data, preprocessing, phrase aggregation, topic modeling, and topic analysis. We found several attracting findings during our experiments. In preprocessing task, product name extraction from URLs assisted to discover the intended product from the customer's conversation. On the other hand, the phrase aggregation task helped us to merge various terms which have same intended meaning, so that, we could obtain better topical model result and easier to determine the topic label.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {121–126},
numpages = {6},
keywords = {Topic modeling, Online Shop, Topic model, Bahasa Indonesia, Latent Dirichlet Allocation},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342833,
author = {Adlaon, Kristine Mae M. and Marcos, Nelson},
title = {Building the Language Resource for a Cebuano-Filipino Neural Machine Translation System},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342833},
doi = {10.1145/3342827.3342833},
abstract = {Parallel corpus is a critical resource in machine learning based translation. The task of collecting, extracting, and aligning texts in order to build an acceptable corpus for doing translation is very tedious most especially for low-resource languages. In this paper, we present the efforts made to build a parallel corpus for Cebuano and Filipino from two different domains: biblical texts and the web. For the biblical resource, subword unit translation for verbs and copy-able approach for nouns were applied to correct inconsistencies in translation. This correction mechanism was applied as a preprocessing technique. On the other hand, for Wikipedia being the main web resource, commonly occurring topic segments were extracted from both the source and the target languages. These observed topic segments are unique in 4 different categories. The identification of these topic segments may be used for automatic extraction of sentences. A Recurrent Neural Network was used to implement the translation using OpenNMT sequence modeling tool in TensorFlow. The two different corpora were then evaluated by using them as two separate inputs in the neural network. Results have shown a difference in BLEU score in both corpora.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {127–132},
numpages = {6},
keywords = {OpenNMT, natural language processing, neural machine translation, Language resources, Cebuano-Filipino translation, recurrent neural network},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342841,
author = {Sodanil, Maleerat and Chotirat, Saranlita and Poomhiran, Lap and Viriyapant, Kanchana},
title = {Guideline for Academic Support of Student Career Path Using Mining Algorithm},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342841},
doi = {10.1145/3342827.3342841},
abstract = {In general, higher education is an important step in preparing a career for students in the future. Graduates should have qualifications that are recognized by both entrepreneurs and society. Therefore, every higher educational institution should make an effort to consider how to assist students' performance. This research aims to analyze the relationships between courses that are likely to produce a future career for students using the Apriori algorithm. The data used in the operation of the association rule was the student's grades from 25 main courses in the field of information technology, Department of Information Technology, Faculty of Science and Technology, Suan Sunandha Rajabhat University. This data was recorded between 2011 and 2019 and stored in the registration and graduate career system. The 14 association rules were determined from the operation by using the Weka 3.8.3 data mining software, this indicated that there were a few courses in which students could have future careers. Most importantly, the results can contribute to guidelines for the academic support of students' future career.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {133–137},
numpages = {5},
keywords = {Academic support, Association rule, Apriori algorithm, Prediction},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342842,
author = {Naksung, Pannathorn and Nicrothanon, Chayaphat and Chunjiree, Putthichot and Chay-intr, Thodsaporn and Theeramunkong, Thanaruk},
title = {A Construction of Hybrid Structural Thai Treebank},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342842},
doi = {10.1145/3342827.3342842},
abstract = {It is possible to include complicated structures into an individual syntactic tree, to enhance the usefulness of parsed text corpus. In this part, existing works on Thai treebank construction have been developed in order to address the lack of high-level syntactic resources. However, it has yet to be sufficient for Thai Natural Language Processing. Furthermore, Thai treebanks have either syntactic or dependency structure only. This paper presents a construction of hybrid structural Thai treebank which includes both syntactic/dependency structure, a tool for conversion between constituency and dependency parse tree, and a web-based GUI for parse tree visualization. Towards the hybrid treebank construction, hundreds of constituent tree are manually annotated with predicate header to each phrase. Once the set of annotated constituent trees are obtained, the conversion procedure will be performed by determining the annotated head and its dependents. As our experiments, features of hybrid treebank are extracted and illustrated. Finally, difficulties and issues in constructing the hybrid Thai treebank are discussed.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {138–143},
numpages = {6},
keywords = {Treebank Annotation Toolset, Treebank Construction, Hybrid Treebank Construction},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342846,
author = {Romanov, Vitaly and Khusainova, Albina},
title = {Evaluation of Morphological Embeddings for the Russian Language},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342846},
doi = {10.1145/3342827.3342846},
abstract = {A number of morphology-based word embedding models were introduced in recent years. However, their evaluation was mostly limited to English, which is known to be a morphologically simple language. In this paper, we explore whether and to what extent incorporating morphology into word embeddings improves performance on downstream NLP tasks, in the case of morphologically rich Russian language. NLP tasks of our choice are POS tagging, Chunking, and NER -- for Russian language, all can be mostly solved using only morphology without understanding the semantics of words. Our experiments show that morphology-based embeddings trained with Skipgram objective do not outperform existing embedding model -- FastText. Moreover, a more complex, but morphology unaware model, BERT, allows to achieve significantly greater performance on the tasks that presumably require understanding of a word's morphology.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {144–148},
numpages = {5},
keywords = {morphological embeddings, Russian language, Embeddings evaluation},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342852,
author = {Seitkali, Dauken and Musabayev, Rustam},
title = {Using Centroid Keywords and Word Mover's Distance for Single Document Extractive Summarization},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342852},
doi = {10.1145/3342827.3342852},
abstract = {This paper presents unsupervised method of single document extractive summarization. The main idea behind the method is in selecting sentences based on Word Mover's Distance Similarity between each sentence and set of centroid keywords. This approach leverages both compositional property of word embeddings and advantages of recently discovered powerful text to text distance metric. ROUGE results on DUC 2002 data set showed that quality of produced summaries can compete with well-known state of the art systems. In this work we also discuss limitations of gold summaries in evaluating quality of summarization systems.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {149–152},
numpages = {4},
keywords = {Centroid, word2vec, WMD, extractive summarization},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342853,
author = {Nualnim, Sureeporn and Romyen, Nirach and Sodanil, Maleerat},
title = {Applicability of Text-Representing Centroids for Thai Language Documents},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342853},
doi = {10.1145/3342827.3342853},
abstract = {Text-representing centroids are investigated method recently used to categorize and compare documents written in European languages. As it will be shown, Asian languages and in particular Thai exhibit completely other language structures. Nevertheless, a strong justification will be given that the methodology of the text-representing centroids can be successfully applied to Thai documents, too. For the experiments, a corpus which contained 100 randomly selected articles from an offline Thai Wikipedia was used. The obtained centroids well reflect the topic of those documents as in the original publication. In addition, the centroids are quite suitable to compare any two files.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {153–157},
numpages = {5},
keywords = {Centroid term, Co-occurrence graph, Text processing, Text Representing centroids, Document similarity, Text Mining},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

@inproceedings{10.1145/3342827.3342857,
author = {Trang, Nguyen Thi Thu and Ky, Nguyen Hoang and Son, Hoang and Hung, Nguyen Thanh and Huan, Nguyen Danh},
title = {Natural Language Understanding in Smartdialog: A Platform for Vietnamese Intelligent Interactions},
year = {2019},
isbn = {9781450362795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342827.3342857},
doi = {10.1145/3342827.3342857},
abstract = {Nowadays in the modern world, interactive smart dialogs with text or voice are gaining traction as the main digital interaction channel between human and machine. However, most of the current platforms do not support or have not fully developed for Vietnamese. In this paper, the authors propose a smart conversational platform through a text channel and/or voice channel in Vietnamese language, including these main steps: (i) Input Conversion and Pre-Processing, (ii) Entity Recognition, (iii) Intent Classification, (iv) Action Prediction and Execution, and (v) Output Generation. This paper focuses on presenting problems related to natural language understanding. To recognize entities in a sentence, the authors studied and optimized the features for Vietnamese with the Conditional Random Field model. With the problem of predicting user intent, this work proposed, experimented, and compared of Random Forest and BiLSTM deep learning model to optimize for the Vietnamese language. A platform was built and deployed for Milo smart speaker application (LUMI smart home) and VADI driver virtual assistant with the accuracy of around 98.7%.},
booktitle = {Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval},
pages = {158–163},
numpages = {6},
keywords = {Conditional Random Field, Entity recognition, Bidirectional Long Short-Term Memory, Random Forest, Intent classification},
location = {Tokushima, Japan},
series = {NLPIR 2019}
}

