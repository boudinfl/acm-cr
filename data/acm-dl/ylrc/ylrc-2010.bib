@inproceedings{10.5555/3045754.3045756,
author = {Chapelle, Olivier and Chang, Yi},
title = {Yahoo! Learning to Rank Challenge Overview},
year = {2010},
publisher = {JMLR.org},
abstract = {Learning to rank for information retrieval has gained a lot of interest in the recent years but there is a lack for large real-world datasets to benchmark algorithms. That led us to publicly release two datasets used internally at Yahoo! for learning the web search ranking function. To promote these datasets and foster the development of state-of-the-art learning to rank algorithms, we organized the Yahoo! Learning to Rank Challenge in spring 2010. This paper provides an overview and an analysis of this challenge, along with a detailed description of the released datasets.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {1–24},
numpages = {24},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045758,
author = {Burges, Christopher J. C. and Svore, Krysta M. and Bennett, Paul N. and Pastusiak, Andrzej and Wu, Qiang},
title = {Learning to Rank Using an Ensemble of Lambda-Gradient Models},
year = {2010},
publisher = {JMLR.org},
abstract = {We describe the system that won Track 1 of the Yahoo! Learning to Rank Challenge.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {25–35},
numpages = {11},
keywords = {lambda gradients, learning to rank, web search, gradient boosted trees},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045759,
author = {Busa-Fekete, R\'{o}bert and K\'{e}gl, Bal\'{a}zs and \'{E}ltet\"{o}, Tam\'{a}s and Szarvas, Gy\"{o}rgy},
title = {Ranking by Calibrated AdaBoost},
year = {2010},
publisher = {JMLR.org},
abstract = {This paper describes the ideas and methodologies that we used in the Yahoo learning-to-rank challenge. Our technique is essentially pointwise with a listwise touch at the last combination step. The main ingredients of our approach are 1) preprocessing (querywise normalization) 2) multi-class AdaBoost.MH 3) regression calibration, and 4) an exponentially weighted forecaster for model combination. In post-challenge analysis we found that preprocessing and training AdaBoost with a wide variety of hyperparameters improved individual models significantly, the final listwise ensemble step was crucial, whereas calibration helped only in creating diversity.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {37–48},
numpages = {12},
keywords = {exponentially weighted forecaster, adaboost, ranking, regression calibration, adaboost.MH},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045760,
author = {Geurts, Pierre and Louppe, Gilles},
title = {Learning to Rank with Extremely Randomized Trees},
year = {2010},
publisher = {JMLR.org},
abstract = {In this paper, we report on our experiments on the Yahoo! Labs Learning to Rank challenge organized in the context of the 23rd International Conference of Machine Learning (ICML 2010). We competed in both the learning to rank and the transfer learning tracks of the challenge with several tree-based ensemble methods, including Tree Bagging (Breiman, 1996), Random Forests (Breiman, 2001), and Extremely Randomized Trees (Geurts et al., 2006). Our methods ranked 10th in the first track and 4th in the second track. Although not at the very top of the ranking, our results show that ensembles of randomized trees are quite competitive for the "learning to rank" problem. The paper also analyzes computing times of our algorithms and presents some post-challenge experiments with transfer learning methods.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {49–61},
numpages = {13},
keywords = {ensemble methods, regression trees, learning to rank, transfer learning},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045761,
author = {Gulin, Andrey and Kuralenok, Igor and Pavlov, Dmitry},
title = {Winning the Transfer Learning Track of Yahoo!'s Learning to Rank Challenge with YetiRank},
year = {2010},
publisher = {JMLR.org},
abstract = {The problem of ranking the documents according to their relevance to a given query is a hot topic in information retrieval. Most learning-to-rank methods are supervised and use human editor judgements for learning. In this paper, we introduce novel pairwise method called YetiRank that modifies Friedman's gradient boosting method in part of gradient computation for optimization and takes uncertainty in human judgements into account. Proposed enhancements allowed YetiRank to outperform many state-of-the-art learning to rank methods in offline experiments as well as take the first place in the second track of the Yahoo! learning-to-rank contest. Even more remarkably, the first result in the learning to rank competition that consisted of a transfer learning task was achieved without ever relying on the bigger data from the "transfer-from" domain.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {63–76},
numpages = {14},
keywords = {learning to rank, gradient boosting, IR evaluation},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045762,
author = {Mohan, Ananth and Chen, Zheng and Weinberger, Kilian},
title = {Web-Search Ranking with Initialized Gradient Boosted Regression Trees},
year = {2010},
publisher = {JMLR.org},
abstract = {In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge. This paper summarizes the approach by the highly placed team Washington University in St. Louis. We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We demonstrate that it yields surprisingly accurate ranking results -- comparable to or better than GBRT. We combine the two algorithms by first learning a ranking function with RF and using it as initialization for GBRT. We refer to this setting as iGBRT. Following a recent discussion by Li et al. (2007), we show that the results of iGBRT can be improved upon even further when the web-search ranking task is cast as classification instead of regression. We provide an upper bound of the Expected Reciprocal Rank (Chapelle et al., 2009) in terms of classification error and demonstrate that iGBRT outperforms GBRT and RF on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising consistency.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {77–89},
numpages = {13},
keywords = {ranking, decision trees, boosting, random forests},
location = {Haifa, Israel},
series = {YLRC'10}
}

@inproceedings{10.5555/3045754.3045764,
author = {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
title = {Future Directions in Learning to Rank},
year = {2010},
publisher = {JMLR.org},
abstract = {The results of the learning to rank challenge showed that the quality of the predictions from the top competitors are very close from each other. This raises a question: is learning to rank a solved problem? On the on hand, it is likely that only small incremental progress can be made in the "core" and traditional problematics of learning to rank. The challenge was set in this standard learning to rank scenario: optimize a ranking measure on a test set. But on the other hand, there are a lot of related questions and settings in learning to rank that have not been yet fully explored. We review some of them in this paper and hope that researchers interested in learning to rank will try to answer these challenging and exciting research questions.},
booktitle = {Proceedings of the 2010 International Conference on Yahoo! Learning to Rank Challenge - Volume 14},
pages = {91–100},
numpages = {10},
location = {Haifa, Israel},
series = {YLRC'10}
}

