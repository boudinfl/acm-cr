@inproceedings{10.1145/1180639.1180641,
author = {Li, Jin},
title = {Peer-to-Peer Multimedia Applications},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180641},
doi = {10.1145/1180639.1180641},
abstract = {In both academia and industry, peer-to-peer (P2P) applications have attracted great attention. Peer-to-peer file sharing applications, such as Napster, Gnutella, Kazaa, BitTorrent, Skype and PPLive, have witnessed tremendous success among end users. And the uses of peer-to-peer network for multimedia streaming, conferencing, gaming, file backup, information retrieval is on the rise. Recent statistics suggests that P2P traffic accounts for as much as 70% of Internet traffic. Unlike a client-server based system, peers bring with them serving capacity. Therefore, as the demand of a peer-to-peer system grows, the capacity of the network grows, too. This enables a peer-to-peer multimedia application to be cheap to build and superb in scalability.The purpose of the tutorial is to examine issues associated with the successful building and deployment of a P2P multimedia application. The technologies discussed can be applied to P2P file sharing, P2P conference, P2P media streaming, P2P VoIP, and P2P storage applications. We start by examining two popular P2P applications, BitTorrent and Skype. The study of the two P2P applications helps us to understand the design principles of P2P applications in general. We then investigate a number of tools for building P2P multimedia applications, such as the overlay network, the scheduling algorithm, the erasure resilient coding, and NAT/firewall traversal. Finally, we move on to critical deployment decisions that make or break the P2P applications, such as P2P economy, security issues in P2P application, peer selection, monitoring and debugging utilities in P2P application.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {3â€“6},
numpages = {4},
keywords = {attack, peer selection, scheduling, P2P economy, distributed debugging, NAT/firewall traversal, erasure resilient coding, overlay, peer-to-peer},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180642,
author = {Cesar, Pablo and Chorianopoulos, Konstantinos},
title = {Interactive Digital Television and Multimedia Systems},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180642},
doi = {10.1145/1180639.1180642},
abstract = {Interactive digital television is an emerging field with a high impact in our societies: it offers interactive services to the masses. This tutorial aims to establish a common framework by summarizing the most significant results in this multidisciplinary field. The review includes topics such as content distribution, system software of the receivers, and user interaction. In addition, we will discuss current commercial events such as the next generation of optical discs (e.g., blue-ray), BBC peer-to-peer service, and mobile television. Based on this discussion, we will formulate an agenda for further research. The agenda includes, for example, end-user enrichment of television content and social television. This half-day tutorial will provide the attendee a solid understanding of the technologies currently in use and an introduction of the open questions in the field.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {7},
numpages = {1},
keywords = {interactive digital television, software architecture, tutorial projects},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180643,
author = {Chakraborty, Samarjit},
title = {Flexible Modelling and Performance Debugging of Real-Time Embedded Multimedia Systems},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180643},
doi = {10.1145/1180639.1180643},
abstract = {Real-time multimedia applications today represent the predominant workload in embedded devices ranging from set-top boxes to mobile phones and PDAs. However, implementing and performance tuning of such applications on embedded architectures is a challenging problem. On one hand, such architectures are increasingly becoming complex, with multiple processors, different kinds of memory subsystems and diverse on-chip communication architectures. On the other hand, designers of embedded devices, as well as application developers targeting such devices are faced with stringent performance constraints and time-to-market pressures. This has led to a lot of interest in (i) generic System-on-Chip (SoC) platform or template architectures which can be easily tuned for the application at hand, (ii) techniques for rapidly mapping/developing applications for such architectures, (iii) models and tools for analyzing and performance debugging of such implementations. This tutorial will provide a comprehensive overview of the recent developments in this area. It will be helpful to students, researchers, application developers and engineers who have a background in traditional real-time multimedia applications and would like to get an overview of the important issues and solutions pertaining to developing and performance debugging of multimedia applications for embedded SoC platforms.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {8},
numpages = {1},
keywords = {multimedia systems, debugging, performance analysis},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180644,
author = {Dubnov, Shlomo},
title = {Computer Audition: An Introduction and Research Survey},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180644},
doi = {10.1145/1180639.1180644},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {9},
numpages = {1},
keywords = {audio processing, computer audition, music information retrieval, style, music cognition},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180645,
author = {Keogh, Eamonn},
title = {Data Mining and Information Retrieval in Time Series/Multimedia Databases},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180645},
doi = {10.1145/1180639.1180645},
abstract = {Time series and multimedia data are ubiquitous; large volumes of such data are routinely created in scientific, industrial, entertainment, medical and biological domains. Examples include gene expression data, X-rays, electrocardiograms, electroencephalograms, gait analysis, stock market quotes, space telemetry etc.A decade ago, a seminal paper by Faloutsos, Ranganathan, Manolopoulos appeared in SIGMOD [1]. The paper, Fast Subsequence Matching in Time-Series Databases, has spawned at least a thousand references and extensions in the database/data mining and information retrieval communities. This tutorial will summarize the decade of progress since this influential paper appeared.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {10},
numpages = {1},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180646,
author = {Moccagatta, Iole},
title = {Recent Developments in Video Compression Standards and Their Impact on Embedded Platforms: From Scalable to Multi-View Video Coding},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180646},
doi = {10.1145/1180639.1180646},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {11},
numpages = {1},
keywords = {embedded platforms, MVC, video compression, implementation trade-offs, SVC, ITU-T, standard, complexity, MPEG},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180647,
author = {Ponceleon, Dulce and Cerruti, Julian},
title = {Multimedia Content Protection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180647},
doi = {10.1145/1180639.1180647},
abstract = {Multimedia content protection is a controversial topic. Content owners want to protect their rights while consumers want flexible usage, privacy and seamless content flow. In this tutorial we cover from cryptography fundamentals, to history, emerging standards, state-of-the-art approaches and live demos.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {12},
numpages = {1},
keywords = {watermarking, AACS, broadcast encryption, encryption, tracing, CPRM, content protection, revocation, piracy},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180648,
author = {Worring, Marcel and Snoek, Cees G. M.},
title = {Semantic Indexing and Retrieval of Video},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180648},
doi = {10.1145/1180639.1180648},
abstract = {This tutorial lays the foundation for the exciting new horizons that arise when multimedia data can automatically be indexed by its semantic content. It will cover basic video analysis techniques, explain the different methods for video indexing, and explore how users can be given interactive access to the data. For both indexing and interactive access TRECVID evaluations will be considered. Finally, some more insight on the challenges ahead and how to meet them will be presented.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {13},
numpages = {1},
keywords = {information visualization, video retrieval, semantic indexing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245528,
author = {Turk, M.},
title = {Session Details: Keynote},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245528},
doi = {10.1145/3245528},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180650,
author = {Goldberg, Ken},
title = {Sensitivity Analysis: Unexpected Outcomes in Art and Engineering},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180650},
doi = {10.1145/1180639.1180650},
abstract = {Contemporary art and engineering research are both at their best when things don't turn out as planned. I'll present selected examples based on artworks developed with students and other collaborators involving robots and networks over the past 20 years. These projects set out to investigate intersections of technology and nature, such as the Telegarden, a robot installation that allowed online participants to remotely tend a living garden; Ballet Mori, a classical dance performed to sounds triggered by live seismic data; and Demonstrate, where an ultra high-resolution video camera raised eyebrows at the 40th anniversary of the Free Speech Movement. Every project led to unexpected twists and complications...I'll also argue that the languages of contemporary art and engineering research are complex, dynamic, and often frustratingly impenetrable to outsiders. In art, a blue disk can be a cliche, or, in the right place at the right time, profound. In engineering, analogous contexts determine the beauty of a coordinate frame or mathematical equation. In both spheres, aesthetic interpretation is based on knowledge of prior art and contemporary dialogues. Being so similar, it is not surprising that unexpected forces arise when these two spheres are brought together.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {15},
numpages = {1},
keywords = {robot, webcamera, network, art, privacy, engineering},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245529,
author = {Klas, W.},
title = {Session Details: Best Papers Session},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245529},
doi = {10.1145/3245529},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180652,
author = {Knees, Peter and Schedl, Markus and Pohle, Tim and Widmer, Gerhard},
title = {An Innovative Three-Dimensional User Interface for Exploring Music Collections Enriched},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180652},
doi = {10.1145/1180639.1180652},
abstract = {We present a novel, innovative user interface to music repositories. Given an arbitrary collection of digital music files, our system creates a virtual landscape which allows the user to freely navigate in this collection. This is accomplished by automatically extracting features from the audio signal and training a Self-Organizing Map (SOM) on them to form clusters of similar sounding pieces of music. Subsequently, a Smoothed Data Histogram (SDH) is calculated on the SOM and interpreted as a three-dimensional height profile. This height profile is visualized as a three-dimensional island landscape containing the pieces of music. While moving through the terrain, the closest sounds with respect to the listener's current position can be heard. This is realized by anisotropic auralization using a 5.1 surround sound model. Additionally, we incorporate knowledge extracted automatically from the web to enrich the landscape with semantic information. More precisely, we display words and related images that describe the heard music on the landscape to support the exploration.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {17â€“24},
numpages = {8},
keywords = {visualization, clustering, web mining, user interface, music similarity, music information retrieval},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180653,
author = {Chen, Jun-Cheng and Chu, Wei-Ta and Kuo, Jin-Hau and Weng, Chung-Yi and Wu, Ja-Ling},
title = {Tiling Slideshow},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180653},
doi = {10.1145/1180639.1180653},
abstract = {This paper presents a new medium, called tiling slideshow, to display photos in a tile-like manner, coordinating with the pace of background music. In contrast to the conventional photo slideshow, multiple photos that have similar characteristics are well arranged and displayed at the same layout. Motivated by the concepts of technical writing, each displaying layout is composed of a larger topic photo and several small-size supportive photos. Based on this idea, the proposed tiling slideshow system consists of three major components: image clustering, music analyzer, and layout organizer. Given the limited displaying space, we consider the context and relationship between photos and model the layout organization as a constrainted optimization problem. Experiments on real consumer photograph collections show that the novel displaying method gives users more pleasant browsing experience than the methods that focus only on single photograph display.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {25â€“34},
numpages = {10},
keywords = {slideshow, image content analysis, music analysis, photo clustering},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180654,
author = {Hsu, Winston H. and Kennedy, Lyndon S. and Chang, Shih-Fu},
title = {Video Search Reranking via Information Bottleneck Principle},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180654},
doi = {10.1145/1180639.1180654},
abstract = {We propose a novel and generic video/image reranking algorithm, IB reranking, which reorders results from text-only searches by discovering the salient visual patterns of relevant and irrelevant shots from the approximate relevance provided by text results. The IB reranking method, based on a rigorous Information Bottleneck (IB) principle, finds the optimal clustering of images that preserves the maximal mutual information between the search relevance and the high-dimensional low-level visual features of the images in the text search results. Evaluating the approach on the TRECVID 2003-2005 data sets shows significant improvement upon the text search baseline, with relative increases in average performance of up to 23%. The method requires no image search examples from the user, but is competitive with other state-of-the-art example-based approaches. The method is also highly generic and performs comparably with sophisticated models which are highly tuned for specific classes of queries, such as named-persons. Our experimental analysis has also confirmed the proposed reranking method works well when there exist sufficient recurrent visual patterns in the search results, as often the case in multi-source news videos.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {35â€“44},
numpages = {10},
keywords = {video search, multimodal fusion, information bottleneck principle},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245530,
author = {Bailey, B.},
title = {Session Details: Short Papers Session 1},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245530},
doi = {10.1145/3245530},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180656,
author = {Wang, Huan and Yan, Shuicheng and Huang, Thomas and Tang, Xiaoou},
title = {Maximum Unfolded Embedding: Formulation, Solution, and Application for Image Clustering},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180656},
doi = {10.1145/1180639.1180656},
abstract = {In this paper, we present a novel spectral analysis algorithm for image clustering. First, the image manifold is embedded onto a low-dimensional feature space with dual objectives, i.e., maximizing the distances of faraway sample pairs meanwhile preserving the local manifold structure, which essentially results in a Trace Ratio optimization problem. Then an efficient iterative procedure is proposed to directly optimize the trace ratio and finally the clustering process is implemented on the derived low-dimensional embedding. Moreover, the linear approximation is also presented for handling the out-of-sample data. Experimental results show that our algorithm, referred to as Maximum Unfolded Embedding, brings an encouraging improvement in clustering accuracy over the state-of-the-art algorithms, such as K-Means, PCA-Kmeans, normalized cut cite shi00normalized, and Locality Preserving Clustering [13].},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {45â€“48},
numpages = {4},
keywords = {spectral analysis, maximum unfolded embedding, image clustering},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180657,
author = {Thomas-Kerr, Joseph and Burnett, Ian and Ritz, Christian},
title = {An Efficient Approach to Generic Multimedia Adaptation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180657},
doi = {10.1145/1180639.1180657},
abstract = {This paper addresses efficiency issues identified in the Bitstream Syntax Description Language used by the MPEG-21 generic multimedia adaptation framework. In particular, when used to adapt modern content formats such as H.264/AVC, the time required for processing increases exponentially relative to the duration of the bitstream. In response, the paper proposes several additional features for the Bitstream Syntax Description Language which reduce the complexity of adaptation using BSDL to a linear function of bitstream duration. These features are implemented and validated using bitstreams of real-world length.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {49â€“52},
numpages = {4},
keywords = {MPEG-21, multimedia content adaptation, bitstream syntax description, H.264/AVC},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180658,
author = {Li, Ying and Park, Youngja and Dorai, Chitra},
title = {Atomic Topical Segments Detection for Instructional Videos},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180658},
doi = {10.1145/1180639.1180658},
abstract = {This paper presents our latest work on structuring instructional videos into units of atomic topical segment so as to facilitate topic-based video browsing and offer efficient video authoring. Specifically, we developed a comprehensive text analysis component to first extract informative text cues such as keyword synonym set and sentence boundary information, from a video's transcript. These text cues are then applied with various audiovisual cues such as silence/music break and speech similarity, to identify topical segments. Early experiments carried out on collections of real data from targeted user communities have yielded good results, and the user feedback on using the generated topical segment information is very encouraging.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {53â€“56},
numpages = {4},
keywords = {text analysis, instructional video content analysis, video segmentation, multimodal feature integration},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180659,
author = {Luo, Hangzai and Fan, Jianping},
title = {Building Concept Ontology for Medical Video Annotation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180659},
doi = {10.1145/1180639.1180659},
abstract = {Most existing systems for content-based video retrieval (CBVR) are now amenable to support automatic low-level video content analysis and feature extraction, but they have limited effectiveness from a user's perspective. To support semantic video retrieval via keywords, we have proposed a novel framework by incorporating the concept ontology to enable more effective modeling and representation of semantic video concepts. Specifically, this novel framework includes: (a) Using the salient objects to achieve a middle-level understanding of the semantics of video contents; (b) Building a domain dependent concept ontology to enable multi-level modeling and representation of semantic video concepts; (c) Developing a multi-task boosting technique to achieve hierarchical video classifier training for automatic multi-level video annotation. The experimental results in a certain domain of medical education videos are also provided.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {57â€“60},
numpages = {4},
keywords = {concept ontology, hierarchical video classification, multi-task boosting},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180660,
author = {Seversky, Lee M. and Yin, Lijun},
title = {Real-Time Automatic 3D Scene Generation from Natural Language Voice and Text Descriptions},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180660},
doi = {10.1145/1180639.1180660},
abstract = {Automatic scene generation using voice and text offers a unique multimedia approach to classic storytelling and human computer interaction with 3D graphics. In this paper, we present a newly developed system that generates 3D scenes from voice and text natural language input. Our system is intended to benefit non-graphics domain users and applications by providing advanced scene production through an automatic system. Scene descriptions are constructed in real-time using a method for depicting spatial relationships between and among different objects. Only the polygon representations of the objects are required for object placement. In addition, our system is robust. The system supports different quality polygon models such as those widely available on the Internet.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {61â€“64},
numpages = {4},
keywords = {3D graphics, storytelling, voice recognition, real-time, text-to-scene, natural language},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180661,
author = {Zeng, Zhihong and Hu, Yuxiao and Liu, Ming and Fu, Yun and Huang, Thomas S.},
title = {Training Combination Strategy of Multi-Stream Fused Hidden Markov Model for Audio-Visual Affect Recognition},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180661},
doi = {10.1145/1180639.1180661},
abstract = {To simulate the human ability to assess affects, an automatic affect recognition system should make use of multi-sensor information. In the framework of multi-stream fused hidden Markov model (MFHMM), we present a training combination strategy towards audio-visual affect recognition. Different from the weighting combination scheme, our approach is able to use a variety of learning methods to obtain a robust multi-stream fusion result. We evaluate our approach in personal-independent recognition of 11 affective states from 20 subjects. The experimental results suggest that MFHMM outperforms IHMM which assumes the independence among streams, and the training combination strategy has the superiority over the weighting combination under clean and varying audio channel noise condition.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {65â€“68},
numpages = {4},
keywords = {multimodal human-computer interaction, emotion recognition, affective computing, affect recognition},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180662,
author = {Gordon, Andrew S.},
title = {Fourth Frame Forums: Interactive Comics for Collaborative Learning},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180662},
doi = {10.1145/1180639.1180662},
abstract = {In this paper, we describe Fourth Frame Forums, an application that combines traditional four-frame comic strips with online web-based discussion forums. In this application, users are presented with a four-frame comic strip where the last dialogue balloon of the fourth frame is left blank. By typing a statement into this dialogue balloon, the user creates a new discussion thread in the forum, where the user's dialogue choice can be critiqued and discussed by other users of the forum. We argue that Fourth Frame Forums provide an elegant and cost-effective solution for online education and training environments for communities of learners. We provide examples from the domain of US Army leadership development, and compare Fourth Frame Forums to alternative methods of story-directed simulation and training.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {69â€“72},
numpages = {4},
keywords = {comics, discussion forums, collaborative learning},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180663,
author = {Li, Na and Moraveji, Neema and Kimura, Hiroaki and Ofek, Eyal},
title = {Improving the Experience of Controlling Avatars in Camera-Based Games Using Physical Input},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180663},
doi = {10.1145/1180639.1180663},
abstract = {This paper investigates two methods of improving the user experience of camera-based interaction. First, problems that arise when avatars are designed to mimic a user's physical actions are presented. Second, a solution is proposed: adding a layer of separation between user and avatar while retaining intuitive user control. Two methods are proposed for this separation: spatially and temporally. Implementations of these methods are then presented in the context of a simple game and evaluate their effect on performance and satisfaction. Results of a human subject experiment are presented, showing that reducing the amount of user control can maintain, and even improve, user satisfaction if the design of such a reduction is appropriate. This is followed by a discussion of how the findings inform camera-based game design.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {73â€“76},
numpages = {4},
keywords = {avatar, camera-based interaction, interaction design, user study},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180664,
author = {Zheng, Qing-Fang and Wang, Wei-Qiang and Gao, Wen},
title = {Effective and Efficient Object-Based Image Retrieval Using Visual Phrases},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180664},
doi = {10.1145/1180639.1180664},
abstract = {In this paper, we draw an analogy between image retrieval and text retrieval and propose a visual phrase-based approach to retrieve images containing desired objects. The visual phrase is defined as a pair of adjacent local image patches and is constructed using data mining. We devise methods on how to construct visual phrases from images and how to encode the visual phrase for indexing and retrieval. Our experiments demonstrate that visual phrase-based retrieval approach can be very efficient and can be 20% more effective than its visual word-based counterpart.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {77â€“80},
numpages = {4},
keywords = {object-based image retrieval, visual phrase, inverted index, SIFT},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180665,
author = {Yang, Yi-Hsuan and Liu, Chia-Chu and Chen, Homer H.},
title = {Music Emotion Classification: A Fuzzy Approach},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180665},
doi = {10.1145/1180639.1180665},
abstract = {Due to the subjective nature of human perception, classification of the emotion of music is a challenging problem. Simply assigning an emotion class to a song segment in a deterministic way does not work well because not all people share the same feeling for a song. In this paper, we consider a different approach to music emotion classification. For each music segment, the approach determines how likely the song segment belongs to an emotion class. Two fuzzy classifiers are adopted to provide the measurement of the emotion strength. The measurement is also found useful for tracking the variation of music emotions in a song. Results are shown to illustrate the effectiveness of the approach.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {81â€“84},
numpages = {4},
keywords = {music emotion variation detection (MEVD), fuzzy vector, model generator (MG), music emotion strength, fuzzy k-NN (FKNN), emotion classifier (EC), fuzzy nearest-mean (FNM)},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180666,
author = {Li, Zhenguo and Liu, Jianzhuang and Tang, Xiaoou},
title = {Shape from Regularities for Interactive 3D Reconstruction of Piecewise Planar Objects from Single Images},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180666},
doi = {10.1145/1180639.1180666},
abstract = {3D object reconstruction from single 2D images has many applications in multimedia. This paper proposes an approach based on image regularities such as connectivity, parallelism, and orthogonality possessed by the objects with simple user interactions. It is assumed that the objects are piecewise planar. By representing the 3D objects as a shape vector consisting of the normals of the faces of the objects, we impose geometric constraints on this shape vector using the regularities of the objects. We derive a system of equations in terms of the shape vector and the focal length, which we can solve for the shape vector optimally. Experimental results on real images are shown to demonstrate the effectiveness of this method.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {85â€“88},
numpages = {4},
keywords = {image regularities, interactive 3D reconstruction, shape vector},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180667,
author = {Tang, Jinhui and Song, Yan and Hua, Xian-Sheng and Mei, Tao and Wu, Xiuqing},
title = {To Construct Optimal Training Set for Video Annotation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180667},
doi = {10.1145/1180639.1180667},
abstract = {This paper exploits the criteria to optimize the training set construction for video annotation. Most existing learning-based semantic annotation approaches require a large training set to achieve good generalization capacity, in which a considerable amount of labor-intensively manual labeling is desirable. However, it is observed that the generalization capacity of a classifier highly depends on the geometrical distribution rather than the size of the training data. We argue that a training set which includes most temporal and spatial distribution of the whole data will achieve a satisfying performance even in the case of limited size of training set. In order to capture the geometrical distribution characteristics of a given video collection, we propose the following four metrics for constructing an optimal training set, including Salience Time Dispersiveness Spatial Dispersiveness and Diversity. Moreover, based on these metrics, we propose a set of optimization rules to capture the most distribution information of the whole data for a training set with a given size. Experimental results demonstrate that these rules are effective for training set construction for video annotation, and significantly outperform random training set selection as well.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {89â€“92},
numpages = {4},
keywords = {video annotation, training set construction},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180668,
author = {Lee, Ho-Jae and Nam, Jeho},
title = {Low Complexity Controllable Scrambler/Descrambler for H.264/AVC in Compressed Domain},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180668},
doi = {10.1145/1180639.1180668},
abstract = {In this paper, we present a novel algorithm that scrambles and descrambles a H.264/AVC video element stream. In particular, the proposed method selectively exploits the unique characteristics of H.264/AVC video coding standard. Specifically, we manipulate DCT coefficients and CABAC initialization table, to secure the visual content of H.264/AVC video. This new algorithm is light-weight and able to support the level of security (scrambling strength) as well. Extensive experimental results indicate that the proposed algorithm is highly effective and feasible, yet computationally very efficient.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {93â€“96},
numpages = {4},
keywords = {security, encryption, H.264, scramble, CABAC, AVC},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180669,
author = {Gao, Yuli and Fan, Jianping},
title = {Automatic Function Selection for Large Scale Salient Object Detection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180669},
doi = {10.1145/1180639.1180669},
abstract = {Robust detection of a large dictionary of salient objects in natural image database is of fundamental importance to image retrieval systems. We review three popular frameworks for salient object detection, i.e., segmentation-based method, grid-based method and part-based method and discuss their advantages and limitations. We argue that using these frameworks individually is generally not enough to handle a large number of salient object classes accurately because of the intrinsic diversity of salient object features. Motivated by this observation, we have proposed a new system which combines the merits of these frameworks into one single hybrid system. The system automatically selects the appropriate modeling method for each individual object class using J measure and shape variance. We conduct comparison experiments on two popular image dataset -- Corel and LabelMe. Empirical results have shown that the proposed hybrid method is more general and can handle much more salient object classes in a robust manner.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {97â€“100},
numpages = {4},
keywords = {automatic image annotation, salient object detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180670,
author = {Rakkolainen, Ismo},
title = {Tracking Users through a Projection Screen},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180670},
doi = {10.1145/1180639.1180670},
abstract = {Smart walls, interactive displays, and immersive projection dis are increasingly used in visualization and multimedia system. Tracking the user is one key element to implement inter, or e.g., to render new views according to the viewer's position to creat virtual reality-type displays. Persons viewing traditional screens or virtual reality rooms (CAVE) can be track with elec, optical, or many other tracking methods. If optica or computer vision track is used, then line of sight between the cameras and the tracked object is required. Opaque projection screens severely restrict the possibl camera setup, especially in CAVE-like environments.We present a novel, patented method to track users in three di through a projection screen. Our novel idea is to emplo a screen material that is opaque in the visible spectrum, and trans in the near-infrared (IR) spec, and then to use com vision -based or other optical tracking method to track the viewer in infrared spectrum. Non-solid screens like the FogScree seem to be one screen material that can deliver our objective, but also solid screens may be possible.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {101â€“104},
numpages = {4},
keywords = {interaction, fogScreen, computer vision, tracking},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180671,
author = {Cao, Liangliang and Liu, Jianzhuang and Tang, Xiaoou},
title = {3D Object Retrieval Using 2D Line Drawing and Graph Based Relevance Reedback},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180671},
doi = {10.1145/1180639.1180671},
abstract = {This paper aims to provide a user-friendly interface for 3D object retrieval. In previous 3D retrieval systems, the user mainly uses two methods to input a query: providing an existing 3D objects, or providing partial shape information of desired objects such as text and 2D shapes. The first method fails when the user does not have a similar 3D object in hand, and the second method cannot sufficiently describe 3D shapes of objects. We believe that the best way is to have a good interface that can convert a 2D sketch drawn by the user into a 3D object as the query. A 2D line drawing is easy to be drawn and is the simplest and most direct way of illustrating a 3D object. In this paper, we develop an interface of 3D object reconstruction from line drawings, which allows the user to draw line drawings of objects with both planar and curved surfaces. In addition, in order to refine the retrieved results, we develop a relevance feedback algorithm based on a novel graph discriminant analysis. Compared with recently published relevance feedback algorithms, our algorithm achieves better retrieval performance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {105â€“108},
numpages = {4},
keywords = {3D object retrieval, 3D reconstruction, query generation, relevance feedback},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180672,
author = {Wang, Huan and Liu, Song and Chia, Liang-Tien},
title = {Does Ontology Help in Image Retrieval? A Comparison between Keyword, Text Ontology and Multi-Modality Ontology Approaches},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180672},
doi = {10.1145/1180639.1180672},
abstract = {Ontologies are effective for representing domain concepts and relations in a form of semantic network. Many efforts have been made to import ontology into information matchmaking and retrieval. This trend is further accelerated by the convergence of various high-level concepts and low-level features supported by ontologies. In this paper we propose a comparison between traditional keyword based image retrieval and the promising ontology based image retrieval. To be complete, we construct the ontologies not only on text annotation, but also on a combination of text annotation and image feature. The experiments are conducted on a medium-sized data set including about 4000 images. The result proved the efficacy of utilizing both text and image features in a multi modality ontology to improve the image retrieval.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {109â€“112},
numpages = {4},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180673,
author = {Lu, Shijian and Tan, Chew Lim},
title = {Automatic Document Orientation Detection and Categorization through Document Vectorization},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180673},
doi = {10.1145/1180639.1180673},
abstract = {This paper presents an automatic orientation detection and categorization technique that is capable of detecting the orientation of multilingual documents with arbitrary skew and categorizing document images according to the underlying languages. We carry out orientation detection and categorization through document vectorization, which encodes document orientation and language information and converts each document image into an electronic document vector through the exploitation of the density and distribution of vertical component runs. For each language of interest, a pair of vector templates is first constructed through a training process. Orientation and category of the query image are then determined based on distances between the query document vector and the constructed vector templates. Experiments over 492 testing document images show that the average orientation detection and categorization rates reach up to 97.56% and 99.59%, respectively.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {113â€“116},
numpages = {4},
keywords = {document image, document orientation detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180674,
author = {Baldi, Mario and De Martin, Juan Carlos and Masala, Enrico and Vesco, Andrea},
title = {Distortion-Aware Video Communication with Pipeline Forwarding},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180674},
doi = {10.1145/1180639.1180674},
abstract = {This paper tackles the issue of optimizing the transport of video over packet networks with respect to both resource utilization and user perceived quality. Previous work showed that the quality of service requirements of multimedia applications can be satisfied by pipeline forwarding of packets. However, the current Internet is not based on such technology and its incremental introduction raises questions on how to handle video packets generated by pipeline forwarding unaware sources at the interface between a subnetwork deploying conventional packet scheduling techniques and one implementing pipeline forwarding. This work proposes to use the perceptual importance of the carried video samples to determine which packets shall be transferred with pipeline forwarding - thus receiving deterministic service - and which with a traditional, e.g., best effort or differentiated, service. Simulation results with the first implemented variants of this solution are presented.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {117â€“120},
numpages = {4},
keywords = {multimedia networking, video streaming, quality of service},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180675,
author = {Zhang, Jiqi and Wong, Hau-San and Yu, Zhiwen},
title = {3D Model Metrieval Based on Volumetric Extended Gaussian Image and Hierarchical Self Organizing Map},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180675},
doi = {10.1145/1180639.1180675},
abstract = {In this paper, we introduce a novel shape signature, called Volumetric Extended Gaussian Image (VEGI). It captures the volumetric distribution of a 3D mesh model along the latitude-longitude direction without conventional pose normalization and is translation and scaling invariant. Rotation invariance is accomplished by further calculating the spherical harmonic transform of this directional distribution. Due to the completeness and orthonormality properties of spherical harmonics, the VEGI also provides multi-resolution description of a model so that a multi-level indexing scheme based on Hierarchical Self Organizing Map (HSOM) can be established to improve retrieval efficiency. Experimental results show that our retrieval architecture has high discriminative power and outperforms many existing methods.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {121â€“124},
numpages = {4},
keywords = {volumetric extended Gaussian image, 3D model retrieval, hierarchical self organizing map},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180676,
author = {Rautiainen, Mika and Sepp\"{a}nen, Tapio and Ojala, Timo},
title = {On the Significance of Cluster-Temporal Browsing for Generic Video Retrieval: A Statistical Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180676},
doi = {10.1145/1180639.1180676},
abstract = {In this paper, we test statistically the effect of content-based browsing in generic video retrieval. Using TRECVID 2004 and 2005 experiments, we demonstrate that content-based browsing improves retrieval over sequential queries and relevance feedback. Two user groups, novices and system developers have been used in the experiments on large and multilingual video collections. Novice users were found to achieve improvement in search effectiveness with cluster-temporal browsing by statistically significant amount. System developers did not have statistically significant difference between the different system configurations.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {125â€“128},
numpages = {4},
keywords = {cluster-temporal browsing, Wilcoxon's signed-rank test, non-parametric test, content-based video retrieval},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180677,
author = {Liu, Qifeng and Jung, Cheolkon and Moon, Youngsu},
title = {Text Segmentation Based on Stroke Filter},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180677},
doi = {10.1145/1180639.1180677},
abstract = {Most existing methods of text segmentation in video images are not robust because they do not consider the intrinsic characteristics of text. In this paper, we propose a novel method of text segmentation based on stroke filter (SF). First, we give the definition of text, which is realized in the form of stroke filter based on local region analysis. Based on stroke filter response, text polarity determination and local region growing modules are performed successively. The effectiveness of our method is validated by experiments on a challenging database.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {129â€“132},
numpages = {4},
keywords = {text segmentation, image processing, text polarity determination},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180678,
author = {Shih, Timothy K. and Tang, Nick C. and Yeh, Wei-Sung and Chen, Ta-Jen and Lee, Wonjun},
title = {Video Inpainting and Implant via Diversified Temporal Continuations},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180678},
doi = {10.1145/1180639.1180678},
abstract = {Recent interesting issues in video inpainting are defect removal and object removal. We take one more step to replace the removed objects in a video sequence by implanting objects from another video. Before implant, we improve an exemplar-based image inpainting algorithm by using a new patch matching strategy which incorporates edge properties. The data term used in a priority computation of candidate patches is also redefined. We take varieties of temporal continuations of foreground and background into consideration. A motion compensated inpainting procedure is then proposed. The inpainted video backgrounds are visually pleasant with smooth transitions. A simple tracking algorithm is then used to produce a foreground video, which is implanted into the inpainted background video. Our results are available at http://www.mine.tku.edu.tw/inpainting.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {133â€“136},
numpages = {4},
keywords = {object removal, video inpainting, temporal continuations, object tracking, video implant},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180679,
author = {Shrestha, Prarthana and Weda, Hans and Barbieri, Mauro and Sekulovski, Dragan},
title = {Synchronization of Multiple Video Recordings Based on Still Camera Flashes},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180679},
doi = {10.1145/1180639.1180679},
abstract = {Combining audiovisual sequences from different cameras requires precise alignment in time. Current synchronization techniques involve using geometrical properties such as camera positions and object features. In this paper, we present a synchronization method based on detecting flashes present in the video content. Such flashes, generated by still cameras, cause a sharp bright frame in the videos. They are detected using an adaptive threshold on luminance variation across the frames. The resulting flash sequences are searched for matches. The matching flashes indicate overlapping content, and allow determining the offset time between cameras. The experimental results from flash detection and flash sequence matching show perfect synchronization in all examined cases.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {137â€“140},
numpages = {4},
keywords = {flash detection, synchronization, inexact string-matching},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180680,
author = {Summet, Jay W. and Flagg, Matthew and Rehg, James M. and Abowd, Gregory D. and Weston, Neil},
title = {GVU-PROCAMS: Enabling Novel Projected Interfaces},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180680},
doi = {10.1145/1180639.1180680},
abstract = {Front projection allows large displays to be deployed relatively easily. However, it is sometimes difficult to find a location to place a projector, especially for ad-hoc installations. Additionally, front projection suffers from shadows and occlusions, making it ill-suited for interactive displays. The GVU-PROCAMS system allows programmers to deploy projectors and displays easily in arbitrary locations by enabling enhanced keystone correction via warping on 3D hardware. In addition, it handles the calibration of multiple projectors using computer vision to produce a redundantly illuminated surface. Redundant illumination offers robustness in the face of occlusions, providing a user with the experience of a rear-projected surface. This paper presents a stand-alone application (WinPVRP) and a programming system (GVU-PROCAMS) that easily allows others to create projected displays with enhanced warping and redundant illumination.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {141â€“144},
numpages = {4},
keywords = {warped projection, projection, redundant projection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180681,
author = {Borba, Gustavo B. and Gamba, Humberto R. and Marques, Oge and Mayron, Liam M.},
title = {An Unsupervised Method for Clustering Images Based on Their Salient Regions of Interest},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180681},
doi = {10.1145/1180639.1180681},
abstract = {We have developed a biologically-motivated, unsupervised way of grouping together images whose salient regions of interest (ROIs) are perceptually similar regardless of the visual contents of other (less relevant) parts of the image. In the implemented model cluster membership is assigned based on feature vectors extracted from salient ROIs. This paper focuses on the experimental evaluation of the proposed approach for several combinations of feature extraction techniques and unsupervised clustering algorithms. The results reported here show that this is a valid approach and encourage further research.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {145â€“148},
numpages = {4},
keywords = {image retrieval, clustering, visual attention},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180682,
author = {Tsui, Tsz Kin and Zhang, Xiao-Ping and Androutsos, Dimitri},
title = {Quaternion Image Watermarking Using the Spatio-Chromatic Fourier Coefficients Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180682},
doi = {10.1145/1180639.1180682},
abstract = {In this paper, a new color watermarking algorithm that uses the quaternion fourier transform (QFT) to mark the La*b* components of color images is presented. First, we propose an interpretation of the QFT coefficients using the Spatio-Chromatic Fourier analysis, so the effects of any changes to the coefficients can be predicted. Next, watermark casting is performed by modifying the positive and negative coefficients together. The idea is twofold: Robustness is achieved by embedding a color watermark in the coefficient with positive frequency, which spreads it to all components in the spatial domain. On the other hand, invisibility is satisfied by modifying the coefficient with negative frequency, such that the combined effects of the two are insensitive to human eyes.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {149â€“152},
numpages = {4},
keywords = {spatio-chromatic image processing, color image watermarking, quaternion fourier transform},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180683,
author = {Qin, Min and Zimmermann, Roger},
title = {Supporting Guaranteed Continuous Media Streaming in Mobile Ad-Hoc Networks with Link Availability Prediction},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180683},
doi = {10.1145/1180639.1180683},
abstract = {As wireless connectivity is integrated into many handheld devices, streaming multimedia content among mobile ad-hoc peers is becoming a popular application. In this paper, we introduce a new QoS criterion called path-availability-based service coverage to mobile streaming applications. Based on this QoS criterion, we propose a dynamic service replication strategy for providing guaranteed continuous streaming service to all nodes in mobile ad-hoc networks (MANETs). Unlike previous approaches that rely on remaining link duration to determine when service replication should be invoked, in our strategy, a client requests service replication when the path availability between the client and a server instance drops below a certain threshold. This strategy is based on our previous work on continuous link availability, which proves to be helpful in achieving smooth streaming performance in MANETs. Simulation results show that our service replication algorithm can quickly increase the service coverage to 100% while reducing the chance of link breakdowns during service replication.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {153â€“156},
numpages = {4},
keywords = {link availability, service replication, streaming, mobile ad-hoc networks, mobility models},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180684,
author = {Yelizaveta, Marchenko and Tat-Seng, Chua and Ramesh, Jain},
title = {Transductive Inference Using Multiple Experts for Brushwork Annotation in Paintings Domain},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180684},
doi = {10.1145/1180639.1180684},
abstract = {Many recent studies perform annotation of paintings based on brushwork. In these studies the brushwork is modeled indirectly as part of the annotation of high-level artistic concepts such as the artist name using low-level texture. In this paper, we develop a serial multi-expert framework for explicit annotation of paintings with brushwork classes. In the proposed framework, each individual expert implements transductive inference by exploiting both labeled and unlabelled data. To minimize the problem of noise in the feature space, the experts select appropriate features based on their relevance to the brushwork classes. The selected features are utilized to generate several models to annotate the unlabelled patterns. The experts select the best performing model based on Vapnik combined bound. The transductive annotation using multiple experts out-performs the conventional baseline method in annotating patterns with brushwork classes.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {157â€“160},
numpages = {4},
keywords = {brushwork, painting, feature selection, transductive inference},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180685,
author = {Kim, Won-gyum and Seo, Yong-seok and Suh, Young-Ho},
title = {Hybrid Watermarking for Improving Detector Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180685},
doi = {10.1145/1180639.1180685},
abstract = {In this paper, we describe the adaptive hybrid watermarking scheme to improve the watermark detector's performance. In the hybrid watermarking, the conventional spatial watermarking and QIM scheme are applied simultaneously to the spatial domain of the image. According to the edge-map of the image the spatial watermarking is applied to the flat region and the QIM is applied to the high frequency region including edge and texture. The threshold between flat and edge is decided empirically and watermark blocks are embedded repeatedly into the image. In the detection process, we show how to estimate the threshold value by ACF(Auto-Correlation Function). Experimental results show the proposed hybrid watermarking estimates the watermark signal correctly than the conventional spatial watermarking.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {161â€“164},
numpages = {4},
keywords = {hybrid, copyright protection, image watermarking},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180686,
author = {Jung, Joonyoung and Kwon, Ohyung and Lee, Sooin},
title = {Design and Implementation of a Multi-Stream CableCARD with a High-Speed DVB-Common Descrambler},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180686},
doi = {10.1145/1180639.1180686},
abstract = {As the digital cable TV broadcasting technique is developed, various applications on a digital cable receiver have been required. Particularly, OpenCableTM specifications for digital cable TV broadcasting have presented a cable receiver with multi-tuner for serving a Digital Video Recorder (DVR) application, a Picture-In-Picture (PIP) application, a Picture-Out-Picture (POP) application and etc. But there are some problems for supporting such applications because the existing CableCARDTM device processes conditional access for only one channel at once. Therefore OpenCableTM specifications have introduced the Multi-Stream CableCARDTM device in order to solve the above problems. The Multi-Stream CableCARDTM device can support conditional access processing for 2 or more channels simultaneously. In this paper, we have designed hardware architecture of the Multi-Stream CableCARDTM device. The designed Multi-Stream CableCARDTM device is implemented based on a FPGA which processes multi-channel filtering, MPEG-2 TS filtering, and DVB-Common descrambling and a microprocessor which manages conditional access applications. Especially we have proposed a descrambler design for the high-speed processing in order to support the 200 Mbps input data bandwidth.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {165â€“168},
numpages = {4},
keywords = {DVB-common descrambling, multi-stream cableCARD},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180687,
author = {Neo, Shi-Yong and Zheng, Yantao and Chua, Tat-Seng and Tian, Qi},
title = {News Video Search with Fuzzy Event Clustering Using High-Level Features},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180687},
doi = {10.1145/1180639.1180687},
abstract = {Precise automated video search is gaining in importance as the amount of multimedia information is increasing at exponential rates. One of the drawbacks that make video retrieval difficult is the lack of available semantics. In this paper, we propose to supplement the semantic knowledge for retrieval by providing useful semantic clusters derived from event entities present in the news video. These entities include the output from keywords derived from the automated speech recognition (ASR) and event-related High-level Features (HLF) extracted from the news video at the pseudo story level. Fuzzy clustering is then carried out to group similar stories together to form semantic clusters. The retrieval system utilizes these clusters to refine the re-ranking process in the Pseudo Relevance Feedback (PRF) step. Initial experiments performed on video search task using the TRECVID 2005 dataset show that the proposed approach can improve the search performance significantly.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {169â€“172},
numpages = {4},
keywords = {event-based clustering, video retrieval},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180688,
author = {Cheng, En and Jing, Feng and Zhang, Lei and Jin, Hai},
title = {Scalable Relevance Feedback Using Click-through Data for Web Image Retrieval},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180688},
doi = {10.1145/1180639.1180688},
abstract = {Relevance feedback (RF) has been extensively studied in the content-based image retrieval community. However, no commercial Web image search engines support RF because of scalability, efficiency and effectiveness issues. In this paper we proposed a scalable relevance feedback mechanism using click-through data for web image retrieval. The proposed mechanism regards users' click-through data as implicit feedback which could be collected at lower cost, in larger quantities and without extra burden on the user. During RF process, both textual feature and visual feature are used in a sequential way. To seamlessly combine textual feature-based RF and visual feature-based RF, a query concept-dependent fusion strategy is automatically learned. Experimental results on a database consisting of nearly three million Web images show that the proposed mechanism is wieldy, scalable and effective.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {173â€“176},
numpages = {4},
keywords = {click-through, implicit feedback, relevance feedback, web image retrieval},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245531,
author = {Peljhan, M.},
title = {Session Details: Arts Short Papers Poster Session 1},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245531},
doi = {10.1145/3245531},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180690,
author = {Mann, Steve and Fung, James and Lo, Raymond},
title = {Cyborglogging with Camera Phones: Steps toward Equiveillance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180690},
doi = {10.1145/1180639.1180690},
abstract = {We present "equiveillance" as a conceptual framework for understanding the balance between surveillance and sousveillance. In addition to this conceptual framework we also present a practical embodiment of equiveillance in the form of a new program called "cyborglogger" ('glogger) that runs on most modern camera phones, along with a server architecture to support 'glogger. Finally we show how the 'glogger program and server architecture create sousveillance communities. Cyborglogger implements features that are ideal for sousveillance such as including continuous capture and real-time upload aimed at communicating personal day-to-day narratives of everyday experiences. The server architecture includes a custom-built community web site that allows a sousveillance community to interact, in real time, with 'glogs from various users of the system. Participants can use their camera phones to display output from other camera phones, resulting in peer-to-peer sharing of visual narratives. This real-time live monitoring creates a social commentary and discourse that runs parallel to the widespread surveillance already present in the world around us. Unlike surveillance, which often happens in secrecy, our tools for sousveillance are freely available and moves personal experience capture into the realm of the everyday, with an open forum for public discourse. We thus explore how users engaging in sousveillance with the 'glogger application provide a balance to existing well-established surveillance practices by examining the philosophical questions that arise from the new artistic practice of sousveillance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {177â€“180},
numpages = {4},
keywords = {cyborg logging, continuous archival retrieval, personal experiences},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180691,
author = {Mann, Steve},
title = {The Andantephone: A Musical Instrument That You Play by Simply Walking},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180691},
doi = {10.1145/1180639.1180691},
abstract = {I present a new way of teaching musical tempo and rhythm by writing out the music on a timeline along the ground, with, for example, chalk, in a form in which each beat of the music corresponds to one footstep. In some setups I use computer vision to track participants so that the music is actually generated by their footsteps moving through the space. In other embodiments I installed patio stones, leading to a musical garden, and outfitted each stone with a pressure sensor. I connected the pressure sensors to a central computer, which I programmed to step through a song, as people walk to the garden. Each footstep activates the next note in the song, so that there is perfect synchronization between the music and the speed of your walking (i.e. if you walk faster the song plays faster, if you stop walking the song stops, etc.). In one embodiment the computer controls an outdoor pipe-organ sculpture that I made from PVC pipes. Another provides a MIDI output to control a piano or other sound-producing device. Some versions of the sculpture are human-powered, either electrically, or wholly acoustically without the use of a computer.I also arranged various musical compositions suitable to this new form of art.This teaching method, together with various sculptural embodiments of it were found to break down social barriers and create cross-cultural and cross-generational ties. For example, children and their grand parents enjoyed walking through the gardens at Pine Hill Estates where a version of my sculpture is permanently installed.Other variations of the sculpture include arrays of hydraulophonic fountain jets that play a song in a water park when a person walks on the water. Each note or beat is triggered by a water pressure increase when one of the water jets is blocked by the foot of a user stepping on it.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {181â€“184},
numpages = {4},
keywords = {musical sculptures, interactive sculptures},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180692,
author = {Rakkolainen, Ismo and Erdem, Tanju and Erdem, \c{C}iundefineddem and \"{O}zkan, Mehmet and Laitinen, Markku},
title = {Interactive "Immaterial" Screen for Performing Arts},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180692},
doi = {10.1145/1180639.1180692},
abstract = {Various performing arts increasingly employ projection screens and other information displays as essential elements of the show. We present some possibilities and lessons learned for the interactiv, "immaterial" walk-through FogScreen in performing arts, analyze its suit for stage setups, and give some guidelines for show desig. We also describe our experiments on composing inter 3D graph for a FogScreen-based performance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {185â€“188},
numpages = {4},
keywords = {performing arts, interaction, FogScreen, 3D graphics},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180693,
author = {Bottoni, Paolo and Labella, Anna and Faralli, Stefano and Pierro, Mario and Scozzafava, Claudio},
title = {Interactive Composition, Performance and Music Generation through Iterative Structures},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180693},
doi = {10.1145/1180639.1180693},
abstract = {We present HYPERSCORE, a system for interactive execution of musical performances coupled with the generation system CLIPSCORE, in which a performer can virtually manipulate execution parameters through gestural interaction in real time. HYPERSCORE allows users to exploit their perceptive capabilities in terms of space and time in order to control and manipulate, via a multimodal interaction, the main component of music, namely time.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {189â€“192},
numpages = {4},
keywords = {multimedia interaction, iterative structures},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180694,
author = {Tseng, Yu-Chuan and Lee, Chia-Hsiang},
title = {Flow: An Interactive AJAX-Based Internet Information Requesting System},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180694},
doi = {10.1145/1180639.1180694},
abstract = {"Flow" is an interactive art work that indicates the chaotic state when we receive too much information from the medium. When the Internet becomes our major resource of information, we try to acquire as much news as possible from the net. That is the reason why we attempt to integrate web AJAX (Asynchronous JavaScript and XML) technology, Text-To-Speech (TTS), images analysis and presentation, and multichannel sound play, to create a context of flowing information which is unable to be hold but in a state of intermix.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {193â€“196},
numpages = {4},
keywords = {TTS, interactive installation, AJAX, digital art, information},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180695,
author = {Morrison, Ann and Mitchell, Peta and Muhlberger, Ralf},
title = {Talk2Me: The Art of Augmenting Conversations},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180695},
doi = {10.1145/1180639.1180695},
abstract = {This paper describes an interactive installation work set in a large dome space. The installation is an audio and physical re-rendition of an interactive writing work. In the original work, the user interacted via keyboard and screen while online. This rendition of the work retains the online interaction, but also places the interaction within a physical space, where the main 'conversation' takes place by the participant-audience speaking through microphones and listening through headphones. The work now also includes voice and SMS input, using speech-to-text and text-to-speech conversion technologies, and audio and displayed text for output. These additions allow the participant-audience to co-author the work while they participate in audible conversation with keyword-triggering characters (bots). Communication in the space can be person-to-computer via microphone, keyboard, and phone; person-to-person via machine and within the physical space; computer-to-computer; and computer-to-person via audio and projected text.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {197â€“200},
numpages = {4},
keywords = {dome, critical technical practices, MOO, co-authoring, locative, interactive writing, interactive installation, engagement},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245532,
author = {Worring, M.},
title = {Session Details: Content Session 1: Multi-Modal Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245532},
doi = {10.1145/3245532},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180697,
author = {Duan, Ling-Yu and Wang, Jinqiao and Zheng, Yantao and Jin, Jesse S. and Lu, Hanqing and Xu, Changsheng},
title = {Segmentation, Categorization, and Identification of Commercial Clips from TV Streams Using Multimodal Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180697},
doi = {10.1145/1180639.1180697},
abstract = {TV advertising is ubiquitous, perseverant, and economically vital. Millions of people's living and working habits are affected by TV commercials. In this paper, we present a multimodal ("visual + audio + text") commercial video digest scheme to segment individual commercials and carry out semantic content analysis within a detected commercial segment from TV streams.Two challenging issues are addressed. Firstly, we propose a multimodal approach to robustly detect the boundaries of individual commercials. Secondly, we attempt to classify a commercial with respect to advertised products/services. For the first, the boundary detection of individual commercials is reduced to the problem of binary classification of shot boundaries via the mid-level features derived from two concepts: Image Frames Marked with Product Information (FMPI) and Audio Scene Change Indicator (ASCI). Moreover, the accurate individual boundary enables us to perform commercial identification by clip matching via a spatial-temporal signature. For the second, commercial classification is formulated as the task of text categorization by expanding sparse texts from ASR/OCR with external knowledge. Our boundary detection has achieved a good result of F1 = 93.7% on the dataset comprising 499 individual commercials from TRECVID'05 video corpus. Commercial classification has obtained a promising accuracy of 80.9% on 141 distinct ones. Based on these achievements, various applications such as an intelligent digital TV set-top box can be accomplished to enhance the TV viewer's capabilities in monitoring and managing commercials from TV streams.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {201â€“210},
numpages = {10},
keywords = {TV commercial, video classification, text categorization, segmentation, semantics, multimodal analysis, mid-level features},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180698,
author = {Zhu, Qiang and Yeh, Mei-Chen and Cheng, Kwang-Ting},
title = {Multimodal Fusion Using Learned Text Concepts for Image Categorization},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180698},
doi = {10.1145/1180639.1180698},
abstract = {Conventional image categorization techniques primarily rely on low-level visual cues. In this paper, we describe a multimodal fusion scheme which improves the image classification accuracy by incorporating the information derived from the embedded texts detected in the image under classification. Specific to each image category, a text concept is first learned from a set of labeled texts in images of the target category using Multiple Instance Learning [1]. For an image under classification which contains multiple detected text lines, we calculate a weighted Euclidian distance between each text line and the learned text concept of the target category. Subsequently, the minimum distance, along with low-level visual cues, are jointly used as the features for SVM-based classification. Experiments on a challenging image database demonstrate that the proposed fusion framework achieves a higher accuracy than the state-of-art methods for image classification.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {211â€“220},
numpages = {10},
keywords = {multiple instance learning, image annotation, multimodal fusion, image categorization, text detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180699,
author = {Xu, Changsheng and Wang, Jinjun and Wan, Kongwah and Li, Yiqun and Duan, Lingyu},
title = {Live Sports Event Detection Based on Broadcast Video and Web-Casting Text},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180699},
doi = {10.1145/1180639.1180699},
abstract = {Event detection is essential for sports video summarization, indexing and retrieval and extensive research efforts have been devoted to this area. However, the previous approaches are heavily relying on video content itself and require the whole video content for event detection. Due to the semantic gap between low-level features and high-level events, it is difficult to come up with a generic framework to achieve a high accuracy of event detection. In addition, the dynamic structures from different sports domains further complicate the analysis and impede the implementation of live event detection systems. In this paper, we present a novel approach for event detection from the live sports game using web-casting text and broadcast video. Web-casting text is a text broadcast source for sports game and can be live captured from the web. Incorporating web-casting text into sports video analysis significantly improves the event detection accuracy. Compared with previous approaches, the proposed approach is able to: (1) detect live event only based on the partial content captured from the web and TV; (2) extract detailed event semantics and detect exact event boundary, which are very difficult or impossible to be handled by previous approaches; and (3) create personalized summary related to certain event, player or team according to user's preference. We present the framework of our approach and details of text analysis, video analysis and text/video alignment. We conducted experiments on both live games and recorded games. The results are encouraging and comparable to the manually detected events. We also give scenarios to illustrate how to apply the proposed solution to professional and consumer services.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {221â€“230},
numpages = {10},
keywords = {event detection, web-casting text, broadcast video},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245533,
author = {Pingali, G.},
title = {Session Details: Applications Session 1: Media Presentation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245533},
doi = {10.1145/3245533},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180701,
author = {Erol, Berna and Berkner, Kathrin and Joshi, Siddharth},
title = {Multimedia Thumbnails for Documents},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180701},
doi = {10.1145/1180639.1180701},
abstract = {As small portable devices are becoming standard personal equipments, there is a great need for the adaptation of information content to small displays. Currently, no good solutions exist for viewing formatted documents, such as pdf documents, on these devices. Adapting content of web pages to small displays is usually achieved by complete redesign of a page or automatically reflowing text for small displays. Such techniques may not be applicable to documents whose format needs to be preserved. To address this problem, we propose a new document representation called Multimedia Thumbnail. Multimedia Thumbnail uses the visual and audio channels of small portable devices to communicate document information in form of a multimedia clip, which can be seen as a movie trailer for a document. Generation of such a clip includes a document analysis step, where salient document information is extracted, an optimization step, where the document information to be included in the thumbnail is determined based on display and time constraints, and a synthesis step, where visual and audible information are formed into a playable Multimedia Thumbnail. We also present user study results that evaluate an initial system design and point to further modification on analysis, optimization, and user interface components.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {231â€“240},
numpages = {10},
keywords = {multimodal processing, summarization, user interface, document representation, automated browsing, mobile},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180702,
author = {Liu, Feng and Gleicher, Michael},
title = {Video Retargeting: Automating Pan and Scan},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180702},
doi = {10.1145/1180639.1180702},
abstract = {When a video is displayed on a smaller display than originally intended, some of the information in the video is necessarily lost. In this paper, we introduce Video Retargeting that adapts video to better suit the target display, minimizing the important information lost. We define a framework that measures the preservation of the source material, and methods for estimating the important information in the video. Video retargeting crops each frame and scales it to fit the target display. An optimization process minimizes information loss by balancing the loss of detail due to scaling with the loss of content and composition due to cropping. The cropping window can be moved during a shot to introduce virtual pans and cuts, subject to constraints that ensure cinematic plausibility. We demonstrate results of adapting a variety of source videos to small display sizes.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {241â€“250},
numpages = {10},
keywords = {video editing, mobile multimedia, importance estimation, video retargeting},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180703,
author = {Wang, Chao and Yang, Qiong and Chen, Mo and Tang, Xiaoou and Ye, Zhongfu},
title = {Progressive Cut},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180703},
doi = {10.1145/1180639.1180703},
abstract = {Recently, interactive image cutout technique becomes prevalent for image segmentation problem due to its easy-to-use nature. However, most existing stroke-based interactive object cutout system did not consider the user intention inherent in the user interaction process. Strokes in sequential steps are treated as a collection rather than a process, and only the color information of the additional stroke is used to update the color model in the graph cut framework. Accordingly, unexpected fluctuation effect may occur during the process of interactive object cutout. In fact, each step of user interaction reflects the user's evaluation of previous result and his/her intention. By analyzing the user's intention behind the interaction, we propose a progressive cut algorithm, which explicitly models the user's intention into a graph cut framework for the object cutout task. Three aspects of user intention are utilized: 1) the color of the stroke indicates the kind of change s/he expects, 2) the location of the stroke indicates the region of interest, 3) the relative position between the stroke and the previous result indicates the segmentation error. By incorporating such information into the cutout system, the new algorithm removes the unexpected fluctuation effect of existing stroke-based graph-cut methods, and thus provides the user a more controllable result with fewer strokes and faster visual feedback. Experiments and user study show the strength of progressive cut in accuracy, speed, controllability, and user experience.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {251â€“260},
numpages = {10},
keywords = {interactive image segmentation, user attention, graph cuts, foreground extraction},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245534,
author = {James, A.},
title = {Session Details: Arts Session 1: Installations and Media Archaeology},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245534},
doi = {10.1145/3245534},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180705,
author = {Owsley, Sara H. and Hammond, Kristian J. and Shamma, David A. and Sood, Sanjay},
title = {Buzz: Telling Compelling Stories},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180705},
doi = {10.1145/1180639.1180705},
abstract = {This paper describes a digital theater installation called Buzz. Buzz consists of virtual actors who express the collective voice generated by weblogs (blogs). These actors find compelling stories from blogs and perform them. In this paper, we explore what it means for a story to be compelling and describe a set of techniques for retrieving compelling stories. We also outline an architecture for high level direction of a performance using Adaptive Retrieval Charts (ARCs), allowing a director-level of interaction with the performance system. Our overall goal in this work is to build a model of human behavior on a new foundation of query formation, information retrieval and filtering.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {261â€“268},
numpages = {8},
keywords = {story generation, emotion, blogs, media arts, world wide web, network arts, culture, software agents},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180706,
author = {Lombardo, Vincenzo and Valle, Andrea and Nunnari, Fabrizio and Giordana, Francesco and Arghinenti, Andrea},
title = {Archeology of Multimedia},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180706},
doi = {10.1145/1180639.1180706},
abstract = {The rapid evolution of technology and the processing aspects of some contemporary art forms make maintenance and re-fruition of a number of past masterpieces a difficult task. This is especially true of multimedia installations, with multiple audio and video sources coordinated through some control device, which also accounts for user interaction issues. So, it is not inappropriate to speak of "archeology of multimedia" in the case of the remise-en-oeuvre of an installation. This paper describes a complete example of archeology of multimedia. In particular, the art exhibit is the reprise of the first truly multimedia show of the electronic era: Le Corbusier's Po\^{e}me \'{e}lectronique, displayed at the World Exhibition in Brussels in 1958. The show, conceived for the Philips Pavilion and consisting of a black&amp;white video, color light ambiances, music moving over sound routes, visual special effects, has never been reprised after the end of the exhibition. The Po\^{e}me \'{e}lectronique has been entirely reconstructed after a philological investigation through image archives, project sketches and technical documentation from Philips and then delivered in two virtual reality settings. Moreover, two aspects of the project are worth being abstracted from the design and realization of this specific reconstruction and contribute to the design, pre-visualization and fruition of novel contemporary artworks: a language for the description of the control score and an architecture design for the integration of several media sources. The language and the architecture, with a sketch of a friendly interface, are illustrated at the end of the paper.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {269â€“278},
numpages = {10},
keywords = {digital media preservation and reconstruction, multimedia authoring, Po\^{e}me \'{e}lectronique},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180707,
author = {Gemeinboeck, Petra and Tanaka, Atau and Dong, Andy},
title = {Instant Archaeologies: Digital Lenses to Probe and to Perforate the Urban Fabric},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180707},
doi = {10.1145/1180639.1180707},
abstract = {The paper discusses the digital artwork series Impossible Geographies, works that weave dynamic cartographies of invisible, fragile and hybrid spaces. Impossible Geographies 01: Memory invokes the memory space of a gallery, producing cracks through which past events seep into the physical present. Net_D\'{e}rive negotiates the space between a gallery and its urban neighborhood, creating hybrid narratives along the grooves left by participants equipped with mobile phones. The playground of Impossible Geographies 02: Urban Fiction spreads across the city and beyond, mixing and shifting multiple urban geographies as participants sweep along the urban fabric with advanced mobile phones. The conceptual and technological methods of 'lens-making' and 'spacing,' developed in these artworks, explore the subjective, hybrid, and migrational nature of the geographies we belonging to. By re-sculpting the fluid nature of the urban fabric the works transform the everyday urbanscape into impossible imaginary geographies.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {279â€“286},
numpages = {8},
keywords = {locative media, urban fabric, negotiation, narrative, digital cartography, memory, mobile phone},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245535,
author = {Kankanhalli, M.},
title = {Session Details: Content Session 2: Machine Learning in Multimedia},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245535},
doi = {10.1145/3245535},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180709,
author = {Zhang, Hui and Rahmani, Rouhollah and Cholleti, Sharath R. and Goldman, Sally A.},
title = {Local Image Representations Using Pruned Salient Points with Applications to CBIR},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180709},
doi = {10.1145/1180639.1180709},
abstract = {Salient points are locations in an image where there is a significant variation with respect to a chosen image feature. Since the set of salient points in an image capture important local characteristics of that image, they can form the basis of a good image representation for content-based image retrieval (CBIR). The features for a salient point should represent the local characteristic of that point so that the similarity between features indicates the similarity between the salient points. Traditional uses of salient points for CBIR assign features to a salient point based on the image features of all pixels in a window around that point. However, since salient points are often on the boundary of objects, the features assigned to a salient point often involve pixels from different objects. In this paper, we propose a CBIR system that uses a novel salient point method that both reduces the number of salient points using a segmentation as a filter, and also improves the representation so that it is a more faithful representation of a single object (or portion of an object) that includes information about its surroundings. We also introduce an improved Expectation Maximization-Diverse Density (EM-DD) based multiple-instance learning algorithm. Experimental results show that our CBIR techniques improve retrieval performance by 5%-11% as compared with current methods.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {287â€“296},
numpages = {10},
keywords = {feature representation, image retrieval, salient points, content-based, interest points, multiple instance learning},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180710,
author = {Yu, Jie and Tian, Qi},
title = {Learning Image Manifolds by Semantic Subspace Projection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180710},
doi = {10.1145/1180639.1180710},
abstract = {In many image retrieval applications, the mapping between high-level semantic concept and low-level features is obtained through a learning process. Traditional approaches often assume that images with same semantic label share strong visual similarities and should be clustered together to facilitate modeling and classification. Our research indicates this assumption is inappropriate in many cases. Instead we model the images as lying on non-linear image subspaces embedded in the high-dimensional space and find that multiple subspaces may correspond to one semantic concept.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {297â€“306},
numpages = {10},
keywords = {relevance, image retrieval, semantic subspace projection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180711,
author = {Geng, Xin and Zhou, Zhi-Hua and Zhang, Yu and Li, Gang and Dai, Honghua},
title = {Learning from Facial Aging Patterns for Automatic Age Estimation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180711},
doi = {10.1145/1180639.1180711},
abstract = {Age Specific Human-Computer Interaction (ASHCI) has vast potential applications in daily life. However, automatic age estimation technique is still underdeveloped. One of the main reasons is that the aging effects on human faces present several unique characteristics which make age estimation a challenging task that requires non-standard classification approaches. According to the speciality of the facial aging effects, this paper proposes the AGES (AGing pattErn Subspace) method for automatic age estimation. The basic idea is to model the aging pattern, which is defined as a sequence of personal aging face images, by learning a representative subspace. The proper aging pattern for an unseen face image is then determined by the projection in the subspace that can best reconstruct the face image, while the position of the face image in that aging pattern will indicate its age. The AGES method has shown encouraging performance in the comparative experiments either as an age estimator or as an age range estimator.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {307â€“316},
numpages = {10},
keywords = {face image, aging pattern, automatic age estimation, age specific human-computer interaction},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180712,
author = {Panda, Navneet and Chang, Edward Y.},
title = {Efficient Top-k Hyperplane Query Processing for Multimedia Information Retrieval},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180712},
doi = {10.1145/1180639.1180712},
abstract = {A query can be answered by a binary classifier, which separates the instances that are relevant to the query from the ones that are not. When kernel methods are employed to train such a classifier, the class boundary is represented as a hyperplane in a projected space. Data instances that are farthest from the hyperplane are deemed to be most relevant to the query, and that are nearest to the hyperplane to be most uncertain to the query. In this paper, we address the twin problems of efficient retrieval of the approximate set of instances (a) farthest from and (b) nearest to a query hyperplane. Retrieval of instances for this hyperplane-based query scenario is mapped to the range-query problem allowing for the reuse of existing index structures. Empirical evaluation on large image datasets confirms the effectiveness of our approach.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {317â€“326},
numpages = {10},
keywords = {kernel based methods, retrieval, support vector machines},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245536,
author = {Chandra, S.},
title = {Session Details: Systems Session 1: Streaming},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245536},
doi = {10.1145/3245536},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180714,
author = {Qureshi, Asfandyar and Carlisle, Jennifer and Guttag, John},
title = {Tavarua: Video Streaming with WWAN Striping},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180714},
doi = {10.1145/1180639.1180714},
abstract = {Tavarua is a multimedia streaming system that leverages network-striping to deliver relatively high bit rate video over present-day cellular wireless wide-area networks. The Tavarua system achieves this by building on our previously developed flexible network-striping middleware. This paper describes a motivating mobile telemedicine application, and the design of the Tavarua system. It also describes experiments in which our initial Tavarua implementation was used to stripe video over multiple 3G cellular-phones from different providers.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {327â€“336},
numpages = {10},
keywords = {mobile systems, video streaming, telemedicine, wireless wide-area networks, network striping},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180715,
author = {Shi, Liqi and Sessini, Phillipa and Mahanti, Anirban and Li, Zongpeng and Eager, Derek L.},
title = {Scalable Streaming for Heterogeneous Clients},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180715},
doi = {10.1145/1180639.1180715},
abstract = {Periodic broadcast protocols enable the efficient streaming of highly popular media files to large numbers of concurrent clients. Most previous periodic broadcast protocols, however, assume that all clients can receive at the same rate, and also assume that available bandwidth is not time-varying. In this paper, we first develop a new periodic broadcast protocol, Optimized Heterogeneous Periodic Broadcast (OHPB), that can be optimized for a given population of clients with heterogeneous reception bandwidths and quality-of-service requirements. The OHPB protocol utilizes an optimized segment size progression determined by solving a linear optimization model that takes as input the client population characteristics and an objective function such as mean client startup delay. We then propose complementary client protocols employing work-ahead buffering of data during playback, so as to enable more uniform playback quality when the available bandwidth is time-varying.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {337â€“346},
numpages = {10},
keywords = {linear programming, periodic broadcasts, quality-of-service, scalable streaming},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180716,
author = {Qudah, Bashar and Sarhan, Nabil J.},
title = {Towards Scalable Delivery of Video Streams to Heterogeneous Receivers},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180716},
doi = {10.1145/1180639.1180716},
abstract = {The required real-time and high-rate transfers for multimedia data severely limit the number of requests that can be serviced concurrently by Video-on-Demand (VOD) servers. Resource sharing techniques can be used to address this problem. We study how VOD servers can support heterogeneous receivers while delivering data in a client-pull fashion using enhanced resource sharing. We propose three hybrid solutions. The first solution simply combines existing resource sharing techniques and deals with clients as two bandwidth classes. The other two solutions, however, classify clients into multiple bandwidth classes and service them accordingly by capturing the proposed ideas of Adaptive Stream Merging or Enhanced Adaptive Stream Merging, respectively. We also discuss how scheduling policies can be adapted to the heterogeneous environment so as to exploit the variations in client bandwidth. We evaluate the effectiveness of the proposed solutions and analyze various scheduling policies through extensive simulation.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {347â€“356},
numpages = {10},
keywords = {client heterogeneity, video streaming, video-on-demand (VOD), multimedia servers, stream merging},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180717,
author = {Gotz, David},
title = {Scalable and Adaptive Streaming for Non-Linear Media},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180717},
doi = {10.1145/1180639.1180717},
abstract = {Streaming of linear media objects, such as audio and video, has become ubiquitous on today's Internet. Large groups of users regularly tune in to a wide variety of online programming, including radio shows, sports events, and news coverage. However, non-linear media objects, such as large 3D computer graphics models and visualization databases, have proven more difficult to stream due to their interactive nature. This paper presents Channel Set Adaptation (CSA), a framework that allows for the efficient streaming of non-linear datasets to large user groups. CSA allows individual clients to request custom data flows for interactive applications using standard broadcast or multicast join and leave operations. CSA scales to support very large user groups while continuing to provide interactive data access to independently operating clients. We discuss a motivating sample application for digital museums and present results from an experimental evaluation of CSA's performance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {357â€“366},
numpages = {10},
keywords = {non-linear, multimedia, scalable, streaming},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245537,
author = {Gemmel, J.},
title = {Session Details: Applications Session 2: Searching Media I},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245537},
doi = {10.1145/3245537},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180719,
author = {Zhang, Lei and Chen, Le and Jing, Feng and Deng, Kefeng and Ma, Wei-Ying},
title = {EnjoyPhoto: A Vertical Image Search Engine for Enjoying High-Quality Photos},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180719},
doi = {10.1145/1180639.1180719},
abstract = {In this paper, we propose building a vertical image search engine called EnjoyPhoto that leverages rich metadata from various photo forum web sites to meet users' requirements for enjoying high-quality photos, which is virtually impossible in traditional image search engines. To solve the ranking problem when aggregating multiple photo forums, we propose a novel rank fusion algorithm that uses duplicate photos to normalize rating scores. To further improve user experiences in enjoying photos, we design an in-place image browsing interface, and compare it with several other interfaces in a user study. With rich metadata and rating information, more attractive user interfaces are enabled, including slideshow authoring and photo recommendations. We conducted experiments and user studies on a 2.5-million image database to evaluate the proposed rank fusion algorithm, investigate the rationale behind building a vertical image search engine, and study user interfaces and preferences for the purpose of enjoying high-quality photos. The experimental results demonstrate the effectiveness of the proposed ranking algorithm. The results also show that the 2.5-million high-quality image database in EnjoyPhoto performs comparably with Google's 1- billion image database for queries related to location, nature, and daily life categories. Finally, our results show that the in-place browsing interface-called Force-Transfer view-is much more convenient for users than traditional interfaces.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {367â€“376},
numpages = {10},
keywords = {image browsing interface, high-quality photo, user interface, user study, vertical search},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180720,
author = {Jing, Feng and Wang, Changhu and Yao, Yuhuan and Deng, Kefeng and Zhang, Lei and Ma, Wei-Ying},
title = {IGroup: Web Image Search Results Clustering},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180720},
doi = {10.1145/1180639.1180720},
abstract = {In this paper, we propose, IGroup, an efficient and effective algorithm that organizes Web image search results into clusters. IGroup is different from all existing Web image search results clustering algorithms that only cluster the top few images using visual or textual features. Our proposed algorithm first identifies several query-related semantic clusters based on a key phrases extraction algorithm originally proposed for clustering general Web search results. Then, all the resulting images are separated and assigned to corresponding clusters. As a result, all the resulting images are organized into a clustering structure with semantic level. To make the best use of the clustering results, a new user interface (UI) is proposed. Different from existing Web image search interfaces, which show only a limited number of suggested query terms or representative image thumbnails of some clusters, the proposed interface displays both representative thumbnails and appropriate titles of semantically coherent image clusters. Comprehensive user studies have been completed to evaluate both the clustering algorithm and the new UI.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {377â€“384},
numpages = {8},
keywords = {search result clustering, user interface design, web image search},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180721,
author = {Hauptmann, Alexander G. and Lin, Wei-Hao and Yan, Rong and Yang, Jun and Chen, Ming-Yu},
title = {Extreme Video Retrieval: Joint Maximization of Human and Computer Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180721},
doi = {10.1145/1180639.1180721},
abstract = {We present an efficient system for video search that maximizes the use of human bandwidth, while at the same time exploiting the machine's ability to learn in real-time from user selected relevant video clips. The system exploits the human capability for rapidly scanning imagery augmenting it with an active learning loop, which attempts to always present the most relevant material based on the current information. Two versions of the human interface were evaluated, one with variable page sizes and manual paging, the other with a fixed page size and automatic paging. Both require absolute attention and focus of the user for optimal performance. In either case, as users search and find relevant results, the system can invisibly re-rank its previous best guesses using a number of knowledge sources, such as image similarity, text similarity, and temporal proximity. Experimental evidence shows a significant improvement using the combined extremes of human and machine power over either approach alone.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {385â€“394},
numpages = {10},
keywords = {human performance optimization, relevance feedback, video retrieval, active learning},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245538,
author = {Bulterman, D.},
title = {Session Details: Applications Session 3: Entertainment &amp; Home Environments CWI},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245538},
doi = {10.1145/3245538},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180723,
author = {Chatting, David J. and Galpin, Josie S. and Donath, Judith S.},
title = {Presence and Portrayal: Video for Casual Home Dialogues},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180723},
doi = {10.1145/1180639.1180723},
abstract = {In this paper we present experimental results rating the experience of users conversing in a casual video/audio dialogue, in a simulated home environment. Here video-realistic images are problematic and prone to numerous "medium effects", such as unaligned eye-gaze, which can be misattributed as personal flaws. We tested three levels of manipulated video to see if they improved user's sense of: (a) presence, (b) portrayal and (c) preference. By blurring the background we found a manipulation that is both preferred and more efficiently coded than the original.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {395â€“401},
numpages = {7},
keywords = {presence, video conferencing, manipulated video, portrayal},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180724,
author = {Seo, Beomjoo and Zimmermann, Roger},
title = {Edge Indexing in a Grid for Highly Dynamic Virtual Environments},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180724},
doi = {10.1145/1180639.1180724},
abstract = {Newly emerging game--based application systems such as Second Life1 provide 3D virtual environments where multiple users interact with each other in real--time. They are filled with autonomous, mutable virtual content which is continuously augmented by the users. To make the systems highly scalable and dynamically extensible, they are usually built on a client--server based grid subspace division where the virtual worlds are partitioned into manageable sub--worlds. In each sub--world, the user continuously receives relevant geometry updates of moving objects from remotely connected servers and renders them according to her viewpoint, rather than retrieving them from a local storage medium. In such systems, the determination of the set of objects that are visible from a user's viewpoint is one of the primary factors that affect server throughput and scalability. Specifically, performing real--time visibility tests in extremely dynamic virtual environments is a very challenging task as millions of objects and sub-millions of active users are moving and interacting. We recognize that the described challenges are closely related to a spatial database problem, and hence we map the moving geometry objects in the virtual space to a set of multi-dimensional objects in a spatial database while modeling each avatar both as a spatial object and a moving query. Unfortunately, existing spatial indexing methods are unsuitable for this kind of new environments.The main goal of this paper is to present an efficient spatial index structure that minimizes unexpected object popping and supports highly scalable real--time visibility determination. We then uncover many useful properties of this structure and compare the index structure with various spatial indexing methods in terms of query quality, system throughput, and resource utilization. We expect our approach to lay the groundwork for next--generation virtual frameworks that may merge into existing web--based services in the near future.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {402â€“411},
numpages = {10},
keywords = {object popping problem, visibility model, 3D object streaming, spatial indexing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180725,
author = {Cui, Bin and Shen, Jialie and Cong, Gao and Shen, Heng Tao and Yu, Cui},
title = {Exploring Composite Acoustic Features for Efficient Music Similarity Query},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180725},
doi = {10.1145/1180639.1180725},
abstract = {Music similarity query based on acoustic content is becoming important with the ever-increasing growth of the music information from emerging applications such as digital libraries and WWW. However, relative techniques are still in their infancy and much less than satisfactory. In this paper, we present a novel index structure, called Composite Feature tree, CF-tree, to facilitate efficient content-based music search adopting multiple musical features. Before constructing the tree structure, we use PCA to transform the extracted features into a new space sorted by the importance of acoustic features. The CF-tree is a balanced multi-way tree structure where each level represents the data space at different dimensionalities. The PCA transformed data and reduced dimensions in the upper levels can alleviate suffering from dimensionality curse. To accurately mimic human perception, an extension, named CF+-tree, is proposed, which further applies multivariable regression to determine the weight of each individual feature. We conduct extensive experiments to evaluate the proposed structures against state-of-art techniques. The experimental results demonstrate superiority of our technique.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {412â€“420},
numpages = {9},
keywords = {music, KNN, CF-tree, similarity query},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245539,
author = {Smeaton, A.},
title = {Session Details: Content Session 3: Semantic Concepts},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245539},
doi = {10.1145/3245539},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180727,
author = {Snoek, Cees G. M. and Worring, Marcel and van Gemert, Jan C. and Geusebroek, Jan-Mark and Smeulders, Arnold W. M.},
title = {The Challenge Problem for Automated Detection of 101 Semantic Concepts in Multimedia},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180727},
doi = {10.1145/1180639.1180727},
abstract = {We introduce the challenge problem for generic video indexing to gain insight in intermediate steps that affect performance of multimedia analysis methods, while at the same time fostering repeatability of experiments. To arrive at a challenge problem, we provide a general scheme for the systematic examination of automated concept detection methods, by decomposing the generic video indexing problem into 2 unimodal analysis experiments, 2 multimodal analysis experiments, and 1 combined analysis experiment. For each experiment, we evaluate generic video indexing performance on 85 hours of international broadcast news data, from the TRECVID 2005/2006 benchmark, using a lexicon of 101 semantic concepts. By establishing a minimum performance on each experiment, the challenge problem allows for component-based optimization of the generic indexing issue, while simultaneously offering other researchers a reference for comparison during indexing methodology development. To stimulate further investigations in intermediate analysis steps that inuence video indexing performance, the challenge offers to the research community a manually annotated concept lexicon, pre-computed low-level multimedia features, trained classifier models, and five experiments together with baseline performance, which are all available at http://www.mediamill.nl/challenge/.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {421â€“430},
numpages = {10},
keywords = {video analysis, generic concept detection, baseline},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180728,
author = {Zhu, Guangyu and Xu, Changsheng and Huang, Qingming and Gao, Wen and Xing, Liyuan},
title = {Player Action Recognition in Broadcast Tennis Video with Applications to Semantic Analysis of Sports Game},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180728},
doi = {10.1145/1180639.1180728},
abstract = {Recognition of player actions in broadcast sports video is a challenging task due to low resolution of the players in video frames. In this paper, we present a novel method to recognize the basic player actions in broadcast tennis video. Different from the existing appearance-based approaches, our method is based on motion analysis and considers the relationship between the movements of different body parts and the regions in the image plane. A novel motion descriptor is proposed and supervised learning is employed to train the action classifier. We also propose a novel framework by combining the player action recognition with other multimodal features for semantic and tactic analysis of the broadcast tennis video. Incorporating action recognition into the framework not only improves the semantic indexing and retrieval performance of the video content, but also conducts highlights ranking and tactics analysis in tennis matches, which is the first solution to our knowledge for tennis game. The experimental results demonstrate that our player action recognition method outperforms existing appearance-based approaches and the multimodal framework is effective for broadcast tennis video analysis.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {431â€“440},
numpages = {10},
keywords = {semantic analysis, action recognition, object tracking, motion representation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180729,
author = {Yuan, Jinhui and Li, Jianmin and Zhang, Bo},
title = {Learning Concepts from Large Scale Imbalanced Data Sets Using Support Cluster Machines},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180729},
doi = {10.1145/1180639.1180729},
abstract = {This paper considers the problem of using Support Vector Machines (SVMs) to learn concepts from large scale imbalanced data sets. The objective of this paper is twofold. Firstly, we investigate the effects of large scale and imbalance on SVMs. We highlight the role of linear non-separability in this problem. Secondly, we develop a both practical and theoretical guaranteed meta-algorithm to handle the trouble of scale and imbalance. The approach is named Support Cluster Machines (SCMs). It incorporates the informative and the representative under-sampling mechanisms to speedup the training procedure. The SCMs differs from the previous similar ideas in two ways, (a) the theoretical foundation has been provided, and (b) the clustering is performed in the feature space rather than in the input space. The theoretical analysis not only provides justification, but also guides the technical choices of the proposed approach. Finally, experiments on both the synthetic and the TRECVID data are carried out. The results support the previous analysis and show that the SCMs are efficient and effective while dealing with large scale imbalanced data sets.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {441â€“450},
numpages = {10},
keywords = {clustering, concept modelling, imbalance, support vector machines, kernel k-means, large scale},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245540,
author = {Legrady, G.},
title = {Session Details: Arts Session 2: Interactive Spaces and Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245540},
doi = {10.1145/3245540},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180731,
author = {Webb, Andrew and Kerne, Andruid and Koh, Eunyee and Joshi, Pranesh and Park, YoungJoo and Graeber, Ross},
title = {Choreographic Buttons: Promoting Social Interaction through Human Movement and Clear Affordances},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180731},
doi = {10.1145/1180639.1180731},
abstract = {We used human movement as the basis for designing a collaborative aesthetic design environment. Our intention was to promote social interaction and creative expression. We employed off-the-shelf computer vision technology. Movement became the basis for the choreography of gestures, the development of gesture recognition, and the development of imagery and visualization. We discovered that the design of clear affordances is no less important in movement-based than in mouse-based systems. Through an integrated and iterative design process, we developed a new type of affordance, the choreographic button, which integrates choreography, gesture recognition, and visual feedback. Jumping, a quick movement, and crouching, a sustained gesture, were choreographed to form a vocabulary that is personally expressive, and which also facilitates automatic recognition.How can we evaluate socially motivated interactive systems? To create a context for evaluation, we held an integrated exhibition, party, and user study event. This mixing of events produced an engaging environment in which participants could choose to interact with each other, as well as with the design environment. We prepared a mouse-based version of the design environment, and compared how people experienced it with the movementbased system. Our study demonstrates that movement-based affordances promote social interaction.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {451â€“460},
numpages = {10},
keywords = {iterative design, Laban notation, mappings, choreographic buttons, gesture, movement, computer-supported cooperative play},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180732,
author = {Nguyen, Quoc and Novakowski, Scott and Boyd, Jeffrey E. and Jacob, Christian and Hushlak, Gerald},
title = {Motion Swarms: Video Interaction for Art in Complex Environments},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180732},
doi = {10.1145/1180639.1180732},
abstract = {We create interactive art that can be enjoyed by groups such as audiences at public events with the intent to encourage communication with those around us as we play with the art. Video systems are an attractive mechanism to provide interaction with artwork. However, public spaces are complex environments for video analysis systems. Interaction becomes even more difficult when the art is viewed by large groups of people. We describe a video system for interaction with art in public spaces and with large audiences using a model-free, appearance-based approach. Our system extracts parameters that describe the field of motion seen by a camera, and then imposes structure on the scene by introducing a swarm of particles that moves in reaction to the motion field. Constraints placed on the particle movement impose further structure on the motion field. The artistic display reacts to the particles in a manner that is interesting and predictable for participants. We demonstrate our video interaction system with a series of interactive art installations tested with the assistance of a volunteer audience.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {461â€“469},
numpages = {9},
keywords = {audience interaction, interaction through video, art installation, motion analysis, motion history image, swarm art},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180733,
author = {James, Jodi and Ingalls, Todd and Qian, Gang and Olsen, Loren and Whiteley, Daniel and Wong, Siew and Rikakis, Thanassis},
title = {Movement-Based Interactive Dance Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180733},
doi = {10.1145/1180639.1180733},
abstract = {Movement-based interactive dance has recently attracted great interest in the performing arts. While utilizing motion capture technology, the goal of this project was to design the necessary real-time motion analysis engine, staging, and communication systems for the completion of a movement-based interactive multimedia dance performance. The movement analysis engine measured the correlation of dance movement between three people wearing similar sets of retro-reflective markers in a motion capture volume. This analysis provided the framework for the creation of an interactive dance piece, Lucidity, which will be described in detail. Staging such a work also presented additional challenges. These challenges and our proposed solutions will be discussed. We conclude with a description of the final work and a summary of our future research objectives.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {470â€“480},
numpages = {11},
keywords = {interactive dance, 3D, real-time visual and audio feedback, animation, system design, live performance, context-aware computing, movement analysis, interaction design},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245541,
author = {Li, B.},
title = {Session Details: Demo Session 1},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245541},
doi = {10.1145/3245541},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180735,
author = {Chabane, Djeraba and Lew, Stanislas and Simovici, Dan and Mongy, Sylvain and Ihaddadene, Nacim},
title = {Eye/Gaze Tracking in Web, Image and Video Documents},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180735},
doi = {10.1145/1180639.1180735},
abstract = {Our demo focuses on eye tracking on web, image and video data. We use some state-of-the-art measurements, such as scan path, to determine how the user sees web documents, images and videos. Our approach is characterised by automatic eye/gaze tracking with non intrusive sensors, mainly infrared cameras of web, image and video documents. We analyse eye/gaze tracking concerns spatial regions of static documents (images and web pages) and spatial zones of dynamic documents (video, sequence of web pages hyperlinked). In the context of dynamic documents, the eye/gaze tracking is processed image-per -image in video documents, and page-per-page in web documents. The result is more rough on video, and more accurate on images and hyperlinked web pages. Eye/gaze tracking on video is relatively new and unexplored in the literature.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {481â€“482},
numpages = {2},
keywords = {heartbeat, eyepath, tracking, stress, documents, eye/gaze, respiration, videos, images, web},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180736,
author = {Holthe, Ole-Ivar and R\o{}nningen, Leif Arne},
title = {Geelix.Com: Sharing Gaming Experiences},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180736},
doi = {10.1145/1180639.1180736},
abstract = {Geelix is an experimental new approach to browsing, playing and sharing gaming experiences, extending the latest in Human Computer Interaction (HCI) Research, hardware-accelerated graphics and streaming video technologies. Geelix provides the best in high quality, cutting edge, streaming video and rich media playback experiences on the Web.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {483â€“484},
numpages = {2},
keywords = {effects, XML, web, shaders, video, games},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180737,
author = {Lee, Benjamin N. and Chen, WenYen and Chang, Edward Y.},
title = {Fotofiti: Web Service for Photo Management},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180737},
doi = {10.1145/1180639.1180737},
abstract = {In this work, we present Fotofiti(FF), a web-based personal photo organizer with automatic image annotation, event management and social network integration. We describe our technique for real-time online semantic annotation of user photos. Additionally, a landmark recognition system which utilizes local features is discussed.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {485â€“486},
numpages = {2},
keywords = {photo annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180738,
author = {Erol, Berna and Berkner, Kathrin and Joshi, Siddharth},
title = {Multimedia Thumbnails for Documents: Implementation and Demonstration},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180738},
doi = {10.1145/1180639.1180738},
abstract = {Browsing multi-page high-resolution documents on devices with small displays and limited navigational capabilities is very difficult. In this paper we demonstrate Multimedia Thumbnails, which are automatically computed guided-tours of documents. Multimedia Thumbnails utilize temporal dimension and the audio channel of the portable device in order to compensate for the limited resolution of the display area.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {487â€“488},
numpages = {2},
keywords = {user interface, multimodal processing, automated browsing, mobile, summarization, document representation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180739,
author = {Lejsek, Herwig and \'{A}smundsson, Fridrik H. and J\'{o}nsson, Bj\"{o}rn Th\'{o}r and Amsaleg, Laurent},
title = {Blazingly Fast Image Copyright Enforcement},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180739},
doi = {10.1145/1180639.1180739},
abstract = {Many photo agencies use the web to sell access to their image collections. Despite significant security measures, images may be stolen and distributed, making it necessary to detect copyright violations. This demonstration paper describes a content-based system for large-scale automatic copyright enforcement. The paper briefly describes the image description, indexing and retrieval algorithm that lie at the heart of the system. It also describes our proposed demonstration, which is a realistic scenario of copyright violations of a large image collection.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {489â€“490},
numpages = {2},
keywords = {NV-tree, copyright protection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180740,
author = {Dorai, Chitra and Farrell, Robert and Katriel, Amy and Kofman, Galina and Li, Ying and Park, Youngja},
title = {MAGICAL Demonstration: System for Automated Metadata Generation for Instructional Content},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180740},
doi = {10.1145/1180639.1180740},
abstract = {The "Tools for Automatic Generation of Learning Object Metadata" project addresses the requirement of developing advanced distributed learning delivery architecture and services for a large US government agency. We have developed a Webbased system called MAGIC (Metadata Automated Generation for Instructional Content) to assist content authors and course developers in generating metadata for learning objects and information assets to enable wider reuse of these objects across departments and organizations. Using the MAGIC system, content authors review and edit automatically-generated metadata sufficient to register and describe their assets for use and discovery in current and future distributed learning applications complying with the ADL SCORM standard. Course developers can use the system to assist in the conversion of existing courses to SCORM format or in developing new SCORM courses. The MAGIC system includes software tools to analyze and extract descriptive metadata from instructional videos, training documents, and other information assets. The tools generate some of the most critical SCORM metadata completely automatically. Benefits of MAGIC include easier reuse and repurposing, improved interoperability, and more timely registration of content for use by course developers. In this paper, we describe the system architecture, analysis tools developed, and services supported. A live demonstration of the system illustrating several use cases of the system will be presented at the conference, with a discussion of results from user studies and evaluation of the system.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {491â€“492},
numpages = {2},
keywords = {SCORM metadata, instructional content, analysis and indexing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180741,
author = {Heesch, Daniel and Yavlinsky, Alexei and R\"{u}ger, Stefan},
title = {NN<sup>k</sup> Networks and Automated Annotation for Browsing Large Image Collections from the World Wide Web},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180741},
doi = {10.1145/1180639.1180741},
abstract = {This paper outlines a system for searching and browsing 1.14 million images from the World Wide Web (WWW) based on their visual content. At the heart of the system lies an automatically constructed network of images that can be navigated quickly by following its edges. The browsing experience is enhanced in a number of ways including multidimensional scaling of the graph neighbourhood for display purposes, Markov clustering of the image network to provide summaries of its content, and automated annotation of the images to allow users to access the network through text queries.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {493â€“494},
numpages = {2},
keywords = {automated image annotation, NNk networks, Markov clustering, MDS},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180742,
author = {Ponceleon, Dulce and Nusser, Stefan and Zbarsky, Vladimir and Cerruti, Julian and Nin, Sigfredo},
title = {Enabling Secure Distribution of Digital Media to SD-Cards},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180742},
doi = {10.1145/1180639.1180742},
abstract = {As the marketplace for digital media increases we witness the rise of new media distribution models where timely delivery, convenience, privacy and personalization are essential features of competitive offerings. Consumers are looking for innovative ways to access content in a service-oriented manner that suits their mobile life style. This paper describes a prototype standard-based system that allows the secure and fast download of content to SD-Card-enabled consumer devices. Content is protected by Content Protection for Recordable Media (CPRM) technology; specifically we use CPRM SD-Video. Our prototype shows Digital Media Terminals (DMT, also denoted kiosks) that enable innovative rental and purchase models. Our solution combines IBM's plug-in web services, Panasonic's leading-edge devices and Porto Media's high-speed data transfer to Secure Digital Memory Flash Cards (SD-Cards).},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {495â€“496},
numpages = {2},
keywords = {content protection, secure media distribution, security, CPRM, SD-video, digital media, SDCard, download},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180743,
author = {Jing, Feng and Wang, Changhu and Yao, Yuhuan and Deng, Kefeng and Zhang, Lei and Ma, Wei-Ying},
title = {IGroup: A Web Image Search Engine with Semantic Clustering of Search Results},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180743},
doi = {10.1145/1180639.1180743},
abstract = {In this demo, we present IGroup, a Web image search engine that organizes the search results into semantic clusters. Different from all existing Web image search results clustering algorithms that only cluster the top few images using visual or textual features, IGroup first identifies several query-related semantic clusters based on a key phrases extraction algorithm originally proposed for clustering general Web search results. Then, all the resulting images are separated and assigned to corresponding clusters. To make the best use of the clustering results, a new user interface is proposed. Please go to http://igroup.msra.cn for real experience.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {497â€“498},
numpages = {2},
keywords = {user interface design, web image search, search result clustering},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180744,
author = {Westermann, Utz and Agaram, Srikanth and Gong, Bo and Jain, Ramesh},
title = {Event-Centric Multimedia Data Management for Reconnaissance Mission Analysis and Reporting},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180744},
doi = {10.1145/1180639.1180744},
abstract = {We demonstrate the concept of event-centric multimedia data management in the context of a multimedia eChronicle for the analysis, exploration, and reporting of events in military reconnaissance missions. Unlike the traditional media-centric approach, event-centricmultimedia data management focuses on the management of real-world events; documenting media are regarded as event metadata. For the detection of mission events, we apply simple but robust spatio-temporal clustering of basic soldier state and media events with good results considering the uncontrolled environment a military patrol constitutes. The core of the architecture is generic and applicable for the event-centric management of multimedia data in other domains as well.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {499â€“500},
numpages = {2},
keywords = {media management, life logs, reconnaissance mission analysis, events, eChronicles, spatio-temporal clustering},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180745,
author = {Chen, Yinpeng and Xu, Weiwei and Wallis, Richard Isaac and Sundaram, Hari and Rikakis, Thanassis and Ingalls, Todd and Olson, Loren and He, Jiping},
title = {A Real-Time, Multimodal Biofeedback System for Stroke Patient Rehabilitation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180745},
doi = {10.1145/1180639.1180745},
abstract = {This paper presents a novel real-time, multi-modal biofeedback system for stoke patient therapy. The problem is important as traditional mechanisms of rehabilitation are monotonous, and do not incorporate detailed quantitative assessment of recovery in addition to traditional clinical schemes. We have been working on developing an experiential media system that integrates task dependent physical therapy and cognitive stimuli within an interactive, multimodal environment. The environment provides a purposeful, engaging, visual and auditory scene in which patients can practice functional therapeutic reaching tasks, while receiving different types of simultaneous feedback indicating measures of both performance and results. There are two contributions of this paper - (a) identification of features and goals for the functional task, (b) the development of sophisticated feedback (auditory and visual) mechanisms that match the semantics of action of the task.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {501â€“502},
numpages = {2},
keywords = {action-feedback coupling, analysis, biofeedback},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180746,
author = {Yang, Jun and Hauptmann, Alexander G.},
title = {3WNews: Who, Where, and When in News Video},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180746},
doi = {10.1145/1180639.1180746},
abstract = {We describe 3WNews as a novel system for browsing news video by the people (who) and locations (where) appearing in the footage as well as the time (when) of news events. The people names, locations, and time expressions are recognized from video transcript and their ambiguous references are resolved. As a key advantage, 3WNews distinguishes the people and locations that actually appear in the video from those merely mentioned in the transcript, and uses them as (better) indexes for browsing. It also supports browsing of news video by event time instead of broadcasting time.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {503â€“504},
numpages = {2},
keywords = {people, browsing, news video, time, location},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180747,
author = {Christodoulou, Lakis and Mayron, Liam M. and Kalva, Hari and Marques, Oge and Furht, Borko},
title = {3D TV Using MPEG-2 and H.264 View Coding and Autostereoscopic Displays},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180747},
doi = {10.1145/1180639.1180747},
abstract = {There is a renewed interest in the 3DTV research primarily due to the advances in low cost 3D display technologies. The two views required for 3DTV can be compressed using standard video compression techniques. MPEG-2 is widely used in digital TV applications today and H.264/MPEG-4 AVC is expected to be the leading video technology standard for digital video in the near future. The compression gains and quality of 3DTV will vary depending on the video coding standard used. While inter-view prediction will likely improve the compression efficiency, new approaches such as asymmetric view coding are necessary to greatly reduce bandwidth requirements for 3DTV. This demonstration will show the quality of 3D video experience on autostereoscopic displays using H.264 and MPEG-2. We will also show the benefits of using asymmetric view coding as well as the role of eye dominance on 3DTV experience.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {505â€“506},
numpages = {2},
keywords = {asymmetric view coding, H.264, eye dominance, 3DTV},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180748,
author = {Liu, Leslie S. and Zimmermann, Roger and Xiao, Baoxuan and Christen, Jon},
title = {PartyPeer: A P2P Massively Multiplayer Online Game},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180748},
doi = {10.1145/1180639.1180748},
abstract = {Using peer-to-Peer (P2P) architectures for large scale interactive applications such as Massively Multiplayer Online Games (MMOG) is very challenging because of the difficulties to maintain a consistent game world in a distributed topology and exchange game state information in the P2P network without a central sever. In this demo proposal we present the innovative design and implementation of PartyPeer, an online social game which supports a massive number of users using our P2P based streaming network called ACTIVE+. We also discuss some of the implementation challenges when building this real-world P2P based game.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {507â€“508},
numpages = {2},
keywords = {peer-to-peer streaming, virtual reality, massively online games},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245542,
author = {Wyse, L.},
title = {Session Details: Arts Session 3: Tools for Creativity and Art Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245542},
doi = {10.1145/3245542},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180750,
author = {Manders, Corey and Mann, Steve},
title = {Handheld Electronic Camera Flash Lamp as a Tangible User-Interface for Creating Expressive Visual Art Works},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180750},
doi = {10.1145/1180639.1180750},
abstract = {We propose the use of a hand-held camera flash as a tangible user interface to a photographic "lightpainting" algorithm/ system. Our system includes a method of photographic multiple-exposure lighting using a single camera flash to obtain the results that would otherwise require complicated and more expensive professional multiple-light photographic studio setups. The system is simple in its approach, because it provides a direct metaphor-free interaction between the user, the camera, and the computer. The system mimics traditional multiple exposure photography by computationally combining images linearly to produce visual art works.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {509â€“518},
numpages = {10},
keywords = {cumulative image photography, multiple image photography, system integration, photoquantigraphic imaging, linear image manipulation, tangible user interface, photoquantimetric imaging, photographic lighting, embedded systems, photographic lighting techniques, lightspace imaging},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180751,
author = {Mann, Steve and Janzen, Ryan and Post, Mark},
title = {Hydraulophone Design Considerations: Absement, Displacement, and Velocity-Sensitive Music Keyboard in Which Each Key is a Water Jet},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180751},
doi = {10.1145/1180639.1180751},
abstract = {We present a musical keyboard that is not only velocity-sensitive, but in fact responds to absement (presement), displacement (placement), velocity, acceleration, jerk, jounce, etc. (i.e. to all the derivatives, as well as the integral, of displacement).Moreover, unlike a piano keyboard in which the keys reach a point of maximal displacement, our keys are essentially infinite in length, and thus never reach an end to their key travel. Our infinite length keys are achieved by using water jet streams that continue to flow past the fingers of a person playing the instrument. The instrument takes the form of a pipe with a row of holes, in which water flows out of each hole, while a user is invited to play the instrument by interfering with the flow of water coming out of the holes. The instrument resembles a large flute, but, unlike a flute, there is no complicated fingering pattern. Instead, each hole (each water jet) corresponds to one note (as with a piano or pipe organ). Therefore, unlike a flute, chords can be played by blocking more than one water jet hole at the same time. Because each note corresponds to only one hole, different fingers of the musician can be inserted into, onto, around, or near several of the instrument's many water jet holes, in a variety of different ways, resulting in an ability to independently control the way in which each note in a chord sounds.Thus the hydraulophone combines the intricate embouchure control of woodwind instruments with the polyphony of keyboard instruments.Various forms of our instrument include totally acoustic, totally electronic, as well as hybrid instruments that are acoustic but also include an interface to a multimedia computer to produce a mixture of sounds that are produced by the acoustic properties of water screeching through orific plates, as well as synthesized sounds.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {519â€“528},
numpages = {10},
keywords = {fluid-user-interface, underwater musical instrument, direct user interface, harmelotron (harmellotron), tangible user interface, hydraulic-action, haptic surface, pneumatophone, harmelodica, water-based immersive multimedia, FUNtain, duringtouch, tracker-action, hydraulophone},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180752,
author = {Yelizaveta, Marchenko and Tat-Seng, Chua and Ramesh, Jain},
title = {Semi-Supervised Annotation of Brushwork in Paintings Domain Using Serial Combinations of Multiple Experts},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180752},
doi = {10.1145/1180639.1180752},
abstract = {Many recent studies perform annotation of paintings based on brushwork. They model the brushwork indirectly as part of annotation of high-level artistic concepts such as artist name using low-level texture features and supervised inference methods. In this paper, we develop a framework for explicit annotation of paintings with brushwork classes. Brushwork classes serve as meta-level semantic concepts for artist names, paintings styles and periods of art and facilitate the incorporation of domain-specific ontologies. In particular, we employ the serial multi-expert framework with semi-supervised clustering methods to perform the annotation of brushwork patterns. Serial combination of multiple experts facilitates step-wise refinement of decisions based on the preferences of individual experts. Each individual expert performs focused subtasks using relevant feature set, which decreases the 'curse of dimensionality' and noise in the feature space. Each expert focuses on the annotation of the currently available samples from its unlabeled pool using semi-supervised agglomerative clustering. This approach is more appropriate as compared to the traditional classification methods since each brushwork class includes a variety of patterns and cannot be represented as a single distribution in the feature space. The experts exploit the distribution of unlabelled patterns and further minimize the annotation error. The multi-expert semi-supervised framework out-performs the conventional methods in annotation of patterns with brushwork classes. This framework will further be adopted to facilitate ontology-based annotation with higher-level semantic concepts such as the artist names, painting styles and periods of art.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {529â€“538},
numpages = {10},
keywords = {brushwork, multiple experts, painting annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245543,
author = {Gotz, D.},
title = {Session Details: Systems Session 2: Distributed Systems},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245543},
doi = {10.1145/3245543},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180754,
author = {Small, Tara and Liang, Ben and Li, Baochun},
title = {Scaling Laws and Tradeoffs in Peer-to-Peer Live Multimedia Streaming},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180754},
doi = {10.1145/1180639.1180754},
abstract = {It is well-known that live multimedia streaming applications operate more efficiently when organized in peer-to-peer (P2P) topologies, since peer upload capacities are utilized to support other peers, and to alleviate the load and operating costs on the streaming servers. To date, there have been a number of existing experimental proposals with respect to how such peer-to-peer topologies are organized to support live streaming sessions. However, most of the existing proposals resort to intuition and heuristics when it comes to the design of such topology construction (i.e., neighbor selection) protocols. In this paper, we investigate the scaling laws of live P2P multimedia streaming, by quantitatively studying the asymptotic effects and tradeoffs among three key parameters in P2P streaming: server bandwidth cost, the maximum number of peers that can be supported, and the maximum number of streaming hops experienced by a peer. To further generalize our studies, we do not make restrictive assumptions in our theoretical analysis of such scaling laws: both peer upload capacities and peer lifetimes in a session may come from arbitrary distributions. With the theoretical insights we have developed, we propose Affinity, a simple and realistic heuristic to demonstrate the key benefits of our theoretical analysis in dynamic P2P networks, as compared to the topology construction algorithms in existing work.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {539â€“548},
numpages = {10},
keywords = {multimedia streaming, scaling laws, resource-performance tradeoff, peer-to-peer},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180755,
author = {Kwon, Gisik and Candan, K. Sel\c{c}uk},
title = {DANS: Decentralized, Autonomous, and Networkwide Service Delivery and Multimedia Workflow Processing},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180755},
doi = {10.1145/1180639.1180755},
abstract = {Fundamental challenges in designing environments with media-rich ambient services involves not only the development of appropriate sensing technologies, but as importantly, the implementation of a distributed media processing system which can process, integrate, and leverage the sensed data in real time to provide the various services. In recent years, a great deal of progress has been made in media service workflow processing systems. In most existing solutions, however, the workflow nodes, which operate on the data, are selected out of a centrally assigned candidate pool. These candidate organizations cause either extensive resource provisioning or poor-quality operator mapping between logical workflow nodes and the available physical resources nodes. Consequently, instantiating a media processing workflow to the underlying hardware before the workflow execution begins does not lends itself to adaptive and autonomous operation of the workflow, scalable to resources and demand.In this paper, we propose a novel decentralized multimedia workflow processing system, DANS, in which operators defined in workflows are mapped into (distributed) physical nodes through Distributed Hash Table (DHT)-based overlay substrate in a purely decentralized and adaptive manner. The redundancy in the system, in terms of availability of multiple nodes able to perform the same task, enables the system to scale with demand. Furthermore, physical workflow nodes (operator instances) are able to locate and select the next filter or fusion operator instance autonomously, while ensuring the correct execution of the workflow.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {549â€“558},
numpages = {10},
keywords = {stream workflow processing, media processing workflows, peer-to-peer computing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180756,
author = {Gu, Xiaohui and Wen, Zhen and Lin, ChingYung and Yu, Philip S.},
title = {ViCo: An Adaptive Distributed Video Correlation System},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180756},
doi = {10.1145/1180639.1180756},
abstract = {Many emerging applications such as video sensor monitoring can benefit from an on-line video correlation system, which can be used to discover linkages between different video streams in realtime. However, on-line video correlations are often resource-intensive where a single host can be easily overloaded. We present a novel adaptive distributed on-line video correlation system called ViCo. Unlike single stream processing, correlations between different video streams require a distributed execution system to observe a new correlation constraint that any two correlated data must be distributed to the same host. ViCo achieves three unique features: (1) correlation-awareness that ViCo can guarantee the correlation accuracy while spreading excessive workload on multiple hosts; (2) adaptability that the system can adjust algorithm behaviors and switch between different algorithms to adapt to dynamic stream environments; and (3) fine-granularity that the workload of one resource-intensive correlation request can be divided and distributed among multiple hosts. We have implemented and deployed a prototype of ViCo on a commercial cluster system. Our experiment results using both real videos and synthetic workloads show that ViCo outperforms existing techniques for scaling-up the performance of video correlations.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {559â€“568},
numpages = {10},
keywords = {adaptive stream processing, video correlation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245544,
author = {Tian, Q.},
title = {Session Details: Applications Session 4: Searching Media II},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245544},
doi = {10.1145/3245544},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180758,
author = {Shen, Jialie and Shepherd, John},
title = {Efficient Benchmarking of Content-Based Image Retrieval via Resampling},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180758},
doi = {10.1145/1180639.1180758},
abstract = {While content-based image retrieval (CBIR) is an expanding field, and new approaches to ever more effective retrieval are frequently proposed, relatively little attention has so far been paid to the process of evaluating the effectiveness of CBIR methods. Most of the reported evaluations use standard IR evaluation methodologies, with little consideration of their statistical significance or appropriateness for CBIR, which makes it difficult to assess the precise impact of individual methods. In this paper, we present a new approach for evaluating CBIR systems which provides both efficient and statistically-sound performance evaluation. The approach is based on stratified sampling, and provides a significant improvement over existing evaluation approaches. Comprehensive experiments using our approach to evaluate a range of CBIR methods have shown that the approach reduces not only the estimation error, but also reduces the size of the test data set required to achieve specific estimation error levels.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {569â€“578},
numpages = {10},
keywords = {evaluation, image retrieval, sampling},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180759,
author = {Greenhill, Stewart and Venkatesh, Svetha},
title = {Virtual Observers in a Mobile Surveillance System},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180759},
doi = {10.1145/1180639.1180759},
abstract = {Conventional wide-area video surveillance systems use a network of fixed cameras positioned close to locations of interest. We describe an alternative and flexible approach to wide area surveillance based on observation streams collected from mobile cameras mounted on buses. We allow a "virtual observer" to be placed anywhere within the space covered by the sensor network, and reconstruct the scene at these arbitrary points. Use of such imagery is challenging because mobile cameras have variable position and orientation, and sample a large spatial area but at low temporal resolution. Additionally, the views of any particular place are distributed across many different video streams. Addressing this problem, we present a system in which views from an arbitrary perspective can be constructed by indexing, organising, and transforming images collected from multiple streams acquired from a network of mobile cameras. Our system supports retrieval of raw images based on constraints of space, time, and geometry (eg. visibility of landmarks). It also allows the synthesis of wide-angle panoramic views in situations where the camera motion produces suitable sampling of the scene and metaphors for query and presentation that overcome the complexity of the data.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {579â€“588},
numpages = {10},
keywords = {panorama, virtual observer, spatial query, video indexing, observation systems, visibility query, mobile surveillance, scene reconstruction},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180760,
author = {Lejsek, Herwig and \'{A}smundsson, Fridrik H. and J\'{o}nsson, Bj\"{o}rn Th\'{o}r and Amsaleg, Laurent},
title = {Scalability of Local Image Descriptors: A Comparative Study},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180760},
doi = {10.1145/1180639.1180760},
abstract = {Computer vision researchers have recently proposed several local descriptor schemes. Due to lack of database support, however, these descriptors have only been evaluated using small image collections. Recently, we have developed the PvS-framework, which allows efficient querying of large local descriptor collections. In this paper, we use the PvSframework to study the scalability of local image descriptors. We propose a new local descriptor scheme and compare it to three other well known schemes. Using a collection of almost thirty thousand images, we show that the new scheme gives the best results in almost all cases. We then give two stop rules to reduce query processing time and show that in many cases only a few query descriptors must be processed to find matching images. Finally, we test our descriptors on a collection of over three hundred thousand images, resulting in over 200 million local descriptors, and show that even at such a large scale the results are still of high quality, with no change in query processing time.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {589â€“598},
numpages = {10},
keywords = {median rank aggregation, scalability, PvS-framework, high-dimensional indexing, local image descriptors},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245545,
author = {Tseng, B.},
title = {Session Details: Short Papers Poster Session 2},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245545},
doi = {10.1145/3245545},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180762,
author = {Jing, Feng and Zhang, Lei and Ma, Wei-Ying},
title = {VirtualTour: An Online Travel Assistant Based on High Quality Images},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180762},
doi = {10.1145/1180639.1180762},
abstract = {With the popularity of both travel and Web, more and more people use online travel services to facilitate their travel activities or share their travel experiences. Considering that existing services emphasize more on the textual content with the pictorial content only as supplement, we propose the VirtualTour system. It is an online travel service dedicated on high quality images, which helps travelers plan their trip. The images of VirtualTour are from photo forum sites. They have rich and accurate metadata which could be used to extract geographic location information of them and assess the quality of them. A representative sights identification algorithm is also proposed to automatically identify the possible related sights of a region. Based on the map services that seamlessly integrated into the system, a wieldy UI is designed to support several useful features, e.g. query by map, location or path.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {599â€“602},
numpages = {4},
keywords = {representative sights identification, user interface design, travel assistant, high quality image search},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180763,
author = {Peng, Yuxin and Ngo, Chong-Wah and Fang, Cuihua and Chen, Xiaoou and Xiao, Jianguo},
title = {Audio Similarity Measure by Graph Modeling and Matching},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180763},
doi = {10.1145/1180639.1180763},
abstract = {This paper proposes a new approach for the similarity measure and ranking of audio clips by graph modeling and matching. Instead of using frame-based or salient-based features to measure the acoustical similarity of audio clips, segment-based similarity is proposed. The novelty of our approach lies in two aspects: segment-based representation, and the similarity measure and ranking based on four kinds of similarity factors. In segmentbased representation, segments not only capture the change property of audio clip, but also keep and present the change relation and temporal order of audio features. In the similarity measure and ranking, four kinds of similarity factors: acoustical, granularity, temporal order and interference are progressively and jointly measured by optimal matching and dynamic programming, which guarantee the comprehensive and sufficient similarity measure between two audio clips. The experimental result shows that the proposed approach is better than some existing methods in terms of retrieval and ranking capabilities.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {603â€“606},
numpages = {4},
keywords = {audio retrieval, audio similarity measure},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180764,
author = {Li, Xirong and Chen, Le and Zhang, Lei and Lin, Fuzong and Ma, Wei-Ying},
title = {Image Annotation by Large-Scale Content-Based Image Retrieval},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180764},
doi = {10.1145/1180639.1180764},
abstract = {Image annotation has been an active research topic in recent years due to its potentially large impact on both image understanding and Web image search. In this paper, we target at solving the automatic image annotation problem in a novel search and mining framework. Given an uncaptioned image, first in the search stage, we perform content-based image retrieval (CBIR) facilitated by high-dimensional indexing to find a set of visually similar images from a large-scale image database. The database consists of images crawled from the World Wide Web with rich annotations, e.g. titles and surrounding text. Then in the mining stage, a search result clustering technique is utilized to find most representative keywords from the annotations of the retrieved image subset. These keywords, after salience ranking, are finally used to annotate the uncaptioned image. Based on search technologies, this framework does not impose an explicit training stage, but efficiently leverages large-scale and well-annotated images, and is potentially capable of dealing with unlimited vocabulary. Based on 2.4 million real Web images, comprehensive evaluation of image annotation on Corel and U. Washington image databases show the effectiveness and efficiency of the proposed approach.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {607â€“610},
numpages = {4},
keywords = {similarity search, result clustering, automatic image annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180765,
author = {Lee, Jeannie S. A. and Jayant, Nikil},
title = {Mixed-Initiative Multimedia for Mobile Devices: A Voting-Based User Interface for News Videos},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180765},
doi = {10.1145/1180639.1180765},
abstract = {A system is being created with the aim of providing news video to mobile devices, focusing on using an "interactive TV-channel" metaphor as one of the possible interaction modes. The user is presented a sequence of recommended news video clips and a set of operations to vote-up or vote-down a video to indicate its relevance. User-machine synergy is harnessed through interaction with the clips, used to tailor subsequent media clip recommendations. A framework for delivering relevant news clips to a mobile device is being developed, and future work and challenges are discussed in further detail. The goal is to create a methodology for presenting a sequence of news video clips in a manner that is seamless and cognitively palatable through usermachine synergy, and switching between automated and manual modes as deemed necessary.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {611â€“614},
numpages = {4},
keywords = {mobile, user interaction, user interface, multimedia, mixed-initiative},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180766,
author = {Ames, Morgan G. and Manguy, Lilia},
title = {PhotoArcs: Ludic Tools for Sharing Photographs},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180766},
doi = {10.1145/1180639.1180766},
abstract = {PhotoArcs is a framework for flexible creation of timeline displays of photographs, photo-narratives, and consistent, collaboratively-created metadata. The interface aims to enable easy and fun manipulation and sharing of digital photographs and stories, organized around chronological "arcs" of photographs, to encourage remote sharing and interaction. We describe the design of the interface and how the design has changed across three iterations. We also report on the results of an exploratory lowfidelity usability study with five participants and an expert evaluation and critique with eleven user experience researchers, and outline future directions for the PhotoArcs project.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {615â€“618},
numpages = {4},
keywords = {photo-narratives, photo-sharing, social uses of photography, photography, timeline interfaces, remote collaboration},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180767,
author = {Yu, Xinguo and Yan, Xin and Phuong Chi, Tran Thi and Cheong, Loong Fah},
title = {Inserting 3D Projected Virtual Content into Broadcast Tennis Video},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180767},
doi = {10.1145/1180639.1180767},
abstract = {The ability to acquire the accurate camera matrix of each frame of a video clip is essential if virtual content is inserted into the images in a believable way. This paper presents our system for inserting projected virtual content into broadcast tennis video based on camera matrix acquired. To achieve the accurate camera matrix, we develop a new algorithm for the 3D camera calibration of broadcast tennis video, which improves the accuracy of camera matrices via the proposed techniques of clip-wise data analysis and Hough-like search. We divide all the camera parameters determining a camera matrix into two categories: clip-varying and frame-varying. For the clip-varying ones, we use a clip-wise data analysis procedure to achieve their accuracy. For the framevarying ones, we use a Hough-like search to tune them for each frame. Preliminary experiments results show that we can seamlessly insert projected virtual content into each frame.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {619â€“622},
numpages = {4},
keywords = {camera calibration, virtual content insertion, sports video, hough-like search, data analysis},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180768,
author = {Yuan, Xun and Hua, Xian-Sheng and Wang, Meng and Wu, Xiu-Qing},
title = {Manifold-Ranking Based Video Concept Detection on Large Database and Feature Pool},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180768},
doi = {10.1145/1180639.1180768},
abstract = {In this paper we discuss a typical case in video concept detection: to learn target concept using only a small number of positive samples. A novel manifold-ranking based scheme is proposed, which consists of three major components: feature pool construction, pre-filtering, and manifold-ranking. First, as there are large variations in the effective features for different concepts, a large feature pool is constructed, from which the most effective features can be selected automatically or semi-automatically. Second, to tackle the issue of large computation cost for successive manifold-ranking process when large video database is incorporated, we employ a pre-filtering process to filter out the majority of irrelevant samples while retaining the most relevant ones. And last, the manifold-ranking algorithm is used to explore the relationship among all of the rest samples based on the selected features. This scheme is extensible and flexible in terms of adding new features into the feature pool, introducing human interactions on selecting features, and defining new concepts.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {623â€“626},
numpages = {4},
keywords = {pre-filtering, manifold-ranking, video concept detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180769,
author = {Lai, Wei and Hua, Xian-Sheng and Ma, Wei-Ying},
title = {Towards Content-Based Relevance Ranking for Video Search},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180769},
doi = {10.1145/1180639.1180769},
abstract = {Most existing web video search engines index videos by file names, URLs, and surrounding texts. These types of video metadata roughly describe the whole video in an abstract level without taking the rich content, such as semantic content descriptions and speech within the video, into consideration. Therefore the relevance ranking of the video search results is not satisfactory as the details of video contents are ignored. In this paper we propose a novel relevance ranking approach for Web-based video search using both video metadata and the rich content contained in the videos. To leverage real content into ranking, the videos are segmented into shots, which are smaller and more semantic-meaningful retrievable units, and then more detailed information of video content such as semantic descriptions and speech of each shots are used to improve the retrieval and ranking performance. With video metadata and content information of shots, we developed an integrated ranking approach, which achieves improved ranking performance. We also introduce machine learning into the ranking system, and compare them with IR-model (information retrieval model) based method. The evaluation results demonstrate the effectiveness of the proposed ranking methods.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {627â€“630},
numpages = {4},
keywords = {video search, content-based ranking, relevance ranking},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180770,
author = {Ionescu, Bogdan and Lambert, Patrick and Coquin, Didier and Ott, Laurent and Buzuloiu, Vasile},
title = {Animation Movies Trailer Computation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180770},
doi = {10.1145/1180639.1180770},
abstract = {This paper presents a method for the automatic generation of animation movie trailers. First, the movie is divided into shots, by detecting the video transitions (cuts, fades and dissolves) and an animation movie specific color effect named short color change or SCC (i.e. explosions, thunders). The movie action content is further highlighted by analyzing the movie at two different granularity levels. First, an inter-shot analysis is performed by measuring the video transition temporal distribution. As repetitive shot changes are related to action, we define as an action shot, a movie segment containing a high shot change ratio. On the other hand, an inter-frame analysis is performed: for each shot within an action shot, an histogram of cumulative inter-frame distances is computed to serve as a measure of the frame spatial activity. Repetitive color changes are also related to action. Since movie trailers only show some of the most attractive movie scenes, the proposed trailer is a moving-image abstract computed on the retained action shots. It provides the user with a compact and efficient representation of the movie action content. The proposed approach was tested on several animation movies.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {631â€“634},
numpages = {4},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180771,
author = {Kim, Junil and Jeong, Yeonjeong and Yoon, Kisong and Ryou, Jaecheol},
title = {A Trustworthy End-to-End Key Management Scheme for Digital Rights Management},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180771},
doi = {10.1145/1180639.1180771},
abstract = {Current studies on Digital Rights Management (DRM) have focused on security and encryption as a means of solving the issue of illegal copying by purchasers. In this paper, we propose an end-to-end key management scheme that can cover a content protection on the overall value-chains of content distribution. The proposed scheme can protect digital content from attacks since an encrypted content is sent by a first package server and only DRM client can decrypt the encrypted digital content. It makes it possible to protect content from content creator to purchaser.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {635â€“638},
numpages = {4},
keywords = {key management, DRM, content protection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180772,
author = {Li, Yun and Xu, Chunjing and Liu, Jianzhuang and Tang, Xiaoou},
title = {Detecting Irregularity in Videos Using Kernel Estimation and KD Trees},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180772},
doi = {10.1145/1180639.1180772},
abstract = {Automatic event understanding is the ultimate goal for many visual surveillance systems. In this paper, we propose a novel approach for on-line detecting unusual human activities in videos without the need to explicitly define all valid configurations. Within the framework of Bayesian inference, the detection process is formulated as an MAP estimation where we attempt to find whether activities in new video segments have similar activities in a video database. Our approach has three contributions: firstly, we build the statistical representation of normal behaviors in the database using nonparametric kernel density estimation; secondly, local feature descriptors are highly compressed using PCA and stored in a K-D tree structure, making the search for behavior-based similarity fast and effective; thirdly, the K-D trees are used to generate multiple hypotheses which compete for the optimal classification. The approach requires no tracking, no explicit motion estimation, and no predefined class-based templates. Experimental results have validated our approach in many real-world video sequences.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {639â€“642},
numpages = {4},
keywords = {irregularity detection, MAP estimation, K-D trees, kernels},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180773,
author = {Ni, Pengpeng and Isovic, Damir and Fohler, Gerhard},
title = {User-Friendly H.264/AVC for Remote Browsing},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180773},
doi = {10.1145/1180639.1180773},
abstract = {With the growing popularity of variable network technologies, it is highly desirable to enable effective and quick browsing of remote multimedia content. In this paper we present a method for quick access of remote video content as an initial step towards a full digital Video Cassette Recording functionality in multimedia streaming applications such as Video-On-Demand, video broadcasting and remote video editing.We propose a transcoding scheme for H.264/AVC video that fully utilizes the benefits of recently proposed SP- and SI-frames to facilitate user-friendly remote stream browsing and editing. The transcoding parameters can be adaptively changed and optimized to support different characteristics of H.264 video streams.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {643â€“646},
numpages = {4},
keywords = {transcoding, H.264/AVC, digital VCR functionality, video encoding, remote video browsing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180774,
author = {Wang, Changhu and Jing, Feng and Zhang, Lei and Zhang, Hong-Jiang},
title = {Image Annotation Refinement Using Random Walk with Restarts},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180774},
doi = {10.1145/1180639.1180774},
abstract = {Image annotation plays an important role in image retrieval and management. However, the results of the state-of-the-art image annotation methods are often unsatisfactory. Therefore, it is necessary to refine the imprecise annotations obtained by existing annotation methods. In this paper, a novel approach to automatically refine the original annotations of images is proposed. On the one hand, for Web images, textual information, e.g. file name and surrounding text, is used to retrieve a set of candidate annotations. On the other hand, for non-Web images that are lack of textual information, a relevance model-based algorithm using visual information is used to decide the candidate annotations. Then, candidate annotations are re-ranked and only the top ones are reserved as the final annotations. To re-rank the annotations, an algorithm using Random Walk with Restarts (RWR) is proposed to leverage both the corpus information and the original confidence information of the annotations. Experimental results on both non-Web images of Corel dataset and Web images of photo forum sites demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {647â€“650},
numpages = {4},
keywords = {image annotation refinement, random walk with restarts},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180775,
author = {Bulterman, Dick C. A. and Cesar, Pablo and Jansen, A. J.},
title = {An Architecture for Viewer-Side Enrichment of TV Content},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180775},
doi = {10.1145/1180639.1180775},
abstract = {This paper presents a user interface model and implementation for exploiting next-generation interactive capabilities with the domain of television content. Our work studies capabilities that extend a user's potential impact over the consumption and sharing of television programs. The main capabilities of our environment include personalized viewing and navigation within a program fragment, and the ability to actively personalize content via various end-user content enrichments (such as line art, referrals and hyperlink insertions). In this paper, we present the implementation of a range of "couch-top" control and editing devices, including personal devices such as personal digital assistants and ad-hoc interactive devices. This paper also presents an architecture that decouples user actions into activators and handlers. We provide an overview of the interaction architecture and report on a series of deployment experiments on a wide range of consumer electronics devices.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {651â€“654},
numpages = {4},
keywords = {personal digital recorder, content enrichment, SMIL},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180776,
author = {Kim, Kihwan and Essa, Irfan and Abowd, Gregory D.},
title = {Interactive Mosaic Generation for Video Navigation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180776},
doi = {10.1145/1180639.1180776},
abstract = {Navigation through large multimedia collections that include videos and images still remains cumbersome. In this paper, we introduce a novel method to visualize and navigate through the collection by creating a mosaic image that visually represents the compilation. This image is generated by a labeling-based layout algorithm using various sizes of sample tile images from the collection. Each tile represents both the photographs and video files representing scenes selected by matching algorithms. This generated mosaic image provides a new way for thematic video and visually summarizes the videos. Users can generate these mosaics with some predefined themes and layouts, or base it on the results of their queries. Our approach supports automatic generation of these layouts by using meta-information such as color, time-line and existence of faces or manually generated annotated information from existing systems (e.g., the Family Video Archive).},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {655â€“658},
numpages = {4},
keywords = {video annotation, mosaics, video navigation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180777,
author = {Iskandar, Denny and Wang, Ye and Kan, Min-Yen and Li, Haizhou},
title = {Syllabic Level Automatic Synchronization of Music Signals and Text Lyrics},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180777},
doi = {10.1145/1180639.1180777},
abstract = {We present a framework to synchronize pop music to corresponding text lyric. We refine line level alignment achievable by existing work to syllabic level by using a dynamic programming process. Our main contribution is using music knowledge to constrain the dynamic programming search. This is done by modeling (1) non-uniform note length distribution and (2) a note length distribution for each section type (for example intro, chorus, and bridge). These reduce alignment error by 6.4% and improve time efficiency by a factor of 2.2.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {659â€“662},
numpages = {4},
keywords = {music structure, hidden Markov model, dynamic programming, voice alignment},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180778,
author = {Bertini, Marco and Del Bimbo, Alberto and Nunziati, Walter},
title = {Automatic Detection of Player's Identity in Soccer Videos Using Faces and Text Cues},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180778},
doi = {10.1145/1180639.1180778},
abstract = {In soccer videos, most significant actions are usually followed by close--up shots of players that take part in the action itself. Automatically annotating the identity of the players present in these shots would be considerably valuable for indexing and retrieval applications. Due to high variations in pose and illumination across shots however, current face recognition methods are not suitable for this task. We show how the inherent multiple media structure of soccer videos can be exploited to understand the players' identity without relying on direct face recognition. The proposed method is based on a combination of interest point detector to "read" textual cues that allow to label a player with its name, such as the number depicted on its jersey, or the superimposed text caption showing its name. Players not identified by this process are then assigned to one of the labeled faces by means of a face similarity measure, again based on the appearance of local salient patches. We present results obtained from soccer videos taken from various recent games between national teams.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {663â€“666},
numpages = {4},
keywords = {content-based retrieval, video annotation, video databases},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180779,
author = {Seok, Jinwuk and Lee, Jeong-Woo and Cho, Chang-Sik},
title = {The Differential Structure of Sub Pixels Interpolated from Integer Pixels Using N-Tab FIR Filters for High Definition H.264 Video Encoding},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180779},
doi = {10.1145/1180639.1180779},
abstract = {In this paper, we propose a differential structure of subpixels which are interpolated from integer pixels using ntab linear FIR filters. The generation and management of sub-pixels are a very serious problem in high-definition H.264 video encoding. Since the amount of sub-pixels in H.264 encoding is about 16 times larger than that of integer pixels, H.264 encoder needs very high speed memory access for realtime encoding of high-definition video. In order to avoid the sub-pixel problem, we estimate the distortion value for a motion estimation of sub-pixel resolution by using the nonlinear combination of the distortion value derived from a motion estimation of related integer pixel, based on the proposed differential structure. Simulation results show that the proposed mathematical properties are valid by the sub-pixel motion estimation based on the proposed mathematical analysis. It also shows that the proposed sub-pixel motion estimation is so effective that the encoding speed is dramatically increased without a considerable depression of video quality.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {667â€“670},
numpages = {4},
keywords = {fast encoding, sub-pixel, H.264, differential structure},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180780,
author = {Schmitz, Patrick},
title = {Leveraging Community Annotations for Image Adaptation to Small Presentation Formats},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180780},
doi = {10.1145/1180639.1180780},
abstract = {In this paper, we describe an investigation into the use of community annotations of photos for creating zoom and pan animations that present a time linear view of the regions of interest. We describe the use of the notes feature of the Flickr photo sharing service, and describe the FlickrBrns system that leverages the metadata in Flickr to generate animation specifications for image adaptation and provides a web viewer and authoring controls for more precise specifications.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {671â€“674},
numpages = {4},
keywords = {image adaptation, Flickr, FlickrBrns, community annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180781,
author = {H\"{u}rst, Wolfgang},
title = {Interactive Audio-Visual Video Browsing},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180781},
doi = {10.1145/1180639.1180781},
abstract = {We present the AV-ZoomSlider interface for video browsing. It complements existing approaches, such as storyboards and video skims by enabling users to interactively navigate along the time line of a video file. Our solution smoothly integrates position- and speed-based navigation concepts and provides synchronized audio-visual feedback during scrolling when applicable.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {675â€“678},
numpages = {4},
keywords = {video browsing, interactivity, multimedia user interfaces},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180782,
author = {Bertini, Marco and Del Bimbo, Alberto and Torniai, Carlo},
title = {Automatic Annotation and Semantic Retrieval of Video Sequences Using Multimedia Ontologies},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180782},
doi = {10.1145/1180639.1180782},
abstract = {Effective usage of multimedia digital libraries has to deal with the problem of building efficient content annotation and retrieval tools. MOM (Multimedia Ontology Manager) is a complete system that allows the creation of multimedia ontologies, supports automatic annotation and creation of extended text (and audio) commentaries of video sequences, and permits complex queries by reasoning on the ontology.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {679â€“682},
numpages = {4},
keywords = {video databases, content-based retrieval, video annotation, RDF, OWL, multimedia ontology},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180783,
author = {Liu, Junfa and Chen, Yiqiang and Gao, Wen},
title = {Mapping Learning in Eigenspace for Harmonious Caricature Generation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180783},
doi = {10.1145/1180639.1180783},
abstract = {This paper proposes a mapping learning approach for caricature auto-generation. Simulating the artist's creativity based on the object's facial feature, our approach targets discovering what are the principal components of the facial features, and what's the difference between facial photograph and caricature measured by those components. In training phase, PCA approach is adopted to obtain the principal components. Then, machine learning of SVR (Support Vector Regression) is carried out to learn the mapping model in principal component space. With the mapping model, in application phase, users just need to input a frontal facial photograph for the caricature generation. The caricature is exaggerated based on the original face while reserving essential similar features. Experiments proved comparatively that our approach could generate more harmonious caricatures.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {683â€“686},
numpages = {4},
keywords = {PCA, caricature, machine learning, subspace},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180784,
author = {Nixdorf, JJ and Gerhard, David},
title = {RITZ: A RealTime Interactive Tool for Spatialization},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180784},
doi = {10.1145/1180639.1180784},
abstract = {In this paper we discuss the use of sound spatialization in western classical music and attempt to explain a potential reason why it is not found in current popular music. We argue that the barriers limiting the use of musical sound spatialization are nearly gone and attempt to facilitate the return by providing a computer interface for musical spatialization. The interface is described in detail and is briefly compared to the few currently available interfaces. The paper focuses on real-time sound spatialization and includes a discussion of real-time spatialization versus spatial choreography.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {687â€“690},
numpages = {4},
keywords = {realtime systems, musical performance interface systems, spatial music, sound spatialization, surround sound},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180785,
author = {Boukerche, Azzedine and Pazzi, Richard Werner Nelem},
title = {Remote Rendering and Streaming of Progressive Panoramas for Mobile Devices},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180785},
doi = {10.1145/1180639.1180785},
abstract = {Providing mobile devices with virtual environment walkthrough and real-time streaming movie playback capability is expected to have a profound impact to the entertainment-based applications, such as virtual guides, online gaming, and e-learning, just to name a few. However, it is well known that it is extremely difficult to render complex 3D scenes at interactive frame rates on thin mobile devices known for their lack of proper resources needed to process large volume of 3D virtual environment data. In order to provide virtual environment navigation on thin mobile clients, we propose a hybrid technique which combines both remote geometry rendering and streaming of warped images. In our approach, the server renders a partial panoramic view, which is based on the user's viewpoint and last movements. The server then warps the image's coordinates into cylindrical coordinates, and streams the images to the client device, which will progressively build the panoramic representation of the scene. Furthermore, in order to enhance streaming performance and quality of the interaction, we propose to use a rate control mechanism as well as a prediction of the user's movements within the virtual scene. In this paper we discuss our scheme for remote rendering and streaming of progressive panoramas for mobile devices, and present our experimental results we have obtained in order to validate our proposed technique. Our results indicate clearly that the proposed solution is able to achieve stable frame rates and throughput in error-prone wireless channels.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {691â€“694},
numpages = {4},
keywords = {image-based rendering, wireless multimedia streaming, remote virtual walkthrough},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180786,
author = {van Gemert, Jan C. and Snoek, Cees G. M. and Veenman, Cor J. and Smeulders, Arnold W. M.},
title = {The Influence of Cross-Validation on Video Classification Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180786},
doi = {10.1145/1180639.1180786},
abstract = {Digital video is sequential in nature. When video data is used in a semantic concept classification task, the episodes are usually summarized with shots. The shots are annotated as containing, or not containing, a certain concept resulting in a labeled dataset. These labeled shots can subsequently be used by supervised learning methods (classifiers) where they are trained to predict the absence or presence of the concept in unseen shots and episodes. The performance of such automatic classification systems is usually estimated with cross-validation. By taking random samples from the dataset for training and testing as such, part of the shots from an episode are in the training set and another part from the same episode is in the test set. Accordingly, data dependence between training and test set is introduced, resulting in too optimistic performance estimates. In this paper, we experimentally show this bias, and propose how this bias can be prevented using episode-constrained crossvalidation. Moreover, we show that a 17% higher classifier performance can be achieved by using episode constrained cross-validation for classifier parameter tuning.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {695â€“698},
numpages = {4},
keywords = {cross-validation, multimedia performance evaluation, parameter tuning, semantic concept detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180787,
author = {Lee, Benjamin N. and Chen, Wen-Yen and Chang, Edward Y.},
title = {A Scalable Service for Photo Annotation, Sharing, and Search},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180787},
doi = {10.1145/1180639.1180787},
abstract = {In this work we present the details of the implementation of Fotofiti(FF), a website that provides automatic semantic annotation of digital photographs, event management and social network integration. We describe our technique for real-time online semantic annotation using global features from both content and context. Classification experiments using various learning techniques were performed on a realworld data-set. Additionally, a scalable landmark recognition system which utilizes local features is discussed.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {699â€“702},
numpages = {4},
keywords = {photo annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180788,
author = {Shah-hosseini, Amin and Knapp, Gerald M.},
title = {Semantic Image Retrieval Based on Probabilistic Latent Semantic Analysis},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180788},
doi = {10.1145/1180639.1180788},
abstract = {Content-based image retrieval (CBIR) systems combine computer vision techniques and learning methodologies to find images in the database similar to the query images. Relevance feedback methods are introduced to the CBIR area as a tool to help the user to guide the retrieval system during the search process. Search history of the retrieval system, which is the accumulated feedbacks from past retrievals, has been recently used as a prior knowledge to improve the image retrieval performance. In this paper, we introduce an image retrieval model based on probabilistic latent semantic analysis (PLSA) that utilizes the system's search history to find hidden image semantics of the database. Image features are integrated to the model as well. The model is capable of detecting images and image features that efficiently represent semantic classes in the database. We demonstrate the effectiveness of our approach by comparing to previous work in this area.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {703â€“706},
numpages = {4},
keywords = {probabilistic latent semantic analysis, relevance feedback, CBIR},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180789,
author = {Song, Kai and Tian, Yonghong and Gao, Wen and Huang, Tiejun},
title = {Diversifying the Image Retrieval Results},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180789},
doi = {10.1145/1180639.1180789},
abstract = {In the area of image retrieval, post-retrieval processing is often used to refine the retrieval results to better satisfy users' requirements. Previous methods mainly focus on presenting users with relevant results. However, in most cases, users cannot clearly present their requirements by several query words. Therefore, relevant results with rich topic coverage are more likely to meet users' ambiguous needs. In this paper, a re-ranking method based on topic richness analysis is proposed to enrich topic coverage in retrieval results. Furthermore, a quantitative criterion called diversity scores (DS) is proposed to evaluate the improvement. Given a set of images, topics that are rarely included in the set are scarce topics, as oppose to rich topics that are widely distributed among the set. Scarce topics contribute more than rich topics do to the DS of images. Five researchers are invited to evaluate the re-ranked results both in topic coverage and relevance. Experimental results on over 20,000 images demonstrate that our proposed approach is effective in improving the topic coverage of retrieval results without loss of relevance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {707â€“710},
numpages = {4},
keywords = {diversity score, re-rank, topic richness, image retrieval, topic coverage},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180790,
author = {Adams, Brett and Greenhill, Stewart and Venkatesh, Svetha},
title = {Browsing Personal Media Archives with Spatial Context Using Panoramas},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180790},
doi = {10.1145/1180639.1180790},
abstract = {This paper presents novel techniques for using panoramas as spatial context to enhance browsing of personal media archives. This context, scenes where frequent media capture takes place, is present in the disparate photos and videos, but not leveraged by traditional browsing techniques (e.g. thumbnails or zoomable interfaces). Coarse geo-position is often an insufficient index at such media capture hotspots. We experiment with panoramic video, which presents archive video organically blended with panoramas of media capture hotspots; Immersive browsing and filtering with media items projected onto spherical panoramas; and Detection and representation of links between panoramas to enable browsing of situated media in quasi-3D. We present proof-of-concept implementations and observations of their effectiveness, limitations, and open problems. Experiments confirm the intuition that each holds promise for augmenting traditional browsing environments.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {711â€“714},
numpages = {4},
keywords = {spatial context, multimedia browsing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180791,
author = {Noor, Humera and Mirza, Shahid H. and Sheikh, Yaser and Jain, Amit and Shah, Mubarak},
title = {Model Generation for Video-Based Object Recognition},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180791},
doi = {10.1145/1180639.1180791},
abstract = {This paper presents a novel approach to object recognition involving a sparse 2D model and matching using video. The model is generated on the basis of geometry and image measurables only. We first identify the underlying topological structure of an image dataset containing different views of the objects and represent it as a neighborhood graph. The graph is then refined by identifying redundant images and removing them using morphing. This gives a smaller dataset leading to reduced space requirements and faster matching. Finally we exploit motion continuity in video and extend our algorithm to perform matching based on video input and demonstrate that the results obtained using a video sequence are much robust than using a single image. Our approach is novel in that we do not require any knowledge of camera calibration or viewpoint while generating the model. We also do not assume any constraint on motion of object in test video other than following a smooth trajectory.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {715â€“718},
numpages = {4},
keywords = {morphing, video-based object recognition},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180792,
author = {Kahol, Kanav and Krishnan, Narayanan C. and Balasubramanian, Vineeth N. and Panchanathan, Sethuraman and Smith, Marshall and Ferrara, John},
title = {Measuring Movement Expertise in Surgical Tasks},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180792},
doi = {10.1145/1180639.1180792},
abstract = {Surgical movement is composed of discrete gestures that are combined to perform complex surgical procedures. A promising approach to objective surgical skill evaluation systems is kinematics and kinetic analysis of hand movement that yields a gesture level analysis of proficiency of a performed movement. In this paper, we propose a novel system that combines surgical gesture segmentation, surgical gesture recognition, and expertise analysis of surgical profiles in minimally invasive surgery (MIS). Kinematic analysis was used to segment gestures from a continuous motion stream. Human anatomy driven Hidden Markov Models (HMMs) are adopted for gesture recognition and expertise identification. When the proposed system was tested on a library of 200 samples for every basic surgical gesture, the gesture recognition module reported a perfect accuracy rate for the basic gestures, while the expertise identification module showed 94.7% accuracy.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {719â€“722},
numpages = {4},
keywords = {surgical motion, surgical skill evaluation, gesture recognition},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180793,
author = {Yang, Zhenyu and Yu, Bin and Wu, Wanmin and Diankov, Ross and Bajscy, Ruzena},
title = {Collaborative Dancing in Tele-Immersive Environment},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180793},
doi = {10.1145/1180639.1180793},
abstract = {We present a study of collaborative dancing between remote dancers in a tele-immersive environment which features 3D full and real body capturing, wide field of view, multi-display 3D rendering, and attachment free participant. We invite two professional dancers to perform collaborative dancing in the environment. The coordination requires one dancer to take the lead while the other follows by appropriate movement. Throughout the experiment, the dancers are dancing at various motion rates to evaluate how well the collaborative dancing is supported with the current technical boundary. Our important findings indicate that 1) tele-immersive environments have strong potential impact on the concept of choreography and communication of live dance performance, 2) the presence of multi-view display, real body 3D rendering, audio channel, and less intrusiveness greatly enhances the immersive and dancing experience, and 3) the level of synchronization achieved by the dancers is higher than that expected from the video rate.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {723â€“726},
numpages = {4},
keywords = {3D tele-immersive environment, dance, collaboration},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180794,
author = {Knoche, Hendrik O. and McCarthy, John D. and Sasse, M. Angela},
title = {Reading the Fine Print: The Effect of Text Legibility on Perceived Video Quality in Mobile Tv},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180794},
doi = {10.1145/1180639.1180794},
abstract = {Mobile TV services are available in an increasing number of countries. For cost reasons, most of these services offer material directly recoded for mobile consumption (i.e. without additional editing). This paper reports the findings of a study on the influence of text legibility and quality on the perceived video quality of mobile TV content. The study, with 64 participants, examined responses to news footage presented at four image resolutions and seven video encoding bitrates. The results showed that a simulated separate delivery of a news ticker and other textual information significantly increased the perceived video quality of the entire screen for native speakers. In addition, some automatable changes to the layout of news content resulted in substantial increases in perceived video quality. The results can be used to quantify the perceived quality gains when considering text delivery separately from the video stream and in the development of more accurate multimedia quality models.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {727â€“730},
numpages = {4},
keywords = {mobile TV, text quality, video quality assessment},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245546,
author = {Peljhan, M.},
title = {Session Details: Arts Short Poster Session 2},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245546},
doi = {10.1145/3245546},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180796,
author = {Willis, Karl D. D.},
title = {User Authorship and Creativity within Interactivity},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180796},
doi = {10.1145/1180639.1180796},
abstract = {This paper tracks the development of the author's work entitled Light Tracer, and examines the surrounding issues of user authorship and creativity within interactivity.Light Tracer is an interactive system which invites the participant to write, draw and trace images in real physical space. The participant is situated in front of a screen reflecting their own image, and by manipulating a series of light sources, marks can be left onscreen such as drawings, messages, traces of physical objects such as faces, hands and bodies.It is the argument of the author that by allowing the user an optimum level of creative authorship within an interactive work, the user can be successfully engaged with the experience of the interaction and in turn produce and create themselves.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {731â€“735},
numpages = {5},
keywords = {light, authorship, tracing, drawing, writing, interactivity, computer vision, user, creativity},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180797,
author = {Knouf, Nicholas},
title = {Variations 10b: A Digital Realization of Cage's Variations II},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180797},
doi = {10.1145/1180639.1180797},
abstract = {Beginning in the middle of the twentieth century, composers of experimental music developed a number of new notational representations, most often falling under the category of graphical scores. John Cage's Variations II is a prime example, utilizing only dots and lines as its basis. I describe an interactive version of Cage's piece, called here Variations 10b, where a performer can change the score and receive immediate auditory feedback as to the results of the manipulation. This stands in contrast to the process of working through the analog score, where the aural output was not coincident with movement of the dots or lines. I suggest that creating and using digital versions of these early experimental music works radically changes the process of interacting with the pieces.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {736â€“739},
numpages = {4},
keywords = {graphical scores, installation, John Cage, experimental music, performance},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180798,
author = {Fujimura, Noriyuki and Fujiyoshi, Satoshi and Hope, Tom and Nishimura, Takuichi},
title = {Tabletop Community: Artwork for Visualization of Social Interactions Using a Bipartite Network},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180798},
doi = {10.1145/1180639.1180798},
abstract = {"Tabletop Community" is an artwork that records interactions among users around a table. The artwork also visualizes a social network of users from data that show these interactions; the piece can present that network to other users as well. Its theme is self-recognition, which emerges from social interactions with others in a human network. It is intended to offer experiences of looking back at a user's past interactions with others within the perspective of a group (social network) in a visual manner. We have shown the artwork at Ubicomp2005. The paper shows the types of interactions, inferred from collected data and comments from the audience, which have delivered some improvements in the artwork since its original version.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {740â€“743},
numpages = {4},
keywords = {social interaction, social network, information visualization},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180799,
author = {Lewis, Jason and Assogba, Yannick},
title = {Taking Sides: Dynamic Text and Hip-Hop Performance},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180799},
doi = {10.1145/1180639.1180799},
abstract = {In this paper we describe Taking Sides, a performance using a real-time speech visualization software system called TextEngine. Taking Sides is a collaboration between our research studio and Montreal hip-hop artist Dwayne Hanley. Our primary goal was to create a strong conceptual link between the text visualization, the content of the artist's lyrics, and his performance style. Additionally we wanted to test the flexibility of TextEngine in developing customized performance applications. Pursuing these goals led us through a three month development effort that cycled tightly between design, performance and programmatic iterations.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {744â€“747},
numpages = {4},
keywords = {rap, hip-hop, speech visualisation, real-time media},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180800,
author = {Dyaberi, Vidyarani and Sundaram, Hari and Rikakis, Thanassis and James, Jodi},
title = {The Computational Extraction of Temporal Formal Structures in the Interactive Dance Work '22'},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180800},
doi = {10.1145/1180639.1180800},
abstract = {In this paper we propose a framework for the computational extraction of time characteristics of a single choreographic work. Computational frameworks can aid in revealing non-salient compositional structures in modern dance. The computational extraction of such features allows for the creation of interactive works where the movement and the digital feedback (graphics, sound etc) are integrally connected at deep level of structures. It also facilitates a better understanding of the choreographic process. There are two key contributions in this paper: (a) a systematic analysis of the observable and non-salient aspects of solo dance form, (b) computational analysis of temporal phrasing structures guided by critical understanding of observable form. Our analysis results are excellent indicating the presence of rich, latent temporal organization in specific semi-improvisatory modern dance works that may provide rich structural material for interactivity.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {748â€“751},
numpages = {4},
keywords = {structure, phrase, dance},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245547,
author = {Boll, S. and Chang, E. and Davis, M. and Schmitz, P.},
title = {Session Details: Multimedia and Web 2.0 - Hype, Challenge, Synergy},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245547},
doi = {10.1145/3245547},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245548,
author = {Pieper, J.},
title = {Session Details: Applications Session 5: Multimedia Applications Potpourri},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245548},
doi = {10.1145/3245548},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180803,
author = {Kamijo, Koichi and Kamijo, Noboru and Sakamoto, Masaharu},
title = {Electronic Clipping System with Invisible Barcodes},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180803},
doi = {10.1145/1180639.1180803},
abstract = {The Internet, digital television, and similar technologies have accelerated the speed of digitalization. Nevertheless, paper media forms, such as newspapers and magazines, still have large market shares, and the authors believe that these two worlds, digital and analog, will coexist even in the future, with their own market areas. However, the demands to connect between these two worlds are increasing, and barcodes and watermarking are examples of technologies used to connect them. However, a barcode occupies space and disrupts the layout of an article, and watermarking has limited capacity for embedding data.In this paper, we propose an "electronic clipping system with invisible barcodes", which connects the analog and digital worlds, addressing these problems. In this system, we overprint invisible 2D barcodes on printed articles using invisible ink, then take pictures of them using a camera equipped with a special LED (Light Emitting Diode)that the invisible ink responds to, decode the encoded data of the 2D barcode after the image is processed, and extract the information. We developed a code extraction system which can extract the information from invisible 2D barcodes even if the barcodes are overprinted on the articles in media such as newspapers, and even if the article is in color. We also made a prototype cell phone which includes both the barcode extraction code and the LED, and confirmed that we can correctly extract the information from the pictures taken by the phone.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {753â€“762},
numpages = {10},
keywords = {2D bar code, invisible ink, uv LED, QR code, cell phone, ubiquitous},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180804,
author = {Chen, Yinpeng and Huang, He and Xu, Weiwei and Wallis, Richard Isaac and Sundaram, Hari and Rikakis, Thanassis and Ingalls, Todd and Olson, Loren and He, Jiping},
title = {The Design of a Real-Time, Multimodal Biofeedback System for Stroke Patient Rehabilitation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180804},
doi = {10.1145/1180639.1180804},
abstract = {This paper presents a novel real-time, multi-modal biofeedback system for stroke patient therapy. The problem is important as traditional mechanisms of rehabilitation are monotonous, and do not incorporate detailed quantitative assessment of recovery in addition to traditional clinical schemes. We have been working on developing an experiential media system that integrates task dependent physical therapy and cognitive stimuli within an interactive, multimodal environment. The environment provides a purposeful, engaging, visual and auditory scene in which patients can practice functional therapeutic reaching tasks, while receiving different types of simultaneous feedback indicating measures of both performance and results. There are three contributions of this paper - (a) identification of features and goals for the functional task (b) The development of sophisticated feedback (auditory and visual) mechanisms that match the semantics of action of the task. We additionally develop novel action-feedback coupling mechanisms. (c) New metrics to validate the ability of the system to promote learnability, stylization and engagement. We have validated the system for nine subjects with excellent results.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {763â€“772},
numpages = {10},
keywords = {analysis, validation, action-feedback coupling, biofeedback},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180805,
author = {Han, Mei and Xu, Wei and Gong, Yihong},
title = {Video Object Segmentation by Motion-Based Sequential Feature Clustering},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180805},
doi = {10.1145/1180639.1180805},
abstract = {Segmentation of video foreground objects from background has many important applications, such as human computer interaction, video compression, multimedia content editing and manipulation. Most existing methods work on image pixels or color segments which are computationally expensive. Some methods require extensive manual inputs, static cameras, and/or rigid scenes. In this paper we propose a fully automatic foreground segmentation method based on sequential clustering of sparse image features. The sparseness makes the method computationally efficient. We use both edge and corner points extracted from each video frame. A joint spatio-temporal linear regression method is developed to compute sparse motion layers of M consecutive frames jointly under the temporal consistency constraint. Once the sparse motion layers have been identified for each frame, the corresponding dense motion layers are created using the Markov Random Field (MRF) model. The MRF model assigns the rest of the image pixels to the motion layers by considering both the color attributes and the spatial relations between each pixel and its surrounding edge/corner points. Experimental evaluations on videos taken by webcams show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {773â€“782},
numpages = {10},
keywords = {feature clustering, linear regression, markov random field, object segmentation, feature extraction},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245549,
author = {Rangaswami, R.},
title = {Session Details: Demo Session 2},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245549},
doi = {10.1145/3245549},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180807,
author = {Luo, Hangzai and Fan, Jianping and Gao, Yuli and Ribarsky, William and Satoh, Shin'ichi},
title = {Large-Scale News Video Retrieval via Visualization},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180807},
doi = {10.1145/1180639.1180807},
abstract = {As the content of everyday news reports is unpredictable, keyword based news search engine can't provide effective services to audiences because the audiences may not be able to figure out proper keywords to search. In this paper, a novel framework is proposed to help audiences browse and retrieve news video clips without the need of keywords. Interesting keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness and informativeness measurement. A computational approach is also developed to quantify the interestingness measurement of video clips. The keyframes and keywords are carefully organized so that the audiences can find news stories of interest at first glance.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {783â€“784},
numpages = {2},
keywords = {semantic video classification, video visualization},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180808,
author = {Worring, Marcel and Snoek, Cees G. M. and Huurnink, Bouke and van Gemert, Jan C. and Koelma, Dennis C. and de Rooij, Ork},
title = {The Mediamill Large.Lexicon Concept Suggestion Engine},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180808},
doi = {10.1145/1180639.1180808},
abstract = {In this technical demonstration we show the current version of the MediaMill system, a search engine that facilitates access to news video archives at a semantic level. The core of the system is a lexicon of 436 automatically detected semantic concepts. To handle such a large lexicon in retrieval, an engine is developed which automatically selects a set of relevant concepts based on the textual query and example images. The result set can be browsed easily to obtain the final result for the query.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {785â€“786},
numpages = {2},
keywords = {video retrieval, information visualization, semantic indexing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180809,
author = {Bertini, Marco and Del Bimbo, Alberto and Torniai, Carlo and Cucchiara, Rita and Grana, Costantino},
title = {MOM: Multimedia Ontology Manager. A Framework for Automatic Annotation and Semantic Retrieval of Video Sequences},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180809},
doi = {10.1145/1180639.1180809},
abstract = {Effective usage of multimedia digital libraries has to deal with the problem of building efficient content annotation and retrieval tools. MOM (Multimedia Ontology Manager) is a complete system that allows the creation of multimedia ontologies, supports automatic annotation and creation of extended text (and audio) commentaries of video sequences, and permits complex queries by reasoning on the ontology.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {787â€“788},
numpages = {2},
keywords = {video annotation, video databases, content-based retrieval, multimedia ontology},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180810,
author = {Sano, Masanori and Kawai, Yoshihiko and Sumiyoshi, Hideki and Yagi, Nobuyuki},
title = {Metadata Production Framework and Metadata Editor},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180810},
doi = {10.1145/1180639.1180810},
abstract = {This paper proposes the Metadata Production Framework (MPF) as a common platform for generating content-based metadata. A lot of research on extracting useful information from audiovisual content has been conducted. Recently it has been necessary to integrate these research studies to get much higher semantic metadata. The aim of MPF is to provide an environment where we can easily make these types of processes, including multimodal integration. We have also released a Metadata Editor where a user can generate content-based metadata both manually and by using automatic metadata extraction modules based on MPF.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {789â€“790},
numpages = {2},
keywords = {semantic metadata, MPEG-7, framework, metadata editor, multimodal integration, metadata production, plug-in module},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180811,
author = {Liu, Qiong and McEvoy, Paul and Lai, Cheng-Jia},
title = {Mobile Camera Supported Document Redirection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180811},
doi = {10.1145/1180639.1180811},
abstract = {In this demonstration, we are going to illustrate how to use a mobile camera to redirect documents to various devices connected to the same network.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {791â€“792},
numpages = {2},
keywords = {device control, mobile camera supported document redirection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180812,
author = {Grana, C. and Vezzani, R. and Bulgarelli, D. and Gualdi, G. and Cucchiara, R. and Bertini, M. and Torniai, C. and Del Bimbo, A.},
title = {PEANO: Pictorial Enriched Annotation of Video},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180812},
doi = {10.1145/1180639.1180812},
abstract = {In this DEMO, we present a tool set for video digital library management that allows i) structural annotation of edited videos in MPEG-7 by automatically extracting shots and clips; ii) automatic semantic annotation based on perceptual similarity against a taxonomy enriched with pictorial concepts iii) video clip access and hierarchical summarization with stand-alone and web interface iv) access to clips from mobile platform in GPRS-UMTS video-streaming. The tools can be applied in different domain-specific Video Digital Libraries. The main novelty is the possibility to enrich the annotation with pictorial concepts that are added to a textual taxonomy in order to make the automatic annotation process more fast and often effective. The resulting multimedia ontology is described in the MPEG-7 framework. The PEANO (Perceptual Annotation of Video) tool has been tested over video art , sport (Soccer, Olimpic Games 2006, Formula 1) and news clips.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {793â€“794},
numpages = {2},
keywords = {video annotation, sports video, MPEG-7},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180813,
author = {Graeber, Ross and Kerne, Andruid and Henderson, M. Kathryn},
title = {ZooMICSS: A Zoomable Map Image Collection Sensemaking System (the Katrina Rita Context)},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180813},
doi = {10.1145/1180639.1180813},
abstract = {Access to devices that integrate Global Positioning data with image and sound acquisition becomes more common, enabling people to build large collections of locative multimedia. As the size and number of these locative media collections grow, so too does the importance of systems that support collection sensemaking. Media semantics, which include automatically acquired location data, as well as user-supplied annotations, play a key role in these user-centered processes of collection utilization. This demo presents a Zoomable Map Image Collection Sensemaking System that enables the collection, organization, browsing, and annotation of locative images. The Zoomable Map Perspective is supplemented by event-based clustering. Dynamic views are generated automatically from captured media. The system is currently being used to document the location and condition of homes and neighborhoods in the aftermath of Hurricane Katrina.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {795â€“796},
numpages = {2},
keywords = {metadata, digital collections, locative media},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180814,
author = {Schmitz, Patrick and Shafton, Peter and Shaw, Ryan and Tripodi, Samantha and Williams, Brian and Yang, Jeannie},
title = {International Remix: Video Editing for the Web},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180814},
doi = {10.1145/1180639.1180814},
abstract = {The long-awaited arrival of video as a major medium on the web is here. But in contrast to the textual web, which enables users to author documents with a minimum of technical knowledge, the video web is still read-only for most users. Developed for the 2006 San Francisco International Film Festival, International Remix is a platform for web-based video editing that provides a simple video authoring experience for novice users. Nineteen directors from nine countries agreed to allow their films to be used as raw material for the creation of remixes using the platform. This paper describes the International Remix system, some of its design features, and its potential as a platform for research into community media usage on the web.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {797â€“798},
numpages = {2},
keywords = {social media, new media, web video, social software, remix},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180815,
author = {Srinivasan, S. H.},
title = {Speakr: Auditory Skimming and Scrolling},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180815},
doi = {10.1145/1180639.1180815},
abstract = {The widespread use of portable devices has led to the resurgence of speech interfaces. There is a crucial difference between the visual and auditory presentation of text. If text is presented visually, the user can skim and scroll through the content to locate relevant information. Audio output is usually presented sequentially with media player controls. For synthesized text a much more precise control is required. Actually the ability to skim and scroll through speech or auditory output will be crucial to the deployment speech output. Speakr is a speech synthesis system which permits different levels of skimming in speech outputs produced using speech synthesis. Speakr exploits the availability of the underlying text representation. Different aspects of the text representation - typographic conventions, markups, syntax, semantics, etc. - are used to achieve skim and scroll effects.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {799â€“800},
numpages = {2},
keywords = {auditory skimming, auditory scrolling},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180816,
author = {Sanyal, Subhajit and Srinivasan, S. H.},
title = {3dB: A System for Geometric Tagging},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180816},
doi = {10.1145/1180639.1180816},
abstract = {There is an explosion of multimedia content, especially image, on the Internet. Images as pixel arrays suffer from two deficiencies: semantic gap and geometric gap. There have been several attempts like object detection for closing the semantic gap. Recently the notion of community tagging of objects in an image has attracted a lot of attention. There has been lot of research in closing the geometric gap by 3D reconstruction. In this paper, we present 3D Buddy (3dB) -a system for geometrically tagging images for 3D reconstruction.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {801â€“802},
numpages = {2},
keywords = {geometric tagging, 3D modeling},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180817,
author = {Chen, Wen-Yen and Lee, Benjamin N. and Chang, Edward Y.},
title = {Fotowiki: Distributed Map Enhancement Service},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180817},
doi = {10.1145/1180639.1180817},
abstract = {Fotowiki (FW) is a wiki-based map service that integrates visual and textual information with map. FW divides a geographical area into sub-areas. An individual responsible for providing information about a sub-area enters collected data into a wiki page. FW uploads distributed wiki-pages, and overlays the information on the map. This demonstration shows FW's architecture and functionalities.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {803â€“804},
numpages = {2},
keywords = {photos, information sharing, collaborative, wiki},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180818,
author = {Palant, Wladimir and Griwodz, Carsten and Halvorsen, P\r{a}l},
title = {GLS: Simulator for Online Multi-Player Games},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180818},
doi = {10.1145/1180639.1180818},
abstract = {One of the most difficult tasks when creating an online multi-player game is to provide the players with a consistent view of the virtual world despite the network delays. Most current games use prediction algorithms to achieve this, however measuring the effect of different approaches is difficult. To solve this problem we introduce a simulator called GLS that gives us a fully controlled environment and allows large-scale experiments to evaluate different aspects of the algorithms.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {805â€“806},
numpages = {2},
keywords = {GLS, latency, games, simulation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180819,
author = {H\"{u}rst, Wolfgang and Lauer, Tobias and Kaschuba, Robert},
title = {Interfaces for Interactive Audio-Visual Media Browsing},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180819},
doi = {10.1145/1180639.1180819},
abstract = {In this demo, we present new interfaces for interactive navigation in continuous, time-based multimedia files, such as video recordings. In contrast to common techniques for multimedia skimming, our approaches enable users to interactively navigate audio files along the timeline and support time-synchronized browsing in both audio as well as visual data streams.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {807â€“808},
numpages = {2},
keywords = {audio interfaces, audio skimming, multimedia browsing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180820,
author = {Yan, Xin and Yu, Xinguo and Chi, Tran Thi Phuong},
title = {A System for 3D Projected Virtual Content Insertion into Broadcast Tennis Video},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180820},
doi = {10.1145/1180639.1180820},
abstract = {This demonstration presents our system for inserting projected virtual content into broadcast tennis video based on camera matrix acquired. This system can automatically acquire the accurate camera matrix for each frame with the tennis court and insert projected virtual content consistent with camera motion. We achieve the accuracy of camera matrices via the proposed techniques of clip-wise data analysis and Hough-like search.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {809â€“810},
numpages = {2},
keywords = {sports video, virtual content insertion},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180821,
author = {Gao, Yul i and Luo, Hangzai and Fan, Jianping},
title = {Searching and Browsing Large Scale Image Database Using Keywords and Ontology},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180821},
doi = {10.1145/1180639.1180821},
abstract = {Automatic image annotation is a promising solution to enable keyword-based semantic image retrieval. In this demo, we present our image search engine I-Search implemented using a multi-level semantic image annotation algorithm. By incorporating a domain-specific ontology into the autogenerated annotations, we are able to organize large scale natural image databases into hierarchical structures for browsing and keyword-based searching without referring to external text information.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {811â€“812},
numpages = {2},
keywords = {multi-level image, salient object detection, annotation, ontology},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180822,
author = {Al Hashimi, Sama'a and Davies, Gordon},
title = {Vocal Telekinesis: Physical Control of Inanimate Objects with Minimal Paralinguistic Voice Input},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180822},
doi = {10.1145/1180639.1180822},
abstract = {Growing awareness of the possible over-dominance of the visual modality in the field of interactive media and of the existence of untapped dimensions of sound has led many developers to embark upon the development of sound-based projects. Many of these projects restrict themselves to delivering and fostering audio-visual and audio-physical art works. In this paper, we advocate the progression of audio-visual applications into voice-visual performances, and the evolution of audio-physical applications into voice-physical installations. We suggest new forms of voice-physical artwork aimed at the use of paralinguistic vocalizations to physically control real inanimate objects.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {813â€“814},
numpages = {2},
keywords = {vocal telekinesis, voice-physical, paralanguage, vocal input},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245550,
author = {Wang, J.},
title = {Session Details: Content Session 4: Event and Copy Detection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245550},
doi = {10.1145/3245550},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180824,
author = {Zhai, Yun and Shah, Mubarak},
title = {Visual Attention Detection in Video Sequences Using Spatiotemporal Cues},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180824},
doi = {10.1145/1180639.1180824},
abstract = {Human vision system actively seeks interesting regions in images to reduce the search effort in tasks, such as object detection and recognition. Similarly, prominent actions in video sequences are more likely to attract our first sight than their surrounding neighbors. In this paper, we propose a spatiotemporal video attention detection technique for detecting the attended regions that correspond to both interesting objects and actions in video sequences. Both spatial and temporal saliency maps are constructed and further fused in a dynamic fashion to produce the overall spatiotemporal attention model. In the temporal attention model, motion contrast is computed based on the planar motions (homography) between images, which is estimated by applying RANSAC on point correspondences in the scene. To compensate the non-uniformity of spatial distribution of interest-points, spanning areas of motion segments are incorporated in the motion contrast computation. In the spatial attention model, a fast method for computing pixel-level saliency maps has been developed using color histograms of images. A hierarchical spatial attention representation is established to reveal the interesting points in images as well as the interesting regions. Finally, a dynamic fusion technique is applied to combine both the temporal and spatial saliency maps, where temporal attention is dominant over the spatial model when large motion contrast exists, and vice versa. The proposed spatiotemporal attention framework has been applied on over 20 testing video sequences, and attended regions are detected to highlight interesting objects and motions present in the sequences with very high user satisfaction rate.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {815â€“824},
numpages = {10},
keywords = {video attention detection, spatiotemporal saliency map},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180825,
author = {Lu, Lie and Hanjalic, Alan},
title = {Towards Optimal Audio "Keywords" Detection for Audio Content Analysis and Discovery},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180825},
doi = {10.1145/1180639.1180825},
abstract = {Natural semantic sound clusters in an audio document, also referred to as audio elements, can be seen as an analogy to words in a text document. Based on the obtained set of audio elements, the key audio elements, or audio "keywords", can be detected, which are most prominent in characterizing the content of audio data. As such, they can be of great use for automatic audio content analysis and discovery. Motivated by the limitations of the existing methods for key audio element detection, we propose in this paper a novel unsupervised approach to audio elements weighting using multiple audio documents, analog to word weighting in text document analysis. In our approach, dominant feature vectors (DFV) are first extracted from each audio element, and used to measure the audio elements similarity, based on which the occurrence probability of one audio element in different audio documents can be estimated. Then, four factors, including expected term frequency, expected inverse document frequency, expected term duration, and expected inverse document duration, are calculated and combined to give the importance weight of each audio element. Evaluation of the obtained audio "keywords" and their usability for auditory scene segmentation and audio document clustering, performed on 5 hours of diverse audio data, shows highly promising results.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {825â€“834},
numpages = {10},
keywords = {content-based audio analysis, audio content classification, key audio element extraction, audio content parsing, audio content mining and knowledge discovery},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180826,
author = {Law-To, Julien and Buisson, Olivier and Gouet-Brunet, Valerie and Boujemaa, Nozha},
title = {Robust Voting Algorithm Based on Labels of Behavior for Video Copy Detection},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180826},
doi = {10.1145/1180639.1180826},
abstract = {This paper presents an efficient approach for copies detection in a large videos archive consisting of several hundred of hours. The video content indexing method consists of extracting the dynamic behavior on the local description of interest points and further on the estimation of their trajectories along the video sequence. Analyzing the low-level description obtained allows to highlight trends of behaviors and then to assign a label of behavior to each local descriptor. Such an indexing approach has several interesting properties: it provides a rich, compact and generic description, while labels of behavior provide a high-level description of the video content. Here, we focus on video Content Based Copy Detection (CBCD). Copy detection is problematic as similarity search problem but with prominent differences. To be efficient, it requires a dedicated on-line retrieval method based on a specific voting function. This voting function must be robust to signal transformations and discriminating versus high similarities which are not copies. The method we propose in this paper is a dedicated on-line retrieval method based on a combination of the different dynamic contexts computed during the off-line indexing. A spatio-temporal registration based on the relevant combination of detected labels is then applied. This approach is evaluated using a huge video database of 300 hours with different video tests. The method is compared to a state-of-the art technique in the same conditions. We illustrate that taking labels into account in the specific voting process reduces false alarms significantly and drastically improves the precision.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {835â€“844},
numpages = {10},
keywords = {label of behavior, content-based video copy detection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180827,
author = {Ngo, Chong-Wah and Zhao, Wan-Lei and Jiang, Yu-Gang},
title = {Fast Tracking of Near-Duplicate Keyframes in Broadcast Domain with Transitivity Propagation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180827},
doi = {10.1145/1180639.1180827},
abstract = {The identification of near-duplicate keyframe (NDK) pairs is a useful task for a variety of applications such as news story threading and content-based video search. In this paper, we propose a novel approach for the discovery and tracking of NDK pairs and threads in the broadcast domain. The detection of NDKs in a large data set is a challenging task due to the fact that when the data set increases linearly, the computational cost increases in a quadratic speed, and so does the number of false alarms. This paper explores the symmetric and transitive nature of near-duplicate for the effective detection and fast tracking of NDK pairs based upon the matching of local keypoints in frames. In the detection phase, we propose a robust measure, namely pattern entropy (PE), to measure the coherency of symmetric keypoint matching across the space of two keyframes. This measure is shown to be effective in discovering the NDK identity of a frame. In the tracking phase, the NDK pairs and threads are rapidly propagated and linked with sitivity without the need of detection. This step ends up a significant boost in speed efficiency. We evaluate proposed approach against a month of the 2004 broadcast videos. The experimental results indicate our approach outperforms other techniques in terms of recall and precision with a large margin. In addition, by considering the transitivity and the underlying distribution of NDK pairs along time span, a speed up of 3 to 5 times is achieved when keeping the performance close enough to the optimal one obtained by exhaustive evaluation.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {845â€“854},
numpages = {10},
keywords = {near-duplicate detection, keypoint matching, transitivity propagation, pattern entropy, keyframe tracking},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245551,
author = {Sebe, N.},
title = {Session Details: Brave New Topics Session 1 - Human-Centered Multimedia},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245551},
doi = {10.1145/3245551},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180829,
author = {Jaimes, Alejandro and Sebe, Nicu and Gatica-Perez, Daniel},
title = {Human-Centered Computing: A Multimedia Perspective},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180829},
doi = {10.1145/1180639.1180829},
abstract = {Human-Centered Computing (HCC) is a set of methodologies that apply to any field that uses computers, in any form, in applications in which humans directly interact with devices or systems that use computer technologies. In this paper, we give an overview of HCC from a Multimedia perspective. We describe what we consider to be the three main areas of Human-Centered Multimedia (HCM): media production, analysis, and interaction. In addition, we identify the core characteristics of HCM, describe example applications, and propose a research agenda for HCM.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {855â€“864},
numpages = {10},
keywords = {multimodal, multimedia, human-centered computing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180830,
author = {Pentland, Alex and Gips, Jonathan and Dong, Wen and Stoltzman, Will},
title = {Human Computing for Interactive Digital Media},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180830},
doi = {10.1145/1180639.1180830},
abstract = {Widespread adoption of interactive, peer-to-peer digital media will require a solution to the Privacy, Sharing, and Interest (PSI) problem: how can we know what the user wants to share with whom, and when, without burdening the user with constant updating of lists of approved users and sharing preferences? We argue that real-time analysis of user behavior provides an automatic PSI capability, allowing media to be automatically and proactively shared with a much lower user burden.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {865â€“870},
numpages = {6},
keywords = {sensors, privacy, interest, multimedia, machine learning, human-centric computing, sharing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180831,
author = {Oviatt, Sharon},
title = {Human-Centered Design Meets Cognitive Load Theory: Designing Interfaces That Help People Think},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180831},
doi = {10.1145/1180639.1180831},
abstract = {Historically, the development of computer systems has been primarily a technology-driven phenomenon, with technologists believing that "users can adapt" to whatever they build. Human-centered design advocates that a more promising and enduring approach is to model users' natural behavior to begin with so that interfaces can be designed that are more intuitive, easier to learn, and freer of performance errors. In this paper, we illustrate different user-centered design principles and specific strategies, as well as their advantages and the manner in which they enhance users' performance. We also summarize recent research findings from our lab comparing the performance characteristics of different educational interfaces that were based on user-centered design principles. One theme throughout our discussion is human-centered design that minimizes users' cognitive load, which effectively frees up mental resources for performing better while also remaining more attuned to the world around them.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {871â€“880},
numpages = {10},
keywords = {educational interfaces, cognitive load, multimodal interfaces, pen-based interfaces, human-centered design, robustness, usability, tangible interfaces, performance metrics, mobile interfaces, spoken language interfaces},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@dataset{10.1145/review-1180639.1180831_R42100,
author = {Glenn, Bernice T.},
title = {Review ID:R42100 for DOI: 10.1145/1180639.1180831},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1180639.1180831_R42100}
}

@inproceedings{10.1145/3245552,
author = {Wu, Y.},
title = {Session Details: Doctoral Symposium Session},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245552},
doi = {10.1145/3245552},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180833,
author = {Yang, Zhenyu},
title = {A Multi-Stream Adaptation Framework for Tele-Immersion},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180833},
doi = {10.1145/1180639.1180833},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {881â€“883},
numpages = {3},
keywords = {adaptation, 3D tele-immersion, bandwidth management},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180834,
author = {Luo, Hangzai and Fan, Jianping},
title = {Large-Scale Video Retrieval via Semantic Classification},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180834},
doi = {10.1145/1180639.1180834},
abstract = {Motivated by Google's great success on text document retrieval and recent progresses of semantic video understanding, researchers begin to build new generation of video retrieval systems that are able to support semantic sensitive video retrieval via keywords. Unfortunately, these systems are not able to provide satisfactory results for the masses because of several inter-related challenging problems. We have proposed novel algorithms to resolve some of these problems. Firstly, the salient object based semantic classification algorithm is proposed to extract semantic concepts of video clips. Secondly, the video visualization based interactive retrieval framework is proposed to help users input semantic and visual queries efficiently and effectively. Finally, the concept-oriented skimming algorithm is proposed to help users efficiently check search results.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {884â€“886},
numpages = {3},
keywords = {video visualization, semantic video classification},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180835,
author = {Korshunov, Pavel},
title = {Rate-Accuracy Tradeoff in Automated, Distributed Video Surveillance Systems},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180835},
doi = {10.1145/1180639.1180835},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {887â€“889},
numpages = {3},
keywords = {video features, rate-accuracy tradeoff, video surveillance, video analysis},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245553,
author = {Nahrstedt, K.},
title = {Session Details: Keynote},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245553},
doi = {10.1145/3245553},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180837,
author = {Horowitz, Bradley},
title = {Implicit Participation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180837},
doi = {10.1145/1180639.1180837},
abstract = {A new generation of simple, affordable and easy to use tools has led to what has been called the "democratization of publishing". Anyone with a camera has become a "photographer", with a keyboard an "author", a microphone a "podcaster", etc. As the means for production and distribution of content have become readily accessible, the most valuable inelastic commodity has become attention. In this talk we will describe both how Yahoo has been working to lower barriers to participation and turn every "consumer" into a "creator", but also how we are using data mining techniques to help identify and leverage high-value content. Yahoo! is creating systems that engender mass participation but also that allow the "cream to rise" by ensuring that every user of the system creates value in his or her wake. Examples from widely available real world products (such as Flickr, Yahoo! Answers, del.icio.us, upcoming.org, etc.) will be used as illustrations.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {890},
numpages = {1},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245554,
author = {Wilcox, L.},
title = {Session Details: Content Session 5: Image Annotation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245554},
doi = {10.1145/3245554},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180839,
author = {Wu, Wen and Yang, Jie},
title = {SmartLabel: An Object Labeling Tool Using Iterated Harmonic Energy Minimization},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180839},
doi = {10.1145/1180639.1180839},
abstract = {Labeling objects in images is an essential prerequisite for many visual learning and recognition applications that depend on training data, such as image retrieval, object detection and recognition. Manually creating labels in images is not only time-consuming but also subject to human labeling errors, and eventually, becomes impossible for a large scale image database. Semi-supervised learning (SSL)algorithms such as Gaussian random field (GRF)can be applied to labeling objects in images since they have the ability to include a large amount of unlabeled data while requiring only a small amount of labeled data. However, the one-shot property of GRF prevents it from achieving good labeling performance. In this paper, we presents a novel object labeling tool, SmartLabel, to semi-automatically label objects in images. The algorithm of SmartLabel has four innovations over GRF:1)soft labeling,2)graph construction with spatial constraints, 3)iterated harmonic energy minimization, and 4)using relevance feedback to incorporate human interaction in the loop. As demonstrated in datasets of six object categories, the proposed SmartLabel not only works effectively even with a very small amount of user input (e.g., 1 .5%of image size)but also achieves significant improvement over GRF.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {891â€“900},
numpages = {10},
keywords = {semi-supervised learning, object labeling, gaussian random field, harmonic energy minimization},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180840,
author = {Gao, Yuli and Fan, Jianping and Xue, Xiangyang and Jain, Ramesh},
title = {Automatic Image Annotation by Incorporating Feature Hierarchy and Boosting to Scale up SVM Classifiers},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180840},
doi = {10.1145/1180639.1180840},
abstract = {The performance of image classifiers largely depends on two inter-related issues:(1)suitable frameworks for image content representation and automatic feature extraction;(2) effective algorithms for image classifier training and feature subset selection. To address the first issue, a multiresolution grid-based framework is proposed for image content representation and feature extraction to bypass the time-consuming and erroneous process for image segmentation. To address the second issue, a hierarchical boosting algorithm is proposed by incorporating feature hierarchy and boosting to scale up SVM image classifier training in high-dimensional feature space. The high-dimensional multi-modal heterogeneous visual features are partitioned into multiple low-dimensional single-modal homogeneous feature subsets and each of them characterizes certain visual property of images. For each homogeneous feature subset, principal component analysis (PCA)is performed to exploit the feature correlations and a weak classifier is learned simultaneously. After the weak classifiers for different feature subsets and grid sizes are available, they are combined to boost an optimal classifier for the given object class or image concept, and the most representative feature subsets and grid sizes are selected. Our experiments on a specific domain of natural images have obtained very positive results.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {901â€“910},
numpages = {10},
keywords = {image annotation, hierarchical boosting},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180841,
author = {Li, Jia and Wang, James Z.},
title = {Real-Time Computerized Annotation of Pictures},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180841},
doi = {10.1145/1180639.1180841},
abstract = {Automated annotation of digital pictures has been a highly challenging problem for computer scientists since the invention of computers. The capability of annotating pictures by computers can lead to breakthroughs in a wide range of applications including Web image search, online picture-sharing communities, and scientific experiments. In our work, by advancing statistical modeling and optimization techniques, we can train computers about hundreds of semantic concepts using example pictures from each concept. The ALIPR (Automatic Linguistic Indexing of Pictures -Real Time)system of fully automatic and high speed annotation for online pictures has been constructed. Thousands of pictures from an Internet photo-sharing site, unrelated to the source of those pictures used in the training process, have been tested. The experimental results show that a single computer processor can suggest annotation terms in real-time and with good accuracy.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {911â€“920},
numpages = {10},
keywords = {modeling, clustering, statistical learning, image annotation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245555,
author = {Prabhakaran, B.},
title = {Session Details: Systems Session 3: Assorted Topics},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245555},
doi = {10.1145/3245555},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180843,
author = {Xu, Min and Li, Jiaming and Chia, Liang-Tien and Hu, Yiqun and Lee, Bu-Sung and Rajan, Deepu and Jin, Jesse S.},
title = {Event on Demand with MPEG-21 Video Adaptation System},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180843},
doi = {10.1145/1180639.1180843},
abstract = {In this paper, we present an event-on-demand (EoD)video adaptation system. The proposed system supports users in deciding their events of interest and considers network conditions to adapt video source by event selection and frame dropping.Firstly, events are detected by audio/video analysis and annotated by the description schemes (DSs)provided by MPEG-7 Multimedia Description Schemes (MDSs). And then, to achieve a generic adaptation solution, the adaptation is developed following MPEG-21 Digital Item Adaptation (DIA)framework. We look at early release of the MPEG-21 Reference Software on XML generation and develop our own system for EoD video adaptation in three steps:1) the event information is parsed from MPEG-7 annotation XML file together with bitstream to generate generic Bitstream Syntax Description (gBSD). 2) Users' preference, Network Characteristic and Adaptation QoS (AQoS) are considered for making adaptation decision. 3) adaptation engine automatically parses adaptation decisions and gBSD to achieve adaptation.Unlike most existing adaptation work, the system adapts video of events with interest according to users' preference. Implementation following MPEG-7 and MPEG-21 standards provides a generic video adaptation solution. gBSD based adaptation avoids complex video computation. 30 students from various departments were invited to test the system and their responses has been positive.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {921â€“930},
numpages = {10},
keywords = {event on demand, event detection, annotation, adaptation, MPEG-7, MPEG-21},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180844,
author = {Fern\'{a}ndez, Gerardo and Cuenca, Pedro and Barbosa, Luis Orozco and Kalva, Hari},
title = {Very Low Complexity MPEG-2 to H.264 Transcoding Using Machine Learning},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180844},
doi = {10.1145/1180639.1180844},
abstract = {This paper presents a novel macroblock mode decision algorithm for inter-frame prediction based on machine learning techniques to be used as part of a very low complexity MPEG-2 to H.264 video transcoder. Since coding mode decisions take up the most resources in video transcoding, a fast macro block (MB) mode estimation would lead to reduced complexity. The proposed approach is based on the hypothesis that MB coding mode decisions in H.264 video have a correlation with the distribution of the motion compensated residual in MPEG-2 video. We use machine learning tools to exploit the correlation and derive decision trees to classify the incoming MPEG-2 MBs into one of the 11 coding modes in H.264. The proposed approach reduces the H.264 MB mode computation process into a decision tree lookup with very low complexity. The proposed transcoder is compared with a reference transcoder comprised of a MPEG-2 decoder and an H.264 encoder. Our results show that the proposed transcoder reduces the H.264 encoding time by over 95% with negligible loss in quality and bitrate.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {931â€“940},
numpages = {10},
keywords = {transcoding, H.264, inter-frame, MPEG-2, machine learning},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180845,
author = {Eichhorn, Alexander},
title = {Modelling Dependency in Multimedia Streams},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180845},
doi = {10.1145/1180639.1180845},
abstract = {Expressing and analysing data dependency in multimedia streams is promising, since content-aware policies at a transport level would benefit from such services. In this paper we present a format-independent dependency model aimed at specifying, validating and reasoning about structural dependency in multimedia streams. Based on this model, we developed a universal dependency description language and a dependency validation service to serve as an infrastructure for content-aware transport layers. Driven by application knowledge, this special form of a cross-layer design enables lower layers to reason about the impact of data loss and drops during transmission while being unaware of the real data format.We outline, how this infrastructure can be used to build content-aware error protection policies and explain how applications need to specify dependency and prepare media streams in order to gain benefits from those policies. While costs and benefits of a dependency model are only quantifiable in conjunction with special policies, we report on the general worst-case costs of our model here.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {941â€“950},
numpages = {10},
keywords = {dependency, content-aware media streaming, error control},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245556,
author = {Brandt, S. and Feng, W.-C.},
title = {Session Details: Open Source and Video Program Session},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245556},
doi = {10.1145/3245556},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180847,
author = {Amatriain, Xavier and Arumi, Pau and Garcia, David},
title = {CLAM: A Framework for Efficient and Rapid Development of Cross-Platform Audio Applications},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180847},
doi = {10.1145/1180639.1180847},
abstract = {CLAM is a C++framework that offers a complete development and research platform for the audio and music domain. Apart from offering an abstract model for audio systems, it also includes a repository of processing algorithms and data types as well as a number of tools such as audio or MIDI input/output. All these features can be exploited to build cross-platform applications or to build rapid prototypes to test signal and media processing algorithms and systems. The framework also includes a number of stand-alone applications that can be used for tasks such as audio analysis/synthesis, plug-in development or metadata annotation.In this article we give a brief overview of CLAM's features and applications.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {951â€“954},
numpages = {4},
keywords = {multimedia, audio, frameworks},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180848,
author = {Chen, Jun-Cheng and Chu, Wei-Ta and Kuo, Jin-Hau and Weng, Chung-Yi and Wu, Ja-Ling},
title = {Audiovisual Slideshow: Present Your Journey by Photos},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180848},
doi = {10.1145/1180639.1180848},
abstract = {This demonstration presents a novel way to systematically display photos and enhance the viewing experience of photo browsing. In contrast to conventional photo slideshow, multiple photos that have similar characteristics are well arranged and displayed at the same layout. Moreover, the displaying pace is coordinated with the beat of the user-selected incidental music. To automatically generate the audiovisual slideshow, we develop a system that consists of three main components: photo analysis, music analysis, and audiovisual composition. Audiovisual content analysis and cross-media synchronization issues are addressed in this work. This novel demonstration is especially suitable to present photos taken in a journey. It vigorously presents the delights of traveling and helps us recall or experience the trip.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {955â€“956},
numpages = {2},
keywords = {photo clustering, music analysis, image content analysis, slideshow},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180849,
author = {Kopf, Stephan and Lampi, Fleming and King, Thomas and Effelsberg, Wolfgang},
title = {Automatic Scaling and Cropping of Videos for Devices with Limited Screen Resolution},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180849},
doi = {10.1145/1180639.1180849},
abstract = {A large number of previously recorded videos cannot be directly visualized on mobile devices like PDAs or mobile phones due to an inappropriate screen resolution of their displays. Transcoding can be used to change the resolution, however, the usual transcoding algorithms have problems preserving the semantic content. For instance, superimposed text is unreadable if the character size drops below a certain value. In this paper, we present a novel adaptation algorithm to scale and crop videos while preserving their semantic content. Semantic features in a shot are combined to select a suitable region to be resented in the adapted video.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {957â€“958},
numpages = {2},
keywords = {transcoding, content repurposing, video adaptation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180850,
author = {Companje, Rick and van Dijk, Nico and Hogenbirk, Hanco and Mast, Daniundefineda},
title = {Globe4D: Time-Traveling with an Interactive Four-Dimensional Globe},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180850},
doi = {10.1145/1180639.1180850},
abstract = {Globe4D is an interactive four-dimensional globe. It is a projection of the Earth's surface on a physical sphere. The sphere can be freely rotated along all axes, viewed from any angle and enables the user to control time as its fourth dimension. An application was created that shows the historical movement of the continents, known as continental drift. Besides the Earth, any planet or spherical object can be projected. This paper describes the background, aims, results and progress of this student project.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {959â€“960},
numpages = {2},
keywords = {direct manipulation device, continental drift, earth, globe projection, physical interaction, spherical display},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180851,
author = {Heck, Rachel and Wallick, Michael and Gleicher, Michael},
title = {Virtual Videography},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180851},
doi = {10.1145/1180639.1180851},
abstract = {Well-produced videos provide a convenient and effective way to archive lectures. In this demonstration, we present a new way to create lecture videos that possess many of the advantages of well-composed recordings without the cost and intrusion of a video production crew. The videos are produced by an automated system called Virtual Videography that employs the art of videography to mimic videographer-produced videos, while being unobtrusive when recording the lectures. The system uses the data recorded by unattended video cameras and microphones to produce a new edited video as an offline post-process. By producing videos offline, our system can use future information when planning shot sequences and synthesizing new shots. Using syntactic cues gathered from the original video and a novel shot planning algorithm, the system makes cinematic decisions without any semantic understanding of the lecture.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {961â€“962},
numpages = {2},
keywords = {automated video editing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180852,
author = {Kopf, Stephan and King, Thomas and Lampi, Fleming and Effelsberg, Wolfgang},
title = {Video Color Adaptation for Mobile Devices},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180852},
doi = {10.1145/1180639.1180852},
abstract = {A large number of videos cannot be visualized on mobile devices (e.g.,PDAs or mobile phones)due to an inappropriate color depth of the displays. Important details are lost if the color depth is reduced. A major challenge is the preservation of the semantic content in spite of this fact. In this paper, we present a novel adaptation algorithm to enable the playback of videos on color-limited mobile devices. Dithering algorithms diffuse the error to neighbor pixels and do not work very well for videos. We propose a non-linear transformation of luminance values and use textures in combination with edges to reduce the color depth in videos.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {963â€“964},
numpages = {2},
keywords = {video adaptation, dithering, transcoding},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180853,
author = {Shih, Timothy K. and Tang, Nick C. and Yeh, Wei-Sung and Chen, Ta-Jen},
title = {Video Inpainting and Implant via Diversified Temporal Continuations (Video Demonstration)},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180853},
doi = {10.1145/1180639.1180853},
abstract = {This video program presents several video inpainting/implant mechanisms implemented under different types of videos with diversified temporal continuations. We use an improved image inpainting method to remove foreground objects. Another set of objects are implanted into the inpainted video. Detailed algorithms are presented in a paper in the 2006 ACM Multimedia conference.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {965â€“966},
numpages = {2},
keywords = {object removal, object tracking, video implant, temporal continuations, video inpainting},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245557,
author = {Natsev, A.},
title = {Session Details: Content Session 6: Multimedia Exploration},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245557},
doi = {10.1145/3245557},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180855,
author = {Wang, Meng and Hua, Xian-Sheng and Song, Yan and Yuan, Xun and Li, Shipeng and Zhang, Hong-Jiang},
title = {Automatic Video Annotation by Semi-Supervised Learning with Kernel Density Estimation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180855},
doi = {10.1145/1180639.1180855},
abstract = {Insufficiency of labeled training data is a major obstacle for automatically annotating large-scale video databases with semantic concepts. Existing semi-supervised learning algorithms based on parametric models try to tackle this issue by incorporating the information in a large amount of unlabeled data. However, they are based on a "model assumption" that the assumed generative model is correct, which usually cannot be satisfied in automatic video annotation due to the large variations of video semantic concepts. In this paper, we propose a novel semi-supervised learning algorithm, named Semi Supervised Learning by Kernel Density Estimation (SSLKDE), which is based on a non-parametric method, and therefore the "model assumption" is avoided. While only labeled data are utilized in the classical Kernel Density Estimation (KDE) approach, in SSLKDE both labeled and unlabeled data are leveraged to estimate class conditional probability densities based on an extended form of KDE. We also investigate the connection between SSLKDE and existing graph-based semi-supervised learning algorithms. Experiments prove that SSLKDE significantly outperforms existing supervised methods for video annotation.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {967â€“976},
numpages = {10},
keywords = {video annotation, SSLKDE, kernel density estimation, semi-supervised learning},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180856,
author = {Datta, Ritendra and Ge, Weina and Li, Jia and Wang, James Z.},
title = {Toward Bridging the Annotation-Retrieval Gap in Image Search by a Generative Modeling Approach},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180856},
doi = {10.1145/1180639.1180856},
abstract = {While automatic image annotation remains an actively pursued research topic, enhancement of image search through its use has not been extensively explored. We propose an annotation-driven image retrieval approach and argue that under a number of different scenarios, this is very effective for semantically meaningful image search. In particular, our system is demonstrated to effectively handle cases of partially tagged and completely untagged image databases, multiple keyword queries, and example based queries with or without tags, all in near-realtime. Because our approach utilizes extra knowledge from a training dataset, it outperforms state-of-the-art visual similarity based retrieval techniques. For this purpose, a novel structure-composition model constructed from Beta distributions is developed to capture the spatial relationship among segmented regions of images. This model combined with the Gaussian mixture model produces scalable categorization of generic images. The categorization results are found to surpass previously reported results in speed and accuracy. Our novel annotation framework utilizes the categorization results to select tags based on term frequency, term saliency, and a WordNet-based measure of congruity, to boost salient tags while penalizing potentially unrelated ones. A bag of words distance measure based on WordNet is used to compute semantic similarity. The effectiveness of our approach is shown through extensive experiments.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {977â€“986},
numpages = {10},
keywords = {automatic annotation, generative models, image search},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180857,
author = {Adams, Brett and Phung, Dinh and Venkatesh, Svetha},
title = {Extraction of Social Context and Application to Personal Multimedia Exploration},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180857},
doi = {10.1145/1180639.1180857},
abstract = {Personal media collections are often viewed and managed along the social dimension, the places we spend time at and the people we see, thus tools for extracting and using this information are required. We present novel algorithms for identifying socially significant places termed social spheres unobtrusively from GPS traces of daily life, and label them as one of Home, Work, or Other, with quantitative evaluation of 9 months taken from 5 users. We extract locational co-presence of these users and formulate a novel measure of social tie strength based on frequency of interaction, and the nature of spheres it occurs within. Comparative user studies of a multimedia browser designed to demonstrate the utility of social metadata indicate the usefulness of a simple interface allowing navigation and filtering in these terms. We note the application of social context is potentially much broader than personal media management, including context-aware device behaviour, life logs, social networks, and location-aware information services.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {987â€“996},
numpages = {10},
keywords = {social context, multimedia browsing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/3245558,
author = {Dimitrova, N.},
title = {Session Details: Brave New Topics Session 2 - Multimedia Signal Processing and Systems in Healthcare and Life Science},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245558},
doi = {10.1145/3245558},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180859,
author = {Ebadollahi, Shahram and Coden, Anni R. and Tanenblatt, Michael A. and Chang, Shih-Fu and Syeda-Mahmood, Tanveer and Amir, Arnon},
title = {Concept-Based Electronic Health Records: Opportunities and Challenges},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180859},
doi = {10.1145/1180639.1180859},
abstract = {Healthcare is a data-rich but information-poor domain. Terabytes of multimedia medical data are being generated on a monthly basis in a typical healthcare organization in order to document patients' health status and care process. Government and health-related organizations are pushing for fully electronic, cross-institution, integrated Electronic Health Records to provide a better, cost effective and more complete access to this data. However, provision of efficient access to the content of such records for timely and decision-enabling information extraction will not be available. Such a capability is essential for providing efficient decision support and objective evidence to clinicians. In addition researchers, medical students, patients, and payers could also benefit from it. We present the idea of concept-based multimedia health records, which aims at organizing the health records at the information level. We will explore the opportunities and possibilities that such an organization will provide, what role the field of multimedia content management could play to materialize this type of health record organization, and what the challenges will be in the quest for realizing the idea.We believe that the field of multimedia can play a very active role in taking healthcare information systems to the next level by facilitating the access to decision-enabling information for different types of users in healthcare. Our goal is to share with the community our thoughts on where the field of multimedia content management research should be focusing its attention to have a fundamental impact on the practice of medicine.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {997â€“1006},
numpages = {10},
keywords = {concept detection, medical decision support, multimedia analytics, electronic health records},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@dataset{10.1145/review-1180639.1180859_R41784,
author = {Hill, David Gary},
title = {Review ID:R41784 for DOI: 10.1145/1180639.1180859},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1180639.1180859_R41784}
}

@inproceedings{10.1145/1180639.1180860,
author = {Andrews, Peter and Wang, Haibin and Valente, Dan and Serkhane, Jih\`{e}ne and Mitra, Partha P. and Saar, Sigal and Tchernichovski, Ofer and Golani, Ilan},
title = {Multimedia Signal Processing for Behavioral Quantification in Neuroscience},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180860},
doi = {10.1145/1180639.1180860},
abstract = {While there have been great advances in quantification of the genotype of organisms, including full genomes for many species, the quantification of phenotype is at a comparatively primitive stage. Part of the reason is technical difficulty: the phenotype covers a wide range of characteristics, ranging from static morphological features, to dynamic behavior. The latter poses challenges that are in the area of multimedia signal processing. Automated analysis of video and audio recordings of animal and human behavior is a growing area of research, ranging from the behavioral phenotyping of genetically modified mice or drosophila to the study of song learning in birds and speech acquisition in human infants. This paper reviews recent advances and identifies key problems for a range of behavior experiments that use audio and video recording. This research area offers both research challenges and an application domain for advanced multimedia signal processing. There are a number of MMSP tools that now exist which are directly relevant for behavioral quantification, such as speech recognition, video analysis and more recently, wired and wireless sensor networks for surveillance. The research challenge is to adapt these tools and to develop new ones required for studying human and animal behavior in a high throughput manner while minimizing human intervention. In contrast with consumer applications, in the research arena there is less of a penalty for computational complexity, so that algorithmic quality can be maximized through the utilization of larger computational resources that are available to the biomedical researcher.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1007â€“1016},
numpages = {10},
keywords = {mouse, video, neuroscience, birdsong, locomotion, zebra finch, human, phenotype, audio, multimedia, signal processing, vocal development, infant, behavior},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180861,
author = {Dimitrova, Nevenka and Cheung, Yee Him and Zhang, Michael},
title = {Analysis and Visualization of DNA Spectrograms: Open Possibilities for the Genome Research},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180861},
doi = {10.1145/1180639.1180861},
abstract = {The demand for technology that can process biological information is becoming more and more obvious and urgent. Existing research in bioinformatics has been focusing on various types of analysis of DNA sequences and various measurements taken at the protein, RNA transcript and DNA level. In this paper we will show the application of spectral analysis and image processing in analyzing DNA sequences of specific structure. In addition, we extend the framework to visualize long DNA sequences and help in identifying patterns that are visible at high resolution of DNA spectral images.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1017â€“1024},
numpages = {8},
keywords = {fourier transform, DNA spectrogram, genome analysis},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180863,
author = {Fernezelyi, M\'{a}rton and Masz\'{a}k, Zolt\'{a}n Szegedy and Langh, R\'{o}bert},
title = {Smalltalk: Interactive Installation},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180863},
doi = {10.1145/1180639.1180863},
abstract = {Smalltalk is an interactive multimedia installation addressing Artificial Intelligence. The central motif of the work is the question of the Turing test: Can machines think? Certainly, equally important is the question of whether a conversation between two robots can be considered a work of art.The field of chatterbots simulating human intelligence is perhaps the best-known, and certainly the most popular subject within the discourse on artificial intelligence. During recent years, the research of the border zones of science and art have once again become the central theme of theoretical literature. Smalltalk is also interpretable within this relational system, as it consciously, and in an ironic format, employs a scientific approach.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1025â€“1026},
numpages = {2},
keywords = {chatbots, interactive, art, artificial intelligence},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180864,
author = {Courchesne, Luc and Langlois, Guillaume and Martinez, Luc},
title = {Where Are You? An Immersive Experience in the Panoscope 360Â°},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180864},
doi = {10.1145/1180639.1180864},
abstract = {The Panoscope 360Â® is a single channel immersive display composed of a large inverted dome, a hemispheric lens and projector, a computer and a surround sound system. From within, visitors can navigate in real-time in a virtual 3D world using a handheld 3-axis pointer/selector. In Where are you?, the featured program, immersed visitors are invited to explore an artificial world of many scales and to meet other live and pre-recorded beings. The program puts the subject (visitor, actor, protagonist) in control of his/her position, the path and speed of his/her journey and, more interestingly perhaps, the scale at which he/she is prepared to "exist.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1027â€“1028},
numpages = {2},
keywords = {immersion, modernity, telepresence, interaction, artificial reality},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180865,
author = {Berk, Jed and Mitter, Nikhil},
title = {Autonomous Light Air Vessels (ALAVs)},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180865},
doi = {10.1145/1180639.1180865},
abstract = {Through a defined research process we designed objects that behave and respond in specific ways and are part of a networked system that emphasizes autonomous and flocking behavior.ALAVs are 3 flying objects that exist in a networked environment and communicate through assigned behaviors forming three scenarios: ALAV with a person, ALAV with other ALAVs, and ALAV alone.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1029â€“1030},
numpages = {2},
keywords = {network, blimps, installation, SunSpots, performance},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180866,
author = {Kang, Eunsu},
title = {Imago},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180866},
doi = {10.1145/1180639.1180866},
abstract = {Imago is an interactive system representing an "alien" that has a unique communication method of movie projection. Imago's body consists of projector turning, projection and human detection system using ultrasonic sensors. Imago projects its messages in the form of movies on the participant's body in response to their position and speed. Imago aims not to make a system simulating human gesture detection, but to represent the communication situation between two creatures having different/limited communication ways as artwork. Participants experience the Imago's method of communication and ruminate human communication requiring efforts to understand differences. Furthermore, Imago is expected to evoke image communication, which has been less emphasized since Gutenberg Galaxy [1], in the participant's mind.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1031â€“1032},
numpages = {2},
keywords = {media art, human detection, imago, image, emotion, unknown zone, projector, communication, ultrasonic sensor, alien, art, movie, interactive, language, motion tracking, installation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180867,
author = {Hosale, Mark David and Thompson, John},
title = {DEFENDEX-ESPGX},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180867},
doi = {10.1145/1180639.1180867},
abstract = {DEFENDEX-ESPGX is an interactive art object that combines real-time audio and video synthesis processing with physical interaction. DEFENDEX-ESPGX is designed to simulate the look and feel of 1950's technology. The content draws on nostalgic reference to bring about implied comparisons between the fearful culture of the Cold War and the culture of fear associated with the current War on Terror.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1033â€“1034},
numpages = {2},
keywords = {media art, interactive, multimedia, multimodal},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180868,
author = {Fujimura, Noriyuki and Fujiyoshi, Satoshi and Hope, Tom and Nishimura, Takuichi},
title = {Tabletop Community: Visualization of Real World Oriented Social Network},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180868},
doi = {10.1145/1180639.1180868},
abstract = {We have undertaken a research project that visualizes a community, especially for events such as academic conferences. As research progresses, we have noticed that small gatherings of a few persons happen during events as a vital component of forming a community. We call these happenings Social Interactions. Typical situations that foster Social Interactions include gathering around a table. Therefore, we think it is possible to visualize larger communities through obtaining and processing Social Interaction data via table-like interfaces.As one part of group research project, here we introduce an art piece, named "Tabletop Community", that enables the visualization of Social Interactions around the table. Through this artwork system, users/participants easily record the state and atmosphere of each Interaction. The system visualizes the state of the entire community as an interactive network visualization. Here we introduce past results along with the current progress of the system.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1035â€“1036},
numpages = {2},
keywords = {real world oriented interface, information visualization, community art, social network, public art},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180869,
author = {Wan, Annie On Ni and Nishino, Hiroki and Pietro, Pamela},
title = {Tre Marie},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180869},
doi = {10.1145/1180639.1180869},
abstract = {Tre Marie is an interactive audio-visual dance improvised performance. The system in progress is a RF-ID (radio-frequency identification) technology for dance performance, which improvises, live visuals on stage. The performance is a reconstructed architecture of space that encodes the spatial aesthetics of the interaction between human, theatrical and cinematic space.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1037â€“1038},
numpages = {2},
keywords = {bluetooth, open sound control, audio-visual improvisation, RF-ID, dance performance},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180870,
author = {Oh, Jee Hyun},
title = {GORI.Node Garden},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180870},
doi = {10.1145/1180639.1180870},
abstract = {Gardening once was the one of art form nurtured by Zen principles. It emphasized the intentional blurring of the distinctions between natural and man-made materials. The idea is adopted into an installation project "GORI.Node Garden" in order to explore an author's alternative view of network and an idea of "Node Gardening". GORI.Node Garden has been developed as a series of network data visualization. While the earlier version of GORI.Node Garden was about nurturing a private network garden by using mobile communication data, "Gardening Two" is an open network garden where anyone on the Internet is able to participate in growing GORIs by instant messaging. This paper is a description of the second one.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1039â€“1040},
numpages = {2},
keywords = {data recycling, network data visualization, information ecosystem},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180871,
author = {Mariano, Sard\'{o}n},
title = {Books of Sand},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180871},
doi = {10.1145/1180639.1180871},
abstract = {Books of Sand are interactive installations that relate the movement of hands in the sand to hypertexts containing Jorge Luis Borges texts taken from the Web. It consists of one or more glass buckets full of sand that when touching it with the hands, projected codes retrieved from the Web arise interacting with the movement of the hands.A video camera captures images of the hands in the sand, which are analysed by software in real time extracting what moves or appears in the image field. Once the significant data of the movement of hands in each image is retrieved, the information is mapped on HTML codes downloaded and stored in a database by a parser program previously.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1041â€“1042},
numpages = {2},
keywords = {interactive art, web, image processing, installation, text parsing},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180872,
author = {Dallet, Jean-Marie and Laroche, Christian and Curien, Fr\'{e}d\'{e}ric},
title = {SLIDERS: A Collective Experience of Interactive Cinema},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180872},
doi = {10.1145/1180639.1180872},
abstract = {SLIDERS is an ambitious artistic and technical endeavour that proposes a new way of imagining and creating cinema. We have invented an open computer machine [1] that enables three "performers" to mount, in real-time and using visual and sound data stored in databases, a new type of film. This film, which has been played at performances in front of spectators, is what we call "the movie to come", that is to say, an N+1 film in which a new track, a programming track, has been added to the video and sound tracks. One of the features of this new type of film is that it has an infinite number of possible configurations or models.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1043â€“1044},
numpages = {2},
keywords = {future cinema, interface design, live interactive performance, real time video and sound mix, multi screen projection},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180873,
author = {Kawashima, Takashi and Kida, Togo and Niwa, Yoshimasa},
title = {Takashi's Seasons},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180873},
doi = {10.1145/1180639.1180873},
abstract = {Takashi's Seasons is a sequential live shadow puppet/video performance in which a number of interpretations of the four seasons are performed by an artist. Controlled with fishing line and wooden dowels, the puppets cast shadows on the screen. At the same time, the puppeteer controls the content being projected, and triggers sound effects using a custom input device. Working in precarious unison, the shadows of the puppets are synchronized with the animation, creating a unique live action performance. Animation and sound are composited with shadows in real time; rather than relying on a series of pre-rendered animation sequences, the artist produces "motion pictures" via a combination of seasonal sounds, live shadow puppet manipulation, and the projection of shadow-like animation sequences.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1045â€“1046},
numpages = {2},
keywords = {digital and analogue, hybrid, shadow puppet},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

@inproceedings{10.1145/1180639.1180874,
author = {Mendelowitz, Eitan},
title = {Drafting Poems: Inverted Potentialities},
year = {2006},
isbn = {1595934472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1180639.1180874},
doi = {10.1145/1180639.1180874},
abstract = {Drafting Poems: Inverted Potentialities challenges preconceived notions of intelligence, creativity, and authorship. In Drafting Poems, an artificially intelligent poet reacts to user stimuli creating meaningful verse. As users sketch on the surface of a glass drafting-table, the system gathers statistics on how the users are drawing. These statistics inform a probabilistic text generation system that creates original poetry. While this work builds on a rich tradition of algorithmic poetry, its focus on the creation of meaning represents a fundamentally different expressive goal and hence Drafting Poems signifies a new movement, the AI Aesthetic.},
booktitle = {Proceedings of the 14th ACM International Conference on Multimedia},
pages = {1047â€“1048},
numpages = {2},
keywords = {poetry generation},
location = {Santa Barbara, CA, USA},
series = {MM '06}
}

