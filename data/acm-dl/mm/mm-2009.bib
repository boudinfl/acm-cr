@inproceedings{10.1145/3251342,
author = {Gao, Wen and Hanjalic, Alan},
title = {Session Details: Keynote Addresses},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251342},
doi = {10.1145/3251342},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631274,
author = {Zhang, HongJiang},
title = {Multimedia Content Analysis and Search: New Perspectives and Approaches},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631274},
doi = {10.1145/1631272.1631274},
abstract = {After 15 years of extensive research, multimedia retrieval has finally come to its prime time when everything becomes accessible on the web. However, web search provides both a new paradigm and poses a new challenge to the multimedia retrieval research community. It calls for a rethinking of the traditional content-based approaches, especially in how to make use of the massive yet noisy meta data associated with the web pages and links. In this talk, we will first review some familiar approaches in content-based multimedia retrieval. Then, we will present a set of efforts in web multimedia search to illustrate interesting new thoughts that potentially will lead to new breakthroughs in multimedia search and stimulate new research ideas.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1–2},
numpages = {2},
keywords = {content analysis, search, multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631275,
author = {Ebrahimi, Touradj},
title = {Quality of Multimedia Experience: Past, Present and Future},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631275},
doi = {10.1145/1631272.1631275},
abstract = {This talk starts by defining what is Quality of Experience. It then provides an overview of state of the art in Quality of Experience in multimedia systems. It will finally conclude by presenting challenges and trends that need to be further addressed.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {3–4},
numpages = {2},
keywords = {QoS, QoE, quality of service, quality of experience},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251343,
author = {Rui, Yong},
title = {Session Details: Best Paper Session},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251343},
doi = {10.1145/3251343},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631277,
author = {Jiang, Wei and Cotton, Courtenay and Chang, Shih-Fu and Ellis, Dan and Loui, Alexander},
title = {Short-Term Audio-Visual Atoms for Generic Video Concept Classification},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631277},
doi = {10.1145/1631272.1631277},
abstract = {We investigate the challenging issue of joint audio-visual analysis of generic videos targeting at semantic concept detection. We propose to extract a novel representation, the Short-term Audio-Visual Atom (S-AVA), for improved concept detection. An S-AVA is defined as a short-term region track associated with regional visual features and background audio features. An effective algorithm, named Short-Term Region tracking with joint Point Tracking and Region Segmentation (STR-PTRS), is developed to extract S-AVAs from generic videos under challenging conditions such as uneven lighting, clutter, occlusions, and complicated motions of both objects and camera. Discriminative audio-visual codebooks are constructed on top of S-AVAs using Multiple Instance Learning. Codebook-based features are generated for semantic concept detection. We extensively evaluate our algorithm over Kodak's consumer benchmark video set from real users. Experimental results confirm significant performance improvements - over 120% MAP gain compared to alternative approaches using static region segmentation without temporal tracking. The joint audio-visual features also outperform visual features alone by an average of 8.5% (in terms of AP) over 21 concepts, with many concepts achieving more than 20%.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {5–14},
numpages = {10},
keywords = {audio-visual codebook, semantic concept detection, short-term audio-visual atom, joint audio-visual analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631278,
author = {Zha, Zheng-Jun and Yang, Linjun and Mei, Tao and Wang, Meng and Wang, Zengfu},
title = {Visual Query Suggestion},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631278},
doi = {10.1145/1631272.1631278},
abstract = {Query suggestion is an effective approach to improve the usability of image search. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This paper proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. It provides a more effective query interface to formulate an intent-specific query by joint text and image suggestions. We show that VQS is able to more precisely and more quickly help users specify and deliver their search intents. When a user submits a text query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. If the user selects one of the suggestions, the corresponding keyword will be added to complement the initial text query as the new text query, while the image collection will be formulated as the visual query. VQS then performs image search based on the new text query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS with three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and search performance.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {15–24},
numpages = {10},
keywords = {image search., query suggestion},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631279,
author = {Yin, Hao and Liu, Xuening and Zhan, Tongyu and Sekar, Vyas and Qiu, Feng and Lin, Chuang and Zhang, Hui and Li, Bo},
title = {Design and Deployment of a Hybrid CDN-P2P System for Live Video Streaming: Experiences with LiveSky},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631279},
doi = {10.1145/1631272.1631279},
abstract = {We present our design and deployment experiences with LiveSky, a commercially deployed hybrid CDN-P2P live streaming system. CDNs and P2P systems are the common techniques used for live streaming, each having its own set of advantages and disadvantages. LiveSky inherits the best of both worlds: the quality control and reliability of a CDN and the inherent scalability of a P2P system. We address several key challenges in the system design and implementation including (a) dynamic resource scaling while guaranteeing stream quality, (b) providing low startup latency, (c) ease of integration with existing CDN infrastructure, and (d) ensuring network-friendliness and upload fairness in the P2P operation. LiveSky has been commercially deployed and used for several large-scale live streaming events serving more than ten million users in China. We evaluate the performance of LiveSky using data from these real-world deployments. Our results indicate that such a hybrid CDN-P2P system provides quality and user performance comparable to a CDN and effectively scales the system capacity when the user volume exceeds the CDN capacity.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {25–34},
numpages = {10},
keywords = {peer-to-peer, live streaming, content delivery networks},
location = {Beijing, China},
series = {MM '09}
}

@dataset{10.1145/review-1631272.1631279_R45422,
author = {Haring, Guenter},
title = {Review ID:R45422 for DOI: 10.1145/1631272.1631279},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1631272.1631279_R45422}
}

@inproceedings{10.1145/1631272.1631280,
author = {Cherubini, Mauro and de Oliveira, Rodrigo and Oliver, Nuria},
title = {Understanding Near-Duplicate Videos: A User-Centric Approach},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631280},
doi = {10.1145/1631272.1631280},
abstract = {Popular content in video sharing web sites (e.g., YouTube) is usually duplicated. Most scholars define near-duplicate video clips (NDVC) based on non-semantic features (e.g., different image/audio quality), while a few also include semantic features (different videos of similar content). However, it is unclear what features contribute to the human perception of similar videos. Findings of two large scale online surveys (N = 1003) confirm the relevance of both types of features. While some of our findings confirm the adopted definitions of NDVC, other findings are surprising. For example, videos that vary in visual content - by overlaying or inserting additional information - may not be perceived as near-duplicate versions of the original videos. Conversely, two different videos with distinct sounds, people, and scenarios were considered to be NDVC because they shared the same semantics (none of the pairs had additional information). Furthermore, the exact role played by semantics in relation to the features that make videos alike is still an open question. In most cases, participants preferred to see only one of the NDVC in the search results of a video search query and they were more tolerant to changes in the audio than in the video tracks. Finally, we propose a user-centric NDVC definition and present implications for how duplicate content should be dealt with by video sharing websites.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {35–44},
numpages = {10},
keywords = {psychophysical experiment, similarity, NDVC, user study, YouTube, video sharing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251344,
author = {Tian, Qi},
title = {Session Details: Content Track C1: Image Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251344},
doi = {10.1145/3251344},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631282,
author = {Zhang, Lijun and Chen, Chun and Chen, Wei and Bu, Jiajun and Cai, Deng and He, Xiaofei},
title = {Convex Experimental Design Using Manifold Structure for Image Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631282},
doi = {10.1145/1631272.1631282},
abstract = {Content Based Image Retrieval (CBIR) has become one of the most active research areas in computer science. Relevance feedback is often used in CBIR systems to bridge the semantic gap. Typically, users are asked to make relevance judgements on some query results, and the feedback information is then used to re-rank the images in the database. An effective relevance feedback algorithm must provide the users with the most informative images with respect to the ranking function. In this paper, we propose a novel active learning algorithm, called Convex Laplacian Regularized Ioptimal Design (CLapRID), for relevance feedback image retrieval. Our algorithm is based on a regression model which minimizes the least square error on the labeled images and simultaneously preserves the intrinsic geometrical structure of the image space. It selects the most informative images which minimize the average predictive variance. The optimization problem of CLapRID can be cast as a semidefinite programming (SDP) problem, and solved via interior-point methods. Experimental results on COREL database have demonstrate the effectiveness of the proposed algorithm for relevance feedback image retrieval.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {45–54},
numpages = {10},
keywords = {convex optimization, image retrieval, relevance feedback, semidefinite programming, active learning},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631283,
author = {Liu, Yiming and Xu, Dong and Tsang, Ivor W. and Luo, Jiebo},
title = {Using Large-Scale Web Data to Facilitate Textual Query Based Retrieval of Consumer Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631283},
doi = {10.1145/1631272.1631283},
abstract = {The rapid popularization of digital cameras and mobile phone cameras has lead to an explosive growth of consumer photo collections. In this paper, we present a (quasi) real-time textual query based personal photo retrieval system by leveraging millions of web images and their associated rich textual descriptions (captions, categories, etc.). After a user provides a textual query (e.g., "pool"), our system exploits the inverted file method to automatically find the positive web images that are related to the textual query "pool" as well as the negative web images which are irrelevant to the textual query. Based on these automatically retrieved relevant and irrelevant web images, we employ two simple but effective classification methods, k Nearest Neighbor (kNN) and decision stumps, to rank personal consumer photos. To further improve the photo retrieval performance, we propose three new relevance feedback methods via cross-domain learning. These methods effectively utilize both the web images and the consumer images. In particular, our proposed cross-domain learning methods can learn robust classifiers with only a very limited amount of labeled consumer photos from the user by leveraging the pre-learned decision stumps at interactive response time. Extensive experiments on both consumer and professional stock photo datasets demonstrated the effectiveness and efficiency of our system, which is also inherently not limited by any predefined lexicon.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {55–64},
numpages = {10},
keywords = {large-scale web data, cross domain learning, textual query based consumer photo retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631284,
author = {Kuo, Yin-Hsi and Chen, Kuan-Ting and Chiang, Chien-Hsing and Hsu, Winston H.},
title = {Query Expansion for Hash-Based Image Object Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631284},
doi = {10.1145/1631272.1631284},
abstract = {An efficient indexing method is essential for content-based image retrieval with the exponential growth in large-scale videos and photos. Recently, hash-based methods (e.g., locality sensitive hashing - LSH) have been shown efficient for similarity search. We extend such hash-based methods for retrieving images represented by bags of (high-dimensional) feature points. Though promising, the hash-based image object search suffers from low recall rates. To boost the hash-based search quality, we propose two novel expansion strategies - intra-expansion and inter-expansion. The former expands more target feature points similar to those in the query and the latter mines those feature points that shall co-occur with the search targets but not present in the query. We further exploit variations for the proposed methods. Experimenting in two consumer-photo benchmarks, we will show that the proposed expansion methods are complementary to each other and can collaboratively contribute up to 76.3% (average) relative improvement over the original hash-based method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {65–74},
numpages = {10},
keywords = {query expansion, locality sensitive hashing (LSH)},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631285,
author = {Zhang, Shiliang and Tian, Qi and Hua, Gang and Huang, Qingming and Li, Shipeng},
title = {Descriptive Visual Words and Visual Phrases for Image Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631285},
doi = {10.1145/1631272.1631285},
abstract = {The Bag-of-visual Words (BoW) image representation has been applied for various problems in the fields of multimedia and computer vision. The basic idea is to represent images as visual documents composed of repeatable and distinctive visual elements, which are comparable to the words in texts. However, massive experiments show that the commonly used visual words are not as expressive as the text words, which is not desirable because it hinders their effectiveness in various applications. In this paper, Descriptive Visual Words (DVWs) and Descriptive Visual Phrases (DVPs) are proposed as the visual correspondences to text words and phrases, where visual phrases refer to the frequently co-occurring visual word pairs. Since images are the carriers of visual objects and scenes, novel descriptive visual element set can be composed by the visual words and their combinations which are effective in representing certain visual objects or scenes. Based on this idea, a general framework is proposed for generating DVWs and DVPs from classic visual words for various applications. In a large-scale image database containing 1506 object and scene categories, the visual words and visual word pairs descriptive to certain scenes or objects are identified as the DVWs and DVPs. Experiments show that the DVWs and DVPs are compact and descriptive, thus are more comparable with the text words than the classic visual words. We apply the identified DVWs and DVPs in several applications including image retrieval, image re-ranking, and object recognition. The DVW and DVP combination outperforms the classic visual words by 19.5% and 80% in image retrieval and object recognition tasks, respectively. The DVW and DVP based image re-ranking algorithm: DWPRank outperforms the state-of-the-art VisualRank by 12.4% in accuracy and about 11 times faster in efficiency.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {75–84},
numpages = {10},
keywords = {object recognition, bag-of-visual words, image re-ranking},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251345,
author = {Yan, Shuicheng},
title = {Session Details: Content Track C2: Content Analysis Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251345},
doi = {10.1145/3251345},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631287,
author = {Ni, Bingbing and Song, Zheng and Yan, Shuicheng},
title = {Web Image Mining towards Universal Age Estimator},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631287},
doi = {10.1145/1631272.1631287},
abstract = {In this paper, we present an automatic web image mining system towards building a universal human age estimator based on facial information, which is applicable to all ethnic groups and various image qualities. First, a large (&lt;391k) yet noisy human aging image dataset is crawled from the photo sharing website Flickr and Google image search engine based on a set of human age related text queries. Then, within each image, several human face detectors of different implementations are used for robust face detection, and all the detected faces with multiple responses are considered as the multiple instances of a bag (image). An outlier removal step with Principal Component Analysis further refines the image set to about 220k faces, and then a robust multi-instance regressor learning algorithm is proposed to learn the kernel-regression based human age estimator under the scenarios with possibly noisy bags. The proposed system has the following characteristics: 1) no manual human age labeling process is required, and the age information is automatically obtained from the age related queries, 2) the derived human age estimator is universal owing to the diversity and richness of Internet images and thus has good generalization capability, and 3) the age estimator learning process is robust to the noises existing in both Internet images and corresponding age labels. This automatically derived human age estimator is extensively evaluated on three popular benchmark human aging databases, and without taking any images from these benchmark databases as training samples, comparable age estimation accuracies with the state-of-the-art results are achieved.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {85–94},
numpages = {10},
keywords = {multi-instance regression, internet vision, age estimation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631288,
author = {Liu, Qingzhong and Sung, Andrew H. and Qiao, Mengyu},
title = {Novel Stream Mining for Audio Steganalysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631288},
doi = {10.1145/1631272.1631288},
abstract = {In this paper, we present a novel stream data mining for audio steganalysis, based on second order derivative of audio streams. We extract Mel-cepstrum coefficients and Markov transition features on the second order derivative, a support vector machine is applied to the features for discovery of the existence of covert message in digital audios. We also explore the relation between signal complexity and detection performance on digital audios, which has not been studied previously. Our study shows that, in comparison with a recently proposed signal stream based Mel-cepstrum steganalysis, our method prominently improves the detection performance, which is not only related to information-hiding ratio but also signal complexity. Generally speaking, signal stream based Mel-cepstrum audio steganalysis performs well in steganalysis of audios with low signal complexity; it does not work so well on audios with high signal complexity. Our stream mining approach for audio steganalysis gains significant advantage in each category of signal complexity - especially in audios with high signal complexity, and thus improves the state of the art in audio steganalysis.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {95–104},
numpages = {10},
keywords = {SVM, audio, spectrum, steganalysis, markov, signal complexity, mel-cepstrum, steganography, second order derivative},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631289,
author = {Ji, Rongrong and Xie, Xing and Yao, Hongxun and Ma, Wei-Ying},
title = {Mining City Landmarks from Blogs by Graph Modeling},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631289},
doi = {10.1145/1631272.1631289},
abstract = {Recent years have witnessed great prosperity in community-contributed multimedia. Discovering, extracting, and summarizing knowledge from these data enables us to make better sense of the world. In this paper, we report our work on mining famous city landmarks from blogs for personalized tourist suggestions. Our main contribution is a graph modeling framework to discover city landmarks by mining blog photo correlations with community supervision. This modeling fuses context, content, and community information in a style that simulates both static (PageRank) and dynamic (HITS) ranking models to highlight representative data from the consensus of blog users.Preliminary, we identify geographical locations of page contents to harvest city sight photos from Web blogs, based on which we structure these photos into a Scene-View hierarchy* within each city. Our graph modeling consists of two phases: First, within a given scene, we present a PhotoRank algorithm to discover its representative views, which analogizes PageRank to model context and content photo correlations for graph-based popularity propagation. Second, among scenes within each city, we present a Landmark-HITS model to discover city landmarks, which integrates author correlations to infer scene popularity in a semi-supervised reinforcement manner. Based on graph modeling, we further achieve personalized tourist suggestions by the collaborative filtering of tourism logs and author correlations. Based on a real-world dataset from Windows Live Spaces blogs containing nearly 400,000 sight photos, we have deployed our framework in a VisualTourism system, with comparisons to state-of-the-arts. We also investigate how the city popularities, user locations (e.g. Asian or Euro. blog users), and sequential events (e.g. Olympic Games) influence our Landmark discovery results and the tourist suggestion tendencies.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {105–114},
numpages = {10},
keywords = {collaborative filtering, HITS, user study, landmark discovery, location extraction, PageRank, web blog, tourist suggestion},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251346,
author = {Luo, Jiebo},
title = {Session Details: Content Track C3: Image Annotation and Tagging},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251346},
doi = {10.1145/3251346},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631291,
author = {Liu, Xiaobai and Cheng, Bin and Yan, Shuicheng and Tang, Jinhui and Chua, Tat Seng and Jin, Hai},
title = {Label to Region by Bi-Layer Sparsity Priors},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631291},
doi = {10.1145/1631272.1631291},
abstract = {In this work, we investigate how to automatically reassign the manually annotated labels at the image-level to those contextually derived semantic regions. First, we propose a bi-layer sparse coding formulation for uncovering how an image or semantic region can be robustly reconstructed from the over-segmented image patches of an image set. We then harness it for the automatic label to region assignment of the entire image set. The solution to bi-layer sparse coding is achieved by convex l1-norm minimization. The underlying philosophy of bi-layer sparse coding is that an image or semantic region can be sparsely reconstructed via the atomic image patches belonging to the images with common labels, while the robustness in label propagation requires that these selected atomic patches come from very few images. Each layer of sparse coding produces the image label assignment to those selected atomic patches and merged candidate regions based on the shared image labels. The results from all bi-layer sparse codings over all candidate regions are then fused to obtain the entire label to region assignments. Besides, the presenting bi-layer sparse coding framework can be naturally applied to perform image annotation on new test images. Extensive experiments on three public image datasets clearly demonstrate the effectiveness of our proposed framework in both label to region assignment and image annotation tasks.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {115–124},
numpages = {10},
keywords = {image parsing, bi-layer sparse coding, label-to-region assignment, image annotation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631292,
author = {Cao, Liangliang and Yu, Jie and Luo, Jiebo and Huang, Thomas S.},
title = {Enhancing Semantic and Geographic Annotation of Web Images via Logistic Canonical Correlation Regression},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631292},
doi = {10.1145/1631272.1631292},
abstract = {Photo community sites such as Flickr and Picasa Web Album host a massive amount of personal photos with millions of new photos uploaded every month. These photos constitute an overwhelming source of images that require effective management. There is an increasingly imperative need for semantic annotation of these web images. This paper addresses the problem by considering two kinds of annotation: semantic annotation and geographic annotation. Both are useful for image search and retrieval and for facilitating communities and social networks. This paper proposes a novel method of Logistic Canonical Correlation Regression (LCCR) for the annotation task. This model exploits the canonical correlation between heterogeneous features and an annotation lexicon of interest, and builds a generalized annotation engine based on canonical correlations in order to produce enhanced annotation for web images. We validate the effectiveness of our algorithm using a dataset of over 380,000 images tagged with GPS coordinates.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {125–134},
numpages = {10},
keywords = {geographic annotation, canonical correlations, web image annotation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631293,
author = {Wu, Lei and Hoi, Steven C.H. and Jin, Rong and Zhu, Jianke and Yu, Nenghai},
title = {Distance Metric Learning from Uncertain Side Information with Application to Automated Photo Tagging},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631293},
doi = {10.1145/1631272.1631293},
abstract = {Automated photo tagging is essential to make massive unlabeled photos searchable by text search engines. Conventional image annotation approaches, though working reasonably well on small testbeds, are either computationally expensive or inaccurate when dealing with large-scale photo tagging. Recently, with the popularity of social networking websites, we observe a massive number of user-tagged images, referred to as "social images", that are available on the web. Unlike traditional web images, social images often contain tags and other user-generated content, which offer a new opportunity to resolve some long-standing challenges in multimedia. In this work, we aim to address the challenge of large-scale automated photo tagging by exploring the social images. We present a retrieval based approach for automated photo tagging. To tag a test image, the proposed approach first retrieves k social images that share the largest visual similarity with the test image. The tags of the test image are then derived based on the tagging of the similar images. Due to the well-known semantic gap issue, a regular Euclidean distance-based retrieval method often fails to find semantically relevant images. To address the challenge of semantic gap, we propose a novel probabilistic distance metric learning scheme that (1) automatically derives constraints from the uncertain side information, and (2) efficiently learns a distance metric from the derived constraints. We apply the proposed technique to automated photo tagging tasks based on a social image testbed with over 200,000 images crawled from Flickr. Encouraging results show that the proposed technique is effective and promising for automated photo tagging.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {135–144},
numpages = {10},
keywords = {automated photo tagging, uncertain side information, distance metric learning},
location = {Beijing, China},
series = {MM '09}
}

@dataset{10.1145/review-1631272.1631293_R45982,
author = {Damova, Mariana},
title = {Review ID:R45982 for DOI: 10.1145/1631272.1631293},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1631272.1631293_R45982}
}

@inproceedings{10.1145/3251347,
author = {Worring, Marcel},
title = {Session Details: Content Track C4: Video Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251347},
doi = {10.1145/3251347},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631295,
author = {Tan, Hung-Khoon and Ngo, Chong-Wah and Hong, Richard and Chua, Tat-Seng},
title = {Scalable Detection of Partial Near-Duplicate Videos by Visual-Temporal Consistency},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631295},
doi = {10.1145/1631272.1631295},
abstract = {Following the exponential growth of social media, there now exist huge repositories of videos online. Among the huge volumes of videos, there exist large numbers of near-duplicate videos. Most existing techniques either focus on the fast retrieval of full copies or near-duplicates, or consider localization in a heuristic manner. This paper considers the scalable detection and localization of partial near-duplicate videos by jointly considering visual similarity and temporal consistency. Temporal constraints are embedded into a network structure as directed edges. Through the structure, partial alignment is novelly converted into a network flow problem where highly efficient solutions exist. To precisely decide the boundaries of the overlapping segments, pair-wise constraints generated from keypoint matching can be added to the network to iteratively refine the localization result. We demonstrate the effectiveness of partial alignment for three different tasks. The first task links partial segments in full-length movies to videos crawled from YouTube. The second task performs fast web video search, while the third performs near-duplicate shot and copy detection. The experimental result demonstrates the effectiveness and efficiency of the proposed method compared to state-of-the-art techniques.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {145–154},
numpages = {10},
keywords = {partial near-duplicate, network flow, temporal graph},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631296,
author = {Jiang, Yu-Gang and Ngo, Chong-Wah and Chang, Shih-Fu},
title = {Semantic Context Transfer across Heterogeneous Sources for Domain Adaptive Video Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631296},
doi = {10.1145/1631272.1631296},
abstract = {Automatic video search based on semantic concept detectors has recently received significant attention. Since the number of available detectors is much smaller than the size of human vocabulary, one major challenge is to select appropriate detectors to response user queries. In this paper, we propose a novel approach that leverages heterogeneous knowledge sources for domain adaptive video search. First, instead of utilizing WordNet as most existing works, we exploit the context information associated with Flickr images to estimate query-detector similarity. The resulting measurement, named Flickr context similarity (FCS), reflects the co-occurrence statistics of words in image context rather than textual corpus. Starting from an initial detector set determined by FCS, our approach novelly transfers semantic context learned from test data domain to adaptively refine the query-detector similarity. The semantic context transfer process provides an effective means to cope with the domain shift between external knowledge source (e.g., Flickr context) and test data, which is a critical issue in video search. To the best of our knowledge, this work represents the first research aiming to tackle the challenging issue of domain change in video search. Extensive experiments on 120 textual queries over TRECVID 2005-2008 data sets demonstrate the effectiveness of semantic context transfer for domain adaptive video search. Results also show that the FCS is suitable for measuring query-detector similarity, producing better performance to various other popular measures.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {155–164},
numpages = {10},
keywords = {Flickr context similarity, domain adaptive video search, heterogeneous sources, semantic context transfer},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631297,
author = {Zhu, Guangyu and Yang, Ming and Yu, Kai and Xu, Wei and Gong, Yihong},
title = {Detecting Video Events Based on Action Recognition in Complex Scenes Using Spatio-Temporal Descriptor},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631297},
doi = {10.1145/1631272.1631297},
abstract = {Event detection plays an essential role in video content analysis and remains a challenging open problem. In particular, the study on detecting human-related video events in complex scenes with both a crowd of people and dynamic motion is still limited. In this paper, we investigate detecting video events that involve elementary human actions, e.g. making cellphone call, putting an object down, and pointing to something, in complex scenes using a novel spatio-temporal descriptor based approach. A new spatio-temporal descriptor, which temporally integrates the statistics of a set of response maps of low-level features, e.g. image gradients and optical flows, in a space-time cube, is proposed to capture the characteristics of actions in terms of their appearance and motion patterns. Based on this kind of descriptors, the bag-of-words method is utilized to describe a human figure as a concise feature vector. Then, these features are employed to train SVM classifiers at multiple spatial pyramid levels to distinguish different actions. Finally, a Gaussian kernel based temporal filtering is conducted to segment the sequences of events from a video stream taking account of the temporal consistency of actions. The proposed approach is capable of tolerating spatial layout variations and local deformations of human actions due to diverse view angles and rough human figure alignment in complex scenes. Extensive experiments on the 50-hour video dataset of TRECVid 2008 event detection task demonstrate that our approach outperforms the well-known SIFT descriptor based methods and effectively detects video events in challenging real-world conditions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {165–174},
numpages = {10},
keywords = {motion representation, action recognition, event detection, semantic analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631298,
author = {Yang, Yi and Xu, Dong and Nie, Feiping and Luo, Jiebo and Zhuang, Yueting},
title = {Ranking with Local Regression and Global Alignment for Cross Media Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631298},
doi = {10.1145/1631272.1631298},
abstract = {Rich multimedia content including images, audio and text are frequently used to describe the same semantics in E-Learning and Ebusiness web pages, instructive slides, multimedia cyclopedias, and so on. In this paper, we present a framework for cross-media retrieval, where the query example and the retrieved result(s) can be of different media types. We first construct Multimedia Correlation Space (MMCS) by exploring the semantic correlation of different multimedia modalities, during which multimedia content and co-occurrence information is utilized. We propose a novel ranking algorithm, namely ranking with Local Regression and Global Alignment (LRGA), which learns a robust Laplacian matrix for data ranking. In LRGA, for each data point, a local linear regression model is used to predict the ranking values of its neighboring points. We propose a unified objective function to globally align the local models from all the data points so that an optimal ranking value can be assigned to each data point. LRGA is insensitive to parameters, making it particularly suitable for data ranking. A relevance feedback algorithm is proposed to improve the retrieval performance. Comprehensive experiments have demonstrated the effectiveness of our methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {175–184},
numpages = {10},
keywords = {ranking algorithm, relevance feedback, content-based multimedia retrieval, cross-media retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251348,
author = {Zhang, Zhengyou},
title = {Session Details: Content Track C5: Audio and Music},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251348},
doi = {10.1145/3251348},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631300,
author = {Stadelmann, Thilo and Freisleben, Bernd},
title = {Unfolding Speaker Clustering Potential: A Biomimetic Approach},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631300},
doi = {10.1145/1631272.1631300},
abstract = {Speaker clustering is the task of grouping a set of speech utterances into speaker-specific classes. The basic techniques for solving this task are similar to those used for speaker verification and identification. The hypothesis of this paper is that the techniques originally developed for speaker verification and identification are not sufficiently discriminative for speaker clustering. However, the processing chain for speaker clustering is quite large - there are many potential areas for improvement. The question is: where should improvements be made to improve the final result? To answer this question, this paper takes a biomimetic approach based on a study with human participants acting as an automatic speaker clustering system. Our findings are twofold: it is the stage of modeling that has the highest potential, and information with respect to the temporal succession of frames is crucially missing. Experimental results with our implementation of a speaker clustering system incorporating our findings and applying it on TIMIT data show the validity of our approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {185–194},
numpages = {10},
keywords = {speaker identification, speaker diarization, GMM, MFCC, one-class SVM, temporal context, speaker clustering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631301,
author = {Friedland, Gerald and Yeo, Chuohao and Hung, Hayley},
title = {Visual Speaker Localization Aided by Acoustic Models},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631301},
doi = {10.1145/1631272.1631301},
abstract = {The following paper presents a novel audio-visual approach for unsupervised speaker locationing. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the art audio-only speaker localization system (traditionally called speaker diarization) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the number of speakers and estimates "who spoke when", then, in a second step, the visual models are used to infer the location of the speakers in the video. The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker locationing at little incremental engineering and computation costs.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {195–202},
numpages = {8},
keywords = {multimodal integration, visual localization, speaker diarization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631302,
author = {Yasuraoka, Naoki and Abe, Takehiro and Itoyama, Katsutoshi and Takahashi, Toru and Ogata, Tetsuya and Okuno, Hiroshi G.},
title = {Changing Timbre and Phrase in Existing Musical Performances as You like: Manipulations of Single Part Using Harmonic and Inharmonic Models},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631302},
doi = {10.1145/1631272.1631302},
abstract = {This paper presents a new music manipulation method that can change the timbre and phrases of an existing instrumental performance in a polyphonic sound mixture. This method consists of three primitive functions: 1) extracting and analyzing of a single instrumental part from polyphonic music signals, 2) mixing the instrument timbre with another, and 3) rendering a new phrase expression for another given score. The resulting customized part is re-mixed with the remaining parts of the original performance to generate new polyphonic music signals. A single instrumental part is extracted by using an integrated tone model that consists of harmonic and inharmonic tone models with the aid of the score of the single instrumental part. The extraction incorporates a residual model for the single instrumental part in order to avoid crosstalk between instrumental parts. The extracted model parameters are classified into their averages and deviations. The former is treated as instrument timbre and is customized by mixing, while the latter is treated as phrase expression and is customized by rendering. We evaluated our method in three experiments. The first experiment focused on introduction of the residual model, and it showed that the model parameters are estimated more accurately by 35.0 points. The second focused on timbral customization, and it showed that our method is more robust by 42.9 points in spectral distance compared with a conventional sound analysis-synthesis method, STRAIGHT. The third focused on the acoustic fidelity of customizing performance, and it showed that rendering phrase expression according to the note sequence leads to more accurate performance by 9.2 points in spectral distance in comparison with a rendering method that ignores the note sequence.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {203–212},
numpages = {10},
keywords = {sound source extraction, performance rendering, timbre mixing, signal processing, music manipulation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631303,
author = {Zhang, Bingjun and Xiang, Qiaoliang and Lu, Huanhuan and Shen, Jialie and Wang, Ye},
title = {Comprehensive Query-Dependent Fusion Using Regression-on-Folksonomies: A Case Study of Multimodal Music Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631303},
doi = {10.1145/1631272.1631303},
abstract = {The combination of heterogeneous knowledge sources has been widely regarded as an effective approach to boost retrieval accuracy in many information retrieval domains. While various technologies have been recently developed for information retrieval, multimodal music search has not kept pace with the enormous growth of data on the Internet. In this paper, we study the problem of integrating multiple online information sources to conduct effective query dependent fusion (QDF) of multiple search experts for music retrieval. We have developed a novel framework to construct a knowledge space of users' information need from online folksonomy data. With this innovation, a large number of comprehensive queries can be automatically constructed to train a better generalized QDF system against unseen user queries. In addition, our framework models QDF problem by regression of the optimal combination strategy on a query. Distinguished from the previous approaches, the regression model of QDF (RQDF) offers superior modeling capability with less constraints and more efficient computation. To validate our approach, a large scale test collection has been collected from different online sources, such as Last.fm, Wikipedia, and YouTube. All test data will be released to the public for better research synergy in multimodal music search. Our performance study indicates that the accuracy, efficiency, and robustness of the multimodal music search can be improved significantly by the proposed folksonomy-RQDF approach. In addition, since no human involvement is required to collect training examples, our approach offers great feasibility and practicality in system development.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {213–222},
numpages = {10},
keywords = {multimodal search, music, query-dependent fusion, folksonomy},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251349,
author = {Del Bimbo, Alberto},
title = {Session Details: Content Track C6: Learning and Concept Detection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251349},
doi = {10.1145/3251349},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631305,
author = {Tang, Jinhui and Yan, Shuicheng and Hong, Richang and Qi, Guo-Jun and Chua, Tat-Seng},
title = {Inferring Semantic Concepts from Community-Contributed Images and Noisy Tags},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631305},
doi = {10.1145/1631272.1631305},
abstract = {In this paper, we exploit the problem of inferring images' semantic concepts from community-contributed images and their associated noisy tags. To infer the concepts more accurately, we propose a novel sparse graph-based semi-supervised learning approach for harnessing the labeled and unlabeled data simultaneously. The sparse graph constructed by datum-wise one-vs-all sparse reconstructions of all samples can remove most of the concept-unrelated links among the data, thus is more robust and discriminative than conventional graphs. More importantly, we propose an effective training label refinement strategy within this graph-based learning framework to handle the noise in the tags, by bringing in a dual regularization for both the quantity and sparsity of the noise. In addition, we construct an informative compact concept space with small semantic gap to infer the semantic concepts in this space to bridge the semantic gap. The relations among different concepts are inherently embedded in this space to help the concept inference. We conduct extensive experiments on a real-world community-contributed image database consisting of 55,615 Flickr images and associated tags. The results demonstrate the effectiveness of the proposed approaches and the capability of our method to deal with the noise in the tags. We further show that we could achieve comparable performance by inferring semantic concepts from training data with noisy tags versus training data with clean ground-truth labels.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {223–232},
numpages = {10},
keywords = {semi-supervised learning, noisy tags, web image, sparse graph, concept space},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631306,
author = {Aly, Robin and Hiemstra, Djoerd},
title = {Concept Detectors: How Good is Good Enough?},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631306},
doi = {10.1145/1631272.1631306},
abstract = {Today, semantic concept based video retrieval systems often show insufficient performance for real-life applications. Clearly, a big share of the reason is the lacking performance of the detectors of these concepts. While concept detectors are on their endeavor to improve, following important questions need to be addressed: "How good do detectors need to be to produce usable search systems?" and "How does the detector performance influence different concept combination methods?". We use Monte Carlo Simulations to provide answers to the above questions. The main contribution of this paper is a probabilistic model of detectors which outputs confidence scores to indicate the likelihood of a concept to occur. This score is also converted into a posterior probability and a binary classification. We investigate the influence of changes to the model's parameters on the performance of multiple concept combination methods. Current web search engines produce a mean average precision (MAP) of around 0.20. Our simulation reveals that the best performing video search method achieve this performance using detectors with 0.60 MAP and is therefore usable in real-life. Furthermore, perfect detection allows the best performing combination method to produce 0.39 search MAP in an artificial environment with Oracle settings. We also find that MAP is not necessarily a good evaluation measure for concept detectors since it is not always correlated with search performance.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {233–242},
numpages = {10},
keywords = {detector simulation, video information retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631307,
author = {Qi, Guo-Jun and Hua, Xian-Sheng and Zhang, Hong-Jiang},
title = {Learning Semantic Distance from Community-Tagged Media Collection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631307},
doi = {10.1145/1631272.1631307},
abstract = {This paper proposes a novel semantic-aware distance metric for images by mining multimedia data on the Internet, in particular, web images and their associated tags. As well known, a proper distance metric between images is a key ingredient in many realistic web image retrieval engines, as well many image understanding techniques. In this paper, we attempt to mine a novel distance metric from the web images by integrating their visual content as well as the associated user tags. Different from many existing distance metric learning algorithms which utilize the dissimilar or similar information between images pixels or features in signal level, the proposed scheme also takes the associated user-input tags into consideration. The visual content of images is also leveraged to respect an intuitive assumption that the visual similar images ought to have a smaller distance. A semi-definite programming is formulated to encode the above two aspects of criteria to learn the distance metric and we show such an optimization problem can be efficiently solved with a closed-form solution. We evaluate the proposed algorithm on two datasets. One is the benchmark Corel dataset and the other is a real-world dataset crawled from the image sharing website Flickr. By comparison with other existing distance learning algorithms, competitive results are obtained by the proposed algorithm in experiments.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {243–252},
numpages = {10},
keywords = {distance metric learning, web image search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251350,
author = {Haenselmann, Thomas},
title = {Session Details: Application Track A1: Interactive Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251350},
doi = {10.1145/3251350},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631309,
author = {Lin, Jin-Yao and Chen, Yen-Yu and Ko, Ju-Chun and Kao, HuiShan and Chen, Wei-Han and Tsai, Tsun-Hung and Hsu, Su-Chu and Hung, Yi-Ping},
title = {I-m-Tube: An Interactive Multi-Resolution Tubular Display},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631309},
doi = {10.1145/1631272.1631309},
abstract = {In this paper we introduce a tubular interface, the i-m-Tube, which provides convenient user interaction with multimedia content by multi-touch input and multi-resolution display. With its tubular surface, the i-m-Tube is suitable for displaying panoramic image content like the Chinese scroll painting "Along the River During the Ch'ingming Festival" which has been regarded as a national treasure and widely known by its extraordinary width and its various details. The strength of our system is that it is not just suitable for displaying panoramic content, but also possible to create an intuitive and natural context for user interaction with multimedia content displayed on the i-m-Tube by multi-finger touch. It also provides a high resolution area on demand, which is projected by a steerable projector in a Full-HD resolution. We have developed our system as an interface which can be applied adaptively to different applications, having the features of multi-touch and multi-resolution interactivity on a tubular display. In this work, we have implemented several prototypes designed for different applications, like "Tetris360" aiming on co-op arcade gaming, and "Panoramic Cover Flow" for interactive digital signage, to show the capabilities and opportunities of the i-m-Tube display system. With our preliminary experiments on the i-m-Tube, it is exciting to see the potential of the i-m-Tube as a new interface for people to meet the interactive multimedia touch in this generation.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {253–260},
numpages = {8},
keywords = {multi-touch display, virtual museum, interactive multimedia, multi-resolution display, e-heritage},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631310,
author = {Lin, Wei-Hao and Haputmann, Alexander},
title = {Identifying News Videos' Ideological Perspectives Using Emphatic Patterns of Visual Concepts},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631310},
doi = {10.1145/1631272.1631310},
abstract = {Television news has become the predominant way of understanding the world around us, but individual news broadcasters can frame or mislead an audience's understanding of political and social issues. We are developing a computer system that can automatically identify highly biased television news and encourage audiences to seek news stories from contrasting viewpoints. But can computers identify the ideological perspective from which a news video was produced? We propose a method based on an empathic pattern of visual concepts: news broadcasters holding contrasting ideological beliefs appear to emphasize different subsets of visual concepts. We formalize the emphatic patterns and propose a statistical model. We evaluate the proposed model on a large broadcast news video archive with promising experimental results.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {261–270},
numpages = {10},
keywords = {machine learning, ideological perspectives, classification, visual concept, broadcast news},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631311,
author = {Taylor, Micah T. and Chandak, Anish and Antani, Lakulish and Manocha, Dinesh},
title = {RESound: Interactive Sound Rendering for Dynamic Virtual Environments},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631311},
doi = {10.1145/1631272.1631311},
abstract = {We present an interactive algorithm and system (RESound) for sound propagation and rendering in virtual environments and media applications. RESound uses geometric propagation techniques for fast computation of propagation paths from a source to a listener and takes into account specular reflections, diffuse reflections, and edge diffraction. In order to perform fast path computation, we use a unified ray-based representation to efficiently trace discrete rays as well as volumetric ray-frusta. RESound further improves sound quality by using statistical reverberation estimation techniques. We also present an interactive audio rendering algorithm to generate spatialized audio signals. The overall approach can render sound in dynamic scenes allowing source, listener, and obstacle motion. Moreover, our algorithm is relatively easy to parallelize on multi-core systems. We demonstrate its performance on complex game-like and architectural environments.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {271–280},
numpages = {10},
keywords = {raytracing, acoustics, sound},
location = {Beijing, China},
series = {MM '09}
}

@dataset{10.1145/review-1631272.1631311_R46312,
author = {Jouvelot, Pierre},
title = {Review ID:R46312 for DOI: 10.1145/1631272.1631311},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1631272.1631311_R46312}
}

@inproceedings{10.1145/1631272.1631312,
author = {Soares, Luiz Fernando Gomes and Costa, Romualdo M.R. and Moreno, Marcio Ferreira and Moreno, Marcelo F.},
title = {Multiple Exhibition Devices in DTV Systems},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631312},
doi = {10.1145/1631272.1631312},
abstract = {Nested Context Language (NCL) is the declarative language of the Brazilian Terrestrial Digital TV System. NCL is part of ISDB (International Standard for Digital Broadcasting) standards and also the ITU-T Recommendation H.761 for IPTV services. This paper presents, discusses, and illustrates the NCL hierarchical control model for multiple exhibition device support. Based on this model, multiple devices are orchestrated to run a DTV application in cooperation. Two types of multiple device exhibitions are distinguished. Those where the same content is shown in a set of devices under a unique control, and those where content is under each individual device control, working completely independent. In this last case, depending on viewer interactions, the resulting presented content can differ from a device to another. Examples of NCL applications using both options are presented and discussed.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {281–290},
numpages = {10},
keywords = {synchronization, digital TV, ginga, DTV middleware, NCL, multiple devices},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251351,
author = {Atrey, Pradeep K.},
title = {Session Details: Application Track A2: Context Awareness},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251351},
doi = {10.1145/3251351},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631314,
author = {Wang, Meng and Liu, Bo and Hua, Xian-Sheng},
title = {Accessible Image Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631314},
doi = {10.1145/1631272.1631314},
abstract = {There are about 8% of men and 0.8% of women suffering from colorblindness. We show that the existing image search techniques cannot provide satisfactory results for these users, since many images will not be well perceived by them due to the loss of color information. In this paper, we introduce a scheme named Accessible Image Search (AIS) to accommodate these users. Different from the general image search scheme that aims at returning more relevant results, AIS further takes into account the colorblind accessibilities of the returned results, i.e., the image qualities in the eyes of colorblind users. The scheme includes two components: accessibility assessment and accessibility improvement. For accessibility assessment, we introduce an analysisbased method and a learning-based method. Based on the measured accessibility scores, different reranking methods can be performed to prioritize the images with high accessibilities. In accessibility improvement component, we propose an efficient recoloring algorithm to modify the colors of the images such that they can be better perceived by colorblind users. We also propose the Accessibility Average Precision (AAP) for AIS as a complementary performance evaluation measure to the conventional relevance-based evaluation methods. Experimental results with more than 60,000 images and 20 anonymous colorblind users demonstrate the effectiveness and usefulness of the proposed scheme.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {291–300},
numpages = {10},
keywords = {colorblindness, image search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631315,
author = {Shi, Liang and Wang, Jinqiao and Duan, Lingyu and Lu, Hanqing},
title = {Consumer Video Retargeting: Context Assisted Spatial-Temporal Grid Optimization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631315},
doi = {10.1145/1631272.1631315},
abstract = {Pervasive multimedia devices require accurate video retargeting, especially in connected consumer electronics platforms. In this paper, we present a context assisted spatialtemporal grid scheme for consumer video retargeting. First, we parse consumer videos from low-level features to highlevel visual concepts, combining visual attention into a more accurate importance description. Then, a semantic importance map is built up representing the spatial importance and temporal continuity, which is incorporated with a 3D rectilinear grid scaleplate to map frames to the target display, thereby keeping the aspect ratio of semantically salient objects as well as the perceptual coherency. Extensive evaluations were done on two popular video genres, sports and advertisements. The comparison with state-of-the-art approaches on both images and videos have demonstrated the advantages of the proposed approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {301–310},
numpages = {10},
keywords = {context analysis, video retargeting},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631316,
author = {Yang, Yi and Zhuang, Yueting and Xu, Dong and Pan, Yunhe and Tao, Dacheng and Maybank, Steve},
title = {Retrieval Based Interactive Cartoon Synthesis via Unsupervised Bi-Distance Metric Learning},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631316},
doi = {10.1145/1631272.1631316},
abstract = {Cartoons play important roles in many areas, but it requires a lot of labor to produce new cartoon clips. In this paper, we propose a gesture recognition method for cartoon character images with two applications, namely content-based cartoon image retrieval and cartoon clip synthesis. We first define Edge Features (EF) and Motion Direction Features (MDF) for cartoon character images. The features are classified into two different groups, namely intra-features and inter-features. An Unsupervised Bi-Distance Metric Learning (UBDML) algorithm is proposed to recognize the gestures of cartoon character images. Different from the previous research efforts on distance metric learning, UBDML learns the optimal distance metric from the heterogeneous distance metrics derived from intra-features and inter-features. Content-based cartoon character image retrieval and cartoon clip synthesis can be carried out based on the distance metric learned by UBDML. Experiments show that the cartoon character image retrieval has a high precision and that the cartoon clip synthesis can be carried out efficiently.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {311–320},
numpages = {10},
keywords = {cartoon image, image retrieval, cartoon clip synthesis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631317,
author = {Kopf, Stephan and Kiess, Johannes and Lemelson, Hendrik and Effelsberg, Wolfgang},
title = {FSCAV: Fast Seam Carving for Size Adaptation of Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631317},
doi = {10.1145/1631272.1631317},
abstract = {The presentation of multimedia data and especially of high resolution videos on small mobile devices is still a great challenge today. Both cropping of borders and scaling of frames may result in the removal of essential content of videos or lost details due to the reduced size of the visual content. Another major problem emerges if the aspect ratio of the original video and the display of the mobile device differ. User evaluations indicate that changing the aspect ratio may reduce the visual quality of videos significantly. In this paper, we present the new FSCAV algorithm (Fast Seam Carving for Size Adaptation of Videos) to adapt the size of videos to the limited display resolution and different aspect ratios of handheld mobile devices. The general idea of the seam carving algorithm for still images is to remove seams in images so that the essential content is preserved. We extended this technique which works very well for images to create videos without jitter or visible artifacts. A major feature of our FSCAV algorithm is the low computational complexity which enables an efficient adaptation of videos to small screens. Nevertheless, severe distortions are clearly visible in some shots of the adapted videos. We present a new heuristic to identify shots with such a low visual quality. If the quality drops below a threshold, a different adaptation technique is used for this shot (e.g., scaling or cropping). User evaluations confirm a very high visual quality of our approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {321–330},
numpages = {10},
keywords = {seam carving, video retargeting, video adaptation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251352,
author = {Mei, Tao},
title = {Session Details: Applications Track A3: Information Summarization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251352},
doi = {10.1145/3251352},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631319,
author = {Thies, William and Hall, Steven and Amarasinghe, Saman},
title = {Manipulating Lossless Video in the Compressed Domain},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631319},
doi = {10.1145/1631272.1631319},
abstract = {A compressed-domain transformation is one that operates directly on the compressed format, rather than requiring conversion to an uncompressed format prior to processing. Performing operations in the compressed domain offers large speedups, as it reduces the volume of data processed and avoids the overhead of re-compression.While previous researchers have focused on compressed-domain techniques for lossy data formats, there are few techniques that apply to lossless formats. In this paper, we present a general technique for transforming lossless data as compressed with the sliding-window Lempel Ziv algorithm (LZ77). We focus on applications in video editing, where our technique supports color adjustment, video compositing, and other operations directly on the Apple Animation format (a variant of LZ77).We implemented a subset of our technique as an automatic program transformation. Using the StreamIt language, users write a program to operate on uncompressed data, and our compiler transforms the program to operate on compressed data. Experiments show that the technique offers speedups roughly proportional to the compression factor. For our benchmark suite of 12 videos in Apple Animation format, speedups range from 1.1x to 471x, with a median of 15x.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {331–340},
numpages = {10},
keywords = {lossless compression, compressed domain, streamit, stream programming, Lempel-Ziv, video editing, LZ77, synchronous dataflow},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631320,
author = {Yu, Yi and Crucianu, Michel and Oria, Vincent and Chen, Lei},
title = {Local Summarization and Multi-Level LSH for Retrieving Multi-Variant Audio Tracks},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631320},
doi = {10.1145/1631272.1631320},
abstract = {In this paper we study the problem of detecting and grouping multi-variant audio tracks in large audio datasets. To address this issue, a fast and reliable retrieval method is necessary. But reliability requires elaborate representations of audio content, which challenges fast retrieval by similarity from a large audio database. To find a better tradeoff between retrieval quality and efficiency, we put forward an approach relying on local summarization and multi-level Locality-Sensitive Hashing (LSH). More precisely, each audio track is divided into multiple Continuously Correlated Periods (CCP) of variable length according to spectral similarity. The description for each CCP is calculated based on its Weighted Mean Chroma (WMC). A track is thus represented as a sequence of WMCs. Then, an adapted two-level LSH is employed for efficiently delineating a narrow relevant search region. The "coarse" hashing level restricts search to items having a non-negligible similarity to the query. The subsequent, "refined" level only returns items showing a much higher similarity. Experimental evaluations performed on a real multi-variant audio dataset confirm that our approach supports fast and reliable retrieval of audio track variants.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {341–350},
numpages = {10},
keywords = {local audio summarization, multi-level LSH, multi-variant musical audio search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631321,
author = {Tang, Lin-Xie and Mei, Tao and Hua, Xian-Sheng},
title = {Near-Lossless Video Summarization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631321},
doi = {10.1145/1631272.1631321},
abstract = {The daunting yet increasing volume of videos on the Internet brings the challenges of storage and indexing to existing online video services. Current techniques like video compression and summarization are still struggling to achieve the two often conflicting goals of low storage and high visual and semantic fidelity. In this work, we develop a new system for video summarization, called "Near-Lossless Video Summarization" (NLVS), which is able to summarize a video stream with the least information loss by using an extremely small piece of metadata. The summary consists of a set of synthesized mosaics and representative keyframes, a compressed audio stream, as well as the metadata about video structure and motion. Although at a very low compression ratio (i.e., 1/30 of H.264 baseline in average, where traditional compression techniques like H.264 fail to preserve the fidelity), the summary still can be used to reconstruct the original video (with the same duration) nearly without semantic information loss. We show that NLVS is a powerful tool for significantly reducing video storage through both objective and subjective comparisons with state-of-the-art video compression and summarization techniques.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {351–360},
numpages = {10},
keywords = {video storage, video summarization, online video service},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251353,
author = {Griwodz, Carsten},
title = {Session Details: System Track S1: Mobile Devices and Hardware/Sensor Support},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251353},
doi = {10.1145/3251353},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631323,
author = {Huang, Yen-Lin and Shen, Yun-Chung and Wu, Ja-Ling},
title = {Scalable Computation for Spatially Scalable Video Coding Using NVIDIA CUDA and Multi-Core CPU},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631323},
doi = {10.1145/1631272.1631323},
abstract = {The scalable video coding (SVC), an extension of H.264/MPEG4-AVC (H.264), was standardized in 2007 by Joint Video Team (JVT). SVC provides spatial, temporal and SNR scalabilities. To achieve these scalabilities, SVC uses additional coding tools and coding modes based on H.264. The coding tools used by SVC and the variety coding modes decision make the corresponding coding complexity become extremely high, so real-time realization of SVC is nearly impossible by using software and single-core CPU only. One possible solution to generate SVC streams in real-time is to parallelize the whole encoding process. Currently, multi-core CPU and GPU are two popular kinds of parallel processing architectures. Not much research has been devoted to realize the parallel SVC encoders based on the co-work of these two architectures. In this paper, a scalable computation model for spatial SVC using multi-core CPU and GPGPU through NVIDIA CUDA is proposed. On the basis of the proposed computational model, a solution to solve the challenging data transition problem (will be detailed later) of this CPU-GPU co-work architecture is then provided. Simulation results show that, through our work, significant speed up gain in spatial SVC encoding can be achieved.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {361–370},
numpages = {10},
keywords = {parallel computing, SVC, GPU, CUDA, multi-core},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631324,
author = {Wu, Nan and Wen, Mei and Wu, Wei and Ren, Ju and Su, Huayou and Xun, Changqing and Zhang, Chunyuan},
title = {Streaming HD H.264 Encoder on Programmable Processors},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631324},
doi = {10.1145/1631272.1631324},
abstract = {Programmable processors have great advantage over dedicated ASIC design under intense time-to-market pressure. However, real-time encoding of high-definition (HD) H.264 video (up to 1080p) is a challenge to most existing programmable processors. On the other hand, model-based design is widely accepted in developing complex media program. Stream model, an emerging model-based programming method, shows surprising efficiency on many compute-intensive domains especially for media processing. On the basis, this paper proposes a set of streaming techniques for H.264 encoding, and then develops all of the code based on the X264 reference code. Our streaming H.264 encoder is a pure software implementation completely written in high-level language without special hardware/algorithm support. Real execution results show that our encoder achieves significant speedup over the original X264 encoder on various programmable architectures: on X86 CoreTM2 E8200 the speedup is 1.8x, on MIPS 4KEc the speedup is 3.7x, on TMS320 C6416 DSP the speedup is 5.5x, on stream processor STORM-SP16 G220 the speedup is 6.1x. Especially, on STORM processor, the streaming encoder achieves the performance of 30.6 frames per second for a 1080P HD sequence, satisfying the real-time requirement. These indicate that streaming is extremely efficient for this kind of media workload. Our work is also applicable for other media processing applications, and provides architecture insights into dedicated ASIC or FPGA HD H.264 encoders.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {371–380},
numpages = {10},
keywords = {stream, H.264 encoder, 1080P HD, real-time, programmable},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631325,
author = {Chen, Xiaoming and Zhao, Zhendong and Rahmati, Ahmad and Wang, Ye and Zhong, Lin},
title = {SaVE: Sensor-Assisted Motion Estimation for Efficient h.264/AVC Video Encoding},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631325},
doi = {10.1145/1631272.1631325},
abstract = {Motion estimation is a key component of modern video encoding and is very compute-intensive. We present a novel Sensor-assisted Video Encoding (SaVE) method to reduce the computational complexity of motion estimation in H.264/AVC encoders, leveraging accelerometers and digital compasses that are increasingly available on mobile devices. Using these sensors, SaVE calculates the rotational movement of a camera and then infers the global motion in the camera image sensor; it subsequently employs the estimated global motion to simplify the state-of-the-art motion estimation algorithms, UMHS and EPZS used in H.264/AVC encoders. We have constructed a prototype of SaVE and report extensive evaluation of it. Our experimental results show that SaVE can reduce the computations of UMHS and EPZS algorithms by up to 27% and 18%, respectively, while achieving the same or better video quality.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {381–390},
numpages = {10},
keywords = {sensor, video encoding, digital compass, motion estimation, MPEG, accelerometer, H.264/AVC},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631326,
author = {Shi, Shu and Jeon, Won J. and Nahrstedt, Klara and Campbell, Roy H.},
title = {Real-Time Remote Rendering of 3D Video for Mobile Devices},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631326},
doi = {10.1145/1631272.1631326},
abstract = {At the convergence of computer vision, graphics, and multimedia, the emerging 3D video technology promises immersive experiences in a truly seamless environment. However, the requirements of huge network bandwidth and computing resources make it still a big challenge to render 3D video on mobile devices at real-time. In this paper, we present how remote rendering framework can be used to solve the problem. The differences between dynamic 3D video and static graphic models are analyzed. A general proxy-based framework is presented to render 3D video streams on the proxy and transmit the rendered scene to mobile devices over a wireless network. An image-based approach is proposed to enhance 3D interactivity and reduce the interaction delay. Experiments prove that the remote rendering framework can be effectively used for quality 3D video rendering on mobile devices in real time.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {391–400},
numpages = {10},
keywords = {remote rendering, 3D video, mobile devices},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251354,
author = {Liu, Yong},
title = {Session Details: System Track S2: Media Streaming and Media Content Distribution},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251354},
doi = {10.1145/3251354},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631328,
author = {Li, Zhi and Zhu, Xiaoqing and Begen, Ali C. and Girod, Bernd},
title = {Peer-Assisted Packet Loss Repair for IPTV Video Multicast},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631328},
doi = {10.1145/1631272.1631328},
abstract = {Emerging IPTV technology uses source-specific IP multicast to deliver TV programs to the end-users. To provide timely and reliable services over the error-prone DSL access networks, a combination of multicast forward error correction and unicast retransmissions is employed to mitigate the impact of impulse noise. In current systems, the retransmission function is provided by the Retransmission Servers.We propose an alternative distributed solution where the burden of loss repair is partially shifted to the peer IP set-top boxes. Through the Peer-assisted Repair (PAR) protocol, we demonstrate how the packet repairs can be delivered in a timely, reliable and decentralized manner using the combination of server-peer coordination and coded redundant repairs. We show through analysis and simulations that this new solution not only effectively mitigates the bottleneck experienced by the Retransmission Servers, thus greatly improving the scalability of the system, but also efficiently deals with peer departures and other uncertainties.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {401–410},
numpages = {10},
keywords = {scalability, peer-to-peer, impulse noise, reliable multicast, error-resilient video, IPTV},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631329,
author = {Hsu, Cheng-Hsin and Hefeeda, Mohamed},
title = {On Statistical Multiplexing of Variable-Bit-Rate Video Streams in Mobile Systems},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631329},
doi = {10.1145/1631272.1631329},
abstract = {We consider the problem of broadcasting multiple variable-bit-rate (VBR) video streams from a base station to many mobile devices over a wireless network, so that: (i) perceived quality on mobile devices is maximized, (ii) bandwidth utilization is maximized, and (iii) energy consumption of mobile devices is minimized. We show that this problem is NP-Complete. We propose an approximation algorithm for the base station to statistically multiplex and transmit multiple VBR streams to achieve these objectives. We analytically analyze the performance of our algorithm and prove that it achieves optimal bandwidth utilization and near-optimal energy saving. Our algorithm frees network operators from the manual and error-prone bandwidth reservation process, which is usually used in practice for broadcasting VBR streams. We implement the proposed algorithm in a trace-driven simulator, and conduct extensive simulations. The simulation results show that our algorithm outperforms the existing algorithms in many aspects, including number of late frames, number of concurrently broadcast video streams, and energy saving of mobile devices. We also implement the proposed algorithm in a real testbed for video broadcasting as a proof of concept. The results from the testbed confirm that the proposed algorithm: (i) does not result in playout glitches, (ii) achieves high energy saving, and (iii) runs in real time.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {411–420},
numpages = {10},
keywords = {transmission scheduling, channel utilization, energy saving, mobile broadcast networks, quality of service},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631330,
author = {Aggarwal, Vaneet and Caldebank, Robert and Gopalakrishnan, Vijay and Jana, Rittwik and Ramakrishnan, K. K. and Yu, Fang},
title = {The Effectiveness of Intelligent Scheduling for Multicast Video-on-Demand},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631330},
doi = {10.1145/1631272.1631330},
abstract = {As more and more video content is made available and accessed on-demand, content and service providers face challenges of scale. Today's delivery mechanisms, especially unicast, require resources to scale linearly with the number of receivers and library sizes. Unlike these mechanisms, with multicast, the load on a server is relatively independent of the number of receivers. Adopting multicast for on-demand access, however, is challenging because of the need to temporally aggregate requests. In this paper, we investigate the importance of an intelligent scheduler and a good data model for achieving good aggregation of requests into multicast groups. We examine the use of an Earliest Deadline First (EDF)-like scheduler that aims to schedule the transmission of "chunks" of video according to their "deadlines" using multicast. We show through analysis that this approach is optimal in terms of the data transmitted by the server. Using trace data from an operational service, we show that our approach reduces server bandwidth by as much as 65% compared to traditional techniques such as unicast and cyclic multicast. Finally, our approach achieves good aggregation even when 50% of the users use a typical VoD stream-control function like skip, to view different parts of the video.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {421–430},
numpages = {10},
keywords = {multicast, VoD, scheduling, EDF},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251355,
author = {Grigorias, Romulus},
title = {Session Details: System Track S3: 3D Mesh Streaming + HCM Track H2},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251355},
doi = {10.1145/3251355},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631332,
author = {He, Ying and Chew, Boon-Seng and Wang, Dayong and Hoi, Chu-Hong and Chau, Lap-Pui},
title = {Streaming 3D Meshes Using Spectral Geometry Images},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631332},
doi = {10.1145/1631272.1631332},
abstract = {The transmission of 3D models in the form of Geometry Images (GI) is an emerging and appealing concept due to the reduction in complexity from R3 to image space and wide availability of mature image processing tools and standards. However, geometry images often suffer from the artifacts and error during compression and transmission. Thus, there is a need to address the artifact reduction, error resilience and protection of such data information during the transmission across an error prone network. In this paper, we introduce a new concept, called Spectral Geometry Images (SGI), which naturally combines the powerful spectral analysis with geometry images. We show that SGI is more effective than GI to generate visually pleasing shapes at high compression rates. Furthermore, by coupling SGI to the proposed error protection scheme, we are able to ensure the smooth delivery of 3D model across error networks for different packet loss rate simulated using the two-state Markov model.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {431–440},
numpages = {10},
keywords = {transmission, conformal parameterization, error resilience, geometry image, image compression, spectral analysis, streaming 3D meshes},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631333,
author = {Cheng, Wei and Liu, Dan and Ooi, Wei Tsang},
title = {Peer-Assisted View-Dependent Progressive Mesh Streaming},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631333},
doi = {10.1145/1631272.1631333},
abstract = {Progressive mesh streaming is increasingly used in 3D networked applications, such as online games, virtual worlds, and digital museums. To scale such applications to a large number of users without high infrastructure cost, we apply peer-to-peer techniques to mesh streaming. We consider two issues: how to partition a progressive mesh into chunks and how to lookup the provider of a chunk. For the latter issue, we investigated into two solutions, which trade off server overhead and response time. The first uses a simple centralized lookup service, while the second organizes peers into groups according to the hierarchical structure of the progressive meshes to take advantage of access pattern. Simulation results show that our proposed systems are robust under high churn rate, reduce the server overhead by more than 90%, keep control overhead below 10%, and achieve low average response time.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {441–450},
numpages = {10},
keywords = {streaming, peer-to-peer, progressive meshes, 3D data},
location = {Beijing, China},
series = {MM '09}
}

@dataset{10.1145/review-1631272.1631333_R46438,
author = {Castelo Branco, Kalinka Regina Lucas Jaquie},
title = {Review ID:R46438 for DOI: 10.1145/1631272.1631333},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1631272.1631333_R46438}
}

@inproceedings{10.1145/1631272.1631334,
author = {Xiao, Bo and Yang, Xiaokang and Xu, Yi and Zha, Hongyuan},
title = {Learning Distance Metric for Regression by Semidefinite Programming with Application to Human Age Estimation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631334},
doi = {10.1145/1631272.1631334},
abstract = {A good distance metric for the input data is crucial in many pattern recognition and machine learning applications. Past studies have demonstrated that learning a metric from labeled samples can significantly improve the performance of classification and clustering algorithms. In this paper, we investigate the problem of learning a distance metric that measures the semantic similarity of input data for regression problems. The particular application we consider is human age estimation. Our guiding principle for learning the distance metric is to preserve the local neighborhoods based on a specially designed distance as well as to maximize the distances between data that are not in the same neighborhood in the semantic space.Without any assumption about the structure and the distribution of the input data, we show that this can be done by using semidefinite programming. Furthermore, the low-level feature space can be mapped to the high-level semantic space by a linear transformation with very low computational cost. Experimental results on the publicly available FG-NET database show that 1) the learned metric correctly discovers the semantic structure of the data even when the amount of training data is small and 2) significant improvement over the traditional Euclidean metric for regression can be obtained using the learned metric. Most importantly, simple regression methods such as k nearest neighbors (kNN), combined with our learned metric, become quite competitive (and sometimes even superior) in terms of accuracy when compared with the state-of-the-art human age estimation approaches.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {451–460},
numpages = {10},
keywords = {human age estimation, kNN regression, metric learning, semidefinite programming},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251356,
author = {Boll, Susanne},
title = {Session Details: HCM Track H1: Human-Centered Multimedia},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251356},
doi = {10.1145/3251356},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631336,
author = {Arapakis, Ioannis and Konstas, Ioannis and Jose, Joemon M.},
title = {Using Facial Expressions and Peripheral Physiological Signals as Implicit Indicators of Topical Relevance},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631336},
doi = {10.1145/1631272.1631336},
abstract = {Multimedia search systems face a number of challenges, emanating mainly from the semantic gap problem. Implicit feedback is considered a useful technique in addressing many of the semantic-related issues. By analysing implicit feedback information search systems can tailor the search criteria to address more effectively users' information needs. In this paper we examine whether we could employ affective feedback as an implicit source of evidence, through the aggregation of information from various sensory channels. These channels range between facial expressions to neuro-physiological signals and are regarded as indicative of the user's affective states. The end-goal is to model user affective responses and predict with reasonable accuracy the topical relevance of information items without the help of explicit judgements. For modelling relevance we extract a set of features from the acquired signals and apply different classification techniques, such as Support Vector Machines and K-Nearest Neighbours. The results of our evaluation suggest that the prediction of topical relevance, using the above approach, is feasible and, to a certain extent, implicit feedback models can benefit from incorporating such affective features.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {461–470},
numpages = {10},
keywords = {pattern recognition, support vector machines, facial expression analysis, affective feedback, multimedia retrieval, physiological signal processing, classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631337,
author = {Bentley, Frank R. and Groble, Michael},
title = {TuVista: Meeting the Multimedia Needs of Mobile Sports Fans},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631337},
doi = {10.1145/1631272.1631337},
abstract = {We describe the TuVista system, a service for viewing near-live sports content consisting of a multimedia editing/bundling station, a cloud-hosted metadata server, and a set of mobile clients. We begin by introducing TuVista I, a proof of concept experience prototype implemented quickly as a probe to understand multimedia needs at a live sporting event. After discussing the results of an initial field trial at Estadio Azteca in Mexico City, we describe the improvements in TuVista II to address the issues identified. These include rapid editing of multiple live video streams, push notifications of new content over XMPP, and an optimized metadata workflow for the content producer that reduced content publication time from fifteen minutes to less than 30 seconds. We conclude with a discussion of rapid prototyping and field deployments as a way to quickly identify user needs.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {471–480},
numpages = {10},
keywords = {mobile, multimedia, XMPP, video editing, sports, video annotation, metadata},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631338,
author = {Wu, Wanmin and Arefin, Ahsan and Rivas, Raoul and Nahrstedt, Klara and Sheppard, Renata and Yang, Zhenyu},
title = {Quality of Experience in Distributed Interactive Multimedia Environments: Toward a Theoretical Framework},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631338},
doi = {10.1145/1631272.1631338},
abstract = {The past decades have witnessed a rapid growth of Distributed Interactive Multimedia Environments (DIMEs). Despite their intensity of user-involved interaction, the existing evaluation frameworks remain very much system-centric. As a step toward the human-centric paradigm, we present a conceptual framework of Quality of Experience (QoE) in DIMEs, to model, measure, and understand user experience and its relationship with the traditional Quality of Service (QoS) metrics. A multi-displinary approach is taken to build up the framework based on the theoretical results from various fields including psychology, cognitive sciences, sociology, and information technology. We introduce a mapping methodology to quantify the correlations between QoS and QoE, and describe our controlled and uncontrolled studies as illustrating examples. The results present the first deep study to model the multi-facet QoE construct, map the QoS-QoE relationship, and capture the human-centric quality modalities in the context of DIMEs.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {481–490},
numpages = {10},
keywords = {QoE, QoS, distributed interactive multimedia environments},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631339,
author = {Chen, Kuan-Ta and Wu, Chen-Chi and Chang, Yu-Chun and Lei, Chin-Laung},
title = {A Crowdsourceable QoE Evaluation Framework for Multimedia Content},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631339},
doi = {10.1145/1631272.1631339},
abstract = {Until recently, QoE (Quality of Experience) experiments had to be conducted in academic laboratories; however, with the advent of ubiquitous Internet access, it is now possible to ask an Internet crowd to conduct experiments on their personal computers. Since such a crowd can be quite large, crowdsourcing enables researchers to conduct experiments with a more diverse set of participants at a lower economic cost than would be possible under laboratory conditions. However, because participants carry out experiments without supervision, they may give erroneous feedback perfunctorily, carelessly, or dishonestly, even if they receive a reward for each experiment.In this paper, we propose a crowdsourceable framework to quantify the QoE of multimedia content. The advantages of our framework over traditional MOS ratings are: 1) it enables crowdsourcing because it supports systematic verification of participants' inputs; 2) the rating procedure is simpler than that of MOS, so there is less burden on participants; and 3) it derives interval-scale scores that enable subsequent quantitative analysis and QoE provisioning. We conducted four case studies, which demonstrated that, with our framework, researchers can outsource their QoE evaluation experiments to an Internet crowd without risking the quality of the results; and at the same time, obtain a higher level of participant diversity at a lower monetary cost.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {491–500},
numpages = {10},
keywords = {probabilistic choice model, crowdsourcing, quality of experience (QoE), mean opinion score (MOS), Bradley-Terry-Luce Model, paired comparison},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251357,
author = {Worring, Marcel},
title = {Session Details: Short Papers Session 1: Content Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251357},
doi = {10.1145/3251357},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631341,
author = {Han, Bo and Yan, Yan and Chen, Zhenghua and Liu, Chang and Wu, Weiguo},
title = {A General Framework for Automatic On-Line Replay Detection in Sports Video},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631341},
doi = {10.1145/1631272.1631341},
abstract = {Replay detection is a pivotal step for sports video highlight extraction, which is a very promising application of multimedia analysis. In this paper, a general framework, which is based on a Bayesian network, is proposed to make full use of the multiple clues, including shot structure, gradual transition pattern, slow-motion, and sports scene. A novel algorithm based on motion vector reliability classification is proposed to analyze the gradual transition patterns, so that the replay detector can meet the requirements of automatic on-line applications. This is the first integrated general replay detection framework proposed in the literature. Extensive experiments on diversified sports games have proven the scheme efficient, accurate and robust.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {501–504},
numpages = {4},
keywords = {motion vector reliability, replay detection, sports video analysis, bayesian network},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631342,
author = {Wu, Xiao and Ngo, Chong-Wah and Li, Jintao and Zhang, Yongdong},
title = {Localizing Volumetric Motion for Action Recognition in Realistic Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631342},
doi = {10.1145/1631272.1631342},
abstract = {This paper presents a novel motion localization approach for recognizing actions and events in real videos. Examples include StandUp and Kiss in Hollywood movies. The challenge can be attributed to the large visual and motion variations imposed by realistic action poses. Previous works mainly focus on learning from descriptors of cuboids around space time interest points (STIP) to characterize actions. The size, shape and space-time position of cuboids are fixed without considering the underlying motion dynamics. This often results in large set of fragmentized cuboids which fail to capture long-term dynamic properties of realistic actions. This paper proposes the detection of spatio-temporal motion volumes (namely Volume of Interest, VOI) of scale and position adaptive to localize actions. First, motions are described as bags of point trajectories by tracking keypoints along the time dimension. VOIs are then adaptively extracted by clustering trajectory on the motion mainfold. The resulting VOIs, of varying scales and centering at arbitrary positions depending on motion dynamics, are eventually described by SIFT and 3D gradient features for action recognition. Comparing with fixed-size cuboids, VOI allows comprehensive modeling of long-term motion and shows better capability in capturing contextual information associated with motion dynamics. Experiments on a realistic Hollywood movie dataset show that the proposed approach can achieve 20% relative improvement compared to the state-of-the-art STIP based algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {505–508},
numpages = {4},
keywords = {keypoint trajectory, motion subspace learning, human action recognition, realistic videos, mean-shift clustering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631343,
author = {Chu, Wei-Ta and Lin, Chia-Hung and Yu, Jen-Yu},
title = {Feature Classification for Representative Photo Selection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631343},
doi = {10.1145/1631272.1631343},
abstract = {This paper points out that different local feature points provide different impacts to near-duplicate detection and related applications. Aiming to automatic representative photo selection, we develop three feature classification methods, i.e., point-based, region-based, and pLSA-based classification, to differentiate local feature points described by SIFT descriptors. We investigate the performance of these classification methods, and discuss how they influence near-duplicate detection and extended applications. Experiments show that, with effective feature classification, more accurate representative selection results can be achieved.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {509–512},
numpages = {4},
keywords = {representative selection, near-duplicate detection, feature classification, probabilistic latent semantic analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631344,
author = {Liu, Yuyu and Sato, Yoichi},
title = {Visual Localization of Non-Stationary Sound Sources},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631344},
doi = {10.1145/1631272.1631344},
abstract = {Sound source can be visually localized by analyzing the correlation between audio and visual data. To correctly analyze this correlation, the sound source is required to be stationary in a scene to date. We introduce a technique that localizes the non-stationary sound sources to overcome this limitation. The problem is formulated as finding the optimal visual trajectories that best represent the movement of the sound source over the pixels in a spatio-temporal volume. Using a beam search, we search these optimal visual trajectories by maximizing the correlation between the newly introduced audiovisual features of inconsistency. An incremental correlation evaluation with mutual information is developed here, which significantly reduces the computational cost. The correlations computed along the optimal trajectories are finally incorporated into a segmentation technique to localize a sound source region in the first visual frame of the current time window. Experimental results demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {513–516},
numpages = {4},
keywords = {sound source localization, mutual information, audiovisual correlation analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631345,
author = {Li, Zhong and Luo, Hangzai and Fan, Jianping},
title = {Incorporating Camera Metadata for Attended Region Detection and Consumer Photo Classification},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631345},
doi = {10.1145/1631272.1631345},
abstract = {Photos taken by human beings significantly differ from the pictures that are taken by a surveillance camera or a vision sensor on a robot, e.g., human beings may intentionally capture photos to express his/her feeling or record a memorial scene. Such a creative photo capture process is accomplished by adjusting two factors: (1) the parameters setting of a camera; and (2) the position between the camera and the interesting objects or scenes. To enable automatic understanding and interpretation of the semantics of photos, it is very important to take all these factors into account. Unfortunately, most existing algorithms for image understanding focus on only the content of the images while completely ignoring these two important factors. In this paper, we have developed a new algorithm to calculate what the interestingness of the photographer is and what the core content of a photo is. The gained information (i.e., attended regions and attention of the photographer) is further used to support more effective photo classification and retrieval. Our experiments on 70,000+ photos taken by 200+ different models of cameras have obtained very positive results.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {517–520},
numpages = {4},
keywords = {attended regions, camera metadata, image classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631346,
author = {Shao, Yuanlong and Zhou, Yuan and He, Xiaofei and Cai, Deng and Bao, Hujun},
title = {Semi-Supervised Topic Modeling for Image Annotation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631346},
doi = {10.1145/1631272.1631346},
abstract = {We propose a novel technique for semi-supervised image annotation which introduces a harmonic regularizer based on the graph Laplacian of the data into the probabilistic semantic model for learning latent topics of the images. By using a probabilistic semantic model, we connect visual features and textual annotations of images by their latent topics. Meanwhile, we incorporate the manifold assumption into the model to say that the probabilities of latent topics of images are drawn from a manifold, so that for images sharing similar visual features or the same annotations, their probability distribution of latent topics should also be similar. We create a nearest neighbor graph to model the manifold and propose a regularized EM algorithm to simultaneously learn a generative model and assign probability density of latent topics to images discriminatively. In this way, databases with very few labeled images can be annotated better than previous works.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {521–524},
numpages = {4},
keywords = {semantic indexing, automatic image annotation, laplacian regularization, semi-supervised learning},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631347,
author = {Gong, Boqing and Xu, Chunjing and Liu, Jianzhuang and Tang, Xiaoou},
title = {Boosting 3D Object Retrieval by Object Flexibility},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631347},
doi = {10.1145/1631272.1631347},
abstract = {In this paper, we propose a novel feature, called object flexibility, at a point of a 3D object to describe how the neighborhood of this point is massively connected to the object. We show that this feature is stable to the deformation of objects' articulations, in addition to commonly concerned linear transforms, i.e., translation, scale, and rotation. A shape descriptor is obtained based on this feature using the bag-of-words model. As an application, the descriptor is used to perform 3D object retrieval. Extensive experiments demonstrate its superiority over a variety of existing 3D shape descriptors in the retrieval of articulated objects, as well as its enhancement of other shape descriptors to retrieve generic 3D objects.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {525–528},
numpages = {4},
keywords = {features, 3D object retrieval, object flexibility},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631348,
author = {Chen, Zhuoyuan and Sun, Lifeng and Yang, Shiqiang},
title = {Auto-Cut for Web Images},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631348},
doi = {10.1145/1631272.1631348},
abstract = {In this paper, we propose a novel automatic algorithm for foreground/background labeling. We aim to generate ROI cutout automatically for further processing such as image editing, classification and information retrieval. Different from traditional semi-supervised segmentation method, we use a rather weak prior on boundary label. Accordingly, a global cost function is proposed to combine our prior knowledge with pixel-level feature. We compute fuzzy matting components as building blocks to construct semantically meaningful mattes. Finally, these mattes are hierarchically clustered and ranked by central preference. Experimental results on a large benchmark data set demonstrate the performance of our algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {529–532},
numpages = {4},
keywords = {semi-supervised learning, attention detection, spectral grouping, matting},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631349,
author = {Liu, Xi and Shi, Zhiping and Li, Zhixin and Shi, Zhongzhi},
title = {Coboost Learning of Visual Categories with 1st and 2nd Order Features from Google Images},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631349},
doi = {10.1145/1631272.1631349},
abstract = {Conventional object recognition techniques rely heavily on manually annotated image datasets to achieve good performances. However, collecting high quality datasets is really laborious. In this paper, we propose a semi-supervised framework for learning visual categories from Google Images. The 1st and 2nd order features, which define bag of words representation and spatial relationship between local features respectively, make up an independent and redundant feature split. We then integrate a cotraining algorithm CoBoost with these two features. We create two boosting classifiers based on the 1st and 2nd order features respectively in the training, during which one classifier provides labels for the other. Besides, the 2nd order features are generated dynamically rather than extracted exhaustively to avoid high computation. An active learning technique is also introduced to further improve the performance. We evaluate our method on the benchmark datasets, showing results competitive with the state-of-the-art unsupervised approaches and some supervised techniques.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {533–536},
numpages = {4},
keywords = {1st and 2nd order features, learn categories, coboost, co-training},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631350,
author = {Liu, Ming and Chen, Shifeng and Liu, Jianzhuang and Tang, Xiaoou},
title = {Video Completion via Motion Guided Spatial-Temporal Global Optimization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631350},
doi = {10.1145/1631272.1631350},
abstract = {In this paper, a novel global optimization based approach is proposed for video completion whose target is to restore the spatial-temporal missing regions of a video in a visually plausible way. Our algorithm consists of two stages: motion field completion and color completion via global optimization. First, local motions within the missing parts are completed patch-by-patch greedily using pre-computed available motions in the video. Then the missing regions are filled by sampling patches from available parts of the video. We formulate the video completion as a global energy minimization problem by Markov random fields (MRFs). Based on the completed motion field of the video, a well-defined energy function involving both spatial and temporal coherence relationship is constructed. A coarse-to-fine Belief Propagation (BP) is proposed to solve the optimization problem. Experimental results have demonstrated the good performance of our algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {537–540},
numpages = {4},
keywords = {belief propagation, video completion, motion},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631351,
author = {Sun, Xiaoshuai and Yao, Hongxun and Ji, Rongrong and Liu, Shaohui},
title = {Photo Assessment Based on Computational Visual Attention Model},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631351},
doi = {10.1145/1631272.1631351},
abstract = {It is difficult to be satisfied for automatic photo assessment using only low level visual features such as brightness, lighting, hue, contrast, color distribution and so on. Instead of using low level visual features, we present a novel computational visual attention model to assess photos. Firstly, a face-sensitive saliency map analysis is deployed to estimate attention distribution. Then, a Rate of Focused Attention (RFA) measurement is proposed to quantify photo quality. By integrating top-down supervision into the visual attention model, we further achieve personalized photo assessment to take user preference into quality evaluation, which can be extended into object or semantic oriented photo assessment scenarios. Experiments on personal photo albums with comparison to ground-truth user evaluations demonstrate the effeteness of the proposed method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {541–544},
numpages = {4},
keywords = {bottom-up and top-down attention, computational visual attention model, photo assessment, rate of focused attention},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631352,
author = {Litayem, Saloua and Joly, Alexis and Boujemaa, Nozha},
title = {Interactive Objects Retrieval with Efficient Boosting},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631352},
doi = {10.1145/1631272.1631352},
abstract = {This paper presents an efficient local features boosting strategy for interactive objects retrieval tasks such as on-line supervised learning or relevance feedback. The prediction time complexity of most existing methods is indeed usually linear in dataset size since the retrieval works by applying a trained classifier on the images of the dataset one by one. In our method, the trained classifier can be computed directly on the whole dataset in sublinear time thanks to distance-based weak classifiers. The idea is to speed-up drastically the prediction of each weak classifier on the whole dataset by performing approximate range queries with an efficient similarity search structure. Experiments on Caltech 256 dataset show that the technique is up to 250 times faster than the naive exhaustive method. Thanks to this efficiency improvement, we developed a relevance feedback mechanism on image regions freely selected by the user and we show how it improves the effectiveness of the retrieval.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {545–548},
numpages = {4},
keywords = {efficiency, boosting, LSH, relevance feedback, scalability, object retrieval, local features},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631353,
author = {Wu, Zhipeng and Jiang, Shuqiang and Huang, Qingming},
title = {Near-Duplicate Video Matching with Transformation Recognition},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631353},
doi = {10.1145/1631272.1631353},
abstract = {Nowadays, the issue of near-duplicate video matching has been extensively studied. However, transformation, which is one of the major causes of near-duplicates, has been little discussed. In this paper, we focus on the fact that a certain kind of feature may per-form excellently to deal with one type of transformation while not be that good on another. We present a self-similarity matrix based near-duplicate video matching scheme with an additional transformation recognition module. By detecting the type of transformations, the near-duplicates can be treated with the 'best' feature which is decided experimentally. Thus, we obtain an enhanced matching result by employing the selected feature. Our work includes seven features and ten transformations respectively, and experimental results show the effectiveness of transformation recognition and the promotion it brings to boost the near-duplicate matching scheme.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {549–552},
numpages = {4},
keywords = {SSM, multi-features, self-similarity, copy detection, near-duplicate, transformation recognition},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631354,
author = {Yang, Yezhou and Song, Mingli and Li, Na and Bu, Jiajun and Chen, Chun},
title = {Visual Attention Analysis by Pseudo Gravitational Field},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631354},
doi = {10.1145/1631272.1631354},
abstract = {As a crucial step of the visual cognition and perception, visual attention analysis shows great importance in many research or application areas. In this paper, we treat this problem from a new angle, inspired by the classic gravitational field theory. By defining "mass" of each pixel, we compute "force" between them to obtain a so-called pseudo gravitational field over an image. Then, we propose an iteration algorithm to simulate the movement of the fixation points affected by this field. Finally, stable visual attention points or areas are obtained when those fixation points finally aggregate around some special pixels or areas. The main contributions are threefold: (1) by introducing classic gravitational field theory into visual attention analysis, a new point of view is proposed; (2) a competition scheme is constructed and the meaning of attraction can be applied into visual attention analysis intuitively; (3) by using pixels into computation through down sampling, a faster analysis method is achieved. The experimental result shows that our method is effective and consistent with the generally accepted definition of visual attention.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {553–556},
numpages = {4},
keywords = {attention analysis, gravitational field, simulation algorithm},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631355,
author = {Bao, Lei and Cao, Juan and Xia, Tian and Zhang, Yong-Dong and Li, Jintao},
title = {Locally Non-Negative Linear Structure Learning for Interactive Image Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631355},
doi = {10.1145/1631272.1631355},
abstract = {A successful interactive image retrieval system is expected to quickly return as many relevant results as possible while costing less users' effort. Considering these system demands, firstly we propose a novel semi-supervised learning algorithm called Locally Non-negative Linear Structure Learning (LNLS), which is based on the assumption that the labels of each data should be sufficiently smooth with respect to the locally non-negative linear structure of dataset. It has two main merits: first, it is robust to the small sample learning problem since it learns structure from both labeled and unlabeled data; second, by emphasizing the non-negativity of locally linear structure, this algorithm preserves the non-negative inherent characteristic of image data and can truly reveal the intrinsic structure of the images corpus, especially the asymmetric relationship between images. Meanwhile, we explore an online updating algorithm for LNLS to tackle the large computation cost. Thus the model can be generalized to the new queries or the newly-labeled samples without retraining. Furthermore, an active learning method for LNLS is proposed to make the most of users' effort to improve the learner. The encouraging experimental results demonstrate the effectiveness and efficiency of our proposed methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {557–560},
numpages = {4},
keywords = {active learning, interactive image retrieval, locally non-negative linear structure},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631356,
author = {You, Junyong and Perkis, Andrew and Hannuksela, Miska M. and Gabbouj, Moncef},
title = {Perceptual Quality Assessment Based on Visual Attention Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631356},
doi = {10.1145/1631272.1631356},
abstract = {Most existing quality metrics do not take the human attention analysis into account. Attention to particular objects or regions is an important attribute of human vision and perception system in measuring perceived image and video qualities. This paper presents an approach for extracting visual attention regions based on a combination of a bottom-up saliency model and semantic image analysis. The use of PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural SIMilarity) in extracted attention regions is analyzed for image/video quality assessment, and a novel quality metric is proposed which can exploit the attributes of visual attention information adequately. The experimental results with respect to the subjective measurement demonstrate that the proposed metric outperforms the current methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {561–564},
numpages = {4},
keywords = {perceptual quality assessment, visual attention, quality metric},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631357,
author = {Irie, Go and Hidaka, Kota and Satou, Takashi and Kojima, Akira and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
title = {Latent Topic Driving Model for Movie Affective Scene Classification},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631357},
doi = {10.1145/1631272.1631357},
abstract = {This paper proposes a latent topic driving model (LTDM) as a novel approach to movie affective scene classification. LTDM is a discriminative model of emotions driven by movie affective contents. Unlike existing methods, our approach is based on movie topic extraction via the latent Dirichlet allocation (LDA) and emotion dynamics modeling with reference to Plutchik's emotion theory. The classification procedure starts by segmenting movie scenes into movie shots, each of which is represented by a histogram of quantized affect-related audio-visual features. LDA is applied to detect topics of each movie shot. Emotions for the current movie shot are estimated based on both the topics of the shot and emotion transition weights determined by Plutchik's emotion theory. We conduct experiments using 206 movie scenes extracted from 24 movie titles (total 6 hours 20 min. 12 sec.) and the labels of eight emotion categories given by 16 subjects are collected. The results show that LTDM outperforms conventional modeling approaches in terms of the subject agreement rate.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {565–568},
numpages = {4},
keywords = {latent dirichlet allocation, latent topic driving model, Plutchik's basic emotions, affective scene classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631358,
author = {Gong, Boqing and Wang, Yueming and Liu, Jianzhuang and Tang, Xiaoou},
title = {Automatic Facial Expression Recognition on a Single 3D Face by Exploring Shape Deformation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631358},
doi = {10.1145/1631272.1631358},
abstract = {Facial expression recognition has many applications in multimedia processing and the development of 3D data acquisition techniques makes it possible to identify expressions using 3D shape information. In this paper, we propose an automatic facial expression recognition approach based on a single 3D face. The shape of an expressional 3D face is approximated as the sum of two parts, a basic facial shape component (BFSC) and an expressional shape component (ESC). The BFSC represents the basic face structure and neutral-style shape and the ESC contains shape changes caused by facial expressions. To separate the BFSC and ESC, our method firstly builds a reference face for each input 3D non-neutral face by a learning method, which well represents the basic facial shape. Then, based on the BFSC and the original expressional face, a facial expression descriptor is designed. The surface depth changes are considered in the descriptor. Finally, the descriptor is input into an SVM to recognize the expression. Unlike previous methods which recognize a facial expression with the help of manually labeled key points and/or a neutral face, our method works on a single 3D face without any manual assistance. Extensive experiments are carried out on the BU-3DFE database and comparisons with existing methods are conducted. The experimental results show the effectiveness of our method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {569–572},
numpages = {4},
keywords = {3D facial expression recognition, expressional shape component, basic facial shape component, shape deformation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631359,
author = {Xu, Hao and Wang, Jingdong and Hua, Xian-Sheng and Li, Shipeng},
title = {Tag Refinement by Regularized LDA},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631359},
doi = {10.1145/1631272.1631359},
abstract = {Tagging is nowadays the most prevalent and practical way to make images searchable. However, in reality many tags are irrelevant to image content. To refine the tags, previous solutions usually mine tag relevance relying on the tag similarity estimated right from the corpus to be refined. The calculation of tag similarity is affected by the noisy tags in the corpus, which is not conducive to estimate accurate tag relevance. In this paper, we propose to do tag refinement from the angle of topic modeling. In the proposed scheme, tag similarity and tag relevance are jointly estimated in an iterative manner, so that they can benefit from each other. Specifically, a novel graphical model, regularized Latent Dirichlet Allocation (rLDA), is presented. It facilitates the topic modeling by exploiting both the statistics of tags and visual affinities of images in the corpus. The experiments on tag ranking and image retrieval demonstrate the advantages of the proposed method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {573–576},
numpages = {4},
keywords = {regularized LDA, tag relevance, tag refinement},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631360,
author = {Liu, Yang and Liu, Yan},
title = {Tensor Distance Based Multilinear Multidimensional Scaling for Image and Video Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631360},
doi = {10.1145/1631272.1631360},
abstract = {This paper presents a novel dimensionality reduction technique named Tensor Distance based Multilinear Multidimensional Scaling (TD-MMDS). First, we propose a new distance metric called Tensor Distance (TD) to build a relationship graph of data points with high-order. Then we employ an iterative strategy to sequentially learn the transformation matrices that can best keep pair-wise TDs of the high-order data in the low-dimensional embedded space. By integrating both tensor distance and tensor embedding, TD-MMDS provides a uniform framework of tensor based dimensionality reduction, which preserves the intrinsic structure of high-order data through the whole learning procedure. Experiments on standard image and video datasets validate the effectiveness of the proposed TD-MMDS.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {577–580},
numpages = {4},
keywords = {tensor distance based multilinear multidimensional scaling, dimensionality reduction, tensor distance, image and video analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631361,
author = {Joly, Alexis and Buisson, Olivier},
title = {Logo Retrieval with a Contrario Visual Query Expansion},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631361},
doi = {10.1145/1631272.1631361},
abstract = {This paper presents a new content-based retrieval framework applied to logo retrieval in large natural image collections. The first contribution is a new challenging dataset, called BelgaLogos, which was created in collaboration with professionals of a press agency, in order to evaluate logo retrieval technologies in real-world scenarios. The second and main contribution is a new visual query expansion method using an a contrario thresholding strategy in order to improve the accuracy of expanded query images. Whereas previous methods based on the same paradigm used a purely hand tuned fixed threshold, we provide a fully adaptive method enhancing both genericity and effectiveness. This new technique is evaluated on both OxfordBuilding dataset and our new BelgaLogos dataset.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {581–584},
numpages = {4},
keywords = {retrieval, a contrario, image, query expansion, logo},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631362,
author = {Favre, Sarah and Dielmann, Alfred and Vinciarelli, Alessandro},
title = {Automatic Role Recognition in Multiparty Recordings Using Social Networks and Probabilistic Sequential Models},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631362},
doi = {10.1145/1631272.1631362},
abstract = {The automatic analysis of social interactions is attracting significant interest in the multimedia community. This work addresses one of the most important aspects of the problem, namely the recognition of roles in social exchanges. The proposed approach is based on Social Network Analysis, for the representation of individuals in terms of their interactions with others, and probabilistic sequential models, for the recognition of role sequences underlying the sequence of speakers in conversations. The experiments are performed over different kinds of data (around 90 hours of broadcast data and meetings), and show that the performance depends on how formal the roles are, i.e. on how much they constrain people behavior.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {585–588},
numpages = {4},
keywords = {HMMS, role recognition, social network analysis, statistical language models},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631363,
author = {Liang, Yingyu and Li, Jianmin and Zhang, Bo},
title = {Vocabulary-Based Hashing for Image Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631363},
doi = {10.1145/1631272.1631363},
abstract = {This paper proposes a hash function family based on feature vocabularies and investigates the application in building indexes for image search. Each hash function is associated with a set of feature points, i.e. a vocabulary, and maps an input point to the ID of the nearest one in the vocabulary. The function family can be employed to build a high-dimensional index for approximate nearest neighbor search. Then we concentrate on its application in image search. Guiding rules for the construction of the vocabularies are derived, which improve the effectiveness of the approach in this context by taking advantage of the data distribution. The rules are applied to design an algorithm for vocabulary construction in practice. Experiments show promising performance of the approach and the effectiveness of the guiding rules. Comparison with the popular Euclidean locality-sensitive hashing also shows the advantage of our approach in image search.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {589–592},
numpages = {4},
keywords = {image search, visual vocabulary, hashing index},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631364,
author = {Zhang, Tong and Fong, Chee Keat and Xiao, Linxing and Zhou, Jie},
title = {Automatic and Instant Ring Tone Generation Based on Music Structure Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631364},
doi = {10.1145/1631272.1631364},
abstract = {Real tones, which are often excerpts from pop songs, have become popular as ring tones. This paper describes how a ring tone can be produced by analyzing the structure of music and selecting the most appropriate portion of the music. With audio feature analysis and pattern recognition methods, the structure of a song can be estimated by deploying both singing voice detection and repetition detection. Then, one or more ring tones can be automatically selected from the song according to heuristic rules. The entire process takes only a few seconds. It is greatly superior in efficiency and ease-of-use than currently available ring tone generation approaches, and can be used in handheld devices, desktop or laptop PCs and web services. Moreover, this unique music structure analysis technology we developed may be used in many other applications as well, such as for browsing, searching and shopping digital music.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {593–596},
numpages = {4},
keywords = {music structure analysis, music repetition detection, music browsing, voice detection, ring tone generation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631365,
author = {Zhang, Tong and Xiao, Jun and Wen, Di and Ding, Xiaoqing},
title = {Face Based Image Navigation and Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631365},
doi = {10.1145/1631272.1631365},
abstract = {People are often the most important subjects in photos, and the ability of finding photos of a particular person easily and quickly in an image collection is highly desired. In this paper, we present a face clustering system which automatically groups photos into clusters, with each cluster containing photos of the same person. This is done based on an advanced face recognition engine and a semi-supervised clustering approach. The system achieved good clustering accuracy when tested on different image sets and by different users. Moreover, features such as adding new images, face cluster navigation and face based image retrieval are added that greatly improve the usability of the system. It also facilitates efficient manual manipulations of clustering results. On top of this technology, image navigation systems have been built, including the "face bubble" visualization which provides one-glance view of a photo collection, and shows the relations among people.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {597–600},
numpages = {4},
keywords = {face based image search, image organization, face clustering, face based image navigation, image retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631366,
author = {Jansohn, Christian and Ulges, Adrian and Breuel, Thomas M.},
title = {Detecting Pornographic Video Content by Combining Image Features with Motion Information},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631366},
doi = {10.1145/1631272.1631366},
abstract = {With the rise of large-scale digital video collections, the challenge of automatically detecting adult video content has gained significant impact with respect to applications such as content filtering or the detection of illegal material. While most systems represent videos with keyframes and then apply techniques well-known for static images, we investigate motion as another discriminative clue for pornography detection. A framework is presented that combines conventional keyframe-based methods with a statistical analysis of MPEG-4 motion vectors. Two general approaches are followed to describe motion patterns, one based on the detection of periodic motion and one on motion histograms. Our experiments on real-world web video data show that this combination with motion information improves the accuracy of pornography detection significantly (equal error is reduced from 9.9% to 6.0%). Comparing both motion descriptors, histograms outperform periodicity detection.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {601–604},
numpages = {4},
keywords = {content-based video retrieval, pornography detection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631367,
author = {Hakeem, Asaad and Lee, Mun Wai and Javed, Omar and Haering, Niels},
title = {Semantic Video Search Using Natural Language Queries},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631367},
doi = {10.1145/1631272.1631367},
abstract = {Recent advances in computer vision and artificial intelligence algorithms have allowed automatic extraction of metadata from video. This metadata can be represented by using the RDF/OWL ontology which can encode scene objects and their relationships in an unambiguous and well-formed manner. The encoded data can be queried using SPARQL. However, SPARQL has a steep learning curve and cannot be directly utilized by a general user for video content search. In this paper, we propose a method to bridge this gap by automatically translating user provided natural language query into an ontology-based SPARQL query for semantic video search. The proposed method consists of three major steps. First, semantically labeled training corpus of natural language query sentences is used for learning the Semantic Stochastic Context Free Grammar (SSCFG). Second, given a user provided natural language query sentence, we use the Earley-Stolcke parsing algorithm to determine the maximum likelihood semantic parsing of the query sentence. This parsing infers the semantic meaning for each word in the query sentence from which the SPARQL query is constructed. Third, the SPARQL query is executed to retrieve relevant video segments from the RDF-OWL video content database. The method is evaluated by running natural language queries on surveillance videos from maritime and land-based domains, though the framework itself is general and extensible to search videos from other domains.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {605–608},
numpages = {4},
keywords = {semantic video retrieval, video search, SPARQL, SSCFG},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631368,
author = {Toderici, George D. and Yagnik, Jay},
title = {Automatic, Efficient, Temporally-Coherent Video Enhancement for Large Scale Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631368},
doi = {10.1145/1631272.1631368},
abstract = {A fast and robust method for video contrast enhancement is presented. The method uses the histogram of each frame, along with upper and lower bounds computed per shot in order to enhance the current frame. This ensures that the artifacts introduced during the enhancement is reduced to a minimum. Traditional methods that do not compute per-shot estimates tend to over-enhance parts of the video such as fades and transitions. Our method does not suffer from this problem, which is essential for a fully automatic algorithm. We present the parameters for our methods which yielded the best human feedback, which showed that out of 208 videos, 203 were enhanced, while the remaining 5 were of too poor quality to be enhanced. Additionally, we present a visual comparison of our work with the recently-proposed Weighted Thresholded Histogram Equalization (WTHE) algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {609–612},
numpages = {4},
keywords = {histogram equalization, video enhancement, temporal coherence},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631369,
author = {Liu, Xianming and Yao, Hongxun and Ji, Rongrong and Xu, Pengfei and Sun, Xiaoshuai},
title = {What is a Complete Set of Keywords for Image Description &amp; Annotation on the Web},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631369},
doi = {10.1145/1631272.1631369},
abstract = {Does there exist a compact set of keywords that can completely and effectively cover the image annotation problem by expanding from it? In this paper, we answer this question by presenting a complete set framework for image annotation, which is motivated by the existence of semantic ontology. To generate this set, we propose a cross model optimization strategy from both textual and visual information for topic decomposition, based on a so-called Bipartite LSA model, which minimize multimodal error energy functions in a probabilistic Latent Semantic Analysis model. To achieve complete set based annotation, we present a Gaussian-Kernel-Generative process based keyword generation procedure, which analogizes keyword annotation in a probabilistic generative manner. A group of experiments is performed on Washington University image database and 80,000 Flickr images with comparisons to the state-of-the-arts. Finally, potential advantages and future improvements of our framework are discussed outside the scope of topic modeling.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {613–616},
numpages = {4},
keywords = {keyword selection, ontology, semantic items, image annotation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631370,
author = {Cui, Xinyi and Liu, Qingshan and Metaxas, Dimitris},
title = {Temporal Spectral Residual: Fast Motion Saliency Detection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631370},
doi = {10.1145/1631272.1631370},
abstract = {Saliency detection has attracted much attention in recent years. It aims at locating semantic regions in images for further image understanding. In this paper, we address the issue of motion saliency detection for video content analysis. Inspired by the idea of Spectral Residual for image saliency detection, we propose a new method Temporal Spectral Residual on video slices along X-T and Y-T planes, which can automatically separate foreground motion objects from backgrounds, also with the help of threshold selection and voting schemes. Different from conventional background modeling methods with complex mathematical model, the proposed method is only based on Fourier spectrum analysis, so it is simple and fast. The power of our proposed method is demonstrated in the experiments of four typical videos with different dynamic background.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {617–620},
numpages = {4},
keywords = {temporal spectral residual, video analysis, motion saliency detection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631371,
author = {Imran, Naveed and Liu, Jingen and Luo, Jiebo and Shah, Mubarak},
title = {Event Recognition from Photo Collections via PageRank},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631371},
doi = {10.1145/1631272.1631371},
abstract = {We propose a method of mining most informative features for the event recognition from photo collections. Our goal is to classify different event categories based on the visual content of a group of photos that constitute the event. Such photo groups are typical in a personal photo collection of different events. Visual features are extracted from the images, yet the features from individual images are often noisy and not all of them represent the distinguishing characteristics of an event. We employ the PageRank technique to mine the most informative features from the images that belong to the same event. Subsequently, we classify different event categories using the multiple images of the same event because we argue that they are more informative about the content of an event rather than any single image. We compare our proposed approach with the standard bag of features method (BOF) and observe considerable improvements in recognition accuracy.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {621–624},
numpages = {4},
keywords = {CBIR, pagerank, event category recognition},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631372,
author = {Chu, Wei-Ta and Lee, Ya-Lin and Yu, Jen-Yu},
title = {Visual Language Model for Face Clustering in Consumer Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631372},
doi = {10.1145/1631272.1631372},
abstract = {For consumer photos, this work clusters faces with large variations in lighting, pose, and expression. After matching face images by local feature points, we transform matching situations into a novel representation called visual sentences. Then, visual language models are constructed to describe the dependency of image patches on faces. With the probabilistic framework, we develop a clustering algorithm to group the same individual's face images into the same cluster. An interesting observation about evaluating face clustering performance is proposed, and we demonstrate the superiority of the proposed visual language model approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {625–628},
numpages = {4},
keywords = {visual language model, face clustering, agglomerative clustering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631373,
author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua and Wang, Liang},
title = {Face Image Modeling by Multilinear Subspace Analysis with Missing Values},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631373},
doi = {10.1145/1631272.1631373},
abstract = {The main difficulty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-$n$ product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difficult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M$^2$SA, which can work on the training tensor with massive missing values. Thus M$^2$SA can be used to model face images even when there are only a small number of face images with limited variations which will cause missing values in the training tensor). Experiments on face recognition show that M$^2$SA can work reasonably well with up to $70%$ missing values in the training tensor.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {629–632},
numpages = {4},
keywords = {face image, multilinear subspace analysis, missing values},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251358,
author = {Erol, Berna},
title = {Session Details: Short Papers Session 2: Content Analysis and HCM},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251358},
doi = {10.1145/3251358},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631375,
author = {Yeh, Mei-Chen and Cheng, Kwang-Ting},
title = {A Compact, Effective Descriptor for Video Copy Detection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631375},
doi = {10.1145/1631272.1631375},
abstract = {Large scale video copy detection tasks require a compact and computational-efficient descriptor that is robust to various transformations that are typically applied to generate copies. In this paper, we propose a new frame-level descriptor for such a task. The descriptor encodes the internal structure of a video frame by computing the pair-wise correlations between geometrically pre-indexed blocks. It is conceptually simple, small in size, and fast to compute. Experiments using the MUSCLE VCD benchmark show its superior performance compared to existing approaches.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {633–636},
numpages = {4},
keywords = {frame descriptor, graph representation, video copy detection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631376,
author = {Hsu, Chao-Yung and Lu, Chun-Shien and Pei, Soo-Chang},
title = {Secure and Robust SIFT},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631376},
doi = {10.1145/1631272.1631376},
abstract = {Scale-invariant feature transform (SIFT) is a powerful tool extensively used in the community of pattern recognition and computer vision. However, the security issue of SIFT is relatively unexplored in the literature. This paper investigates the potential weakness of SIFT, meaning that the SIFT features can be deleted or destroyed while maintaining acceptable visual qualities. We then propose an improved scheme to enhance the security of SIFT by introducing a key-based transform process to images. Experimental results demonstrate the effectiveness of our methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {637–640},
numpages = {4},
keywords = {robustness, security, attack, SIFT, keypoint},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631377,
author = {Dao, Minh-Son and Sharma, Ishan Nath and Babaguchi, Noboru},
title = {Preserving Topological Information in Sub-Trajectories-Based Representation for Spatio-Temporal Trajectories Indexing and Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631377},
doi = {10.1145/1631272.1631377},
abstract = {Trajectories of moving objects are known as one of the most important cues for understanding semantics in video data. Although there are a lot of significant researches dealing with trajectory analysis tailored to indexing and retrieval, several problems still remain. One of them is a trade-off between whole trajectory- and sub-trajectories- based methods. The former problem is that representing a trajectory as a whole is not appropriate for detecting similar patterns of the trajectory. In contrast, the latter is that even though some key portion of two trajectories share similar patterns, the whole trajectories may be totally different. Therefore, this paper proposes a novel method to optimize such trade-off. By representing a trajectory as a combination of sequence of "word" - each word's character represents one distinct feature extracted from sub-trajectories (i.e. segments), and a topological graph of trajectory's segments, the proposed method is shift and scale invariant, can handle occlusion and distortion, and can discover similar patterns among trajectories. Thorough comparisons with well-known methods demonstrate the superiority of the proposed method in terms of precision recall ratios.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {641–644},
numpages = {4},
keywords = {spectral clustering, trajectory segmentation, topological graph, PCA, trajectory},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631378,
author = {Cao, Juan and Jing, HongFang and Ngo, Chong-Wah and Zhang, YongDong},
title = {Distribution-Based Concept Selection for Concept-Based Video Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631378},
doi = {10.1145/1631272.1631378},
abstract = {Query-to-concept mapping plays one of the keys to concept-based video retrieval. Conventional approaches try to find concepts that are likely to co-occur in the relevant shots from the lexical or statistical aspects. However, the high probability of co-occurrence alone cannot ensure its effectiveness to distinguish the relevant shots from the irrelevant ones. In this paper, we propose distribution based concept selection (DBCS) for query-to-concept mapping by analyzing concept score distributions of within and between relevant and irrelevant sets. In view of the imbalance between relevant and irrelevant examples, two variants of DBCS are proposed respectively by considering the two-sided and onesided metrics of concept distributions. Specifically, the impact of positive and negative concepts toward search is explicitly considered. DBCS is found to be appropriate for both automatic and interactive video search. Using TRECVID 2008 video dataset for experiments, improvements of 50% and 34% are reported when compared to text-based and visual-based query-to concept mapping respectively in automatic search. Meanwhile, DBCS shows continuous improvement for all rounds of user feedbacks in interactive search.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {645–648},
numpages = {4},
keywords = {concept-based video retrieval, distribution, query-to-concept mapping},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631379,
author = {Fauzi, Fariza and Hong, Jer-Lang and Belkhatir, Mohammed},
title = {Webpage Segmentation for Extracting Images and Their Surrounding Contextual Information},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631379},
doi = {10.1145/1631272.1631379},
abstract = {Web images come in hand with valuable contextual information. Although this information has long been mined for various uses such as image annotation, clustering of images, inference of image semantic content, etc., insufficient attention has been given to address issues in mining this contextual information. In this paper, we propose a webpage segmentation algorithm targeting the extraction of web images and their contextual information based on their characteristics as they appear on webpages. We conducted a user study to obtain a human-labeled dataset to validate the effectiveness of our method and experiments demonstrated that our method can achieve better results compared to an existing segmentation algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {649–652},
numpages = {4},
keywords = {dom tree-based algorithm, www images, surrounding information, webpage segmentation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631380,
author = {Li, Lingfang and Zhang, Ning and Duan, Ling-Yu and Huang, Qingming and Du, Jun and Guan, Ling},
title = {Automatic Sports Genre Categorization and View-Type Classification over Large-Scale Dataset},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631380},
doi = {10.1145/1631272.1631380},
abstract = {This paper presents a framework with two automatic tasks targeting large-scale and low quality sports video archives collected from online video streams. The framework is based on the bag of visual-words model using speeded-up robust features (SURF). The first task is sports genre categorization based on hierarchical structure. Following on the second task which is based on automatically obtained genre, views are classified using support vector machines (SVMs). As a consequence, the views classification result can be used in video parsing and highlight extraction. As compared with state-of-the-art methods, our approach is fully automatic as well as domain knowledge free and thus provides a better extensibility. Furthermore, our dataset consists of 14 sport genres with 6850 minutes in total. Both sport genre categorization and view type classification have more than 80% accuracy rates, which validate this framework's robustness and potential in web-based applications.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {653–656},
numpages = {4},
keywords = {codebook, genre categorization, scene classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631381,
author = {Popescu, Adrian and Mo\"{e}llic, Pierre-Alain and Kanellos, Ioannis and Landais, R\'{e}mi},
title = {Lightweight Web Image Reranking},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631381},
doi = {10.1145/1631272.1631381},
abstract = {Web image search is inspired by text search techniques; it mainly relies on indexing textual data that surround the image file. But retrieval results are often noisy and image processing techniques have been proposed to rerank images. Unfortunately, these techniques usually imply a computational overload that makes the reranking process intractable in real time. We introduce here a lightweight reranking method that compares each result not only to the other query results but also to an external, contrastive class of items. The external class contains diversified images; the intuition supporting our approach is that results that are visually similar to other query results but dissimilar to elements of the contrastive class are likely to be good answers. The success of visual reranking depends on the visual coherence of queries; we measure this coherence in order to evaluate the chances of success. Visual reranking tends to emerge near duplicate images and we complement it with a diversification function which ensures that different aspects of a query are presented to the user. Our method is evaluated against a standard search engine using 210 diversified queries. Significant improvements are reported for both quantitative and qualitative tests.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {657–660},
numpages = {4},
keywords = {K-NN, image retrieval, reranking},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631382,
author = {Li, Xirong and Snoek, Cees G.M.},
title = {Visual Categorization with Negative Examples for Free},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631382},
doi = {10.1145/1631272.1631382},
abstract = {Automatic visual categorization is critically dependent on labeled examples for supervised learning. As an alternative to traditional expert labeling, social-tagged multimedia is becoming a novel yet subjective and inaccurate source of learning examples. Different from existing work focusing on collecting positive examples, we study in this paper the potential of substituting social tagging for expert labeling for creating negative examples. We present an empirical study using 6.5 million Flickr photos as a source of social tagging. Our experiments on the PASCAL VOC challenge 2008 show that with a relative loss of only 4.3% in terms of mean average precision, expert-labeled negative examples can be completely replaced by social-tagged negative examples for consumer photo categorization.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {661–664},
numpages = {4},
keywords = {negative examples, visual categorization, social tagging},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631383,
author = {Sidiropoulos, Panagiotis and Mezaris, Vasileios and Kompatsiaris, Ioannis and Meinedo, Hugo and Trancoso, Isabel},
title = {Multi-Modal Scene Segmentation Using Scene Transition Graphs},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631383},
doi = {10.1145/1631272.1631383},
abstract = {In this work the problem of automatic decomposition of video into elementary semantic units, known in the literature as scenes, is addressed. Two multi-modal automatic scene segmentation techniques are proposed, both building upon the Scene Transition Graph (STG). In the first of the proposed approaches, speaker diarization results are used for introducing a post-processing step to the STG construction algorithm, with the objective of discarding scene boundaries erroneously identified according to visual-only dissimilarity. In the second approach, speaker diarization and additional audio analysis results are employed and a separate audio-based STG is constructed, in parallel to the original STG based on visual information. The two STGs are subsequently combined. Preliminary results from the application of the proposed techniques to broadcast videos reveal their improved performance over previous approaches.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {665–668},
numpages = {4},
keywords = {scene segmentation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631384,
author = {Nishiyama, Masashi and Okabe, Takahiro and Sato, Yoichi and Sato, Imari},
title = {Sensation-Based Photo Cropping},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631384},
doi = {10.1145/1631272.1631384},
abstract = {This paper proposes a novel method for automatically cropping a photo using a quality classifier that assesses whether the cropped region is agreeable to users. We statistically build this quality classifier using large photo collections available on websites where people manually insert quality scores to photos. We first trim the original image and then decide on the candidates for cropping. We find the cropped region with the highest quality score by applying the quality classifier to the candidates. Current automatic photo cropping techniques search for attention grabbing regions that consist of salient pixels from the original photo. They are not always pleasant to users because they do not take into account the quality of the cropped region. Our method with the quality classifier outperforms a state-of-the-art method that takes into consideration only the user's attention for automatic photo cropping.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {669–672},
numpages = {4},
keywords = {sensation, quality, photograph, cropping},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631385,
author = {Thomee, Bart and Huiskes, Mark J. and Bakker, Erwin and Lew, Michael S.},
title = {Deep Exploration for Experiential Image Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631385},
doi = {10.1145/1631272.1631385},
abstract = {Experiential image retrieval systems aim to provide the user with a natural and intuitive search experience. The goal is to empower the user to navigate large collections based on his own needs and preferences, while simultaneously providing him with an accurate sense of what the database has to offer. In this paper we integrate a new browsing mechanism called deep exploration with the proven technique of retrieval by relevance feedback. In our approach, relevance feedback focuses the search on relevant regions, while deep exploration facilitates transparent navigation to promising regions of feature space that would normally remain unreachable. Optimal feature weights are determined automatically based on the evidential support for the relevance of each single feature. To achieve efficient refinement of the search space, images are ranked and presented to the user based on their likelihood of being useful for further exploration.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {673–676},
numpages = {4},
keywords = {content-based image retrieval, deep exploration, relevance feedback, feature space exploration, feature selection and weighting},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631386,
author = {Aradhye, Hrishikesh and Toderici, George D. and Yagnik, Jay},
title = {Adaptive, Selective, Automatic Tonal Enhancement of Faces},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631386},
doi = {10.1145/1631272.1631386},
abstract = {This paper presents an efficient, personalizable and yet completely automatic algorithm for enhancing the brightness, tonal balance, and contrast of faces in thumbnails of online videos where multiple colored illumination sources are the norm and artifacts such as poor illumination and backlight are common. These artifacts significantly lower the perceptual quality of faces and skin, and cannot be easily corrected by common global image transforms. The same identifiable user, however, often uploads or participates in multiple photos, videos, or video chat sessions with varying illumination conditions. The proposed algorithm adaptively transforms the skin pixels in a poor illumination environment to match the skin color model of a prototypical face of the same user in a better illumination environment. It leaves the remaining non-skin portions of the image virtually unchanged while ascertaining a smooth, natural appearance. A component of our system automatically selects such a prototypical face for each user given a collection of uploaded videos/photo albums or prior video chat sessions by that user. We present several human rating studies on YouTube data that quantitatively demonstrate significant improvement in facial quality using the proposed algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {677–680},
numpages = {4},
keywords = {color transfer, facial enhancement, image quality},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631387,
author = {Garau, Giulia and Ba, Sileye and Bourlard, Herv\'{e} and Odobez, Jean-Marc},
title = {Investigating the Use of Visual Focus of Attention for Audio-Visual Speaker Diarisation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631387},
doi = {10.1145/1631272.1631387},
abstract = {Audio-visual speaker diarisation is the task of estimating ``who spoke when'' using audio and visual cues.In this paper we propose the combination of an audio diarisation system with psychology inspired visual features, reporting experiments on multiparty meetings, a challenging domain characterised by unconstrained interaction and participant movements.More precisely the role of gaze in coordinating speaker turns was exploited by the use of Visual Focus of Attention features. Experiments were performed both with the reference and 3 automatic VFoA estimation systems, based on head pose and visual activity cues, of increasing complexity. VFoA features yielded consistent speaker diarisation improvements in combination with audio features using a multi-stream approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {681–684},
numpages = {4},
keywords = {visual focus of attention, audio-visual speaker diarisation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631388,
author = {Cooray, Saman H. and Bredin, Herv\'{e} and Xu, Li-Qun and O'Connor, Noel E.},
title = {An Interactive and Multi-Level Framework for Summarising User Generated Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631388},
doi = {10.1145/1631272.1631388},
abstract = {We present an interactive and multi-level abstraction framework for user-generated video (UGV) summarisation, allowing a user the flexibility to select a summarisation criterion out of a number of methods provided by the system. First, a given raw video is segmented into shots, and each shot is further decomposed into sub-shots in line with the change in dominant camera motion. Secondly, principal component analysis (PCA) is applied to the colour representation of the collection of sub-shots, and a content map is created using the first few components. Each sub-shot is represented with a "footprint" on the content map, which reveals its content significance (coverage) and the most dynamic segment. The final stage of abstraction is devised in a user-assisted manner whereby a user is able to specify a desired summary length, with options to interactively perform abstraction at different granularity of visual comprehension. The results obtained show the potential benefit in significantly alleviating the burden of laborious user intervention associated with conventional video editing/browsing.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {685–688},
numpages = {4},
keywords = {camera motion estimation, interactivity, ugvs, shot-cut detection, video summarisation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631389,
author = {Chen, Yanhua and Dong, Ming and Wan, Wanggen},
title = {Image Co-Clustering with Multi-Modality Features and User Feedbacks},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631389},
doi = {10.1145/1631272.1631389},
abstract = {In Content-based Image Retrieval (CBIR) research, advanced technology that fuses the heterogeneous information into image clustering has drawn extensive attention recently. However, using multiple features for co-clustering images without any user feedbacks is a challenging problem. In this paper, we propose a Semi-Supervised Non-negative Matrix Factorization (SS-NMF) framework for image co-clustering. Our method computes new relational matrices by incorporating user provided feedbacks into images through simultaneous distance metric learning and feature selection for different low-level visual features. Using an iterative algorithm, we perform tri-factorizations of the new matrices to infer image clusters. Theoretically, we show the convergence and correctness of SS-NMF co-clustering and the advantages of SS-NMF co-clustering over existing approaches. Through extensive experiments conducted on image data sets, we demonstrate that SS-NMF provides an effective and efficient solution for image co-clustering.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {689–692},
numpages = {4},
keywords = {multi-modality features, user feedbacks, co-clustering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631390,
author = {Liu, Danzhou and Hua, Kien A.},
title = {Transfer Non-Metric Measures into Metric for Similarity Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631390},
doi = {10.1145/1631272.1631390},
abstract = {Similarity search is widely used in multimedia retrieval systems to find the most similar ones for a given object. Some similarity measures, however, are not metric, leading to existing metric index structures cannot be directly used. To address this issue, we propose a simulated-annealing-based technique to derive optimized mapping functions that transfer non-metric measures into metric, and still preserve the original similarity orderings. Then existing metric index structures can be used to speed up similarity search by exploiting the triangular inequality property. The experimental study confirms the efficacy of our approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {693–696},
numpages = {4},
keywords = {relevance feedback, multimedia retrieval, non-metric similarity},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631391,
author = {Beecks, Christian and Uysal, Merih Seran and Seidl, Thomas},
title = {Signature Quadratic Form Distances for Content-Based Similarity},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631391},
doi = {10.1145/1631272.1631391},
abstract = {Determining similarity is a fundamental task in querying multimedia databases in a content-based way. For this challenging task, there exist numerous similarity models which measure the similarity among objects by using their contents. In order to cope with voluminous multimedia data, similarity models are supposed to be both effective and efficient. To this end, we introduce the Signature Quadratic Form Distance measure which allows efficient similarity computations based on flexible feature representations. Our new approach bridges the gap between the well-known concept of Quadratic Form Distances and feature signatures. Experimentation indicates that our similarity measure is able to compete with state-of-the-art similarity models regarding effectiveness of content-based similarity search. Moreover, our Signature Quadratic Form Distance outperforms the established Earth Mover's Distance in efficiency: we obtain a speed-up factor of greater than 50.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {697–700},
numpages = {4},
keywords = {feature signatures, quadratic form distance, content-based similarity search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631392,
author = {Tang, Feng and Gao, Yuli},
title = {Fast near Duplicate Detection for Personal Image Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631392},
doi = {10.1145/1631272.1631392},
abstract = {Due to the rapid growth in personal image collections, there is increasing interest on automatic detection of near duplicates. In this paper, we propose a novel fast near duplicate detection framework that takes advantages of heterogeneous features like EXIF data, global image histogram and local features. To improve the accuracy of local feature matching, we have developed a structure matching algorithm that takes into account of a local feature's neighborhood which can effectively reject mismatches. In addition, we developed a computation-sensitive cascade framework to combine stage classifiers trained on different feature spaces with different computational cost. This method can quickly accept easily identified duplicates using only cheap features without the need to extract more sophisticate but expensive ones. Compared with existing approaches, our experiments show very promising results using our new approach in terms of both efficiency and effectiveness.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {701–704},
numpages = {4},
keywords = {structure image matching, near duplicate detection, computation-sensitive cascade classifier},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631393,
author = {Ness, Steven R. and Theocharis, Anthony and Tzanetakis, George and Martins, Luis Gustavo},
title = {Improving Automatic Music Tag Annotation Using Stacked Generalization of Probabilistic SVM Outputs},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631393},
doi = {10.1145/1631272.1631393},
abstract = {Music listeners frequently use words to describe music. Personalized music recommendation systems such as Last.fm and Pandora rely on manual annotations (tags) as a mechanism for querying and navigating large music collections. A well-known issue in such recommendation systems is known as the cold-start problem: it is not possible to recommend new songs/tracks until those songs/tracks have been manually annotated. Automatic tag annotation based on content analysis is a potential solution to this problem and has recently been gaining attention. We describe how stacked generalization can be used to improve the performance of a state-of-the-art automatic tag annotation system for music based on audio content analysis and report results on two publicly available datasets.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {705–708},
numpages = {4},
keywords = {folksonomies, music information retrieval, tags, music recommendation, sound analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631394,
author = {Wu, Peng and Tretter, Dan},
title = {Close &amp; Closer: Social Cluster and Closeness from Photo Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631394},
doi = {10.1145/1631272.1631394},
abstract = {We investigate the discovery of social clusters from consumer photo collections. People's participation in various social activities is the base on which social clusters are formed. The photos that record those social activities can reflect the social structure of people to a certain degree, depending on the extent of coverage of the photos on the social activities. In this paper, we propose a scheme to construct a weighted undirected graph from photo collections by examining the co-appearance of individuals in photos, wherein the weights of edges are measures of the social closeness of the involved individuals (vertices in the graph). We further apply a graph clustering algorithm that maximizes the modularity of the graph partition to detect the embedded social clusters. The experiment results demonstrate that this scheme can reveal the social cluster with high precision rate. In addition, we also introduce a few photo management capabilities enabled by the social graph and discovered social clusters.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {709–712},
numpages = {4},
keywords = {graph clustering, social relationship, photo collection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631395,
author = {Rho, Seungmin and Han, Byeong-jun and Hwang, Eenjun},
title = {SVR-Based Music Mood Classification and Context-Based Music Recommendation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631395},
doi = {10.1145/1631272.1631395},
abstract = {With the advent of the ubiquitous era, context-based music recommendation has become one of rapidly emerging applications. Context-based music recommendation requires multidisciplinary efforts including low level feature extraction, music mood classification and human emotion prediction. Especially, in this paper, we focus on the implementation issues of context-based mood classification and music recommendation. For mood classification, we reformulate it into a regression problem based on support vector regression (SVR). Through the use of the SVR-based mood classifier, we achieved 87.8% accuracy. For music recommendation, we reason about the user's mood and situation using both collaborative filtering and ontology technology. We implement a prototype music recommendation system based on this scheme and report some of the results that we obtained.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {713–716},
numpages = {4},
keywords = {classification, recommendation, support vector regression, music mood, ontology},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631396,
author = {Yang, Yingzhen and Zhu, Yin and Peng, Qunsheng},
title = {Image Completion Using Structural Priority Belief Propagation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631396},
doi = {10.1145/1631272.1631396},
abstract = {A new image completion algorithm called Structural Priority Belief Propagation (SPBP) is presented to deal with LDV based image completion in this paper. LDV completion is a new form of image completion based on another large displacement view (LDV) of the same scene, no wonder, it has the potential of repairing large unknown region with salient structure information. In order to complete such unknown region, SPBP makes two important extensions over existing Priority-BP: dynamic weight of structural consistency and structural priority inheritance so as to propagate linear structure with correct priority, meanwhile it promotes texture propagation adhering to a global optimization scheme. Experimental results demonstrate that SPBP can obtain more satisfactory results than other LDV completion algorithms and it also performs well for traditional single image completion.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {717–720},
numpages = {4},
keywords = {image completion, large displacement view (LDV), structural priority belief propagation (SPBP)},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631397,
author = {Ramachandran, Chandrasekar and Malik, Rahul and Jin, Xin and Gao, Jing and Nahrstedt, Klara and Han, Jiawei},
title = {VideoMule: A Consensus Learning Approach to Multi-Label Classification from Noisy User-Generated Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631397},
doi = {10.1145/1631272.1631397},
abstract = {With the growing proliferation of conversational media and devices for generating multimedia content, the Internet has seen an expansion in websites catering to user-generated media. Most of the user-generated content is multimodal in nature as it has videos, audio, text (in the form of tags), comments and so on. Content analysis is a challenging problem on this type of media since it is noisy, unstructured and unreliable. In this paper we propose VideoMule, a consensus learning approach for multi-label video classification from noisy user-generated videos. In our scheme, we train classification and clustering algorithms on individual modes of information such as user comments, tags, video features and so on. We then combine the results of trained classifiers and clustering algorithms using a novel heuristic consensus learning algorithm which as a whole performs better than each individual learning model.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {721–724},
numpages = {4},
keywords = {video classification, multimodal information processing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631398,
author = {Wang, Li and Yang, Linjun and Tian, Xinmei},
title = {Query Aware Visual Similarity Propagation for Image Search Reranking},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631398},
doi = {10.1145/1631272.1631398},
abstract = {Image search reranking is an effective approach to refining the text-based image search result. In the reranking process, the estimation of visual similarity is critical to the performance. However, the existing measures, based on global or local features, cannot be adapted to different queries. In this paper, we propose to estimate a query aware image similarity by incorporating the global visual similarity, local visual similarity and visual word co-occurrence into an iterative propagation framework. After the propagation, a query aware image similarity combining the advantages of both global and local similarities is achieved and applied to image search reranking. The experiments on a real-world Web image dataset demonstrate that the proposed query aware similarity outperforms the global, local similarity and their linear combination, for image search reranking.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {725–728},
numpages = {4},
keywords = {similarity, visual reranking, image search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631399,
author = {Ramanathan, Subramanian and Katti, Harish and Huang, Raymond and Chua, Tat-Seng and Kankanhalli, Mohan},
title = {Automated Localization of Affective Objects and Actions in Images via Caption Text-Cum-Eye Gaze Analysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631399},
doi = {10.1145/1631272.1631399},
abstract = {We propose a novel framework to localize and label affective objects and actions in images through a combination of text, visual and gaze-based analysis. Human gaze provides useful cues to infer locations and interactions of affective objects. While concepts (labels) associated with an image can be determined from its caption, we demonstrate localization of these concepts upon learning from a statistical affect model for world concepts. The affect model is derived from non-invasively acquired fixation patterns on labeled images, and guides localization of affective objects (faces, reptiles) and actions (look, read) from fixations in unlabeled images. Experimental results obtained on a database of 500 images confirm the effectiveness and promise of the proposed approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {729–732},
numpages = {4},
keywords = {statistical model, caption text-cum-eye gaze analysis, automated localization and labeling, affect model for world concepts},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631400,
author = {Lepri, Bruno and Mana, Nadia and Cappelletti, Alessandro and Pianesi, Fabio},
title = {Automatic Prediction of Individual Performance from "Thin Slices" of Social Behavior},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631400},
doi = {10.1145/1631272.1631400},
abstract = {This paper targets the automatic detection of individual performances in group tasks by means of short sequences, "thin slices", of nonverbal behavior. We designed our task as a classification one. We also investigated the relevance of social context in our task and the effectiveness of our feature selection.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {733–736},
numpages = {4},
keywords = {intelligent environments, support vector machines, small group interaction, performance prediction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631401,
author = {Choujaa, Driss and Dulay, Naranker},
title = {Routine Classification through Sequence Alignment},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631401},
doi = {10.1145/1631272.1631401},
abstract = {In this paper we draw a methodological connection between human routine classification and the sequence alignment problem in bioinformatics. We first observe that human days exhibit important time shifts and therefore align them for comparison prior to classification. Our technique is evaluated on bimodal data including GSM and Bluetooth information collected on mobile phones. The introduction of new alignment features is found to significantly improve the accuracy of routine classification.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {737–740},
numpages = {4},
keywords = {profile-HMM, sequence alignment, mobile phone, human behaviour analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631402,
author = {Bailer, Werner and Rehatschek, Herwig},
title = {Comparing Fact Finding Tasks and User Survey for Evaluating a Video Browsing Tool},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631402},
doi = {10.1145/1631272.1631402},
abstract = {There are still no established methods for the evaluation of browsing and exploratory search tools. In the (multimedia) information retrieval community evaluations following the Cranfield paradigm (as e.g. used in TRECVID) have been widely adopted. We have applied two TRECVID style fact finding approaches (retrieval and question answering tasks) and a user survey to the evaluation of a video browsing tool. We analyze the correlation between the results of the different methods, whether different aspects can be evaluated independently with the survey, and if a learning effect can be measured with the different methods. The results show that the retrieval task correlates better with the user experience according to the survey than the question answering tasks. It turns out that the survey rather measures the general user experience while different aspects of the usability cannot be analyzed independently.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {741–744},
numpages = {4},
keywords = {evaluation, user study, video browsing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631403,
author = {Xie, Jinjing and Chen, Yiqiang and Liu, Junfa and Miao, Chunyan and Gao, Xingyu},
title = {Interactive 3D Caricature Generation Based on Double Sampling},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631403},
doi = {10.1145/1631272.1631403},
abstract = {Recently, 3D caricature generation and applications have attracted wide attention from both the research community and the entertainment industry. This paper proposes a novel interactive approach for various and interesting 3D caricature generation based on double sampling. Firstly, according to user's operation, we obtain a coarse 3D caricature with local features transformation by sampling in well-built principle component analysis (PCA) subspace. Secondly, to utilize information of the 2D caricature dataset, we sample in the local linear embedding (LLE) manifold subspace. Finally, we use the learned 2D caricature information to further refine the coarse caricature by applying Kriging interpolation. The experiments show that the 3D caricature generated by our method can preserve highly artistic styles and also reflect the user's intention.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {745–748},
numpages = {4},
keywords = {PCA, sampling, manifold, LLE, 3D caricature},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631404,
author = {Ma, Xiaojuan and Nikolova, Sonya and Cook, Perry R.},
title = {W2ANE: When Words Are Not Enough: Online Multimedia Language Assistant for People with Aphasia},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631404},
doi = {10.1145/1631272.1631404},
abstract = {In this paper, we introduce W2ANE, an Online Multimedia Language Assistant for individuals with aphasia, a language disorder that affects millions of people. W2ANE offers a rich online multimedia library (OMLA) supported by an adaptable and adaptive vocabulary scaffold (ViVA). The system, accessible over the Internet, provides a platform for applications such as looking up unknown words, constructing phrases for communication, practicing pronunciations, and accessing content. W2ANE also enables resource sharing and remote collaboration.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {749–752},
numpages = {4},
keywords = {adaptive vocabulary, aphasia, multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631405,
author = {Zhang, Luming and Song, Mingli and Li, Na and Bu, Jiajun and Chen, Chun},
title = {Feature Selection for Fast Speech Emotion Recognition},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631405},
doi = {10.1145/1631272.1631405},
abstract = {In speech based emotion recognition, both acoustic features extraction and features classification are usually time consuming,which obstruct the system to be real time. In this paper, we proposea novel feature selection (FSalgorithm to filter out the low efficiency features towards fast speech emotion recognition.Firstly, each acoustic feature's discriminative ability, time consumption and redundancy are calculated. Then, we map the original feature space into a nonlinear one to select nonlinear features,which can exploit the underlying relationship among the original features. Thirdly, high discriminative nonlinear feature with low time consumption is initially preserved. Finally, a further selection is followed to obtain low redundant features based on these preserved features. The final selected nonlinear features are used in features' extraction and features' classification in our approach, we call them qualified features. The experimental results demonstrate that recognition time consumption can be dramatically reduced in not only the extraction phase but also the classification phase. Moreover, a competitive of recognition accuracy has been observed in the speech emotion recognition.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {753–756},
numpages = {4},
keywords = {emotion recognition, nonlinear space, time consumption, feature selection, qualified features},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631406,
author = {Ren, Kan and Calic, Janko},
title = {FreeEye: Interactive Intuitive Interface for Large-Scale Image Browsing},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631406},
doi = {10.1145/1631272.1631406},
abstract = {Intuitive interfaces have become increasingly important multimedia applications, from personal photo collection to professional management systems. This paper presents a novel intuitive interactive interface for browsing of large image collections that visualizes underlying structure of the dataset by its size and spatial relations. In order to achieve this, images are initially clustered using an unsupervised graph-based clustering algorithm. By selecting images in a hierarchical layout of the screen, user can intuitively navigate through the collection. The experimental results demonstrate a significant speed-up in a content search scenario compared to a standard browsing interface, as well as inherent intuitiveness of the system.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {757–760},
numpages = {4},
keywords = {image clustering, multimedia systems, HCI, image browsing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631407,
author = {Rahman, ASM Mahfujur and Hossain, M. Anwar and Parra, Jorge and El Saddik, Abdulmotaleb},
title = {Motion-Path Based Gesture Interaction with Smart Home Services},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631407},
doi = {10.1145/1631272.1631407},
abstract = {In this paper, we propose a motion-path based gesture recognition technique and show its application in a smart home environment. Users hand gestures are recognized by capturing the motion-path while they draw different symbols in the air. In order to capture the motion-path, we use infra-red camera's IR sensing capability. The IR camera tracks the infra-red emitter attached to the user's hand gloves and produces a sequence of motion-points, which are then analyzed syntactically to recognize the intended hand gesture. The recognized gesture is used to interact with the intelligent environment for accessing various services. Toggling a lamp switch, changing the light intensity, and playing/pausing a movie are few examples where we have integrated the gesture-based interaction. Our experiment shows that the proposed gesture recognition technique is robust and its use in the smart home environment is interesting and appealing to the people.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {761–764},
numpages = {4},
keywords = {ambient media services, hand gesture, gesture-based interaction, smart home environment},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631408,
author = {Zhao, Liyue and Sukthankar, Gita},
title = {An Active Learning Approach for Segmenting Human Activity Datasets},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631408},
doi = {10.1145/1631272.1631408},
abstract = {Human activity datasets collected under natural conditions are an important source of data. Since these contain multiple activities in unscripted sequence, temporal segmentation of multimodal datasets is an important precursor to recognition and analysis. Manual segmentation is prohibitively time consuming and unsupervised approaches for segmentation are unreliable since they fail to exploit the semantic context of the data. Gathering labels for supervised learning places a large workload on the human user since it is relatively easy to gather a mass of unlabeled data but expensive to annotate. This paper proposes an active learning approach for segmenting large motion capture datasets with both small training sets and working sets. Support Vector Machines (SVMs) are learned using an active learning paradigm; after the classifiers are initialized with a small set of labeled data, the users are iteratively queried for labels as needed. We propose a novel method for initializing the classifiers, based on unsupervised segmentation and clustering of the dataset. By identifying and training the SVM with points from pure clusters, we can improve upon a random sampling strategy for creating the query set. Our active learning approach improves upon the initial unsupervised segmentation used to initialize the classifier, while requiring substantially less data than a fully supervised method; the resulting segmentation is comparable to the latter while requiring significantly less effort from the user.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {765–768},
numpages = {4},
keywords = {semi-supervised learning, active learning, motion segmentation, support vector machines (SVM)},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251359,
author = {Li, Baochun},
title = {Session Details: Short Papers Session 3: Applications and Systems},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251359},
doi = {10.1145/3251359},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631410,
author = {Sabin, Andrew T. and Pardo, Bryan},
title = {A Method for Rapid Personalization of Audio Equalization Parameters},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631410},
doi = {10.1145/1631272.1631410},
abstract = {Potential users of audio production software, such as audio equalizers, may be discouraged by the complexity of the interface. We describe a system that simplifies the interface by quickly mapping an individual's preferred sound manipulation onto parameters for audio equalization. This system learns mappings by presenting a sequence of equalizer settings to the user and correlating the gain in each frequency band with the user's preference rating. Learning typically converges in 25 user ratings (under two minutes). The system then creates a simple on-screen slider that lets the user manipulate the audio in terms of the descriptive term, without need to learn or use the parameters of an equalizer. Results are reported on the speed and effectiveness of the system for a set of 19 users and a set of five descriptive terms.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {769–772},
numpages = {4},
keywords = {interface, audio, peronalization, music, equalization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631411,
author = {Li, Guangda and Ming, Zhaoyan and Li, Haojie and Chua, Tat-Seng},
title = {Video Reference: Question Answering on YouTube},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631411},
doi = {10.1145/1631272.1631411},
abstract = {Community-based question answering systems have become very popular for providing answers to a wide variety of "how-to" questions. However most such systems present only textual answers. In many cases, users would prefer visual answers such as videos which are more direct and intuitive.Currently, there is very little research on automatically presenting precise reference videos based on user's question. In this paper, we explore how to leverage YouTube video collections as a source of reference to fulfilll such task and develop a novel multimedia application named:Video Reference. There are two steps to generating a video reference. The first is recall-driven video search, which is to increase the coverage of question by finding other similar questions. The second is precision-based video ranking. A three level ranking scheme based on visual analysis, opinion analysis and video redundancy is adopted to find the most relevant video reference from YouTube. Experiments conducted using questions from Consumer Electronics domain of Yahoo! Answers archive show the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {773–776},
numpages = {4},
keywords = {video question answering, video content analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631412,
author = {Tian, Hui and Zhou, Ke and Jiang, Hong and Feng, Dan},
title = {Digital Logic Based Encoding Strategies for Steganography on Voice-over-IP},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631412},
doi = {10.1145/1631272.1631412},
abstract = {This paper presents three encoding strategies based on digital logic for steganography on Voice over IP (VoIP), which aim to enhance the embedding transparency. Differing from previous approaches, our strategies reduce the embedding distortion by improving the similarity between the cover and the covert message using digital logical transformations, instead of reducing the amount of the substitution bits. Therefore, by contrast, our strategies will improve the embedding transparency without sacrificing the embedding capacity. Of these three strategies, the first one adopts logical operations, the second one employs circular shifting operations, and the third one combines the operations of the first two. All of them are evaluated through comparing their prototype implementations with some existing methods in a prototypical covert communication system based on VoIP (called StegVoIP). The experimental results show that the proposed strategies can effectively enhance the embedding transparency while maintaining the maximum embedding capacity.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {777–780},
numpages = {4},
keywords = {VoIP, steganography, digital logic, transparency},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631413,
author = {Anderson, Richard J. and Pranowo, Devy and Prince, Craig M. and Videon, Fred},
title = {Integrating Corrections into Digital Ink Playback},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631413},
doi = {10.1145/1631272.1631413},
abstract = {In this paper, we describe preliminary work on an ink editing application that allows an instructor to correct mistakes to digital ink written during a presentation that is to be archived. These corrections are then seamlessly reintegrated into the digital archive so that when the presentation is replayed the corrected ink is displayed instead of the original incorrect ink. We base our results on a system we have developed and prototype the work flow from initial presentation, through correction, updating the archive and playback. We show that a simple mechanism for correction is effective and low effort for the instructor. A key technical challenge that is addressed is the substitution of strokes by matching of the original and corrected ink.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {781–784},
numpages = {4},
keywords = {editing, workflow, lecture playback, classroom technology, digital ink},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631414,
author = {de Silva, Gamhewage C. and Aizawa, Kiyoharu},
title = {Retrieving Multimedia Travel Stories Using Location Data and Spatial Queries},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631414},
doi = {10.1145/1631272.1631414},
abstract = {We propose a system for retrieving multimedia related to a person's travel, using location data captured with a GPS receiver, mobile phone or a camera. The user makes simple sketches on a map displayed on a computer screen, to submit spatial, temporal or spatio-temporal queries regarding his travel. The system segments the location data and images, analyzes sketches made by a user, identifies the query, and retrieves relevant results. These results, combined with online maps and virtual tours rendered using street view panoramas, form multimedia travel stories. We present the system's current status and conclude with future directions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {785–788},
numpages = {4},
keywords = {sketch-based querying, travel diary, multimedia retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631415,
author = {Chen, Wei-Chao and Battestini, Agathe and Gelfand, Natasha and Setlur, Vidya},
title = {Visual Summaries of Popular Landmarks from Community Photo Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631415},
doi = {10.1145/1631272.1631415},
abstract = {We present a novel data-driven algorithm that leverages online image repositories such as Flickr for automatically generating tourist maps. Our hypothesis is that, given a large enough dataset of images with geo-based metadata, clusters of matching images from that dataset tend to provide reliable cues as to what the popular tourist spots may be. Our algorithm takes the geographical area of interest as input and retrieves geotagged photos from online photo collections. By clustering the photos based on their locations and identifying the popular tags for each cluster, our algorithm generates a set of points of interest (POIs) for the area. After retrieving additional photos based on these discovered POI tags, we use image matching to find the most representative landmark view for each POI. Finally, we remove clutter from the representative image and apply tooning to generate a map icon for each landmark.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {789–792},
numpages = {4},
keywords = {tourist maps, geotagged photos, points of interest},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631416,
author = {Liu, Dechao and Scott, Matthew R. and Ji, Rongrong and Jiang, Wei and Yao, Hongxun and Xie, Xing},
title = {Location Sensitive Indexing for Image-Based Advertising},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631416},
doi = {10.1145/1631272.1631416},
abstract = {This paper introduces the architecture of our location sensitive indexing model which is used in a platform designed to deliver advertisements to users who primarily utilize images as queries instead of textual keywords. The indexing model facilitates an advertiser's ability to bid on images, such as billboards or logos, in order to obtain user feedback in judging image attractiveness. Additionally, the model enables automatic evaluation of advertisement popularity by mining users' query logs, which is critical for generating advertisement recommendations. The location sensitive architecture of this model enables effective and efficient functionality in large-scale scenarios. In the model's structure, our Location Sensitive Visual Indexing model (LSVI) incorporates location information that subdivides geographical regions for precise and localized image matching. By collecting feedback from mobile users, location-based mining can also help discover popular advertisements as well as their representative images. We have deployed our platform into a real-world advertising system in Beijing, China, which demonstrates effective results in comparative studies with both alternative and state-of-the-art approaches.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {793–796},
numpages = {4},
keywords = {image content bidding, hybrid indexing structure, image search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631417,
author = {Shahabuddin, Sharmeen and Iqbal, Razib and Nazari, Ali and Shirmohammadi, Shervin},
title = {Compressed Domain Spatial Adaptation for H.264 Video},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631417},
doi = {10.1145/1631272.1631417},
abstract = {In this paper, we present a metadata-based compressed-domain spatial adaptation scheme for H.264/AVC video. We have enhanced the H.264/AVC encoder with our proposed adaptation strategies in order to reduce video size by cropping individual frames in an intermediary node prior to transmitting that video to heterogeneous devices. In this regard, we exploit the sliced architecture of the video frames within the first version of the H.264/AVC specification and devise different slicing strategies. The compressed-domain bitstream modification is performed at the intermediary nodes, avoiding the need for any cascaded operations. Here, we briefly present our adaptation scheme as well as evaluation results showing the effectiveness of the slicing strategies on the bitrate reduction and processing time. A comparison of our approach with an existing cropping scheme is also presented.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {797–800},
numpages = {4},
keywords = {spatial adaptation, cropping, H.264/avc, MPEG-21},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631418,
author = {Hao, Qiang and Cai, Rui and Wang, Xin-Jing and Yang, Jiang-Ming and Pang, Yanwei and Zhang, Lei},
title = {Generating Location Overviews with Images and Tags by Mining User-Generated Travelogues},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631418},
doi = {10.1145/1631272.1631418},
abstract = {Automatically generating location overviews in the form of both visual and textual descriptions is highly desired for online services such as travel planning, to provide attractive and comprehensive outlines of travel destinations. Actually, user-generated content (e.g., travelogues) on the Web provides abundant information to various aspects (e.g., landmarks, styles, activities) of most locations in the world. To leverage the experience shared by Web users, in this paper we propose a location overview generation approach, which first mines location-representative tags from travelogues and then uses such tags to retrieve web images. The learnt tags and retrieved images are finally presented via a novel user interface which provides an informative overview for a given location. Experimental results based on 23,756 travelogues and evaluation over 20 locations show promising results on both travelogue mining and location overview generation.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {801–804},
numpages = {4},
keywords = {location overview generation, virtual tour, travelogue mining},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631419,
author = {Luo, Dijun and Huang, Heng},
title = {Link Prediction of Multimedia Social Network via Unsupervised Face Recognition},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631419},
doi = {10.1145/1631272.1631419},
abstract = {We propose a new challenge for predicting links of social networks by unsupervised face recognition on photo albums. We solve the task by formulating it into Kernel Set Discovery problem. We enhance Affinity Propagation algorithm to tackle the problem with more constraints. More specifically, the face cannot appear more than once in the same photo and we impose constraints such that detected face images in the same photograph are never clustered into the same person. We construct a synthetic dataset based on AT&amp;T image benchmark for empirical validation. Moreover, we validate our algorithms by a real world application which contains a real friend relation on the Web 2.0 social network system. Results indicate our Constraint Affinity Propagation method is suitable to unsupervisedly predict links of social network.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {805–808},
numpages = {4},
keywords = {constraint affinity propagation, link prediction, social networks},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631420,
author = {Liu, Dong and Wang, Meng and Hua, Xian-Sheng and Zhang, Hong-Jiang},
title = {Smart Batch Tagging of Photo Albums},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631420},
doi = {10.1145/1631272.1631420},
abstract = {As one of the emerging Web 2.0 activities, tagging becomes a popular approach to manage personal media data, such as photo albums. However, exhaustively tagging all photos in an album is a labor-intensive and time-consuming task, and simply entering tags for the whole album will significantly degrade the tagging accuracy. In this paper, we propose a smart batch tagging scheme that aims at facilitating users in album tagging. For a given album, it selects a set of representative exemplars for manual tagging, where the number of exemplars is dependent on the content of the photos.Then the tags of the rest photos are automatically inferred.In this way, the number of tagged photos is significantly reduced and we will show that high tagging accuracy can still be maintained. Therefore, a good trade-off between manual efforts and tagging performance can be achieved. Experimental results have demonstrated the effectiveness and usefulness of the proposed approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {809–812},
numpages = {4},
keywords = {batch tagging, AP, flickr, propagation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631421,
author = {Negoescu, Radu-Andrei and Adams, Brett and Phung, Dinh and Venkatesh, Svetha and Gatica-Perez, Daniel},
title = {Flickr Hypergroups},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631421},
doi = {10.1145/1631272.1631421},
abstract = {The amount of multimedia content available online constantly increases, and this leads to problems for users who search for content or similar communities. Users in Flickr often self-organize in user communities through Flickr Groups. These groups are particularly interesting as they are a natural instantiation of the content~+~relations social media paradigm. We propose a novel approach to group searching through hypergroup discovery. Starting from roughly 11,000 Flickr groups' content and membership information, we create three different bag-of-word representations for groups, on which we learn probabilistic topic models. Finally, we cast the hypergroup discovery as a clustering problem that is solved via probabilistic affinity propagation. We show that hypergroups so found are generally consistent and can be described through topic-based and similarity-based measures. Our proposed solution could be relatively easily implemented as an application to enrich Flickr's traditional group search.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {813–816},
numpages = {4},
keywords = {flickr, hypergroups},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631422,
author = {Yan, Chih-Yu and Tien, Ming-Chun and Wu, Ja-Ling},
title = {Interactive Background Blurring},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631422},
doi = {10.1145/1631272.1631422},
abstract = {Photographers usually take a shallow focus image to highlight the main subject in the picture and blur the distractions in the background region. Softening these distractions can not only produce special vision effect but also keep the privacy for people in the background. In this work, we develop a brand new defocusing algorithm to simulate the blur effect in the background caused by a wide aperture camera. We also design an easy-to-use interactive interface for user to segment foreground/background objects, adjust the depth of field for images, and assign camera settings. Techniques including lazy snapping, alpha matting, depth information generation and defocusing are integrated in the system. The experimental results show the effectiveness of the proposed methodology.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {817–820},
numpages = {4},
keywords = {depth of field, defocusing, image refocusing, shallow focus},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631423,
author = {Chang, Ming-Hsiu and Tien, Ming-Chun and Wu, Ja-Ling},
title = {WOW: Wild-Open Warning for Broadcast Basketball Video Based on Player Trajectory},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631423},
doi = {10.1145/1631272.1631423},
abstract = {In basketball games, wild-open means that there is an offensive player not well defended by his/her opponents. The occurrence of wild-open usually implies the existence of a successful offense tactic. In this paper, a Wild-Open Warning (WOW) system is designed to assist basketball coaches/players in revealing possible tactics of their opponents through watching the broadcast game videos. The system automatically extracts semantic objects such as the court and the players in the video, and calibrates the players' positions to the real-world court coordinates. A robust and efficient algorithm for court detection and camera calibration is proposed for basketball videos. Wild-open is detected when the position of an offensive player satisfies three predefined criteria. In the mean time, the system will mark the wild-open players to warn the viewers such that they should keep attention to certain players.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {821–824},
numpages = {4},
keywords = {player tracking, camera calibration, basketball video},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631424,
author = {Anh, Nguyen Thi Nhat and Yang, Wenxian and Cai, Jianfei},
title = {Seam Carving Extension: A Compression Perspective},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631424},
doi = {10.1145/1631272.1631424},
abstract = {There is an increasing demand on image compression adaptive to different display sizes. However, existing spatial scalable coding only supports dyadic resolutions and is not content-aware. In this paper, we apply the recently developed image resizing algorithm, seam carving, for content-aware multi-size image compression. Our proposed codec encodes an image into a content-aware progressive bitstream that allows decoding into arbitrary display resolution. In addition, seam insertion is incorporated into the proposed framework to improve the performance in low bitrate image transmission applications. To the best of our knowledge, this is the first applicable content-aware multi-size image coding work in literature.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {825–828},
numpages = {4},
keywords = {image compression, multi-size, content-based, image resizing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631425,
author = {Bajcsy, Peter and McHenry, Kenton and Na, Hye-Jung and Malik, Rahul and Spencer, Andrew and Lee, Suk-Kyu and Kooper, Rob and Frogley, Mike},
title = {Immersive Environments for Rehabilitation Activities},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631425},
doi = {10.1145/1631272.1631425},
abstract = {This paper presents (a) new technologies for real-time immersion of humans into virtual reality (VR) environments with non-invasive real-time three-dimensional (3D) imaging, (b) a new methodology for evaluating immersive VR spaces in rehabilitation applications, and (c) experimental results documenting the benefits of Immersive VR (IVR) spaces for regaining proprioception. Our work focuses on designing immersive VR spaces with non-invasive multimedia sensory inputs where real time digital clones of humans are fused with virtual scenes for rehabilitation purposes. We hypothesize that humans with proprioceptive impairments can use their other senses as the proprioceptive feedback from real-time 3D+color reconstructions of their bodies in space. The objective is to investigate this hypothesis and quantify any benefits of immersive environments for regaining proprioception as one example of a rehabilitation application. The paper describes (a) the portable immersive VR system for real time 3D imaging, reconstruction and rendering, (b) a new methodology for quantitative evaluations of rehabilitation experiments in immersive VR spaces, and (c) the experimental results obtained for validating the above hypothesis with wheelchair basketball athletes. The novelty of the work lies in the first of its kind evaluation of the benefits of immersive VR spaces with multimedia cues for regaining proprioception.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {829–832},
numpages = {4},
keywords = {evaluation methodology, 3D video, citizens with disabilities},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631426,
author = {Biel, Joan-Isaac and Gatica-Perez, Daniel},
title = {Wearing a YouTube Hat: Directors, Comedians, Gurus, and User Aggregated Behavior},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631426},
doi = {10.1145/1631272.1631426},
abstract = {While existing studies on YouTube's massive user-generated video content have mostly focused on the analysis of videos, their characteristics, and network properties, little attention has been paid to the analysis of users' long-term behavior as it relates to the roles they self-define and (explicitly or not) play in the site. In this paper, we present a novel statistical analysis of aggregated user behavior in YouTube from the novel perspective of user categories, a feature that allows people to ascribe to popular roles and to potentially reach certain communities. Using a sample of 270,000 users, we found that a high level of interaction and participation is concentrated on a relatively small, yet significant, group of users, following recognizable patterns of personal and social involvement. Based on our analysis, we also show that by using simple behavioral features from user profiles, people can be automatically classified according to their category with accuracy rates of up to 73%.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {833–836},
numpages = {4},
keywords = {YouTube, user aggregated behavior, social networks, video-sharing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631427,
author = {Healy, Graham and Smeaton, Alan F.},
title = {An Outdoor Spatially-Aware Audio Playback Platform Exemplified by a Virtual Zoo},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631427},
doi = {10.1145/1631272.1631427},
abstract = {Outlined in this short paper is a framework for the construction of outdoor location-and direction-aware audio applications along with an example application to showcase the strengths of the framework and to demonstrate how it works. Although there has been previous work in this area which has concentrated on the spatial presentation of sound through wireless headphones, typically such sounds are presented as though originating from specific, defined spatial locations within a 3D environment. Allowing a user to move freely within this space and adjusting the sound dynamically as we do here, further enhances the perceived reality of the virtual environment. Techniques to realise this are implemented by the real-time adjustment of the presented 2 channels of audio to the headphones, using readings of the user's head orientation and location which in turn are made possible by sensors mounted upon the headphones.Aside from proof of concept indoor applications, more user-responsive applications of spatial audio delivery have not been prototyped or explored. In this paper we present an audio-spatial presentation platform along with a primary demonstration application for an outdoor environment which we call a virtual audio zoo. This application explores our techniques to further improve the realism of the audio-spatial environments we can create, and to assess what types of future application are possible.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {837–840},
numpages = {4},
keywords = {audio-spatial applications, spatial sound, HRTF, virtual zoo application},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631428,
author = {Peng, Yuxin and Lu, Zhiwu and Xiao, Jianguo},
title = {Semantic Concept Annotation Based on Audio PLSA Model},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631428},
doi = {10.1145/1631272.1631428},
abstract = {This paper proposes a new approach and algorithm for the semantic concept annotation based on audio PLSA (probabilistic latent semantic analysis) model. The novelty of our approach includes two sides: Audio vocabulary construction, and audio PLSA model. In audio vocabulary construction, we first segment an audio-clip into a few homogeneous audio-segments according to its content change, which not only capture the change property of audio-clip, but also keep and present the change relation and temporal order of audio features. Then an audio vocabulary is constructed by the RPCL (rival penalized competitive learning) clustering of audio-segments. In this way, each audio-clip can be represented by a bag-of-word form. In audio PLSA model, PLSA is employed to discover the latent topics existing in audio-clips. Based on the discovered topics, the concept classification is then carried out by a support vector machine (SVM) classifier. In addition, we also combine the local features extracted by PLSA and global features in audio-clip to further improve the performance of concept annotation. The experiments are evaluated on 85 hours of audio data from the TRECVID 2005, and show the encouraging results of our approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {841–844},
numpages = {4},
keywords = {audio vocabulary, audio PLSA model, concept annotation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631429,
author = {Liu, Anan and Zhang, Yongdong and Li, Jintao},
title = {Personalized Movie Recommendation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631429},
doi = {10.1145/1631272.1631429},
abstract = {Facing the vast amount of novel production in the movie industry, people are in favor of choosing their favorite candidates quickly and previewing movie contents conveniently so as to decide whether they appeal to their personal taste. To meet this growing need, researchers are paying more attention on Personalization and Recommendation, the new trends of multimedia information retrieval, by integrating content and contextual information. In this paper, we propose a hierarchical framework for personalized movie recommendation. First, movie weekly ranking information is utilized for movie association and recommendation. Then, an integrated graph with both movie content and user preference is constructed to generate dynamic movie synopsis for personalized navigation. The superiorities of the proposed method have two aspects: 1) The prior knowledge independent recommendation scheme is implemented to replace the traditional ranking method for novel information access; 2) Personalized movie synopsis is interactively produced to replace the current movie trailer for preview. The promising results of subjective evaluation indicate that the proposed framework can discover the latent relationship between movies as well as movie highlights and therefore provide personalized movie recommendation to effectively lead movie access in an individualized manner.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {845–848},
numpages = {4},
keywords = {movie, personalized, recommendation, semantic, user preference},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631430,
author = {Baccot, Benoit and Choudary, Omar and Grigoras, Romulus and Charvillat, Vincent},
title = {On the Impact of Sequence and Time in Rich Media Advertising},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631430},
doi = {10.1145/1631272.1631430},
abstract = {Nowadays, commercial websites use an increasing amount of rich media ads, since these ads grab users' attention more easily. Ad overloading has a counterproductive effect and we advocate a more parsimonious and finely-tuned use of ads. In this paper we stress the importance of using ads in the right sequence and at the right time. We also present how to use our web adaptation system for a classic web marketing multivariable test. The experimental results show the usefulness of optimizing banners' sequences and timing.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {849–852},
numpages = {4},
keywords = {rich media ads, advertisement impact, banner sequences},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631431,
author = {Ren, Tongwei and Liu, Yan and Wu, Gangshan},
title = {Image Retargeting Using Multi-Map Constrained Region Warping},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631431},
doi = {10.1145/1631272.1631431},
abstract = {Image retargeting aims to adapt images to various screens with small sizes and arbitrary aspect ratios. In this paper, we propose a novel image retargeting approach based on region warping, which emphasizes the image parts with important content while reducing the visual distortion over the whole image. First, the original image is decomposed into homogeneous regions and further represented by curve-edge trapezoid meshes. Then, two kinds of energy maps, importance map and sensitivity map, are calculated by visual attention model and weighted gradient map respectively. With mesh representation and energy map constraints, image retargeting is formulated to a constrained optimization problem of mesh vertexes relocation. Finally, the target image is generated by separately warping the regions based on the deduced optimal solution. The experiments on different images demonstrate the effective and efficiency of our algorithm.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {853–856},
numpages = {4},
keywords = {image retargeting, multiple energy maps, region warping, mesh representation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631432,
author = {Li, Qi and Ma, Huadong},
title = {GBED: Group Based Event Detection Method for Audio Sensor Networks},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631432},
doi = {10.1145/1631272.1631432},
abstract = {Audio event detection is one of the most important applications in the multimedia sensor networks. In this paper, we present a Group Based Event Detection method, called GBED, to detect the audio events in the audio sensor networks. In the proposed method, we train the basic audio events separately based on statistical learning, and combine them together by some prior knowledge in specific domains. By using GBED we can combine the statistical learning and human knowledge together and reduce the training complexity for complex real-world situations. We deploy this approach on an audio sensor network to evaluate its performance, and the experiment evaluations demonstrate that GBED can achieve satisfied results.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {857–860},
numpages = {4},
keywords = {audio event detection, audio surveillance systems, audio sensor networks},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631433,
author = {Sohn, Hosik and De Neve, Wesley and Ro, Yongman},
title = {Region-of-Interest Scrambling for Scalable Surveillance Video Using JPEG XR},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631433},
doi = {10.1145/1631272.1631433},
abstract = {Present-day video surveillance systems are often required not to intrude upon the privacy of the general public. In this paper, we discuss a privacy-protected video surveillance system that makes use of the JPEG XR standard. This standard offers a low-complexity solution for the scalable coding of high-resolution images. To address privacy concerns, face regions are detected and subsequently scrambled in the transform domain, taking into account the spatial and quality scalability features of JPEG XR. A number of experiments were conducted in order to investigate the efficiency of our video surveillance system, considering bit stream overhead and security aspects.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {861–864},
numpages = {4},
keywords = {JPEG XR, image coding, video surveillance, scrambling},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631434,
author = {Zhang, Wenhao and Men, Aidong and Chang, Kan},
title = {Adaptive Optimizing Filter for Inter-Layer Intra Prediction in SVC},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631434},
doi = {10.1145/1631272.1631434},
abstract = {Several inter-layer prediction techniques are adopted in the H.264 Scalable Video Coding Extension to increase the compression efficiency. For spatial scalable intra-coded frames, the prediction of macroblock in the enhancement layer is obtained by upsampling the co-located reconstructed block in the base layer. This inter-layer intra prediction can remove the redundancies between the different layers; however, its performance is limited by the non-ideal upsampling filter and the coding losses of base layer. In order to improve the performance of the inter-layer intra prediction, we propose an optimizing filtering method to enhance the prediction signal in this paper. Adaptive Wiener filters, which are calculated for each slice independently, are used to generate a prediction signal with minimum error energy in a statisitcal way. After this optimizing, the coding efficiency can be increased progressively. Experiments show that up to 6.45% and 15.45% bit rate reduction is achieved for QCIF-CIF scenario and CIF-4CIF scenario, respectively.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {865–868},
numpages = {4},
keywords = {H.264, scalable video coding, adaptive wiener filter, inter-layer prediction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631435,
author = {Shen, Zhijie and Zimmermann, Roger},
title = {ISP-Friendly Peer Selection in P2P Networks},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631435},
doi = {10.1145/1631272.1631435},
abstract = {Peer-to-peer (P2P) multicast is a scalable solution adopted by many video streaming systems. However, a prevalence of P2P applications has caused heavy traffic on the Internet. While P2P streaming users enjoy high-quality online video, the Internet Service Providers (ISPs) may experience large expenditure because of cross-ISP traffic. To reduce this traffic, it has been suggested that ISPs cooperate with P2P systems by exposing information about the networking layer to guide the topology construction. In this paper, we propose a novel peer selection algorithm that leverages the ISPs' service to form a local clustering topology through gossip communication. Additionally, this algorithm is adaptive to prevent peers from being trapped in a local cluster where they cannot obtain an adequate streaming rate.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {869–872},
numpages = {4},
keywords = {overlay multicast, streaming, peer selection, ISP-friendly},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631436,
author = {Liu, Qingzhong and Sung, Andrew H. and Qiao, Mengyu},
title = {Improved Detection and Evaluation for JPEG Steganalysis},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631436},
doi = {10.1145/1631272.1631436},
abstract = {Detection of information-hiding in JPEG images is actively delivered in steganalysis community due to the fact that JPEG is a widely used compression standard and several steganographic systems have been designed for covert communication in JPEG images. In this paper, we propose a novel method of JPEG steganalysis. Based on an observation of bi-variate generalized Gaussian distribution in Discrete Cosine Transform (DCT) domain, neighboring joint density features on both intra-block and inter-block are extracted. Support Vector Machines (SVMs) are applied for detection. Experimental results indicate that this new method prominently improves a current art of steganalysis in detecting several steganographic systems in JPEG images. Our study also shows that it is more accurate to evaluate the detection performance in terms of both image complexity and information hiding ratio.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {873–876},
numpages = {4},
keywords = {joint density, image complexity, Markov, JPEG, steganalysis, generalized gaussian distribution, SVM},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631437,
author = {Wu, Wanmin and Rivas, Raoul and Arefin, Ahsan and Shi, Shu and Sheppard, Renata M. and Bui, Bach D. and Nahrstedt, Klara},
title = {MobileTI: A Portable Tele-Immersive System},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631437},
doi = {10.1145/1631272.1631437},
abstract = {We present MobileTI, a portable tele-immersive system that merges 3D video representations of users in real time to enable remote collaboration across geographical distances. With portability as a main goal, we address the challenges in the camera setup, time synchronization, video acquisition, and networking in the design and implementation of the system. Having been deployed in public performances, MobileTI proves to be effective, efficient, and user-friendly. Our experimental findings in terms of technical performance and user feedback are presented.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {877–880},
numpages = {4},
keywords = {portability, tele-immersion},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631438,
author = {De Silva, Ransi Nilaksha and Cheng, Wei and Liu, Dan and Ooi, Wei Tsang and Zhao, Shengdong},
title = {Towards Characterizing User Interaction with Progressively Transmitted 3D Meshes},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631438},
doi = {10.1145/1631272.1631438},
abstract = {We collected traces of how 37 users interacted with 9 progressively streamed and rendered 3D meshes. We analyze the traces and discuss the insights that we learned in relation to design of efficient and scalable progressive mesh streaming systems. Our traces indicate that user actions are predictable and exhibit skewed access pattern. This finding could lead to design of efficient pre-fetching and caching techniques for progressive mesh streaming.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {881–884},
numpages = {4},
keywords = {user behavior, progressive meshes, interaction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631439,
author = {Ni, Pengpeng and Gaarder, Fredrik and Griwodz, Carsten and Halvorsen, P\r{a}l},
title = {Video Streaming into Virtual Worlds: The Effects of Virtual Screen Distance and Angle on Perceived Quality},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631439},
doi = {10.1145/1631272.1631439},
abstract = {There is an increasing trend to include streamed video data in 3D environments. Such environments allow potentially several concurrently visible videos on a single display device, and consequently, network and processing bottlenecks. As a first step towards an avoidance of such problems, we have performed subjective assessments using a 3D application prototype to determine how positioning of video in the 3D environment influences the user perception of reduced-quality videos. Using video clips from several genres, we have compared the influence of various ways to reduce video quality and users' perception of degraded quality. We evaluated the influence of distance and angle of the placement.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {885–888},
numpages = {4},
keywords = {video quality, 3D virtual world, user perception},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631440,
author = {Tournoux, Pierre-Ugo and Bouabdallah, Amine and Lacan, J\'{e}r\^{o}me and Lochin, Emmanuel},
title = {On-the-Fly Coding for Real-Time Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631440},
doi = {10.1145/1631272.1631440},
abstract = {Although ironically it does not offer any real-time guarantee, Internet is a popular solution to support multimedia time-constrained applications (e.g. VoIP, Video Conferencing, ...).Following this trend, this paper focuses on the performance of these applications by studying the benefit of using a novel reliability concept which aims at significantly improving the performance of these time constrained applications over lossy best-effort networks.This reliability mechanism emerged from several recent works from both network and coding theories. Its principle is to integrate feedbacks in an on-the fly coding scheme in order to optimize the trade-off "packet decoding delay" vs "throughput". We present the first evaluations of this mechanism for VoIP and video-conferencing applications for various erasure channels. Compared to classic block-based erasure codes, the results show significant gains in terms of quality observed by the user for both applications.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {889–892},
numpages = {4},
keywords = {video-conferencing, erasure code, reliability, VoIP},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631441,
author = {Malik, Rahul and Bajcsy, Peter},
title = {Achieving Color Constancy across Multiple Cameras},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631441},
doi = {10.1145/1631272.1631441},
abstract = {NOTE FROM ACM: It has been determined that this article plagiarized the contents of a previously published paper. Therefore ACM has shut off access to this paper.This article has been removed from the ACM Digital Library because it was found to plagiarize an earlier work written by Ilie and Welch published by IEEE and entitled . Upon discovering the plagiarism, Peter Bajcsy reported it to ACM.For further information, contact the },
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {893–896},
numpages = {4},
keywords = {color constancy, 3D tele-immersion},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631442,
author = {Tan, Yih Han and Lee, Wei Siong and Tham, Jo Yew},
title = {Complexity Control and Computational Resource Allocation during H.264/SVC Encoding},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631442},
doi = {10.1145/1631272.1631442},
abstract = {In this paper, a singularly parameterized complexity-scalable scheme is proposed for designing power-aware and power-adaptive H.264 video encoder. The proposed scheme enables adaptive control of the trade-off between the encoder's complexity and coding performance by dynamically updating a control parameter during encoding.This enables the encoder to perform optimally on a variety of computing platforms and runtime environments by adaption to instantaneous available computing resources and varying source video characteristics. The proposed scheme also allows computational resource allocation across layers in SVC encoders.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {897–900},
numpages = {4},
keywords = {H.264, encoder complexity control, scalable video coding},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631443,
author = {Liu, Yi and Hsu, Cheng-Hsin and Hefeeda, Mohamed},
title = {On the Benefits of Cooperative Video Broadcast over WMANs and WLANs},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631443},
doi = {10.1145/1631272.1631443},
abstract = {We study the problem of broadcasting video streams over a WMAN to many mobile devices. We propose to form a cooperative network among mobile devices that receive the same video stream, and share received video data over a WLAN. We analytically show that the proposed system outperforms current systems in terms of energy consumption and channel switching delay. Our trace-based simulation results show that the proposed system: (i) achieves as high as 70% of energy saving gain, (ii) outperforms current systems with only two cooperative mobile devices, (iii) reduce channel switching delay by up to 98%, (iv) is robust under device failure and quickly reacts to network dynamics, and (v) uniformly distributes the load on all cooperative devices.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {901–904},
numpages = {4},
keywords = {energy saving, mobile TV, video streaming, video broadcast},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631444,
author = {Yang, Wenming and Yu, Xiang and Liao, Qingmin},
title = {Personal Authentication Using Finger Vein Pattern and Finger-Dorsa Texture Fusion},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631444},
doi = {10.1145/1631272.1631444},
abstract = {Personal authentication has attracted great attention due to its large potential of security application, and many researches have shown that fusion of features or decisions obtained from various single-modal biometrics verification systems can enhance the overall performance of system. In this paper, we proposed a novel multimodal biometric approach fusing finger vein pattern with finger-dorsa texture. Firstly, Finger Vein image and finger-dorsa image from the same finger are captured simultaneously, and a method is designed to segment Regions Of Interest(ROI) of vein image and dorsal image. Secondly, two strategies are designed to extract finger vein pattern and finger-dorsa texture respectively. Vein extraction strategy consists of four steps: local thresholding, modified line tracking, thorough probability map creating and directional neighbor analysis. Gray normalization is performed on finger-dorsa image to extract main finger-dorsa texture. Thirdly, the binarized vein pattern and normalized dorsal texture are fused into one feature image. Finally, a block-based texture feature is proposed for personal authentication. Experimental results showed that the proposed fusion method outperforms any one of finger-dorsa and finger vein methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {905–908},
numpages = {4},
keywords = {multimodal biometric technology, feature fusion, finger-dorsa texture, finger vein pattern},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251360,
author = {Jain, Ramesh},
title = {Session Details: Panels},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251360},
doi = {10.1145/3251360},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631446,
author = {Zhang, Lei and Tian, Qi},
title = {Multimedia Content Analysis: Model-Based Approaches vs. Data-Driven Approaches},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631446},
doi = {10.1145/1631272.1631446},
abstract = {The explosive growth of multimedia data on the Web has had a great impact on research, and opened a potentially controversial topic in the multimedia community about model-based approaches and data-driven approaches. In this panel, we expect intelligent discussions between the panelists and the audience on this topic. For example, which approach is more advantageous: model-based or data-driven? How can these two approaches learn from the latest developments of the other to further improve existing limitations? The debate in this panel and the insight shared by the panelists are highly anticipated and expected to inspire and drive researchers to answer these questions and push them into working on other, related problems.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {909–910},
numpages = {2},
keywords = {image annotation, object recognition, data-driven approaches, multimedia content analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631447,
author = {Erol, Berna and Luo, Jiebo and Chang, Shih-Fu and Etoh, Minoru and Hon, Hsiao-Wuen and Lin, Qian and Setlur, Vidya},
title = {Mobile Media Search: Has Media Search Finally Found Its Perfect Platform? Part II},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631447},
doi = {10.1145/1631272.1631447},
abstract = {Recently, many exciting media search applications have been introduced to take advantage of smart phones' audiovisual capture capabilities and their being always on and connected. These applications address a real pain point for most mobile users and allow them to search with minimal text entry, if any. Is the mobile platform an ideal fit for media search? Are audio and visual signal processing technologies sufficiently accurate to support most mobile search applications? What are the killer applications of mobile media search? Earlier in 2009 at ICASSP, a panel on this topic stirred up great interest and enthusiasm while leaving many questions untouched due to the limited time.In this panel, we continue and expand the discussions on the challenges and potentials of mobile media search, with a different mix in the panel.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {911–912},
numpages = {2},
keywords = {interfaces, mobile media search, social networks, video retrieval, visualization, image retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251361,
author = {Venkatesh, Svetha and Satoh, Shin'ichi},
title = {Session Details: Tutorials},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251361},
doi = {10.1145/3251361},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631449,
author = {Yan, Rong and Hsu, Winston H.},
title = {Content-Based and Concept-Based Retrieval for Large-Scale Image/Video Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631449},
doi = {10.1145/1631272.1631449},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {913–914},
numpages = {2},
keywords = {content-based retrieval, large-scale image/video retrieval, concept-based retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631450,
author = {Tzanetakis, George},
title = {Music Information Retrieval: Theory and Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631450},
doi = {10.1145/1631272.1631450},
abstract = {The goal of this turorial is to provide a thorough theoretical overview of the state-of-the-art in Music Information Retrieval followed by a practical hands-on demonstration of several existing tools and resources that can be used for research in this area. Specific emphasis will be given on how MIR techniques relate to other fields of current multimedia research. MIR is an inherently interdisciplinary area touching on several research areas such as digital signal processing, machine learning, perception, visualization, human-computer interaction, content-based retrieval and digital libraries. Music has several unique characteristics that differentiate it from other areas of multimedia research. The different problems and techniques proposed to solve them that will described in the first part of the tutorial will be followed with concrete practical examples of applying these techniques using existing software tools and datasets.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {915–916},
numpages = {2},
keywords = {music information retrieval, sound analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631451,
author = {Chang, Edward Y. and Bai, Hongjie and Zhu, Kaihua},
title = {Parallel Algorithms for Mining Large-Scale Rich-Media Data},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631451},
doi = {10.1145/1631272.1631451},
abstract = {The amount of online photos and videos is now at the scale of tens of billions. To organize, index, and retrieve these large-scale rich-media data, a system must employ scalable data management and mining algorithms. The research community needs to consider solving large scale problems rather than solving problems with small datasets that do not reflect real life scenarios. This tutorial introduces key challenges in large-scale rich-media data mining, and presents parallel algorithms for tackling such challenges. We present our parallel implementations of Spectral Clustering (PSC), FP-Growth (PFP), Latent Dirichlet Allocation (PLDA), and Support Vector Machines (PSVM).},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {917–918},
numpages = {2},
keywords = {PLDA, large-scale machine learning, PSVM, large-scale data mining, PFP, distributed optimization, PSC, distributed computing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631452,
author = {Plagemann, Thomas and Goebel, Vera},
title = {The Future Internet and Its Prospects for Distributed Multimedia Systems and Applications},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631452},
doi = {10.1145/1631272.1631452},
abstract = {Today's Internet has evolved from an early experimental small-scale network to a world-wide network which infrastructure that is used on a day to day basis by companies, public bodies, governments, individuals etc. Many shortcomings of the Internet have been addressed over the last years as "add-ons", but recently many "Future Internet" initiatives have been started with the challenge to re-design the Internet to meet the future challenges. Since the majority of multimedia systems and applications are depending on the Internet we aim in this tutorial to give an overview on ongoing initiatives, identify new concepts and study how they might be leveraged for multimedia systems and applications.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {919–920},
numpages = {2},
keywords = {autonomic networks, multimedia requirements, future internet},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631453,
author = {Kim, Duk-Jin and Prabhakaran, Balakrishnan},
title = {Multimedia Aspects in Health Care},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631453},
doi = {10.1145/1631272.1631453},
abstract = {Recently, Body Sensor Networks (BSNs) are being deployed for monitoring and managing medical conditions as well as human performance in sports. These BSNs include various sensors such as accelerometers, gyroscopes, EMG (Electromyogram), EKG (Electro-cardiograms), and other sensors depending on the needs of the medical conditions. Data from these sensors are typically Time Series data and the data from multiple sensors form multiple, multidimensional time series data.This tutorial describes the technologies that go behind BSNs -- both in terms of the hardware infrastructure as well as the basic software. First, we outline the BSN hardware features and the related requirements. We then discuss the energy and communication choices for BSNs. Next, we discuss approaches for classification, data mining, visualization, and securing these data. We also show several demonstrations of body sensor networks as well as the software that aid in analyzing the data.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {921–922},
numpages = {2},
keywords = {qos, tinyos, body sensor networks, factor analysis., multidimensional, association rules},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631454,
author = {Lugmayr, Artur R.},
title = {Ambient Media, Ambient Media Computation, and Media Technology beyond the Current State},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631454},
doi = {10.1145/1631272.1631454},
abstract = {McLuhan's statement, "the medium is the message" is all-present in discussions within the media community. However, how does ubiquitous and pervasive computation impact when the medium is 'inside' the natural environment of humans? How do location based services, context awareness, emotional responsive interfaces, touch and gesture based interfaces, haptic devices, biometrics, sensor data fusion, mobile embedded systems, distributed networks, and smart data mining change the way of how media are presented, distributed, and consumed? These technical enablers go far beyond existing well known computer screen concepts or simple keyboard and mouse interaction methods. Within the scope of the tutorial several aspects and viewpoints of ambient media are presented: media, technology, HCI, and business. The tutorial presents case-studies and latest research methods in the field of ambient media. Examples of existing services are coming from ambient assisting living, user experience design, sensor networks, distributed systems, and mobile location based services ambient media are explained in further detail. The tutorial intends to train participants in the principles of ambient media and its concepts, content creation techniques, and methods. The tutorial rounds up with a more visionary viewpoint towards media technology in the future: the use of biological metaphors in presenting media, shortly called biological media - 'biomedia'.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {923–924},
numpages = {2},
keywords = {biomedia, biological media, media technology, pervasive computation, ubiqutious computation, ambient computation, ambient media},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251362,
author = {Natsev, Apostol},
title = {Session Details: Open Source Software Competition},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251362},
doi = {10.1145/3251362},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631456,
author = {Lux, Mathias},
title = {Caliph &amp; Emir: MPEG-7 Photo Annotation and Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631456},
doi = {10.1145/1631272.1631456},
abstract = {Caliph &amp; Emir are Java-based applications for image annotation and retrieval. They implement a large part of MPEG-7 descriptors and support annotation and retrieval based on the descriptors. Manual annotation is based on text and the MPEG-7 semantic description scheme. Automatic extraction of low level features and existing metadata is also supported. Retrieval features include: linear search, content based image retrieval, textual metadata and graph indexing, and two-dimensional repository visualization.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {925–926},
numpages = {2},
keywords = {metadata, MPEG-7, open source, multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631457,
author = {Wei\ss{}mann, Steffen and Gunn, Charles and Brinkmann, Peter and Hoffmann, Tim and Pinkall, Ulrich},
title = {JReality: A Java Library for Real-Time Interactive 3D Graphics and Audio},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631457},
doi = {10.1145/1631272.1631457},
abstract = {We introduce jReality, a Java library for creating real-time interactive audiovisual applications with three-dimensional computer graphics and spatialized audio. Applications written for jReality will run unchanged on software and hardware platforms ranging from desktop machines with a single screen and stereo speakers to immersive virtual environments with motion tracking, multiple screens with 3D stereo projection, and multi-channel audio.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {927–928},
numpages = {2},
keywords = {interactive spatial audio, java, immersive environments, virtual reality},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631458,
author = {Rombaut, Alexis and Staelens, Nicolas and Vercammen, Nick and Vermeulen, Brecht and Demeester, Piet},
title = {XStreamer: Modular Multimedia Streaming},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631458},
doi = {10.1145/1631272.1631458},
abstract = {The xStreamer intends to be a flexible and modular open source streamer. The selection of current open source streamers which support both video and audio is limited, with VLC Media Player, Darwin Streaming Server and Helix DNA Server being the foremost solutions. The xStreamer distinguishes itself by providing a modularity that goes beyond the mere modular programming offered by the current open source solutions and that manifests itself in how the user controls and configures the streamer.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {929–930},
numpages = {2},
keywords = {multimedia, modular, streaming, open source},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631459,
author = {Tzanetakis, George},
title = {Music Analysis, Retrieval and Synthesis of Audio Signals MARSYAS},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631459},
doi = {10.1145/1631272.1631459},
abstract = {Marsyas is an open source software framework for music analysis, retrieval and synthesis with specific emphasis on Music Information Retreival applications. It has been in development for 10 years and has been used for a variety of projects in both academia and industry in several countries. Based on a novel dataflow architecture named implicit patching it provides a variety of existing processing modules for digital signal processing, machine learning and audio input/output that can be combined at run-time to form complex dataflow networks expressing audio processing algorithms (black-box functionality). In addition it allows the easy addition of new processing modules that need to be compiled for performance purposes. Finally Marsyas is designed with inter-operability in mind and provides various mechanisms for communicating with other software including bindings to the run-time functionality in scripting languages (Python, Ruby), run-time data interchange with MATLAB, support for the Music Instrument Digital Interface (MIDI) protocol and Open Sound Control (OSC) for communicating with controller devices, and infrastructure for easy interfacing to the GUI components of the Qt toolkit.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {931–932},
numpages = {2},
keywords = {music information retrieval, sound analysis},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631460,
author = {Misra, Ananya and Wang, Ge and Cook, Perry R.},
title = {TAPESTREA: A New Way to Design Sound},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631460},
doi = {10.1145/1631272.1631460},
abstract = {TAPESTREA is a sound design and composition framework that facilitates the creation of new sound from existing digital audio recordings, through interactive analysis, transformation and re-synthesis. During analysis, sound templates of different types are extracted using a variety of techniques. Each extracted template is transformed and synthesized independently, allowing specialized transformations on each template based on its type. The user interacts with TAPESTREA via a set of graphical interfaces that offer parametric control over every stage of analysis, transformation and re-synthesis. Synthesis is further controlled through ChucK scripts. These combined techniques form a workbench for completely transforming a sound scene, dynamically generating soundscapes, or creating musical tapestries by weaving together transformed elements from different recordings. Thus, TAPESTREA introduces a new paradigm for composition, sound design, and sonic sculpting tasks.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {933–934},
numpages = {2},
keywords = {sound design, real-time, multimedia, audio, signal processing, composition, open source software},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251363,
author = {Kankanhalli, Mohan},
title = {Session Details: Brave New Topics},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251363},
doi = {10.1145/3251363},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631462,
author = {Benko, Hrvoje},
title = {Beyond Flat Surface Computing: Challenges of Depth-Aware and Curved Interfaces},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631462},
doi = {10.1145/1631272.1631462},
abstract = {In the past decade, multi-touch-sensitive interactive surfaces have transitioned from pure research prototypes in the lab, to commercial products with wide-spread adoption. One of the longer term visions of this research follows the idea of ubiquitous computing, where everyday surfaces in our environment are made interactive. However, most of current interfaces remain firmly tied to the traditional flat rectangular displays of the today's computers and while they benefit from the directness and the ease of use, they are often not much more than touch-enabled standard desktop interfaces.In this paper, we argue for explorations that transcend the traditional notion of the flat display, and envision interfaces that are curved, three-dimensional, or that cross the boundary between the digital and physical world. In particular, we present two research directions that explore this idea: (a) exploring the three-dimensional interaction space above the display and (b) enabling gestural and touch interactions on curved devices for novel interaction possibilities. To illustrate both of these, we draw examples from our own work and the work of others, and guide the reader through several case studies that highlight the challenges and benefits of such novel interfaces. The implications on media requirements and collaboration aspects are discussed in detail, and, whenever possible, we highlight promising directions of future research. We believe that the compelling application design for future non-flat user interfaces will greatly depend on exploiting the unique characteristics of the given form factor.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {935–944},
numpages = {10},
keywords = {interactive surfaces, depth-sensing cameras, spherical displays, multi-touch interactions, gestures, surface computing, curved interfaces},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631463,
author = {Wang, Jun and Pohlmeyer, Eric and Hanna, Barbara and Jiang, Yu-Gang and Sajda, Paul and Chang, Shih-Fu},
title = {Brain State Decoding for Rapid Image Retrieval},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631463},
doi = {10.1145/1631272.1631463},
abstract = {Human visual perception is able to recognize a wide range of targets under challenging conditions, but has limited throughput. Machine vision and automatic content analytics can process images at a high speed, but suffers from inadequate recognition accuracy for general target classes. In this paper, we propose a new paradigm to explore and combine the strengths of both systems. A single trial EEG-based brain machine interface (BCI) subsystem is used to detect objects of interest of arbitrary classes from an initial subset of images. The EEG detection outcomes are used as input to a graph-based pattern mining subsystem to identify, refine, and propagate the labels to retrieve relevant images from a much larger pool. The combined strategy is unique in its generality, robustness, and high throughput. It has great potential for advancing the state of the art in media retrieval applications. We have evaluated and demonstrated significant performance gains of the proposed system with multiple and diverse image classes over several data sets, including those from Internet (Caltech 101) and remote sensing images. In this paper, we will also present insights learned from the experiments and discuss future research directions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {945–954},
numpages = {10},
keywords = {brain computer interface, image annotation and search, noisy label refinement, visual pattern mining},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251364,
author = {Hua, Xian-Sheng},
title = {Session Details: Technical Demonstrations Session 1},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251364},
doi = {10.1145/3251364},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631465,
author = {Doran, Andra and Mondet, Sebastien and Grigoras, Romulus and Morin, Geraldine and Ooi, Wei Tsang and Boudon, Frederic},
title = {A Demonstration of MobiTree: Progressive 3D Tree Models Streaming on Mobile Clients},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631465},
doi = {10.1145/1631272.1631465},
abstract = {We demonstrate MobiTree, a system we built that allows progressive streaming and rendering of 3D tree models on a mobile phone. MobiTree enables user to retrieve not only typical media describing a species (such as text and image), but also a 3D model that yields detail information about the structure of the branches and the foliage. MobiTree adopts our previous proposed progressive representation to speed up display of trees at the mobile client, trading off latency and quality. Progressivity also allows MobiTree to flexibly adopt the level of details to the capability of the mobile devices.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {955–956},
numpages = {2},
keywords = {streaming, mobile applications, progressive transmission, progressive coding, plant models},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631466,
author = {Wang, Xin-Jing and Yu, Mo and Zhang, Lei and Ma, Wei-Ying},
title = {Argo: Intelligent Advertising Made Possible from Users' Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631466},
doi = {10.1145/1631272.1631466},
abstract = {Though monetizing user-generated photos has a great potential in image business, this topic is seldom touched due to the difficulties of both image understanding and ads-to-images vocabulary matching. In this technical demonstration, we show case the Argo system, which attempts to monetize UGC (user-generated content) photos by mining a user's interest from a group of his photos and advertising the photos accordingly. Given a page of photos, it first auto-tags each photo by a large-scale search-based image annotation method, then maps both image annotations and the textual descriptions of ads onto an ODP-based topic hierarchy. The mapping produces semantic features which are statistical distributions on ODP topics. Ads are ranked by their similarities to such topic distributions of the photos and the top-ranked ones are output.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {957–958},
numpages = {2},
keywords = {user interest modeling, image understanding, photo monetization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631467,
author = {Lam, Billy and Stavness, Ian and Barr, Ryan and Fels, Sidney},
title = {Interacting with a Personal Cubic 3D Display},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631467},
doi = {10.1145/1631272.1631467},
abstract = {We describe a demonstration of four novel interaction techniques for a cubic head-coupled 3D display. The interactions illustrated include: viewing a static scene, navigating through a large landscape, playing with colliding objects inside a box, and stylus-based manipulation of objects. Users experience new interaction techniques for 3D scene manipulation in a cubic display.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {959–960},
numpages = {2},
keywords = {3D display, physical interaction, handheld device, user interface, multi-screen display, head-coupled rendering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631468,
author = {Zheng, Yan-Tao and Zhao, Ming and Song, Yang and Adam, Hartwig and Buddemeier, Ulrich and Bissacco, Alessandro and Brucher, Fernando and Chua, Tat-Seng and Neven, Hartmut and Yagnik, Jay},
title = {Tour the World: A Technical Demonstration of a Web-Scale Landmark Recognition Engine},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631468},
doi = {10.1145/1631272.1631468},
abstract = {We present a technical demonstration of a world-scale touristic landmark recognition engine. To build such an engine, we leverage ~21.4 million images, from photo sharing websites and Google Image Search, and around two thousand web articles to mine the landmark names and learn the visual models. The landmark recognition engine incorporates 5312 landmarks from 1259 cities in 144 countries. This demonstration gives three exhibits: (1) a live landmark recognition engine that can visually recognize landmarks in a given image; (2) an interactive navigation tool showing landmarks on Google Earth; and (3) sample visual clusters (landmark model images) and a list of 1000 randomly selected landmarks from our recognition engine with their iconic images.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {961–962},
numpages = {2},
keywords = {touristic landmark recongition},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631469,
author = {Luo, Zhiping and Li, Haojie and Tang, Jinhui and Hong, Richang and Chua, Tat-Seng},
title = {ViewFocus: Explore Places of Interests on Google Maps Using Photos with View Direction Filtering},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631469},
doi = {10.1145/1631272.1631469},
abstract = {This paper presents a novel system to explore places of interests based on the large amount of photos that are placed on Google Maps. The system, named ViewFocus, estimates the view directions of photos via robust object matching and camera reconstruction techniques, and geo-registers the directions on the map. Thus users are able to select the places they are interested in, and the system automatically returns a set of precise photos of the target places for users to focus their exploration, by filtering out photos that are pointing to other directions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {963–964},
numpages = {2},
keywords = {view direction, map interface, world explore},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631470,
author = {Alisi, Thomas and Bertini, Marco and D'Amico, Gianpaolo and Del Bimbo, Alberto and Ferracani, Andrea and Pernici, Federico and Serra, Giuseppe},
title = {Arneb: A Rich Internet Application for Ground Truth Annotation of Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631470},
doi = {10.1145/1631272.1631470},
abstract = {In this technical demonstration we show the current version of Arneb, a web-based system for manual annotation of videos, developed within the EU VidiVideo project. This tool has been developed with the aim of creating ground truth annotations, that can be used for training and evaluating automatic video annotation systems. Annotations can be exported to MPEG-7 and OWL ontologies. The system has been developed according to the Rich Internet Application paradigm, allowing collaborative web-based annotation.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {965–966},
numpages = {2},
keywords = {video streaming, video annotation, ground truth},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631471,
author = {Alisi, Thomas and Bertini, Marco and D'Amico, Gianpaolo and Del Bimbo, Alberto and Ferracani, Andrea and Pernici, Federico and Serra, Giuseppe},
title = {Sirio: An Ontology-Based Web Search Engine for Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631471},
doi = {10.1145/1631272.1631471},
abstract = {In this technical demonstration we show a web video search engine based on ontologies, the Sirio system, that has been developed within the EU VidiVideo project. The goal of the system is to provide a search engine for videos for both technical and non-technical users. In fact, the system has different interfaces that permit different query modalities: free-text, natural language, graphical composition of concepts using boolean and temporal relations and query by visual example. In addition, the ontology structure is exploited to encode semantic relations between concepts permitting, for example, to expand queries to synonyms and concept specializations.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {967–968},
numpages = {2},
keywords = {video retrieval, ontologies, web services},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631472,
author = {Liao, Chunyuan and Liu, Qiong},
title = {PACER: Toward a Cameraphone-Based Paper Interface for Fine-Grained and Flexible Interaction with Documents},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631472},
doi = {10.1145/1631272.1631472},
abstract = {Existing cameraphone-based interactive paper systems fall short of the flexibility of GUIs, partly due to their deficient fine-grained interactions, limited interaction styles and inadequate targeted document types. We present PACER, a platform for applications to interact with document details (e.g. individual words, East Asian characters, math symbols, music notes, and user-specified arbitrary image regions) of generic paper documents through a camera phone. With a see-through phone interface, a user can discover symbol recurrences in a document by pointing the phone's crosshair to a symbol within a printout. The user can also continuously move the phone over a printout for gestures to copy and email an arbitrary region, or play music notes on the printout.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {969–970},
numpages = {2},
keywords = {generic document, fine-grained, paper interface, camera phone},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631473,
author = {Spicer, Ryan P. and Lin, Yu-Ru and Kelliher, Aisling and Sundaram, Hari},
title = {A Slide-Ware Application to Support Discursive Presentations},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631473},
doi = {10.1145/1631272.1631473},
abstract = {Transdisciplinary collaborations call for dynamic, responsive slide-ware presentations beyond the linear structure afforded by traditional tools. The NextSlidePlease application addresses this through a novel authoring and presentation interface. The application also features an innovative algorithm to enhance presentation time management. The cross-platform Java application is currently being evaluated in a variety of real-world presentation contexts.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {971–972},
numpages = {2},
keywords = {presentations, hypermedia, slide-ware, navigation, hyperpresentations},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631474,
author = {Zhang, Bingjun and Xiang, Qiaoliang and Wang, Ye and Shen, Jialie},
title = {CompositeMap: A Novel Music Similarity Measure for Personalized Multimodal Music Search},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631474},
doi = {10.1145/1631272.1631474},
abstract = {How to measure and model the similarity between different music items is one of the most fundamental yet challenging research problems in music information retrieval. This paper demonstrates a novel multimodal and adaptive music similarity measure (CompositeMap) with its application in a personalized multimodal music search system. CompositeMap can effectively combine music properties from different aspects into compact signatures via supervised learning, which lays the foundation for effective and efficient music search. In addition, an incremental Locality Sensitive Hashing algorithm is developed to support more efficient search processes. Experimental results based on two large music collections reveal various advantages in effectiveness, efficiency, adaptiveness, and scalability of the proposed music similarity measure and the music search system.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {973–974},
numpages = {2},
keywords = {personalization, music, search, similarity measure},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631475,
author = {Lee, Shang Ping and Qui, Tran Cong Thien and Loy, Shing Chuan and Pensyl, William Russell},
title = {Haptic Interaction in Augmented Reality},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631475},
doi = {10.1145/1631272.1631475},
abstract = {In an interactive augmented reality (AR) system, people are, most of the time, immersed in an environment rich of visual and audio feedback. The sense of touch is lacking in most of the AR environment. We present an augmented reality system where people can see life-size virtual human avatar in a real environment, and be able to have a physical haptic interaction with the avatar.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {975–976},
numpages = {2},
keywords = {large-scale virtual and augmented reality, haptic interaction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631476,
author = {Arslan Ay, Sakire and Zhang, Lingyan and Kim, Seon Ho and He, Ma and Zimmermann, Roger},
title = {GRVS: A Georeferenced Video Search Engine},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631476},
doi = {10.1145/1631272.1631476},
abstract = {An increasing number of recorded videos are being tagged with geographic properties of the camera scenes. This meta-data is of significant use for storing, indexing and searching large collections of videos. By considering video related meta-information, more relevant and precisely delimited search results can be returned. Our system implementation demonstrates a prototype of a georeferenced video search engine (GRVS) that utilizes an estimation model of a camera's viewable scene for efficient video search. For video acquisition, our system provides an automated annotation software that captures videos and their respective field of views (FOV). The acquisition software allows community-driven data contributions to the search engine.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {977–978},
numpages = {2},
keywords = {GPS, georeferencing, meta-data fusion, video search},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631477,
author = {Gao, Yuli and Atkins, Clayton Brian and Cheatle, Phil and Xiao, Jun and Zhang, Xuemei and Chao, Hui and Wu, Peng and Tretter, Daniel and Slatter, David and Carter, Andrew and Penny, Roland and Willis, Chris},
title = {MagicPhotobook: Designer Inspired, User Perfected Photo Albums},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631477},
doi = {10.1145/1631272.1631477},
abstract = {Computer-assisted photo album creation continues to be a challenging application as it requires integrated technical solutions to many difficult problems. Effective solutions must leverage both design knowledge and image understanding algorithms to automate time-consuming tasks like image selection, grouping, cropping, layout and background selection. At the same time, they should allow the user to cater to personal tastes by fine-tuning aspects of album appearance. MagicPhotobook is a photobook authoring system that takes steps in these directions by providing advances over prior solutions in the following areas: automatic image selection and theme-based image grouping; dynamic page layout; automatic cropping; automatic background selection; design-preserving background artwork transformation; and a simple yet powerful user interface for personalization.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {979–980},
numpages = {2},
keywords = {user-centered design, background assignment, image triage, photo albums, auto-crop, image clustering, page layout},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631478,
author = {Shi, Liang and Wang, Jinqiao and Duan, Lingyu and Lu, Hanqing},
title = {Sports Video Retargeting},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631478},
doi = {10.1145/1631272.1631478},
abstract = {With the proliferation of diverse multimedia terminals, the request for elegantly retargeting videos to different display devices is evident, especially in sports. This demonstration presents a Sports Video Retargeting(SVR) technique, that utilized domain based structure parsing to build a semantic importance map for video retargeting. The system enables flexible and coherent aspect-ratio change of the output sports videos with a spatial-temporal 3D rectilinear grid framework, which are free from significant loss of information or distortion on salient and important regions. Results in various sports type have shown that SVR is promising for content adaptation on mobile media.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {981–982},
numpages = {2},
keywords = {domain-based analysis, video retargeting},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631479,
author = {Liu, Yiming and Xu, Dong and Tsang, Ivor W. and Luo, Jiebo},
title = {T-IRS: Textual Query Based Image Retrieval System for Consumer Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631479},
doi = {10.1145/1631272.1631479},
abstract = {In this demonstration, we present a (quasi) real-time textual query based image retrieval system (T-IRS) for consumer photos by leveraging millions of web images and their associated rich textual descriptions (captions, categories, etc.). After a user provides a textual query (e.g., "boat"), our system automatically finds the positive web images that are related to the textual query "boat" as well as the negative web images which are irrelevant to the textual query. Based on these automatically retrieved positive and negative web images, we employ the decision stump ensemble classifier to rank personal consumer photos. To further improve the photo retrieval performance, we also develop a novel relevance feedback method, referred to as Cross-Domain Regularized Regression (CDRR), which effectively utilizes both the web images and the consumer images. Our system is inherently not limited by any predefined lexicon.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {983–984},
numpages = {2},
keywords = {cross domain learning, text based photo retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631480,
author = {Seeliger, Robert and Friedrich, Oliver and Arbanowski, Stefan},
title = {The Future of TV and Media Services: Interactive Live Demo of Fraunhofer FOKUS Standardized Converged Rich Media and IPTV Solution},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631480},
doi = {10.1145/1631272.1631480},
abstract = {IPTV, TV2.0, interactive TV, TV over Next Generation Networks, convergence, triple and quadruple play - all these words become mandatory and part of the language used when getting in touch with a proposed new kind of media: Television and corresponding interactive premium services delivered over IP networks. The Competence Center Future Applications and Media bundles Fraunhofer FOKUS expertise in the areas of interactive applications, streaming technologies, multimedia content and telecommunication services in Next Generation Networks (NGN). The Lab realizes standard compliant Service Delivery Platforms, Client solutions and tools to enable real interactive, personalized and context-aware rich media content for a wide range of application domains. In this direction an ecosystem for Next Generation IPTV Services and Rich Media experience has been built on top of state-of-the-art technologies following upcoming standardized solutions for NGN-based IPTV by ETSI TISPAN R2 and Open IPTV Forum.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {985–986},
numpages = {2},
keywords = {personalization, interactivity, NGN, convergence, IPTV, standardization, IMS, community, rich media},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631481,
author = {Wu, Zhipeng and Jiang, Shuqiang and Huang, Qingming},
title = {Friend Recommendation According to Appearances on Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631481},
doi = {10.1145/1631272.1631481},
abstract = {Unlike the questionnaire based friend recommendation scheme used in Social Network Service (SNS) websites nowadays (e.g. online dating sites, online matchmaking sites), we focus on the fact that most of the online users may be interested in the strangers whose appearances are somehow attractive according to their own preferences. In this paper, we present a friend recommendation system based on the appearances on photos. The system is built upon 5000 portraits photos as source dataset with another 50 photos as training set. Once the user provides rating to several photos in the training set, we first build his/her appearance prefe-rence model based on face detection and multi-features cooperation. Then, the images in the source are ranked according to different features respectively. Finally, the results of multi-features are fused via the method of Borda count. The system is a useful complement to the conventional psychological tests based friend rec-ommendation scheme. It is easy to play with and of a lot of fun.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {987–988},
numpages = {2},
keywords = {friend recommendation, appearance preference},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631482,
author = {Johansen, Dag and Johansen, H\r{a}vard and Aarflot, Tjalve and Hurley, Joseph and Kvalnes, \r{A}ge and Gurrin, Cathal and Zav, Sorin and Olstad, Bj\o{}rn and Aaberg, Erik and Endestad, Tore and Riiser, Haakon and Griwidz, Carsten and Halvorsen, P\r{a}l},
title = {DAVVI: A Prototype for the next Generation Multimedia Entertainment Platform},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631482},
doi = {10.1145/1631272.1631482},
abstract = {In this demo, we present DAVVI, a prototype of the next generation multimedia entertainment platform. It delivers multi-quality video content in a torrent-similar way like known systems from Move Networks, Microsoft and Apple do. However, it also provides a brand new, personalized user experience. Through applied search, personalization and recommendation technologies, end-users can efficiently search and retrieve highlights and combine arbitrary events in a customized manner using drag and drop. The created playlists of video segments are then delivered back to the system to improve future search and recommendation results. Here, we demonstrate this system using a soccer example.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {989–990},
numpages = {2},
keywords = {video search and recommendation, personalization, torrent-like dissemination, annotation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251365,
author = {Au, Oscar},
title = {Session Details: Technical Demonstrations Session 2},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251365},
doi = {10.1145/3251365},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631484,
author = {Yoo, Jong-Woon and Choi, Woong and Park, Ki-Woong and Park, Kyu Ho},
title = {An Intuitive Data Transfer Technique Using Bartender's Gestures},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631484},
doi = {10.1145/1631272.1631484},
abstract = {This technical demonstration presents Cocktail, a gesture-based mobile interaction system, which is designed for providing a user-friendly way of exchanging multimedia data with intuitive gestures. Our system is motivated by bartenders who make cocktails with interesting gestures, such as pouring and shaking. These gestures are used in our interaction system for data transfer and contents creation: a user can pour (transfer) data in his/her mobile phone to other devices in the same way that a bartender pours drinks to a shaker. The user can also mix music files and pictures into a multimedia content, such as a music video file, by shaking the mobile phone, as a bartender does to mix different drinks into a cocktail. We have implemented a prototype of Cocktail using smart phones and demonstrate its usability.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {991–992},
numpages = {2},
keywords = {data transfer, mobile interaction, gesture, shaking},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631485,
author = {Haenselmann, Thomas and Lemelson, Hendrik and Adam, Kerstin and Effelsberg, Wolfgang},
title = {A Tangible MIDI Sequencer for Visually Impaired People},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631485},
doi = {10.1145/1631272.1631485},
abstract = {In contrast to many applications on the PC and in the Internet which are accessible to visually impaired people today, making electronic music with limited eye-sight is still a challenge. Standard software and hardware are both strongly dominated by graphical output. In order to close this cap, we developed a MIDI (Musical Instrument Digital Interface) sequencer with audio-feedback and a new interaction paradigm which eliminates interaction with the PC's keyboard and screen. The blind musician relies solely on input via the instrument itself. He can both, record and play music via the instrument's black &amp; white keys but at the same time control all functions of a multi-track MIDI sequencer without ever taking the hands off the instrument. We also use the MIDI-connection for coding different kinds of feedback to the user in an efficient way. The software which runs on a PC and is connected to an electronic instrument has been evaluated and improved extensively and is made available free of charge for visually impaired and normal-sighted people.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {993–994},
numpages = {2},
keywords = {tactile interface, visually impaired, MIDI, electronic music production},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631486,
author = {Chiang, Ming-Che and Chang, Chia-Hu and Wu, Ja-Ling},
title = {Evolution-Based Virtual Content Insertion},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631486},
doi = {10.1145/1631272.1631486},
abstract = {This demonstration presents an innovative framework for virtual content insertion in an interactive way. The virtual content is inserted into video with evolved animations according to predefined behaviors, and generates interactions with video contents. In order to reduce the intrusiveness and improve the impression for viewers, the evolution process is divided into distinct yet dependent phases, in which the virtual content evolves its appearances and behaviors according to the incremental interactions. Therefore, the augmented videos generated by the proposed system establish visually relevant connection between the inserted virtual content and the source videos, and increase the acceptability and the attractiveness at the same time. Moreover, the proposed system enables video owners to create entertaining personalized videos effectively, and engages viewers with the storyline.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {995–996},
numpages = {2},
keywords = {interaction, animation, virtual content insertion, simulated evolution},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631487,
author = {Ahn, Miru and Choe, Sungwon Peter and Kwon, Sungjun and Park, Byunglim and Park, Taiwoo and Cho, Sooho and Park, Jaesang and Rhee, Yunseok and Song, Junehwa},
title = {Swan Boat: Pervasive Social Game to Enhance Treadmill Running},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631487},
doi = {10.1145/1631272.1631487},
abstract = {We designed and implemented a pervasive game called Swan Boat that targets the bland and tedious nature of running on a treadmill, making it fun through social interaction and immersive game play. We developed Swan Boat on top of PSD, a platform for pervasive games, and using the Interactive Treadmill hardware. We conducted a user study to evaluate our game.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {997–998},
numpages = {2},
keywords = {interactive treadmill, fitness games, swan boat, PSD, sensor-bracelet, pervasive games},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631488,
author = {\'{A}smundsson, Fri\dh{}rik H. and Lejsek, Herwig and Da\dh{}ason, Kristleifur and J\'{o}nsson, Bj\"{o}rn \TH{} and Amsaleg, Laurent},
title = {Videntifier™ Forensic: Robust and Efficient Detection of Illegal Multimedia},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631488},
doi = {10.1145/1631272.1631488},
abstract = {A large portion of the video material available on the Internet is distributed illegally. In this demonstration we present Videntifier Forensic, a new law enforcement solution for automatically identifying videos and images. Videntifier Forensic is very robust and efficient, even at a very large scale. We encourage ACM Multimedia participants to bring original videos and modified (yet visually acceptable) copies to challenge the capabilities of the system.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {999–1000},
numpages = {2},
keywords = {scalability, video detection, robustness},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631489,
author = {Cheng, Ran and Huang, Zi and Shen, Heng Tao and Zhou, Xiaofang},
title = {Interactive Near-Duplicate Video Retrieval and Detection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631489},
doi = {10.1145/1631272.1631489},
abstract = {Video near-duplicate retrieval has become a compelling researching topic in recent years, due to the proliferation of online video uploading/sharing sites and the exponential explosion of video data. Previously, we introduced one new "realtime near-duplicate video retrieval/detection system" -- UQLIPS. UQLIPS is a Web-based integrated platform which performs fast retrieval of near-duplicate clips from segmented video collections, as well as online detection of near-duplicate occurrences over continuous video streams. On top of the showcase we have demonstrated, in this paper we introduce recently designed new user interaction features in our system. These new user interaction features provide users the options to make their own near-duplicate videos by edition operations, tune system parameters, visualize video features, and generate diverse views.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1001–1002},
numpages = {2},
keywords = {near-duplicate video, interactive retrieval and detection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631490,
author = {Tang, Sheng and Li, Jintao and Zhang, Yongdong and Xie, Cheng and Li, Ming and Liu, Yizhi and Hua, Xiufeng and Zheng, Yan-Tao and Tang, Jinhui and Chua, Tat-Seng},
title = {Pornprobe: An LDA-SVM Based Pornography Detection System},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631490},
doi = {10.1145/1631272.1631490},
abstract = {We present PornProbe, a pornography detection system that detects pornographic contents in videos. To build such a detection system, we leverage a large scale training data set with 65,827 positive training image samples out of a total of 420,615 training samples, and a novel detection scheme based on hierarchical LDA-SVM. The system combines the unsupervised clustering in Latent Dirichlet Allocation (LDA) and supervised learning in Support Vector Machine, so as to achieve both high precision and recall while ensuring efficiency in both training and testing. This demonstration shows how the system detects the pornographic scenes in restricted artistic (RA) movies.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1003–1004},
numpages = {2},
keywords = {pornography detection, SVM, latent dirichlet allocation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631491,
author = {Zhou, Yinsheng and Li, Zhonghua and Tan, Dillion and Percival, Graham and Wang, Ye},
title = {MOGFUN: Musical MObile Group for FUN},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631491},
doi = {10.1145/1631272.1631491},
abstract = {The computational power and sensory capabilities of mobile devices are increasing dramatically these days, rendering them suitable for real-time sound synthesis and various musical expressions. In this paper, we demonstrate a novel mobile music making system which leverages the ubiquity, ultra-mobility, and multi-modality of mobile devices (iPod touch) for people to create and compose music collaboratively. Unlike the conventional music making applications which generate the music on a single mobile device with a preset sound and interface, our system allows several players in a group to be connected together through wireless LAN network, creating music with different sounds and interfaces. Finally, the performance can be recorded as a single music file and played back in the future. The paper also shows some application scenarios for this collaborative music making system in future research.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1005–1006},
numpages = {2},
keywords = {new music instruments, community music making, mobile devices},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631492,
author = {Iqbal, Razib and Shahabuddin, Sharmeen and Shirmohammadi, Shervin},
title = {A Compressed-Domain Spatio-Temporal Adaptation System for Video Delivery},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631492},
doi = {10.1145/1631272.1631492},
abstract = {In this demo, we present a working system that we have built for metadata-based compressed-domain spatio-temporal video adaptation of H.264/AVC videos. Our approach is in contrast to most existing approaches that do adaptation in a series of cascaded decode/re-encode procedures. We show that by applying compressed-domain operations, adaptation can be expedited for real-time application scenarios like news/sports broadcasting. Moreover, the system can be applied as a tool box for distributed adaptation within P2P video distribution applications to support heterogeneous devices.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1007–1008},
numpages = {2},
keywords = {MPEG-21, temporal adaptation, spatial adaptation, H.264/avc, live video},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631493,
author = {El-Saban, Motaz Ahmad and Refaat, Mahmoud and Kaheel, Ayman and Abdul-Hamid, Ahmed},
title = {Stitching Videos Streamed by Mobile Phones in Real-Time},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631493},
doi = {10.1145/1631272.1631493},
abstract = {User generated videos with mobile phone cameras are becoming more and more ubiquitous allowing people to share live content with remote parties, possibly in real-time. However with limited mobile phone capabilities, these videos are usually of small resolution, resulting in a small field of view for acceptable quality. Fortunately with the proliferation of video capture-enabled mobile phones, there is high chance that one or more persons will be shooting the same scene from different views. In this demonstration, we are showing an end-to-end system which receives video streams coming from different mobile phones, time synchronizes the streams and produces a single composite mosaic video, and all of this is done in real-time. The proposed system operates without coordination between users. The system has been tested under various capturing conditions such as indoor, outdoor, day and night conditions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1009–1010},
numpages = {2},
keywords = {video stitching, real-time, mobile video capturing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631494,
author = {Lee, Jeannie S.A. and Jayant, Nikil},
title = {Gestures for Mixed-Initiative News Video Browsing on Mobile Devices},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631494},
doi = {10.1145/1631272.1631494},
abstract = {Although small screen sizes and limited input methods are challenges for mobile devices, there are affordances like gestures and motion detection through on-device sensors. A prototype system has been implemented on a HTC Dream (T-Mobile G1) mobile phone demonstrating an "adaptive TV-channel" like mixed-initiative interface used to solicit user relevance feedback, provide recommendations and facilitate news video watching. The user is provided on-screen finger gesture operations to vote-up, vote-down or skip a video. Shaking the device resets the video sequence. This creates a cognitively palatable stream of videos and a seamless lower-latency user experience easily operated with one hand.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1011–1012},
numpages = {2},
keywords = {multimedia, gesture, user interface, user interaction, mobile, information retrieval, mixed-initiative},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631495,
author = {Yang, Xin and Zhu, Qiang and Cheng, Kwang-Ting},
title = {MyFinder: Near-Duplicate Detection for Large Image Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631495},
doi = {10.1145/1631272.1631495},
abstract = {The explosive growth of multimedia data poses serious challenges to data storage, management and search. Efficient near-duplicate detection is one of the required technologies for various applications. In this paper, we introduce MyFinder, an image near-duplicate detection system for large image collections. MyFinder consists of three major components: 1) a local-feature-based image representation utilizing the proposed LDP (Local-Difference-Pattern) feature, 2) the Locality-Sensitive-Hashing (LSH) as the core indexing structure to assure the most frequent data access occurred in the main memory, and 3) multi-step verification for queries to best exclude false positives and to increase the precision.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1013–1014},
numpages = {2},
keywords = {image feature, near-duplicate detection, database retrieval},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631496,
author = {Yin, Hao and Hui, Wen and Miao, Quan and Li, Zheng and Lin, Chuang},
title = {<i>IVForensic</i>: A Digital Forensics Service Platform for Internet Videos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631496},
doi = {10.1145/1631272.1631496},
abstract = {IVForensic is a digital forensics service platform for Internet videos, with the aim of revealing illegal videos and preventing them from spreading over the Internet. It a) implements a flexible, secure and scalable architecture for large-scale online monitoring, b) provides an effective and efficient forensic countermeasure for widely-sourced Internet videos, c) guarantees good end-user experience such as flexible settings, simple operations and low startup latency. The results of performance evaluation using data obtained from real-world deployments demonstrate the effectiveness of the platform.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1015–1016},
numpages = {2},
keywords = {video fingerprint, digital forensics, digital video},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631497,
author = {Sano, Masanori and Sumiyoshi, Hideki and Shibata, Masahiro and Yagi, Nobuyuki},
title = {Metadata Production Framework (MPF) Version 2.0: Designed for Effective Generation of Content-Based Metadata},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631497},
doi = {10.1145/1631272.1631497},
abstract = {In this paper, we describe the latest specifications of the Metadata Production Framework (MPF) and its reference software with which the basic functionality of MPF can be tested over networks. MPF was originally designed as an efficient environment for generating metadata for TV programs from the viewpoint of the broadcaster, but recent developments indicate that the domains where MPF can be applied have expanded to include even ordinary households. This trend suggests that the metadata market might be much bigger in the near future, and we believe MPF can become an infrastructure standard in these circumstances.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1017–1018},
numpages = {2},
keywords = {metadata generation, MPEG-7, metadata editor, framework},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631498,
author = {Qui, Tran Cong Thien and Lee, Shang Ping and Loy, Shing Chuan and Pensyl, William Russell},
title = {Tiger Training in Augmented Reality},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631498},
doi = {10.1145/1631272.1631498},
abstract = {"Tiger Training" explores the relationship between real and virtual worlds via interaction with virtual animals in immersive mixed reality environments. This interactive, participative installation encourages viewers to "train" virtual animals using hand signals and voice commands. Virtual tigers are not easily "trained" requiring learning appropriate ways to coax responsive actions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1019–1020},
numpages = {2},
keywords = {interaction, animal training, augmented reality},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631499,
author = {Hao, Qiang and Cai, Rui and Yang, Jiang-Ming and Xiao, Rong and Liu, Like and Wang, Shuo and Zhang, Lei},
title = {TravelScope: Standing on the Shoulders of Dedicated Travelers},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631499},
doi = {10.1145/1631272.1631499},
abstract = {In this paper, we propose a system called TravelScope that helps users experience virtual tours by presenting information mined from user-generated travelogues and photos. The system can (1) recommend popular places for a given region; (2) characterize comprehensive aspects (e.g., landmarks, styles, activities) for a location; and (3) show representative images for a landmark. A novel user interface is designed to provide a better user experience, by organizing both the textual and visual information generated by dedicated travelers in an attractive way.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1021–1022},
numpages = {2},
keywords = {virtual tour, location extraction, travelogue mining},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631500,
author = {Choudary, Omar and Charvillat, Vincent and Grigoras, Romulus and Gurdjos, Pierre},
title = {MARCH: Mobile Augmented Reality for Cultural Heritage},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631500},
doi = {10.1145/1631272.1631500},
abstract = {We present MARCH, a complete solution for enhanced cultural heritage discovery using the mobile phone. Simply point the camera of a mobile device at prehistoric cave engravings. Then MARCH augments the captured images with the expert's drawings, highlighting in real time the animal engravings, which are almost impossible to observe with the naked eye. We have created a mobile augmented reality application which runs at 14 FPS for 320x240 frames on a Nokia N95 smartphone. We describe the optimizations and the requirements needed to obtain these results on mobile devices.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1023–1024},
numpages = {2},
keywords = {mobile application, cultural heritage, augmented reality},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631501,
author = {Wang, Patricia P. and Wang, Tao and Ding, Dayong and Zhang, Yimin and Bi, Wenyuan and Bao, Yingze},
title = {Mirror World Navigation for Mobile Users Based on Augmented Reality},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631501},
doi = {10.1145/1631272.1631501},
abstract = {Finding a destination in unfamiliar environments of complex office building or shopping mall has been always bothering us in daily life. Fine-scale directional guidance, a combination of location based service and context aware service, is emerging along with the technology developments of mobile computing, wireless communication, and augmented reality. This paper addresses the core problem of how to align what we see with where we are. In order to achieve robust and accurate navigation by 2D/3D alignment, we developed a hybrid solution by fusing GPS, inertial measurement units and computer vision methods together. We have implemented a prototype of Mobile Augmented Reality and 3D navigation on the manually created virtual environment of Intel China Research Center office and surroundings.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1025–1026},
numpages = {2},
keywords = {mirror world navigation, parallel tracking and mapping, mobile augmented reality, camera pose estimation, INS, GPS},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631502,
author = {Zhang, Wei and Wang, Qiang and Tang, Xiaoou},
title = {Performance Driven Face Animation via Non-Rigid 3d Tracking},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631502},
doi = {10.1145/1631272.1631502},
abstract = {In this demo, a performance driven 3D face animation system is proposed. The proposed system consists of two key components: a robust non-rigid 3D tracking module and a MPEG4 compliant facial animation module. Firstly, the facial motion is tracked from source videos which contain both the rigid 3D head motion (6 DOF) and the non-rigid expression variations. Afterward, the tracked facial motion is parameterized via estimating a set of MPEG4 facial animation parameters(FAP). As the final step, these FAP values are transferred to the MPEG4-compliant face model for the animation purpose. The proposed tracking and animation system has a strong generalization ability and can be used in the indoor environment with no additional assumptions.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1027–1028},
numpages = {2},
keywords = {3D tracking, performance driven animation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251366,
author = {Yang, Jie},
title = {Session Details: Video Program},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251366},
doi = {10.1145/3251366},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631504,
author = {Kelliher, Aisling and Birchfield, David and Campana, Ellen and Hatton, Sarah and Johnson-Glenberg, Mina and Martinez, Christopher and Olson, Loren and Savvides, Philippos and Tolentino, Lisa and Phillips, Kelly and Uysal, Sibel},
title = {SMALLab: A Mixed-Reality Environment for Embodied and Mediated Learning},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631504},
doi = {10.1145/1631272.1631504},
abstract = {In this video presentation, we introduce the Situated Multimedia Arts Learning Lab [SMALLab], a mixed-reality learning environment that supports interactive engagement through full body 3D movements and gestures within a collaborative, computationally mediated space. The video begins by describing the holistic approach to embodied and mediated learning developed by our transdisciplinary research team, grounded in understandings derived from research in the learning sciences, digital media and human computer interaction. We then outline the three core tenets of effective learning exemplified by our research -- embodiment, multimodality and collaboration. The video next demonstrates the design and functionality of the physical and digital components of SMALLab. We conclude by illustrating our partner collaborations with K12 teachers and students with four scenarios depicting Geography, Physics, Language Arts and Chemistry learning modules.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1029–1032},
numpages = {4},
keywords = {student centered learning environments, experiential media, interactivity, learning, situated multimedia, k-12 education},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631505,
author = {Misra, Ananya and Wang, Ge and Cook, Perry R.},
title = {TAPESTREA: A New Way to Design Sound},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631505},
doi = {10.1145/1631272.1631505},
abstract = {TAPESTREA is a sound design and composition framework that facilitates the creation of new sound from existing digital audio recordings, through interactive analysis, transformation and re-synthesis. During analysis, sound templates of diffierent types are extracted using a variety of techniques. Each extracted template is transformed and synthesized independently, allowing specialized transformations on each template based on its type. The user interacts with TAPESTREA via a set of graphical interfaces that offer parametric control over every stage of analysis, transformation and re-synthesis. Synthesis is further controlled through ChucK scripts. These combined techniques form a workbench for completely transforming a sound scene, dynamically generating soundscapes, or creating musical tapestries by weaving together transformed elements from different recordings. Thus, TAPESTREA introduces a new paradigm for sound design, composition and sonic sculpting tasks.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1033–1036},
numpages = {4},
keywords = {multimedia, signal processing, composition, real-time, open source software, audio, sound design},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631506,
author = {Tang, Nick C. and Zhong, Hsing-Ying and Tsai, Joseph C. and Shih, Timothy K. and Liao, Mark},
title = {Motion Inpainting and Extrapolation for Special Effect Production},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631506},
doi = {10.1145/1631272.1631506},
abstract = {Special effect production is an important technology for movie industry. Although sophisticated equipments can be used to produce scenery such as fire and smoke, usually, the approach is expensive and perhaps dangerous. Can sceneries be re-produced based on existing videos using digital technology? The answer is yes but it is difficult. In this video demonstration, we propose new mechanisms using motion inpainting technologies. Dynamic texture can be altered and reused under control. In addition, actors with repeated motion can be interpolated or extrapolated and incorporated into the falsified scenery. We demonstrate using our tool to produce realistic video narratives based on existing videos. Five sets of results as well as the source videos used are available at http://member.mine.tku.edu.tw/www/ACMMM09VideoDemo.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1037–1040},
numpages = {4},
keywords = {motion inpainting, layer segmentation, tracking, motion interpolation, special effect},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631507,
author = {Rieffel, Eleanor G. and Gattepally, Sagar and Kimber, Don and Shingu, Jun and Vaughan, Jim and Doherty, John},
title = {Marking up a World: Physical Markup for Virtual Contentcreation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631507},
doi = {10.1145/1631272.1631507},
abstract = {The Pantheia system enables users to create virtual models by `marking up' the real world with pre-printed markers. The markers have predefined meanings that guide the system as it creates models. Pantheia takes as input user captured images or video of the marked up space. This video illustrates the workings of the system and shows it being used to create three models, one of a cabinet, one of a lab, and one of a conference room. As part of the Pantheia system, we also developed a 3D viewer that spatially integrates a model with images of the model.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1041–1044},
numpages = {4},
keywords = {image-based modeling, virtual environments},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631508,
author = {Spicer, Ryan P. and Lin, Yu-Ru and Kelliher, Aisling},
title = {NextSlidePlease: Agile Hyperpresentations},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631508},
doi = {10.1145/1631272.1631508},
abstract = {In this video presentation, we introduce NextSlidePlease, a novel slide authoring and presentation application. The video begins with a dramatization illustrating the shortcomings of existing slide-ware tools identified through our prior research. We then describe our theoretical framework for addressing these identified problems and present a dramatization of the process by which our NextSlidePlease application can be used to overcome such issues in a business context. In addition, we illustrate the novel functional aspects of our application algorithm that enable effective time management and flexible presentations. Finally, we present promising results from two user studies.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1045–1048},
numpages = {4},
keywords = {navigation, slide-ware, hyperpresentations, presentations, hypermedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631509,
author = {Tang, Lin-Xie and Mei, Tao and Hua, Xian-Sheng},
title = {NLVS: A near-Lossless Video Summarization System},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631509},
doi = {10.1145/1631272.1631509},
abstract = {This video demonstration presents a new system for video summarization, called ``Near-Lossless Video Summarization" (NLVS) to tackle with the challenges of storage and indexing brought by the current boom of videos for existing online video services. Unlike current techniques such as video compression and summarization which are still struggling to achieve the two often conflicting goals of low storage and high visual and semantic fidelity, NLVS is able to summarize a video stream with least information loss by using an extremely small piece of metadata. Although at a very low compression ratio (1/30 of H.264 baseline in average, where traditional compression techniques like H.264 fail to preserve the fidelity), the summary still can be used to reconstruct the original video (with the same duration) nearly without semantic information loss.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1049–1052},
numpages = {4},
keywords = {video storage, video summarization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251367,
author = {Nack, Frank},
title = {Session Details: Interactive Art Program Track 1: Virtual},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251367},
doi = {10.1145/3251367},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631511,
author = {Lombardo, Vincenzo and Valle, Andrea and Nunnari, Fabrizio},
title = {Tabula Ex-Cambio},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631511},
doi = {10.1145/1631272.1631511},
abstract = {Tabula-ex-cambio is an interactive installation that delivers visual and audio content. It works as a sort of perpetual billiard game, powered by the data originating from the trend of a stock exchange index. The visual content is a real-time animated 3D computer graphics, that represents a billiard game where each ball is associated with a stock in the index. The audio content is an electroacoustic music composition featuring as many layers of sounds as stocks in the index. All the balls in the billiard are musical sources that play an iterated sequence of sounds while moving on the billiard table; each sequence is altered in frequency by the trend of the related company index. The final delivered visual and audio contents depend on the interaction with the user, who can select a view/listening point on the billiard game. The physical installation consists of a deformed billiard structure connected to a stele through cables; the stele is a vertical display where the billiard game takes place. The visual display relies on the graphic engine Ogre, while the aural display is implemented in SuperCollider. The installation was exposed in Shanghai, Beijing, Birmingham, and Terni (Italy).},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1053–1062},
numpages = {10},
keywords = {art installation, stock index presentation, real-time computer graphics, auralization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631512,
author = {Tuomola, Mika Lumi and Korpilahti, Teemu and Pesonen, Jaakko and Singh, Abhigyan and Villa, Robert and Punitha, P. and Feng, Yue and Jose, Joemon M.},
title = {Concept, Content and the Convict},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631512},
doi = {10.1145/1631272.1631512},
abstract = {This paper describes the concepts behind and implementation of the multimedia art work Alan01 / AlanOnline, which wakes up the 1952 criminally convicted Alan Turing as a piece of code within the art work - thus fulfilling Turing's own vision of preserving human consciousness in a computer. The work's context is described within the development of associative storytelling structures built up by interactive user feedback via an image and video retrieval system. The input to the retrieval system is generated by Alan01 / AlanOnline via their respective sketch interfaces, the output of the retrieval system being fed back to Alan01 / AlanOnline for further processing and presentation to the user within the context of the overall artistic experience. This paper, in addition to presenting the productions and image retrieval system, also presents the installation and online production user reception and some of the issues and observations made during the development of the systems.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1063–1072},
numpages = {10},
keywords = {Alan Turing, image retrieval, online production, art installation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251368,
author = {Nack, Frank},
title = {Session Details: Interactive Art Program Track 2: Natural},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251368},
doi = {10.1145/3251368},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631514,
author = {Niemeyer, Greg and Garcia, Antero and Naima, Reza},
title = {Black Cloud: Patterns towards Da Future},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631514},
doi = {10.1145/1631272.1631514},
abstract = {The authors developed and tested a hyper-local air quality sensor network and a fictional game narrative to evaluate the pedagogical potential of Alternate Reality games for high school students in Los Angeles. This study examined how Deweyan concepts of learning can be applied to game play. The authors found that students developed a unique language to discuss real pollution issues within a fictional construct. Engaging in both civic engagement and educational rigor, student learning was situated in a framework of instruction John Dewey outlines as counter to traditional models of schooling. Despite limitations, including some authoritarian and competitive structures implicit in games, students found new reasons to communicate with real-world adults in verbal and written form. Game-based learning inspired substantial qualitative progress and high levels of engagement among students, compared to traditional teaching methods.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1073–1082},
numpages = {10},
keywords = {climate sensing, charts, John Dewey, public art, high school, situated learning, SMS text messaging, VOC, interactive art, alternate reality game, C02},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631515,
author = {Redfern, Tim and Haahr, Mads},
title = {Kosmoscope: A Seismic Observatory},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631515},
doi = {10.1145/1631272.1631515},
abstract = {Earth tremors are a constant, intrinsic part of our planet's nature and a visceral experience when first encountered.Kosmoscope, a telematic art installation by Tim Redfern, marries two venerable technologies, the kaleidoscope and the seismograph, in order to immerse viewers within an abstract audiovisual representation of earth tremors. Kosmoscope uses state of the art seismic monitoring to render live scientific data impressionistically, making the worldwide network of ultra-sensitive seismic microphones audible and creating an imposing, visually fractured narrative that evokes the frailty of humanity in the face of geological forces. This paper describes the Kosmoscope's influences, aims, design and realisation.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1083–1086},
numpages = {4},
keywords = {installation, telematic, seismic, art},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631516,
author = {Legrady, George and Villegas, Javier and Burbano, Andres},
title = {The "We Are Stardust" Installation},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631516},
doi = {10.1145/1631272.1631516},
abstract = {"We Are Stardust"" is a digital media interactive installation consisting of two large projected visualizations and a ceiling mounted full rotational FLIR systems infrared imaging camera. Both projections in the installation map the sequence and locations of sky observations by the Spitzer Space Telescope, an infrared temperature sensing instrument managed by the NASA Spitzer Science Center. The intent of the project is to visually map the schedule of scientific observations: What celestial bodies were looked at, when, for how long, and by whom. This document describes the installation and depicts the engineering work behind the design and construction of the experience.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1087–1090},
numpages = {4},
keywords = {data visualization, information aesthetics video processing, interactive installation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251369,
author = {Shamma, David A.},
title = {Session Details: Interactive Art Program Track 3: Visual},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251369},
doi = {10.1145/3251369},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631518,
author = {Sha, Xin Wei and Fortin, Michael and Rousseau, Jean-S\'{e}bastien},
title = {Calligraphic Video: A Phenomenological Approach to Dense Visual Interaction},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631518},
doi = {10.1145/1631272.1631518},
abstract = {No matter what how the image is computationally produced, screen-based graphics are still typically presented on a two-dimensional surface like a screen, wall, or electronic paper. The limit of manipulating objects in a two-dimensional graphical display is where each pixel is an independent object. These two observations motivate the development of calligraphic video, textures that can be manipulated by the user using intuitions about physical material such as water, ink, or smoke. We argue for a phenomenological approach to complex visual interaction based on corporeal kinesthetic intuition, and provide an effective way to provide such texture-based interaction using computational physics. A motivating application is to create palpable, highly textured video that can be projected as structured light fields in responsive environments.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1091–1100},
numpages = {10},
keywords = {realtime video, realtime sound, design, responsive media, computational physics, responsive environment, tangible media, phenomenology, gesture-based interaction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631519,
author = {Cady, Stephen},
title = {Chiasmus},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631519},
doi = {10.1145/1631272.1631519},
abstract = {Chiasmus is a responsive and dynamically reflective, two-sided volumetric projection surface that embodies phenomenological issues such as the formation and reception of images, observer and machine perception and the dynamics of the screen as a space of image reception. It consists of a square grid of 64 individually motorized cube elements engineered to move linearly. Each cube is controlled by custom software that analyzes video imagery for luminance values and sends these values to the motor control mechanisms to coordinate the individual movements. The resolution of the sculptural screen from the individual movements allows its volume to dynamically alter, providing novel and unique perspectives of its mobile form to an observer.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1101–1104},
numpages = {4},
keywords = {responsive architecture, perception, kinetic sculpture, video installation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631520,
author = {Sillam, K\'{e}vin and Luciani, Annie},
title = {Somnambule},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631520},
doi = {10.1145/1631272.1631520},
abstract = {« Somnambule » is a global instrument which produces sound and picture, generated by a gesture - more precisly the pianistic gesture. The sound has its source in two different worlds : the real world for the sound of piano, and the virtual world for the synthetic sounds. The video derives directly from the dynamic of the pianistic gesture, where a character evolves, visible only by the tracks of his movement into a surface. This surface is a "dynamic pinscreen" which has a physical behavior computed in real time on GPU. This work illustrates the difference between the two worlds : the real and the virtual. The two have not the same constraints. Overall, the gesture dynamic is the origin of the sound dynamic and the picture dynamic.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1105–1108},
numpages = {4},
keywords = {visual instrument, visualization, pinscreen, GPU, interactive art, phyxels field},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251370,
author = {Shamma, David Ayman and Chua, Tat-Seng and Naaman, Mor},
title = {Session Details: Multimedia Grand Challenge},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251370},
doi = {10.1145/3251370},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631522,
author = {Wu, Xiao and Zhao, Wan-Lei and Ngo, Chong-Wah},
title = {Towards Google Challenge: Combining Contextual and Social Information for Web Video Categorization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631522},
doi = {10.1145/1631272.1631522},
abstract = {Web video categorization is a fundamental task for web video search. In this paper, we explore the Google challenge from a new perspective by combing contextual and social information under the scenario of social web. The semantic meaning of text (title and tags), video relevance from related videos, and user interest induced from user videos, are integrated to robustly determine the video category. Experiments on YouTube videos demonstrate the effectiveness of the proposed solution. The performance reaches 60% improvement compared to the traditional text based classifiers.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1109–1110},
numpages = {2},
keywords = {classification, categorization, social web, context, web video},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631523,
author = {Borth, Damian and Hees, J\"{o}rn and Koch, Markus and Ulges, Adrian and Schulze, Christian and Breuel, Thomas and Paredes, Roberto},
title = {TubeFiler: An Automatic Web Video Categorizer},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631523},
doi = {10.1145/1631272.1631523},
abstract = {While hierarchies are powerful tools for organizing content in other application areas, current web video platforms offer only limited support for a taxonomy-based browsing. To overcome this limitation, we present a framework called TubeFiler. Its two key features are an automatic multimodal categorization of videos into a genre hierarchy, and a support of additional fine-grained hierarchy levels based on unsupervised learning. We present experimental results on real-world YouTube clips with a 2-level 46-category genre hierarchy, indicating that - though the problem is clearly challenging - good category suggestions can be achieved. For example, if TubeFiler suggests 5 categories, it hits the right one (or at least its supercategory) in 91.8% of cases.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1111–1112},
numpages = {2},
keywords = {multimodal classification, web video categorization, hierarchical genres},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631524,
author = {Song, Yicheng and Zhang, Yong-dong and Zhang, Xu and Cao, Juan and Li, Jing-Tao},
title = {Google Challenge: Incremental-Learning for Web Video Categorization on Robust Semantic Feature Space},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631524},
doi = {10.1145/1631272.1631524},
abstract = {With the advent of video sharing websites, the amount of videos on the internet grows rapidly. Web video categorization is an efficient methodology to organize the huge amount of data. In this paper, we propose an effective web video categorization algorithm for the large scale dataset. It includes two factors: 1) For the great diversity of web videos, we develop an effective semantic feature space called Concept Collection for Web Video Categorization (CCWV-CD) to represent web videos, which consists of concepts with small semantic gap and high distinguishing ability. Meanwhile, the online Wikipedia API is employed to diffuse the concept correlations in this space. 2) We propose an incremental support vector machine with fixed number of support vectors (n-ISVM) to fit the large scale incremental learning problem in web video categorization. Extensive experiments are conducted on the dataset of 80024 most representative videos on YouTube demonstrate that the semantic space with Wikipedia prorogation is more representative for web videos, and n-ISVM outperforms other algorithms in efficiency when performs the incremental learning.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1113–1114},
numpages = {2},
keywords = {n-ISVM, wikipedia propagation, web video categorization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631525,
author = {Friedland, Gerald and Gottlieb, Luke and Janin, Adam},
title = {Joke-o-Mat: Browsing Sitcoms Punchline by Punchline},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631525},
doi = {10.1145/1631272.1631525},
abstract = {This paper summarizes our contribution to the Yahoo! task of the ACM Multimedia Grand Challenge. This challenge asks for the robust automatic segmentation of videos according to "narrative themes". Based on the automatic segmentation methods presented in [1] and partly [2], we describe a system to navigate Seinfeld episodes based on automatic segmentation of the audio track only. The system distinguishes laughter, music, and other noise as well as speech segments. Speech segments are identified against pre-trained speaker models of the actors. Given this segmentation and the artistic production rules that underlie the genre situation comedy and Seinfeld in particular, the system enables a user to browse an episode by scene, by punchline, and by dialog segments. The themes can be filtered by the main actors, e.g. the user can select to see only punchlines by Jerry and Kramer. Based on the length of the laughter, the top 5 punchlines are also identified and presented to the user.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1115–1116},
numpages = {2},
keywords = {video navigation, speaker ID, acoustic event detection},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631526,
author = {Kofler, Christoph and Lux, Mathias},
title = {Dynamic Presentation Adaptation Based on User Intent Classification},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631526},
doi = {10.1145/1631272.1631526},
abstract = {Results of internet searches are typically presented as lists. When searching for digital photos different search result presentations however offer different benefits. If users are primarily interested in the visual content of images a thumbnail grid may be more appropriate than a list. For people searching photos taken at a specific place image metadata in the result presentation is of interest too. In this paper we present an application which monitors a user's behavior while searching for digital photos and classifies the user's intention. Based on the intention, the result is adapted to support the user in an optimal way.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1117–1118},
numpages = {2},
keywords = {retrieval, multimedia, user intention},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631527,
author = {Law-To, Julien and Grefenstette, Gregory and Gauvain, Jean-Luc},
title = {VoxaleadNews: Robust Automatic Segmentation of Video into Browsable Content},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631527},
doi = {10.1145/1631272.1631527},
abstract = {We present an interface to video and audio podcasts that extracts semantics from the speech content, and packages the extracted information in a variety of navigation tools. The user can jump to the relevant sections and browse from relevant section to relevant section. This interface is related to the Yahoo! Challenge: Robust Automatic Segmentation of Video According to Narrative Themes.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1119–1120},
numpages = {2},
keywords = {video search, video browsing, video indexing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631528,
author = {Hsieh, Liang-Chi and Chen, Kuan-Ting and Chiang, Chien-Hsing and Yang, Yi-Hsuan and Wu, Guan-Long and Ferng, Chun-Sung and Hsueh, Hsiu-Wen and Tsai, Angela Charng-Rurng and Hsu, Winston H.},
title = {Canonical Image Selection and Efficient Image Graph Construction for Large-Scale Flickr Photos},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631528},
doi = {10.1145/1631272.1631528},
abstract = {Efficient image search clustering is prominent for image search engines for exponentially growing photo collections. In this work, we propose an image search clustering approach which selects multiple canonical images from image search results and constructs image clusters in real time on an image sub-graph for the search results. The efficiency is achieved with the help of offline-computed image context graphs by distributed computing methods. Extending our prior works, we demonstrate the results of the proposed canonical image selection and preliminary outcomes of large-scale image graph construction in this proposal. We experiment in Flickr550 dataset, containing 540,321 Flickr photos.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1121–1122},
numpages = {2},
keywords = {graph construction, image search clustering, canonical images},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631529,
author = {Gong, Wei and Luo, Hangzai and Fan, Jianping},
title = {Extracting Informative Images from Web News Pages via Imbalanced Classification},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631529},
doi = {10.1145/1631272.1631529},
abstract = {In this paper we propose an imbalanced classification algorithm to extract informative images from web news pages. Our algorithm resolve the difficult problem based on two approaches. First, we limit our dataset to a specific application area so that the patterns of the informative images can be captured by existing classification algorithms. Second, we propose an automatic negative samples filtering algorithm to eliminate most negative samples, so that the classification training data is rebalanced. Because most classification algorithms have reduced performance on imbalanced training data, our algorithm improves the overall performance significantly. In addition, our approach is inherently robust to new web sites and style/layout change of web sites.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1123–1124},
numpages = {2},
keywords = {informative image, imbalanced classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631530,
author = {Seeoun, Tewson and Haruechaiyasak, Choochart and Kondo, Toshiaki},
title = {Identifying Auxiliary Web Images Using Combination of Analyses},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631530},
doi = {10.1145/1631272.1631530},
abstract = {As the Web gains more popularity, Web sites become richer in media. Besides text, another most common form of media is image. A Web page can utilize images in various ways such as to illustrate stories, to summarize data and to decorate the page. This leads to a large amount of images embedded in Web pages. However, not all Web images are informative, i.e., engaged with the page for the purpose of delivering useful information. The uninformative or auxiliary images are, for example, logos and banner advertisements. The benefit of classifying Web images as ``informative" or "auxiliary" is the efficient use of available resources. The auxiliary images are insignificant and can be ignored in many tasks including search engine's indexing, for the sake of conciseness of search results, and Web page printing, to reduce ink usage. This paper proposes a solution for the HP Multimedia Grand Challenge to identify informative multimedia contents in Web pages. Our approach is based on a supervised machine learning model trained from a set of 32 features gathered from content analysis of images, Web page layout, and domain name. We adopt the Support Vector Machines (SVM) algorithm to train the classifier. The model is optimized by a grid search technique to select the appropriate set of kernel parameters. The evaluation results based on the 10-fold cross-validation yielded the classification accuracy of 94.08%. The classification results are used to annotate each image accordingly, as in the prototype implementtaion, each image is highlighted with different border color.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1125–1126},
numpages = {2},
keywords = {support vector machines, web image classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631531,
author = {Ren, Kan and Sarvas, Risto and Calic, Janko},
title = {FreeEye: Intuitive Summarisation of Photo Collections},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631531},
doi = {10.1145/1631272.1631531},
abstract = {This paper presents user evaluation of the FreeEye tool for intuitive browsing and summarization of large-scale photo collections. The tool was tested in three different personal photo selection scenarios: a short-time event, a vacation and a yearbook. The experiments were conducted with five participants, evaluating their satisfaction with the summarization result and the overall process. The results demonstrate good usability of the FreeEye tool and improvement when compared to the standard methods of the participants for selection from large personal photo collections.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1127–1128},
numpages = {2},
keywords = {HCI, multimedia systems, image browsing, image clustering},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631532,
author = {Chu, Wei-Ta and Lin, Chia-Hung},
title = {Automatic Summarization of Travel Photos Using Near-Duplication Detection and Feature Filtering},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631532},
doi = {10.1145/1631272.1631532},
abstract = {We try to address part of the challenge proposed by CeWe. A photo summarization method is developed to select representative photos. Based on the observation that the most important objects/views are often captured several times, we exploit near-duplicate detection techniques to represent a sequence of photo as a graph, and then the graph structure is analyzed to facilitate importance ranking. We focus on summarizing hundreds of photos taken in journeys lasting for one or two weeks. The qualitative and quantitative measurement results demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1129–1130},
numpages = {2},
keywords = {representative selection, photo summarization, feature classification},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631533,
author = {Sinha, Pinaki and Pirsiavash, Hamed and Jain, Ramesh},
title = {Personal Photo Album Summarization},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631533},
doi = {10.1145/1631272.1631533},
abstract = {Photo album summarization is the process of selecting a subset of photos from a larger collection which best preserves the information in the entire set and is semantically coherent. In this paper we propose a system which uses heterogeneous information sources associated with digital photos and generates a summary. Our algorithm adapts itself based on the type of event it is summarizing (Yearbook, Week or Single Day Event) We model the summarization problem as a retrieval problem based on different types of queries. We propose some evaluation metrics for the summary. We use an intuitive web based interface to present the results so that users can further explore the summary in an interactive way. This system is our submission to the CeWe Challenge for the Next Generation of Tangible Multimedia Products.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1131–1132},
numpages = {2},
keywords = {CeWe, summary, multimedia grand challenge, photo album summarization},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631534,
author = {Tien, Ming-Chun and Lin, Yin-Tzu and Wu, Ja-Ling},
title = {Sports Wizard: Sports Video Browsing Based on Semantic Concepts and Game Structure},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631534},
doi = {10.1145/1631272.1631534},
abstract = {A convenient video browsing system for popular sports such as baseball, tennis and billiards is proposed in this work. A sports video is automatically segmented into video clips and annotated with semantic concepts in terms of event type. For sports with specific game structure, e.g. baseball and tennis, we further analyze the entire video of the match with the aid of caption information, webcast information, and the domain knowledge of the corresponding sport. Each segmented video clip with semantic annotation is then mapped to a certain part of the game structure. Finally, the proposed system, name as Sports Wizard, provides a favorable way for the user to browse sports videos based on semantic concepts or game structure.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1133–1134},
numpages = {2},
keywords = {structure analysis, sports video, semantic concept},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631535,
author = {Cha, Jongeun and Eid, Mohamad and Barghout, Ahmad and Rahman, ASM Mahfujur and El Saddik, Abdulmotaleb},
title = {HugMe: Synchronous Haptic Teleconferencing},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631535},
doi = {10.1145/1631272.1631535},
abstract = {Traditional teleconferencing systems have enabled remote communications via audiovisual modalities. However, in real life, human touch such as encouraging pat plays a fundamental role to physical and emotional communication between persons. This paper presents a synchronous haptic teleconferencing system with touch interaction to convey affection and intimacy. We present a preliminary prototype called HugMe. In this system, two remote users could see as well as touch each other.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1135–1136},
numpages = {2},
keywords = {haptics, interpersonal communication, teleconferencing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/3251371,
author = {Lienhart, Rainer and Huet, Benoit},
title = {Session Details: Doctoral Symposium},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251371},
doi = {10.1145/3251371},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631537,
author = {Sarin, Supheakmungkol and Kameyama, Wataru},
title = {Classification and Quality Assessment of High Quality Digital Photographs},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631537},
doi = {10.1145/1631272.1631537},
abstract = {We are interested in high quality photographs. This paper outlines our research proposal for the tasks of classification and quality assessment. We address these challenges by exploring the aesthetics from the combined perspectives of the artists and the photographers. We propose to use the aesthetic primitives of images for visualization as a guideline for high and low-level image feature extraction and to classify this high quality content into six creative exposure themes, which are commonly followed by the professional photographers. Then, we suggest to evaluate the quality of the photograph accordingly to these themes. We solve the problems using statistical modeling and learning approach.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1137–1138},
numpages = {2},
keywords = {aesthetics, quality assessment, creative exposure theme, classification, photograph, feature extraction},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631538,
author = {Imran, Ali Shariq},
title = {Interactive Media Learning Object in Distance and Blended Education},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631538},
doi = {10.1145/1631272.1631538},
abstract = {Lecture videos contain most of the instructional content. These videos contents are in general non-scripted and unedited and thus do not provide the required level of interactivity from such material. Therefore, these videos fail to captivate students' attention for long and thus their effective use remains a challenge. In this regard, Media Learning Object (MLO) can play an important role in future flexible education. MLOs are media rich documents that have pedagogical values encapsulated with all needed features such as navigation structures and surrogates. The aim of this research work is to develop a framework for the creation, storage, distribution and evaluation of MLOs. We will use the developed framework in pilot courses and evaluate the learning experience and outcome of teaching experience based on MLOs. The ultimate goal is to develop this framework as a platform with a set of tools for processing and analyzing the media collected from the different courses and their annotation with automatically extracted meta-data. Additionally, media browsing and interaction structures will be developed and embedded in the created MLOs to facilitate the interaction and use of such objects in everyday studies.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1139–1140},
numpages = {2},
keywords = {meta-data extraction, multimedia, learning objects, distance learning, video indexing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631539,
author = {Barrios, Juan Manuel},
title = {Content-Based Video Copy Detection},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631539},
doi = {10.1145/1631272.1631539},
abstract = {Content-Based Video Copy Detection consists in retrieving all the modified versions of an original document in a video collection. It relies on two tasks: content description, for extracting one or many fingerprints from a video document, and similarity search, for determining the set of extracted fingerprints that make a close match.For the similarity search task, a copy detection system usually relies on a metric distance for measuring the degree of similarity between fingerprints. The metric properties represent a tradeoff between efficiency and effectiveness for a similarity search. A metric distance allows the use of well studied indexing structures. However, the metric properties restrict the similarity model that can be used for comparing two objects.For the present thesis, the main focus will be on researching similarity models for video sequences that do not necessarily comply the metric properties. In particular, we plan to research multi-metric and non-metric similarity measures for fulfilling effective and efficient detection. The issues involved in video copy detection (visual transformations, local and global fingerprints, temporal dimension, and approximated searches) make this problem a relevant topic for researching new similarity models.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1141–1142},
numpages = {2},
keywords = {non-metric spaces, content-based copy detection, multi-metric spaces},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631541,
author = {Leung, Howard and Zhang, Cha and Li, Qing and Lau, Rynson W.H. and Wah, Benjamin and El Saddik, Abdulmotaleb and Candan, K. Sel\c{c}uk and Cheng, Irene},
title = {ACM 2009 Workshop on Ambient Media Computing (AMC'09) Overview},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631541},
doi = {10.1145/1631272.1631541},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1143–1144},
numpages = {2},
keywords = {sensory media, media data integration, ambient media computing},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631542,
author = {Sano, Mutsuo and Ide, Ichiro and Miyawaki, Kenzaburo},
title = {Overview of the ACM Multimedia 2009 Workshop on Multimedia for Cooking and Eating Activities (CEA'09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631542},
doi = {10.1145/1631272.1631542},
abstract = {This overview introduces the aim of the CEA'09 workshop and the list of papers presented in the workshop.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1145–1146},
numpages = {2},
keywords = {overview, eating, cooking, workshop},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631543,
author = {Scher, Ansgar and Jain, Ramesh and Kankanhalli, Mohan},
title = {Events in Multimedia},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631543},
doi = {10.1145/1631272.1631543},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1147–1148},
numpages = {2},
keywords = {multimedia, objects, event-based applications, events, detection of events in multimedia data, event models},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631544,
author = {Shao, Ling and Shan, Caifeng and Luo, Jiebo and Etoh, Minoru},
title = {1st ACM International Workshop on Interactive Multimedia for Consumer Electronics (IMCE'09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631544},
doi = {10.1145/1631272.1631544},
abstract = {The ACM International workshop on Interactive Multimedia for Consumer Electronics (IMCE) aims to bring together researchers from both academia and industry in domains including computer vision, machine learning, audio and speech processing, communications, artificial intelligence and media technology to share and discuss recent advances in interactive user interfaces and multimedia applications. Multimedia interaction is becoming a technology applied in many consumer electronics devices and can make user interfaces more intuitive and controllable. Multiple modalities including audio, video and haptics can be utilized and fused for media interaction.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1149–1150},
numpages = {2},
keywords = {consumer electronics, human-computer interaction, multimedia, computer vision, context-awareness},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631545,
author = {Yan, Rong and Smith, John R. and Tian, Qi and Sukthankar, Rahul},
title = {The 1st Workshop on Large-Scale Multimedia Retrieval and Mining (LS-MMRM'09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631545},
doi = {10.1145/1631272.1631545},
abstract = {This workshop, as the first of its kind, aims to bring together researchers and industrial practitioners interested in large-scale multimedia data retrieval and mining. The workshop will provide a venue for the participants to explore a variety of aspects and applications on how advanced multimedia analysis techniques can be leveraged to address the challenges in large-scale data collections.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1151–1152},
numpages = {2},
keywords = {multimedia retrieval, large scale, multimedia mining},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631546,
author = {Worring, Marcel and Cucchiara, Rita},
title = {Multimedia in Forensics},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631546},
doi = {10.1145/1631272.1631546},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1153–1154},
numpages = {2},
keywords = {surveillance, forensic mining, computer vision, multimedia search, pattern recognition, forensic investigation},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631547,
author = {Fonseca, David and Garcia, Oscar and Villegas, Eva and Duran, Jaume},
title = {1st International Workshop on Media Studies and Implementations That Help Improving Access to Disabled Users (MSIADU'09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631547},
doi = {10.1145/1631272.1631547},
abstract = {This Workshop is based on the work carried out during the 3rd International Workshop on Human-Centered Computing (HCC08) held at the ACM Multimedia 2008 conference in Vancouver [1]. There, participants discussed on how to use technology to achieve an accessible environment for the disabled.The event seeks to find original and highly innovative research in the area of Multimedia (studies and implementations) in order to improve accessibility to real and virtual scenarios that might be applied to all kinds of users with disabilities or difficulties when implementing concrete actions (navigation or interaction with a particular medium). Furthermore, we need solutions to adapt the environment, products and services, to the final needs of the user.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1155–1156},
numpages = {2},
keywords = {accessibility, design, user experience, human-centered computing, multimedia},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631548,
author = {Shih, Timothy K. and Lau, Rynson W.H. and Yen, Neil Y.},
title = {ACM International Workshop on Multimedia Technologies for Distance Learning (MTDL 2009)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631548},
doi = {10.1145/1631272.1631548},
abstract = {The MTDL 2009 workshop aims to discuss the impact of multimedia technologies to e-Learning. This workshop is held in conjunction with the ACM Multimedia 2009 Conference in Beijing. As a cover paper of this workshop, we briefly summarize important technologies used in e-learning in the first section, followed by a discussion of important issues proposed in the 11 papers accepted to the workshop (among the 24 submissions).},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1157–1158},
numpages = {2},
keywords = {learning methodologies, multimedia technologies, distance learning, e-learning, game-based learning},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631549,
author = {Larson, Martha and Ordelman, Roeland and de Jong, Franciska and Kraaij, Wessel and Kohler, Joachim},
title = {Searching Multimedia Content with a Spontaneous Conversational Speech Track},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631549},
doi = {10.1145/1631272.1631549},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1159–1160},
numpages = {2},
keywords = {audio-visual retrieval, speech, spoken content, multimedia access, speech recognition},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631550,
author = {Boll, Susanne and Hoi, Steven C.H. and Luo, Jiebo and Jin, Rong and King, Irwin and Xu, Dong},
title = {First ACM SIGMM International Workshop Onsocial Media (WSM'09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631550},
doi = {10.1145/1631272.1631550},
abstract = {The ACM SIGMM International Workshop on Social Media (WSM'09) is the first workshop held in conjunction with the ACM International Multimedia Conference (MM'09) at Bejing, P.R. China, 2009. This workshop provides a forum for researchers and practitioners from all over the world to share information on their latest investigations on social media analysis, exploration, search, mining, and emerging new social media applications.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1161–1162},
numpages = {2},
keywords = {social media, multimedia, social web and social network},
location = {Beijing, China},
series = {MM '09}
}

@inproceedings{10.1145/1631272.1631551,
author = {Huet, Benoit and Tang, Jinhui and Hauptmann, Alex},
title = {ACM SIGMM the First Workshop on Web-Scale Multimedia Corpus (WSMC09)},
year = {2009},
isbn = {9781605586083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1631272.1631551},
doi = {10.1145/1631272.1631551},
abstract = {The purpose of this workshop is to bring together researchers interested in the construction and analysis of Web Scale multimedia datasets and resources. The Workshop will provide a forum to consolidate key factors related to research on very large scale multimedia dataset such as the construction of dataset, creation of ground truth, sharing and extension of such resources in terms of ground truth, features, algorithms and tools etc. The Workshop will discuss and formulate action plan towards these goals.},
booktitle = {Proceedings of the 17th ACM International Conference on Multimedia},
pages = {1163–1164},
numpages = {2},
keywords = {corpus, multimedia},
location = {Beijing, China},
series = {MM '09}
}

