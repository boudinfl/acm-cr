@inproceedings{10.1145/3286915,
author = {Changwen, Chen},
title = {Session Details: FF-1},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286915},
doi = {10.1145/3286915},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240547,
author = {Li, Chuan-Xiang and Chen, Zhen-Duo and Zhang, Peng-Fei and Luo, Xin and Nie, Liqiang and Zhang, Wei and Xu, Xin-Shun},
title = {SCRATCH: A Scalable Discrete Matrix Factorization Hashing for Cross-Modal Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240547},
doi = {10.1145/3240508.3240547},
abstract = {In recent years, many hashing methods have been proposed for the cross-modal retrieval task. However, there are still some issues that need to be further explored. For example, some of them relax the binary constraints to generate the hash codes, which may generate large quantization error. Although some discrete schemes have been proposed, most of them are time-consuming. In addition, most of the existing supervised hashing methods use an n x n similarity matrix during the optimization, making them unscalable. To address these issues, in this paper, we present a novel supervised cross-modal hashing method---Scalable disCRete mATrix faCtorization Hashing, SCRATCH for short. It leverages the collective matrix factorization on the kernelized features and the semantic embedding with labels to find a latent semantic space to preserve the intra- and inter-modality similarities. In addition, it incorporates the label matrix instead of the similarity matrix into the loss function. Based on the proposed loss function and the iterative optimization algorithm, it can learn the hash functions and binary codes simultaneously. Moreover, the binary codes can be generated discretely, reducing the quantization error generated by the relaxation scheme. Its time complexity is linear to the size of the dataset, making it scalable to large-scale datasets. Extensive experiments on three benchmark datasets, namely, Wiki, MIRFlickr-25K, and NUS-WIDE, have verified that our proposed SCRATCH model outperforms several state-of-the-art unsupervised and supervised hashing methods for cross-modal retrieval.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1–9},
numpages = {9},
keywords = {discrete optimization, hashing, cross-modal retrieval, matrix factorization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240624,
author = {Garcia del Molino, Ana and Lim, Joo-Hwee and Tan, Ah-Hwee},
title = {Predicting Visual Context for Unsupervised Event Segmentation in Continuous Photo-Streams},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240624},
doi = {10.1145/3240508.3240624},
abstract = {Segmenting video content into events provides semantic structures for indexing, retrieval, and summarization. Since motion cues are not available in continuous photo-streams, and annotations in lifelogging are scarce and costly, the frames are usually clustered into events by comparing the visual features between them in an unsupervised way. However, such methodologies are ineffective to deal with heterogeneous events, e.g. taking a walk, and temporary changes in the sight direction, e.g. at a meeting. To address these limitations, we propose Contextual Event Segmentation (CES), a novel segmentation paradigm that uses an LSTM-based generative network to model the photo-stream sequences, predict their visual context, and track their evolution. CES decides whether a frame is an event boundary by comparing the visual context generated from the frames in the past, to the visual context predicted from the future. We implemented CES on a new and massive lifelogging dataset consisting of more than 1.5 million images spanning over 1,723 days. Experiments on the popular EDUB-Seg dataset show that our model outperforms the state-of-the-art by over 16% in f-measure. Furthermore, CES' performance is only 3 points below that of human annotators.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {10–17},
numpages = {8},
keywords = {visual context prediction, event segmentation, lifelogging},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240708,
author = {Wei, Xingxing and Zhu, Jun and Feng, Sitong and Su, Hang},
title = {Video-to-Video Translation with Global Temporal Consistency},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240708},
doi = {10.1145/3240508.3240708},
abstract = {Although image-to-image translation has been widely studied, the video-to-video translation is rarely mentioned. In this paper, we propose an unified video-to-video translation framework to accom- plish different tasks, like video super-resolution, video colouriza- tion, and video segmentation, etc. A consequent question within video-to-video translation lies in the flickering appearance along with the varying frames. To overcome this issue, a usual method is to incorporate the temporal loss between adjacent frames in the optimization, which is a kind of local frame-wise temporal con- sistency. We instead present a residual error based mechanism to ensure the video-level consistency of the same location in different frames (called (lobal temporal consistency). The global and local consistency are simultaneously integrated into our video-to-video framework to achieve more stable videos. Our method is based on the GAN framework, where we present a two-channel discrimina- tor. One channel is to encode the video RGB space, and another is to encode the residual error of the video as a whole to meet the global consistency. Extensive experiments conducted on different video- to-video translation tasks verify the effectiveness and flexibleness of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {18–25},
numpages = {8},
keywords = {generative adversarial network, video-to-video translation, temporal consistency},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240520,
author = {Li, Jinxing and Zhang, Bob and Lu, Guangming and Zhang, David},
title = {Shared Linear Encoder-Based Gaussian Process Latent Variable Model for Visual Classification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240520},
doi = {10.1145/3240508.3240520},
abstract = {Multi-view learning has shown its powerful potential in many applications and achieved outstanding performances compared with the single-view based methods. In this paper, we propose a novel multi-view learning model based on the Gaussian Process Latent Variable Model (GPLVM) to learn a shared latent variable in the manifold space with a linear and gaussian process prior based back projection. Different from existing GPLVM methods which only consider a mapping from the latent space to the observed space, the proposed method simultaneously takes a back projection from the observation to the latent variable into account. Concretely, due to the various dimensions of different views, a projection for each view is first learned to linearly map its observation to a subspace. The gaussian process prior is then imposed on another transformation to non-linearly and efficiently map the learned subspace to a shared manifold space. In order to apply the proposed approach to the classification, a discriminative regularization is also embedded to exploit the label information. Experimental results on three real-world databases substantiate the effectiveness and superiority of the proposed approach as compared with several state-of-the-art approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {26–34},
numpages = {9},
keywords = {multi-view, classification, gaussian process, latent variable},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240511,
author = {Zhong, Jia-Xing and Li, Nannan and Kong, Weijie and Zhang, Tao and Li, Thomas H. and Li, Ge},
title = {Step-by-Step Erasion, One-by-One Collection: A Weakly Supervised Temporal Action Detector},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240511},
doi = {10.1145/3240508.3240511},
abstract = {Weakly supervised temporal action detection is a Herculean task in understanding untrimmed videos, since no supervisory signal except the video-level category label is available on training data. Under the supervision of category labels, weakly supervised detectors are usually built upon classifiers. However, there is an inherent contradiction between classifier and detector; i.e., a classifier in pursuit of high classification performance prefers top-level discriminative video clips that are extremely fragmentary, whereas a detector is obliged to discover the whole action instance without missing any relevant snippet. To reconcile this contradiction, we train a detector by driving a series of classifiers to find new actionness clips progressively, via step-by-step erasion from a complete video. During the test phase, all we need to do is to collect detection results from the one-by-one trained classifiers at various erasing steps. To assist in the collection process, a fully connected conditional random field is established to refine the temporal localization outputs. We evaluate our approach on two prevailing datasets, THUMOS'14 and ActivityNet. The experiments show that our detector advances state-of-the-art weakly supervised temporal action detection results, and even compares with quite a few strongly supervised methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {35–44},
numpages = {10},
keywords = {weakly supervised video understanding, untrimmed video, temporal action detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240515,
author = {Li, Jianshu and Zhao, Jian and Chen, Yunpeng and Roy, Sujoy and Yan, Shuicheng and Feng, Jiashi and Sim, Terence},
title = {Multi-Human Parsing Machines},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240515},
doi = {10.1145/3240508.3240515},
abstract = {Human parsing is an important task in human-centric analysis. Despite the remarkable progress in single-human parsing, the more realistic case of multi-human parsing remains challenging in terms of the data and the model. Compared with the considerable number of available single-human parsing datasets, the datasets for multi-human parsing are very limited in number mainly due to the huge annotation effort required. Besides the data challenge to multi-human parsing, the persons in real-world scenarios are often entangled with each other due to close interaction and body occlusion, making it difficult to distinguish body parts from different person instances. In this paper we propose the Multi-Human Parsing Machines (MHPM) system, which contains an MHP Montage model and an MHP Solver, to address both challenges in multi-human parsing. Specifically, the MHP Montage model in MHPM generates realistic images with multiple persons together with the parsing labels. It intelligently composes single persons onto background scene images while maintaining the structural information between persons and the scene. The generated images can be used to train better multi-human parsing algorithms. On the other hand, the MHP Solver in MHPM solves the bottleneck of distinguishing multiple entangled persons with close interaction. It employs a Group-Individual Push and Pull (GIPP) loss function, which can effectively separate persons with close interaction. We experimentally show that the proposed MHPM can achieve state-of-the-art performance on the multi-human parsing benchmark and the person individualization benchmark, which distinguishes closely entangled person instances.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {45–53},
numpages = {9},
keywords = {multi-human parsing, human parsing, image composition, generative adversarial networks, instance segmentation, human-centric image analysis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240527,
author = {Dong, Xuanyi and Zhu, Linchao and Zhang, De and Yang, Yi and Wu, Fei},
title = {Fast Parameter Adaptation for Few-Shot Image Captioning and Visual Question Answering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240527},
doi = {10.1145/3240508.3240527},
abstract = {Given only a few image-text pairs, humans can learn to detect semantic concepts and describe the content. For machine learning algorithms, they usually require a lot of data to train a deep neural network to solve the problem. However, it is challenging for the existing systems to generalize well to the few-shot multi-modal scenario, because the learner should understand not only images and texts but also their relationships from only a few examples. In this paper, we tackle two multi-modal problems, i.e., image captioning and visual question answering (VQA), in the few-shot setting.We propose Fast Parameter Adaptation for Image-Text Modeling (FPAIT) that learns to learn jointly understanding image and text data by a few examples. In practice, FPAIT has two benefits. (1) Fast learning ability. FPAIT learns proper initial parameters for the joint image-text learner from a large number of different tasks. When a new task comes, FPAIT can use a small number of gradient steps to achieve a good performance. (2) Robust to few examples. In few-shot tasks, the small training data will introduce large biases in Convolutional Neural Networks (CNN) and damage the learner's performance. FPAIT leverages dynamic linear transformations to alleviate the side effects of the small training set. In this way, FPAIT flexibly normalizes the features and thus reduces the biases during training. Quantitatively, FPAIT achieves superior performance on both few-shot image captioning and VQA benchmarks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {54–62},
numpages = {9},
keywords = {visual question answering, image captioning, few-shot learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240538,
author = {Wang, Junbo and Wang, Wei and Huang, Yan and Wang, Liang and Tan, Tieniu},
title = {Hierarchical Memory Modelling for Video Captioning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240538},
doi = {10.1145/3240508.3240538},
abstract = {Translating videos into natural language sentences has drawn much attention recently. The framework of combining visual attention with Long Short-Term Memory (LSTM) based text decoder has achieved much progress. However, the vision-language translation still remains unsolved due to the semantic gap and misalignment between video content and described semantic concept. In this paper, we propose a Hierarchical Memory Model (HMM) - a novel deep video captioning architecture which unifies a textual memory, a visual memory and an attribute memory in a hierarchical way. These memories can guide attention for efficient video representation extraction and semantic attribute selection in addition to modelling the long-term dependency for video sequence and sentences, respectively. Compared with traditional vision-based text decoder, the proposed attribute-based text decoder can largely reduce the semantic discrepancy between video and sentence. To prove the effectiveness of the proposed model, we perform extensive experiments on two public benchmark datasets: MSVD and MSR-VTT. Experiments show that our model not only can discover appropriate video representation and semantic attributes but also can achieve comparable or superior performances than state-of-the-art methods on these datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {63–71},
numpages = {9},
keywords = {visual attention, video captioning, hierarchical memory model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240510,
author = {Wang, Zheng and Bai, Xiang and Ye, Mang and Satoh, Shin'ichi},
title = {Incremental Deep Hidden Attribute Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240510},
doi = {10.1145/3240508.3240510},
abstract = {Person re-identifcation is a key technique to match person images captured in non-overlapping camera views. Due to the sensitivity of visual features to environmental changes, semantic attributes, such as "short-hair" or "long-hair", begin to be investigated to represent person's appearance to improve the re-identifcation performance. Generally, training semantic attribute representations requires massive annotated samples, which limits the applicability on the large-scale practical applications. To alleviate the reliance on annotation efforts, we propose a new person representation with hidden attributes by mining latent information from visual feature in an unsupervised manner. In particular, an auto-encoder model is plugged-in to the deep learning network to compose a Deep Hidden Attribute Network (DHA-Net). The learnt hidden attribute representation preserves the robustness of semantic attributes and simultaneously inherits the discrimination ability of visual features. Experiments conducted on public datasets have validated the effectiveness of DHA-Net. On two large-scale datasets, i.e., Market-1501 and DukeMTMC-reID, the proposed method outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {72–80},
numpages = {9},
keywords = {hidden attribute, person re-identification, unsupervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240517,
author = {Chen, Huarong and Wang, Bin and Pan, Tianxiang and Zhou, Liwang and Zeng, Hua},
title = {CropNet: Real-Time Thumbnailing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240517},
doi = {10.1145/3240508.3240517},
abstract = {We present a deep learning-based thumbnail generation method called CropNet in this paper. Unlike previous deep learning-based methods, such as Fast-AT, which can utilize detectors introduced in object detection frameworks and generate thousands of proposals, our detector is straightforward and concise, thereby ensuring that the final cropping window is computed by its center and width, with the input aspect ratio. To achieve this goal, CropNet learns specific filters to estimate the center position and utilizes a cascade structure of filters and single neuron for width inference. In addition, CropNet optimizes the center and width jointly for optimal results. We collect a data set of more than 29,000 thumbnail annotations to train CropNet and perform cross-validation between existing data sets. Experiments show that CropNet outperforms existing techniques. Our result is achieved at a test-time speed of 17 ms per image, which is six times faster than the fastest method at present.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {81–89},
numpages = {9},
keywords = {automatic cropping, convolutional neural network, object detection, thumbnail generation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240518,
author = {Cheng, Zhi-Qi and Wu, Xiao and Huang, Siyu and Li, Jun-Xiu and Hauptmann, Alexander G. and Peng, Qiang},
title = {Learning to Transfer: Generalizable Attribute Learning with Multitask Neural Model Search},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240518},
doi = {10.1145/3240508.3240518},
abstract = {As attribute leaning brings mid-level semantic properties for objects, it can benefit many traditional learning problems in multimedia and computer vision communities. When facing the huge number of attributes, it is extremely challenging to automatically design a generalizable neural network for other attribute learning tasks. Even for a specific attribute domain, the exploration of the neural network architecture is always optimized by a combination of heuristics and grid search, from which there is a large space of possible choices to be searched. In this paper, Generalizable Attribute Learning Model (GALM) is proposed to automatically design the neural networks for generalizable attribute learning. The main novelty of GALM is that it fully exploits the Multi-Task Learning and Reinforcement Learning to speed up the search procedure. With the help of parameter sharing, GALM is able to transfer the pre-searched architecture to different attribute domains. In experiments, we comprehensively evaluate GALM on 251 attributes from three domains: animals, objects, and scenes. Extensive experimental results demonstrate that GALM significantly outperforms the state-of-the-art attribute learning approaches and previous neural architecture search methods on two generalizable attribute learning scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {90–98},
numpages = {9},
keywords = {generalizable attribute learning, reinforcement learning, multi-task learning, transfer learning, neural architecture search},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240525,
author = {Zhu, Yingying and Wang, Jiong and Xie, Lingxi and Zheng, Liang},
title = {Attention-Based Pyramid Aggregation Network for Visual Place Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240525},
doi = {10.1145/3240508.3240525},
abstract = {Visual place recognition is challenging in the urban environment and is usually viewed as a large scale image retrieval task. The intrinsic challenges in place recognition exist that the confusing objects such as cars and trees frequently occur in the complex urban scene, and buildings with repetitive structures may cause over-counting and the burstiness problem degrading the image representations. To address these problems, we present an Attention-based Pyramid Aggregation Network (APANet), which is trained in an end-to-end manner for place recognition. One main component of APANet, the spatial pyramid pooling, can effectively encode the multi-size buildings containing geo-information. The other one, the attention block, is adopted as a region evaluator for suppressing the confusing regional features while highlighting the discriminative ones. When testing, we further propose a simple yet effective PCA power whitening strategy, which significantly improves the widely used PCA whitening by reasonably limiting the impact of over-counting. Experimental evaluations demonstrate that the proposed APANet outperforms the state-of-the-art methods on two place recognition benchmarks, and generalizes well on standard image retrieval datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {99–107},
numpages = {9},
keywords = {convolutional neural network, place recognition, attention mechanism, content-based image retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240528,
author = {Du, Changde and Du, Changying and Wang, Hao and Li, Jinpeng and Zheng, Wei-Long and Lu, Bao-Liang and He, Huiguang},
title = {Semi-Supervised Deep Generative Modelling of Incomplete Multi-Modality Emotional Data},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240528},
doi = {10.1145/3240508.3240528},
abstract = {There are threefold challenges in emotion recognition. First, it is difficult to recognize human's emotional states only considering a single modality. Second, it is expensive to manually annotate the emotional data. Third, emotional data often suffers from missing modalities due to unforeseeable sensor malfunction or configuration issues. In this paper, we address all these problems under a novel multi-view deep generative framework. Specifically, we propose to model the statistical relationships of multi-modality emotional data using multiple modality-specific generative networks with a shared latent space. By imposing a Gaussian mixture assumption on the posterior approximation of the shared latent variables, our framework can learn the joint deep representation from multiple modalities and evaluate the importance of each modality simultaneously. To solve the labeled-data-scarcity problem, we extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. To address the missing-modality problem, we further extend our semi-supervised multi-view model to deal with incomplete data, where a missing view is treated as a latent variable and integrated out during inference. This way, the proposed overall framework can utilize all available (both labeled and unlabeled, as well as both complete and incomplete) data to improve its generalization ability. The experiments conducted on two real multi-modal emotion datasets demonstrated the superiority of our framework.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {108–116},
numpages = {9},
keywords = {multi-view semi-supervised learning, deep generative model, incomplete data, multi-modal emotion recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240533,
author = {Chen, Yuxiao and Yuan, Jianbo and You, Quanzeng and Luo, Jiebo},
title = {Twitter Sentiment Analysis via Bi-Sense Emoji Embedding and Attention-Based LSTM},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240533},
doi = {10.1145/3240508.3240533},
abstract = {Sentiment analysis on large-scale social media data is important to bridge the gaps between social media contents and real world activities including political election prediction, individual and public emotional status monitoring and analysis, and so on. Although textual sentiment analysis has been well studied based on platforms such as Twitter and Instagram, analysis of the role of extensive emoji uses in sentiment analysis remains light. In this paper, we propose a novel scheme for Twitter sentiment analysis with extra attention on emojis. We first learn bi-sense emoji embeddings under positive and negative sentimental tweets individually, and then train a sentiment classifier by attending on these bi-sense emoji embeddings with an attention-based long short-term memory network (LSTM). Our experiments show that the bi-sense embedding is effective for extracting sentiment-aware embeddings of emojis and outperforms the state-of-the-art models. We also visualize the attentions to show that the bi-sense emoji embedding provides better guidance on the attention mechanism to obtain a more robust understanding of the semantics and sentiments.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {117–125},
numpages = {9},
keywords = {emoji, bi-sense embedding, attention, sentiment analysis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240574,
author = {Zhang, Feifei and Zhang, Tianzhu and Mao, Qirong and Duan, Lingyu and Xu, Changsheng},
title = {Facial Expression Recognition in the Wild: A Cycle-Consistent Adversarial Attention Transfer Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240574},
doi = {10.1145/3240508.3240574},
abstract = {Facial expression recognition (FER) is a very challenging problem due to different expressions under arbitrary poses. Most conventional approaches mainly perform FER under laboratory controlled environment. Different from existing methods, in this paper, we formulate the FER in the wild as a domain adaptation problem, and propose a novel auxiliary domain guided Cycle-consistent adversarial Attention Transfer model (CycleAT) for simultaneous facial image synthesis and facial expression recognition in the wild. The proposed model utilizes large-scale unlabeled web facial images as an auxiliary domain to reduce the gap between source domain and target domain based on generative adversarial networks (GAN) embedded with an effective attention transfer module, which enjoys several merits. First, the GAN-based method can automatically generate labeled facial images in the wild through harnessing information from labeled facial images in source domain and unlabeled web facial images in auxiliary domain. Second, the class-discriminative spatial attention maps from the classifier in source domain are leveraged to boost the performance of the classifier in target domain. Third, it can effectively preserve the structural consistency of local pixels and global attributes in the synthesized facial images through pixel cycle-consistency and discriminative loss. Quantitative and qualitative evaluations on two challenging in-the-wild datasets demonstrate that the proposed model performs favorably against state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {126–135},
numpages = {10},
keywords = {emotional cue extraction, domain adaptation, generative adversarial networks, facial expression recognition, attention transfer},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240575,
author = {Li, Runnan and Wu, Zhiyong and Jia, Jia and Li, Jingbei and Chen, Wei and Meng, Helen},
title = {Inferring User Emotive State Changes in Realistic Human-Computer Conversational Dialogs},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240575},
doi = {10.1145/3240508.3240575},
abstract = {Human-computer conversational interactions are increasingly pervasive in real-world applications, such as chatbots and virtual assistants. The user experience can be enhanced through affective design of such conversational dialogs, especially in enabling the computer to understand the emotive state in the user's input, and to generate an appropriate system response within the dialog turn. Such a system response may further influence the user's emotive state in the subsequent dialog turn. In this paper, we focus on the change in the user's emotive states in adjacent dialog turns, to which we refer as user emotive state change. We propose a multi-modal, multi-task deep learning framework to infer the user's emotive states and emotive state changes simultaneously. Multi-task learning convolution fusion auto-encoder is applied to fuse the acoustic and textual features to generate a robust representation of the user's input. Long-short term memory recurrent auto-encoder is employed to extract features of system responses at the sentence-level to better capture factors affecting user emotive states. Multi-task learned structured output layer is adopted to model the dependency of user emotive state change, conditioned upon the user input's emotive states and system response in current dialog turn. Experimental results demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {136–144},
numpages = {9},
keywords = {multi-modal multi-task deep learning, convolution fusion auto-encoder, emotive state changes prediction, structured output layer, human-computer interaction, recurrent auto-encoder},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240530,
author = {Liu, Zhengzhe and Qi, Xiaojuan and Pang, Lei},
title = {Self-Boosted Gesture Interactive System with ST-Net},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240530},
doi = {10.1145/3240508.3240530},
abstract = {In this paper, we propose a self-boosted intelligent system for joint sign language recognition and automatic education. A novel Spatial-Temporal Net (ST-Net) is designed to exploit the temporal dynamics of localized hands for sign language recognition. Features from ST-Net can be deployed by our education system to detect failure modes of the learners. Moreover, the education system can help collect a vast amount of data for training ST-Net. Our sign language recognition and education system help improve each other step-by-step.On the one hand, benefited from accurate recognition system, the education system can detect the failure parts of the learner more precisely. On the other hand, with more training data gathered from the education system, the recognition system becomes more robust and accurate. Experiments on Hong Kong sign language dataset containing 227 commonly used words validate the effectiveness of our joint recognition and education system.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {145–153},
numpages = {9},
keywords = {convolutional neural networks, interactive system},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240537,
author = {Kosmalla, Felix and Murlowski, Christian and Daiber, Florian and Kr\"{u}ger, Antonio},
title = {Slackliner - An Interactive Slackline Training Assistant},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240537},
doi = {10.1145/3240508.3240537},
abstract = {In this paper we present Slackliner, an interactive slackline training assistant which features a life-size projection, skeleton tracking and real-time feedback. As in other sports, proper training leads to a faster buildup of skill and lessens the risk for injuries. We chose a set of exercises from slackline literature and implemented an interactive trainer which guides the user through the exercises and gives feedback if the exercise was executed correctly.A post analysis gives the user feedback about her performance. We conducted a user study to compare the interactive slackline training system with a classic approach using a personal trainer. No significant difference was found between groups regarding balancing time, number of steps and the walking distance on the line for the left and right foot. Significant main effects for the balancing time on line, without considering the group, have been found. User feedback acquired by questionnaires and semi-structured interviews was very positive. Overall, the results indicate that the interactive slackline training system can be used as an enjoyable and effective alternative to classic training methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {154–162},
numpages = {9},
keywords = {sports technologies, real-time feedback, slackline, projection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240573,
author = {Li, Yaoyu and Zhang, Tianzhu and Duan, Lingyu and Xu, Changsheng},
title = {A Unified Generative Adversarial Framework for Image Generation and Person Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240573},
doi = {10.1145/3240508.3240573},
abstract = {Person re-identification (re-id) aims to match a certain person across multiple non-overlapping cameras. It is a challenging task because the same person's appearance can be very different across camera views due to the presence of large pose variations. To overcome this issue, in this paper, we propose a novel unified person re-id framework by exploiting person poses and identities jointly for simultaneous person image synthesis under arbitrary poses and pose-invariant person re-identification. The framework is composed of a GAN based network and two Feature Extraction Networks (FEN), and enjoys following merits. First, it is a unified generative adversarial model for person image generation and person re-identification. Second, a pose estimator is utilized into the generator as a supervisor in the training process, which can effectively help pose transfer and guide the image generation with any desired pose. As a result, the proposed model can automatically generate a person image under an arbitrary pose. Third, the identity-sensitive representation is explicitly disentangled from pose variations through the person identity and pose embedding. Fourth, the learned re-id model can have better generalizability on a new person re-id dataset by using the synthesized images as auxiliary samples. Extensive experimental results on four standard benchmarks including Market-1501 [69], DukeMTMC-reID [40], CUHK03 [23], and CUHK01 [22] demonstrate that the proposed model can perform favorably against state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {163–172},
numpages = {10},
keywords = {person re-identification, gan, multimedia system},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240680,
author = {Mahzari, Anahita and Taghavi Nasrabadi, Afshin and Samiei, Aliehsan and Prakash, Ravi},
title = {FoV-Aware Edge Caching for Adaptive 360° Video Streaming},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240680},
doi = {10.1145/3240508.3240680},
abstract = {In recent years, there has been growing popularity of Virtual Reality (VR), enabled by technologies like 360° video streaming. Streaming 360° video is extremely challenging due to high bandwidth and low latency requirements. Some VR solutions employ adaptive 360° video streaming which tries to reduce bandwidth consumption by only streaming high resolution video for user's Field of View (FoV). FoV is the part of the video which is being viewed by the user at any given time. Although FoV-adaptive 360° video streaming has been helpful in reducing bandwidth requirements, streaming 360° video from distant content servers is still challenging due to network latency. Caching popular content close to the end users not only decreases network latency, but also alleviates network bandwidth demands by reducing the number of future requests that have to be sent all the way to remote content servers. In this paper, we propose a novel caching policy based on users' FoV, called FoV-aware caching policy. In FoV-aware caching policy, we learn a probabilistic model of common-FoV for each 360° video based on previous users' viewing histories to improve caching performance. Through experiments with real users' head movement dataset, we show that our proposed approach improves cache hit ratio compared to Least Frequently Used (LFU) and Least Recently Used (LRU) caching policies by at least 40% and 17%, respectively.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {173–181},
numpages = {9},
keywords = {caching strategy, edge caching, field of view (fov), adaptive 360° video streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286916,
author = {Boll, Susanne},
title = {Session Details: Keynote 1},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286916},
doi = {10.1145/3286916},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3267343,
author = {Obrist, Marianna},
title = {Don't Just Look -- Smell, Taste, and Feel the Interaction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3267343},
doi = {10.1145/3240508.3267343},
abstract = {While our understanding of the sensory modalities for human-computer interaction (HCI) is advancing, there is still a huge gap in our understanding of 'how' to best integrate different modalities into the interaction with technology. While we have established a rich ecosystem for visual and auditory design, our sense of touch, taste, and smell are not yet supported with dedicated software tools, interaction techniques, and design tools. Hence, we are missing out on the opportunity to exploit the power of those sensory modalities, their strong link to emotions and memory, and their ability to facilitate recall and recognition in information processing and decision making. In this keynote, Dr Obrist presents an overview of scientific and technological developments in multisensory research and design with an emphasis on an experience-centered design approach that bridges and integrates knowledge on human sensory perception and advances in computing technology.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {182},
numpages = {1},
keywords = {smell, touch, taste, interaction design, hci, multisensory experiences, user experience, interaction modalities},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286917,
author = {Cui, Peng},
title = {Session Details: FF-2},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286917},
doi = {10.1145/3286917},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240524,
author = {Zhang, Rui and Tang, Sheng and Li, Yu and Guo, Junbo and Zhang, Yongdong and Li, Jintao and Yan, Shuicheng},
title = {Style Separation and Synthesis via Generative Adversarial Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240524},
doi = {10.1145/3240508.3240524},
abstract = {Style synthesis attracts great interests recently, while few works focus on its dual problem "style separation". In this paper, we propose the Style Separation and Synthesis Generative Adversarial Network (S3-GAN) to simultaneously implement style separation and style synthesis on object photographs of specific categories. Based on the assumption that the object photographs lie on a manifold, and the contents and styles are independent, we employ S3-GAN to build mappings between the manifold and a latent vector space for separating and synthesizing the contents and styles. The S3-GAN consists of an encoder network, a generator network, and an adversarial network. The encoder network performs style separation by mapping an object photograph to a latent vector. Two halves of the latent vector represent the content and style, respectively. The generator network performs style synthesis by taking a concatenated vector as input. The concatenated vector contains the style half vector of the style target image and the content half vector of the content target image. Once obtaining the images from the generator network, an adversarial network is imposed to generate more photo-realistic images. Experiments on CelebA and UT Zappos 50K datasets demonstrate that the S3-GAN has the capacity of style separation and synthesis simultaneously, and could capture various styles in a single model.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {183–191},
numpages = {9},
keywords = {style separation, style synthesis, generative adversarial network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240539,
author = {Xiao, Hao and Lin, Weiyao and Sheng, Bin and Lu, Ke and Yan, Junchi and Wang, Jingdong and Ding, Errui and Zhang, Yihao and Xiong, Hongkai},
title = {Group Re-Identification: Leveraging and Integrating Multi-Grain Information},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240539},
doi = {10.1145/3240508.3240539},
abstract = {This paper addresses an important yet less-studied problem: re-identifying groups of people in different camera views. Group re-identification (Re-ID) is very challenging since it is not only interfered by view-point and human pose variations in the traditional single-object Re-ID tasks, but also suffers from group layout and group member variations. To handle these issues, we propose to leverage the information of multi-grain objects: individual person and subgroups of two and three people inside a group image. We compute multi-grain representations to characterize the appearance and spatial features of multi-grain objects and evaluate the importance weight of each object for group Re-ID, so as to handle the interferences from group dynamics. We compute the optimal group-wise matching by using a multi-order matching process based on the multi-grain representation and importance weights. Furthermore, we dynamically update the importance weights according to the current matching results and then compute a new optimal group-wise matching. The two steps are iteratively conducted, yielding the final matching results.Experimental results on various datasets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {192–200},
numpages = {9},
keywords = {group re-identification, multi-grain representation, re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240548,
author = {Gao, Xu and Jiang, Tingting},
title = {OSMO: Online Specific Models for Occlusion in Multiple Object Tracking under Surveillance Scene},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240548},
doi = {10.1145/3240508.3240548},
abstract = {With demands of the intelligent monitoring, multiple object tracking (MOT) in surveillance scene has become an essential but challenging task. Occlusion is the primary difficulty in surveillance MOT, which can be categorized into the inter-object occlusion and the obstacle occlusion. Many current studies on general MOT focus on the former occlusion, but few studies have been conducted on the latter one. In fact, there are useful prior knowledge in surveillance videos, because the scene structure is fixed. Hence, we propose two models for dealing with these two kinds of occlusions. The attention-based appearance model is proposed to solve the inter-object occlusion, and the scene structure model is proposed to solve the obstacle occlusion. We also design an obstacle map segmentation method for segmenting obstacles from the surveillance scene. Furthermore, to evaluate our method, we propose four new surveillance datasets that contain videos with obstacles. Experimental results show the effectiveness of our two models.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {201–210},
numpages = {10},
keywords = {surveillance, scene structure model, attention-based appearance model, obstacle map, multiple object tracking},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240551,
author = {Li, Yuke},
title = {Video Forecasting with Forward-Backward-Net: Delving Deeper into Spatiotemporal Consistency},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240551},
doi = {10.1145/3240508.3240551},
abstract = {Video forecasting is an emerging topic in the computer vision field, and it is a pivotal step toward unsupervised video understanding. However, the predictions generated from the state-of-the-art methods might be far from ideal quality, due to a lack of guidance from the labeled data of correct predictions (e.g., the annotated future pose of a person). Hence, building a network for better predicting future sequences in an unsupervised manner has to be further pursued. To this end, we put forth a novel Forward-Backward-Net (FB-Net) architecture, which delves deeper into spatiotemporal consistency. It first derives the forward consistency from the raw historical observations. In contrast to mainstream video forecasting approaches, FB-Net then investigates the backward consistency from the future to the past to reinforce the predictions. The final predicted results are inferred by jointly taking both the forward and backward consistencies into account. Moreover, we embed the motion dynamics and the visual content into a single framework via the FB-Net architecture, which significantly differs from learning each component throughout the videos separately. We evaluate our FB-Net on the large-scale KTH and UCF101 datasets. The experiments show that it can introduce considerable margin improvements with respect to most recent leading studies.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {211–219},
numpages = {9},
keywords = {unsupervised learning, deep neural network, spatiotemporal consistency, computer vision, video forecasting},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240562,
author = {Shao, Rui and Lan, Xiangyuan and Yuen, Pong C.},
title = {Feature Constrained by Pixel: Hierarchical Adversarial Deep Domain Adaptation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240562},
doi = {10.1145/3240508.3240562},
abstract = {In multimedia analysis, one objective of unsupervised visual domain adaptation is to train a classifier that works well on a target domain given labeled source samples and unlabeled target samples. Feature alignment of two domains is the key issue which should be addressed to achieve this objective. Inspired by the recent study of Generative Adversarial Networks (GAN) in domain adaptation, this paper proposes a new model based on Generative Adversarial Network, named Hierarchical Adversarial Deep Network (HADN), which jointly optimizes the feature-level and pixel-level adversarial adaptation within a hierarchical network structure. Specifically, the hierarchical network structure ensures that the knowledge from pixel-level adversarial adaptation can be back propagated to facilitate the feature-level adaptation, which achieves a better feature alignment under the constraint of pixel-level adversarial adaptation. Extensive experiments on various visual recognition tasks show that the proposed method performs favorably against or better than competitive state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {220–228},
numpages = {9},
keywords = {generative adversarial network, unsupervised domain adaptation, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240568,
author = {Chen, Zhixing and Huang, Di and Wang, Yunhong and Chen, Liming},
title = {Fast and Light Manifold CNN Based 3D Facial Expression Recognition across Pose Variations},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240568},
doi = {10.1145/3240508.3240568},
abstract = {This paper proposes a novel approach to 3D Facial Expression Recognition (FER), and it is based on a Fast and Light Manifold CNN model, namely FLM-CNN. Different from current manifold CNNs, FLM-CNN adopts a human vision inspired pooling structure and a multi-scale encoding strategy to enhance geometry representation, which highlights shape characteristics of expressions and runs efficiently. Furthermore, a sampling tree based preprocessing method is presented, and it sharply saves memory when applied to 3D facial surfaces, without much information loss of original data. More importantly, due to the property of manifold CNN features of being rotation-invariant, the proposed method shows a high robustness to pose variations. Extensive experiments are conducted on BU-3DFE, and state-of-the-art results are achieved, indicating its effectiveness.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {229–238},
numpages = {10},
keywords = {rotation-invariance, 3d facial expression recognition, manifold convolutional neural network, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240563,
author = {Song, Xiaomeng and Shi, Yucheng and Chen, Xin and Han, Yahong},
title = {Explore Multi-Step Reasoning in Video Question Answering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240563},
doi = {10.1145/3240508.3240563},
abstract = {Video question answering (VideoQA) always involves visual reasoning. When answering questions composing of multiple logic correlations, models need to perform multi-step reasoning. In this paper, we formulate multi-step reasoning in VideoQA as a new task to answer compositional and logical structured questions based on video content. Existing VideoQA datasets are inadequate as benchmarks for the multi-step reasoning due to limitations such as lacking logical structure and having language biases. Thus we design a system to automatically generate a large-scale dataset, namely SVQA (Synthetic Video Question Answering). Compared with other VideoQA datasets, SVQA contains exclusively long and structured questions with various spatial and temporal relations between objects. More importantly, questions in SVQA can be decomposed into human readable logical tree or chain layouts, each node of which represents a sub-task requiring a reasoning operation such as comparison or arithmetic. Towards automatic question answering in SVQA, we develop a new VideoQA model. Particularly, we construct a new attention module, which contains spatial attention mechanism to address crucial and multiple logical sub-tasks embedded in questions, as well as a refined GRU called ta-GRU (temporal-attention GRU) to capture the long-term temporal dependency and gather complete visual cues. Experimental results show the capability of multi-step reasoning of SVQA and the effectiveness of our model when compared with other existing models.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {239–247},
numpages = {9},
keywords = {video question answering, multi-step reasoning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240571,
author = {Fang, Shancheng and Xie, Hongtao and Zha, Zheng-Jun and Sun, Nannan and Tan, Jianlong and Zhang, Yongdong},
title = {Attention and Language Ensemble for Scene Text Recognition with Convolutional Sequence Modeling},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240571},
doi = {10.1145/3240508.3240571},
abstract = {Recent dominant approaches for scene text recognition are mainly based on convolutional neural network (CNN) and recurrent neural network (RNN), where the CNN processes images and the RNN generates character sequences. Different from these methods, we propose an attention-based architecture1 which is completely based on CNNs. The distinctive characteristics of our method include: (1) the method follows encoder-decoder architecture, in which the encoder is a two-dimensional residual CNN and the decoder is a deep one-dimensional CNN. (2) An attention module that captures visual cues, and a language module that models linguistic rules are designed equally in the decoder. Therefore the attention and language can be viewed as an ensemble to boost predictions jointly. (3) Instead of using a single loss from language aspect, multiple losses from attention and language are accumulated for training the networks in an end-to-end way. We conduct experiments on standard datasets for scene text recognition, including Street View Text, IIIT5K and ICDAR datasets. The experimental results show our CNN-based method has achieved state-of-the-art performance on several benchmark datasets, even without the use of RNN.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {248–256},
numpages = {9},
keywords = {text recognition, attention model, convolutional neural networks, multi-level supervised information},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240534,
author = {Zhang, Zhaoyang and Kuang, Zhanghui and Luo, Ping and Feng, Litong and Zhang, Wei},
title = {Temporal Sequence Distillation: Towards Few-Frame Action Recognition in Videos},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240534},
doi = {10.1145/3240508.3240534},
abstract = {Video Analytics Software as a Service (VA SaaS) has been rapidly growing in recent years. VA SaaS is typically accessed by users using a lightweight client. Because the transmission bandwidth between the client and cloud is usually limited and expensive, it brings great benefits to design cloud video analysis algorithms with a limited data transmission requirement. Although considerable research has been devoted to video analysis, to our best knowledge, little of them has paid attention to the transmission bandwidth limitation in SaaS. As the first attempt in this direction, this work introduces a problem of few-frame action recognition, which aims at maintaining high recognition accuracy, when accessing only a few frames during both training and test. Unlike previous work that processed dense frames, we present Temporal Sequence Distillation (TSD), which distills a long video sequence into a very short one for transmission. By end-to-end training with 3D CNNs for video action recognition, TSD learns a compact and discriminative temporal and spatial representation of video frames. On Kinetics dataset, TSD+I3D typically requires only 50% of the number of frames compared to I3D, a state-of-the-art video action recognition algorithm, to achieve almost the same accuracies. The proposed TSD has three appealing advantages. Firstly, TSD has a lightweight architecture and can be deployed in the client, eg., mobile devices, to produce compressed representative frames to save transmission bandwidth. Secondly, TSD significantly reduces the computations to run video action recognition with compressed frames on the cloud, while maintaining high recognition accuracies. Thirdly, TSD can be plugged in as a preprocessing module of any existing 3D CNNs. Extensive experiments show the effectiveness and characteristics of TSD.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {257–264},
numpages = {8},
keywords = {temporal sequence distillation, video action recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240544,
author = {Fu, Zhihang and Jin, Zhongming and Qi, Guo-Jun and Shen, Chen and Jiang, Rongxin and Chen, Yaowu and Hua, Xian-Sheng},
title = {Previewer for Multi-Scale Object Detector},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240544},
doi = {10.1145/3240508.3240544},
abstract = {Most multi-scale detectors face a challenge of small-size false positives due to the inadequacy of low-level features, which have small receptive field sizes and weak semantic capabilities. This paper demonstrates independent predictions from different feature layers on the same region is beneficial for reducing false positives. We propose a novel light-weight previewer block, which previews the objectness probability for the potential regression region of each prior box, using the stronger features with larger receptive fields and more contextual information for better predictions. This previewer block is generic and can be easily implemented in multi-scale detectors, such as SSD, RFBNet and MS-CNN. Extensive experiments are conducted on PASCAL VOC and KITTI pedestrian benchmark to show the superiority of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {265–273},
numpages = {9},
keywords = {receptive field, convolutional neural networks, object detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240552,
author = {Wang, Guanshuo and Yuan, Yufeng and Chen, Xiong and Li, Jiwei and Zhou, Xi},
title = {Learning Discriminative Features with Multiple Granularities for Person Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240552},
doi = {10.1145/3240508.3240552},
abstract = {The combination of global and partial features has been an essential solution to improve discriminative performances in person re-identification (Re-ID) tasks. Previous part-based methods mainly focus on locating regions with specific pre-defined semantics to learn local representations, which increases learning difficulty but not efficient or robust to scenarios with large variances. In this paper, we propose an end-to-end feature learning strategy integrating discriminative information with various granularities. We carefully design the Multiple Granularity Network (MGN), a multi-branch deep network architecture consisting of one branch for global feature representations and two branches for local feature representations. Instead of learning on semantic regions, we uniformly partition the images into several stripes, and vary the number of parts in different local branches to obtain local feature representations with multiple granularities. Comprehensive experiments implemented on the mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03 indicate that our method robustly achieves state-of-the-art performances and outperforms any existing approaches by a large margin. For example, on Market-1501 dataset in single query mode, we obtain a top result of Rank-1/mAP=96.6%/94.2% with this method after re-ranking.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {274–282},
numpages = {9},
keywords = {feature learning, multi-branch deep network, person re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240553,
author = {Qu, Guoxiang and Zhang, Wenwei and Wang, Zhe and Dai, Xing and Shi, Jianping and He, Junjun and Li, Fei and Zhang, Xiulan and Qiao, Yu},
title = {StripNet: Towards Topology Consistent Strip Structure Segmentation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240553},
doi = {10.1145/3240508.3240553},
abstract = {In this work, we propose to study a special semantic segmentation problem where the targets are long and continuous strip patterns. Strip patterns widely exist in medical images and natural photos, such as retinal layers in OCT images and lanes on the roads, and segmentation of them has practical significance. Traditional pixel-level segmentation methods largely ignore the structure prior of strip patterns and thus easily suffer from the topological inconformity problem, such as holes and isolated islands in segmentation results. To tackle this problem, we design a novel deep framework, StripNet, that leverages the strong end-to-end learning ability of CNNs to predict the structured outputs as a sequence of boundary locations of the target strips. Specifically, StripNet decomposes the original segmentation problem into more easily solved local boundary-regression problems, and takes account of the topological constraints on the predicted boundaries. Moreover, our framework adopts a coarse-to-fine strategy and uses carefully designed heatmaps for training the boundary localization network. We examine StripNet on two challenging strip pattern segmentation tasks, retinal layer segmentation and lane detection. Extensive experiments demonstrate that StripNet achieves excellent results and outperforms state-of-the-art methods in both tasks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {283–291},
numpages = {9},
keywords = {retinal layer segmentation, strip segmentation, lane detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240578,
author = {Albanie, Samuel and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew},
title = {Emotion Recognition in Speech Using Cross-Modal Transfer in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240578},
doi = {10.1145/3240508.3240578},
abstract = {Obtaining large, human labelled speech datasets to train models for emotion recognition is a notoriously challenging task, hindered by annotation cost and label ambiguity. In this work, we consider the task of learning embeddings for speech classification without access to any form of labelled audio. We base our approach on a simple hypothesis: that the emotional content of speech correlates with the facial expression of the speaker. By exploiting this relationship, we show that annotations of expression can be transferred from the visual domain (faces) to the speech domain (voices) through cross-modal distillation. We make the following contributions: (i) we develop a strong teacher network for facial emotion recognition that achieves the state of the art on a standard benchmark; (ii) we use the teacher to train a student, tabula rasa, to learn representations (embeddings) for speech emotion recognition without access to labelled audio data; and (iii) we show that the speech emotion embedding can be used for speech emotion recognition on external benchmark datasets. Code, models and data are available.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {292–301},
numpages = {10},
keywords = {speech emotion recognition, cross-modal transfer},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240613,
author = {Wang, Can and Wang, Shangfei},
title = {Personalized Multiple Facial Action Unit Recognition through Generative Adversarial Recognition Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240613},
doi = {10.1145/3240508.3240613},
abstract = {Personalized facial action unit (AU) recognition is challenging due to subject-dependent facial behavior. This paper proposes a method to recognize personalized multiple facial AUs through a novel generative adversarial network, which adapts the distribution of source domain facial images to that of target domain facial images and detects multiple AUs by leveraging AU dependencies. Specifically, we use a generative adversarial network to generate synthetic images from source domain; the synthetic images have a similar appearance to the target subject and retain the AU patterns of the source images. We simultaneously leverage AU dependencies to train a multiple AU classifier. Experimental results on three benchmark databases demonstrate that the proposed method can successfully realize unsupervised domain adaptation for individual AU detection, and thus outperforms state-of-the-art AU detection methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {302–310},
numpages = {9},
keywords = {personalized au recognition, generative adversarial networks, domain adaptation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240685,
author = {Beyan, Cigdem and Shahid, Muhammad and Murino, Vittorio},
title = {Investigation of Small Group Social Interactions Using Deep Visual Activity-Based Nonverbal Features},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240685},
doi = {10.1145/3240508.3240685},
abstract = {Understanding small group face-to-face interactions is a prominent research problem for social psychology while the automatic realization of it recently became popular in social computing. This is mainly investigated in terms of nonverbal behaviors, as they are one of the main facet of communication. Among several multi-modal nonverbal cues, visual activity is an important one and its sufficiently good performance can be crucial for instance, when the audio sensors are missing. The existing visual activity-based nonverbal features, which are all hand-crafted, were able to perform well enough for some applications while did not perform well for some other problems. Given these observations, we claim that there is a need of more robust feature representations, which can be learned from data itself. To realize this, we propose a novel method, which is composed of optical flow computation, deep neural network based feature learning, feature encoding and classification. Additionally, a comprehensive analysis between different feature encoding techniques is also presented. The proposed method is tested on three research topics, which can be perceived during small group interactions i.e. meetings: i) emergent leader detection, ii) emergent leadership style prediction, and iii) high/low extraversion classification. The proposed method shows (significantly) better results not only as compared to the state of the art visual activity based-nonverbal features but also when the state of the art visual activity based-nonverbal features are combined with other audio-based and video-based nonverbal features.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {311–319},
numpages = {9},
keywords = {social interactions, feature encoding, nonverbal behavior, visual activity, meetings, small groups, deep neural network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240710,
author = {Fu, Eugene Yujun and Huang, Michael Xuelin and Leong, Hong Va and Ngai, Grace},
title = {Cross-Species Learning: A Low-Cost Approach to Learning Human Fight from Animal Fight},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240710},
doi = {10.1145/3240508.3240710},
abstract = {Detecting human fight behavior from videos is important in social signal processing, especially in the context of surveillance. However, the uncommon occurrence of real human fight events generally restricts the data collection for fight detection in machine learning, and thus hampers the performance of contemporary data-driven approaches. To address this challenge, we present a novel cross-species learning method with a set of low-computational cost motion features for fight detection. It effectively circumvents the problem of limited human fight data for data-demaining approaches. Our method exploits the intrinsic commonality between human and animal fights, such as the physical acceleration of moving body parts. It also leverages an ensemble learning mechanism to adapt useful knowledge from similar source subsets across species. Our evaluation results demonstrate the effectiveness of the proposed feature representation for cross-species adaptation. We believe that cross-species learning is not only a promising solution to the data constraint issue, but it also sheds lights on the studies of other human mental and social behaviors in cross-disciplinary research.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {320–327},
numpages = {8},
keywords = {domain adaptation, violence surveillance, transfer learning, motion analysis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240598,
author = {Xu, Qianli and Subbaraju, Vigneshwaran and Cheong, Chee How and Wang, Aijing and Kang, Kathleen and Bashir, Munirah and Dong, Yanhong and Li, Liyuan and Lim, Joo-Hwee},
title = {Personalized Serious Games for Cognitive Intervention with Lifelog Visual Analytics},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240598},
doi = {10.1145/3240508.3240598},
abstract = {This paper presents a novel serious game app and a method to cre- ate and integrate personalized game content based on lifelog visual analytics. The main objective is to extract personalized content from visual lifelogs, integrate it into mobile games, and evaluate the effect of personalization on user experience. First, a suite of visual analysis methods is proposed to extract semantic informa- tion from visual lifelogs and discover the association among the lifelog entities. The outcome is dataset that contains augmented and personal lifelog images. Next, a mobile game app is developed that makes use of the dataset as game content. Finally, an experiment is conducted to evaluate user gameplay behaviors in the wild over three months, where a mixture of generic and personalized game content is deployed. It is observed that user adherence is heightened by personalized game content as compared to generic content. Also observed is a higher enjoyment level in personalized than generic game content. The result provides the first empirical evidence of the effect of personalized games on user adherence and preference for cognitive intervention. This work paves the way for effective cognitive training with user-generated content.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {328–336},
numpages = {9},
keywords = {visual analytics, serious games, cognitive intervention, lifelog},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240692,
author = {Bolier, Wendy and H\"{u}rst, Wolfgang and van Bommel, Guido and Bosman, Joost and Bosman, Harri\"{e}t},
title = {Drawing in a Virtual 3D Space - Introducing VR Drawing in Elementary School Art Education},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240692},
doi = {10.1145/3240508.3240692},
abstract = {Drawing is an important part of elementary school education, especially since it contributes to the development of spatial skills. Virtual reality enables us to draw not just on a flat 2D surface, but in 3D space. Our research aims at showing if and how this form of 3D drawing can be beneficial for art education. This paper presents first insights into potential benefits and obstacles when introducing 3D drawing at elementary schools. In an experiment with 18 children, we studied practical aspects, proficiency, and spatial ability development. Our results show improvement in the children's 3D drawing skills but not in their spatial abilities. Their drawing skills also do seem to be correlated with their mental rotation ability, although further research is needed to conclusively confirm this.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {337–345},
numpages = {9},
keywords = {drawing, painting, 3d, virtual reality, children, spatial abilities, mental rotation, spatial visualization, art education},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240697,
author = {Lovagnini, Luca and Zhang, Wenxiao and Bijarbooneh, Farshid Hassani and Hui, Pan},
title = {CIRCE: Real-Time Caching for Instance Recognition on Cloud Environments and Multi-Core Architectures},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240697},
doi = {10.1145/3240508.3240697},
abstract = {In the smartphone era, instance recognition (IR) applications are widely used on mobile devices. However, IR applications are computationally expensive for a single mobile device, and usually they are delegated to back end servers. Nevertheless, even by exploiting the computational power offered by cloud services, IR tasks are not executed in real-time (i.e., in less than 30ms), which is crucial for interactive mobile applications. In this work, we present caching for instance recognition on cloud environments (CIRCE), a similarity caching (SC) framework designed to improve the performance of mobile IR applications in terms of execution time. Additionally, we introduce a parallel version of the Hessian-Affine detector combined with the SIFT descriptor, and a novel cache hit threshold (CHT) algorithm. Finally, we present two new public image datasets that we create to evaluate CIRCE. By using a cache size of just few hundreds, we obtain a hit ratio of at least 66% and a precision of at least 97%. In case of a cache hit, our system performs IR tasks in at most 14ms on three different applications.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {346–354},
numpages = {9},
keywords = {image search, cloud computing, parallel algorithm, caching, instance recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240561,
author = {Zhang, Wenxiao and Han, Bo and Hui, Pan},
title = {Jaguar: Low Latency Mobile Augmented Reality with Flexible Tracking},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240561},
doi = {10.1145/3240508.3240561},
abstract = {In this paper, we present the design, implementation and evaluation of Jaguar, a mobile Augmented Reality (AR) system that features accurate, low-latency, and large-scale object recognition and flexible, robust, and context-aware tracking. Jaguar pushes the limit of mobile AR's end-to-end latency by leveraging hardware acceleration with GPUs on edge cloud. Another distinctive aspect of Jaguar is that it seamlessly integrates marker-less object tracking offered by the recently released AR development tools (e.g., ARCore and ARKit) into its design. Indeed, some approaches used in Jaguar have been studied before in a standalone manner, e.g., it is known that cloud offloading can significantly decrease the computational latency of AR. However, the question of whether the combination of marker-less tracking, cloud offloading and GPU acceleration would satisfy the desired end-to-end latency of mobile AR (i.e., the interval of camera frames) has not been eloquently addressed yet. We demonstrate via a prototype implementation of our proposed holistic solution that Jaguar reduces the end-to-end latency to ~33 ms. It also achieves accurate six degrees of freedom tracking and 97% recognition accuracy for a dataset with 10,000 images.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {355–363},
numpages = {9},
keywords = {mobile edge computing, object recognition, augmented reality, gpu acceleration, object tracking},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286918,
author = {Mei, Tao},
title = {Session Details: Keynote 2},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286918},
doi = {10.1145/3286918},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3267342,
author = {Hua, Xian-Sheng},
title = {Challenges and Practices of Large Scale Visual Intelligence in the Real-World},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3267342},
doi = {10.1145/3240508.3267342},
abstract = {Visual intelligence is one of the key aspects of Artificial Intelligence. Considerable technology progresses along this direction have been made in the past a few years. However, how to incubate the right technologies and convert them into real business values in the real-world remains a challenge. In this talk, we will analyze current challenges of visual intelligence in the real-world and try to summarize a few key points that help us successfully develop and apply technologies to solve real-world problems. In particular, we will introduce a few successful examples, including "City Brain", "Luban (visual design)", from the problem definition/discovery, to technology development, to product design, and to realizing business values. City Brain: A city is an aggregate of a huge amount of heterogeneous data. However, extracting meaningful values from that data is nontrivial. City Brain is an end-to-end system whose goal is to glean irreplaceable values from big-city data, specifically videos, with the assistance of rapidly evolving AI technologies and fast-growing computing capacity. From cognition to optimization, to decision-making, from search to prediction and ultimately, to intervention, City Brain improves the way we manage the city, as well as the way we live in it. In this talk, we will introduce current practices of the City Brain platform, as well as what we can do to achieve the goal and make it a reality, step by step. Luban: Different from most typical visual intelligence technologies, which are more focused on analyzing, recognizing or searching visual objects, the goal of Luban (visual design) is to create visual content. In particular, we will introduce an automatic 2D banner design technique that is based on deep learning and reinforcement learning. We will detail how Luban was created and how it changed the world of 2D banner design by creating 50M banners a day.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {364},
numpages = {1},
keywords = {computer vision, system, artificial intelligence, big data},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286919,
author = {Sebe, Nicu},
title = {Session Details: Deep-1 (Image Translation)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286919},
doi = {10.1145/3286919},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240637,
author = {Zhi, Yuheng and Wei, Huawei and Ni, Bingbing},
title = {Structure Guided Photorealistic Style Transfer},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240637},
doi = {10.1145/3240508.3240637},
abstract = {Recent style transfer methods based on deep networks strive to generate more content matching stylized images by adding semantic guidance in the iterative process. However, these approaches can just guarantee the transfer of integral color and texture distribution between semantically equivalent regions, but local variation within these regions cannot be accurately captured. Therefore, the resulting image lacks local plausibility. To this end, we develop a non-parametric patch based style transfer framework to synthesize more content coherent images. By designing a novel patch matching algorithm which simultaneously takes high-level category information and geometric structure information (e.g., human pose and building structure) into account, our proposed method is capable of transferring more detailed distribution and producing more photorealistic stylized images. We show that our approach achieves remarkable style transfer results on contents with geometric structure, including human body, vehicles, buildings, etc.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {365–373},
numpages = {9},
keywords = {photorealistic, correspondence, style transfer},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240716,
author = {Yang, Xuewen and Xie, Dongliang and Wang, Xin},
title = {Crossing-Domain Generative Adversarial Networks for Unsupervised Multi-Domain Image-to-Image Translation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240716},
doi = {10.1145/3240508.3240716},
abstract = {State-of-the-art techniques in Generative Adversarial Networks (GANs) have shown remarkable success in image-to-image translation from peer domain X to domain Y using paired image data. However, obtaining abundant paired data is a non-trivial and expensive process in the majority of applications. When there is a need to translate images across n domains, if the training is performed between every two domains, the complexity of the training will increase quadratically. Moreover, training with data from two domains only at a time cannot benefit from data of other domains, which prevents the extraction of more useful features and hinders the progress of this research area. In this work, we propose a general framework for unsupervised image-to-image translation across multiple domains, which can translate images from domain X to any a domain without requiring direct training between the two domains involved in image translation. A byproduct of the framework is the reduction of computing time and computing resources since it needs less time than training the domains in pairs as is done in state-of-the-art works. Our proposed framework consists of a pair of encoders along with a pair of GANs which learns high-level features across different domains to generate diverse and realistic samples from. Our framework shows competing results on many image-to-image tasks compared with state-of-the-art techniques.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {374–382},
numpages = {9},
keywords = {neural networks, unsupervised learning, gan, image-to-image translation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240536,
author = {Zhao, Bo and Wu, Xiao and Cheng, Zhi-Qi and Liu, Hao and Jie, Zequn and Feng, Jiashi},
title = {Multi-View Image Generation from a Single-View},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240536},
doi = {10.1145/3240508.3240536},
abstract = {How to generate multi-view images with realistic-looking appearance from only a single view input is a challenging problem. In this paper, we attack this problem by proposing a novel image generation model termed VariGANs, which combines the merits of the variational inference and the Generative Adversarial Networks (GANs). It generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produces coarse images of different views. Conditioned on the generated coarse images, it then performs adversarial learning to fill details consistent with the input and generate the fine images. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that the generated images with the proposed VariGANs are more plausible than those generated by existing approaches, which provide more consistent global appearance as well as richer and sharper details.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {383–391},
numpages = {9},
keywords = {generative adversarial networks, image generation, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240594,
author = {Zhang, Jichao and Shu, Yezhi and Xu, Songhua and Cao, Gongze and Zhong, Fan and Liu, Meng and Qin, Xueying},
title = {Sparsely Grouped Multi-Task Generative Adversarial Networks for Facial Attribute Manipulation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240594},
doi = {10.1145/3240508.3240594},
abstract = {Recently, Image-to-Image Translation (IIT) has achieved great progress in image style transfer and semantic context manipulation for images. However, existing approaches require exhaustively labelling training data, which is labor demanding, difficult to scale up, and hard to adapt to a new domain. To overcome such a key limitation, we propose Sparsely Grouped Generative Adversarial Networks (SG-GAN) as a novel approach that can translate images in sparsely grouped datasets where only a few train samples are labelled. Using a one-input multi-output architecture, SG-GAN is well-suited for tackling multi-task learning and sparsely grouped learning tasks. The new model is able to translate images among multiple groups using only a single trained model. To experimentally validate the advantages of the new model, we apply the proposed method to tackle a series of attribute manipulation tasks for facial images as a case study. Experimental results show that SG-GAN can achieve comparable results with state-of-the-art methods on adequately labelled datasets while attaining a superior image translation quality on sparsely grouped datasets~footnoteCode is available at https://github.com/zhangqianhui/SGGAN-tensorflow..},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {392–401},
numpages = {10},
keywords = {deep learning, generative adversarial networks, image translation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286920,
author = {Song, Jingkuan},
title = {Session Details: Vision-1 (Machine Learning)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286920},
doi = {10.1145/3286920},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240512,
author = {Wang, Jindong and Feng, Wenjie and Chen, Yiqiang and Yu, Han and Huang, Meiyu and Yu, Philip S.},
title = {Visual Domain Adaptation with Manifold Embedded Distribution Alignment},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240512},
doi = {10.1145/3240508.3240512},
abstract = {Visual domain adaptation aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Existing methods either attempt to align the cross-domain distributions, or perform manifold subspace learning. However, there are two significant challenges: (1) degenerated feature transformation, which means that distribution alignment is often performed in the original feature space, where feature distortions are hard to overcome. On the other hand, subspace learning is not sufficient to reduce the distribution divergence. (2) unevaluated distribution alignment, which means that existing distribution alignment methods only align the marginal and conditional distributions with equal importance, while they fail to evaluate the different importance of these two distributions in real applications. In this paper, we propose a Manifold Embedded Distribution Alignment (MEDA) approach to address these challenges. MEDA learns a domain-invariant classifier in Grassmann manifold with structural risk minimization, while performing dynamic distribution alignment to quantitatively account for the relative importance of marginal and conditional distributions. To the best of our knowledge, MEDA is the first attempt to perform dynamic distribution alignment for manifold domain adaptation. Extensive experiments demonstrate that MEDA shows significant improvements in classification accuracy compared to state-of-the-art traditional and deep methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {402–410},
numpages = {9},
keywords = {distribution alignment, domain adaptation, subspace learning, transfer learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240577,
author = {Shen, Zheyan and Cui, Peng and Kuang, Kun and Li, Bo and Chen, Peixuan},
title = {Causally Regularized Learning with Agnostic Data Selection Bias},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240577},
doi = {10.1145/3240508.3240577},
abstract = {Most of previous machine learning algorithms are proposed based on the i.i.d. hypothesis. However, this ideal assumption is often violated in real applications, where selection bias may arise between training and testing process. Moreover, in many scenarios, the testing data is not even available during the training process, which makes the traditional methods like transfer learning infeasible due to their need on prior of test distribution. Therefore, how to address the agnostic selection bias for robust model learning is of paramount importance for both academic research and real applications. In this paper, under the assumption that causal relationships among variables are robust across domains, we incorporate causal technique into predictive modeling and propose a novel Causally Regularized Logistic Regression (CRLR) algorithm by jointly optimize global confounder balancing and weighted logistic regression. Global confounder balancing helps to identify causal features, whose causal effect on outcome are stable across domains, then performing logistic regression on those causal features constructs a robust predictive model against the agnostic bias. To validate the effectiveness of our CRLR algorithm, we conduct comprehensive experiments on both synthetic and real world datasets. Experimental results clearly demonstrate that our CRLR algorithm outperforms the state-of-the-art methods, and the interpretability of our method can be fully depicted by the feature visualization.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {411–419},
numpages = {9},
keywords = {data selection bias, causal inference, causal regularizer},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240709,
author = {Liang, Yanjie and Wu, Qiangqiang and Liu, Yi and Yan, Yan and Wang, Hanzi},
title = {Robust Correlation Filter Tracking with Shepherded Instance-Aware Proposals},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240709},
doi = {10.1145/3240508.3240709},
abstract = {In recent years, convolutional neural network (CNN) based correlation filter trackers have achieved state-of-the-art results on the benchmark datasets. However, the CNN based correlation filters cannot effectively handle large scale variation and distortion (such as fast motion, background clutter, occlusion, etc.), leading to the sub-optimal performance. In this paper, we propose a novel CNN based correlation filter tracker with shepherded instance-aware proposals, namely DeepCFIAP, which automatically estimates the target scale in each frame and re-detects the target when distortion happens. DeepCFIAP is proposed to take advantage of the merits of both instance-aware proposals and CNN based correlation filters. Compared with the CNN based correlation filter trackers, DeepCFIAP can successfully solve the problems of large scale variation and distortion via the shepherded instance-aware proposals, resulting in more robust tracking performance. Specifically, we develop a novel proposal ranking algorithm based on the similarities between proposals and instances. In contrast to the detection proposal based trackers, DeepCFIAP shepherds the instance-aware proposals towards their optimal positions via the CNN based correlation filters, resulting in more accurate tracking results. Extensive experiments on two challenging benchmark datasets demonstrate that the proposed DeepCFIAP performs favorably against state-of-the-art trackers and it is especially feasible for long-term tracking.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {420–428},
numpages = {9},
keywords = {shepherded instance-aware proposals, correlation filter, visual tracking},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240633,
author = {Qi, Fan and Yang, Xiaoshan and Xu, Changsheng},
title = {A Unified Framework for Multimodal Domain Adaptation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240633},
doi = {10.1145/3240508.3240633},
abstract = {Domain adaptation aims to train a model on labeled data from a source domain while minimizing test error on a target domain. Most of existing domain adaptation methods only focus on reducing domain shift of single-modal data. In this paper, we consider a new problem of multimodal domain adaptation and propose a unified framework to solve it. The proposed multimodal domain adaptation neural networks(MDANN) consist of three important modules. (1) A covariant multimodal attention is designed to learn a common feature representation for multiple modalities. (2) A fusion module adaptively fuses attended features of different modalities. (3) Hybrid domain constraints are proposed to comprehensively learn domain-invariant features by constraining single modal features, fused features, and attention scores. Through jointly attending and fusing under an adversarial objective, the most discriminative and domain-adaptive parts of the features are adaptively fused together. Extensive experimental results on two real-world cross-domain applications (emotion recognition and cross-media retrieval) demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {429–437},
numpages = {9},
keywords = {multimodal analysis, domain adaptation, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286921,
author = {Liao, Mark},
title = {Session Details: Multimedia-1 (Multimedia Recommendation &amp; Discovery)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286921},
doi = {10.1145/3286921},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240546,
author = {Hidayati, Shintami Chusnul and Hsu, Cheng-Chun and Chang, Yu-Ting and Hua, Kai-Lung and Fu, Jianlong and Cheng, Wen-Huang},
title = {What Dress Fits Me Best? Fashion Recommendation on the Clothing Style for Personal Body Shape},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240546},
doi = {10.1145/3240508.3240546},
abstract = {Clothing is an integral part of life. Also, it is always an uneasy task for people to make decisions on what to wear. An essential style tip is to dress for the body shape, i.e., knowing one's own body shape (e.g., hourglass, rectangle, round and inverted triangle) and selecting the types of clothes that will accentuate the body's good features. In the literature, although various fashion recommendation systems for clothing items have been developed, none of them had explicitly taken the user's basic body shape into consideration. In this paper, therefore, we proposed a first framework for learning the compatibility of clothing styles and body shapes from social big data, with the goal to recommend a user about what to wear better in relation to his/her essential body attributes. The experimental results demonstrate the superiority of our proposed approach, leading to a new aspect for research into fashion recommendation.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {438–446},
numpages = {9},
keywords = {correlation, clothing style, recommender system, human body shape, fashion analysis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240609,
author = {Huang, Xiaowen and Qian, Shengsheng and Fang, Quan and Sang, Jitao and Xu, Changsheng},
title = {CSAN: Contextual Self-Attention Network for User Sequential Recommendation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240609},
doi = {10.1145/3240508.3240609},
abstract = {The sequential recommendation is an important task for online user-oriented services, such as purchasing products, watching videos, and social media consumption. Recent work usually used RNN-based methods to derive an overall embedding of the whole behavior sequence, which fails to discriminate the significance of individual user behaviors and thus decreases the recommendation performance. Besides, RNN-based encoding has fixed size and makes further recommendation application inefficient and inflexible. The online sequential behaviors of a user are generally heterogeneous, polysemous, and dynamically context-dependent. In this paper, we propose a unified Contextual Self-Attention Network (CSAN) to address the three properties. Heterogeneous user behaviors are considered in our model that are projected into a common latent semantic space. Then the output is fed into the feature-wise self-attention network to capture the polysemy of user behaviors. In addition, the forward and backward position encoding matrices are proposed to model dynamic contextual dependency. Through extensive experiments on two real-world datasets, we demonstrate the superior performance of the proposed model compared with other state-of-the-art algorithms.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {447–455},
numpages = {9},
keywords = {multi-modal, sequential recommendation, contextual, self-attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240626,
author = {Hu, Jun and Qian, Shengsheng and Fang, Quan and Xu, Changsheng},
title = {Attentive Interactive Convolutional Matching for Community Question Answering in Social Multimedia},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240626},
doi = {10.1145/3240508.3240626},
abstract = {Nowadays, community-based question answering (CQA) services have accumulated millions of users to share valuable knowledge. An essential function in CQA tasks is the accurate matching of answers w.r.t given questions. Existing methods usually ignore the redundant, heterogeneous, and multi-modal properties of CQA systems. In this paper, we propose a multi-modal attentive interactive convolutional matching method (MMAICM) to model the multi-modal content and social context jointly for questions and answers in a unified framework for CQA retrieval, which explores the redundant, heterogeneous, and multi-modal properties of CQA systems jointly. A well-designed attention mechanism is proposed to focus on useful word-pair interactions and neglect meaningless and noisy word-pair interactions. Moreover, a multi-modal interaction matrix method and a novel meta-path based network representation approach are proposed to consider the multi-modal content and social context, respectively. The attentive interactive convolutional matching network is proposed to infer the relevance between questions and answers, which can capture both the lexical and the sequential information of the contents. Experiment results on two real-world datasets demonstrate the superior performance of MMAICM compared with other state-of-the-art algorithms.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {456–464},
numpages = {9},
keywords = {multi-modal, attention, social multimedia, question-answering},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240689,
author = {Gelli, Francesco and Uricchio, Tiberio and He, Xiangnan and Del Bimbo, Alberto and Chua, Tat-Seng},
title = {Beyond the Product: Discovering Image Posts for Brands in Social Media},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240689},
doi = {10.1145/3240508.3240689},
abstract = {Brands and organizations are using social networks such as Instagram to share image or video posts regularly, in order to engage and maximize their presence to the users. Differently from the traditional advertising paradigm, these posts feature not only specific products, but also the value and philosophy of the brand, known as brand associations in marketing literature. In fact, marketers are spending considerable resources to generate their content in-house, and increasingly often, to discover and repost the content generated by users. However, to choose the right posts for a brand in social media remains an open problem. Driven by this real-life application, we define the new task of content discovery for brands, which aims to discover posts that match the marketing value and brand associations of a target brand. We identify two main challenges in this new task: high inter-brand similarity and brand-post sparsity; and propose a tailored content-based learning-to-rank system to discover content for a target brand. Specifically, our method learns fine-grained brand representation via explicit modeling of brand associations, which can be interpreted as visual words shared among brands. We collected a new large-scale Instagram dataset, consisting of more than 1.1 million image and video posts from the history of 927 brands of fourteen verticals such as food and fashion. Extensive experiments indicate that our model can effectively learn fine-grained brand representations and outperform the closest state-of-the-art solutions.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {465–473},
numpages = {9},
keywords = {image ranking, computational marketing, content discovery},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286922,
author = {Zha, Zheng-Jun},
title = {Session Details: Vision-2 (Object &amp; Scene Understanding)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286922},
doi = {10.1145/3286922},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240540,
author = {Zhang, Lishi and Fu, Chenghan and Li, Jia},
title = {Collaborative Annotation of Semantic Objects in Images with Multi-Granularity Supervisions},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240540},
doi = {10.1145/3240508.3240540},
abstract = {Per-pixel masks of semantic objects are very useful in many applications, which, however, are tedious to be annotated. In this paper, we propose a collaborative annotation approach to efficiently generate per-pixel masks of semantic objects in tagged images with multi-granularity supervisions. Given a set of tagged images, a computer agent is dynamically generated to roughly localize the semantic objects described by the tag. The agent first extracts massive object proposals and then infer the tag-related ones under the weak and strong supervisions from linguistically and visually similar images as well as previously annotated objects. By representing such supervisions by over-complete dictionaries, tag-related proposals can pop-out according to their sparse coding length, which are then converted to superpixels with binary labels. After that, human annotators participate in the annotation by flipping labels and dividing superpixels with clicks, which are used as click supervisions that teaches the agent to recover false positives/negatives in processing images with the same tags. Experimental results show that our approach can facilitate the annotation and generate object masks that are consistent with those generated by the LabelMe toolbox.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {474–482},
numpages = {9},
keywords = {object annotation, sparse coding, human-agent collaboration},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240542,
author = {Pu, Mengyang and Huang, Yaping and Guan, Qingji and Zou, Qi},
title = {GraphNet: Learning Image Pseudo Annotations for Weakly-Supervised Semantic Segmentation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240542},
doi = {10.1145/3240508.3240542},
abstract = {Weakly-supervised semantic image segmentation suffers from lacking accurate pixel-level annotations. In this paper, we propose a novel graph convolutional network-based method, called GraphNet, to learn pixel-wise labels from weak annotations. Firstly, we construct a graph on the superpixels of a training image by combining the low-level spatial relation and high-level semantic content. Meanwhile, scribble or bounding box annotations are embedded into the graph, respectively. Then, GraphNet takes the graph as input and learns to predict high-confidence pseudo image masks by a convolutional network operating directly on graphs. At last, a segmentation network is trained supervised by these pseudo image masks. We comprehensively conduct experiments on the PASCAL VOC 2012 and PASCAL-CONTEXT segmentation benchmarks. Experimental results demonstrate that GraphNet is effective to predict the pixel labels with scribble or bounding box annotations. The proposed framework yields state-of-the-art results in the community.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {483–491},
numpages = {9},
keywords = {weakly supervised learning, semantic segmentation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240657,
author = {Shi, Hengcan and Li, Hongliang and Wu, Qingbo and Meng, Fanman and Ngan, King N.},
title = {Boosting Scene Parsing Performance via Reliable Scale Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240657},
doi = {10.1145/3240508.3240657},
abstract = {Segmenting objects on suitable scales is a key factor to improve the scene parsing performance. Existing methods either simply average multi-scale results or predict scales by weakly-supervised models, due to the lack of scale labels. In this paper, we propose a novel fully-supervised Scale Prediction Model. On one hand, the proposed Scale Prediction Model learns parsing scales by the strong scale supervision, which is automatically generated from the scene parsing ground truth without any extra manually annotation. On the other hand, we explore the relationship between scale and object class, and propose to use the object class information to further improve the reliability of the scale prediction. The proposed Scale Prediction Model improves 23.1%, 20.1% and 29.3% scale prediction accuracies on the NYU Depth v2, PASCAL-Context and SIFT Flow datasets, respectively. Based on the Scale Prediction Model, we design a Scale Parsing Net (SPNet) for scene parsing, which segments each object on the scale predicted by the Scale Prediction Model. Moreover, SPNet leverages the intermediate result (i.e., the object class) to refine the parsing results. The experiment results show that SPNet outperforms many state-of-the-art methods on multiple scene parsing datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {492–500},
numpages = {9},
keywords = {scene parsing, deep learning, scale prediction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240700,
author = {Zhu, Fan and Liu, Li and Xie, Jin and Shen, Fumin and Shao, Ling and Fang, Yi},
title = {Learning to Synthesize 3D Indoor Scenes from Monocular Images},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240700},
doi = {10.1145/3240508.3240700},
abstract = {Depth images have always been playing critical roles for indoor scene understanding problems, and are particularly important for tasks in which 3D inferences are involved. However, since depth images are not universally available, abandoning them from the testing stage can significantly improve the generality of a method. In this work, we consider the scenarios where depth images are not available in the testing data, and propose to learn a convolutional long short-term memory (Conv LSTM) network and a regression convolutional neural network (regression ConvNet) using only monocular RGB images. The proposed networks benefit from 2D segmentations, object-level spatial context, object-scene dependencies and objects' geometric information, where optimization is governed by the semantic label loss, which measures the label consistencies of both objects and scenes, and the 3D geometrical loss, which measures the correctness of objects' 6Dof estimation. Conv LSTM and regression ConvNet are applied to scene/object classification, object detection and 6Dof estimation tasks respectively, where we utilize the joint inference from both networks and further provide the perspective of synthesizing fully rigged 3D scenes according to objects' arrangements in monocular images. Both quantitative and qualitative experimental results are provided on the NYU-v2 dataset, and we demonstrate that the proposed Conv LSTM can achieve state-of-the-art performance without requiring the depth information.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {501–509},
numpages = {9},
keywords = {object detection, cnn, indoor scene understanding, scene classification, lstm},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286923,
author = {Hua, Xian-Sheng},
title = {Session Details: Multimodal-1 (Multimodal Reasoning)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286923},
doi = {10.1145/3286923},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240611,
author = {Han, Chaojun and Shen, Fumin and Liu, Li and Yang, Yang and Shen, Heng Tao},
title = {Visual Spatial Attention Network for Relationship Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240611},
doi = {10.1145/3240508.3240611},
abstract = {Visual relationship detection, which aims to predict a <subject, predicate, object> triplet with the detected objects, has attracted increasing attention in the scene understanding study. During tackling this problem, dealing with varying scales of the subjects and objects is of great importance, which has been less studied. To overcome this challenge, we propose a novel Vision Spatial Attention Network (VSA-Net), which employs a two-dimensional normal distribution attention scheme to effectively model small objects. In addition, we design a Subject-Object-layer (SO-layer) to distinguish between the subject and object to attain more precise results. To the best of our knowledge, VSA-Net is the first end-to-end attention mechanism based visual relationship detection model. Extensive experiments on the benchmark datasets (VRD and VG) show that, by using pure vision information, our VSA-Net achieves state-of-the-art performance for predicate detection, phrase detection, and relationship detection.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {510–518},
numpages = {9},
keywords = {object detection, relationship detection, attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240513,
author = {Wu, Chenfei and Liu, Jinlai and Wang, Xiaojie and Dong, Xuan},
title = {Object-Difference Attention: A Simple Relational Attention for Visual Question Answering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240513},
doi = {10.1145/3240508.3240513},
abstract = {Attention mechanism has greatly promoted the development of Visual Question Answering (VQA). Attention distribution, which weights differently on objects (such as image regions or bounding boxes) in an image according to their importance for answering a question, plays a crucial role in attention mechanism. Most of the existing work focuses on fusing image features and text features to calculate the attention distribution without comparisons between different image objects. As a major property of attention, selectivity depends on comparisons between different objects. Comparisons provide more information for assigning attentions better. For achieving this, we propose an object-difference attention (ODA) which calculates the probability of attention by implementing difference operator between different image objects in an image under the guidance of questions in hand. Experimental results on three publicly available datasets show our ODA based VQA model achieves the state-of-the-art results. Furthermore, a general form of relational attention is proposed. Besides ODA, several other relational attentions are given. Experimental results show those relational attentions have strengths on different types of questions.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {519–527},
numpages = {9},
keywords = {attention, object-difference attention, visual question answering},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240558,
author = {Qi, Jinwei and Peng, Yuxin and Zhuo, Yunkan},
title = {Life-Long Cross-Media Correlation Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240558},
doi = {10.1145/3240508.3240558},
abstract = {With the numerous and dynamically increasing of multimedia data, such as image and text, lying in different domains, there arise two major challenges for cross-media retrieval. First, measuring the similarities for cross-media correlation between different media types is quite difficult, due to their inconsistent distributions and representations. Second, storing and retraining on such data becomes infeasible, because data of new domain arrives in sequence while the existing ones are not always available. Thus, it requires to only utilize the data of new domain for training while preserving the original correlation capabilities simultaneously. To address the above issues, in this paper we propose Cross-media Life-long Learning (CmLL) approach, which can leverage the knowledge learned from the existing data, to obtain better correlation performance in new domain. The main contributions are summarized as follows: (1) Cross-media adapting network. We construct hierarchical network to not only share the knowledge from different media types in high level, but also realize life-long learning on new cross-media domain by expanding network capacity adaptively, which can support the adaptivity and extensibility for cross-media correlation learning. (2) Cross-media life-long learning. We propose both intra-domain distribution alignment as well as inter-domain knowledge distillation, which can not only effectively preserve the correlation ability in old cross-media domains, but also improve the performance in new domain by transferring knowledge among different domains. We conduct extensive experiments to verify the effectiveness of our proposed CmLL approach, which are performed on multiple cross-media datasets for different domains under lifelong learning scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {528–536},
numpages = {9},
keywords = {cross-media retrieval, cross-media correlation learning, life-long learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240714,
author = {Gu, Yue and Li, Xinyu and Huang, Kaixiang and Fu, Shiyu and Yang, Kangning and Chen, Shuhong and Zhou, Moliang and Marsic, Ivan},
title = {Human Conversation Analysis Using Attentive Multimodal Networks with Hierarchical Encoder-Decoder},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240714},
doi = {10.1145/3240508.3240714},
abstract = {Human conversation analysis is challenging because the meaning can be expressed through words, intonation, or even body language and facial expression. We introduce a hierarchical encoder-decoder structure with attention mechanism for conversation analysis. The hierarchical encoder learns word-level features from video, audio, and text data that are then formulated into conversation-level features. The corresponding hierarchical decoder is able to predict different attributes at given time instances. To integrate multiple sensory inputs, we introduce a novel fusion strategy with modality attention. We evaluated our system on published emotion recognition, sentiment analysis, and speaker trait analysis datasets. Our system outperformed previous state-of-the-art approaches in both classification and regressions tasks on three datasets. We also outperformed previous approaches in generalization tests on two commonly used datasets. We achieved comparable performance in predicting co-existing labels using the proposed model instead of multiple individual models. In addition, the easily-visualized modality and temporal attention demonstrated that the proposed attention mechanism helps feature selection and improves model interpretability.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {537–545},
numpages = {9},
keywords = {attention mechanism., hierarchical encoder-decoder structure, human conversation analysis, sensor fusion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286924,
author = {Yang, Xin},
title = {Session Details: System-1 (Video Analysis &amp; Streaming)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286924},
doi = {10.1145/3286924},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240643,
author = {Liu, Wentao and Duanmu, Zhengfang and Wang, Zhou},
title = {End-to-End Blind Quality Assessment of Compressed Videos Using Deep Neural Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240643},
doi = {10.1145/3240508.3240643},
abstract = {Blind video quality assessment (BVQA) algorithms are traditionally designed with a two-stage approach - a feature extraction stage that computes typically hand-crafted spatial and/or temporal features, and a regression stage working in the feature space that predicts the perceptual quality of the video. Unlike the traditional BVQA methods, we propose a Video Multi-task End-to-end Optimized neural Network (V-MEON) that merges the two stages into one, where the feature extractor and the regressor are jointly optimized. Our model uses a multi-task DNN framework that not only estimates the perceptual quality of the test video but also provides a probabilistic prediction of its codec type. This framework allows us to train the network with two complementary sets of labels, both of which can be obtained at low cost. The training process is composed of two steps. In the first step, early convolutional layers are pre-trained to extract spatiotemporal quality-related features with the codec classification subtask. In the second step, initialized with the pre-trained feature extractor, the whole network is jointly optimized with the two subtasks together. An additional critical step is the adoption of 3D convolutional layers, which creates novel spatiotemporal features that lead to a significant performance boost. Experimental results show that the proposed model clearly outperforms state-of-the-art BVQA methods.The source code of V-MEON is available at https://ece.uwaterloo.ca/~zduanmu/acmmm2018bvqa.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {546–554},
numpages = {9},
keywords = {blind video quality assessment, multi-task learning, convolutional neural network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240676,
author = {Ben Mustafa, Ibrahim and Nadeem, Tamer and Halepovic, Emir},
title = {FlexStream: Towards Flexible Adaptive Video Streaming on End Devices Using Extreme SDN},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240676},
doi = {10.1145/3240508.3240676},
abstract = {We present FlexStream, a programmable framework realized by implementing Software-Defined Networking (SDN) functionality on end devices. FlexStream exploits the benefits of both centralized and distributed components to achieve dynamic management of end devices, as required and in accordance with specified policies. We evaluate FlexStream on one example use case -- the adaptive video streaming, where bandwidth control is employed to drive selection of video bitrates, improve stability and increase robustness against background traffic. When applied to competing streaming clients, FlexStream reduces bitrate switching by 81%, stall duration by 92%, and startup delay by 44%, while improving fairness among players. In addition, we report the first implementation of SDN-based control in Android devices running in real Wi-Fi and live cellular networks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {555–563},
numpages = {9},
keywords = {abr, sdn, dash, video streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240556,
author = {Xie, Lan and Zhang, Xinggong and Guo, Zongming},
title = {CLS: A Cross-User Learning Based System for Improving QoE in 360-Degree Video Adaptive Streaming},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240556},
doi = {10.1145/3240508.3240556},
abstract = {Viewport adaptive streaming is emerging as a promising way to deliver high quality 360-degree video. It is still a critical issue to predict user's viewpoint and deliver partial video within the viewport. Current widely-used motion-based or content-saliency methods have low precision, especially for long-term prediction. In this paper, benefiting from data-driven learning, we propose a Cross-user Learning based System (CLS) to improve the precision of viewport prediction. Since users have similar region-of-interest (ROI) when watching a same video, it is possible to exploit cross-users' ROI behavior to predict viewport. We use a machine learning algorithm to group users according to historical fixations, and predict the viewing probability by the class. Additionally, we present a QoE-driven rate allocation to minimize the expected streaming distortion under bandwidth constraint, and give a Multiple-Choice Knapsack solution. Experiments demonstrate that CLS provides 2dB quality improvement than full-image streaming and 1.5 dB quality improvement than linear regression (LR) method. On average, the precision of viewpoint prediction improve 15% compared with LR.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {564–572},
numpages = {9},
keywords = {tile-based adaptive streaming, viewport prediction, 360-degree video, qoe-driven rate allocation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240589,
author = {Bentaleb, Abdelhak and Begen, Ali C. and Harous, Saad and Zimmermann, Roger},
title = {A Distributed Approach for Bitrate Selection in HTTP Adaptive Streaming},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240589},
doi = {10.1145/3240508.3240589},
abstract = {Past research has shown that concurrent HTTP adaptive streaming (HAS) players behave selfishly and the resulting competition for shared resources leads to underutilization or oversubscription of the network, presentation quality instability and unfairness among the players, all of which adversely impact the viewer experience. While coordination among the players, as opposed to all being selfish, has its merits and may alleviate some of these issues. A fully distributed architecture is still desirable in many deployments and better reflects the design spirit of HAS. In this study, we focus on and propose a distributed bitrate adaptation scheme for HAS that borrows ideas from consensus and game theory frameworks. Experimental results show that the proposed distributed approach provides significant improvements in terms of viewer experience, presentation quality stability, fairness and network utilization, without using any explicit communication between the players.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {573–581},
numpages = {9},
keywords = {distributed algorithms, qoe, bitrate selection, abr, resource allocation, has, game theory, consensus},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286925,
author = {Li, Zhu},
title = {Session Details: FF-3},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286925},
doi = {10.1145/3286925},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240595,
author = {Zhang, Qing and Yuan, Ganzhao and Xiao, Chunxia and Zhu, Lei and Zheng, Wei-Shi},
title = {High-Quality Exposure Correction of Underexposed Photos},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240595},
doi = {10.1145/3240508.3240595},
abstract = {We address the problem of correcting the exposure of underexposed photos. Previous methods have tackled this problem from many different perspectives and achieved remarkable progress. However, they usually fail to produce natural-looking results due to the existence of visual artifacts such as color distortion, loss of detail, exposure inconsistency, etc. We find that the main reason why existing methods induce these artifacts is because they break a perceptually similarity between the input and output. Based on this observation, an effective criterion, termed as perceptually bidirectional similarity (PBS) is proposed. Based on this criterion and the Retinex theory, we cast the exposure correction problem as an illumination estimation optimization, where PBS is defined as three constraints for estimating illumination that can generate the desired result with even exposure, vivid color and clear textures. Qualitative and quantitative comparisons, and the user study demonstrate the superiority of our method over the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {582–590},
numpages = {9},
keywords = {perceptual similarity, illumination estimation, exposure correction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240597,
author = {Xu, Qianqian and Xiong, Jiechao and Sun, Xinwei and Yang, Zhiyong and Cao, Xiaochun and Huang, Qingming and Yao, Yuan},
title = {A Margin-Based MLE for Crowdsourced Partial Ranking},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240597},
doi = {10.1145/3240508.3240597},
abstract = {A preference order or ranking aggregated from pairwise comparison data is commonly understood as a strict total order. However, in real-world scenarios, some items are intrinsically ambiguous in comparisons, which may very well be an inherent uncertainty of the data. In this case, the conventional total order ranking can not capture such uncertainty with mere global ranking or utility scores. In this paper, we are specifically interested in the recent surge in crowdsourcing applications to predict partial but more accurate (i.e., making less incorrect statements) orders rather than complete ones. To do so, we propose a novel framework to learn some probabilistic models of partial orders as a margin-based Maximum Likelihood Estimate (MLE) method. We prove that the induced MLE is a joint convex optimization problem with respect to all the parameters, including the global ranking scores and margin parameter. Moreover, three kinds of generalized linear models are studied, including the basic uniform model, Bradley-Terry model, and Thurstone-Mosteller model, equipped with some theoretical analysis on FDR and Power control for the proposed methods. The validity of these models are supported by experiments with both simulated and real-world datasets, which shows that the proposed models exhibit improvements compared with traditional state-of-the-art algorithms.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {591–599},
numpages = {9},
keywords = {partial ranking, marginbased mle, pairwise comparison, crowdsourcing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240599,
author = {Garcia del Molino, Ana and Gygli, Michael},
title = {PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240599},
doi = {10.1145/3240508.3240599},
abstract = {Highlight detection models are typically trained to identify cues that make visual content appealing or interesting for the general public, with the objective of reducing a video to such moments. However, this "interestingness" of a video segment or image is subjective. Thus, such highlight models provide results of limited relevance for the individual user. On the other hand, training one model per user is inefficient and requires large amounts of personal information which is typically not available. To overcome these limitations, we present a global ranking model which can condition on a particular user's interests. Rather than training one model per user, our model is personalized via its inputs, which allows it to effectively adapt its predictions, given only a few user-specific examples. To train this model, we create a large-scale dataset of users and the GIFs they created, giving us an accurate indication of their interests. Our experiments show that using the user history substantially improves the prediction accuracy. On a test set of 850 videos, our model improves the recall by 8% with respect to generic highlight detectors. Furthermore, our method proves more precise than the user-agnostic baselines even with only one single person-specific example.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {600–608},
numpages = {9},
keywords = {highlight detection, personalization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240606,
author = {Pang, Lu and Wang, Yaowei and Song, Yi-Zhe and Huang, Tiejun and Tian, Yonghong},
title = {Cross-Domain Adversarial Feature Learning for Sketch Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240606},
doi = {10.1145/3240508.3240606},
abstract = {Under person re-identification (Re-ID), a query photo of the target person is often required for retrieval. However, one is not always guaranteed to have such a photo readily available under a practical forensic setting. In this paper, we define the problem of Sketch Re-ID, which instead of using a photo as input, it initiates the query process using a professional sketch of the target person. This is akin to the traditional problem of forensic facial sketch recognition, yet with the major difference that our sketches are whole-body other than just the face. This problem is challenging because sketches and photos are in two distinct domains. Specifically, a sketch is the abstract description of a person. Besides, person appearance in photos is variational due to camera viewpoint, human pose and occlusion. We address the Sketch Re-ID problem by proposing a cross-domain adversarial feature learning approach to jointly learn the identity features and domain-invariant features. We employ adversarial feature learning to filter low-level interfering features and remain high-level semantic information. We also contribute to the community the first Sketch Re-ID dataset with 200 persons, where each person has one sketch and two photos from different cameras associated. Extensive experiments have been performed on the proposed dataset and other common sketch datasets including CUFSF and QUML-shoe. Results show that the proposed method outperforms the state-of-the-arts.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {609–617},
numpages = {9},
keywords = {domain-invariant features, adversarial feature learning, sketch re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240610,
author = {Chen, Quan and Ge, Tiezheng and Xu, Yanyu and Zhang, Zhiqiang and Yang, Xinxin and Gai, Kun},
title = {Semantic Human Matting},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240610},
doi = {10.1145/3240508.3240610},
abstract = {Human matting, high quality extraction of humans from natural images, is crucial for a wide variety of applications. Since the matting problem is severely under-constrained, most previous methods require user interactions to take user designated trimaps or scribbles as constraints. This user-in-the-loop nature makes them difficult to be applied to large scale data or time-sensitive scenarios. In this paper, instead of using explicit user input constraints, we employ implicit semantic constraints learned from data and propose an automatic human matting algorithm Semantic Human Matting(SHM). SHM is the first algorithm that learns to jointly fit both semantic information and high quality details with deep networks. In practice, simultaneously learning both coarse semantics and fine details is challenging. We propose a novel fusion strategy which naturally gives a probabilistic estimation of the alpha matte. We also construct a very large dataset with high quality annotations consisting of 35,513 unique foregrounds to facilitate the learning and evaluation of human matting. Extensive experiments on this dataset and plenty of real images show that SHM achieves comparable results with state-of-the-art interactive matting methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {618–626},
numpages = {9},
keywords = {matting, human matting, semantic segmentation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240612,
author = {Song, Lingxiao and Lu, Zhihe and He, Ran and Sun, Zhenan and Tan, Tieniu},
title = {Geometry Guided Adversarial Facial Expression Synthesis},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240612},
doi = {10.1145/3240508.3240612},
abstract = {Facial expression synthesis has drawn much attention in the field of computer graphics and pattern recognition. It has been widely used in face animation and recognition. However, it is still challenging due to the high-level semantic presence of large and non-linear face geometry variations. This paper proposes a Geometry-Guided Generative Adversarial Network (G2-GAN) for continuously-adjusting and identity-preserving facial expression synthesis. We employ facial geometry (fiducial points) as a controllable condition to guide facial texture synthesis with specific expression. A pair of generative adversarial subnetworks is jointly trained towards opposite tasks: expression removal and expression synthesis. The paired networks form a mapping cycle between neutral expression and arbitrary expressions, with which the proposed approach can be conducted among unpaired data. The proposed paired networks also facilitate other applications such as face transfer, expression interpolation and expression-invariant face recognition. Experimental results on several facial expression databases show that our method can generate compelling perceptual results on different expression editing tasks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {627–635},
numpages = {9},
keywords = {generative adversarial networks, unpaired image-to-image transformation, facial expression synthesis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240615,
author = {Wang, Siqi and Zeng, Yijie and Liu, Qiang and Zhu, Chengzhang and Zhu, En and Yin, Jianping},
title = {Detecting Abnormality without Knowing Normality: A Two-Stage Approach for Unsupervised Video Abnormal Event Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240615},
doi = {10.1145/3240508.3240615},
abstract = {Abnormal event detection in video surveillance is a valuable but challenging problem. Most methods adopt a supervised setting that requires collecting videos with only normal events for training. However, very few attempts are made under unsupervised setting that detects abnormality without priorly knowing normal events. Existing unsupervised methods detect drastic local changes as abnormality, which overlooks the global spatio-temporal context. This paper proposes a novel unsupervised approach, which not only avoids manually specifying normality for training as supervised methods do, but also takes the whole spatio-temporal context into consideration. Our approach consists of two stages: First, normality estimation stage trains an autoencoder and estimates the normal events globally from the entire unlabeled videos by a self-adaptive reconstruction loss thresholding scheme. Second, normality modeling stage feeds the estimated normal events from the previous stage into one-class support vector machine to build a refined normality model, which can further exclude abnormal events and enhance abnormality detection performance. Experiments on various benchmark datasets reveal that our method is not only able to outperform existing unsupervised methods by a large margin (up to 14.2% AUC gain), but also favorably yields comparable or even superior performance to state-of-the-art supervised methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {636–644},
numpages = {9},
keywords = {video abnormal event detection, unsupervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240618,
author = {Li, Tingting and Qian, Ruihe and Dong, Chao and Liu, Si and Yan, Qiong and Zhu, Wenwu and Lin, Liang},
title = {BeautyGAN: Instance-Level Facial Makeup Transfer with Deep Generative Adversarial Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240618},
doi = {10.1145/3240508.3240618},
abstract = {Facial makeup transfer aims to translate the makeup style from a given reference makeup face image to another non-makeup one while preserving face identity. Such an instance-level transfer problem is more challenging than conventional domain-level transfer tasks, especially when paired data is unavailable. Makeup style is also different from global styles (e.g., paintings) in that it consists of several local styles/cosmetics, including eye shadow, lipstick, foundation, and so on. Extracting and transferring such local and delicate makeup information is infeasible for existing style transfer methods. We address the issue by incorporating both global domain-level loss and local instance-level loss in an dual input/output Generative Adversarial Network, called BeautyGAN. Specifically, the domain-level transfer is ensured by discriminators that distinguish generated images from domains' real samples. The instance-level loss is calculated by pixel-level histogram loss on separate local facial regions. We further introduce perceptual loss and cycle consistency loss to generate high quality faces and preserve identity. The overall objective function enables the network to learn translation on instance-level through unsupervised adversarial learning. We also build up a new makeup dataset that consists of 3834 high-resolution face images. Extensive experiments show that BeautyGAN could generate visually pleasant makeup faces and accurate transferring results. Data and code are available at http://liusi-group.com/projects/BeautyGAN.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {645–653},
numpages = {9},
keywords = {facial makeup transfer, generative adversarial network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240634,
author = {Luo, Xianghui and Su, Zhuo and Guo, Jiaming and Zhang, Gengwei and He, Xiangjian},
title = {Trusted Guidance Pyramid Network for Human Parsing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240634},
doi = {10.1145/3240508.3240634},
abstract = {Human parsing, which segments a human-centric image into pixel-wise categorization, has a wide range of applications. However, none of the existing methods can productively solve the issue of label parsing fragmentation due to confused and complicated annotations. In this paper, we propose a novel Trusted Guidance Pyramid Network (TGPNet) to address this limitation. Based on a pyramid architecture, we design a Pyramid Residual Pooling (PRP) module setting at the end of a bottom-up approach to capture both global and local level context. In the top-down approach, we propose a Trusted Guidance Multi-scale Supervision (TGMS) that efficiently integrates and supervises multi-scale contextual information. Furthermore, we present a simple yet powerful Trusted Guidance Framework (TGF) which imposes global-level semantics into parsing results directly without extra ground truth labels in model training. Extensive experiments on two public human parsing benchmarks well demonstrate that our TGPNet has a strong ability in solving label parsing fragmentation problem and has an obtained improvement than other methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {654–662},
numpages = {9},
keywords = {human parsing, semantic segmentation, pyramid network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240579,
author = {Li, Jingjing and Zhu, Lei and Huang, Zi and Lu, Ke and Zhao, Jidong},
title = {I Read, I Saw, I Tell: Texts Assisted Fine-Grained Visual Classification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240579},
doi = {10.1145/3240508.3240579},
abstract = {In visual classification tasks, it is hard to tell the subtle differences from one species to another similar breeds. Such a challenging problem is generally known as Fine-Grained Visual Classification (FGVC). In this paper, we propose a novel FGVC approach called Texts Assisted Fine-Grained Visual Classification (TA-FGVC). TA-FGVC reads from texts to gain attention, sees the images with the gained attention and then tells the subtle differences. Technically, we propose a deep neural network which learns a visual-semantic embedding model. The proposed deep architecture mainly consists of two parts: one for visual localization, and the other for visual to semantic projection. The model is fed with both visual features which are extracted from raw images and semantic information which are learned from two sources: gleaned from unannotated texts and gathered from image attributes. At the very last layer of the model, each image is embedded into the semantic space which is related to class labels. Finally, the categorization results from both visual stream and visual-semantic stream are combined to achieve the ultimate decision. Extensive experiments on open standard benchmarks verify the superiority of our model against several state of the art work.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {663–671},
numpages = {9},
keywords = {transfer learning, deep learning, fine-grained visual classification, multi-modal analysis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240583,
author = {Wang, Ziwei and Luo, Yadan and Li, Yang and Huang, Zi and Yin, Hongzhi},
title = {Look Deeper See Richer: Depth-Aware Image Paragraph Captioning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240583},
doi = {10.1145/3240508.3240583},
abstract = {With the widespread availability of image captioning at a sentence level, how to automatically generate image paragraphs is yet well explored. Describing an image by a full paragraph involves organising sentences orderly, coherently and diversely, inevitably leading higher complexity than by a single sentence. Existing image paragraph captioning methods give a series of sentences to represent the objects and regions of interests, where the descriptions are essentially generated by feeding the image fragments containing objects and regions into conventional image single-sentence captioning models. This strategy is difficult to generate the descriptions that guarantee the stereoscopic hierarchy and non-overlapping objects. In this paper, we propose a Depth-aware Attention Model (textitDAM ) to generate paragraph captions for images. The depths of image areas are firstly estimated in order to discriminate objects in a range of spatial locations, which can further guide the linguistic decoder to reveal spatial relationships among objects. This model completes the paragraph in a logical and coherent manner. By incorporating the attention mechanism, the learned model swiftly shifts the sentence focus during paragraph generation, whilst avoiding verbose descriptions on a same object. Extensive quantitative experiments and the user study have been conducted on the Visual Genome dataset, which demonstrate the effectiveness and the interpretability of the proposed model.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {672–680},
numpages = {9},
keywords = {paragraph captioning, depth estimation, attention mechanism},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240586,
author = {Zhang, Huaiwen and Fang, Quan and Qian, Shengsheng and Xu, Changsheng},
title = {Learning Multimodal Taxonomy via Variational Deep Graph Embedding and Clustering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240586},
doi = {10.1145/3240508.3240586},
abstract = {Taxonomy learning is an important problem and facilitates various applications such as semantic understanding and information retrieval. Previous work for building semantic taxonomies has primarily relied on labor-intensive human contributions or focused on text-based extraction. In this paper, we investigate the problem of automatically learning multimodal taxonomies from the multimedia data on the Web. A systematic framework called Variational Deep Graph Embedding and Clustering (VDGEC) is proposed consisting of two stages as concept graph construction and taxonomy induction via variational deep graph embedding and clustering. VDGEC discovers hierarchical concept relationships by exploiting the semantic textual-visual correspondences and contextual co-occurrences in an unsupervised manner. The unstructured semantics and noisy issues of multimedia documents are carefully addressed by VDGEC for high quality taxonomy induction. We conduct extensive experiments on the real-world datasets. Experimental results demonstrate the effectiveness of the proposed framework, where VDGEC outperforms previous unsupervised approaches by a large gap.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {681–689},
numpages = {9},
keywords = {variational deep graph embedding, taxonomy learning, multimodal taxonomy},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240566,
author = {Gao, Junyu and Zhang, Tianzhu and Xu, Changsheng},
title = {Watch, Think and Attend: End-to-End Video Classification via Dynamic Knowledge Evolution Modeling},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240566},
doi = {10.1145/3240508.3240566},
abstract = {Video classification has been achieved by automatically mining the underlying concepts (eg actions, events) in videos, which plays an essential role in intelligent video analysis. However, most existing algorithms only exploit the visual cues of these concepts but ignore external knowledge information for modeling their relationships during the evolution of videos. In fact, humans have remarkable ability to utilize acquired knowledge to reason about the dynamically changing world. To narrow the knowledge gap between existing methods and humans, we propose an end-to-end video classification framework based on a structured knowledge graph, which can model the dynamic knowledge evolution in videos overtime. Here, we map the concepts of videos to the nodes of the knowledge graph. To effectively leverage the knowledge graph, we adopt a graph convLSTM model to not only identify local knowledge structures in each video shot but also model dynamic patterns of knowledge evolution across these shots. Furthermore, a novel knowledge-based attention model is designed by considering the importance of each video shot and relationships between concepts. We show that by using knowledge graphs, our framework is able to improve the performance of various existing methods. Extensive experimental results on two video classification benchmarks UCF101 and Youtube-8M demonstrate the favorable performance of the proposed framework.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {690–699},
numpages = {10},
keywords = {graph neural networks, knowledge graph, video classification, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240567,
author = {Liu, Yongcheng and Sheng, Lu and Shao, Jing and Yan, Junjie and Xiang, Shiming and Pan, Chunhong},
title = {Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240567},
doi = {10.1145/3240508.3240567},
abstract = {Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {700–708},
numpages = {9},
keywords = {weakly-supervised detection, multi-label image classification, knowledge distillation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240569,
author = {Wang, Jiayu and Zhou, Wengang and Tang, Jinhui and Fu, Zhongqian and Tian, Qi and Li, Houqiang},
title = {Unregularized Auto-Encoder with Generative Adversarial Networks for Image Generation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240569},
doi = {10.1145/3240508.3240569},
abstract = {With the development of deep neural networks, recent years have witnessed the increasing research interest on generative models. Specificly, Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN) have achieved impressive results in various generative tasks. VAE is well established and theoretically elegant, but tends to generate blurry samples. In contrast, GAN has shown the advantage in visual quality of generated images, but suffers the difficulty in translating a random vector into a desired high-dimensional sample. As a result, the training dynamics in GAN are often unstable and the generated samples could collapse to limited modes. In this paper, we propose a new Auto-Encoder Generative Adversarial Networks (AEGAN), which takes advantages of both VAE and GAN. In our approach, instead of matching the encoded distribution of training samples to the prior Pz as in VAE, we map the random vector into the encoded latent space by adversarial training based on GAN. Besides, we also match the decoded distribution of training samples with that from random vectors. To evaluate our approach, we make comparison with other encoder-decoder based generative models on three public datasets. The experiments with both qualitative and quantitative results demonstrate the superiority of our algorithm over the comparison generative models.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {709–717},
numpages = {9},
keywords = {vae, deep learning, gan, neural network, generative model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240582,
author = {Jiang, Yangbangyan and Yang, Zhiyong and Xu, Qianqian and Cao, Xiaochun and Huang, Qingming},
title = {When to Learn What: Deep Cognitive Subspace Clustering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240582},
doi = {10.1145/3240508.3240582},
abstract = {Subspace clustering aims at clustering data points drawn from a union of low-dimensional subspaces. Recently deep neural networks are introduced into this problem to improve both representation ability and precision for non-linear data. However, such models are sensitive to noise and outliers, since both difficult and easy samples are treated equally. On the contrary, in the human cognitive process, individuals tend to follow a learning paradigm from easy to hard and less to more. In other words, human beings always learn from simple concepts, then absorb more complicated ones gradually. Inspired by such learning scheme, in this paper, we propose a robust deep subspace clustering framework based on the principle of human cognitive process. Specifically, we measure the easinesses of samples dynamically so that our proposed method could gradually utilize instances from easy to more complex ones in a robust way. Meanwhile, a promising solution is designed to update the weights and parameters using an alternative optimization strategy, followed by a theoretical analysis to demonstrated the rationality of the proposed method. Experimental results on three popular benchmark datasets demonstrate the validity of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {718–726},
numpages = {9},
keywords = {deep learning, subspace clustering, self-paced learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240584,
author = {Zhang, Wendong and Gao, Feng and Ni, Bingbing and Duan, Lingyu and Yan, Yichao and Xu, Jingwei and Yang, Xiaokang},
title = {Depth Structure Preserving Scene Image Generation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240584},
doi = {10.1145/3240508.3240584},
abstract = {Key to automatically generate natural scene images is to properly arrange amongst various spatial elements, especially in the depth cue. To this end, we introduce a novel depth structure preserving scene image generation network (DSP-GAN), which favors a hierarchical architecture, for the purpose of depth structure preserving scene image generation. The main trunk of the proposed infrastructure is built upon a Hawkes point process that models high-order spatial dependency between different depth layers. Within each layer generative adversarial sub-networks are trained collaboratively to generate realistic scene components, conditioned on the layer information produced by the point process. We experiment our model on annotated natural scene images collected from SUN dataset and demonstrate that our models are capable of generating depth-realistic natural scene image.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {727–736},
numpages = {10},
keywords = {depth structure, scene image generation, hawkes process},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240585,
author = {Liu, Jiawei and Zha, Zheng-Jun and Xie, Hongtao and Xiong, Zhiwei and Zhang, Yongdong},
title = {CA<sub>3</sub>Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240585},
doi = {10.1145/3240508.3240585},
abstract = {Person re-identification aims to identify the same pedestrian across non-overlapping camera views. Deep learning techniques have been applied for person re-identification recently, towards learning representation of pedestrian appearance. This paper presents a novel Contextual-Attentional Attribute-Appearance Network ($rm CA^3Net$) for person re-identification. The $rm CA^3Net$ simultaneously exploits the complementarity between semantic attributes and visual appearance, the semantic context among attributes, visual attention on attributes as well as spatial dependencies among body parts, leading to discriminative and robust pedestrian representation. Specifically, an attribute network within $rm CA^3Net$ is designed with an Attention-LSTM module. It concentrates the network on latent image regions related to each attribute as well as exploits the semantic context among attributes by a LSTM module. An appearance network is developed to learn appearance features from the full body, horizontal and vertical body parts of pedestrians with spatial dependencies among body parts. The $rm CA^3Net$ jointly learns the attribute and appearance features in a multi-task learning manner, generating comprehensive representation of pedestrians. Extensive experiments on two challenging benchmarks, i.e., Market-1501 and DukeMTMC-reID datasets, have demonstrated the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {737–745},
numpages = {9},
keywords = {appearance, person re-identification, attribute, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240621,
author = {Te, Gusi and Hu, Wei and Zheng, Amin and Guo, Zongming},
title = {RGCNN: Regularized Graph CNN for Point Cloud Segmentation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240621},
doi = {10.1145/3240508.3240621},
abstract = {Point cloud, an efficient 3D object representation, has become popular with the development of depth sensing and 3D laser scanning techniques. It has attracted attention in various applications such as 3D tele-presence, navigation for unmanned vehicles and heritage reconstruction. The understanding of point clouds, such as point cloud segmentation, is crucial in exploiting the informative value of point clouds for such applications. Due to the irregularity of the data format, previous deep learning works often convert point clouds to regular 3D voxel grids or collections of images before feeding them into neural networks, which leads to voluminous data and quantization artifacts. In this paper, we instead propose a regularized graph convolutional neural network (RGCNN) that directly consumes point clouds. Leveraging on spectral graph theory, we treat features of points in a point cloud as signals on graph, and define the convolution over graph by Chebyshev polynomial approximation. In particular, we update the graph Laplacian matrix that describes the connectivity of features in each layer according to the corresponding learned features, which adaptively captures the structure of dynamic graphs. Further, we deploy a graph-signal smoothness prior in the loss function, thus regularizing the learning process. Experimental results on the ShapeNet part dataset show that the proposed approach significantly reduces the computational complexity while achieving competitive performance with the state of the art. Also, experiments show RGCNN is much more robust to both noise and point cloud density in comparison with other methods. We further apply RGCNN to point cloud classification and achieve competitive results on ModelNet40 dataset.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {746–754},
numpages = {9},
keywords = {updated graph laplacian, point cloud segmentation, graph-signal smoothness prior, graph cnn},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240516,
author = {Liu, Bin and Cao, Yue and Long, Mingsheng and Wang, Jianmin and Wang, Jingdong},
title = {Deep Triplet Quantization},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240516},
doi = {10.1145/3240508.3240516},
abstract = {Deep hashing establishes efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We present a compact coding solution, focusing on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We propose Deep Triplet Quantization (DTQ), a novel approach to learning deep quantization models from the similarity triplets. To enable more effective triplet training, we design a new triplet selection approach, Group Hard, that randomly selects hard triplets in each image group. To generate compact binary codes, we further apply a triplet quantization with weak orthogonality during triplet training. The quantization loss reduces the codebook redundancy and enhances the quantizability of deep representations through back-propagation. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes, which yields state-of-the-art image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {755–763},
numpages = {9},
keywords = {deep hashing, quantization, image search},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286926,
author = {Luo, Jiebo},
title = {Session Details: Keynote 3},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286926},
doi = {10.1145/3286926},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3267345,
author = {Edmonds, Ernest A.},
title = {What Has Art Got to Do With It?},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3267345},
doi = {10.1145/3240508.3267345},
abstract = {What can multi-media systems design learn from art? How can the research agenda be advanced by looking at art? How can we improve creativity support and the amplification of that important human capability? Interactive art has become a common part of life as a result of the many ways in which the computer and the Internet have facilitated it. Multi-media computing is as important to interactive art as mixing the colors of paint are to painting. This talk reviews recent work that looks at these issues through art research. In interactive digital art, the artist is concerned with how the artwork behaves, how the audience interacts with it, and, ultimately, how participants experience art as well as their degree of engagement. The talk examines these issues and brings together a collection of research results from art practice that illuminates this significant new and expanding area. In particular, this work points towards a much-needed critical language that can be used to describe, compare and frame research into the support of creativity.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {773},
numpages = {1},
keywords = {multi-media, hci, art},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286927,
author = {Mei, Tao and Lienhart, Rainer},
title = {Session Details: Best Paper Session},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286927},
doi = {10.1145/3286927},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240704,
author = {Tang, Hao and Wang, Wei and Xu, Dan and Yan, Yan and Sebe, Nicu},
title = {GestureGAN for Hand Gesture-to-Gesture Translation in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240704},
doi = {10.1145/3240508.3240704},
abstract = {Hand gesture-to-gesture translation in the wild is a challenging task since hand gestures can have arbitrary poses, sizes, locations and self-occlusions. Therefore, this task requires a high-level understanding of the mapping between the input source gesture and the output target gesture. To tackle this problem, we propose a novel hand Gesture Generative Adversarial Network (GestureGAN). GestureGAN consists of a single generator G and a discriminator D, which takes as input a conditional hand image and a target hand skeleton image. GestureGAN utilizes the hand skeleton information explicitly, and learns the gesture-to-gesture mapping through two novel losses, the color loss and the cycle-consistency loss. The proposed color loss handles the issue of "channel pollution" while back-propagating the gradients. In addition, we present the Frechet ResNet Distance (FRD) to evaluate the quality of generated images. Extensive experiments on two widely used benchmark datasets demonstrate that the proposed GestureGAN achieves state-of-the-art performance on the unconstrained hand gesture-to-gesture translation task. Meanwhile, the generated images are in high-quality and are photo-realistic, allowing them to be used as data augmentation to improve the performance of a hand gesture classifier. Our model and code are available at https://github.com/Ha0Tang/GestureGAN.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {774–782},
numpages = {9},
keywords = {hand gesture, generative adversarial networks, image translation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240587,
author = {Liu, Bei and Fu, Jianlong and Kato, Makoto P. and Yoshikawa, Masatoshi},
title = {Beyond Narrative Description: Generating Poetry from Images by Multi-Adversarial Training},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240587},
doi = {10.1145/3240508.3240587},
abstract = {Automatic generation of natural language from images has attracted extensive attention. In this paper, we take one step further to investigate generation of poetic language (with multiple lines) to an image for automatic poetry creation. This task involves multiple challenges, including discovering poetic clues from the image (e.g., hope from green), and generating poems to satisfy both relevance to the image and poeticness in language level. To solve the above challenges, we formulate the task of poem generation into two correlated sub-tasks by multi-adversarial training via policy gradient, through which the cross-modal relevance and poetic language style can be ensured. To extract poetic clues from images, we propose to learn a deep coupled visual-poetic embedding, in which the poetic representation from objects, sentiments footnoteWe consider both adjectives and verbs that can express emotions and feelings as sentiment words in this research. and scenes in an image can be jointly learned. Two discriminative networks are further introduced to guide the poem generation, including a multi-modal discriminator and a poem-style discriminator. To facilitate the research, we have released two poem datasets by human annotators with two distinct properties: 1) the first human annotated image-to-poem pair dataset (with $8,292$ pairs in total), and 2) to-date the largest public English poem corpus dataset (with $92,265$ different poems in total). Extensive experiments are conducted with 8K images, among which 1.5K image are randomly picked for evaluation. Both objective and subjective evaluations show the superior performances against the state-of-the-art methods for poem generation from images. Turing test carried out with over $500$ human subjects, among which 30 evaluators are poetry experts, demonstrates the effectiveness of our approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {783–791},
numpages = {9},
keywords = {adversarial training, image, poetry generation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240509,
author = {Zhao, Jian and Li, Jianshu and Cheng, Yu and Sim, Terence and Yan, Shuicheng and Feng, Jiashi},
title = {Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240509},
doi = {10.1145/3240508.3240509},
abstract = {Despite the noticeable progress in perceptual tasks like detection, instance segmentation and human parsing, computers still perform unsatisfactorily on visually understanding humans in crowded scenes, such as group behavior analysis, person re-identification and autonomous driving, etc. To this end, models need to comprehensively perceive the semantic information and the differences between instances in a multi-human image, which is recently defined as the multi-human parsing task. In this paper, we present a new large-scale database "Multi-Human Parsing (MHP)" for algorithm development and evaluation, and advances the state-of-the-art in understanding humans in crowded scenes. MHP contains 25,403 elaborately annotated images with 58 fine-grained semantic category labels, involving 2-26 persons per image and captured in real-world scenes from various viewpoints, poses, occlusion, interactions and background. We further propose a novel deep Nested Adversarial Network (NAN) model for multi-human parsing. NAN consists of three Generative Adversarial Network (GAN)-like sub-nets, respectively performing semantic saliency prediction, instance-agnostic parsing and instance-aware clustering. These sub-nets form a nested structure and are carefully designed to learn jointly in an end-to-end way. NAN consistently outperforms existing state-of-the-art solutions on our MHP and several other datasets, and serves as a strong baseline to drive the future research for multi-human parsing.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {792–800},
numpages = {9},
keywords = {nested adversarial learning, generative adversarial networks, human parsing, multi-human parsing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240605,
author = {Liao, Lizi and Ma, Yunshan and He, Xiangnan and Hong, Richang and Chua, Tat-Seng},
title = {Knowledge-Aware Multimodal Dialogue Systems},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240605},
doi = {10.1145/3240508.3240605},
abstract = {By offering a natural way for information seeking, multimodal dialogue systems are attracting increasing attention in several domains such as retail, travel etc. However, most existing dialogue systems are limited to textual modality, which cannot be easily extended to capture the rich semantics in visual modality such as product images. For example, in fashion domain, the visual appearance of clothes and matching styles play a crucial role in understanding the user's intention. Without considering these, the dialogue agent may fail to generate desirable responses for users. In this paper, we present a Knowledge-aware Multimodal Dialogue (KMD) model to address the limitation of text-based dialogue systems. It gives special consideration to the semantics and domain knowledge revealed in visual content, and is featured with three key components. First, we build a taxonomy-based learning module to capture the fine-grained semantics in images the category and attributes of a product). Second, we propose an end-to-end neural conversational model to generate responses based on the conversation history, visual semantics, and domain knowledge. Lastly, to avoid inconsistent dialogues, we adopt a deep reinforcement learning method which accounts for future rewards to optimize the neural conversational model. We perform extensive evaluation on a multi-turn task-oriented dialogue dataset in fashion domain. Experiment results show that our method significantly outperforms state-of-the-art methods, demonstrating the efficacy of modeling visual modality and domain knowledge for dialogue systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {801–809},
numpages = {9},
keywords = {fashion, domain knowledge, multimodal dialogue},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286928,
author = {Wang, Meng},
title = {Session Details: Doctoral Symposium},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286928},
doi = {10.1145/3286928},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243933,
author = {Zhao, Na},
title = {End2End Semantic Segmentation for 3D Indoor Scenes},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243933},
doi = {10.1145/3240508.3243933},
abstract = {This research is concerned with semantic segmentation of 3D point clouds arising from videos of 3D indoor scenes. It is an important building block of 3D scene understanding and has promising applications such as augmented reality and robotics. Although various deep learning based approaches have been proposed to replicate the success of 2D semantic segmentation in 3D domain, they either result in severe information loss or fail to model the geometric structures well. In this paper, we aim to model the local and global geometric structures of 3D scenes by designing an end-to-end 3D semantic segmentation framework. It captures the local geometries from point-level feature learning and voxel-level aggregation, models the global structures via 3D CNN, and enforces label consistency with high-order CRF. Through preliminary experiments conducted on two indoor datasets, we describe our insights on the proposed approach, and present some directions to be pursued in the future.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {810–814},
numpages = {5},
keywords = {3d point cloud, deep learning, semantic segmentation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243934,
author = {Kletz, Sabrina},
title = {On Reducing Effort in Evaluating Laparoscopic Skills},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243934},
doi = {10.1145/3240508.3243934},
abstract = {Training and evaluation of laparoscopic skills have become an important aspect of young surgeons' education. The evaluation process is currently performed manually by experienced surgeons through reviewing video recordings of laparoscopic procedures for detecting technical errors using conventional video players and specific pen and paper rating schemes. The problem is, that the manual review process is time-consuming and exhausting, but nevertheless necessary to support young surgeons in their educational training. Motivated by the need to reduce the effort in evaluating laparoscopic skills, this PhD project aims at investigating state-of-the-art content analysis approaches for finding error-prone video sections in surgery videos. In this proposal, the focus specifically lies on performance assessment in gynecologic laparoscopy using the Generic Error Rating Tool (GERT).},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {815–819},
numpages = {5},
keywords = {interactive systems, laparoscopic skill evaluation, instrument detection, gynecology, video analysis, error detection, laparoscopy, machine learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243935,
author = {Hu, Tianran},
title = {Decode Human Life from Social Media},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243935},
doi = {10.1145/3240508.3243935},
abstract = {In this big data era, people leave clues of their life consciously or unconsciously on many social media platforms in various forms. By mining data from social media, researchers can uncover the patterns of human life at both individual and group levels. Social media is one of the major data sources for such studies for mainly two reasons. 1) The huge volume and open access of data on these platforms, and 2) the diversity of data on different platforms, such as multimedia data on Twitter and Facebook, geolocation data on Foursquare and Yelp, as well as career data on Linkedin. In this paper, we introduce our work on studying human life based on social media data, and report the plan for our subsequent studies. Our work is intended to decodes human life from two perspectives. From a linguistic perspective, we study the language patterns of different social groups of people. The learned language patterns can reveal the specific characteristics of these groups, and provide novel angles to understanding people. From a mobility perspective, we extract the mobility patterns of individual person, and groups of people such as residents of certain regions. Using the detected mobility patterns, we mine knowledge of human life including the lifestyles and shopping patterns of cities and regions. We intend to combine these two perspectives in our ongoing work, and introduce a novel framework for study human life.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {820–824},
numpages = {5},
keywords = {linguistic, social media, lifestyles, mobility},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286929,
author = {Cheng, Wen-Huang},
title = {Session Details: FF-4},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286929},
doi = {10.1145/3286929},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240521,
author = {Wu, Yiling and Wang, Shuhui and Huang, Qingming},
title = {Learning Semantic Structure-Preserved Embeddings for Cross-Modal Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240521},
doi = {10.1145/3240508.3240521},
abstract = {This paper learns semantic embeddings for multi-label cross-modal retrieval. Our method exploits the structure in semantics represented by label vectors to guide the learning of embeddings. First, we construct a semantic graph based on label vectors which incorporates data from both modalities, and enforce the embeddings to preserve the local structure of this semantic graph. Second, we enforce the embeddings to well reconstruct the labels, i.e., the global semantic structure. In addition, we encourage the embeddings to preserve local geometric structure of each modality. Accordingly, the local and global semantic structure consistencies as well as the local geometric structure consistency are enforced, simultaneously. The mappings between inputs and embeddings are designed to be nonlinear neural network with larger capacity and more flexibility. The overall objective function is optimized by stochastic gradient descent to gain the scalability on large datasets. Experiments conducted on three real world datasets clearly demonstrate the superiority of our proposed approach over the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {825–833},
numpages = {9},
keywords = {cross-modal retrieval, graph embeddings, semantic embeddings},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240529,
author = {Mao, Zhendong and Wang, Quan and Zhang, Yongdong and Wang, Bin},
title = {Post Tuned Hashing: A New Approach to Indexing High-Dimensional Data},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240529},
doi = {10.1145/3240508.3240529},
abstract = {Learning to hash has proven to be an effective solution for indexing high-dimensional data by projecting them to similarity-preserving binary codes. However, most existing methods end up the learning scheme with a binarization stage, i.e. binary quantization, which inevitably destroys the neighborhood structure of original data. As a result, those methods still suffer from great similarity loss and result in unsatisfactory indexing performance. In this paper we propose a novel hashing model, namely Post Tuned Hashing (PTH), which includes a new post-tuning stage to refine the binary codes after binarization. The post-tuning seeks to rebuild the destroyed neighborhood structure, and hence significantly improves the indexing performance. We cast the post-tuning into a binary quadratic optimization framework and, despite its NP-hardness, give a practical algorithm to efficiently obtain a high-quality solution. Experimental results on five noted image benchmarks show that our PTH improves previous state-of-the-art methods by 13-58% in mean average precision.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {834–842},
numpages = {9},
keywords = {unsupervised binary hashing, learning to hash, ann search},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240549,
author = {Liu, Meng and Wang, Xiang and Nie, Liqiang and Tian, Qi and Chen, Baoquan and Chua, Tat-Seng},
title = {Cross-Modal Moment Localization in Videos},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240549},
doi = {10.1145/3240508.3240549},
abstract = {In this paper, we address the temporal moment localization issue, namely, localizing a video moment described by a natural language query in an untrimmed video. This is a general yet challenging vision-language task since it requires not only the localization of moments, but also the multimodal comprehension of textual-temporal information (e.g., "first" and "leaving") that helps to distinguish the desired moment from the others, especially those with the similar visual content. While existing studies treat a given language query as a single unit, we propose to decompose it into two components: the relevant cue related to the desired moment localization and the irrelevant one meaningless to the localization. This allows us to flexibly adapt to arbitrary queries in an end-to-end framework. In our proposed model, a language-temporal attention network is utilized to learn the word attention based on the temporal context information in the video. Therefore, our model can automatically select "what words to listen to" for localizing the desired moment. We evaluate the proposed model on two public benchmark datasets: DiDeMo and Charades-STA. The experimental results verify its superiority over several state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {843–851},
numpages = {9},
keywords = {cross-modal video retrieval, moment localization, language-temporal attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240560,
author = {Ye, Zhaoda and Peng, Yuxin},
title = {Multi-Scale Correlation for Sequential Cross-Modal Hashing Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240560},
doi = {10.1145/3240508.3240560},
abstract = {Cross-modal hashing aims to learn hash functions, which map heterogeneous multimedia data into common Hamming space for fast and flexible cross-modal retrieval. Recently, several cross-modal hashing methods learn the hash functions by mining the correlation among multimedia data. However, they ignore two properties of cross-modal data: 1) The features of different scale in single modality consist different information, such as texture, object and scene feature in the image, which can provide multi-scale information on retrieval task. 2) The correlation among the features of different modalities and scales can provide multi-scale relationship for better cross-modal hashing learning. In this paper, we propose Multi-scale Correlation Sequential Cross-modal Hashing Learning (MCSCH) approach. The main contributions of the MCSCH can be summarized as follows: 1) We propose a multi-scale feature guided sequential hashing learning method which sequentially generates the hash code guided by different scale features through a RNN based network. The multi-scale feature guided sequential hashing learning method utilizes the scale information, which enhances the diversity of the hash codes and reduces the error caused by extreme situation in specifc features. 2) We propose a multi-scale correlation mining strategy during the multi-scale feature guided sequential hashing learning, which can simultaneously mine the correlation among the features of different modalities and scales. Through this strategy, we can mine any pair of scale features in different modalities and obtain abundant scale correlation for better cross-modal retrieval. Experiments on two widely-used datasets demonstrate the effectiveness of our proposed MCSCH approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {852–860},
numpages = {9},
keywords = {cross-modal hash learning, deep model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240590,
author = {Yu, Litao and Gao, Yongsheng and Zhou, Jun},
title = {Generative Adversarial Product Quantisation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240590},
doi = {10.1145/3240508.3240590},
abstract = {Product Quantisation (PQ) has been recognised as an effective encoding technique for scalable multimedia content analysis. In this paper, we propose a novel learning framework that enables an end-to-end encoding strategy from raw images to compact PQ codes. The system aims to learn both PQ encoding functions and codewords for content-based image retrieval. In detail, we first design a trainable encoding layer that is pluggable into neural networks, so the codewords can be trained in back-forward propagation. Then we integrate it into a Deep Convolutional Generative Adversarial Network (DC-GAN). In our proposed encoding framework, the raw images are directly encoded by passing through the convolutional and encoding layers, and the generator aims to use the codewords as constrained inputs to generate full image representations that are visually similar to the original images. By taking the advantages of the generative adversarial model, our proposed system can produce high-quality PQ codewords and encoding functions for scalable multimedia retrieval tasks. Experiments show that the proposed architecture GA-PQ outperforms the state-of-the-art encoding techniques on three public image datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {861–869},
numpages = {9},
keywords = {product quantisation, generative adversarial nets, image retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240531,
author = {Deng, Yubin and Loy, Chen Change and Tang, Xiaoou},
title = {Aesthetic-Driven Image Enhancement by Adversarial Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240531},
doi = {10.1145/3240508.3240531},
abstract = {We introduce EnhanceGAN, an adversarial learning based model that performs automatic image enhancement. Traditional image enhancement frameworks typically involve training models in a fully-supervised manner, which require expensive annotations in the form of aligned image pairs. In contrast to these approaches, our proposed EnhanceGAN only requires weak supervision (binary labels on image aesthetic quality) and is able to learn enhancement operators for the task of aesthetic-based image enhancement. In particular, we show the effectiveness of a piecewise color enhancement module trained with weak supervision, and extend the proposed EnhanceGAN framework to learning a deep filtering-based aesthetic enhancer. The full differentiability of our image enhancement operators enables the training of EnhanceGAN in an end-to-end manner. We further demonstrate the capability of EnhanceGAN in learning aesthetic-based image cropping without any groundtruth cropping pairs. Our weakly-supervised EnhanceGAN reports competitive quantitative results on aesthetic-based color enhancement as well as automatic image cropping, and a user study confirms that our image enhancement results are on par with or even preferred over professional enhancement.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {870–878},
numpages = {9},
keywords = {image enhancement, weakly-supervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240554,
author = {Sheng, Kekai and Dong, Weiming and Ma, Chongyang and Mei, Xing and Huang, Feiyue and Hu, Bao-Gang},
title = {Attention-Based Multi-Patch Aggregation for Image Aesthetic Assessment},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240554},
doi = {10.1145/3240508.3240554},
abstract = {Aggregation structures with explicit information, such as image attributes and scene semantics, are effective and popular for intelligent systems for assessing aesthetics of visual data. However, useful information may not be available due to the high cost of manual annotation and expert design. In this paper, we present a novel multi-patch (MP) aggregation method for image aesthetic assessment. Different from state-of-the-art methods, which augment an MP aggregation network with various visual attributes, we train the model in an end-to-end manner with aesthetic labels only (i.e., aesthetically positive or negative). We achieve the goal by resorting to an attention-based mechanism that adaptively adjusts the weight of each patch during the training process to improve learning efficiency. In addition, we propose a set of objectives with three typical attention mechanisms (i.e., average, minimum, and adaptive) and evaluate their effectiveness on the Aesthetic Visual Analysis (AVA) benchmark. Numerical results show that our approach outperforms existing methods by a large margin. We further verify the effectiveness of the proposed attention-based objectives via ablation studies and shed light on the design of aesthetic assessment systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {879–886},
numpages = {8},
keywords = {image aesthetic assessment, convolutional neural network, multi-patch aggregation, attention mechanism},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240555,
author = {He, Zheqi and Zhou, Yafeng and Wang, Yongtao and Wang, Siwei and Lu, Xiaoqing and Tang, Zhi and Cai, Ling},
title = {An End-to-End Quadrilateral Regression Network for Comic Panel Extraction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240555},
doi = {10.1145/3240508.3240555},
abstract = {Comic panel extraction, i.e., decomposing a comic page image into panels, has become a fundamental technique for meeting many practical needs of mobile comic reading such as comic content adaptation and comic animating. Most of existing approaches are based on handcrafted low-level visual patterns and heuristics rules, thus having limited ability to deal with irregular comic panels. Only one existing method is based on deep learning and achieves better experimental results, but its architecture is redundant and its time efficiency is not good. To address these problems, we propose an end-to-end, two-stage quadrilateral regressing network architecture for comic panel detection, which inherits the architecture of Faster R-CNN. At the first stage, we propose a quadrilateral region proposal network for generating panel proposals, based on a newly proposed quadrilateral regression method. At the second stage, we classify the proposals and refine their shapes with the proposed quadrilateral regression method again. Extensive experimental results demonstrate that the proposed method significantly outperforms the existing comic panel detection methods on multiple datasets by F1-score and page accuracy.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {887–895},
numpages = {9},
keywords = {panel extraction, quadrilateral object detection, comics processing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240564,
author = {Yang, Xin and Chen, Jinyu and Wang, Zhiwei and Zhang, Qiaozhe and Liu, Wenyu and Liao, Chunyuan and Cheng, Kwang-Ting},
title = {Monocular Camera Based Real-Time Dense Mapping Using Generative Adversarial Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240564},
doi = {10.1145/3240508.3240564},
abstract = {Monocular simultaneous localization and mapping (SLAM) is a key enabling technique for many computer vision and robotics applications. However, existing methods either can obtain only sparse or semi-dense maps in highly-textured image areas or fail to achieve a satisfactory reconstruction accuracy. In this paper, we present a new method based on a generative adversarial network,named DM-GAN, for real-time dense mapping based on a monocular camera. Specifcally, our depth generator network takes a semidense map obtained from motion stereo matching as a guidance to supervise dense depth prediction of a single RGB image. The depth generator is trained based on a combination of two loss functions, i.e. an adversarial loss for enforcing the generated depth maps to reside on the manifold of the true depth maps and a pixel-wise mean square error (MSE) for ensuring the correct absolute depth values. Extensive experiments on three public datasets demonstrate that our DM-GAN signifcantly outperforms the state-of-the-art methods in terms of greater reconstruction accuracy and higher depth completeness.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {896–904},
numpages = {9},
keywords = {convolutional neural network, slam, generative adversarial network, dense mapping},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240672,
author = {Ma, Xiaojing and Liu, Changming and Cao, Sixing and Zhu, Bin B.},
title = {JPEG Decompression in the Homomorphic Encryption Domain},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240672},
doi = {10.1145/3240508.3240672},
abstract = {Privacy-preserving processing is desirable for cloud computing to relieve users' concern of loss of control of their uploaded data. This may be fulfilled with homomorphic encryption. With widely used JPEG, it is desirable to enable JPEG decompression in the homomorphic encryption domain. This is a great challenge since JPEG decoding needs to determine a matched codeword, which then extracts a codeword-dependent number of coefficients. With no access to the information of encrypted content, a decoder does not know which codeword is matched, and thus cannot tell how many coefficients to extract, not to mention to compute their values. In this paper, we propose a novel scheme that enables JPEG decompression in the homomorphic encryption domain. The scheme applies a statically controlled iterative procedure to decode one coefficient per iteration. In one iteration, each codeword is compared with the bitstream to compute an encrypted Boolean that represents if the codeword is a match or not. Each codeword would produce an output coefficient and generate a new bitstream by dropping consumed bits as if it were a match. If a codeword is associated with more than one coefficient, the codeword is replaced with the codeword representing the remaining undecoded coefficients for the next decoding iteration. The summation of each codeword's output multiplied by its matching Boolean is the output of the current iteration. This is equivalent to selecting the output of a matched codeword. A side benefit of our statically controlled decoding procedure is that paralleled Single-Instruction Multiple-Data (SIMD) is fully supported, wherein multiple plaintexts are encrypted into a single plaintext, and decoding a ciphertext block corresponds to decoding all corresponding plaintext blocks. SIMD also reduces the total size of ciphertexts of an image. Experimental results are reported to show the performance of our proposed scheme.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {905–913},
numpages = {9},
keywords = {privacy-preserving decompression, homomorphic encryption, jpeg., privacy-preserving processing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240705,
author = {Xiao, Mengbai and Wang, Shuoqian and Zhou, Chao and Liu, Li and Li, Zhenhua and Liu, Yao and Chen, Songqing},
title = {MiniView Layout for Bandwidth-Efficient 360-Degree Video},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240705},
doi = {10.1145/3240508.3240705},
abstract = {With the recent increase in popularity of VR devices, 360-degree video has become increasingly popular. As more users experience this new medium, it will likely see further increases in popularity as users experience its greater immersiveness compared to traditional video streams. 360-degree video streams must encode the omnidirectional view, and, with current encoding techniques, these views require significantly higher bandwidth than traditional video streams. These larger bandwidth requirements comprise the main barrier toward wider adoption by video streaming services. To reduce bandwidth requirements of 360-degree streaming, we propose the MiniView Layout. Compared to the standard cube layout, with equal pixel densities, 360-degree videos encoded in the MiniView Layout can save 16% of the encoded video size while delivering similar visual qualities. In conjunction with the MiniView Layout, we make the following contributions toward improving the 360-degree video ecosystem: i) We create a "projection efficiency" metric that quantifies the efficiencies of sphere-to-2D projections. ii) We introduce the ffmpeg360 tool. ffmpeg360 transcodes 360-degree videos and measures comparative 360-degree video quality given user head movement traces. The tool performs these tasks efficiently, using OpenGL for GPU acceleration.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {914–922},
numpages = {9},
keywords = {visual quality, spherical projection, 360-degree video, video streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240570,
author = {Song, Guoxian and Cai, Jianfei and Cham, Tat-Jen and Zheng, Jianmin and Zhang, Juyong and Fuchs, Henry},
title = {Real-Time 3D Face-Eye Performance Capture of a Person Wearing VR Headset},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240570},
doi = {10.1145/3240508.3240570},
abstract = {Teleconference or telepresence based on virtual reality (VR) head-mount display (HMD) device is a very interesting and promising application since HMD can provide immersive feelings for users. However, in order to facilitate face-to-face communications for HMD users, real-time 3D facial performance capture of a person wearing HMD is needed, which is a very challenging task due to the large occlusion caused by HMD. The existing limited solutions are very complex either in setting or in approach as well as lacking the performance capture of 3D eye gaze movement. In this paper, we propose a convolutional neural network (CNN) based solution for real-time 3D face-eye performance capture of HMD users without complex modification to devices. To address the issue of lacking training data, we generate massive pairs of HMD face-label dataset by data synthesis as well as collecting VR-IR eye dataset from multiple subjects. Then, we train a dense-fitting network for facial region and an eye gaze network to regress 3D eye model parameters. Extensive experimental results demonstrate that our system can efficiently and effectively produce in real time a vivid personalized 3D avatar with the correct identity, pose, expression and eye motion corresponding to the HMD user.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {923–931},
numpages = {9},
keywords = {3d facial reconstruction, gaze estimation, hmds},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240581,
author = {Li, Chen and Xu, Mai and Du, Xinzhe and Wang, Zulin},
title = {Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240581},
doi = {10.1145/3240508.3240581},
abstract = {Omnidirectional video enables spherical stimuli with the $360 times 180^ circ$ viewing range. Meanwhile, only the viewport region of omnidirectional video can be seen by the observer through head movement (HM), and an even smaller region within the viewport can be clearly perceived through eye movement (EM). Thus, the subjective quality of omnidirectional video may be correlated with HM and EM of human behavior. To fill in the gap between subjective quality and human behavior, this paper proposes a large-scale visual quality assessment (VQA) dataset of omnidirectional video, called VQA-OV, which collects 60 reference sequences and 540 impaired sequences. Our VQA-OV dataset provides not only the subjective quality scores of sequences but also the HM and EM data of subjects. By mining our dataset, we find that the subjective quality of omnidirectional video is indeed related to HM and EM. Hence, we develop a deep learning model, which embeds HM and EM, for objective VQA on omnidirectional video. Experimental results show that our model significantly improves the state-of-the-art performance of VQA on omnidirectional video.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {932–940},
numpages = {9},
keywords = {omnidirectional video, visual quality assessment, human behavior},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240638,
author = {Zhang, Zongpu and Hua, Yang and Song, Tao and Xue, Zhengui and Ma, Ruhui and Robertson, Neil and Guan, Haibing},
title = {Tracking-Assisted Weakly Supervised Online Visual Object Segmentation in Unconstrained Videos},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240638},
doi = {10.1145/3240508.3240638},
abstract = {This paper tackles the task of online video object segmentation with weak supervision, i.e., labeling the target object and background with pixel-level accuracy in unconstrained videos, given only one bounding box information in the first frame. We present a novel tracking-assisted visual object segmentation framework to achieve this. On the one hand, initialized with a given bounding box in the first frame, the auxiliary object tracking module guides the segmentation module frame by frame by providing motion and region information, which is usually missing in semi-supervised methods. Moreover, compared with the unsupervised approach, our approach with such minimum supervision can focus on the target object without bringing unrelated objects into the final results. On the other hand, the video object segmentation module also improves the robustness of the visual object tracking module by pixel-level localization and objectness information. Thus, segmentation and tracking in our framework can mutually help each other in an online manner. To verify the generality and effectiveness of the proposed framework, we evaluate our weakly supervised method on two cross-domain datasets, i.e., the DAVIS and VOT2016 datasets, with the same configuration and parameter setting. Experimental results show the top performance of our method, which is even better than the leading semi-supervised methods. Furthermore, we conduct the extensive ablation study on our approach to investigate the influence of each component and main parameters.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {941–949},
numpages = {9},
keywords = {visual object tracking, video object segmentation, video analysis, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240641,
author = {Tirupattur, Praveen and Rawat, Yogesh Singh and Spampinato, Concetto and Shah, Mubarak},
title = {ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240641},
doi = {10.1145/3240508.3240641},
abstract = {Studying human brain signals has always gathered great attention from the scientific community. In Brain Computer Interface (BCI) research, for example, changes of brain signals in relation to specific tasks (e.g., thinking something) are detected and used to control machines. While extracting spatio-temporal cues from brain signals for classifying state of human mind is an explored path, decoding and visualizing brain states is new and futuristic. Following this latter direction, in this paper, we propose an approach that is able not only to read the mind, but also to decode and visualize human thoughts. More specifically, we analyze brain activity, recorded by an ElectroEncephaloGram (EEG), of a subject while thinking about a digit, character or an object and synthesize visually the thought item. To accomplish this, we leverage the recent progress of adversarial learning by devising a conditional Generative Adversarial Network (GAN), which takes, as input, encoded EEG signals and generates corresponding images. In addition, since collecting large EEG signals in not trivial, our GAN model allows for learning distributions with limited training data. Performance analysis carried out on three different datasets -- brain signals of multiple subjects thinking digits, characters, and objects -- show that our approach is able to effectively generate images from thoughts of a person. They also demonstrate that EEG signals encode explicitly cues from thoughts which can be effectively used for generating semantically relevant visualizations.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {950–958},
numpages = {9},
keywords = {image generation, generative adversarial networks, eeg},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240648,
author = {Zheng, Xiaoju and Zha, Zheng-Jun and Zhuang, Liansheng},
title = {A Feature-Adaptive Semi-Supervised Framework for Co-Saliency Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240648},
doi = {10.1145/3240508.3240648},
abstract = {Co-saliency detection, which refers to the discovery of common salient foreground regions in a group of relevant images, has attracted increasing attention due to its widespread applications in many vision tasks. Existing methods assemble features from multiple views toward a comprehensive representation, however overlook the efficacy disparity among various features in detecting co-saliency. This paper proposes a novel feature-adaptive semi-supervised (FASS) framework for co-saliency detection, which seamlessly integrates multi-view feature learning, graph structure optimization and co-saliency prediction in a unified solution. In particular, the FASS exploits the efficacy disparity of multi-view features at both view and element levels by a joint formulation of view-wise feature weighting and element-wise feature selection, leading to an effective representation robust to feature noise and redundancy as well as adaptive to the task at hand. It predicts co-saliency map by optimizing co-saliency label prorogation over a graph of both labeled and unlabeled image regions. The graph structure is optimized jointly with feature learning and co-saliency prediction to precisely characterize underlying correlation among regions. The FASS is thus able to produce satisfactory co-saliency map based on the effective exploration of multi-view features as well as inter-region correlation. Extensive experiments on three benchmark datasets, i.e., iCoseg, Cosal2015 and MSRC, have demonstrated that the proposed FASS outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {959–966},
numpages = {8},
keywords = {graph optimization, co-saliency detection, multi-view feature, semi-supervised learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240650,
author = {Kundu, Jogendra Nath and Ganeshan, Aditya and M. V., Rahul and Prakash, Aditya and R., Venkatesh Babu},
title = {ISPA-Net: Iterative Semantic Pose Alignment Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240650},
doi = {10.1145/3240508.3240650},
abstract = {Understanding and extracting 3D information of objects from monocular 2D images is a fundamental problem in computer vision. In the task of 3D object pose estimation, recent data driven deep neural network based approaches suffer from scarcity of real images with 3D keypoint and pose annotations. Drawing inspiration from human cognition, where the annotators use a 3D CAD model as structural reference to acquire ground-truth viewpoints for real images; we propose an iterative Semantic Pose Alignment Network, called iSPA-Net. Our approach focuses on exploiting semantic 3D structural regularity to solve the task of fine-grained pose estimation by predicting viewpoint difference between a given pair of images. Such image comparison based approach also alleviates the problem of data scarcity and hence enhances scalability of the proposed approach for novel object categories with minimal annotation. The fine-grained object pose estimator is also aided by correspondence of learned spatial descriptor of the input image pair. The proposed pose alignment framework enjoys the faculty to refine its initial pose estimation in consecutive iterations by utilizing an online rendering setup along with effectiveness of a non-uniform bin classification of pose-difference. This enables iSPA-Net to achieve state-of-the-art performance on various real image viewpoint estimation datasets. Further, we demonstrate effectiveness of the approach for multiple applications. First, we show results for active object viewpoint localization to capture images from similar pose considering only a single image as pose reference. Second, we demonstrate the ability of the learned semantic correspondence to perform unsupervised part-segmentation transfer using only a single part-annotated 3D template model per object class. To encourage reproducible research, we have released the codes for our proposed algorithm.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {967–975},
numpages = {9},
keywords = {pose-estimation, pose-invarient representation, part-segmentation, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240651,
author = {Feng, Litong and Li, Ziyin and Kuang, Zhanghui and Zhang, Wei},
title = {Extractive Video Summarizer with Memory Augmented Neural Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240651},
doi = {10.1145/3240508.3240651},
abstract = {Online videos have been growing explosively in recent years. How to help human users efficiently browse videos becomes more and more important. Video summarization can automatically shorten a video through extracting key-shots from the raw video, which is helpful for digesting video data. State-of-the-art supervised video summarization algorithms directly learn from manually-created summaries to mimic the key-frame/key-shot selection criterion of humans. Humans usually create a summary after viewing and understanding the whole video, and the global attention mechanism capturing information from all video frames plays a key role in the summarization process. However, previous supervised approaches ignored the temporal relations or simply modeled local inter-dependency across frames. Motivated by this observation, we proposed a memory augmented extractive video summarizer, which utilizes an external memory to record visual information of the whole video with high capacity. With the external memory, the video summarizer simply predicts the importance score of a video shot based on the global understanding of the video frames. The proposed method outperforms previous state-of-the-art algorithms on the public SumMe and TVSum datasets. More importantly, we demonstrate that the global attention modeling has two advantages: good transferring ability across datasets and high robustness to noisy videos.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {976–983},
numpages = {8},
keywords = {memory networks, global attention, video summarization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240653,
author = {Zhang, Jing and Cao, Yang and Wang, Yang and Wen, Chenglin and Chen, Chang Wen},
title = {Fully Point-Wise Convolutional Neural Network for Modeling Statistical Regularities in Natural Images},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240653},
doi = {10.1145/3240508.3240653},
abstract = {Modeling statistical regularity plays an essential role in ill-posed image processing problems. Recently, deep learning based methods have been presented to implicitly learn statistical representation of pixel distributions in natural images and leverage it as a constraint to facilitate subsequent tasks, such as color constancy and image dehazing. However, the existing CNN architecture is prone to variability and diversity of pixel intensity within and between local regions, which may result in inaccurate statistical representation. To address this problem, this paper presents a novel fully point-wise CNN architecture for modeling statistical regularities in natural images. Specifically, we propose to randomly shuffle the pixels in the origin images and leverage the shuffled image as input to make CNN more concerned with the statistical properties. Moreover, since the pixels in the shuffled image are independent identically distributed, we can replace all the large convolution kernels in CNN with point-wise (1*1) convolution kernels while maintaining the representation ability. Experimental results on two applications: color constancy and image dehazing, demonstrate the superiority of our proposed network over the existing architectures, i.e., using 1/10~1/100 network parameters and computational cost while achieving comparable performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {984–992},
numpages = {9},
keywords = {statistical regularity, haze removal, point-wise convolution, color constancy},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240659,
author = {Huang, Jingjia and Li, Nannan and Zhong, Jiaxing and Li, Thomas H. and Li, Ge},
title = {Online Action Tube Detection via Resolving the Spatio-Temporal Context Pattern},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240659},
doi = {10.1145/3240508.3240659},
abstract = {At present, spatio-temporal action detection in the video is still a challenging problem, considering the complexity of the background, the variety of the action or the change of the viewpoint in the unconstrained environment. Most of current approaches solve the problem via a two-step processing: first detecting actions at each frame; then linking them, which neglects the continuity of the action and operates in an offline and batch processing manner. In this paper, we attempt to build an online action detection model that introduces the spatio-temporal coherence existed among action regions when performing action category inference and position localization. Specifically, we seek to represent the spatio-temporal context pattern via establishing an encoder-decoder model based on the convolutional recurrent network. The model accepts a video snippet as input and encodes the dynamic information of the action in the forward pass. During the backward pass, it resolves such information at each time instant for action detection via fusing the current static or motion cue. Additionally, we propose an incremental action tube generation algorithm, which accomplishes action bounding-boxes association, action label determination and the temporal trimming in a single pass. Our model takes in the appearance, motion or fused signals as input and is tested on two prevailing datasets, UCF-Sports and UCF-101. The experiment results demonstrate the effectiveness of our method which achieves a performance superior or comparable to compared existing approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {993–1001},
numpages = {9},
keywords = {spatio-temporal action detection, online action tune generation, encoder-decoder model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240662,
author = {Fang, Zhiwei and Liu, Jing and Qiao, Yanyuan and Tang, Qu and Li, Yong and Lu, Hanqing},
title = {Enhancing Visual Question Answering Using Dropout},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240662},
doi = {10.1145/3240508.3240662},
abstract = {Using dropout in Visual Question Answering (VQA) is a common practice to prevent overfitting. However, in multi-path networks, the current way to use dropout may cause two problems: the co-adaptations of neurons and the explosion of output variance. In this paper, we propose the coherent dropout and the siamese dropouy to solve the two problems, respectively. Specifically, in coherent dropout, all relevant dropout layers in multiple paths are forced to work coherently to maximize the ability of preventing neuron co-adaptations. We show that the coherent dropout is simple in implementation but very effective to overcome overfitting. As for the explosion of output variance, we develop a siamese dropout mechanism to explicitly minimize the difference between the two output vectors produced from the same input data during training phase. Such mechanism can reduce the gap between training and inference phases and make the VQA model more robust. Extensive experiments are conducted to verify the effectiveness of coherent dropout and siamese dropout. And the results also show that our methods can bring additional improvements on the state-of-the-art VQA models.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1002–1010},
numpages = {9},
keywords = {coherent dropout, siamese dropout, visual question answering},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240601,
author = {Horiguchi, Shota and Kanda, Naoyuki and Nagamatsu, Kenji},
title = {Face-Voice Matching Using Cross-Modal Embeddings},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240601},
doi = {10.1145/3240508.3240601},
abstract = {Face-voice matching is a task to find correspondence between faces and voices. Many researches in cognitive science have confirmed human ability in the face-voice matching tasks. Such ability is useful for creating natural human machine interaction systems and in many other applications. In this paper, we propose a face-voice matching model that learns cross-modal embeddings between face images and voice characteristics. We constructed a novel FVCeleb dataset which consists of face images and utterances from 1,078 persons. These persons were selected from the MS-Celeb-1M face image dataset and the VoxCeleb audio dataset. In two-alternative forced-choice matching task with an audio input and two face-image candidates of the same gender, our model achieved 62.2% and 56.5% accuracy on the FVCeleb and the subset of the GRID corpus, respectively. These results are very similar to human performance reported in cognitive science studies.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1011–1019},
numpages = {9},
keywords = {multimodal, face-voice matching, neural network, cross-modal embedding},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240627,
author = {Chen, Jing-Jing and Ngo, Chong-Wah and Feng, Fu-Li and Chua, Tat-Seng},
title = {Deep Understanding of Cooking Procedure for Cross-Modal Recipe Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240627},
doi = {10.1145/3240508.3240627},
abstract = {Finding a right recipe that describes the cooking procedure for a dish from just one picture is inherently a difficult problem. Food preparation undergoes a complex process involving raw ingredients, utensils, cutting and cooking operations. This process gives clues to the multimedia presentation of a dish (e.g., taste, colour, shape). However, the description of the process is implicit, implying only the em cause of dish presentation rather than the visual em effect that can be vividly observed on a picture. Therefore, different from other cross-modal retrieval problems in the literature, recipe search requires the understanding of textually described procedure to predict its possible consequence on visual appearance. In this paper, we approach this problem from the perspective of attention modeling. Specifically, we model the attention of words and sentences in a recipe and align them with its image feature such that both text and visual features share high similarity in multi-dimensional space. Through a large food dataset, Recipe1M, we empirically demonstrate that understanding the cooking procedure can lead to improvement in a large margin compared to the existing methods which mostly consider only ingredient information. Furthermore, with attention modeling, we show that language-specific named-entity extraction based on domain knowledge becomes optional. The result gives light to the feasibility of performing cross-lingual cross-modal recipe retrieval with off-the-shelf machine translation engines.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1020–1028},
numpages = {9},
keywords = {recipe retrieval, cross-modal learning, hierarchical attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240640,
author = {Wu, Yu and Zhu, Linchao and Jiang, Lu and Yang, Yi},
title = {Decoupled Novel Object Captioner},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240640},
doi = {10.1145/3240508.3240640},
abstract = {Image captioning is a challenging task where the machine automatically describes an image by sentences or phrases. It often requires a large number of paired image-sentence annotations for training. However, a pre-trained captioning model can hardly be applied to a new domain in which some novel object categories exist, i.e., the objects and their description words are unseen during model training. To correctly caption the novel object, it requires professional human workers to annotate the images by sentences with the novel words. It is labor expensive and thus limits its usage in real-world applications. In this paper, we introduce the zero-shot novel object captioning task where the machine generates descriptions without extra training sentences about the novel object. To tackle the challenging problem, we propose a Decoupled Novel Object Captioner (DNOC) framework that can fully decouple the language sequence model from the object descriptions. DNOC has two components. 1) A Sequence Model with the Placeholder (SM-P) generates a sentence containing placeholders. The placeholder represents an unseen novel object. Thus, the sequence model can be decoupled from the novel object descriptions. 2) A key-value object memory built upon the freely available detection model, contains the visual information and the corresponding word for each object. A query generated from the SM-P is used to retrieve the words from the object memory. The placeholder will further be filled with the correct word, resulting in a caption with novel object descriptions. The experimental results on the held-out MSCOCO dataset demonstrate the ability of DNOC in describing novel concepts.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1029–1037},
numpages = {9},
keywords = {novel object captioning, novel object, image captioning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240665,
author = {Semedo, David and Magalhaes, Joao},
title = {Temporal Cross-Media Retrieval with Soft-Smoothing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240665},
doi = {10.1145/3240508.3240665},
abstract = {Multimedia information have strong temporal correlations that shape the way modalities co-occur over time. In this paper we study the dynamic nature of multimedia and social-media information, where the temporal dimension emerges as a strong source of evidence for learning the temporal correlations across visual and textual modalities. So far, cross-media retrieval models, explored the correlations between different modalities (e.g. text and image) to learn a common subspace, in which semantically similar instances lie in the same neighbourhood. Building on such knowledge, we propose a novel temporal cross-media neural architecture, that departs from standard cross-media methods, by explicitly accounting for the temporal dimension through temporal subspace learning. The model is softly-constrained with temporal and inter-modality constraints that guide the new subspace learning task by favouring temporal correlations between semantically similar and temporally close instances. Experiments on three distinct datasets show that accounting for time turns out to be important for cross-media retrieval. Namely, the proposed method outperforms a set of baselines on the task of temporal cross-media retrieval, demonstrating its effectiveness for performing temporal subspace learning.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1038–1046},
numpages = {9},
keywords = {multimedia retrieval, cross-media, temporal smoothing, temporal cross-media},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240623,
author = {Song, Yu and Tang, Fan and Dong, Weiming and Zhang, Xiaopeng and Deussen, Oliver and Lee, Tong-Yee},
title = {Photo Squarization by Deep Multi-Operator Retargeting},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240623},
doi = {10.1145/3240508.3240623},
abstract = {Squared forms of photos are widely used in social media as album covers or thumbnails of image streams. In this study, we realize photo squarization by modeling Retargeting Visual Perception Issues, which reflect human perception preference toward image ratargeting. General image retargeting techniques deal with three common issues, namely, salient content, object shape, and scene composition, to preserve the important information of original image. We propose a new way based on multi-operator techniques to investigate human behavior in balancing the three issues. We establish a new dataset and observe human behavior by inviting investigators to retarget images to square manually. We propose a data-driven approach composed of perception and distillation modules by using deep learning techniques to predict human perception preference. The perception part learns the relations among the three issues, and the distillation part transfers the learned relations to a simple but effective network. Our study contributes to deep learning literature by optimizing a network index and lightening its running burden. Experimental results show that photo squarization results generated by the proposed model are consistent with human visual perception results.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1047–1055},
numpages = {9},
keywords = {retargeting visual perception issues, photo squarization, image retargeting, distillation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240636,
author = {Li, Guanbin and He, Xiang and Zhang, Wei and Chang, Huiyou and Dong, Le and Lin, Liang},
title = {Non-Locally Enhanced Encoder-Decoder Network for Single Image De-Raining},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240636},
doi = {10.1145/3240508.3240636},
abstract = {Single image rain streaks removal has recently witnessed substantial progress due to the development of deep convolutional neural networks. However, existing deep learning based methods either focus on the entrance and exit of the network by decomposing the input image into high and low frequency information and employing residual learning to reduce the mapping range, or focus on the introduction of cascaded learning scheme to decompose the task of rain streaks removal into multi-stages. These methods treat the convolutional neural network as an encapsulated end-to-end mapping module without deepening into the rationality and superiority of neural network design. In this paper, we delve into an effective end-to-end neural network structure for stronger feature expression and spatial correlation learning. Specifically, we propose a non-locally enhanced encoder-decoder network framework, which consists of a pooling indices embedded encoder-decoder network to efficiently learn increasingly abstract feature representation for more accurate rain streaks modeling while perfectly preserving the image detail. The proposed encoder-decoder framework is composed of a series of non-locally enhanced dense blocks that are designed to not only fully exploit hierarchical features from all the convolutional layers but also well capture the long-distance dependencies and structural information. Extensive experiments on synthetic and real datasets demonstrate that the proposed method can effectively remove rain-streaks on rainy image of various densities while well preserving the image details, which achieves significant improvements over the recent state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1056–1064},
numpages = {9},
keywords = {non-local mean calculation, image de-raining, dense network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240639,
author = {Zhao, Pu and Liu, Sijia and Wang, Yanzhi and Lin, Xue},
title = {An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240639},
doi = {10.1145/3240508.3240639},
abstract = {Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That is, adversarial examples, obtained by adding delicately crafted distortions onto original legal inputs, can mislead a DNN to classify them as any target labels. In a successful adversarial attack, the targeted mis-classification should be achieved with the minimal distortion added. In the literature, the added distortions are usually measured by $L_0$, $L_1$, $L_2$, and $L_infty $ norms, namely, L_0, L_1, L_2, and L_∞ attacks, respectively. However, there lacks a versatile framework for all types of adversarial attacks. This work for the first time unifies the methods of generating adversarial examples by leveraging ADMM (Alternating Direction Method of Multipliers), an operator splitting optimization approach, such that $L_0$, $L_1$, $L_2$, and $L_infty $ attacks can be effectively implemented by this general framework with little modifications. Comparing with the state-of-the-art attacks in each category, our ADMM-based attacks are so far the strongest, achieving both the 100% attack success rate and the minimal distortion.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1065–1073},
numpages = {9},
keywords = {adversarial attacks, admm (alternating direction method of multipliers), deep neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240645,
author = {Yang, Jiwei and Shen, Xu and Tian, Xinmei and Li, Houqiang and Huang, Jianqiang and Hua, Xian-Sheng},
title = {Local Convolutional Neural Networks for Person Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240645},
doi = {10.1145/3240508.3240645},
abstract = {Recent works have shown that person re-identification can be substantially improved by introducing attention mechanisms, which allow learning both global and local representations. However, all these works learn global and local features in separate branches. As a consequence, the interaction/boosting of global and local information are not allowed, except in the final feature embedding layer. In this paper, we propose local operations as a generic family of building blocks for synthesizing global and local information in any layer. This building block can be inserted into any convolutional networks with only a small amount of prior knowledge about the approximate locations of local parts. For the task of person re-identification, even with only one local block inserted, our local convolutional neural networks (Local CNN) can outperform state-of-the-art methods consistently on three large-scale benchmarks, including Market-1501, CUHK03, and DukeMTMC-ReID.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1074–1082},
numpages = {9},
keywords = {person re-identification, local convolutional neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240647,
author = {Lu, Zhihe and Hu, Tanhao and Song, Lingxiao and Zhang, Zhaoxiang and He, Ran},
title = {Conditional Expression Synthesis with Face Parsing Transformation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240647},
doi = {10.1145/3240508.3240647},
abstract = {Facial expression synthesis with various intensities is a challenging synthesis task due to large identity appearance variations and a paucity of efficient means for intensity measurement. This paper advances the expression synthesis domain by the introduction of a Couple-Agent Face Parsing based Generative Adversarial Network (CAFP-GAN) that unites the knowledge of facial semantic regions and controllable expression signals. Specially, we employ a face parsing map as a controllable condition to guide facial texture generation with a special expression, which can provide a semantic representation of every pixel of facial regions. Our method consists of two sub-networks: face parsing prediction network (FPPN) uses controllable labels (expression and intensity) to generate a face parsing map transformation that corresponds to the labels from the input neutral face, and facial expression synthesis network (FESN) makes the pretrained FPPN as a part of it to provide the face parsing map as a guidance for expression synthesis. To enhance the reality of results, couple-agent discriminators are served to distinguish fake-real pairs in both two sub-nets. Moreover, we only need the neutral face and the labels to synthesize the unknown expression with different intensities. Experimental results on three popular facial expression databases show that our method has the compelling ability on continuous expression synthesis.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1083–1091},
numpages = {9},
keywords = {expression synthesis, face parsing, generative adversarial network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240649,
author = {Li, Liang and Wang, Shuhui and Jiang, Shuqiang and Huang, Qingming},
title = {Attentive Recurrent Neural Network for Weak-Supervised Multi-Label Image Classification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240649},
doi = {10.1145/3240508.3240649},
abstract = {Multi-label image classification is a fundamental and challenging task in computer vision, and recently achieved significant progress by exploiting semantic relations among labels. However, the spatial positions of labels for multi-labels images are usually not provided in real scenarios, which brings insuperable barrier to conventional models. In this paper, we propose an end-to-end attentive recurrent neural network for multi-label image classification under only image-level supervision, which learns the discriminative feature representations and models the label relations simultaneously. First, inspired by attention mechanism, we propose a recurrent highlight network (RHN) which focuses on the most related regions in the image to learn the discriminative feature representations for different objects in an iterative manner. Second, we develop a gated recurrent relation extractor (GRRE) to model the label relations using multiplicative gates in a recurrent fashion, which learns to decide how multiple labels of the image influence the relation extraction. Extensive experiments on three benchmark datasets show that our model outperforms the state-of-the-arts, and performs better on small-object categories and under the scenario with large number of labels.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1092–1100},
numpages = {9},
keywords = {label relations modeling, attentive recurrent neural network, reinforcement learning, multi-label image classification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240658,
author = {Garg, Jatin and Peri, Skand Vishwanath and Tolani, Himanshu and Krishnan, Narayanan C.},
title = {Deep Cross Modal Learning for Caricature Verification and Identification (CaVINet)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240658},
doi = {10.1145/3240508.3240658},
abstract = {Learning from different modalities is a challenging task. In this paper, we look at the challenging problem of cross modal face verification and recognition between caricature and visual image modalities. Caricature have exaggerations of facial features of a person. Due to the significant variations in the caricatures, building vision models for recognizing and verifying data from this modality is an extremely challenging task. Visual images with significantly lesser amount of distortions can act as a bridge for the analysis of caricature modality. We introduce a publicly available large Caricature-VIsual dataset [CaVI] with images from both the modalities that captures the rich variations in the caricature of an identity. This paper presents the first cross modal architecture that handles extreme distortions of caricatures using a deep learning network that learns similar representations across the modalities. We use two convolutional networks along with transformations that are subjected to orthogonality constraints to capture the shared and modality specific representations. In contrast to prior research, our approach neither depends on manually extracted facial landmarks for learning the representations, nor on the identities of the person for performing verification. The learned shared representation achieves 91% accuracy for verifying unseen images and 75% accuracy on unseen identities. Further, recognizing the identity in the image by knowledge transfer using a combination of shared and modality specific representations, resulted in an unprecedented performance of 85% rank-1 accuracy for caricatures and 95% rank-1 accuracy for visual images.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1101–1109},
numpages = {9},
keywords = {cross-modal recognition, deep learning, caricature verification and recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240592,
author = {Inoue, Nakamasa and Shinoda, Koichi},
title = {Few-Shot Adaptation for Multimedia Semantic Indexing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240592},
doi = {10.1145/3240508.3240592},
abstract = {We propose a few-shot adaptation framework, which bridges zero-shot learning and supervised many-shot learning, for semantic indexing of image and video data. Few-shot adaptation provides robust parameter estimation with few training examples, by optimizing the parameters of zero-shot learning and supervised many-shot learning simultaneously. In this method, first we build a zero-shot detector, and then update it by using the few examples. Our experiments show the effectiveness of the proposed framework on three datasets: TRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we show that our method outperforms recent few-shot learning methods. On the TRECVID 2014 dataset, we achieve 15.19~% and 35.98~% in Mean Average Precision under the zero-shot condition and the supervised condition, respectively. To the best of our knowledge, these are the best results on this dataset.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1110–1118},
numpages = {9},
keywords = {few-shot learning, zero-shot learning, semantic indexing, many-shot learning, word vectors},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240596,
author = {Zhou, Zhengzhong and Di, Xiu and Zhou, Wei and Zhang, Liqing},
title = {Fashion Sensitive Clothing Recommendation Using Hierarchical Collocation Model},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240596},
doi = {10.1145/3240508.3240596},
abstract = {Automatic clothing recommendation grows dramatically due to the booming of apparel e-commerce. In this paper, we propose a novel clothing recommendation approach which is sensitive to the fashion trend. The proposed approach incorporates the expert knowledge into multiple dimensional information including purchase behaviors, image contents and product descriptions so as to provide recommendation of clothing in line with the forefront of fashion. Meanwhile, to meet with human visual aesthetics and user's collocation experience, we propose the integration of the convolutional neural network and the hierarchical collocation model (HCM) into our framework. The former is to extract effective visual features and attribute descriptors from the clothing items, while the latter embeds them into the concept of style topics which interpret the collocation pattern from a higher level of semantic knowledge. Such a data driven recommendation approach is able to learn clothing collocation metric from multi-dimensional clothing information. Experimental results show that our HCM method achieves better performance than other state-of-the-art baselines. Besides, it also ensures the fashion sensitivity of the recommended outfits.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1119–1127},
numpages = {9},
keywords = {clothing recommendation, hierarchical collocation model, topic model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240602,
author = {Lou, Yihang and Bai, Yan and Wang, Shiqi and Duan, Ling-Yu},
title = {Multi-Scale Context Attention Network for Image Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240602},
doi = {10.1145/3240508.3240602},
abstract = {Recent attempts on the Convolutional Neural Network (CNN) based image retrieval usually adopt the output of a specific convolutional or fully connected layer as feature representation. Though superior representation capability has yielded better retrieval performance, the scale variation and clutter distracting remain to be two challenging problems in CNN based image retrieval. In this work, we propose a Multi-Scale Context Attention Network (MSCAN) to generate global descriptors, which is able to selectively focus on the informative regions with the assistance of multi-scale context information. We model the multi-scale context information by an improved Long Short-Term Memory (LSTM) network across different layers. As such, the proposed global descriptor is equipped with the scale aware attention capability. Experimental results show that our proposed method can effectively capture the informative regions in images and retain reliable attention responses when encountering scale variation and clutter distracting. Moreover, we compare the performance of the proposed scheme with the state-of-the-art global descriptors, and extensive results verify that the proposed MSCAN can achieve superior performance on several image retrieval benchmarks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1128–1136},
numpages = {9},
keywords = {multi-scale context, image retrieval, attention network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240607,
author = {Zhan, Yibing and Yu, Jun and Yu, Zhou and Zhang, Rong and Tao, Dacheng and Tian, Qi},
title = {Comprehensive Distance-Preserving Autoencoders for Cross-Modal Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240607},
doi = {10.1145/3240508.3240607},
abstract = {In this paper, we propose a novel method with comprehensive distance-preserving autoencoders (CDPAE) to address the problem of unsupervised cross-modal retrieval. Previous unsupervised methods rely primarily on pairwise distances of representations extracted from cross media spaces that co-occur and belong to the same objects. However, besides pairwise distances, the CDPAE also considers heterogeneous distances of representations extracted from cross media spaces as well as homogeneous distances of representations extracted from single media spaces that belong to different objects. The CDPAE consists of four components. First, denoising autoencoders are used to retain the information from the representations and to reduce the negative influence of redundant noises. Second, a comprehensive distance-preserving common space is proposed to explore the correlations among different representations. This aims to preserve the respective distances between the representations within the common space so that they are consistent with the distances in their original media spaces. Third, a novel joint loss function is defined to simultaneously calculate the reconstruction loss of the denoising autoencoders and the correlation loss of the comprehensive distance-preserving common space. Finally, an unsupervised cross-modal similarity measurement is proposed to further improve the retrieval performance. This is carried out by calculating the marginal probability of two media objects based on a kNN classifier. The CDPAE is tested on four public datasets with two cross-modal retrieval tasks: "query images by texts" and "query texts by images". Compared with eight state-of-the-art cross-modal retrieval methods, the experimental results demonstrate that the CDPAE outperforms all the unsupervised methods and performs competitively with the supervised methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1137–1145},
numpages = {9},
keywords = {autoencoder, similarity measurement, comprehensive distancepreserving, unsupervised, cross-modal retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240617,
author = {Chen, Xusong and Liu, Dong and Zha, Zheng-Jun and Zhou, Wengang and Xiong, Zhiwei and Li, Yan},
title = {Temporal Hierarchical Attention at Category- and Item-Level for Micro-Video Click-Through Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240617},
doi = {10.1145/3240508.3240617},
abstract = {Micro-video sharing gains great popularity in recent years, which calls for effective recommendation algorithm to help user find their interested micro-videos. Compared with traditional online (e.g. YouTube) videos, micro-videos contributed by grass-root users and taken by smartphones are much shorter (tens of seconds) and more short of tags or descriptive text, making the recommendation of micro-videos a challenging task. In this paper, we investigate how to model user's historical behaviors so as to predict the user's click-through of micro-videos. Inspired by the recent deep network-based methods, we propose a Temporal Hierarchical Attention at Category- and Item-Level (THACIL) network for user behavior modeling. First, we use temporal windows to capture the short-term dynamics of user interests; Second, we leverage a category-level attention mechanism to characterize user's diverse interests, as well as an item-level attention mechanism for fine-grained profiling of user interests; Third, we adopt forward multi-head self-attention to capture the long-term correlation within user behaviors. Our proposed THACIL network was tested on MicroVideo-1.7M, a new dataset of 1.7 million micro-videos, coming from real data of a micro-video sharing service in China. Experimental results demonstrate the effectiveness of the proposed method in comparison with the state-of-the-art solutions.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1146–1153},
numpages = {8},
keywords = {attention mechanism, recommendation algorithm, micro-video, click-through prediction, user modeling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240593,
author = {Yang, Jufeng and Chen, Liyi and Zhang, Le and Sun, Xiaoxiao and She, Dongyu and Lu, Shao-Ping and Cheng, Ming-Ming},
title = {Historical Context-Based Style Classification of Painting Images via Label Distribution Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240593},
doi = {10.1145/3240508.3240593},
abstract = {Analyzing and categorizing the style of visual art images, especially paintings, is gaining popularity owing to its importance in understanding and appreciating the art. The evolution of painting style is both continuous, in a sense that new styles may inherit, develop or even mutate from their predecessors and multi-modal because of various issues such as the visual appearance, the birthplace, the origin time and the art movement. Motivated by this peculiarity, we introduce a novel knowledge distilling strategy to assist visual feature learning in the convolutional neural network for painting style classification. More specifically, a multi-factor distribution is employed as soft-labels to distill complementary information with visual input, which extracts from different historical context via label distribution learning. The proposed method is well-encapsulated in a multi-task learning framework which allows end-to-end training. We demonstrate the superiority of the proposed method over the state-of-the-art approaches on Painting91, OilPainting, and Pandora datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1154–1162},
numpages = {9},
keywords = {painting style classification, label distribution learning, art history},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240629,
author = {Wu, Hao and Sun, Zhengxing and Yuan, Weihang},
title = {Direction-Aware Neural Style Transfer},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240629},
doi = {10.1145/3240508.3240629},
abstract = {Neural learning methods have been shown to be effective in style transfer. These methods, which are called NST, aim to synthesize a new image that retains the high-level structure of a content image while keeps the low-level features of a style image. However, these models using convolutional structures only extract local statistical features of style images and semantic features of content images. Since the absence of low-level features in the content image, these methods would synthesize images that look unnatural and full of traces of machines. In this paper, we find that direction, that is, the orientation of each painting stroke, can capture the soul of image style preferably and thus generates much more natural and vivid stylizations. According to this observation, we propose a Direction-aware Neural Style Transfer (DaNST) with two major innovations. First, a novel direction field loss is proposed to steer the direction of strokes in the synthesized image. And to build this loss function, we propose novel direction field loss networks to generate and compare the direction fields of content image and synthesized image. By incorporating the direction field loss in neural style transfer, we obtain a new optimization objective. Through minimizing this objective, we can produce synthesized images that better follow the direction field of the content image. Second, our method provides a simple interaction mechanism to control the generated direction fields, and further control the texture direction in synthesized images. Experiments show that our method outperforms state-of-the-art in most styles such as oil painting and mosaic.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1163–1171},
numpages = {9},
keywords = {neural style transfer, convolutional neural networks, direction field},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240655,
author = {He, Bin and Gao, Feng and Ma, Daiqian and Shi, Boxin and Duan, Ling-Yu},
title = {ChipGAN: A Generative Adversarial Network for Chinese Ink Wash Painting Style Transfer},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240655},
doi = {10.1145/3240508.3240655},
abstract = {Style transfer has been successfully applied on photos to generate realistic western paintings. However, because of the inherently different painting techniques adopted by Chinese and western paintings, directly applying existing methods cannot generate satisfactory results for Chinese ink wash painting style transfer. This paper proposes ChipGAN, an end-to-end Generative Adversarial Network based architecture for photo to Chinese ink wash painting style transfer. The core modules of ChipGAN enforce three constraints -- voids, brush strokes, and ink wash tone and diffusion -- to address three key techniques commonly adopted in Chinese ink wash painting. We conduct stylization perceptual study to score the similarity of generated paintings to real paintings by consulting with professional artists based on the newly built Chinese ink wash photo and image dataset. The advantages in visual quality compared with state-of-the-art networks and high stylization perceptual study scores show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1172–1180},
numpages = {9},
keywords = {generative adversarial network, style transfer, painting},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240620,
author = {K\"{a}m\"{a}r\"{a}inen, Teemu and Siekkinen, Matti and Eerik\"{a}inen, Jukka and Yl\"{a}-J\"{a}\"{a}ski, Antti},
title = {CloudVR: Cloud Accelerated Interactive Mobile Virtual Reality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240620},
doi = {10.1145/3240508.3240620},
abstract = {High quality immersive Virtual Reality experience currently requires a PC setup with cable connected head mounted display, which is expensive and restricts user mobility. This paper presents CloudVR which is a system for cloud accelerated interactive mobile VR. It is designed to provide short rotation and interaction latencies through panoramic rendering and dynamic object placement. CloudVR also includes rendering optimizations to reduce server-side computational load and bandwidth requirements between the server and client. Performance measurements with a CloudVR prototype suggest that the optimizations make it possible to double the server's framerate and halve the amount of bandwidth required and that small objects can be quickly moved at run time to client device for rendering to provide shorter interaction latency. A small-scale user study indicates that CloudVR users do not notice small network latencies (20ms) and even much longer ones (100-200ms) become non-trivial to detect when they do not affect the interaction with objects. Finally, we present a design of CloudVR extension to multi-user scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1181–1189},
numpages = {9},
keywords = {edge computing, cloud, virtual reality, optimization, rendering, unity},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240669,
author = {Nguyen, Anh and Yan, Zhisheng and Nahrstedt, Klara},
title = {Your Attention is Unique: Detecting 360-Degree Video Saliency in Head-Mounted Display for Head Movement Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240669},
doi = {10.1145/3240508.3240669},
abstract = {Head movement prediction is the key enabler for the emerging 360-degree videos since it can enhance both streaming and rendering efficiency. To achieve accurate head movement prediction, it becomes imperative to understand user's visual attention on 360-degree videos under head-mounted display (HMD). Despite the rich history of saliency detection research, we observe that traditional models are designed for regular images/videos fixed at a single viewport and would introduce problems such as central bias and multi-object confusion when applied to the multi-viewport 360-degree videos switched by user interaction. To fill in this gap, this paper shifts the traditional single-viewport saliency models that have been extensively studied for decades to a fresh panoramic saliency detection specifically tailored for 360-degree videos, and thus maximally enhances the head movement prediction performance. The proposed head movement prediction framework is empowered by a newly created dataset for 360-degree video saliency, a panoramic saliency detection model and an integration of saliency and head tracking history for the ultimate head movement prediction. Experimental results demonstrate the measurable gain of both the proposed panoramic saliency detection and head movement prediction over traditional models for regular images/videos.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1190–1198},
numpages = {9},
keywords = {360-degree video, head movement prediction, saliency},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240696,
author = {Shao, Yiting and Zhang, Qi and Li, Ge and Li, Zhu and Li, Li},
title = {Hybrid Point Cloud Attribute Compression Using Slice-Based Layered Structure and Block-Based Intra Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240696},
doi = {10.1145/3240508.3240696},
abstract = {Point cloud compression is a key enabler for the emerging applications of immersive visual communication, autonomous driving and smart cities, etc. In this paper, we propose a hybrid point cloud attribute compression scheme built on an original layered data structure. First, a slice partition scheme and a geometry-adaptive k-dimensional tree (k-d tree) method are devised to generate layer structures. Second, we introduce an efficient block-based intra prediction scheme containing to exploit spatial correlations among adjacent points. Third, an adaptive transform scheme based on Graph Fourier Transform (GFT) is Lagrangian optimized to achieve better transform efficiency. The Lagrange multiplier is off-line derived based on the statistics of attribute coding. Last but not least, multiple scan modes are dedicated to improve coding efficiency for entropy coding. Experimental results demonstrate that our method performs better than the state-of-the-art region-adaptive hierarchical transform (RAHT) system, and on average a 37.21% BD-rate gain is achieved. Comparing with the test model for category 1 (TMC1) anchors, which were recently published by MPEG-3DG group on 121st MPEG meeting, a 8.81% BD-rate gain is obtained.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1199–1207},
numpages = {9},
keywords = {layered structure, adaptive reordering scan, lagrange multiplier, gft, block-based intra prediction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240545,
author = {Huang, Tianchi and Zhang, Rui-Xiao and Zhou, Chao and Sun, Lifeng},
title = {QARC: Video Quality Aware Rate Control for Real-Time Video Streaming Based on Deep Reinforcement Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240545},
doi = {10.1145/3240508.3240545},
abstract = {Real-time video streaming is now one of the main applications in all network environments. Due to the fluctuation of throughput under various network conditions, how to choose a proper bitrate adaptively has become an upcoming and interesting issue. To tackle this problem, most proposed rate control methods work for providing high video bitrates instead of video qualities. Nevertheless, we notice that there exists a trade-off between sending bitrate and video quality, which motivates us to focus on how to reach a balance between them. In this paper, we propose QARC (video Quality Aware Rate Control), a rate control algorithm that aims to obtain a higher perceptual video quality with possible lower sending rate and transmission latency. Starting from scratch, QARC uses deep reinforcement learning(DRL) algorithm to train a neural network to select future bitrates based on previously observed network status and past video frames. To overcome the "state explosion problem'', we design a neural network to predict future perceptual video quality as a vector for taking the place of the raw picture in the DRL's inputs.We evaluate QARC via trace-driven simulation, outperforming existing approach with improvements in average video quality of 18% - 25% and decreasing in average latency with 23% -45%. Meanwhile, comparing QARC with offline optimal high bitrate method on various network conditions, we find that QARC also yields a solid result.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1208–1216},
numpages = {9},
keywords = {deep reinforcement learning, rate control, video quality},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240642,
author = {Pang, Haitian and Zhang, Cong and Wang, Fangxin and Hu, Han and Wang, Zhi and Liu, Jiangchuan and Sun, Lifeng},
title = {Optimizing Personalized Interaction Experience in Crowd-Interactive Livecast: A Cloud-Edge Approach},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240642},
doi = {10.1145/3240508.3240642},
abstract = {Enabling users to interact with broadcasters and audience, the crowd-interactive livecast greatly improves viewer's quality of experience (QoE) and attracts millions of daily active users recently. In addition to striking the balance between resource utilization and viewers' QoE met in the traditional video streaming service, this novel service needs to take supererogatory efforts to improve the interaction QoE, which reflects the viewer interaction experience. To tackle this issue, we conduct measurement studies over a large-scale dataset crawled from a representative livecast service provider. We observe that the individual's interaction pattern is quite heterogeneous: only 10% viewers proactively participate in the interaction, and the rest viewers usually watch passively. Incorporating the insight into the emerging cloud-edge architecture, we propose a framework PIECE, which optimizes the Personalized Interaction Experience with Cloud-Edge architecture (PIECE) for intelligent user access control and livecast distribution. In particular, we first devise a novel deep neural network based algorithm to predict users' interaction intensity using the historical viewer pattern. We then design an algorithm to maximize the individual's QoE, by strategically matching viewer sessions and transcoding-delivery paths over cloud-edge infrastructure. Finally, we use trace-driven experiments to verify the effectiveness of PIECE. Our results show that our prediction algorithm outperforms the state-of-the-art algorithms with a much smaller mean absolute error (40% reduction). Furthermore, in comparison with the cloud-based video delivery strategy, the proposed framework can simultaneously improve the average viewers QoE (26% improvement) and interaction QoE (21% improvement), while maintaining a high streaming bitrate.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1217–1225},
numpages = {9},
keywords = {viewer interaction, interactive live streaming, cloud-edge},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286930,
author = {Ro, Ying Man and Sohn, Kwanghoon},
title = {Session Details: Demo + Video + Makers' Program},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286930},
doi = {10.1145/3286930},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241384,
author = {Peng, Songyou and Zhang, Le and Winkler, Stefan and Winslett, Marianne},
title = {Give Me One Portrait Image, I Will Tell You Your Emotion and Personality},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241384},
doi = {10.1145/3240508.3241384},
abstract = {Personality and emotion are both central to affective computing. Existing works address them individually. In this demo we investigate if such high-level affect traits and their relationship can be jointly learned from face images in the wild. To this end, we introduce an end-to-end trainable and deep Siamese-like network. At inference time, our system can take one portrait photo as input and predict one's Big-Five apparent personality as well as emotion attributes. With such a system, we also demonstrate the feasibility of inferring the apparent personality directly fro emotion.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1226–1227},
numpages = {2},
keywords = {deep learning, personality, emotion-to-personality, emotion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241385,
author = {Liu, Yang and Yang, Yang and Fang, Weidong and Zhang, Wuxiong},
title = {Demo: Phase-Based Acoustic Localization and Motion Tracking for Mobile Interaction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241385},
doi = {10.1145/3240508.3241385},
abstract = {Motion tracking, as a mechanism of mobile interaction, allows devices to get fine-gained user input by locating the real-time position of target devices (e.g., smart phones, smart watches) in the air. With the proliferation of mobile devices and smart multimedia devices (e.g., smart TV, home audio system), the ubiquitous speakers and microphones in the devices provide more diverse ways of acoustic-based mobile interaction. In this demonstration, we propose a fine-gained motion tracking system, which can be developed on commercial mobile devices and track the devices with millimeter level (mm-level) accuracy. We first compensate the phase offset between receiver and audio source at each frequency. We then use the acoustic phase change at receiver to achieve accurate distance measurement. Finally, we implement our system on off-the-shelf devices, and achieve a fine-gained motion tracking in two-dimensional space. Our experiments show that our system achieves high accuracy as well as high sensitivity: our system could detect the sight and slow movement caused by human breathing for example.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1228–1230},
numpages = {3},
keywords = {phase calibration, mobile interaction, acoustic motion tracking},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241386,
author = {Zhang, Cunjun and Lei, Kehua and Jia, Jia and Ma, Yihui and Hu, Zhiyuan},
title = {AI Painting: An Aesthetic Painting Generation System},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241386},
doi = {10.1145/3240508.3241386},
abstract = {There are many great works done in image generation. However, it is still an open problem how to generate a painting, which is meeting the aesthetic rules in specific style. Therefore, in this paper, we propose a demonstration to generate a specific painting based on users' input. In the system called AI Painting, we generate an original image from content text, transfer the image into a specific aesthetic effect, simulate the image into specific artistic genre, and illustrate the painting process.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1231–1233},
numpages = {3},
keywords = {artistic effect simulation, painting process illustration, aesthetic effect modification, painting content generation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241387,
author = {Farseev, Aleksandr and Lepikhin, Kirill and Schwartz, Hendrik and Ang, Eu Khoon and Powar, Kenny},
title = {SoMin.Ai: Social Multimedia Influencer Discovery Marketplace},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241387},
doi = {10.1145/3240508.3241387},
abstract = {In this technical demonstration, we showcase the first ai-driven social multimedia influencer discovery marketplace, called SoMin. The platform combines advanced data analytics and behavioral science to help marketers find, understand their audience and engage the most relevant social media micro-influencers at a large scale. SoMin harvests brand-specific life social multimedia streams in a specified market domain, followed by rich analytics and semantic-based influencer search. The Individual User Profiling models extrapolate the key personal characteristics of the brand audience, while the influencer retrieval engine reveals the semantically-matching social media influencers to the platform users. The influencers are matched in terms of both their-posted content and social media audiences, while the evaluation results demonstrate an excellent performance of the proposed recommender framework. By leveraging influencers at a large scale, marketers will be able to execute more effective marketing campaigns of higher trust and at a lower cost.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1234–1236},
numpages = {3},
keywords = {SoMin, social discovery, SoMin.ai, machine learning, influencer marketing, social listening, social media, analytics, artificial intelligence, marketing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241388,
author = {Tang, Taoran and Mao, Hanyang and Jia, Jia},
title = {AniDance: Real-Time Dance Motion Synthesize to the Song},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241388},
doi = {10.1145/3240508.3241388},
abstract = {In this paper, we present a demo named AniDance that can synthesize dance motions with melody in real-time. When users sing a song or play one in their phone to AniDance, their melody will drive the 3D-space character to dance to create a lively dance animation. In practice, we conduct a music oriented 3D-space dance motion dataset by capturing real dance performances, using LSTM-autoencoder to identify the relation between music and dance. Based on these technologies, users can create valid choreographies that capable of musical expression, witch can promote their learning ability and interest in dance and music.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1237–1239},
numpages = {3},
keywords = {lstm, autoencoder, music-dance dataset, multisensory interaction, 3d motion capture, motion synthesis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241389,
author = {Strezoski, Gjorgji and Groenen, Inske and Besenbruch, Jurriaan and Worring, Marcel},
title = {ArtSight: An Artistic Data Exploration Engine},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241389},
doi = {10.1145/3240508.3241389},
abstract = {This technical demo presents ArtSight, a comprehensive query-by-color explorative interface built on top of the large scale artistic dataset OmniArt. Color is of paramount importance in the artistic realm and querying such large data collections by colors that appear in their palette allows for intuitive exploration. This demo allows users to browse the 3 million artwork items in the OmniArt collection by color, and hierarchically filter each result-set by multiple attributes existing in the collection itself. Colors are extracted from the digital photographic reproductions in an unsupervised fashion in palettes of twelve and matched with their meta-data seamlessly to exploit both modalities in our filtering module. The user interaction quality is moderated by a responsive framework with touch capability and an unfoldable interactive 3D sphere visualization offering two exploration options - CompactExplore or GridExplore.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1240–1241},
numpages = {2},
keywords = {artistic data, multi-attribute filtering, information visualization, omniart},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241390,
author = {Park, Yoon Jung and Yang, Yoonsik and Ro, Hyocheol and Byun, JungHyun and Chae, Seougho and Han, Tack Don},
title = {Meet AR-Bot: Meeting Anywhere, Anytime with Movable Spatial AR Robot},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241390},
doi = {10.1145/3240508.3241390},
abstract = {Many kinds of preparations are needed when meeting. For example, projector, laptop, cables and ETC. As such, this video have constructed Meet AR-bot, which helps users to keep meeting going smoothly, based on the projection of Augmented Reality(AR). Our system can easily provide meeting room environment through the movable setting via wheel-based stand. Users do not need to carry a personal laptop and connect them to the projector. Robot reconstructs the 3D geometry information through pan-tilt system and compute projection areas to project information in the space. Users can also control through mobile devices. We offer presentation, table interaction, file sharing and virtual object registration by mobile device.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1242–1243},
numpages = {2},
keywords = {interaction, meeting room, spatial augmented reality},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241391,
author = {Tanno, Ryosuke and Horita, Daichi and Shimoda, Wataru and Yanai, Keiji},
title = {Magical Rice Bowl: A Real-Time Food Category Changer},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241391},
doi = {10.1145/3240508.3241391},
abstract = {In this demo, we demonstrate "Real-time Food Category Change'' based on a Conditional Cycle GAN (cCycle GAN) with a large-scale food image data collected from the Twitter Stream. Conditional Cycle GAN is an extension of CycleGAN, which enables "Food Category Change'' among ten kinds of typical foods served in bowl-type dishes such as beef rice bowl and ramen noodles. The proposed system enables us to change the appearance of a given food photo according to the given category keeping the shape of the given food but exchanging its textures. For training, we used two hundred and thirty thousand food images which achieved very natural food category change among ten kinds of typical Japanese foods: ramen noodle, curry rice, fried rice, beef rice bowl, chilled noodle, spaghetti with meat source, white rice, eel bowl, and fried noodle.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1244–1246},
numpages = {3},
keywords = {conditional cycle gan, food image transformation, food category change, food image generation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241392,
author = {Ren, Haolin and Renoust, Benjamin and Melan\c{c}on, Guy and Viaud, Marie-Luce and Satoh, Shin'ichi},
title = {Exploring Temporal Communities in Mass Media Archives},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241392},
doi = {10.1145/3240508.3241392},
abstract = {One task key to the analysis of large multimedia archive over time is to dynamically monitor the activity of concepts and entities with their interactions. This is helpful to analyze threads of topics over news archives (how stories unfold), or to monitor evolutions and development of social groups. Dynamic graph modeling is a powerful tool to capture these interactions over time, while visualization and finding communities still remain difficult, especially with a high density of links. We propose to extract the backbone of dynamic graphs in order to ease community detection and guide the exploration of trends evolution. Through the graph structure, we interactively coordinate node-link diagrams, Sankey diagrams, time series, and animations in order to extract patterns and follow community behavior. We illustrate our system with the exploration of the role of soccer in 6 years of TV/radio magazines in France, and the role of North Korea in about 10 years of Japanese news.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1247–1249},
numpages = {3},
keywords = {large-scale archive, multimedia analytics, visual analytics, dynamic networks, community detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241393,
author = {Zeppelzauer, Matthias and Ringot, Alexis and Taurer, Florian},
title = {SoniControl - A Mobile Ultrasonic Firewall},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241393},
doi = {10.1145/3240508.3241393},
abstract = {The exchange of data between mobile devices in the near-ultrasonic frequency band is a new promising technology for near field communication (NFC) but also raises a number of privacy concerns. We present the first ultrasonic firewall that reliably detects ultrasonic communication and provides the user with effective means to prevent hidden data exchange. This demonstration showcases a new media-based communication technology ("data over audio") together with its related privacy concerns. It enables users to (i) interactively test out and experience ultrasonic information exchange and (ii) shows how to protect oneself against unwanted tracking.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1250–1252},
numpages = {3},
keywords = {acoustic tracking, audio detection, anomaly detection, privacy protection, data over audio},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241394,
author = {Baig, Mohammed Habibullah and Varghese, Jibin Rajan and Wang, Zhangyang},
title = {MusicMapp: A Deep Learning Based Solution for Music Exploration and Visual Interaction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241394},
doi = {10.1145/3240508.3241394},
abstract = {We present MusicMapp, the world's first large-scale interactive visualization of full-length songs as a point-cloud map, based on high-level features extracted using a customized deep convolutional recurrent neural network (Deep CRNN). MusicMapp will provide the audience with a novel way of experiencing music, opening up new horizons for research and exploration in musicology, regarding how music is perceived, consumed, and interacted with. The demo of MusicMapp will highlight a series of features, including but not limited to: 1) a cloud-based Android App visualizing songs as a point cloud; 2) personalized music exploration and recommendation; and 3) a social-network sharing mechanism built among the users exploring songs.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1253–1255},
numpages = {3},
keywords = {visualization, deep learning, music classification, clustering},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241395,
author = {G\'{o}mez Duran, Paula and Mohedano, Eva and McGuinness, Kevin and Gir\'{o}-i-Nieto, Xavier and O'Connor, Noel E.},
title = {Demonstration of an Open Source Framework for Qualitative Evaluation of CBIR Systems},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241395},
doi = {10.1145/3240508.3241395},
abstract = {Evaluating image retrieval systems in a quantitative way, for example by computing measures like mean average precision, allows for objective comparisons with a ground-truth. However, in cases where ground-truth is not available, the only alternative is to collect feedback from a user. Thus, qualitative assessments become important to better understand how the system works. Visualizing the results could be, in some scenarios, the only way to evaluate the results obtained and also the only opportunity to identify that a system is failing. This necessitates developing a User Interface (UI) for a Content Based Image Retrieval (CBIR) system that allows visualization of results and improvement via capturing user relevance feedback. A well-designed UI facilitates understanding of the performance of the system, both in cases where it works well and perhaps more importantly those which highlight the need for improvement. Our open-source system implements three components to facilitate researchers to quickly develop these capabilities for their retrieval engine. We present: a web-based user interface to visualize retrieval results and collect user annotations; a server that simplifies connection with any underlying CBIR system; and a server that manages the search engine data. The software itself is described in a separate submission to the ACM MM Open Source Software Competition.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1256–1257},
numpages = {2},
keywords = {relevance feedback, latex, visual searching, visualization tool, ACM proceedings, annotation tool, image retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241396,
author = {Cheong, Yun-Gyung and Park, Woo-Hyun and Yu, Hye-Yeon},
title = {A Demonstration of an Intelligent Storytelling System},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241396},
doi = {10.1145/3240508.3241396},
abstract = {Automated story generation has received much attention in the last few decades, as storytelling enhances the user experience in the game, education, and training domains. In this paper, we demonstrate an intelligent story generation system which plans a story using an AI planner. It then displays the story as a 3D animation using the Unity Game engine. The central idea underlying our system lies in the use of a database as a communication channel among sub-modules. The demonstration showcases the complete pipeline process of story generation and visualization where the story changes as the user interacts with the system.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1258–1259},
numpages = {2},
keywords = {3D animation, intelligent story generation, game development},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241397,
author = {Bu, Yaohua and Jia, Jia and Li, Xiang and Zhou, Suping and Lu, Xiaobo},
title = {IcooBook: When the Picture Book for Children Encounters Aesthetics of Interaction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241397},
doi = {10.1145/3240508.3241397},
abstract = {In this work, we propose a novel PCA (Perception &amp; Cognition &amp; Affection) model from the prospective of aesthetics in interaction. Based on PCA, we establish a new electronic interactive picture book for children, named IcooBook. At the first level of perception, the proposed IcooBook provides interfaces of multi-sensory interaction; at the second level of cognition, IcooBook builds immersive interactive scenes; at the third level of affection, IcooBook creates high-level interaction modes based on automatic emotion recognition. The research on user study had proved the effectiveness of IcooBook in helping children being focusing on reading, getting better understanding about the context, and further encouraging children to appreciate the beauty of deep affective interaction.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1260–1262},
numpages = {3},
keywords = {aesthetics of interaction, affection computing, electronic picture books},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241398,
author = {Forgione, Thomas and Carlier, Axel and Morin, G\'{e}raldine and Ooi, Wei Tsang and Charvillat, Vincent and Yadav, Praveen Kumar},
title = {An Implementation of a DASH Client for Browsing Networked Virtual Environment},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241398},
doi = {10.1145/3240508.3241398},
abstract = {We demonstrate the use of DASH, a widely-deployed standard for streaming video content, for streaming 3D content in an NVE (Networked Virtual Environment) consisting of 3D geometry and associated textures. We have developed a DASH client for NVE to show how NVE benefits from the advantages of DASH: it offers a scalable, easy-to-deploy 3D streaming framework. In our system, the 3D content is first statically partitioned into compliant DASH data, and metadata is provided in order for the client to manage which data to download. Based on a proposed utility metric for geometry and texture at the different resolution, the client can choose the content to request depending on its viewpoint. We effectively provide a Web-based client to navigate through our sample 3D scene, while deriving the streaming requests from its computation of the necessary online parameters, in a receiver-driven manner.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1263–1264},
numpages = {2},
keywords = {networked virtual environment, 3D streaming, DASH},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241399,
author = {Liao, Lizi and Zhou, You and Ma, Yunshan and Hong, Richang and Chua, Tat-Seng},
title = {Knowledge-Aware Multimodal Fashion Chatbot},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241399},
doi = {10.1145/3240508.3241399},
abstract = {Multimodal fashion chatbot provides a natural and informative way to fulfill customers' fashion needs. However, making it 'smart' in generating substantive responses remains a challenging problem. In this paper, we present a multimodal domain knowledge enriched fashion chatbot. It forms a taxonomy-based learning module to capture the fine-grained semantics in images and leverages an end-to-end neural conversational model to generate responses based on the conversation history, visual semantics, and domain knowledge. To avoid inconsistent dialogues, deep reinforcement learning method is used to further optimize the model.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1265–1266},
numpages = {2},
keywords = {fashion chatbot, multimodal knowledge},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241400,
author = {Lee, Alex and Kwak, Chang-Uk and Son, Jeong-Woo and Kim, Sun-Joong},
title = {SVIAS: Scene-Segmented Video Information Annotation System},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241400},
doi = {10.1145/3240508.3241400},
abstract = {We had designed the scene-segmented video information annotation system using video segmentation and information annotation. For video segmentation, the proposed system adapts the multiview deep convolution neural network. Segmented scenes are annotated by using the unsupervised sentence embedding model for closed captions. Both functionalities effectively work together with the web interface designed to tie not only our functionalities but also external content providers.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1267–1269},
numpages = {3},
keywords = {video segmentation, deep learning, information annotation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241401,
author = {Kwak, Chang-Uk and Han, Min-Ho and Kim, Sun-Joong and Hahm, Gyeong-June},
title = {Interactive Story Maker: Tagged Video Retrieval System for Video Re-Creation Service},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241401},
doi = {10.1145/3240508.3241401},
abstract = {Users who want to reuse existing videos and re-create new videos need a video retrieval system that searches relevant video clips from a vast amount of video clips. In addition, a re-creation tool is needed that allows users to arrange videos according to the user defined story line using the retrieved videos. Current video retrieval services are based on simple tags, they have potential limitations on searching relevant videos for natural language queries. That is, if you enter a scenario that describes a scene as a query, there is a limitation in getting the appropriate retrieval results. To address these problems, this paper introduces a system that performs query preprocessing and expansions and provides users with retrieval results for a natural language query. In addition, we introduce a web based re-creation tool that can construct story line using retrieved videos and play a re-created video.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1270–1271},
numpages = {2},
keywords = {video retrieval, video re-creation, query processing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241402,
author = {Liu, Xingyu and Guo, Jingfan and Ren, Tongwei and Han, Yahong and Huang, Lei and Wu, Gangshan},
title = {HeterStyle: A Heterogeneous Video Style Transfer Application},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241402},
doi = {10.1145/3240508.3241402},
abstract = {Video style transfer aims to synthesize a stylized video that preserves the content of a given video and is rendered in the style of a reference image.A key issue in video style transfer is how to balance video content preservation and reference style rendering, in order to avoid over-stylization with serious video content loss or under-stylization with unrecognized reference style. In this demonstration, we illustrate a novel video style transfer application, named HeterStyle, which can stylize different regions in the video with adaptive intensities.The core algorithm of HeterStyle application is our proposed heterogeneous video style transfer method, which minimizes a heterogeneous style transfer loss function considering content, style and temporal consistency in a Convolutional Neural Networks based optimization framework.With the HeterStyle application, a user can easily generate the stylized videos with good video content preservation and reference style rendering.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1272–1273},
numpages = {2},
keywords = {optical flow, video style transfer, heterogeneous stylization, salient object detection, convolutional neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241359,
author = {Ro, Hyocheol and Kim, Inhwan and Byun, JungHyun and Yang, Yoonsik and Park, Yoon Jung and Chae, Seungho and Han, Tackdon},
title = {PAMI: Projection Augmented Meeting Interface for Video Conferencing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241359},
doi = {10.1145/3240508.3241359},
abstract = {Video conferencing, which helps gather opinions and make decisions quickly among employees who are not in the same location, is now a very important communication tool in the workplace. Our research is one of these video conferencing solutions, specifically proposed to address the difficulties of analog materials sharing and feedback, and has added some useful features for the smooth use of conference participants. We conducted a comparative experiment on our proposed method of file sharing and the method that we had previously used in video conferencing. As a result, the proposed system yielded better results in terms of time and usability during a full-scale collaborative situation in which feedback was provided.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1274–1277},
numpages = {4},
keywords = {collaboration, projection augmented reality, video conferencing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241362,
author = {Park, Yoon Jung and Yang, Yoonsik and Ro, Hyocheol and Cha, Jinwon and Kim, Kyuri and Han, Tack Don},
title = {ChildAR-Bot: Educational Playing Projection-Based AR Robot for Children},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241362},
doi = {10.1145/3240508.3241362},
abstract = {Children encounter a variety of experiences through play, which can improve their ability to form ideas and undergo multi-faceted development. Using Augmented Reality (AR) technology to integrate various digital learning elements with real environments can lead to increased learning ability. This study proposes a 360° rotatable and portable system specialized for education and development through projection-based AR play. This system allows existing projection-based AR technology, which once could only be experienced at large-scale exhibitions and experience centers, to be used in individual and small-scale spaces. It also promotes the development of multi-sensory abilities through a multi-modality which provides various intuitive and sensory interactions. By experiencing the various educational play applications provided by the proposed system, children can increase their physical, perceptive, and emotional abilities and thinking skills.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1278–1282},
numpages = {5},
keywords = {augmented reality, educational play, spatial augmented reality, multi-modal interaction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286931,
author = {Jin, Qin},
title = {Session Details: Deep-2 (Recognition)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286931},
doi = {10.1145/3286931},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240576,
author = {Tang, Yansong and Wang, Zian and Li, Peiyang and Lu, Jiwen and Yang, Ming and Zhou, Jie},
title = {Mining Semantics-Preserving Attention for Group Activity Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240576},
doi = {10.1145/3240508.3240576},
abstract = {In this paper, we propose a Semantics-Preserving Teacher-Student (SPTS) model for group activity recognition in videos, which aims to mine the semantics-preserving attention to automatically seek the key people and discard the misleading people. Conventional methods usually aggregate the features extracted from individual persons by pooling operations, which cannot fully explore the contextual information for group activity recognition. To address this, our SPTS networks first learn a Teacher Network in semantic domain, which classifies the word of group activity based on the words of individual actions. Then we carefully design a Student Network in vision domain, which recognizes the group activity according to the input videos, and enforce the Student Network to mimic the Teacher Network during the learning process. In this way, we allocate semantics-preserving attention to different people, which adequately explores the contextual information of different people and requires no extra labelled data. Experimental results on two widely used benchmarks for group activity recognition clearly show the superior performance of our method in comparisons with the state-of-the-arts.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1283–1291},
numpages = {9},
keywords = {semantic-preserving, attention, teacher-student networks, group activity recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240572,
author = {Yan, Rui and Tang, Jinhui and Shu, Xiangbo and Li, Zechao and Tian, Qi},
title = {Participation-Contributed Temporal Dynamic Model for Group Activity Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240572},
doi = {10.1145/3240508.3240572},
abstract = {Group activity recognition, a challenging task that a number of individuals occur in the scene of activity while only a small subset of them participate in, has received increasing attentions. However, most of the previous methods model all the individuals' actions equivalently while ignoring a fact that not all of them are contributed to the discrimination of group activity. That is to say, only a small number of key actors (participants) play important roles in the whole group activity. Inspired by this, we explore a new "One to Key" idea to progressively aggregate temporal dynamics of key actors with different participation degrees over time from each person. Here, we focus on two types of key actors in the whole activity, who steadily move in the whole process (long moving time) or intensely move (but closely related to the group activity) at a significant moment. Based on this, we propose a novel Participation-Contributed Temporal Dynamic Model (PC-TDM) to recognize group activity, which mainly consists of a "One" network and a "One to Key" network. Specifically, "One" network aims at modeling the individual dynamic of each person. "One to Key" network feeds the outputs from the "One" network into a Bidirectional LSTM (Bi-LSTM) according to the order of individual's moving time. Subsequently, each output state of Bi-LSTM weighted by a trainable time-varying attention factor is aggregated by going through LSTM one-by-one. Experimental results on two benchmarks demonstrate that the proposed method improves group activity recognition performance compared to the state-of-the-arts.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1292–1300},
numpages = {9},
keywords = {long short term memory, video analysis, group activity recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240616,
author = {Zhuang, Peiqin and Wang, Yali and Qiao, Yu},
title = {WildFish: A Large Benchmark for Fish Recognition in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240616},
doi = {10.1145/3240508.3240616},
abstract = {Fish recognition is an important task to understand the marine ecosystem and biodiversity. It is often challenging to identify fish species in the wild, due to the following difficulties. First, most fish benchmarks are small-scale, which may limit the representation power of machine learning models. Second, the number of fish species is huge, and there may still exist unknown categories in our planet. The traditional classifiers often fail to deal with this open-set scenario. Third, certain fish species are highly-confused. It is often hard to figure out the subtle differences, only by the unconstrained images. Motivated by these facts, we introduce a large-scale WildFish benchmark for fish recognition in the wild. Specifically, we make three contributions in this paper. First, WildFish is the largest image data set for wild fish recognition, to our best knowledge. It consists of 1000 fish categories with 54,459 unconstrained images, allowing to train high-capacity models for automatic fish classification. Second, we propose a novel open-set fish classification task for realistic scenarios, and investigate the open-set deep learning framework with a number of practical designs. Third, we propose a novel fine-grained recognition task, with the guidance of pairwise textual descriptions. Via leveraging the comparison knowledge in the sentence, we design a multi-modal fish net to effectively distinguish two confused categories in a pair. Finally, we release WildFish (https://github.com/PeiqinZhuang/WildFish), in order to bring benefit to more research studies in multimedia and beyond.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1301–1309},
numpages = {9},
keywords = {fine-grained recognition, open-set classification, vision-text modeling, deep learning, fish classification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240702,
author = {You, Haoxuan and Feng, Yifan and Ji, Rongrong and Gao, Yue},
title = {PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240702},
doi = {10.1145/3240508.3240702},
abstract = {3D object recognition has attracted wide research attention in the field of multimedia and computer vision. With the recent proliferation of deep learning, various deep models with different representations have achieved the state-of-the-art performance. Among them, point cloud and multi-view based 3D shape representations are promising recently, and their corresponding deep models have shown significant performance on 3D shape recognition. However, there is little effort concentrating point cloud data and multi-view data for 3D shape representation, which is, in our consideration, beneficial and compensated to each other. In this paper, we propose the Point-View Network (PVNet), the first framework integrating both the point cloud and the multi-view data towards joint 3D shape recognition. More specifically, an embedding attention fusion scheme is proposed that could employ high-level features from the multi-view data to model the intrinsic correlation and discriminability of different structure features from the point cloud data. In particular, the discriminative descriptions are quantified and leveraged as the soft attention mask to further refine the structure feature of the 3D shape. We have evaluated the proposed method on the ModelNet40 dataset for 3D shape classification and retrieval tasks. Experimental results and comparisons with state-of-the-art methods demonstrate that our framework can achieve superior performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1310–1318},
numpages = {9},
keywords = {point cloud, point-view net, 3d shape recognition, multi-view},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286932,
author = {Ji, Rongrong},
title = {Session Details: Multimedia-2 (Socical &amp; Emotional Multimedia)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286932},
doi = {10.1145/3286932},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240591,
author = {Zhao, Sicheng and Zhao, Xin and Ding, Guiguang and Keutzer, Kurt},
title = {EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240591},
doi = {10.1145/3240508.3240591},
abstract = {Deep neural networks have performed well on various benchmark vision tasks with large-scale labeled training data; however, such training data is expensive and time-consuming to obtain. Due to domain shift or dataset bias, directly transferring models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain often results in poor performance. In this paper, we consider the domain adaptation problem in image emotion recognition. Specifically, we study how to adapt the discrete probability distributions of image emotions from a source domain to a target domain in an unsupervised manner. We develop a novel adversarial model for emotion distribution learning, termed EmotionGAN, which alternately optimizes the Generative Adversarial Network (GAN) loss, semantic consistency loss, and regression loss. The EmotionGAN model can adapt source domain images such that they appear as if they were drawn from the target domain, while preserving the annotation information. Extensive experiments are conducted on the FlickrLDL and TwitterLDL datasets, and the results demonstrate the superiority of the proposed method as compared to state-of-the-art approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1319–1327},
numpages = {9},
keywords = {visual emotions, distribution learning, unsupervised domain adaptation, gan, semantic consistency},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240635,
author = {Lv, Pei and Wang, Meng and Xu, Yongbo and Peng, Ze and Sun, Junyi and Su, Shimei and Zhou, Bing and Xu, Mingliang},
title = {USAR: An Interactive User-Specific Aesthetic Ranking Framework for Images},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240635},
doi = {10.1145/3240508.3240635},
abstract = {When assessing whether an image is of high or low quality, it is indispensable to take personal preference into account. Existing aesthetic models lay emphasis on hand-crafted features or deep features commonly shared by high quality images, but with limited or no consideration for personal preference and user interaction. To that end, we propose a novel and user-friendly aesthetic ranking framework via powerful deep neural network and a small amount of user interaction, which can automatically estimate and rank the aesthetic characteristics of images in accordance with users' preference. Our framework takes as input a series of photos that users prefer, and produces as output a reliable, user-specific aesthetic ranking model matching with users' preference. Considering the subjectivity of personal preference and the uncertainty of user's single selection, a unique and exclusive dataset will be constructed interactively to describe the preference of one individual by retrieving the most similar images with regard to those specified by users. Based on this unique user-specific dataset and sufficient well-designed aesthetic attributes, a customized aesthetic distribution model can be learned, which concatenates both personalized preference and aesthetic rules. We conduct extensive experiments and user studies on two large-scale public datasets, and demonstrate that our framework outperforms those work based on conventional aesthetic assessment or ranking model.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1328–1336},
numpages = {9},
keywords = {aesthetic assessment, user-specific, deep learning, ranking model},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240707,
author = {Sabir, Ekraam and AbdAlmageed, Wael and Wu, Yue and Natarajan, Prem},
title = {Deep Multimodal Image-Repurposing Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240707},
doi = {10.1145/3240508.3240707},
abstract = {Nefarious actors on social media and other platforms often spread rumors and falsehoods through images whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/falsehood. This type of modification is referred to as image repurposing, in which often an unmanipulated image is published along with incorrect or manipulated metadata to serve the actor's ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR) dataset, a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr. We also present a novel, end-to-end, deep multimodal learning model for assessing the integrity of an image by combining information extracted from the image with related information from a knowledge base. The proposed method is compared against state-of-the-art techniques on existing datasets as well as MEIR, where it outperforms existing methods across the board, with AUC improvement up to 0.23.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1337–1345},
numpages = {9},
keywords = {multi-task learning, rumor detection, deep learning, computer vision, fake news},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240608,
author = {Pan, Bowen and Wang, Shangfei},
title = {Facial Expression Recognition Enhanced by Thermal Images through Adversarial Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240608},
doi = {10.1145/3240508.3240608},
abstract = {Currently, fusing visible and thermal images for facial expression recognition requires two modalities during both training and testing. Visible cameras are commonly used in real-life applications, and thermal cameras are typically only available in lab situations due to their high price. Thermal imaging for facial expression recognition is not frequently used in real-world situations. To address this, we propose a novel thermally enhanced facial expression recognition method which uses thermal images as privileged information to construct better visible feature representation and improved classifiers by incorporating adversarial learning and similarity constraints during training. Specifically, we train two deep neural networks from visible images and thermal images. We impose adversarial loss to enforce statistical similarity between the learned representations of two modalities, and a similarity constraint to regulate the mapping functions from visible and thermal representation to expressions. Thus, thermal images are leveraged to simultaneously improve visible feature representation and classification during training. To mimic real-world scenarios, only visible images are available during testing. We further extend the proposed expression recognition method for partially unpaired data to explore thermal images' supplementary role in visible facial expression recognition when visible images and thermal images are not synchronously recorded. Experimental results on the MAHNOB Laughter database demonstrate that our proposed method can effectively regularize visible representation and expression classifiers with the help of thermal images, achieving state-of-the-art recognition performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1346–1353},
numpages = {8},
keywords = {facial expression recognition, adversarial learning, privileged information},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286933,
author = {Sang, Jitao and Yu, Jun},
title = {Session Details: Panel-1},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286933},
doi = {10.1145/3286933},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243931,
author = {Sang, Jitao and Yu, Jun and Jain, Ramesh and Lienhart, Rainer and Cui, Peng and Feng, Jiashi},
title = {Deep Learning for Multimedia: Science or Technology?},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243931},
doi = {10.1145/3240508.3243931},
abstract = {Deep learning has been successfully explored in addressing different multimedia topics recent years, ranging from object detection, semantic classification, entity annotation, to multimedia captioning, multimedia question answering and storytelling. Open source libraries and platforms such as Tensorflow, Caffe, MXnet significantly help promote the wide deployment of deep learning in solving real-world applications. On one hand, deep learning practitioners, while not necessary to understand the involved math behind, are able to set up and make use of a complex deep network. One recent deep learning tool based on Keras even provides the graphical interface to enable straightforward 'drag and drop' operation for deep learning programming. On the other hand, however, some general theoretical problems of learning such as the interpretation and generalization, have only achieved limited progress. Most deep learning papers published these days follow the pipeline of designing/modifying network structures - tuning parameters - reporting performance improvement in specific applications. We have even seen many deep learning application papers without one single equation. Theoretical interpretation and the science behind the study are largely ignored. While excited about the successful application of deep learning in classical and novel problems, we multimedia researchers are responsible to think and solve the fundamental topics in deep learning science. Prof. Guanrong Chen recently wrote an editorial note titled 'Science and Technology, not SciTech' [1]. This panel falls into similar discussion and aims to invite prestigious multimedia researchers and active deep learning practitioners to discuss the positioning of deep learning research now and in the future. Specifically, each panelist is asked to present their opinions on the following five questions: 1)How do you think the current phenomenon that deep learning applications are explosively growing, while the general theoretical problems remain slow progress? 2)Do you agree that deployment of deep learning techniques is getting easy (with a low barrier), while deep learning research is difficult (with a high barrier) 3)What do you think are the core problems for deep learning techniques? 4)What do you think are the core problems for deep learning science? 5)What's your suggestion on the multimedia research in the post-deep learning era?},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1354–1355},
numpages = {2},
keywords = {application, theory, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286934,
author = {Hu, Min-Chun},
title = {Session Details: Open Source Software Competition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286934},
doi = {10.1145/3286934},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243653,
author = {Lai, Kuan-Ting and Lin, Chia-Chih and Kang, Chun-Yao and Liao, Mei-Enn and Chen, Ming-Syan},
title = {VIVID: Virtual Environment for Visual Deep Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243653},
doi = {10.1145/3240508.3243653},
abstract = {Due to the advances in deep reinforcement learning and the demand of large training data, virtual-to-real learning has gained lots of attention from computer vision community recently. As state-of-the-art 3D engines can generate photo-realistic images suitable for training deep neural networks, researchers have been gradually applied 3D virtual environment to learn different tasks including autonomous driving, collision avoidance, and image segmentation, to name a few. Although there are already many open-source simulation environments readily available, most of them either provide small scenes or have limited interactions with objects in the environment. To facilitate visual recognition learning, we present a new Virtual Environment for Visual Deep Learning (VIVID), which offers large-scale diversified indoor and outdoor scenes. Moreover, VIVID leverages the advanced human skeleton system, which enables us to simulate numerous complex human actions. VIVID has a wide range of applications and can be used for learning indoor navigation, action recognition, event detection, etc. We also release several deep learning examples in Python to demonstrate the capabilities and advantages of our system.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1356–1359},
numpages = {4},
keywords = {event detection, visual recognition, deep learning, virtual reality, autonomous navigation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243654,
author = {Huang, Tsung-Wei and Lin, Chun-Xun and Guo, Guannan and Wong, Martin D. F.},
title = {A General-Purpose Distributed Programming System Using Data-Parallel Streams},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243654},
doi = {10.1145/3240508.3243654},
abstract = {In this paper we present DtCraft, a distributed execution engine that enables a new powerful programming model to streamline cluster computing. Applications are described in a set of data-parallel streams, leaving difficult execution details and concurrency controls handled by our system kernel transparently. Compared with existing systems, DtCraft is unique in (1) an efficient stream-oriented programming paradigm using modern C++17, (2) an in-context resource controller and task executor based on Linux container technology, and (3) ease of development from prototyping machines to production cloud environments. These capabilities power industry applications and create new research directions in machine learning, stream processing, and distributed multimedia systems.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1360–1363},
numpages = {4},
keywords = {stream processing, distributed system, machine learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243655,
author = {Zampogiannis, Konstantinos and Fermuller, Cornelia and Aloimonos, Yiannis},
title = {Cilantro: A Lean, Versatile, and Efficient Library for Point Cloud Data Processing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243655},
doi = {10.1145/3240508.3243655},
abstract = {We introduce Cilantro, an open-source C++ library for geometric and general-purpose point cloud data processing. The library provides functionality that covers low-level point cloud operations, spatial reasoning, various methods for point cloud segmentation and generic data clustering, flexible algorithms for robust or local geometric alignment, model fitting, as well as powerful visualization tools. To accommodate all kinds of workflows, Cilantro is almost fully templated, and most of its generic algorithms operate in arbitrary data dimension. At the same time, the library is easy to use and highly expressive, promoting a clean and concise coding style. Cilantro is highly optimized, has a minimal set of external dependencies, and supports rapid development of performant point cloud processing software in a wide variety of contexts.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1364–1367},
numpages = {4},
keywords = {clustering, geometric registration, point cloud processing, model fitting, spatial reasoning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243656,
author = {Pizenberg, Matthieu and Carlier, Axel and Faure, Emmanuel and Charvillat, Vincent},
title = {Web-Based Configurable Image Annotations},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243656},
doi = {10.1145/3240508.3243656},
abstract = {We introduce a new application for annotating images, with the purpose of constituting training datasets for machine learning algorithms. Our open-source software is meant to be easily used and deployed, configured to meet the annotation needs of any use case, and embeddable in crowdsourcing campaigns using the Amazon Mechanical Turk service.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1368–1371},
numpages = {4},
keywords = {open source software, annotation, dataset},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286935,
author = {Nie, Liqiang},
title = {Session Details: Vision-3 (Applications in Multimedia)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286935},
doi = {10.1145/3286935},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240557,
author = {He, Xiangteng and Peng, Yuxin},
title = {Only Learn One Sample: Fine-Grained Visual Categorization with One Sample Training},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240557},
doi = {10.1145/3240508.3240557},
abstract = {The progress of fine-grained visual categorization (FGVC) benefits from the application of deep neural networks, especially convolutional neural networks (CNNs), which heavily rely on large amounts of labeled data for training. However, it is hard to obtain the accurate labels of similar fine-grained subcategories because labeling needs professional knowledge, which is labor-consuming and time-consuming. Therefore, it is appealing and significant to recognize these similar fine-grained subcategories with a few labeled samples or even only one for training, which is a highly challenging task. In this paper, we propose OLOS (Only Learn One Sample), a new data augmentation approach for fine-grained visual categorization with only one sample training, and its main novelties are: (1) A 4-stage data augmentation approach is proposed to increase both the volume and variety of the one training image, which provides more visual information with multiple views and scales. It consists of a 2-stage data generation and a 2-stage data selection. (2) The 2-stage data generation approach is proposed to produce image patches relevant to the object and its parts for the one training image, as well as produce new images conditioned on the textual descriptions of the training image. (3) The 2-stage data selection approach is proposed to conduct screening on the generated images in order that useful information is remained and noisy information is eliminated. Experimental results and analyses on fine-grained visual categorization benchmark demonstrate that our proposed OLOS approach can be applied on top of existing methods, and improves their categorization performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1372–1380},
numpages = {9},
keywords = {one sample training, data selection, data augmentation, data generation, fine-grained visual categorization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240628,
author = {Zheng, Kecheng and Zha, Zheng-Jun and Cao, Yang and Chen, Xuejin and Wu, Feng},
title = {LA-Net: Layout-Aware Dense Network for Monocular Depth Estimation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240628},
doi = {10.1145/3240508.3240628},
abstract = {Depth estimation from monocular images is an ill-posed and inherently ambiguous problem. Recently, deep learning technique has been applied for monocular depth estimation seeking data-driven solutions. However, most existing methods focus on pursuing the minimization of average depth regression error at pixel level and neglect to encode the global layout of scene, resulting in layout-inconsistent depth map. This paper proposes a novel Layout-Aware Convolutional Neural Network (LA-Net) for accurate monocular depth estimation by simultaneously perceiving scene layout and local depth details. Specifically, a Spatial Layout Network (SL-Net) is proposed to learn a layout map representing the depth ordering between local patches. A Layout-Aware Depth Estimation Network (LDE-Net) is proposed to estimate pixel-level depth details using multi-scale layout maps as structural guidance, leading to layout-consistent depth map. A dense network module is used as the base network to learn effective visual details resorting to dense feed-forward connections. Moreover, we formulate an order-sensitive softmax loss to well constrain the ill-posed depth inferring problem. Extensive experiments on both indoor scene (NYUD-v2) and outdoor scene (Make3D) datasets have demonstrated that the proposed LA-Net outperforms the state-of-the-art methods and leads to faithful 3D projections.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1381–1388},
numpages = {8},
keywords = {monocular depth estimation, scene layout, 3d project, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240690,
author = {Huang, Ziqing and Liu, Shiguang},
title = {Robustness and Discrimination Oriented Hashing Combining Texture and Invariant Vector Distance},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240690},
doi = {10.1145/3240508.3240690},
abstract = {Image hashing is a novel technology of multimedia processing with wide applications. Robustness and discrimination are two of the most important objectives of image hashing. Different from existing hashing methods without a good balance with respect to robustness and discrimination, which largely restrict the application in image retrieval and copy detection, i.e., seriously reducing the retrieval accuracy of similar images, we propose a new hashing method which can preserve two kinds of complementary features (global feature via texture and local feature via DCT coefficients) to achieve a good balance between robustness and discrimination. Specifically, the statistical characteristics in gray-level co-occurrence matrix (GLCM) are extracted to well reveal the texture changes of an image, which is of great benefit to improve the perceptual robustness. Then, the normalized image is divided into image blocks, and the dominant DCT coefficients in the first row/column are selected to form a feature matrix. The Euclidean distance between vectors of the feature matrix is invariant to commonly-used digital operations, which helps make hash more compact. Various experiments show that our approach achieves a better balance between robustness and discrimination than the state-of-the-art algorithms.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1389–1397},
numpages = {9},
keywords = {image hashing, invariant vector distance, dominant dct coefficients, robustness and discrimination},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240535,
author = {Wang, Shuhui and Chen, Yangyu and Zhuo, Junbao and Huang, Qingming and Tian, Qi},
title = {Joint Global and Co-Attentive Representation Learning for Image-Sentence Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240535},
doi = {10.1145/3240508.3240535},
abstract = {In image-sentence retrieval task, correlated images and sentences involve different levels of semantic relevance. However, existing multi-modal representation learning paradigms fail to capture the meaningful component relation on word and phrase level, while the attention-based methods still suffer from component-level mismatching and huge computation burden. We propose a Joint Global and Co-Attentive Representation learning method (JGCAR) for image-sentence retrieval. We formulate a global representation learning task which utilizes both intra-modal and inter-modal relative similarity to optimize the semantic consistency of the visual/textual component representations. We further develop a co-attention learning procedure to fully exploit different levels of visual-linguistic relations. We design a novel softmax-like bi-directional ranking loss to learn the co-attentive representation for image-sentence similarity computation. It is capable of discovering the correlative components and rectifying inappropriate component-level correlation to produce more accurate sentence-level ranking results. By joint global and co-attentive representation learning, the latter benefits from the former by producing more semantically consistent component representation, and the former also benefits from the latter by back-propagating the contextual information. Image-sentence retrieval is performed as a two-step process in the testing stage, inheriting advantages on both effectiveness and efficiency. Experiments show that JGCAR outperforms existing methods on MSCOCO and Flickr30K image-sentence retrieval tasks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1398–1406},
numpages = {9},
keywords = {cross-modal representation learning, image-sentence retrieval, joint learning, co-attentive representation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286936,
author = {Yamasaki, Toshihiko},
title = {Session Details: Multimodal-2 (Cross-Modal Translation)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286936},
doi = {10.1145/3286936},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240559,
author = {Yuan, Mingkuan and Peng, Yuxin},
title = {Text-to-Image Synthesis via Symmetrical Distillation Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240559},
doi = {10.1145/3240508.3240559},
abstract = {Text-to-image synthesis aims to automatically generate images according to text descriptions given by users, which is a highly challenging task. The main issues of text-to-image synthesis lie in two gaps: the heterogeneous and homogeneous gaps. The heterogeneous gap is between the high-level concepts of text descriptions and the pixel-level contents of images, while the homogeneous gap exists between synthetic image distributions and real image distributions. For addressing these problems, we exploit the excellent capability of generic discriminative models (e.g. VGG19), which can guide the training process of a new generative model on multiple levels to bridge the two gaps. The high-level representations can teach the generative model to extract necessary visual information from text descriptions, which can bridge the heterogeneous gap. The mid-level and low-level representations can lead it to learn structures and details of images respectively, which relieves the homogeneous gap. Therefore, we propose Symmetrical Distillation Networks (SDN) composed of a source discriminative model as "teacher" and a target generative model as "student". The target generative model has a symmetrical structure with the source discriminative model, in order to transfer hierarchical knowledge accessibly. Moreover, we decompose the training process into two stages with different distillation paradigms for promoting the performance of the target generative model. Experiments on two widely-used datasets are conducted to verify the effectiveness of our proposed SDN.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1407–1415},
numpages = {9},
keywords = {distillation network, knowledge transferring, text-to-image synthesis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240632,
author = {Liu, Daqing and Zha, Zheng-Jun and Zhang, Hanwang and Zhang, Yongdong and Wu, Feng},
title = {Context-Aware Visual Policy Network for Sequence-Level Image Captioning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240632},
doi = {10.1145/3240508.3240632},
abstract = {Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the "exposure bias'' during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (eg, visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (eg, "man riding horse'') and comparisons (eg. "smaller cat"). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at urlhttps://github.com/daqingliu/CAVP},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1416–1424},
numpages = {9},
keywords = {visual context, reinforcement learning, policy network, image captioning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240667,
author = {Liu, Sheng and Ren, Zhou and Yuan, Junsong},
title = {SibNet: Sibling Convolutional Encoder for Video Captioning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240667},
doi = {10.1145/3240508.3240667},
abstract = {Video captioning is a challenging task owing to the complexity of understanding the copious visual information in videos and describing it using natural language. Different from previous work that encodes video information using a single flow, in this work, we introduce a novel Sibling Convolutional Encoder (SibNet) for video captioning, which utilizes a two-branch architecture to collaboratively encode videos. The first content branch encodes the visual content information of the video via autoencoder, and the second semantic branch encodes the semantic information by visual-semantic joint embedding. Then both branches are effectively combined with soft-attention mechanism and finally fed into a RNN decoder to generate captions. With our SibNet explicitly capturing both content and semantic information, the proposed method can better represent the rich information in videos. Extensive experiments on YouTube2Text and MSR-VTT datasets validate that the proposed architecture outperforms existing methods by a large margin across different evaluation metrics.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1425–1434},
numpages = {10},
keywords = {visual semantic joint embedding, video captioning, autoencoder},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240695,
author = {Che, Wenbin and Fan, Xiaopeng and Xiong, Ruiqin and Zhao, Debin},
title = {Paragraph Generation Network with Visual Relationship Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240695},
doi = {10.1145/3240508.3240695},
abstract = {Paragraph generation of images is a new concept, aiming to produce multiple sentences to describe a given image. In this paper, we propose a paragraph generation network with introducing visual relationship detection. We first detect regions which may contain important visual objects and then predict their relationships. Paragraphs are produced based on object regions which have valid relationship with others. Compared with previous works which generate sentences based on region features, we explicitly explore and utilize visual relationships in order to improve final captions. The experimental results show that such strategy could improve paragraph generating performance from two aspects: more details about object relations are detected and more accurate sentences are obtained. Furthermore, our model is more robust to region detection fluctuation.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1435–1443},
numpages = {9},
keywords = {relationship detection, object detection, paragraph generation, image caption},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286937,
author = {Cheng, Wen-Huang and Liu, Jiaying},
title = {Session Details: Panel-2},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286937},
doi = {10.1145/3286937},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243932,
author = {Cheng, Wen-Huang and Liu, Jiaying and Kankanhalli, Mohan S. and El Saddik, Abdulmotaleb and Huet, Benoit},
title = {AI + Multimedia Make Better Life?},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243932},
doi = {10.1145/3240508.3243932},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1455–1456},
numpages = {2},
keywords = {life, multimedia, panel, artificial intelligence},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286938,
author = {Li, Xirong},
title = {Session Details: FF-5},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286938},
doi = {10.1145/3286938},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240663,
author = {Jiang, Na and Bai, SiChen and Xu, Yue and Xing, Chang and Zhou, Zhong and Wu, Wei},
title = {Online Inter-Camera Trajectory Association Exploiting Person Re-Identification and Camera Topology},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240663},
doi = {10.1145/3240508.3240663},
abstract = {Online inter-camera trajectory association is a promising topic in intelligent video surveillance, which concentrates on associating trajectories belong to the same individual across different cameras according to time. It remains challenging due to the inconsistent appearance of a person in different cameras and the lack of spatio-temporal constraints between cameras. Besides, the orientation variations and the partial occlusions significantly increase the difficulty of inter-camera trajectory association. Targeting to solve these problems, this work proposes an orientation-driven person re-identification (ODPR) and an effective camera topology estimation based on appearance features for online inter-camera trajectory association. ODPR explicitly leverages the orientation cues and stable torso features to learn discriminative feature representations for identifying trajectories across cameras, which alleviates the pedestrian orientation variations by the designed orientation-driven loss function and orientation aware weights. The effective camera topology estimation introduces appearance features to generate the correct spatio-temporal constraints for narrowing the retrieval range, which improves the time efficiency and provides the possibility for intelligent inter-camera trajectory association in large-scale surveillance environments. Extensive experimental results demonstrate that our proposed approach significantly outperforms most state-of-the-art methods on the popular person re-identification datasets and the public multi-target, multi-camera tracking benchmark.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1457–1465},
numpages = {9},
keywords = {camera topology estimation, inter-camera trajectory association, person re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240666,
author = {Zhu, Jing and Fang, Yi},
title = {Learning Local Descriptors with Adversarial Enhancer from Volumetric Geometry Patches},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240666},
doi = {10.1145/3240508.3240666},
abstract = {Local matching problems (e.g. key point matching, geometry registration) are significant but challenging tasks in computer vision field. In this paper, we propose to learn a robust local 3D descriptor from volumetric point patches to tackle the local matching tasks. Intuitively, given two inputs, it would be easy for a network to map the inputs to a space with similar characteristics (e.g. similar outputs for similar inputs, far different outputs for far different inputs), but the difficult case for a network would be to map the inputs into a space with opposite characteristics (e.g. far different outputs for very similar inputs but very similar outputs for far different inputs). Inspired by this intuition, in our proposed method, we design a siamese-network-based local descriptor generator to learn a local descriptor with small distances between match pairs and large distances between non-match pairs. Specifically, an adversarial enhancer is introduced to map the outputs of the local descriptor generator into an opposite space that match pairs have the maximum differences and non-match pairs have the minimum differences. The local descriptor generator and the adversarial enhancer are trained in an adversarial manner. By competing with the adversarial enhancer, the local descriptor generator learns to generate a much stronger descriptor for given volumetric point patches. The experiments conducted on real-world scan datasets, including 7-scenes and SUN3D, and the synthetic scan augmented ICL-NUIM dataset show that our method can achieve superior performance over other state-of-the-art approaches on both keypoint matching and geometry registration, such as fragment alignment and scene reconstruction.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1466–1474},
numpages = {9},
keywords = {adversarial learning, keypoint matching, geometry registration},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240668,
author = {Cui, Zhen and Xu, Chunyan and Zheng, Wenming and Yang, Jian},
title = {Context-Dependent Diffusion Network for Visual Relationship Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240668},
doi = {10.1145/3240508.3240668},
abstract = {Visual relationship detection can bridge the gap between computer vision and natural language for scene understanding of images. Different from pure object recognition tasks, the relation triplets of subject-predicate-object lie on an extreme diversity space, such asperson-behind-person andcar-behind-building, while suffering from the problem of combinatorial explosion. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. To capture the interactions of different object instances, two types of graphs, word semantic graph and visual scene graph, are constructed to encode global context interdependency. The semantic graph is built through language priors to model semantic correlations across objects, whilst the visual scene graph defines the connections of scene objects so as to utilize the surrounding scene information. For the graph-structured data, we design a diffusion network to adaptively aggregate information from contexts, which can effectively learn latent representations of visual relationships and well cater to visual relationship detection in view of its isomorphic invariance to graphs. Experiments on two widely-used datasets demonstrate that our proposed method is more effective and achieves the state-of-the-art performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1475–1482},
numpages = {8},
keywords = {visual relation tagging, graph convolution network, relation association, visual relation detection, graph diffusion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240671,
author = {Wang, Shuo and Guo, Dan and Zhou, Wen-gang and Zha, Zheng-Jun and Wang, Meng},
title = {Connectionist Temporal Fusion for Sign Language Translation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240671},
doi = {10.1145/3240508.3240671},
abstract = {Continuous sign language translation (CSLT) is a weakly supervised problem aiming at translating vision-based videos into natural languages under complicated sign linguistics, where the ordered words in a sentence label have no exact boundary of each sign action in the video. This paper proposes a hybrid deep architecture which consists of a temporal convolution module (TCOV), a bidirectional gated recurrent unit module (BGRU), and a fusion layer module (FL) to address the CSLT problem. TCOV captures short-term temporal transition on adjacent clip features (local pattern), while BGRU keeps the long-term context transition across temporal dimension (global pattern). FL concatenates the feature embedding of TCOV and BGRU to learn their complementary relationship (mutual pattern). Thus we propose a joint connectionist temporal fusion (CTF) mechanism to utilize the merit of each module. The proposed joint CTC loss optimization and deep classification score-based decoding fusion strategy are designed to boost performance. With only once training, our model under the CTC constraints achieves comparable performance to other existing methods with multiple EM iterations. Experiments are tested and verified on a benchmark, i.e. the RWTH-PHOENIX-Weather dataset, which demonstrate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1483–1491},
numpages = {9},
keywords = {temporal cov, ctc, bgru, fusion, sign language translation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240674,
author = {Li, Kai and Ding, Zhengming and Li, Kunpeng and Zhang, Yulun and Fu, Yun},
title = {Support Neighbor Loss for Person Re-Identification},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240674},
doi = {10.1145/3240508.3240674},
abstract = {Person re-identification (re-ID) has recently been tremendously boosted due to the advancement of deep convolutional neural networks (CNN). The majority of deep re-ID methods focus on designing new CNN architectures, while less attention is paid on investigating the loss functions. Verification loss and identification loss are two types of losses widely used to train various deep re-ID models, both of which however have limitations. Verification loss guides the networks to generate feature embeddings of which the intra-class variance is decreased while the inter-class ones is enlarged. However, training networks with verification loss tends to be of slow convergence and unstable performance when the number of training samples is large. On the other hand, identification loss has good separating and scalable property. But its neglect to explicitly reduce the intra-class variance limits its performance on re-ID, because the same person may have significant appearance disparity across different camera views. To avoid the limitations of the two types of losses, we propose a new loss, called support neighbor (SN) loss. Rather than being derived from data sample pairs or triplets, SN loss is calculated based on the positive and negative support neighbor sets of each anchor sample, which contain more valuable contextual information and neighborhood structure that are beneficial for more stable performance. To ensure scalability and separability, a softmax-like function is formulated to push apart the positive and negative support sets. To reduce intra-class variance, the distance between the anchor's nearest positive neighbor and furthest positive sample is penalized. Integrating SN loss on top of Resnet50, superior re-ID results to the state-of-the-art ones are obtained on several widely used datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1492–1500},
numpages = {9},
keywords = {loss function, deep neural networks, person re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240682,
author = {Li, Bing and Lin, Chia-Wen and Liu, Shan and Huang, Tiejun and Gao, Wen and Kuo, C.-C. Jay},
title = {Perceptual Temporal Incoherence Aware Stereo Video Retargeting},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240682},
doi = {10.1145/3240508.3240682},
abstract = {Stereo video retargeting aims to avoid shape and depth distortions while maintaining temporal coherence of shape and depth while resizing a stereo video to a desired size. Existing methods resort to extending stereo image retargeting schemes to stereo video retargeting by imposing temporal constraints to consistently resize all corresponding regions so as to maintain temporal coherence. However, such a direct extension often incurs conflicts among the requirements for preserving shape information and depth information and maintaining their temporal coherence, thereby failing to meet one or more of these requirements. We find that properly relaxing temporal constraints for non-paired regions at frame boundaries can effectively mitigate conflicts among depth, shape, and temporal constraints without severely degrading temporal coherence perceptually. Based on this new finding, we derive effective temporal constraints to improve the viewing experience of a 3D scene for stereo video retargeting. Accordingly, we propose an efficient grid-based implementation for our method. Experimental results show that our method achieves superior visual quality over existing methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1501–1509},
numpages = {9},
keywords = {shape preservation, non-uniform warping, temporal coherence, depth preservation, stereo video retargeting},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240675,
author = {Ji, Yanli and Xu, Feixiang and Yang, Yang and Shen, Fumin and Shen, Heng Tao and Zheng, Wei-Shi},
title = {A Large-Scale RGB-D Database for Arbitrary-View Human Action Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240675},
doi = {10.1145/3240508.3240675},
abstract = {Current researches mainly focus on single-view and multiview human action recognition, which can hardly satisfy the requirements of human-robot interaction (HRI) applications to recognize actions from arbitrary views. The lack of databases also sets up barriers. In this paper, we newly collect a large-scale RGB-D action database for arbitrary-view action analysis, including RGB videos, depth and skeleton sequences. The database includes action samples captured in 8 fixed viewpoints and varying-view sequences which covers the entire 360 view angles. In total, 118 persons are invited to act 40 action categories, and 25,600 video samples are collected. Our database involves more articipants, more viewpoints and a large number of samples. More importantly, it is the first database containing the entire 360? varying-view sequences. The database provides sufficient data for cross-view and arbitrary-view action analysis. Besides, we propose a View-guided Skeleton CNN (VS-CNN) to tackle the problem of arbitrary-view action recognition. Experiment results show that the VS-CNN achieves superior performance.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1510–1518},
numpages = {9},
keywords = {cross-view recognition, hri, human action recognition, arbitrary-view recognition, rgb-d action database},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240677,
author = {Wang, Huiyun and Xu, Youjiang and Han, Yahong},
title = {Spotting and Aggregating Salient Regions for Video Captioning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240677},
doi = {10.1145/3240508.3240677},
abstract = {Towards an interpretable video captioning process, we target to locate salient regions of video objects along with the sequentially uttering words. This paper proposes a new framework to automatically spot salient regions in each video frame and simultaneously learn a discriminative spatio-temporal representation for video captioning. First, in a Spot Module, we automatically learn the saliency value of each location to separate salient regions from video content as the foreground and the rest as background by two operations of 'hard separation' and 'soft separation', respectively. Then, in an Aggregate Module, to aggregate the foreground/background descriptors into a discriminative spatio-temporal representation, we devise a trainable video VLAD process to learn the aggregation parameters. Finally, we utilize the attention mechanism to decode the spatio-temporal representations of different regions into video descriptions. Experiments on two benchmark datasets demonstrate our method outperforms most of the state-of-the-art methods in terms of Bleu@4, METEOR and CIDEr metrics for the task of video captioning. Also examples demonstrate our method can successfully utter words to sequentially salient regions of video objects.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1519–1526},
numpages = {8},
keywords = {salient regions, spatio-temporal representation, video captioning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240660,
author = {Zhou, Qixian and Liang, Xiaodan and Gong, Ke and Lin, Liang},
title = {Adaptive Temporal Encoding Network for Video Instance-Level Human Parsing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240660},
doi = {10.1145/3240508.3240660},
abstract = {Beyond the existing single-person and multiple-person human parsing tasks in static images, this paper makes the first attempt to investigate a more realistic video instance-level human parsing that simultaneously segments out each person instance and parses each instance into more fine-grained parts (eg, head, leg, dress). We introduce a novel Adaptive Temporal Encoding Network (ATEN) that alternatively performs temporal encoding among key frames and flow-guided feature propagation from other consecutive frames between two key frames. Specifically, ATEN first incorporates a Parsing-RCNN to produce the instance-level parsing result for each key frame, which integrates both the global human parsing and instance-level human segmentation into a unified model. To balance between accuracy and efficiency, the flow-guided feature propagation is used to directly parse consecutive frames according to their identified temporal consistency with key frames. On the other hand, ATEN leverages the convolution gated recurrent units (convGRU) to exploit temporal changes over a series of key frames, which are further used to facilitate the frame-level instance-level parsing. By alternatively performing direct feature propagation between consistent frames and temporal encoding network among key frames, our ATEN achieves a good balance between frame-level accuracy and time efficiency, which is a common crucial problem in video object segmentation research. To demonstrate the superiority of our ATEN, extensive experiments are conducted on the most popular video segmentation benchmark (DAVIS) and a newly collected Video Instance-level Parsing (VIP) dataset, which is the first video instance-level human parsing dataset comprised of 404 sequences and over 20k frames with instance-level and pixel-wise annotations.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1527–1535},
numpages = {9},
keywords = {adaptive learning, video instance-level human parsing, temporal encoding network},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240661,
author = {Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
title = {User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240661},
doi = {10.1145/3240508.3240661},
abstract = {Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1536–1544},
numpages = {9},
keywords = {interactive colorization, gans, edit propagation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240673,
author = {Zhao, Tianli and He, Xiangyu and Cheng, Jian and Hu, Jing},
title = {BitStream: Efficient Computing Architecture for Real-Time Low-Power Inference of Binary Neural Networks on CPUs},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240673},
doi = {10.1145/3240508.3240673},
abstract = {Convolutional Neural Networks (CNN) have been widely used in many multimedia applications such as image classification, speech recognition, natural language processing and so on. Nevertheless, the high performance of deep CNN models also comes with high consumption of computation and storage resources, making it difficult to run CNN models in real time applications on mobile devices, where computational ability, memory resource and power are largely constrained. Binary network is a recently proposed tech- nique to reduce the computational and memory complexity of CNN, in which the expensive floating-point operations can be replaced by much cheaper bit-wise operations. Despite its obvious advantages, only few works explored the efficient implementation of Binary Neural Networks (BNN). In this work, we present a general architecture for efficient binary convolution referred to as BitStream with the latest computation flow for BNNs instead of the traditional row-major im2col based one. We mainly optimize memory access during computation of BNNs, the proposed calculation flow is cache friendly as well as can largely reduce memory overhead of BNNs, decidedly leading to its memory efficiency and further computational efficiency. Extensive evaluations on various networks demon- strate the efficiency of the proposed method. For instance, memory consumption of BitStream is reduced by 18-32\texttimes{} than original networks and 3\texttimes{} than existing implementations of BNNs during inference. Moreover, our implemented binary Alexnet can achieve 8.07\texttimes{} and 2.84\texttimes{} speedup over floating point precision model and conventional implementations of BNNs on 8 \texttimes{} Cortex A53 CPU s. With 4 \texttimes{} Intel CORE i7 6700 CPUs, the binary vgg-like convolutional network on CIFAR-10 runs even 1.69\texttimes{} faster than floating point precision version on TitanX GPU.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1545–1552},
numpages = {8},
keywords = {binary neural networks, convolutional neural networks},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240681,
author = {Liu, Lingbo and Zhang, Ruimao and Peng, Jiefeng and Li, Guanbin and Du, Bowen and Lin, Liang},
title = {Attentive Crowd Flow Machines},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240681},
doi = {10.1145/3240508.3240681},
abstract = {Traffic flow prediction is crucial for urban traffic management and public safety. Its key challenges lie in how to adaptively integrate the various factors that affect the flow changes. In this paper, we propose a unified neural network module to address this problem, called Attentive Crowd Flow Machine~(ACFM), which is able to infer the evolution of the crowd flow by learning dynamic representations of temporally-varying data with an attention mechanism. Specifically, the ACFM is composed of two progressive ConvLSTM units connected with a convolutional layer for spatial weight prediction. The first LSTM takes the sequential flow density representation as input and generates a hidden state at each time-step for attention map inference, while the second LSTM aims at learning the effective spatial-temporal feature expression from attentionally weighted crowd flow features. Based on the ACFM, we further build a deep architecture with the application to citywide crowd flow prediction, which naturally incorporates the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks (i.e., crowd flow in Beijing and New York City) show that the proposed method achieves significant improvements over the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1553–1561},
numpages = {9},
keywords = {traffic flow prediction, memory and attention neural networks, mobility data, spatial-temporal modeling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240622,
author = {Ouyang, Deqiang and Shao, Jie and Zhang, Yonghui and Yang, Yang and Shen, Heng Tao},
title = {Video-Based Person Re-Identification via Self-Paced Learning and Deep Reinforcement Learning Framework},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240622},
doi = {10.1145/3240508.3240622},
abstract = {Person re-identification is an important task in video surveillance, focusing on finding the same person across different cameras. However, most existing methods of video-based person re-identification still have some limitations (e.g., the lack of effective deep learning framework, the robustness of the model, and the same treatment for all video frames) which make them unable to achieve better recognition performance. In this paper, we propose a novel self-paced learning algorithm for video-based person re-identification, which could gradually learn from simple to complex samples for a mature and stable model. Self-paced learning is employed to enhance video-based person re-identification based on deep neural network, so that deep neural network and self-paced learning are unified into one frame. Then, based on the trained self-paced learning, we propose to employ deep reinforcement learning to discard misleading and confounding frames and find the most representative frames from video pairs. With the advantage of deep reinforcement learning, our method can learn strategies to select the optimal frame groups. Experiments show that the proposed framework outperforms the existing methods on the iLIDS-VID, PRID-2011 and MARS datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1562–1570},
numpages = {9},
keywords = {self-paced learning, video-based person re-identification, deep reinforcement learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240646,
author = {Liao, Lizi and He, Xiangnan and Zhao, Bo and Ngo, Chong-Wah and Chua, Tat-Seng},
title = {Interpretable Multimodal Retrieval for Fashion Products},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240646},
doi = {10.1145/3240508.3240646},
abstract = {Deep learning methods have been successfully applied to fashion retrieval. However, the latent meaning of learned feature vectors hinders the explanation of retrieval results and integration of user feedback. Fortunately, there are many online shopping websites organizing fashion items into hierarchical structures based on product taxonomy and domain knowledge. Such structures help to reveal how human perceive the relatedness among fashion products. Nevertheless, incorporating structural knowledge for deep learning remains a challenging problem. This paper presents techniques for organizing and utilizing the fashion hierarchies in deep learning to facilitate the reasoning of search results and user intent. The novelty of our work originates from the development of an EI (Exclusive &amp; Independent) tree that can cooperate with deep models for end-to-end multimodal learning. EI tree organizes the fashion concepts into multiple semantic levels and augments the tree structure with exclusive as well as independent constraints. It describes the different relationships among sibling concepts and guides the end-to-end learning of multi-level fashion semantics. From EI tree, we learn an explicit hierarchical similarity function to characterize the semantic similarities among fashion products. It facilitates the interpretable retrieval scheme that can integrate the concept-level feedback. Experiment results on two large fashion datasets show that the proposed approach can characterize the semantic similarities among fashion items accurately and capture user's search intent precisely, leading to more accurate search results as compared to the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1571–1579},
numpages = {9},
keywords = {attribute manipulation, multimodal fashion retrieval, ei tree},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240670,
author = {Chen, Chieh-Yu and Lai, Wenze and Hsieh, Hsin-Ying and Zheng, Wen-Hao and Wang, Yu-Shuen and Chuang, Jung-Hong},
title = {Generating Defensive Plays in Basketball Games},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240670},
doi = {10.1145/3240508.3240670},
abstract = {In this paper, we present a method to generate realistic defensive plays in a basketball game based on the ball and the offensive team's movements. Our system allows players and coaches to simulate how the opposing team will react to a newly developed offensive strategy for evaluating its effectiveness. To achieve the aim, we train on the NBA dataset a conditional generative adversarial network that learns spatio-temporal interactions between players' movements. The network consists of two components: a generator that takes a latent noise vector and the offensive team's trajectories as input to generate defensive team's trajectories; and a discriminator that evaluates the realistic degree of the generated results. Since a basketball game can be easily identified as fake if the ball handler, who is not defended, does not shoot the ball or cut into the restricted area, we add the wide open penalty to the objective function to assist model training. To evaluate the results, we compared the similarity of the real and the generated defensive plays, in terms of the players' movement speed and acceleration, distance to defend ball handlers and non- ball handlers, and the frequency of wide open occurrences. In addition, we conducted a user study with 59 participants for subjective tests. Experimental results show the high fidelity of the generated defensive plays to real data and demonstrate the feasibility of our algorithm.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1580–1588},
numpages = {9},
keywords = {conditional adversarial network, defensive strategies, basketball},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240684,
author = {Liu, Hong and Lin, Mingbao and Zhang, Shengchuan and Wu, Yongjian and Huang, Feiyue and Ji, Rongrong},
title = {Dense Auto-Encoder Hashing for Robust Cross-Modality Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240684},
doi = {10.1145/3240508.3240684},
abstract = {Cross-modality retrieval has been widely studied, which aims to search images as response to text queries or vice versa. When faced with large-scale dataset, cross-modality hashing serves as an efficient and effective solution, which learns binary codes to approximate the cross-modality similarity in the Hamming space. Most recent cross-modality hashing schemes focus on learning the hash functions from data instances with fully modalities. However, how to learn robust binary codes when facing incomplete modality (i.e., with one modality missed or partially observed), is left unexploited, which however widely occurs in real-world applications. In this paper, we propose a novel cross-modality hashing, termed Dense Auto-encoder Hashing (DAH), which can explicitly impute the missed modality and produce robust binary codes by leveraging the relatedness among different modalities. To that effect, we propose a novel Dense Auto-encoder Network (DAN) to impute the missing modalities, which densely connects each layer to every other layer in a feed-forward fashion. For each layer, a noisy auto-encoder block is designed to calculate the residue between the current prediction and original data. Finally, a hash-layer is added to the end of DAN, which serves as a special binary encoder model to deal with the incomplete modality input. Quantitative experiments on three cross-modality visual search benchmarks, i.e., the Wiki, NUS-WIDE, and FLICKR-25K, have shown that the proposed DAH has superior performance over the state-of-the-art approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1589–1597},
numpages = {9},
keywords = {binary code learning, hash learning, cross-modal search, large- scale image retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240526,
author = {Tang, Taoran and Jia, Jia and Mao, Hanyang},
title = {Dance with Melody: An LSTM-Autoencoder Approach to Music-Oriented Dance Synthesis},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240526},
doi = {10.1145/3240508.3240526},
abstract = {Dance is greatly influenced by music. Studies on how to synthesize music-oriented dance choreography can promote research in many fields, such as dance teaching and human behavior research. Although considerable effort has been directed toward investigating the relationship between music and dance, the synthesis of appropriate dance choreography based on music remains an open problem. There are two main challenges: 1) how to choose appropriate dance figures, i.e., groups of steps that are named and specified in technical dance manuals, in accordance with music and 2) how to artistically enhance choreography in accordance with music. To solve these problems, in this paper, we propose a music-oriented dance choreography synthesis method using a long short-term memory (LSTM)-autoencoder model to extract a mapping between acoustic and motion features. Moreover, we improve our model with temporal indexes and a masking method to achieve better performance. Because of the lack of data available for model training, we constructed a music-dance dataset containing choreographies for four types of dance, totaling 907,200 frames of 3D dance motions and accompanying music, and extracted multidimensional features for model training. We employed this dataset to train and optimize the proposed models and conducted several qualitative and quantitative experiments to select the best-fitted model. Finally, our model proved to be effective and efficient in synthesizing valid choreographies that are also capable of musical expression.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1598–1606},
numpages = {9},
keywords = {autoencoder, lstm, 3d motion capture, music-dance dataset, motion synthesis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240604,
author = {Chen, Gong and Liu, Yan and Zhong, Sheng-hua and Zhang, Xiang},
title = {Musicality-Novelty Generative Adversarial Nets for Algorithmic Composition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240604},
doi = {10.1145/3240508.3240604},
abstract = {Algorithmic composition, which enables computer to generate music like human composers, has lasting charm because it intends to approximate artistic creation, most mysterious part of human intelligence. To deliver both melodious and refreshing music, this paper proposes the Musicality-Novelty Generative Adversarial Nets for algorithmic composition. With the same generator, two adversarial nets alternately optimize the musicality and novelty of the machine-composed music. A new model called novelty game is presented to maximize the minimal distance between the machine-composed music sample and any human-composed music sample in the novelty space, where all well-known human composed music products are far from each other. We implement the proposed framework using three supervised CNNs with one for generator, one for musicality critic and one for novelty critic on the time-pitch feature space. Specifically, the novelty critic is implemented by Siamese neural networks with temporal alignment using dynamic time warping. We provide empirical validations by generating the music samples under various scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1607–1615},
numpages = {9},
keywords = {algorithmic composition, music, generative adversarial nets},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240664,
author = {Bhat, Divyashri and Deshmukh, Rajvardhan and Zink, Michael},
title = {Improving QoE of ABR Streaming Sessions through QUIC Retransmissions},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240664},
doi = {10.1145/3240508.3240664},
abstract = {While adaptive bitrate (ABR) streaming has contributed significantly to the reduction of video playout stalling, ABR clients continue to suffer from the variation of bit rate qualities over the duration of a streaming session. Similar to stalling, these variations in bit rate quality have a negative impact on the users' Quality of Experience (QoE). In this paper, we use a trace from a large-scale CDN to show that such quality changes occur in a significant amount of streaming sessions and investigate an ABR video segment retransmission approach to reduce the number of such quality changes. As the new HTTP/2 standard is becoming increasingly popular, we also see an increase in the usage of QUIC as an alternative protocol for the transmission of web traffic including video streaming. Using various network conditions, we conduct a systematic comparison of existing transport layer approaches for HTTP/2 that is best suited for ABR segment retransmissions. Since it is well known that both protocols provide a series of improvements over HTTP/1.1, we perform experiments both in controlled environments and over transcontinental links in the Internet and find that these benefits also "trickle up'' into the application layer when it comes to ABR video streaming where QUIC retransmissions can significantly improve the average quality bitrate while simultaneously minimizing bit rate variations over the duration of a streaming session.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1616–1624},
numpages = {9},
keywords = {quic, qoe, http/2, dash, abr streaming},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240654,
author = {Chen, Ziqian and Wang, Shiqi and Wu, Dapeng Oliver and Huang, Tiejun and Duan, Ling-Yu},
title = {From Data to Knowledge: Deep Learning Model Compression, Transmission and Communication},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240654},
doi = {10.1145/3240508.3240654},
abstract = {With the advances of artificial intelligence, recent years have witnessed a gradual transition from the big data to the big knowledge. Based on the knowledge-powered deep learning models, the big data such as the vast text, images and videos can be efficiently analyzed. As such, in addition to data, the communication of knowledge implied in the deep learning models is also strongly desired. As a specific example regarding the concept of knowledge creation and communication in the context of Knowledge Centric Networking (KCN), we investigate the deep learning model compression and demonstrate its promise use through a set of experiments. In particular, towards future KCN, we introduce efficient transmission of deep learning models in terms of both single model compression and multiple model prediction. The necessity, importance and open problems regarding the standardization of deep learning models, which enables the interoperability with the standardized compact model representation bitstream syntax, are also discussed.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1625–1633},
numpages = {9},
keywords = {deep learning model compression, standardization, knowledge communication},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286939,
author = {Lee, Kyoung Mu},
title = {Session Details: Keynote 4},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286939},
doi = {10.1145/3286939},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3267344,
author = {Lee, Gary Geunbae},
title = {Living with AI in Connected Devices for Valuable Experience},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3267344},
doi = {10.1145/3240508.3267344},
abstract = {This talk describes how AI technology can be used in multiple connected devices around us such as smartphone, TV, refrigerator and several consumer electronic devices, thus giving new and exciting customer experiences and values. This talk starts with Samsung's AI vision as a device company, and introduces 5 strategic principles along with industrial usages of AI technology including speech and natural language, visual understanding, data intelligence and autonomous driving all with deep learning techniques heavily involved. These applications naturally form a platform for both on device/edge, cloud and machine learning services for various current and future Samsung devices.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1634},
numpages = {1},
keywords = {artificial intelligence, computer vision, autonomous driving, deep learning, natural language and speech, ai platform},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286940,
author = {Sang, Jitao},
title = {Session Details: Multimedia -3 (Multimedia Search)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286940},
doi = {10.1145/3286940},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240519,
author = {Lin, Mingbao and Ji, Rongrong and Liu, Hong and Wu, Yongjian},
title = {Supervised Online Hashing via Hadamard Codebook Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240519},
doi = {10.1145/3240508.3240519},
abstract = {In recent years, binary code learning, a.k.a. hashing, has received extensive attention in large-scale multimedia retrieval. It aims to encode high-dimensional data points into binary codes, hence the original high-dimensional metric space can be efficiently approximated via Hamming space. However, most existing hashing methods adopted offline batch learning, which is not suitable to handle incremental datasets with streaming data or new instances. In contrast, the robustness of the existing online hashing remains as an open problem, while the embedding of supervised/semantic information hardly boosts the performance of the online hashing, mainly due to the defect of unknown category numbers in supervised learning. In this paper, we propose an online hashing scheme, termed Hadamard Codebook based Online Hashing (HCOH), which aims to solving the above problems towards robust and supervised online hashing. In particular, we first assign an appropriate high-dimensional binary codes to each class label, which is generated randomly by Hadamard codes. Subsequently, LSH is adopted to reduce the length of such Hadamard codes in accordance with the hash bits, which can adapt the predefined binary codes online, and theoretically guarantee the semantic similarity. Finally, we consider the setting of stochastic data acquisition, which facilitates our method to efficiently learn the corresponding hashing functions via stochastic gradient descend (SGD) online. Notably, the proposed HCOH can be embedded with supervised labels and is not limited to a predefined category number. Extensive experiments on three widely-used benchmarks demonstrate the merits of the proposed scheme over the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1635–1643},
numpages = {9},
keywords = {binary code learning, large-scale image retrieval, hadamard codesbook, online hashing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240532,
author = {Fang, Yuanqiang and Zhou, Wengang and Lu, Yijuan and Tang, Jinhui and Tian, Qi and Li, Houqiang},
title = {Cascaded Feature Augmentation with Diffusion for Image Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240532},
doi = {10.1145/3240508.3240532},
abstract = {Recently, as an effective re-ranking technique, diffusion has attracted considerable attention in research on image retrieval. It inherits from random surfer model and is effective to deeply explore data manifold structure. However, as a common practice, diffusion is performed at query time which relies heavily on initial retrieval shortlists and suffers the bottleneck of online time-efficiency. To this end, in this paper, we present a more generalized method named CFA (cascaded feature augmentation) based on diffusion. First of all, we transfer diffusion process from online stage to offline stage and innovatively utilize output of diffusion to augment database features in a cascaded mode, which can eliminate iteration process at query time radically. Second, to scale the diffusion method to large image database, we propose a cascaded cluster diffusion technique for feature augmentation which largely reduces computational cost. Third, we extend our cascaded feature augmentation scheme to cases with multiple features without involving extra memory and time cost. Our CFA is compatible with other re-ranking methods. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed algorithm.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1644–1652},
numpages = {9},
keywords = {offline cascaded feature augmentation, diffusion, image retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240543,
author = {Cao, Zhangjie and Sun, Ziping and Long, Mingsheng and Wang, Jianmin and Yu, Philip S.},
title = {Deep Priority Hashing},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240543},
doi = {10.1145/3240508.3240543},
abstract = {Deep hashing enables image retrieval by end-to-end learning of deep representations and hash codes from training data with pairwise similarity information. Subject to the distribution skewness underlying the similarity information, most existing deep hashing methods may underperform for imbalanced data due to misspecified loss functions. This paper presents Deep Priority Hashing (DPH), an end-to-end architecture that generates compact and balanced hash codes in a Bayesian learning framework. The main idea is to reshape the standard cross-entropy loss for similarity-preserving learning such that it down-weighs the loss associated to highly-confident pairs. This idea leads to a novel priority cross-entropy loss, which prioritizes the training on uncertain pairs over confident pairs. Also, we propose another priority quantization loss, which prioritizes hard-to-quantize examples for generation of nearly lossless hash codes. Extensive experiments demonstrate that DPH can generate high-quality hash codes and yield state-of-the-art image retrieval results on three datasets, ImageNet, NUS-WIDE, and MS-COCO.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1653–1661},
numpages = {9},
keywords = {image search, deep hashing, priority loss},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240683,
author = {Liu, Xingbo and Nie, Xiushan and Zeng, Wenjun and Cui, Chaoran and Zhu, Lei and Yin, Yilong},
title = {Fast Discrete Cross-Modal Hashing With Regressing From Semantic Labels},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240683},
doi = {10.1145/3240508.3240683},
abstract = {Hashing has recently received great attention in cross-modal retrieval. Cross-modal retrieval aims at retrieving information across heterogeneous modalities (e.g., texts vs. images). Cross-modal hashing compresses heterogeneous high-dimensional data into compact binary codes with similarity preserving, which provides efficiency and facility in both retrieval and storage. In this study, we propose a novel fast discrete cross-modal hashing (FDCH) method with regressing from semantic labels to take advantage of supervised labels to improve retrieval performance. In contrast to existing methods that learn the projection from hash codes to semantic labels, the proposed FDCH regresses the semantic labels of training examples to the corresponding hash codes with a drift. It not only accelerates the hash learning process, but also helps generate stable hash codes. Furthermore, the drift can adjust the regression and enhance the discriminative capability of hash codes. Especially in the case of training efficiency, FDCH is much faster than existing methods. Comparisons with several state-of-the-art techniques on three benchmark datasets have demonstrated the superiority of FDCH under various cross-modal retrieval scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1662–1669},
numpages = {8},
keywords = {learning-based hashing, supervised hashing, cross-modal retrieval},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286941,
author = {Yan, Zhisheng},
title = {Session Details: Experience-1 (Multimedia Entertainment and Experience)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286941},
doi = {10.1145/3286941},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240652,
author = {Zheng, Shuai and Yang, Fan and Kiapour, M. Hadi and Piramuthu, Robinson},
title = {ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240652},
doi = {10.1145/3240508.3240652},
abstract = {Understanding clothes from a single image would have huge commercial and cultural impacts on modern societies. However, this task remains a challenging computer vision problem due to wide variations in the appearance, style, brand and layering of clothing items. We present a new database called ModaNet, a large-scale collection of images based on Paperdoll dataset. Our dataset provides 55,176 street images, fully annotated with polygons on top of the 1 million weakly annotated street images in Paperdoll. ModaNet aims to provide a technical benchmark to fairly evaluate the progress of applying the latest computer vision techniques that rely on large data for fashion understanding. The rich annotation of the dataset allows to measure the performance of state-of-the-art algorithms for object detection, semantic segmentation and polygon prediction on street fashion images in detail.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1670–1678},
numpages = {9},
keywords = {fashion, computer vision, dataset},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240691,
author = {Murad, Dania and Wang, Riwu and Turnbull, Douglas and Wang, Ye},
title = {SLIONS: A Karaoke Application to Enhance Foreign Language Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240691},
doi = {10.1145/3240508.3240691},
abstract = {Singing songs can be an engaging and effective activity when learning a foreign language. In this paper, we describe a multi-language karaoke application called SLIONS: Singing and Listening to Improve Our Natural Speaking. When developing this application, we followed a user-centered design process which was informed by conducting interviews with domain experts, extensive usability testing, and reviewing existing gamified karaoke and language learning applications. The key feature of SLIONS is that we used automatic speech recognition (ASR) to provide students with personalized, granular feedback based on their singing pronunciation. We also provided multi-modal instruction: audio of music and singing tracks, video of a professional singer and translated text of lyrics to help students learn and master each song in the foreign language. To test the efficacy of SLIONS, we conducted a one-week pilot study with English and Chinese language learning students (N=15). The initial quantitative results show that our application can improve pronunciation and may improve vocabulary. In addition, the qualitative feedback from the students suggests that SLIONS is both fun to use and motivates students to practice speaking and singing in a foreign language.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1679–1687},
numpages = {9},
keywords = {educational technology, mobile application, music technology, intelligent tutoring systems, foreign language learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240580,
author = {Yang, Shuai and Liu, Jiaying and Yang, Wenhan and Guo, Zongming},
title = {Context-Aware Unsupervised Text Stylization},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240580},
doi = {10.1145/3240508.3240580},
abstract = {In this work, we present a novel algorithm to stylize the text without supervision, which provides a flexible and convenient way to invoke fantastic text expressions. Rather than employing the fixed pair of target text and source style images, our unsupervised framework establishes an implicit mapping for them by using an abstract imagery of the style image as bridges. Based on the mapping, we progressively narrow the visual discrepancy between text and style images by the proposed legibility-preserving structure transfer and texture transfer algorithms, which effectively balance the text legibility and style consistency. Furthermore, we explore a seamless composition of the stylized text and a background image, in which the optimal text layout is determined by a context-aware layout design algorithm utilizing cues for both seamlessness and aesthetics. Given the layout, the text can be seamlessly embedded into the background by texture synthesis under a context-aware boundary constraint. Experimental results demonstrate the effectiveness of the proposed method in automatic artistic typography creation and visual-textual presentation synthesis.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1688–1696},
numpages = {9},
keywords = {context-aware, structure synthesis, texture synthesis, style transfer, unsupervised method},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240619,
author = {Kato, Jun and Ogata, Masa and Inoue, Takahiro and Goto, Masataka},
title = {Songle Sync: A Large-Scale Web-Based Platform for Controlling Various Devices in Synchronization with Music},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240619},
doi = {10.1145/3240508.3240619},
abstract = {This paper presents Songle Sync, a web-based platform on which hundreds of Internet-connected devices - including smartphones, computers, and other physical computing devices - can be controlled to synchronize with music playback. It uses music-understanding technologies to dynamically synthesize music-driven multimedia performances from a musical piece of choice. To simultaneously control hundreds of devices, a conventional architecture keeps always-on connections between them. However, it does not scale and suffers from latency and jitter issues when there are various devices with potentially unstable networks. We address this with a novel autonomous control architecture in which each device is notified of forthcoming musical events (e.g., beats and chorus sections) to automatically drive various changes in multimedia performances. Moreover, we provide a development kit of an event-driven multimedia framework for JavaScript, example programs, and an interactive tutorial. To evaluate the platform, we compared latencies, jitters, and amounts of network traffic between ours and the conventional architecture. To examine use cases in the wild, we deployed the platform to drive over a hundred of a variety of devices. We also developed a web browser-based application for a multimedia performance with music playback. It provided audiences of hundreds with a bring-your-own-device experience of synchronized animations on smartphones. In addition, the development kit was used in a two-day hackathon. We report lessons learned from these studies and discuss the future of the Internet of Musical Things.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1697–1705},
numpages = {9},
keywords = {application programming interface, internet of musical things, multimedia control, music synchronization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286942,
author = {Lu, Yijuan},
title = {Session Details: System-2 (Smart Multimedia Systems)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286942},
doi = {10.1145/3286942},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240522,
author = {Geng, Weidong and Han, Feilin and Lin, Jiangke and Zhu, Liuyi and Bai, Jieming and Wang, Suzhen and He, Lin and Xiao, Qiang and Lai, Zhangjiong},
title = {Fine-Grained Grocery Product Recognition by One-Shot Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240522},
doi = {10.1145/3240508.3240522},
abstract = {Fine-grained grocery product recognition via camera is a challenging task to identify the visually similar products with subtle differences by using single-shot training examples. To address this issue? we present a novel hybrid classification approach that combines feature-based matching and one-shot deep learning with a coarse-to-fine strategy. The candidate regions of product instances are first detected and coarsely labeled by recurring features in product images without any training. Then, attention maps are generated to guide the classifier to focus on fine discriminative details by magnifying the influences of the features in the candidate regions of interest (ROI) and suppressing the interferences of the features outside, improving the accuracy of fine-grained grocery products recognition effectively. Our framework also performs a good adaptability which allows existing classifier to be refined without retraining for new coming product classes. As an additional contribution, we collect a new grocery product database with 102 classes from 2 stores. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1706–1714},
numpages = {9},
keywords = {product categorization, fine-grained object recognition},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240630,
author = {Matsui, Yusuke and Hinami, Ryota and Satoh, Shin'ichi},
title = {Reconfigurable Inverted Index},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240630},
doi = {10.1145/3240508.3240630},
abstract = {Existing approximate nearest neighbor search systems suffer from two fundamental problems that are of practical importance but have not received sufficient attention from the research community. First, although existing systems perform well for the whole database, it is difficult to run a search over a subset of the database. Second, there has been no discussion concerning the performance decrement after many items have been newly added to a system. We develop a reconfigurable inverted index (Rii) to resolve these two issues. Based on the standard IVFADC system, we design a data layout such that items are stored linearly. This enables us to efficiently run a subset search by switching the search method to a linear PQ scan if the size of a subset is small. Owing to the linear layout, the data structure can be dynamically adjusted after new items are added, maintaining the fast speed of the system. Extensive comparisons show that Rii achieves a comparable performance with state-of-the art systems such as Faiss.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1715–1723},
numpages = {9},
keywords = {reconfigure, approximate nearest neighbor search, subset search, inverted index, product quantization},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240514,
author = {Sankoh, Hiroshi and Naito, Sei and Nonaka, Keisuke and Sabirin, Houari and Chen, Jun},
title = {Robust Billboard-Based, Free-Viewpoint Video Synthesis Algorithm to Overcome Occlusions under Challenging Outdoor Sport Scenes},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240514},
doi = {10.1145/3240508.3240514},
abstract = {The paper proposes an algorithm to robustly reconstruct an accurate billboard model of an individual object including an occluded one in each camera. Each billboard model is utilized to synthesize high-quality, free-viewpoint video especially for outdoor sport scenes in which roughly calibrated cameras are sparsely placed. The two main contributions of the proposed algorithm are (1) robustness to occlusions caused by overlaps of multiple objects in every camera, that is one of the biggest issues for billboard-based method, and (2) applicability to challenging shooting conditions in which accurate 3D model cannot be reconstructed because of calibration errors, small number of cameras and so on. In order to achieve the contributions above, the algorithm does not try to reproduce an accurate 3D model of each object but utilize a "rough 3D model". The algorithm precisely extracts an individual object region in every camera by reconstructing a "rough 3D model" of each object and back-projecting it to every camera. The 3D coordinate for each billboard to be located is calculated based on the position of a rough 3D model. Experimental results compare the visual quality of free-viewpoint videos synthesized with our proposed method and conventional methods and show the effectiveness of our proposed method in terms of the naturalness of positional relationships and the fineness of the surface textures of all the objects.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1724–1732},
numpages = {9},
keywords = {sport, free-viewpoint video, billboard, occlusion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240600,
author = {Cheng, Wei and Xu, Lan and Han, Lei and Guo, Yuanfang and Fang, Lu},
title = {IHuman3D: Intelligent Human Body 3D Reconstruction Using a Single Flying Camera},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240600},
doi = {10.1145/3240508.3240600},
abstract = {Aiming at autonomous, adaptive and real-time human body reconstruction technique, this paper presents iHuman3D: an intelligent human body 3D reconstruction system using a single aerial robot integrated with an RGB-D camera. Specifically, we propose a real-time and active view planning strategy based on a highly efficient ray casting algorithm in GPU and a novel information gain formulation directly in TSDF. We also propose the human body reconstruction module by revising the traditional volumetric fusion pipeline with a compactly-designed non-rigid deformation for slight motion of the human target. We unify both the active view planning and human body reconstruction in the same TSDF volume-based representation. Quantitative and qualitative experiments are conducted to validate that the proposed iHuman3D system effectively removes the constraint of extra manual labor, enabling real-time and autonomous reconstruction of human body.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1733–1741},
numpages = {9},
keywords = {tsdf, flying camera, human 3d reconstruction, next best view},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286943,
author = {Huet, Benoit},
title = {Session Details: FF-6},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286943},
doi = {10.1145/3286943},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240687,
author = {Gao, Lianli and Zeng, Pengpeng and Song, Jingkuan and Liu, Xianglong and Shen, Heng Tao},
title = {Examine before You Answer: Multi-Task Learning with Adaptive-Attentions for Multiple-Choice VQA},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240687},
doi = {10.1145/3240508.3240687},
abstract = {Multiple-choice (MC) Visual Question Answering (VQA) is a similar but essentially different task to open-ended VQA because the answer options are provided. Most of existing works tackle them in a unified pipeline by solving a multi-class problem to infer the best answer from a predefined answer set. The option that matches the best answer is selected for MC VQA. Nevertheless, this violates human thinking logics. Normally, people examine the questions, answer options and the reference image before inferring a MC VQA. For MC VQA, human either rely on the question and answer options to directly deduce a correct answer if the question is not image-related, or read the question and answer options and then purposefully search for answers in a reference image. Therefore, we propose a novel approach, namely Multi-task Learning with Adaptive-attention (MTA), to simulate human logics for MC VQA. Specifically, we first fuse the answer options and question features, and then adaptively attend to the visual features for inferring a MC VQA. Furthermore, we design our model as a multi-task learning architecture by integrating the open-ended VQA task to further boost the performance of MC VQA. We evaluate our approach on two standard benchmark datasets: VQA and Visual7W and our approach sets new records on both datasets for MC VQA task, reaching 73.5% and 65.9% average accuracy respectively.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1742–1750},
numpages = {9},
keywords = {visual question answering, open-ended, multiple-choice},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240694,
author = {Fan, Zhiwen and Wu, Huafeng and Fu, Xueyang and Huang, Yue and Ding, Xinghao},
title = {Residual-Guide Network for Single Image Deraining},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240694},
doi = {10.1145/3240508.3240694},
abstract = {Single image rain streaks removal is extremely important since rainy condition adversely affects many computer vision systems. Deep learning based methods have great success in image deraining tasks. In this paper, we propose a novel residual-guide feature fusion network, called ResGuideNet, for single image deraining that progressively predicts high-quality reconstruction while using fewer parameters than previous methods. Specifically, we propose a cascaded network and adopt residuals from shallower blocks to guide deeper blocks. We can obtain a coarse-to-fine estimation of negative residual as the blocks go deeper with this strategy. The outputs of different blocks are merged into the final reconstruction. We adopt recursive convolution to build each block and apply supervision to intermediate de-rained results. ResGuideNet is detachable to meet different rainy conditions. For images with light rain streaks and limited computational resource at test time, we can obtain a decent performance even with several building blocks. Experiments validate that ResGuideNet can benefit other low- and high-level vision tasks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1751–1759},
numpages = {9},
keywords = {single image deraining, vehicle detection, residual learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240698,
author = {Zhao, Zhengyu and Larson, Martha},
title = {From Volcano to Toyshop: Adaptive Discriminative Region Discovery for Scene Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240698},
doi = {10.1145/3240508.3240698},
abstract = {As deep learning approaches to scene recognition emerge, they have continued to leverage discriminative regions at multiple scales, building on practices established by conventional image classification research. However, approaches remain largely generic, and do not carefully consider the special properties of scenes. In this paper, inspired by the intuitive differences between scenes and objects, we propose Adi-Red, an adaptive approach to discriminative region discovery for scene recognition. Adi-Red uses a CNN classifier, which was pre-trained using only image-level scene labels, to discover discriminative image regions directly. These regions are then used as a source of features to perform scene recognition. The use of the CNN classifier makes it possible to adapt the number of discriminative regions per image using a simple, yet elegant, threshold, at relatively low computational cost. Experimental results on the scene recognition benchmark dataset SUN397 demonstrate the ability of Adi-Red to outperform the state of the art. Additional experimental analysis on the Places dataset reveals the advantages of Adi-Red, and highlight how they are specific to scenes. We attribute the effectiveness of Adi-Red to the ability of adaptive region discovery to avoid introducing noise, while also not missing out on important information.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1760–1768},
numpages = {9},
keywords = {multi-scale feature aggregation, scene recognition, adaptive discriminative region discovery},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240703,
author = {Sowerby, Joshua and Zhang, Yang and Agrafiotis, Dimitris},
title = {The Effect of Foveation on High Dynamic Range Video Perception},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240703},
doi = {10.1145/3240508.3240703},
abstract = {When watching a video, the viewer's eyes will fixate on a certain point within each frame. Areas far from the viewers gaze location are perceived with much lower visual acuity than those around the fixation point. This effect is known as foveation. In this paper, the effect of foveation on High Dynamic Range (HDR) video perception is investigated. Using eye tracking data recorded from six different HDR sequences, the bit depth of individual frames are variably encoded, with the pixels with the highest bit depth corresponding to areas around the most likely fixation point for the frame. The bit depth of pixels within the modified frame will then gradually reduce, dependent on how far the pixel is located from the fixation point. To lower the bit depth of the HDR content, a tone mapping operator (TMO) is used. The particular TMO that is used generates an optimal tone mapping curve for every frame, which is used for both tone mapping to reduce the bit depth, and for inverse tone mapping for display purposes. However, this procedure can often cause large amounts of flickering, as well as banding artefacts, which reduce the perceptual quality of the video. Methods to mitigate these effects are proposed and implemented in this paper. Subjective performance evaluations were carried out involving 17 participants in order to evaluate the proposed methodology. Results show that when the lowest bit depth is 8 bits, the modified video is indistinguishable from the original. However, when 6 bit regions are introduced, a significant difference is noticed. Dithering and increasing the foveation region significantly improves the perceptual quality of the modified sequence.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1769–1776},
numpages = {8},
keywords = {inverse tone mapping, human visual perception, dithering, tone mapping, high dynamic range video, flickering},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240706,
author = {Cui, Wenxue and Jiang, Feng and Gao, Xinwei and Zhang, Shengping and Zhao, Debin},
title = {An Efficient Deep Quantized Compressed Sensing Coding Framework of Natural Images},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240706},
doi = {10.1145/3240508.3240706},
abstract = {Traditional image compressed sensing (CS) coding frameworks solve an inverse problem that is based on the measurement coding tools (prediction, quantization, entropy coding, etc.) and the optimization based image reconstruction method. These CS coding frameworks face the challenges of improving the coding efficiency at the encoder, while also suffering from high computational complexity at the decoder. In this paper, we move forward a step and propose a novel deep network based CS coding framework of natural images, which consists of three sub-networks: sampling sub-network, offset sub-network and reconstruction sub-network that responsible for sampling, quantization and reconstruction, respectively. By cooperatively utilizing these sub-networks, it can be trained in the form of an end-to-end metric with a proposed rate-distortion optimization loss function. The proposed framework not only improves the coding performance, but also reduces the computational cost of the image reconstruction dramatically. Experimental results on benchmark datasets demonstrate that the proposed method is capable of achieving superior rate-distortion performance against state-of-the-art methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1777–1785},
numpages = {9},
keywords = {deep neural network, compressed sensing coding, image compression},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240711,
author = {Nguyen, Diep Thi Ngoc and Nakayama, Hideki and Okazaki, Naoaki and Sakaeda, Tatsuya},
title = {PoB: Toward Reasoning Patterns of Beauty in Image Data},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240711},
doi = {10.1145/3240508.3240711},
abstract = {Aiming to develop of computational grammar system for visual information, we design a 4-tier framework that consists of four levels of 'visual grammar of images.' As a first step of realization, we propose a new dataset, named the PoB dataset, in which each image is annotated with multiple labels of armature patterns that compose the pictorial scene. The PoB dataset includes of a 10,000-painting dataset for art and a 4,959-image dataset for photography. In this paper, we discuss the consistency analysis of our dataset and its applicability. We also demonstrate how the armature patterns in the PoB dataset are useful in assessing aesthetic quality of images, and how well a deep learning algorithm can recognize these patterns. This paper seeks to set a new direction in image understanding with a more holistic approach beyond discrete objects and in aesthetic reasoning with a more interpretative way.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1786–1793},
numpages = {8},
keywords = {composition, armature pattern, image dataset, visual grammar, aesthetic assessment},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240679,
author = {Xu, Nan and Guo, Yanqing and Zheng, Xin and Wang, Qianyu and Luo, Xiangyang},
title = {Partial Multi-View Subspace Clustering},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240679},
doi = {10.1145/3240508.3240679},
abstract = {For many real-world multimedia applications, data are often described by multiple views. Therefore, multi-view learning researches are of great significance. Traditional multi-view clustering methods assume that each view has complete data. However, missing data or partial data are more common in real tasks, which results in partial multi-view learning. Therefore, we propose a novel multi-view clustering method, called Partial Multi-view Subspace Clustering (PMSC), to address the partial multi-view problem. Unlike most existing partial multi-view clustering methods that only learn a new representation of the original data, our method seeks the latent space and performs data reconstruction simultaneously to learn the subspace representation. The learned subspace representation can reveal the underlying subspace structure embedded in original data, leading to a more comprehensive data description. In addition, we enforce the subspace representation to be non-negative, yielding an intuitive weight interpretation among different data. The proposed method can be optimized by the Augmented Lagrange Multiplier (ALM) algorithm. Experiments on one synthetic dataset and four benchmark datasets validate the effectiveness of PMSC under the partial multi-view scenario.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1794–1801},
numpages = {8},
keywords = {subspace clustering, partial multi-view data, subspace structure, latent space},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240715,
author = {Long, Teng and Xu, Xing and Li, Youyou and Shen, Fumin and Song, Jingkuan and Shen, Heng Tao},
title = {Pseudo Transfer with Marginalized Corrupted Attribute for Zero-Shot Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240715},
doi = {10.1145/3240508.3240715},
abstract = {Zero-shot learning (ZSL) aims to recognize unseen classes that are excluded from training classes. ZSL suffers from 1) Zero-shot bias (Z-Bias) --- model is biased towards seen classes because unseen data is inaccessible for training; 2) Zero-shot variance (Z-Variance) --- associating different images to same semantic embedding yields large associating error. To reduce Z-Bias, we propose a pseudo transfer mechanism, where we first synthesize the distribution of unseen data using semantic embeddings, then we minimize the mismatch between the seen distribution and the synthesized unseen distribution. To reduce Z-Variance, we implicitly corrupted one semantic embedding multiple times to generate image-wise semantic vectors, with which our model learn robust classifiers. Lastly, we integrate our Z-Bias and Z-variance reduction techniques with a linear ZSL model to show its usefulness. Our proposed model successfully overcomes the Z-bias and Z-variance problems. Extensive experiments on five benchmark datasets including ImageNet-1K demonstrate that our model outperforms the state-of-the-art methods with fast training.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1802–1810},
numpages = {9},
keywords = {pseudo transfer, marginalized corrupted attributes, zero-shot learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240693,
author = {Han, Guangxing and Zhang, Xuan and Li, Chongrong},
title = {Semi-Supervised DFF: Decoupling Detection and Feature Flow for Video Object Detectors},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240693},
doi = {10.1145/3240508.3240693},
abstract = {For efficient video object detection, our detector consists of a spatial module and a temporal module. The spatial module aims to detect objects in static frames using convolutional networks, and the temporal module propagates high-level CNN features to nearby frames via light-weight feature flow. Alternating the spatial and temporal module by a proper interval makes our detector fast and accurate. Then we propose a two-stage semi-supervised learning framework to train our detector, which fully exploits unlabeled videos by decoupling the spatial and temporal module. In the first stage, the spatial module is learned by traditional supervised learning. In the second stage, we employ both feature regression loss and feature semantic loss to learn our temporal module via unsupervised learning. Different to traditional methods, our method can largely exploit unlabeled videos and bridges the gap of object detectors in image and video domain. Experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of our method. Code will be made publicly available.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1811–1819},
numpages = {9},
keywords = {video object detection, feature flow, knowledge distillation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240699,
author = {Wang, Lingjing and Qian, Cheng and Wang, Jifei and Fang, Yi},
title = {Unsupervised Learning of 3D Model Reconstruction from Hand-Drawn Sketches},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240699},
doi = {10.1145/3240508.3240699},
abstract = {3D objects modeling has gained considerable attention in the visual computing community. We propose a low-cost unsupervised learning model for 3D objects reconstruction from hand-drawn sketches. Recent advancements in deep learning opened new opportunities to learn high-quality 3D objects from 2D sketches via supervised networks. However, the limited availability of labeled 2D hand-drawn sketches data (i.e. sketches and its corresponding 3D ground truth models) hinders the training process of supervised methods. In this paper, driven by a novel design of combination of retrieval and reconstruction process, we developed a learning paradigm to reconstruct 3D objects from hand-drawn sketches, without the use of well-labeled hand-drawn sketch data during the entire training process. Specifically, the paradigm begins with the training of an adaption network via autoencoder with adversarial loss, embedding the unpaired 2D rendered image domain with the hand-drawn sketch domain to a shared latent vector space. Then from the embedding latent space, for each testing sketch image, we retrieve a few (e.g. five) nearest neighbors from the training 3D data set as prior knowledge for a 3D Generative Adversarial Network. Our experiments verify our network's robust and superior performance in handling 3D volumetric object generation from single hand-drawn sketch without requiring any 3D ground truth labels.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1820–1828},
numpages = {9},
keywords = {generative model, unsupervised learning, sketch modeling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240713,
author = {Song, Sibo and Cheung, Ngai-Man and Chandrasekhar, Vijay and Mandal, Bappaditya},
title = {Deep Adaptive Temporal Pooling for Activity Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240713},
doi = {10.1145/3240508.3240713},
abstract = {Deep neural networks have recently achieved competitive accuracy for human activity recognition. However, there is room for improvement, especially in modeling of long-term temporal importance and determining the activity relevance of different temporal segments in a video. To address this problem, we propose a learnable and differentiable module: Deep Adaptive Temporal Pooling (DATP). DATP applies a self-attention mechanism to adaptively pool the classification scores of different video segments. Specifically, using frame-level features, DATP regresses importance of different temporal segments, and generates weights for them. Remarkably, DATP is trained using only the video-level label. There is no need of additional supervision except video-level activity class label. We conduct extensive experiments to investigate various input features and different weight models. Experimental results show that DATP can learn to assign large weights to key video segments. More importantly, DATP can improve training of frame-level feature extractor. This is because relevant temporal segments are assigned large weights during back-propagation. Overall, we achieve state-of-the-art performance on UCF101, HMDB51 and Kinetics datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1829–1837},
numpages = {9},
keywords = {human activity recognition, adaptive temporal pooling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240717,
author = {Zeng, Mingyong and Tian, Chang and Wu, Zemin},
title = {Person Re-Identification with Hierarchical Deep Learning Feature and Efficient XQDA Metric},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240717},
doi = {10.1145/3240508.3240717},
abstract = {Feature learning and metric learning are two important components in person re-identification (re-id). In this paper, we utilize both aspects to refresh the current State-Of-The-Arts (SOTA). Our solution is based on a classification network with label smoothing regularization (LSR) and multi-branch tree structure. The insight is that some middle network layers are found surprisingly better than the last layers on the re-id task. A Hierarchical Deep Learning Feature (HDLF) is thus proposed by combining such useful middle layers. To learn the best metric for the high-dimensional HDLF, an efficient eXQDA metric is proposed to deal with the large-scale big-data scenarios. The proposed HDLF and eXQDA are evaluated with current SOTA methods on five benchmark datasets. Our methods achieve very high re-id results, which are far beyond state-of-the-art solutions. For example, our approach reaches 81.6%, 96.1% and 95.6% Rank-1 accuracies on the ILIDS-VID, PRID2011 and Market-1501 datasets. Besides, the code and related materials (lists of over 1800 re-id papers and 170 top conference re-id papers) are released for research purposes.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1838–1846},
numpages = {9},
keywords = {metric learning, deep learning, person re-identification},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240688,
author = {Song, Jingkuan and Zhou, Zhilong and Gao, Lianli and Xu, Xing and Shen, Heng Tao},
title = {Cumulative Nets for Edge Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240688},
doi = {10.1145/3240508.3240688},
abstract = {Lots of recent progress have been made by using Convolutional Neural Networks (CNN) for edge detection. Due to the nature of hierarchical representations learned in CNN, it is intuitive to design side networks utilizing the richer convolutional features to improve the edge detection. However, different side networks are isolated, and the final results are usually weighted sum of the side outputs with uneven qualities. To tackle these issues, we propose a Cumulative Network (C-Net), which learns the side network cumulatively based on current visual features and low-level side outputs, to gradually remove detailed or sharp boundaries to enable high-resolution and accurate edge detection. Therefore, the lower-level edge information is cumulatively inherited while the superfluous details are progressively abandoned. In fact, recursively Learningwhere to remove superfluous details from the current edge map with the supervision of a higher-level visual feature is challenging. Furthermore, we employ atrous convolution (AC) and atrous convolution pyramid pooling (ASPP) to robustly detect object boundaries at multiple scales and aspect ratios. Also, cumulatively refining edges using high-level visual information and lower-lever edge maps is achieved by our designed cumulative residual attention (CRA) block. Experimental results show that our C-Net sets new records for edge detection on both two benchmark datasets: BSDS500 (i.e., .819 ODS, .835 OIS and .862 AP) and NYUDV2 (i.e., .762 ODS, .781 OIS, .797 AP). C-Net has great potential to be applied to other deep learning based applications, e.g., image classification and segmentation.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1847–1855},
numpages = {9},
keywords = {edge detection, cnn, cumulative residual attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240712,
author = {Mithun, Niluthpol Chowdhury and Panda, Rameswar and Papalexakis, Evangelos E. and Roy-Chowdhury, Amit K.},
title = {Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240712},
doi = {10.1145/3240508.3240712},
abstract = {Cross-modal retrieval between visual data and natural language description remains a long-standing challenge in multimedia. While recent image-text retrieval methods offer great promise by learning deep representations aligned across modalities, most of these methods are plagued by the issue of training with small-scale datasets covering a limited number of images with ground-truth sentences. Moreover, it is extremely expensive to create a larger dataset by annotating millions of images with sentences and may lead to a biased model. Inspired by the recent success of webly supervised learning in deep neural networks, we capitalize on readily-available web images with noisy annotations to learn robust image-text joint representation. Specifically, our main idea is to leverage web images and corresponding tags, along with fully annotated datasets, in training for learning the visual-semantic joint embedding. We propose a two-stage approach for the task that can augment a typical supervised pair-wise ranking loss based formulation with weakly-annotated web images to learn a more robust visual-semantic embedding. Experiments on two standard benchmark datasets demonstrate that our method achieves a significant performance gain in image-text retrieval compared to state-of-the-art approaches.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1856–1864},
numpages = {9},
keywords = {image-text retrieval, webly supervised learning, joint embedding},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240541,
author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Xu, Xin-Shun and Kankanhalli, Mohan},
title = {Multi-Modal Preference Modeling for Product Search},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240541},
doi = {10.1145/3240508.3240541},
abstract = {The visual preference of users for products has been largely ignored by the existing product search methods. In this work, we propose a multi-modal personalized product search method, which aims to search products which not only are relevant to the submitted textual query, but also match the user preferences from both textual and visual modalities. To achieve the goal, we first leverage the also_view and buy_after_viewing products to construct the visual and textual latent spaces, which are expected to preserve the visual similarity and semantic similarity of products, respectively. We then propose a translation-based search model (TranSearch ) to 1) learn a multi-modal latent space based on the pre-trained visual and textual latent spaces; and 2) map the users, queries and products into this space for direct matching. The TranSearch model is trained based on a comparative learning strategy, such that the multi-modal latent space is oriented to personalized ranking in the training stage. Experiments have been conducted on real-world datasets to validate the effectiveness of our method. The results demonstrate that our method outperforms the state-of-the-art method by a large margin.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1865–1873},
numpages = {9},
keywords = {product search, personalization, multi-modal fusion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240614,
author = {Huang, Feiran and Zhang, Xiaoming and Li, Zhoujun},
title = {Learning Joint Multimodal Representation with Adversarial Attention Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240614},
doi = {10.1145/3240508.3240614},
abstract = {Recently, learning a joint representation for the multimodal data (e.g., containing both visual content and text description) has attracted extensive research interests. Usually, the features of different modalities are correlational and compositive, and thus a joint representation capturing the correlation is more effective than a subset of the features. Most of existing multimodal representation learning methods suffer from lack of additional constraints to enhance the robustness of the learned representations. In this paper, a novel Adversarial Attention Networks (AAN) is proposed to incorporate both the attention mechanism and the adversarial networks for effective and robust multimodal representation learning. Specifically, a visual-semantic attention model with siamese learning strategy is proposed to encode the fine-grained correlation between visual and textual modalities. Meanwhile, the adversarial learning model is employed to regularize the generated representation by matching the posterior distribution of the representation to the given priors. Then, the two modules are incorporated into a integrated learning framework to learn the joint multimodal representation. Experimental results in two tasks, i.e., multi-label classification and tag recommendation, show that the proposed model outperforms state-of-the-art representation learning methods.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1874–1882},
numpages = {9},
keywords = {attention model, representation learning, multimodal, adversarial networks, siamese learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240656,
author = {Liao, Binbing and Zhang, Jingqing and Cai, Ming and Tang, Siliang and Gao, Yifan and Wu, Chao and Yang, Shengwen and Zhu, Wenwu and Guo, Yike and Wu, Fei},
title = {Dest-ResNet: A Deep Spatiotemporal Residual Network for Hotspot Traffic Speed Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240656},
doi = {10.1145/3240508.3240656},
abstract = {With the ever-increasing urbanization process, the traffic jam has become a common problem in the metropolises around the world, making the traffic speed prediction a crucial and fundamental task. This task is difficult due to the dynamic and intrinsic complexity of the traffic environment in urban cities, yet the emergence of crowd map query data sheds new light on it. In general, a burst of crowd map queries for the same destination in a short duration (called "hotspot'') could lead to traffic congestion. For example, queries of the Capital Gym burst on weekend evenings lead to traffic jams around the gym. However, unleashing the power of crowd map queries is challenging due to the innate spatiotemporal characteristics of the crowd queries. To bridge the gap, this paper firstly discovers hotspots underlying crowd map queries. These discovered hotspots address the spatiotemporal variations. Then Dest-ResNet (Deep spatiotemporal Residual Network) is proposed for hotspot traffic speed prediction. Dest-ResNet is a sequence learning framework that jointly deals with two sequences in different modalities, i.e., the traffic speed sequence and the query sequence. The main idea of Dest-ResNet is to learn to explain and amend the errors caused when the unimodal information is applied individually. In this way, Dest-ResNet addresses the temporal causal correlation between queries and the traffic speed. As a result, Dest-ResNet shows a 30% relative boost over the state-of-the-art methods on real-world datasets from Baidu Map.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1883–1891},
numpages = {9},
keywords = {traffic speed prediction, lstm, crowd map query, social media},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240631,
author = {Yin, Yifang and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {Learning and Fusing Multimodal Deep Features for Acoustic Scene Categorization},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240631},
doi = {10.1145/3240508.3240631},
abstract = {Convolutional Neural Networks (CNNs) have been widely applied to audio classification recently where promising results have been obtained. Previous CNN-based systems mostly learn from two-dimensional time-frequency representations such as MFCC and spectrograms, which may tend to emphasize more on the background noise of the scene. To learn the key acoustic events, we introduce a three-dimensional CNN to emphasize on the different spectral characteristics from neighboring regions in spatial-temporal domain. A novel acoustic scene classification system based on multimodal deep feature fusion is proposed in this paper, where three CNNs have been presented to perform 1D raw waveform modeling, 2D time-frequency image modeling, and 3D spatial-temporal dynamics modeling, respectively. The learnt features are shown to be highly complementary to each other, which are next combined in a feature fusion network to obtain significantly improved classification predictions. Comprehensive experiments have been conducted on two large-scale acoustic scene datasets, namely the DCASE16 dataset and the LITIS Rouen dataset. Experimental results demonstrate the effectiveness of our proposed approach, as our solution achieves state-of-the-art classification rates and improves the average classification accuracy by 1.5% - 8.2% compared to the top ranked systems in the DCASE16 challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1892–1900},
numpages = {9},
keywords = {acoustic scene classification, multimodal deep features, cnn fusion, spatial-temporal dynamics modeling},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240644,
author = {Tang, Zhenyu and Morales, Nicolas and Manocha, Dinesh},
title = {Dynamic Sound Field Synthesis for Speech and Music Optimization},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240644},
doi = {10.1145/3240508.3240644},
abstract = {We present a novel acoustic optimization algorithm to synthesize dynamic sound fields in a static scene. Our approach places new active loudspeakers or virtual sources in the scene so that the dynamic sound field in a region satisfies optimization criteria to improve speech and music perception. We use a frequency domain formulation of sound propagation and reduce the computation of dynamic sound field synthesis to solving a linear least squares problem, and do not impose any constraints on the environment or loudspeakers type, or loudspeaker placement. We highlight the performance on complex indoor scenes in terms of speech and music improvements. We evaluate the performance with a user study and highlight the perceptual benefits for virtual reality and multimedia applications.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1901–1909},
numpages = {9},
keywords = {speech improvement, sound propagation, virtual environments, acoustic optimization, music reinforcement},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240701,
author = {Forgione, Thomas and Carlier, Axel and Morin, G\'{e}raldine and Ooi, Wei Tsang and Charvillat, Vincent and Yadav, Praveen Kumar},
title = {DASH for 3D Networked Virtual Environment},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240701},
doi = {10.1145/3240508.3240701},
abstract = {DASH is now a widely deployed standard for streaming video content due to its simplicity, scalability, and ease of deployment. In this paper, we explore the use of DASH for a different type of media content -- networked virtual environment (NVE), with different properties and requirements. We organize a polygon soup with textures into a structure that is compatible with DASH MPD (Media Presentation Description), with a minimal set of view-independent metadata for the client to make intelligent decisions about what data to download at which resolution. We also present a DASH-based NVE client that uses a view-dependent and network dependent utility metric to decide what to download, based only on the information in the MPD file. We show that DASH can be used on NVE for 3D content streaming. Our work opens up the possibility of using DASH for highly interactive applications, beyond its current use in video streaming.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1910–1918},
numpages = {9},
keywords = {dash, 3d streaming, networked virtual environment},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286944,
author = {Zhu, Wenwu},
title = {Session Details: Keynote 5},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286944},
doi = {10.1145/3286944},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3267341,
author = {Zhou, Bowen},
title = {Transforming Retailing Experiences with Artificial Intelligence},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3267341},
doi = {10.1145/3240508.3267341},
abstract = {Artificial Intelligence (AI) is making big impacts in our daily life. In this talk, we will show how AI is transforming retail industry. In particular, we propose the brand-new concept of Retail as a Service (RaaS), where retail is redefined as the natural combination of content and interaction. With the capability of knowing more about consumers, products and retail scenarios integrating online and offline, AI is providing more personalized and comprehensive multimodal content and enabling more natural interactions between consumers and services, through the innovative technologies we invented at JD.com. We will show 1) how computer vision techniques can better understand consumers, help consumers easily discover products, and support multimodal content generation, 2) how the natural language processing techniques can be used to support intelligent customer services through emotion computing, 3) how AI is building the very fundamental technology infrastructure for RaaS.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1919–1920},
numpages = {2},
keywords = {retail as a service (raas), artificial intelligence (ai)},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286945,
author = {Jin, Qin},
title = {Session Details: Deep-3 (Image Processing-Inpainting, Super-Resolution, Deblurring)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286945},
doi = {10.1145/3286945},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240565,
author = {Liu, Risheng and He, Yi and Cheng, Shichao and Fan, Xin and Luo, Zhongxuan},
title = {Learning Collaborative Generation Correction Modules for Blind Image Deblurring and Beyond},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240565},
doi = {10.1145/3240508.3240565},
abstract = {Blind image deblurring plays a very important role in many vision and multimedia applications. Most existing works tend to introduce complex priors to estimate the sharp image structures for blur kernel estimation. However, it has been verified that directly optimizing these models is challenging and easy to fall into degenerate solutions. Although several experience-based heuristic inference strategies, including trained networks and designed iterations, have been developed, it is still hard to obtain theoretically guaranteed accurate solutions. In this work, a collaborative learning framework is established to address the above issues. Specifically, we first design two modules, named Generator and Corrector, to extract the intrinsic image structures from the data-driven and knowledge-based perspectives, respectively. By introducing a collaborative methodology to cascade these modules, we can strictly prove the convergence of our image propagations to a deblurring-related optimal solution. As a nontrivial byproduct, we also apply the proposed method to address other related tasks, such as image interpolation and edge-preserved smoothing. Plenty of experiments demonstrate that our method can outperform the state-of-the-art approaches on both synthetic and real datasets.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1921–1929},
numpages = {9},
keywords = {blind image deblurring, generator and corrector, theoretical convergence, collaborative learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240603,
author = {Yin, Minghao and Zhang, Yongbing and Li, Xiu and Wang, Shiqi},
title = {When Deep Fool Meets Deep Prior: Adversarial Attack on Super-Resolution Network},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240603},
doi = {10.1145/3240508.3240603},
abstract = {This paper investigates the vulnerability of the deep prior used in deep learning based image restoration. In particular, the image super-resolution, which relies on the strong prior information to regularize the solution space and plays important roles in the image pre-processing for future viewing and analysis, is shown to be vulnerable to the well-designed adversarial examples. We formulate the adversarial example generation process as an optimization problem, and given super-resolution model three different types of attack are designed based on the subsequent tasks: (i) style transfer attack; (ii) classification attack; (iii) caption attack. Another interesting property of our design is that the attack is hidden behind the super-resolution process, such that the utilization of low resolution images is not significantly influenced. We show that the vulnerability to adversarial examples could bring risks to the pre-processing modules such as super-resolution deep neural network, which is also of paramount significance for the security of the whole system. Our results also shed light on the potential security issues of the pre-processing modules, and raise concerns regarding the corresponding countermeasures for adversarial examples.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1930–1938},
numpages = {9},
keywords = {super-resolution, image classification, adversarial attack, caption, style transfer, deep prior},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240625,
author = {Zhang, Haoran and Hu, Zhenzhen and Luo, Changzhi and Zuo, Wangmeng and Wang, Meng},
title = {Semantic Image Inpainting with Progressive Generative Networks},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240625},
doi = {10.1145/3240508.3240625},
abstract = {Recently, image inpainting task has revived with the help of deep learning techniques. Deep neural networks, especially the generative adversarial networks~(GANs) make it possible to recover the missing details in images. Due to the lack of sufficient context information, most existing methods fail to get satisfactory inpainting results. This work investigates a more challenging problem, e.g., the newly-emerging semantic image inpainting - a task to fill in large holes in natural images. In this paper, we propose an end-to-end framework named progressive generative networks~(PGN), which regards the semantic image inpainting task as a curriculum learning problem. Specifically, we divide the hole filling process into several different phases and each phase aims to finish a course of the entire curriculum. After that, an LSTM framework is used to string all the phases together. By introducing this learning strategy, our approach is able to progressively shrink the large corrupted regions in natural images and yields promising inpainting results. Moreover, the proposed approach is quite fast to evaluate as the entire hole filling is performed in a single forward pass. Extensive experiments on Paris Street View and ImageNet dataset clearly demonstrate the superiority of our approach. Code for our models is available at https://github.com/crashmoon/Progressive-Generative-Networks.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1939–1947},
numpages = {9},
keywords = {semantic image inpainting, lstm, progressive generative networks, curriculum learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240678,
author = {Vo, Huy V. and Duong, Ngoc Q. K. and P\'{e}rez, Patrick},
title = {Structural Inpainting},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240678},
doi = {10.1145/3240508.3240678},
abstract = {Scene-agnostic visual inpainting remains very challenging despite progress in patch-based methods. Recently, Pathak et al. [26] have introduced convolutional "context encoders'' (CEs) for unsupervised feature learning through image completion tasks. With the additional help of adversarial training, CEs turned out to be a promising tool to complete complex structures in real inpainting problems. In the present paper we propose to push further this key ability by relying on perceptual reconstruction losses at training time. We show on a wide variety of visual scenes the merit of the approach forstructural inpainting, and confirm it through a user study. Combined with the optimization-based refinement of [32] with neural patches, our context encoder opens up new opportunities for prior-free visual inpainting.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1948–1956},
numpages = {9},
keywords = {deep neural network (dnn)., context encoders, plausible structural completion, adversarial loss, image inpainting},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286946,
author = {Aizawa, Kiyoharu},
title = {Session Details: Brave New Ideas},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286946},
doi = {10.1145/3286946},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241916,
author = {Andriluka, Mykhaylo and Uijlings, Jasper R. R. and Ferrari, Vittorio},
title = {Fluid Annotation: A Human-Machine Collaboration Interface for Full Image Annotation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241916},
doi = {10.1145/3240508.3241916},
abstract = {We introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid annotation is based on three principles:(I) Strong Machine-Learning aid. We start from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions.The edit operations are also assisted by the model.(II) Full image annotation in a single pass. As opposed to performing a series of small annotation tasks in isolation [51,68], we propose a unified interface for full image annotation in a single pass.(III) Empower the annotator.We empower the annotator to choose what to annotate and in which order. This enables concentrating on what the ma-chine does not already know, i.e. putting human effort only on the errors it made. This helps using the annotation budget effectively.Through extensive experiments on the COCO+Stuff dataset [11,51], we demonstrate that Fluid Annotation leads to accurate an-notations very efficiently, taking 3x less annotation time than the popular LabelMe interface [70].},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1957–1966},
numpages = {10},
keywords = {human-machine collaboration, computer vision, image annotation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241910,
author = {Liu, Lixin and Wan, Xiaojun and Guo, Zongming},
title = {Images2Poem: Generating Chinese Poetry from Image Streams},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241910},
doi = {10.1145/3240508.3241910},
abstract = {Natural language generation from visual inputs has attracted extensive research attention recently. Generating poetry from visual content is an interesting but very challenging task. We propose and address the new multimedia task of generating classical Chinese poetry from image streams. In this paper, we propose an Images2Poem model with a selection mechanism and an adaptive self-attention mechanism for the problem. The model first selects representative images to summarize the image stream. During decoding, it adaptively pays attention to the information from either source-side image stream or target-side previously generated characters. It jointly summarizes the images and generates relevant, high-quality poetry from image streams. Experimental results demonstrate the effectiveness of the proposed approach. Our model outperforms baselines in different human evaluation metrics.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1967–1975},
numpages = {9},
keywords = {image streams, adaptive self-attention mechanism, poetry generation, selection mechanism},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241911,
author = {Kumar, Yaman and Aggarwal, Mayank and Nawal, Pratham and Satoh, Shin'ichi and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {Harnessing AI for Speech Reconstruction Using Multi-View Silent Video Feed},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241911},
doi = {10.1145/3240508.3241911},
abstract = {Speechreading or lipreading is the technique of understanding and getting phonetic features from a speaker's visual features such as movement of lips, face, teeth and tongue. It has a wide range of multimedia applications such as in surveillance, Internet telephony, and as an aid to a person with hearing impairments. However, most of the work in speechreading has been limited to text generation from silent videos. Recently, research has started venturing into generating (audio) speech from silent video sequences but there have been no developments thus far in dealing with divergent views and poses of a speaker. Thus although, we have multiple camera feeds for the speech of a user, but we have failed in using these multiple video feeds for dealing with the different poses. To this end, this paper presents the world's first ever multi-view speech reading and reconstruction system. This work encompasses the boundaries of multimedia research by putting forth a model which leverages silent video feeds from multiple cameras recording the same subject to generate intelligent speech for a speaker. Initial results confirm the usefulness of exploiting multiple camera views in building an efficient speech reading and reconstruction system. It further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Next, it lays out various innovative applications for the proposed system focusing on its potential prodigious impact in not just security arena but in many other multimedia analytics problems.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1976–1983},
numpages = {8},
keywords = {cnn-lstm models, speech reconstruction, multimedia systems, automatic speech reconstruction, speechreading, lipreading},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241912,
author = {Bahirat, Kanchan and Shah, Umang and Cardenas, Alvaro A. and Prabhakaran, Balakrishnan},
title = {ALERT: Adding a Secure Layer in Decision Support for Advanced Driver Assistance System (ADAS)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241912},
doi = {10.1145/3240508.3241912},
abstract = {With the ever-increasing popularity of LiDAR (Light Image Detection and Ranging) sensors, a wide range of applications such as vehicle automation and robot navigation are developed utilizing the 3D LiDAR data. Many of these applications involve remote guidance - either for safety or for the task performance - of these vehicles and robots. Research studies have exposed vulnerabilities of using LiDAR data by considering different security attack scenarios. Considering the security risks associated with the improper behavior of these applications, it has become crucial to authenticate the 3D LiDAR data that highly influence the decision making in such applications. In this paper, we propose a framework, ALERT (Authentication, Localization, and Estimation of Risks and Threats), as a secure layer in the decision support system used in the navigation control of vehicles and robots. To start with, ALERT tamper-proofs 3D LiDAR data by employing an innovative mechanism for creating and extracting a dynamic watermark. Next, when tampering is detected (because of the inability to verify the dynamic watermark), ALERT then carries out cross-modal authentication for localizing the tampered region. Finally, ALERT estimates the level of risk and threat based on the temporal and spatial nature of the attacks on LiDAR data. This estimation of risk and threats can then be incorporated into the decision support system used by ADAS (Advanced Driver Assistance System). We carried out several experiments to evaluate the efficacy of the proposed ALERT for ADAS and the experimental results demonstrate the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1984–1992},
numpages = {9},
keywords = {adas, 3d watermarking, lidar},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241913,
author = {Nag, Nitish and Pandey, Vaibhav and Putzel, Preston J. and Bhimaraju, Hari and Krishnan, Srikanth and Jain, Ramesh},
title = {Cross-Modal Health State Estimation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241913},
doi = {10.1145/3240508.3241913},
abstract = {Individuals create and consume more diverse data about themselves today than any time in history. Sources of this data include wearable devices, images, social media, geo-spatial information and more. A tremendous opportunity rests within cross-modal data analysis that leverages existing domain knowledge methods to understand and guide human health. Especially in chronic diseases, current medical practice uses a combination of sparse hospital based biological metrics (blood tests, expensive imaging, etc.) to understand the evolving health status of an individual. Future health systems must integrate data created at the individual level to better understand health status perpetually, especially in a cybernetic framework. In this work we fuse multiple user created and open source data streams along with established biomedical domain knowledge to give two types of quantitative state estimates of cardiovascular health. First, we use wearable devices to calculate cardiorespiratory fitness (CRF), a known quantitative leading predictor of heart disease which is not routinely collected in clinical settings. Second, we estimate inherent genetic traits, living environmental risks, circadian rhythm, and biological metrics from a diverse dataset. Our experimental results on 24 subjects demonstrate how multi-modal data can provide personalized health insight. Understanding the dynamic nature of health status will pave the way for better health based recommendation engines, better clinical decision making and positive lifestyle changes.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1993–2002},
numpages = {10},
keywords = {cybernetic health, wearables, health informatics, health situation, multimedia, personal health navigation, cross-modal data},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286947,
author = {Jiang, Shuqiang},
title = {Session Details: Grand Challenge-1},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286947},
doi = {10.1145/3286947},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266438,
author = {Li, Liuwu and Huang, Sihong and He, Ziliang and Liu, Wenyin},
title = {An Effective Text-Based Characterization Combined with Numerical Features for Social Media Headline Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266438},
doi = {10.1145/3240508.3266438},
abstract = {In this paper, a text-based characterization combined with numerical features for Social Media Headline Prediction (SMHP) is proposed. Description of images, users' emotions and opinions are all described in text, our text-based characterization learns these important features by training a Doc2vec model. Numerical features of social cues contain general characteristics of social media headline, we build an effective method to extract numerical features. Experiments conducted on real-world SMHP dataset manifest the effectiveness of the proposed approach, which achieves the following performance: Spearmanr's Rho: 0.4559, MAE:1.9797.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2003–2007},
numpages = {5},
keywords = {SMHP, text-based characterization, combination},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266443,
author = {Hsu, Chih-Chung and Lee, Chia-Yen and Liao, Ting-Xuan and Lee, Jun-Yi and Hou, Tsai-Yne and Kuo, Ying-Chu and Lin, Jing-Wen and Hsueh, Ching-Yi and Zhang, Zhong-Xuan and Chien, Hsiang-Chin},
title = {An Iterative Refinement Approach for Social Media Headline Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266443},
doi = {10.1145/3240508.3266443},
abstract = {In this study, we propose a novel iterative refinement approach to predict the popularity score of the social media meta-data effectively. With the rapid growth of the social media on the Internet, how to adequately forecast the view count or popularity becomes more important. Conventionally, the ensemble approach such as random forest regression achieves high and stable performance on various prediction tasks. However, most of the regression methods may not precisely predict the extreme high or low values. To address this issue, we first predict the initial popularity score and retrieve their residues. In order to correctly compensate those extreme values, we adopt an ensemble regressor to compensate the residues to further improve the prediction performance. Comprehensive experiments are conducted to demonstrate the proposed iterative refinement approach outperforms the state-of-the-art regression approach.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2008–2012},
numpages = {5},
keywords = {ensemble learning, regression, random forest, iterative refinement},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266439,
author = {Huang, Feitao and Chen, Junhong and Lin, Zehang and Kang, Peipei and Yang, Zhenguo},
title = {Random Forest Exploiting Post-Related and User-Related Features for Social Media Popularity Prediction},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266439},
doi = {10.1145/3240508.3266439},
abstract = {Social media headline prediction (SMHP) is a thriving application scenario, which aims to predict the popularity of the post data shared on social media. In this paper, we propose to use multi-aspect features combined with the random forest (RF) model for popularity predictions. Firstly, we extract features by combining both metadata of the posts and users' features. More specifically, we adopt the binary coding strategy for dimensionality reduction and deal with the missing values by using some strategies, i.e., estimating the missing geographic information according to the information of users, and filling the missing features with median. Furthermore, regression models can be used directly to make predictions. In particular, a random forest (RF) model is adopted since it does not require much effort in tuning hyper-parameters and performs effectively. Extensive experiments conducted on the SMHP dataset consisting of 340K image posts shared by 80K users manifest the effectiveness of our method. Our approach achieves the 4nd place in the leader board of the Grand Challenge of SMHP in ACM Multimedia 2018.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2013–2017},
numpages = {5},
keywords = {regression model, social media, popularity prediction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266434,
author = {Chen, Xusong and Zhao, Rui and Ma, Shengjie and Liu, Dong and Zha, Zheng-Jun},
title = {Content-Based Video Relevance Prediction with Second-Order Relevance and Attention Modeling},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266434},
doi = {10.1145/3240508.3266434},
abstract = {This paper describes our proposed method for the Content-Based Video Relevance Prediction (CBVRP) challenge. Our method is based on deep learning, i.e. we train a deep network to predict the relevance between two video sequences from their features. We explore the usage of second-order relevance, both in preparing training data, and in extending the deep network. Second-order relevance refers to e.g. the relevance between x and z if x is relevant to y and y is relevant to z. In our proposed method, we use second-order relevance to increase positive samples and decrease negative samples, when preparing training data. We further extend the deep network with an attention module, where the attention mechanism is designed for second-order relevant video sequences. We verify the effectiveness of our method on the validation set of the CBVRP challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2018–2022},
numpages = {5},
keywords = {content-based filtering, video relevance prediction, attention mechanism, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286948,
author = {Worring, Marcel},
title = {Session Details: Vision-4 (Representation Learning)},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286948},
doi = {10.1145/3286948},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240523,
author = {Chen, Tianshui and Wu, Wenxi and Gao, Yuefang and Dong, Le and Luo, Xiaonan and Lin, Liang},
title = {Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240523},
doi = {10.1145/3240508.3240523},
abstract = {Object categories inherently form a hierarchy with different levels of concept abstraction, especially for fine-grained categories. For example, birds (Aves) can be categorized according to a four-level hierarchy of order, family, genus, and species. This hierarchy encodes rich correlations among various categories across different levels, which can effectively regularize the semantic space and thus make prediction less ambiguous. However, previous studies of fine-grained image recognition primarily focus on categories of one certain level and usually overlook this correlation information. In this work, we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding (HSE) framework. Specifically, the HSE framework sequentially predicts the category score vector of each level in the hierarchy, from highest to lowest. At each level, it incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation. During training, the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories. To evaluate the proposed framework, we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories. Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2023–2031},
numpages = {9},
keywords = {fine-grained image recognition, category hierarchy, semantic embedding},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240686,
author = {Yang, Gang and Liu, Jinlu and Xu, Jieping and Li, Xirong},
title = {Dissimilarity Representation Learning for Generalized Zero-Shot Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240686},
doi = {10.1145/3240508.3240686},
abstract = {Generalized zero-shot learning (GZSL) aims to recognize any test instance coming either from a known class or from a novel class that has no training instance. To synthesize training instances for novel classes and thus resolving GZSL as a common classification problem, we propose a Dissimilarity Representation Learning (DSS) method. Dissimilarity representation is to represent a specific instance in terms of its (dis)similarity to other instances in a visual or attribute based feature space. In the dissimilarity space, instances of the novel classes are synthesized by an end-to-end optimized neural network. The neural network realizes two-level feature mappings and domain adaptions in the dissimilarity space and the attribute based feature space. Experimental results on five benchmark datasets, i.e., AWA, AWA$_2$, SUN, CUB, and aPY, show that the proposed method improves the state-of-the-art with a large margin, approximately 10% gain in terms of the harmonic mean of the top-1 accuracy. Consequently, this paper establishes a new baseline for GZSL.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2032–2039},
numpages = {8},
keywords = {dissimilarity representation, generalized zero-shot learning, feature mapping},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240550,
author = {Han, Kai and Guo, Jianyuan and Zhang, Chao and Zhu, Mingjian},
title = {Attribute-Aware Attention Model for Fine-Grained Representation Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240550},
doi = {10.1145/3240508.3240550},
abstract = {How to learn a discriminative fine-grained representation is a key point in many computer vision applications, such as person re-identification, fine-grained classification, fine-grained image retrieval, etc. Most of the previous methods focus on learning metrics or ensemble to derive better global representation, which are usually lack of local information. Based on the considerations above, we propose a novel Attribute-Aware Attention Model ($A^3M$), which can learn local attribute representation and global category representation simultaneously in an end-to-end manner. The proposed model contains two attention models: attribute-guided attention module uses attribute information to help select category features in different regions, at the same time, category-guided attention module selects local features of different attributes with the help of category cues. Through this attribute-category reciprocal process, local and global features benefit from each other. Finally, the resulting feature contains more intrinsic information for image recognition instead of the noisy and irrelevant features. Extensive experiments conducted on Market-1501, CompCars, CUB-200-2011 and CARS196 demonstrate the effectiveness of our $A^3M$.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2040–2048},
numpages = {9},
keywords = {deep learning, fine-grained recognition, attribute-aware attention},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3240588,
author = {Huang, Siyu and Li, Xi and Cheng, Zhi-Qi and Zhang, Zhongfei and Hauptmann, Alexander},
title = {GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute Learning},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240588},
doi = {10.1145/3240508.3240588},
abstract = {A key problem in deep multi-attribute learning is to effectively discover the inter-attribute correlation structures. Typically, the conventional deep multi-attribute learning approaches follow the pipeline of manually designing the network architectures based on task-specific expertise prior knowledge and careful network tunings, leading to the inflexibility for various complicated scenarios in practice. Motivated by addressing this problem, we propose an efficient greedy neural architecture search approach (GNAS) to automatically discover the optimal tree-like deep architecture for multi-attribute learning. In a greedy manner, GNAS divides the optimization of global architecture into the optimizations of individual connections step by step. By iteratively updating the local architectures, the global tree-like architecture gets converged where the bottom layers are shared across relevant attributes and the branches in top layers more encode attribute-specific features. Experiments on three benchmark multi-attribute datasets show the effectiveness and compactness of neural architectures derived by GNAS, and also demonstrate the efficiency of GNAS in searching neural architectures.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2049–2057},
numpages = {9},
keywords = {greedy algorithm, neural architecture search, multi-attribute analysis, multi-task learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286949,
author = {Jiang, Shuqiang},
title = {Session Details: Grand Challenge-2},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286949},
doi = {10.1145/3286949},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266441,
author = {Dong, Jianfeng and Li, Xirong and Xu, Chaoxi and Yang, Gang and Wang, Xun},
title = {Feature Re-Learning with Data Augmentation for Content-Based Video Recommendation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266441},
doi = {10.1145/3240508.3266441},
abstract = {This paper describes our solution for the Hulu Content-based Video Relevance Prediction Challenge. Noting the deficiency of the original features, we propose feature re-learning to improve video relevance prediction. To generate more training instances for supervised learning, we develop two data augmentation strategies, one for frame-level features and the other for video-level features. In addition, late fusion of multiple models is employed to further boost the performance. Evaluation conducted by the organizers shows that our best run outperforms the Hulu baseline, obtaining relative improvements of 26.2% and 30.2% on the TV-shows track and the Movies track, respectively, in terms of recall@100. The results clearly justify the effectiveness of the proposed solution.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2058–2062},
numpages = {5},
keywords = {feature re-learning, data augmentation, content based video recommendation},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266431,
author = {Wang, Qi and Lai, Jingxiang and Xu, Kai and Liu, Wenyin and Lei, Liang},
title = {Beauty Product Image Retrieval Based on Multi-Feature Fusion and Feature Aggregation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266431},
doi = {10.1145/3240508.3266431},
abstract = {We propose a beauty product image retrieval method based on multi-feature fusion and feature aggregation. The key idea is representing the image with the feature vector obtained by multi-feature fusion and feature aggregation. VGG16 and ResNet50 are chosen to extract image features, and Crow is adopted to perform deep feature aggregation. Benefited from the idea of transfer learning, we fine turn VGG16 on the Perfect-500K data set to improve the performance of image retrieval. The proposed method won the third price in Perfect Corp. Challenge 2018 with the best result 0.270676 mAP. We released our code on GitHub: https://github.com/wangqi12332155/ACMMM-beauty-AI-challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2063–2067},
numpages = {5},
keywords = {image retrieval, feature aggregation, multi-feature fusion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266433,
author = {Lim, Jian Han and Japar, Nurul and Ng, Chun Chet and Chan, Chee Seng},
title = {Unprecedented Usage of Pre-Trained CNNs on Beauty Product},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266433},
doi = {10.1145/3240508.3266433},
abstract = {How does a pre-trained Convolution Neural Network (CNN) model perform on beauty and personal care items (i.e Perfect-500K) This is the question we attempt to answer in this paper by adopting several well known deep learning models pre-trained on ImageNet, and evaluate their performance using different distance metrics. In the Perfect Corp Challenge, we manage to secure fourth position by using only the pre-trained model.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2068–2072},
numpages = {5},
keywords = {beauty product classification, deep learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3266436,
author = {Lin, Zehang and Yang, Zhenguo and Huang, Feitao and Chen, Junhong},
title = {Regional Maximum Activations of Convolutions with Attention for Cross-Domain Beauty and Personal Care Product Retrieval},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3266436},
doi = {10.1145/3240508.3266436},
abstract = {Cross-domain beauty and personal care product image retrieval is a challenging problem due to data variations (e.g., brightness, viewpoint, and scale), and the rich types of items. In this paper, we present a regional maximum activations of convolutions with attention (RA-MAC) descriptor to extract image features for retrieval. RA-MAC improves the regional maximum activations of convolutions (R-MAC) descriptor considering the influence of background in cross-domain images (i.e., shopper domain and seller domain). More specifically, RA-MAC utilizes the characteristics of the convolutional layer to find the attention of an image, and reduces the influence of the unimportant regions in an unsupervised manner. Furthermore, a few strategies have been exploited to improve the performance, such as multiple features fusion, query expansion, and database augmentation. Extensive experiments conducted on a dataset consisting of half a million images of beauty care products (Perfect-500K) manifest the effectiveness of RA-MAC. Our approach achieves the 2nd place in the leader board of the Grand Challenge of AI Meets Beauty in ACM Multimedia 2018. Our code is available at: https://github.com/RetrainIt/Perfect-Half-Million-Beauty-Product-Image-Recognition-Challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2073–2077},
numpages = {5},
keywords = {RA-MAC, cross-domain image retrieval, attention mechanism},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3286950,
author = {Shim, Hyunjung},
title = {Session Details: Interactive Art},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286950},
doi = {10.1145/3286950},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3264576,
author = {Chen, Lyn Chao-ling and Luo, He-lin},
title = {Shadow Calligraphy of Dance: An Image-Based Interactive Installation for Capturing Flowing Human Figures},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3264576},
doi = {10.1145/3240508.3264576},
abstract = {In the artwork, the topic of flowing human figures has been discussed. People pass through familiar places day by day, in which they create connection among them and the city. The impressions, memories and experiences turn the definition of the space in the city into place, and it is meaningful and creates a virtual layer upon the physical world. The artwork tried to arouse people to aware the connection among them and the environment by revealing the invisible traces. The interactive installation was set in outdoor exhibition, and a camera was set align the road and a projector was used for performing image on the wall of the nearby building. Object detection technology has been used in the interactive installation for capturing movements of people. GMM modeling was adopted for capturing frames with vivid face features, and the parameters was set for generating afterimage effect. The projected picture on the wall combined with 25 frames in different update time setting for performing a delayed vision, and only one region in the center of the image played the current frame in real-time, for arousing audience to notice the connection between their movements and the projected picture. In addition, some of them were reversed in horizontal direction for creating a dynamic Chinese brush painting with aesthetic composition. The remaining figures on the wall as mark or print remind people their traces in the city, and that creates the connection among the city and people who has been to the place at the same time. In the interactive installation, the improvisational painting of body calligraphy was exhibited in a collaborative way, in which revealed the face features or human shapes of the crowd in physical point, and also the collaborative experiences or memories in mental aspect.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2078–2080},
numpages = {3},
keywords = {interactive installation, image sequence analysis, psycogeography},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3264577,
author = {Haron, Anis and Soon Xuan, Yong and Chee Onn, Wong},
title = {Cellular Music: An Interactive Game of Life Sequencer},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3264577},
doi = {10.1145/3240508.3264577},
abstract = {Cellular Music is an algorithmic sequencer based on the rules of Conway's game of life presented as an interactive audio based installation. The installation uses a camera to detect motion in space and uses this data as seeds for the game of life algorithm. The algorithm runs on a 20x20 grid with 20 iterations of the algorithm visualised in individual layers, creating a cubical grid of 20x20x20 cells. Cells are mapped to musical pitches and scanned to generate music.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2081–2083},
numpages = {3},
keywords = {audio based installation, cellular automata, algorithmic sequencer},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3264580,
author = {Soon Xuan, Yong and Chee Onn, Wong and Kong Cheng, Tan and Haron, Anis},
title = {TAGapp Visualization: An Application Based Visual Art Installation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3264580},
doi = {10.1145/3240508.3264580},
abstract = {"TAGapp Visualization" is an interactive visual art installation, which can be set up easily at any TAGapp related venue such as event, galleries and museum. The interaction of TAGapp visualization is mobile application, audiences can download the application into smart device. The features of the application are information retriever, indoor navigation and positioning. Audiences obtain the information by a very natural gesture, TAGapp visualization will display the journey of every individual audience. After download the application, audience have to generate their own account and pick the color, so the visualization will display the sphere according to what the audience pick. Each sphere represent each audience. TAGapp will track the visiting journey of every single audiences and stored into the database and all these actions would determine the visual component of the visual art. Besides that, the uses of this art installation can help the event, galleries and museum management. This real-time participatory interactive visual art enable to connect with the audiences' behavior.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2084–2086},
numpages = {3},
keywords = {interactive artworks, smartphones and mobile applications, mobile application visualization, tagapp, mobile art},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241468,
author = {Sedmidubsky, Jan and Zezula, Pavel},
title = {Similarity-Based Processing of Motion Capture Data},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241468},
doi = {10.1145/3240508.3241468},
abstract = {Motion capture technologies digitize human movements by tracking 3D positions of specific skeleton joints in time. Such spatio-temporal data have an enormous application potential in many fields, ranging from computer animation, through security and sports to medicine, but their computerized processing is a difficult problem. The recorded data can be imprecise, voluminous, and the same movement action can be performed by various subjects in a number of alternatives that can vary in speed, timing or a position in space. This requires employing completely different data-processing paradigms compared to the traditional domains such as attributes, text or images. The objective of this tutorial is to explain fundamental principles and technologies designed for similarity comparison, searching, subsequence matching, classification and action detection in the motion capture data. Specifically, we emphasize the importance of similarity needed to express the degree of accordance between pairs of motion sequences and also discuss the machine-learning approaches able to automatically acquire content-descriptive movement features. We explain how the concept of similarity together with the learned features can be employed for searching similar occurrences of interested actions within a long motion sequence. Assuming a user-provided categorization of example motions, we discuss techniques able to recognize types of specific movement actions and detect such kinds of actions within continuous motion sequences. Selected operations will be demonstrated by on-line web applications.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2087–2089},
numpages = {3},
keywords = {motion capture data, stream-based processing, annotation, action detection, similarity searching, subsequence matching},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241469,
author = {Wei, Yunchao and Liang, Xiaodan and Liu, Si and Lin, Liang},
title = {Structured Deep Learning for Pixel-Level Understanding},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241469},
doi = {10.1145/3240508.3241469},
abstract = {This article summarizes the corresponding half-day tutorial at ACM Multimedia 2018. This tutorial reviews recent progresses for pixel-level understanding with structured deep learning, including 1) human-centric analysis: human parsing and pose estimation; 2) part-based analysis: object part and face parsing; 3) weakly-supervised analysis: object localization and semantic segmentation; 4) depth estimation: stereo matching.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2090–2092},
numpages = {3},
keywords = {human parsing, deep learning, depth estimation, weakly-supervised semantic segmentation, part/face parsing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241470,
author = {Joo, Jungseock and Steinert-Threlkeld, Zachary C. and Luo, Jiebo},
title = {Social and Political Event Analysis Based on Rich Media},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241470},
doi = {10.1145/3240508.3241470},
abstract = {This tutorial aims to provide a comprehensive overview on the applications of rich social media data for real world social and political event analysis, which is a new emerging topic in multimedia research. We will discuss the recent evolution of social media as venues for social and political interaction and their impacts on the real world events using specific examples. We will introduce large scale datasets drawn from social media sources and review concrete research projects that build on computer vision and deep learning based methods. Existing researches in social media have examined various patterns of information diffusion and contagion, user activities and networking, and social media-based predictions of real world events. Most existing works, however, rely on non-content or text based features and do not fully leverage rich multiple modalities -- visuals and acoustics -- which are prevalent in most online social media. Such approaches underutilize vibrant and integrated characteristics of social media especially because the current audiences are getting more attracted to visual information centric media. This proposal highlights the impacts of rich multimodal data to the real world events and elaborates on relevant recent research projects -- the concrete development, data governance, technical details, and their implications to politics and society -- on the following topics. 1) Decoding non-verbal content to identify intent and impact in political messages in mass and social media, such as political advertisements, debates, or news footage; 2) Recognition of emotion, expressions, and viewer perception from communicative gestures, gazes, and facial expressions; 3) Geo-coded Twitter image analysis for protest and social movement analysis; 4) Election outcome prediction and voter understanding by using social media post; and 5) Detection of misinformation, rumors, and fake news and analyzing their impacts in major political events such as the U.S. presidential election.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2093–2095},
numpages = {3},
keywords = {protest and social movements, misinformation, political analysis, social media analysis, fake news, election prediction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241471,
author = {Robinson, Joseph P. and Shao, Ming and Fu, Yun},
title = {To Recognize Families In the Wild: A Machine Vision Tutorial},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241471},
doi = {10.1145/3240508.3241471},
abstract = {Automatic kinship recognition has relevance in an abundance of applications. For starters, aiding forensic investigations, as kinship is a powerful cue that could narrow the search space (e.g., knowledge that the 'Boston Bombers' were brothers could have helped identify the suspects sooner). In short, there are many beneficiaries that could result from such technologies: whether the consumer (e.g., automatic photo library management), scholar (e.g., historic lineage &amp; genealogical studies), data analyzer (e.g., social-media- based analysis), investigator (e.g., cases of missing children and human trafficking. For instance, it is unlikely that a missing child found online would be in any database, however, more than likely a family member would be), or even refugees. Besides application- based problems, and as already hinted, kinship is a powerful cue that could serve as a face attribute capable of greatly reducing the search space in more general face-recognition problems. In this tutorial, we will introduce the background information, progress leading us up to these points, several current state-of-the-art algorithms spanning various views of the kinship recognition problem (e.g., verification, classification, tri-subject). We will then cover our large-scale Families In the Wild (FIW) image collection, several challenge competitions it as been used in, along with the top per- forming deep learning approaches. The tutorial will end with a discussion about future research directions and practical use-cases.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2096–2097},
numpages = {2},
keywords = {family recognition, visual kinship understanding, deep learning, familiar feature, big data},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241472,
author = {Sang, Jitao},
title = {Deep Learning Interpretation},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241472},
doi = {10.1145/3240508.3241472},
abstract = {Deep learning has been successfully exploited in addressing different multimedia problems in recent years. The academic researchers are now transferring their attention from identifying what problem deep learning CAN address to exploring what problem deep learning CAN NOT address. This tutorial starts with a summarization of six 'CAN NOT' problems deep learning fails to solve at the current stage, i.e., low stability, debugging difficulty, poor parameter transparency, poor incrementality, poor reasoning ability, and machine bias. These problems share a common origin from the lack of deep learning interpretation. This tutorial attempts to correspond the six 'NOT' problems to three levels of deep learning interpretation: (1) Locating - accurately and efficiently locating which feature contributes much to the output. (2) Understanding - bidirectional semantic accessing between human knowledge and deep learning algorithm. (3) Expandability - well storing, accumulating and reusing the models learned from deep learning. Existing studies falling into these three levels will be reviewed in detail, and a discussion on the future interesting directions will be provided in the end.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2098–2100},
numpages = {3},
keywords = {deep learning, interpretable machine learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241473,
author = {Schoeffmann, Klaus and Bailer, Werner and Gurrin, Cathal and Awad, George and Loko\v{c}, Jakub},
title = {Interactive Video Search: Where is the User in the Age of Deep Learning?},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241473},
doi = {10.1145/3240508.3241473},
abstract = {In this tutorial we discuss interactive video search tools and methods, review their need in the age of deep learning, and explore video and multimedia search challenges and their role as evaluation benchmarks in the field of multimedia information retrieval. We cover three different campaigns (TRECVID, Video Browser Showdown, and the Lifelog Search Challenge), discuss their goals and rules, and present their achieved findings over the last half-decade. Moreover, we talk about datasets, tasks, evaluation procedures, and examples of interactive video search tools, as well as how they evolved over the years. Participants of this tutorial will be able to gain collective insights from all three challenges and use them for focusing their research efforts on outstanding problems that still remain unsolved in this area.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2101–2103},
numpages = {3},
keywords = {interactive video retrieval, video browsing, evaluation of retrieval results, retrieval models and ranking},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241474,
author = {Yao, Ting and Liu, Jingen},
title = {Human Behavior Understanding: From Action Recognition to Complex Event Detection},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241474},
doi = {10.1145/3240508.3241474},
abstract = {Analyzing human behaviour in videos is one of the fundamental problems of computer vision and multimedia understanding. The task is very challenging as video is an information-intensive media with large variations and complexities in content. With the development of deep learning techniques, researchers have strived to push the limits of human behaviour understanding in a wide variety of applications from action recognition to event detection. This tutorial will present recent advances under the umbrella of human behaviour understanding, which range from the fundamental problem of how to learn "good" video representations, to the challenges of categorizing video content into human action classes, finally to multimedia event detection and surveillance event detection in complex scenarios.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2104–2105},
numpages = {2},
keywords = {surveillance event detection, action recognition, video representation learning, deep learning, video understand, multimedia event detection},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3241475,
author = {Riegler, Michael and Halvorsen, P\r{a}l and M\"{u}nzer, Bernd and Schoeffmann, Klaus},
title = {The Importance of Medical Multimedia},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3241475},
doi = {10.1145/3240508.3241475},
abstract = {Multimedia research is becoming more and more important for the medical domain, where an increasing number of videos and images are integrated in the daily routine of surgical and diagnostic work. While the collection of medical multimedia data is not an issue, appropriate tools for efficient use of this data are missing. This includes management and inspection of the data, visual analytics, as well as learning relevant semantics and using recognition results for optimizing surgical and diagnostic processes. The characteristics and requirements in this interesting but challenging field are different than the ones in classic multimedia domains. Therefore, this tutorial gives a general introduction to the field, provides a broad overview of specific requirements and challenges, discusses existing work and open challenges, and elaborates in detail how machine learning approaches can help in multimedia-related fields to improve the performance of surgeons/clinicians.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2106–2108},
numpages = {3},
keywords = {medical imaging, endoscopy, minimally-invasive surgery, video analysis, computer aided diagnosis},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243718,
author = {Chambel, Teresa and De Simone, Francesca and Kaiser, Rene and Ranasinghe, Nimesha and Van den Broeck, Wendy},
title = {AltMM 2018 - 3rd International Workshop on Multimedia Alternate Realities},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243718},
doi = {10.1145/3240508.3243718},
abstract = {AltMM 2018 is the 3rd edition of the International Workshop on Multimedia Alternate Realities at ACM Multimedia. Our ambition remains to engage researchers and practitioners in discussions on how we can successfully create meaningful multimedia 'alternate realities' experiences. One of the main strengths of this workshop is that we combine different perspectives to explore how the synergy between multimedia technologies can foster and shape the creation of alternate realities and make their access an enriching and valuable experience.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2109–2110},
numpages = {2},
keywords = {alternate realities, interactive multimedia, mulsemedia, qoe, experiences, immersive media, multimedia experience},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243719,
author = {Ringeval, Fabien and Schuller, Bj\"{o}rn and Valstar, Michel and Cowie, Roddy and Pantic, Maja},
title = {Summary for AVEC 2018: Bipolar Disorder and Cross-Cultural Affect Recognition},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243719},
doi = {10.1145/3240508.3243719},
abstract = {The eighth Audio-Visual Emotion Challenge and workshop AVEC 2018 was held in conjunction with ACM Multimedia'18. This year, the AVEC series addressed major novelties with three distinct sub-challenges: bipolar disorder classification, cross-cultural dimensional emotion recognition, and emotional label generation from individual ratings. The Bipolar Disorder Sub-challenge was based on a novel dataset of structured interviews of patients suffering from bipolar disorder (BD corpus), the Cross-cultural Emotion Sub-challenge relied on an extension of the SEWA dataset, which includes human-human interactions recorded 'in-the-wild' for the German and the Hungarian cultures, and the Gold-standard Emotion Sub-challenge was based on the RECOLA dataset, which was previously used in the AVEC series for emotion recognition. In this summary, we mainly describe participation and conditions of the AVEC Challenge.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2111–2112},
numpages = {2},
keywords = {affective computing, bipolar disorder, cross-cultural emotion},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243720,
author = {Sohn, Kwanghoon and Yang, Ming-Hsuan and Byun, Hyeran and Lim, Jongwoo and Hsu, Jison and Lin, Stephen and Kim, Euntai and Kim, Seungryong},
title = {CoVieW'18: The 1st Workshop and Challenge on Comprehensive Video Understanding in the Wild},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243720},
doi = {10.1145/3240508.3243720},
abstract = {The 1st Workshop and Challenge on Comprehensive Video Understanding in the Wild, dubbed CoVieW'18, is held in Seoul, Korea on October 22, 2018, in conjuction with ACM Multimedia 2018. The workshop aims to solve the joint and comprehensive understanding problem in untrimmed videos with a particular emphasis on joint action and scene recognition. The workshop encourages researchers to participate in joint action and scene recognition challenge in untrimmed videos and to report their results. The workshop program includes 1 keynote speech, 2 invited speakers, 6 regular and challenge papers. The developments made in the workshop will deliver a step change in a variety of video applications.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2113–2115},
numpages = {3},
keywords = {scene recognition, comprehensive video understanding, action recognition, multi-task learning},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243722,
author = {Meyer, Jochen and Boll, Susanne and O'Connor, Noel E. and Jain, Ramesh and McDaniel, Troy},
title = {HealthMedia 2018: Third International Workshop on Multimedia for Personal Health and Health Care},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243722},
doi = {10.1145/3240508.3243722},
abstract = {Research in multimedia and health is driven by the current technological advancements in sensors and personalized healthcare. There is an increasing amount of work that shows how core multimedia research is becoming an important enabler for solutions with applications and relevance for the societal questions of health. This workshop brings together researchers from diverse topics such as multimedia, pervasive health, lifelogging, accessibility, HCI, but also health, medicine, and psychology to address challenges and opportunities of multimedia in and for health.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2116–2117},
numpages = {2},
keywords = {personal health, health care, multimedia},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243723,
author = {Liu, Xueliang and Min, Rui and Huet, Benoit and Jia, Jia},
title = {MAHCI 2018: The 1st Workshop on Multimedia for Accessible Human Computer Interface},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243723},
doi = {10.1145/3240508.3243723},
abstract = {In the developing of advanced Human-Computer Interaction, multimedia technology plays a fundamental role to increase usability, and accessibility of computer interfaces. The first workshop on Multimedia for Accessible Human Computer Interface (MAHCI) provides a forum to both multimedia and HCI researchers to discuss the accessible human computer interface design, development, and evaluations with the state-of-the-art multimedia technology. It also enables multimedia community to expand its interaction with the HCI industry and broaden the scope of deploying multimedia technology in practical applications. The workshop features 5 papers which cover a number of novel applications and new methodologies in a half day program.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2118–2119},
numpages = {2},
keywords = {accessibility, multimedia, human-computer interaction},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243724,
author = {Huang, Dong-Yan and Zhao, Sicheng and Schuller, Bj\"{o}rn W. and Yao, Hongxun and Tao, Jianhua and Xu, Min and Xie, Lei and Huang, Qingming and Yang, Jie},
title = {ASMMC-MMAC 2018: The Joint Workshop of 4th the Workshop on Affective Social Multimedia Computing and First Multi-Modal Affective Computing of Large-Scale Multimedia Data Workshop},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243724},
doi = {10.1145/3240508.3243724},
abstract = {Affective social multimedia computing is an emergent research topic for both affective computing and multimedia research communities. Social multimedia is fundamentally changing how we communicate, interact, and collaborate with other people in our daily lives. Social multimedia contains much affective information. Effective extraction of affective information from social multimedia can greatly help social multimedia computing (e.g., processing, index, retrieval, and understanding). Besides, with the rapid development of digital photography and social networks, people get used to sharing their lives and expressing their opinions online. As a result, user-generated social media data, including text, images, audios, and videos, grow rapidly, which urgently demands advanced techniques on the management, retrieval, and understanding of these data.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2120–2121},
numpages = {2},
keywords = {large-scale multimedia data, social multimedia, affective computing},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243725,
author = {Hilton, Adrian and Kang, Hong-Goo and Kim, Hansung and Sohn, Kwanghoon},
title = {AVSU: Workshop on Audio-Visual Scene Understanding for Immersive Multimedia},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243725},
doi = {10.1145/3240508.3243725},
abstract = {This workshop aims to provide a forum to exchange ideas in scene understanding techniques researched in audio and visual communities, and to ultimately unlock the creative potential of joint audio-visual signal processing to deliver a step change in various multimedia applications. Papers and talks presented in this workshop will contribute to the emerging technology for audio and visual information that can improve traditional approaches for multimedia content production and reproduction. The goals of this workshop are to (1) present and discuss the latest trends in audio and computer vision fields for the common research goals, (2) understand state-of-the-art techniques and bottlenecks in the other's discipline for the common topics, (3) investigate research opportunities of joint audio-visual scene understandings in multimedia content production. This workshop will be a good opportunity to bring together leading experts in audio processing and computer vision, and will bridge the gap between two research fields in multimedia content production and reproduction.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2122–2124},
numpages = {3},
keywords = {immersive multimedia, audio-visual signal processing, scene understanding, virtual/augmented reality},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243726,
author = {Lienhart, Rainer and Moeslund, Thomas B. and Saito, Hideo},
title = {1<sup>st</sup> ACM International Workshop on Multimedia Content Analysis in Sports},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243726},
doi = {10.1145/3240508.3243726},
abstract = {The first ACM International Workshop on Multimedia Content Analysis in Sports (ACM MMSports'18) is held in Seoul, South Korea on October 26th, 2018 and is co-located with the ACM International Conference on Multimedia 2018 (ACM Multimedia 2018). The goal of this workshop is to bring together researchers and practitioners from academia and industry to address challenges and report progress in mining and content analysis of multimedia/multimodal data in sports. The combination of sports and modern technology offers a novel and intriguing field of research with promising approaches for visual broadcast augmentation, understanding, statistical analysis and evaluation, and sensor fusion. There is a lack of research communities focusing on the fusion of multiple modalities. We are helping to close this research gap with this first workshop of a serious workshops on multimedia content analysis in sports.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2125–2126},
numpages = {2},
keywords = {multimedia/multimodal content analysis, multimedia/multimodal understanding in sports, visual broadcast augmentation, multimedia/multimodal sensor fusion in sports, multimedia/multimodal statistical analysis and evaluation in sports},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@inproceedings{10.1145/3240508.3243721,
author = {Alameda-Pineda, Xavier and Redi, Miriam and Sebe, Nicu and Chang, Shih-Fu and Luo, Jiebo},
title = {EE-USAD: ACM MM 2018Workshop on UnderstandingSubjective Attributes of Data Focus on Evoked Emotions},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3243721},
doi = {10.1145/3240508.3243721},
abstract = {The series of events devoted to the computational Understanding of Subjective Attributes (e.g. beauty, sentiment) of Data (USAD)provide a complementary perspective to the analysis of tangible properties (objects, scenes), which overwhelmingly covered the spectra of applications in multimedia. Partly fostered by the wide-spread usage of social media, the analysis of subjective attributes has attracted lots of attention in the recent years, and many research teams at the crossroads of multimedia, computer vision and social sciences, devoted time and effort to this topic. Among the subjective attributes there are those assessed by individuals (e.g. safety,interestingness, evoked emotions [2], memorability [3]) as well as aggregated emergent properties (such as popularity or virality [1]).This edition of the workshop (see below for the workshop's history)is devoted to the multimodal recognition of evoked emotions (EE).},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {2127–2128},
numpages = {2},
keywords = {evoked emotions, multimodal recognition, subjective attributes},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

