@inproceedings{10.1145/957013.957017,
author = {Li, Liyuan and Huang, Weimin and Gu, Irene Y. H. and Tian, Qi},
title = {Foreground Object Detection from Videos Containing Complex Background},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957017},
doi = {10.1145/957013.957017},
abstract = {This paper proposes a novel method for detection and segmentation of foreground objects from a video which contains both stationary and moving background objects and undergoes both gradual and sudden "once-off" changes. A Bayes decision rule for classification of background and foreground from selected feature vectors is formulated. Under this rule, different types of background objects will be classified from foreground objects by choosing a proper feature vector. The stationary background object is described by the color feature, and the moving background object is represented by the color co-occurrence feature. Foreground objects are extracted by fusing the classification results from both stationary and moving pixels. Learning strategies for the gradual and sudden "once-off" background changes are proposed to adapt to various changes in background through the video. The convergence of the learning process is proved and a formula to select a proper learning rate is also derived. Experiments have shown promising results in extracting foreground objects from many complex backgrounds including wavering tree branches, flickering screens and water surfaces, moving escalators, opening and closing doors, switching lights and shadows of moving objects.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {2–10},
numpages = {9},
keywords = {video processing, Bayes model, video surveillance, background modeling, foreground segmentation, color co-occurrence},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957018,
author = {Yu, Xinguo and Xu, Changsheng and Leong, Hon Wai and Tian, Qi and Tang, Qing and Wan, Kong Wah},
title = {Trajectory-Based Ball Detection and Tracking with Applications to Semantic Analysis of Broadcast Soccer Video},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957018},
doi = {10.1145/957013.957018},
abstract = {This paper first presents an improved trajectory-based algorithm for automatically detecting and tracking the ball in broadcast soccer video. Unlike the object-based algorithms, our algorithm does not evaluate whether a sole object is a ball. Instead, it evaluates whether a candidate trajectory, which is generated from the candidate feature image by a candidate verification procedure based on Kalman filter,, which is generated from the candidate feature image by a candidate verification procedure based on Kalman filter, is a ball trajectory. Secondly, a new approach for automatically analyzing broadcast soccer video is proposed, which is based on the ball trajectory. The algorithms in this approach not only improve play-break analysis and high-level semantic event detection, but also detect the basic actions and analyze team ball possession, which may not be analyzed based only on the low-level feature. Moreover, experimental results show that our ball detection and tracking algorithm can achieve above 96% accuracy for the video segments with the soccer field. Compared with the existing methods, a higher accuracy is achieved on goal detection and play-break segmentation. To the best of our knowledge, we present the first solution in detecting the basic actions such as touching and passing, and analyzing the team ball possession in broadcast soccer video.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {11–20},
numpages = {10},
keywords = {ball detection and tracking, semantic analysis, trajectory-based, event detection},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957019,
author = {Wold Eide, Viktor S. and Eliassen, Frank and Granmo, Ole-Christoffer and Lysne, Olav},
title = {Supporting Timeliness and Accuracy in Distributed Real-Time Content-Based Video Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957019},
doi = {10.1145/957013.957019},
abstract = {Real-time content-based access to live video data requires content analysis applications that are able to process the video data at least as fast as the video data is made available to the application and with an acceptable error rate. Statements as this express quality of service (QoS) requirements to the application. In order to provide some level of control of the QoS provided, the video content analysis application must be scalable and resource aware so that requirements of timeliness and accuracy can be met by allocating additional processing resources.In this paper we present a general architecture of video content analysis applications including a model for specifying requirements of timeliness and accuracy. The salient features of the architecture include its combination of probabilistic knowledge-based media content analysis with QoS and distributed resource management to handle QoS requirements, and its independent scalability at multiple logical levels of distribution. We also present experimental results with an algorithm for QoS-aware selection of configurations of feature extractor and classification algorithms that can be used to balance requirements of timeliness and accuracy against available processing resources. Experiments with an implementation of a real-time motion vector based object-tracking application, demonstrate the scalability of the architecture.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {21–32},
numpages = {12},
keywords = {task graph scheduling, QoS and resource management, real-time video content analysis, event-based communication, parallel processing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957020,
author = {Duan, Ling-Yu and Xu, Min and Chua, Tat-Seng and Tian, Qi and Xu, Chang-Sheng},
title = {A Mid-Level Representation Framework for Semantic Sports Video Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957020},
doi = {10.1145/957013.957020},
abstract = {Sports video has been widely studied due to its tremendous commercial potentials. Despite encouraging results from various specific sports games, it is almost impossible to extend a system for a new sports game because they usually employ different sets of low-level features appropriate for the specific games and closely coupled with the use of game specific rules to detect events or highlights. There is a lack of internal representation and structure to be generic and applicable for many different sports. In this paper, we present a generic mid-level representation framework for semantic sports video analysis. The mid-level representation layer is introduced between the low-level audio-visual processing and high-level semantic analysis. It allows us to separate sports specific knowledge and rules from the low-level and mid-level feature extraction. This makes sports video analysis more efficient, effective, and less ad-hoc for various types of sports. To achieve robustness of the low-level feature analysis, a non-parametric clustering, mean shift procedure, has been successfully applied to both color and motion analysis. The proposed framework has been tested for five field-ball type sports covering duration of about 8 hours. Experiments have shown its robust performance in semantic analysis and event detection. We believe that the proposed mid-level representation framework can be used for event detection, highlight extraction, summarization and personalization of many types of sports video.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {33–44},
numpages = {12},
keywords = {sports video, semantics, mid-level representation, events},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957022,
author = {Hefeeda, Mohamed and Habib, Ahsan and Botev, Boyan and Xu, Dongyan and Bhargava, Bharat},
title = {PROMISE: Peer-to-Peer Media Streaming Using CollectCast},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957022},
doi = {10.1145/957013.957022},
abstract = {We present the design, implementation, and evaluation of PROMISE, a novel peer-to-peer media streaming system encompassing the key functions of peer lookup, peer-based aggregated streaming, and dynamic adaptations to network and peer conditions. Particularly, PROMISE is based on a new application level P2P service called CollectCast. CollectCast performs three main functions: (1) inferring and leveraging the underlying network topology and performance information for the selection of senders; (2) monitoring the status of peers and connections and reacting to peer/connection failure or degradation with low overhead; (3) dynamically switching active senders and standby senders, so that the collective network performance out of the active senders remains satisfactory. Based on both real-world measurement and simulation, we evaluate the performance of PROMISE, and discuss lessons learned from our experience with respect to the practicality and further optimization of PROMISE.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {45–54},
numpages = {10},
keywords = {peer-to-peer systems, multimedia streaming},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957023,
author = {de Cuetos, Philippe and Ross, Keith W.},
title = {Optimal Streaming of Layered Video: Joint Scheduling and Error Concealment},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957023},
doi = {10.1145/957013.957023},
abstract = {We consider streaming layered video (live and stored) over a lossy packet network in order to maximize the video quality that is rendered at the receiver. We propose a framework, called joint scheduling and error concealment (Joint S+EC), in which packet scheduling decisions at the sender explicitly account for the error concealment mechanism at the receiver. We show how the theory of infinite--horizon, average--reward Markov decision processes (MDPs) with average--cost constraints can be applied to the joint scheduling and error concealment problem for low--delay transmission channels. The formulation allows for a wide variety of performance metrics, including metrics that take quality variation into account. We demonstrate the framework and MDP solution procedure using MPEG--4 FGS video traces. The main conclusions are (1) Joint S+EC optimal policies perform better than Disjoint S+EC optimal policies, and (2) the performance of the optimal Disjoint S+EC policy is not significantly better than very simple static policies.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {55–64},
numpages = {10},
keywords = {error concealment, layered--video, video streaming},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957024,
author = {Lund, Ketil and Goebel, Vera},
title = {Adaptive Disk Scheduling in a Multimedia DBMS},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957024},
doi = {10.1145/957013.957024},
abstract = {In this paper, we present APEX, a disk scheduling framework with QoS support, designed for environments with highly varying disk bandwidth usage. In particular, we focus on a Learning-on-Demand scenario supported by a multimedia database management system, where students can search for, and play back multimedia-based learning material. APEX is based on a two-level scheduling architecture, where the upper level realizes different service classes using a set of queues, while the lower level distributes available disk bandwidth among these queues.In this paper, we focus on the low-level scheduling in APEX, which is based on an extended token bucket algorithm. The disk requests scheduled for service are assembled into batches, which render possible good efficiency for the disk. Combined with a very efficient work-conservation scheme, this enables APEX to apply bandwidth where it is needed, without efficiency loss. We demonstrate, through simulations, that APEX provides both higher throughput and lower response times than other mixed-media disk schedulers, while still avoiding deadline violations for real-time requests. We also show its robustness with respect to misaligned bandwidth allocation.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {65–74},
numpages = {10},
keywords = {disk scheduling, QoS, MMDBMS},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957025,
author = {Zimmermann, Roger and Fu, Kun},
title = {Comprehensive Statistical Admission Control for Streaming Media Servers},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957025},
doi = {10.1145/957013.957025},
abstract = {Streaming media servers and digital continuous media recorders require the scheduling of I/O requests to disk drives in real time. There are two accepted paradigms to achieve this: deterministic or statistical. The deterministic approach must assume larger bounds on such disk parameters as the seek time, the rotational latency and the transfer rate, to guarantee the timely service of I/O requests. The statistical approach generally allows higher utilization of resources, in exchange for a residual probability of missed I/O request deadlines. We propose a novel statistical admission control algorithm called TRAC based on a comprehensive three random variable (3RV) model to support both reading and writing of multiple variable bit rate media streams on current generation disk drives. Its major distinctions from previous work include (1) a very realistic disk model which considers multi-zoning of disks, seek and rotational latency profiles, and unequal reading and writing data rate limits, (2) a dynamic bandwidth sharing mechanism between reading and writing, and (3) support for random placement of data blocks. We evaluate the TRAC algorithm through an extensive numerical analysis and real device measurements. The results show that it achieves a much more realistic resource utilization (up to 38% higher) as compared with the best, previously proposed algorithm based on a single random variable (1RV) model. Most impressive, in all the experiments the difference between the results generated by TRAC and the actual disk device measurements match closely.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {75–85},
numpages = {11},
keywords = {statistical modeling, admission control, disk performance, streaming media},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957027,
author = {Amini, Lisa and Rose, Raymond and Venkatramani, Chitra and Verscheure, Olivier and Westerink, Peter and Frossard, Pascal},
title = {ARMS: Adaptive Rich Media Secure Streaming},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957027},
doi = {10.1145/957013.957027},
abstract = {In this demonstration we present the ARMS system which enables secure and adaptive rich media streaming to a large-scale, heterogeneous client population. The ARMS system dynamically adapts streams to available bandwidth, client capabilities, packet loss, and administratively imposed policies - all while maintaining full content security. The ARMS system is completely standards compliant and to our knowledge is the first such end-to-end MPEG-4-based system.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {86–87},
numpages = {2},
keywords = {streaming, encrypted, video server, MPEG-4, scalability, adaptive},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957028,
author = {Davis, Marc and Heer, Jeffrey and Ramirez, Ana},
title = {Active Capture: Automatic Direction for Automatic Movies},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957028},
doi = {10.1145/957013.957028},
abstract = {The Active Capture demonstration is part of a new computational media production paradigm that transforms media production from a manual mechanical process into an automated computational one that can produce mass customized and personalized media integrating video of non-actors. Active Capture leverages media production knowledge, computer vision and audition, and user interaction design to automate direction and cinematography and thus enables the automatic production of annotated, high quality, reusable media assets. The implemented system automates the process of capturing a non-actor performing two simple reusable actions ("screaming" and "turning her head to look at the camera") and automatically integrates those shots into various commercials and movie trailers.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {88–89},
numpages = {2},
keywords = {active capture, automated direction, human-in-the-loop, recognition, metadata, video capture},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957029,
author = {Feng, Wu-chi and Code, Brian and Kaiser, Ed and Shea, Mike and Feng, Wu-chang},
title = {Panoptes: Scalable Low-Power Video Sensor Networking Technologies},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957029},
doi = {10.1145/957013.957029},
abstract = {This demonstration will show the video sensor networking technologies developed at the OGI School of Science and Engineering. The general purpose video sensors allow programmers to create application-specific filtering, power management, and event triggering mechanisms. The demo will show a handful of video sensors operating under a variety of conditions including intermittent network connectivity as one might see in an environmental observation application.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {90–91},
numpages = {2},
keywords = {sensors, MPEG, video streaming, video sensors, JPEG},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957030,
author = {Girgensohn, Andreas and Shipman, Frank and Wilcox, Lynn},
title = {Hyper-Hitchcock: Authoring Interactive Videos and Generating Interactive Summaries},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957030},
doi = {10.1145/957013.957030},
abstract = {To simplify the process of editing interactive video, we developed the concept of "detail-on-demand" video as a subset of general hypervideo. Detail-on-demand video keeps the authoring and viewing interfaces relatively simple while supporting a wide range of interactive video applications. Our editor, Hyper-Hitchcock, provides a direct manipulation environment in which authors can combine video clips and place hyperlinks between them. To summarize a video, Hyper-Hitchcock can also automatically generate a hypervideo composed of multiple video summary levels and navigational links between these summaries and the original video. Viewers may interactively select the amount of detail they see, access more detailed summaries, and navigate to the source video through the summary.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {92–93},
numpages = {2},
keywords = {hypervideo, video summarization, video editing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957031,
author = {Graham, Jamey and Erol, Berna and Hull, Jonathan J. and Lee, Dar-Shyang},
title = {The Video Paper Multimedia Playback System},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957031},
doi = {10.1145/957013.957031},
abstract = {Video Paper is a prototype system for multimedia browsing, analysis, and replay. Key frames extracted from a video recording are printed on paper together with bar codes that allow for random access and replay. A transcript for the audio track can also be shown so that users can read what was said, thus making the document a stand-alone representation for the contents of the multimedia recording. The Video Paper system has been used for several applications, including the analysis of recorded meetings, broadcast news, oral histories and personal recordings. This demonstration will show how the Video Paper system was applied to these domains and the various replay systems that were developed, including a self-contained portable implementation on a PDA and a fixed implementation on desktop PC.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {94–95},
numpages = {2},
keywords = {retrieval, access, replay, paper-based multimedia browsing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957032,
author = {Lam, Kam-Yiu and Chiu, Calvin K. H.},
title = {Mobile Video Stream Monitoring System},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957032},
doi = {10.1145/957013.957032},
abstract = {IMVS (Intelligent Mobile Video Stream Monitoring System) is a mobile video surveillance system. The objective of IMVS is to design a high performance video stream monitoring system in a mobile computing environment. In particular, the technical questions to be addressed are: (1) how to minimize the amount of video signals to be transmitted between the front-end mobile device and the backend server over the mobile network; and (2) how to divide the jobs to be performed between the front-end and backend processes so that the workload at the front-end mobile device can be maintained within its processing capacity.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {96–97},
numpages = {2},
keywords = {video surveillance, wireless computing, scheduling and rule evaluation},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957033,
author = {Lin, Ching-Yung and Tseng, Belle L. and Naphade, Milind and Natsev, Apostol and Smith, John R.},
title = {MPEG-7 Video Automatic Labeling System},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957033},
doi = {10.1145/957013.957033},
abstract = {In this demo, we show a novel end-to-end video automatic labeling system, which accepts MPEG-1 sequence inputs and generates MPEG-7 XML metadata files. Detections are based on the prior established anchor models. This system has two parts: model training process and labeling process. They are comprised of seven modules: Shot Segmentation, Region Segmentation, Annotation, Feature Extraction, Model Learning, Classification, and XML Rendering.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {98–99},
numpages = {2},
keywords = {video indexing, concept learning, concept labeling, MPEG-7, annotation, feature extraction},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957034,
author = {Ou, Jiazhi and Chen, Xilin and Fussell, Susan R. and Yang, Jie},
title = {DOVE: Drawing over Video Environment},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957034},
doi = {10.1145/957013.957034},
abstract = {We demonstrate a multimedia system that integrates pen-based gesture and live video to support collaboration on physical tasks. The system combines network IP cameras, desktop PCs, and tablet PCs (or PDAs) to allow a remote helper to draw on a video feed of a workspace as he/she provides task instructions. A gesture recognition component enables the system both to normalize freehand drawings to facilitate communication with remote partners and to use pen-based input as a camera control device. The system also embeds some tools, such as controlled video delay, gesture delay, and remote camera pan-tilt-zoom control. The system provides a software environment for studying multimodal/multimedia communication for remote collaborative physical tasks.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {100–101},
numpages = {2},
keywords = {gestural communication, multimodal interaction, computer-supported cooperative work, video mediated communication, gesture recognition, video conferencing, video stream},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957035,
author = {Shih, Timothy K. and Lu, Liang-Chen and Chang, Rong-Chi},
title = {An Automatic Image Inpaint Tool},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957035},
doi = {10.1145/957013.957035},
abstract = {Automatic digital inpainting is a challenge but interesting research area. This demonstration presents a tool, which uses a color interpolation mechanism to restore damaged images. The mechanism checks the variation of pixel blocks and restores pixels using different strategies. We test more than 1000 images, including photos, paintings, and cartoon drawings. The proposed system is also available at: http://www.mine.tku.edu.tw/demos/inpaint.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {102–103},
numpages = {2},
keywords = {image restoration, image processing, multi-resolution inpainting, digital inpainting},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957036,
author = {Song, Dezhen and Goldberg, Ken},
title = {The Co-Opticon: Shared Access to a Robotic Streaming Video Camera},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957036},
doi = {10.1145/957013.957036},
abstract = {The "co-opticon" is a robotic pan, tilt, and zoom streaming video camera controlled by simultaneous frame requests from remote users. Robotic webcameras are commercially available but currently restrict control to only one user at a time. The co-opticon introduces a new interface that allows simultaneous control by many users. We will demonstrate the implemented system using a Java-based interface at the conference linked via the Internet to a camera on the UC Berkeley campus. We will also discuss system architecture and several new algorithms we've developed to compute optimal camera paramters based on user frame requests. The co-opticon can be tested online at: www.tele-actor.net/co-opticon.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {104–105},
numpages = {2},
keywords = {teleoperation, collaborative control, videoconferencing, webcam, internet robotics},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957037,
author = {Wang, Ming-Yu and Xie, Xing and Ma, Wei-Ying and Zhang, Hong-Jiang},
title = {MobiPicture: Browsing Pictures on Mobile Devices},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957037},
doi = {10.1145/957013.957037},
abstract = {Pictures have become increasingly common and popular in mobile communication. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of pictures on the small screen. MobiPicture is a prototype system which includes a set of novel features to aid or automate a set of common image browsing tasks such as the thumbnail view, set-as-background, zooming and scrolling.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {106–107},
numpages = {2},
keywords = {adaptive content delivery, image adaptation, form factor, attention model},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957038,
author = {Zheng, Jiang Yu and Shi, Min and Kato, Makoto},
title = {Route Panoramas for City Navigation},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957038},
doi = {10.1145/957013.957038},
abstract = {This paper presents a new medium called route panorama (RP) for visualizing a large-scale environment such as a town or a city. An RP is captured by a slit camera mounted on a vehicle. It is a continuous, compact and complete visual representation of scenes along a route. It can be transmitted on the Internet in real time as a streaming media and displayed in various styles for virtual city traversing. The application of RPs includes virtual tour, navigation, heritage archiving, urban planning, city indexing, etc.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {108–109},
numpages = {2},
keywords = {route panorama, internet media, cityscape visualization, environment navigation, virtual tour},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957040,
author = {Hoashi, Keiichiro and Matsumoto, Kazunori and Inoue, Naomi},
title = {Personalization of User Profiles for Content-Based Music Retrieval Based on Relevance Feedback},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957040},
doi = {10.1145/957013.957040},
abstract = {Numerous efforts on content-based music information retrieval have been presented in recent years. However, the object of such existing research is to retrieve a specific song from a large music database. In this research, we propose a music retrieval method which retrieves songs based on the user's musical preferences. This enables users to discover new songs which they are expected to like. Since music preferences are expected to be highly ambiguous, we propose the implementation of relevance feedback methods to improve the performance of our music information retrieval method. In order to reduce the burden of users to input learning data to the system, we also propose a method to generate user profiles based on genre preferences, and refinement of such profiles based on relevance feedback. Evaluation experiments are conducted based on a corpus of music data with user ratings. Results of these experiments prove the effectiveness of our method.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {110–119},
numpages = {10},
keywords = {relevance feedback, tree-based vector quantization, music information retrieval, user preferences},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957041,
author = {Lavrenko, Victor and Pickens, Jeremy},
title = {Polyphonic Music Modeling with Random Fields},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957041},
doi = {10.1145/957013.957041},
abstract = {Recent interest in the area of music information retrieval and related technologies is exploding. However, very few of the existing techniques take advantage of recent developments in statistical modeling. In this paper we discuss an application of Random Fields to the problem of creating accurate yet flexible statistical models of polyphonic music. With such models in hand, the challenges of developing effective searching, browsing and organization techniques for the growing bodies of music collections may be successfully met. We offer an evaluation of these models in terms of perplexity and prediction accuracy, and show that random fields not only outperform Markov chains, but are much more robust in terms of overfitting.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {120–129},
numpages = {10},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957042,
author = {Kline, Richard L. and Glinert, Ephraim P.},
title = {Approximate Matching Algorithms for Music Information Retrieval Using Vocal Input},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957042},
doi = {10.1145/957013.957042},
abstract = {Effective use of multimedia collections requires efficient and intuitive methods of searching and browsing. This work considers databases which store music and explores how these may best be searched by providing input queries in some musical form. For the average person, humming several notes of the desired melody is the most straightforward method for providing this input, but such input is very likely to contain several errors. Previously proposed implementations of so-called query-by-humming systems are effective only when the number of input errors is small. We conducted experiments which revealed that the expected error rate for user queries is much higher than existing algorithms can tolerate. We then developed algorithms based on approximate matching techniques which deliver much improved results when comparing error-filled vocal user queries against a music collection.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {130–139},
numpages = {10},
keywords = {music information retrieval, query by humming},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957043,
author = {Lu, Lie and Zhang, Hong-Jiang},
title = {Automated Extraction of Music Snippets},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957043},
doi = {10.1145/957013.957043},
abstract = {Similar to image and video thumbnail, music snippet is defined as the most representative or highlight excerpt of a music clip, and can be used efficiently for fast browsing large number of music files. Music snippet is usually a part of the repeated melody, main theme or chorus. In this paper, we present an approach to extracting music snippet automatically. In our approach, the most salient segment of the music is firstly detected based on its occurrence frequency and energy information. Meanwhile, the boundaries of musical phrases are also detected based on the estimated phrase length and phrase boundary confidence of each frame. These boundaries are used to ensure that an extracted snippet does not break musical phrases. Finally, the musical phrases including the most salient segment are extracted as music snippet. User study indicates that the proposed algorithm works very well on our music database.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {140–147},
numpages = {8},
keywords = {tempo estimation, music thumbnail, musical phrase, music saliency, music snippet, music structure},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957045,
author = {Liu, Hao and Xie, Xing and Ma, Wei-Ying and Zhang, Hong-Jiang},
title = {Automatic Browsing of Large Pictures on Mobile Devices},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957045},
doi = {10.1145/957013.957045},
abstract = {Pictures have become increasingly common and popular in mobile communications. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of large pictures on the small screen. In this paper, we propose a novel approach which is able to automate the scrolling and navigation of a large picture with a minimal amount of user interaction on mobile devices. An image attention model is employed to illustrate the information structure within an image. An optimal image browsing path is then calculated based on the image attention model to simulate the human browsing behaviors. Experimental evaluations of the proposed mechanism indicate that our approach is an effective way for viewing large images on small displays.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {148–155},
numpages = {8},
keywords = {browsing path, attention model, information foraging, form factor, image adaptation, adaptive content delivery},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957046,
author = {Toyama, Kentaro and Logan, Ron and Roseway, Asta},
title = {Geographic Location Tags on Digital Images},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957046},
doi = {10.1145/957013.957046},
abstract = {We describe an end-to-end system that capitalizes on geographic location tags for digital photographs. The World Wide Media eXchange (WWMX) database indexes large collections of image media by several pieces of metadata including timestamp, owner, and critically, location stamp. The location where a photo was shot is important because it says much about its semantic content, while being relatively easy to acquire, index, and search.The process of building, browsing, and writing applications for such a database raises issues that have heretofore been un- addressed in either the multimedia or the GIS community. This paper brings all of these issues together, explores different options, and offers novel solutions where necessary. Topics include acquisition of location tags for image media, data structures for location tags on photos, database optimization for location-tagged image media, and an intuitive UI for browsing a massive location-tagged image database. We end by describing an application built on top of the WWMX, a lightweight travelogue-authoring tool that automatically creates appropriate context maps for a slideshow of location-tagged photographs.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {156–166},
numpages = {11},
keywords = {digital photography, GIS, image databases, geographic interfaces},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957047,
author = {Yanai, Keiji},
title = {Generic Image Classification Using Visual Knowledge on the Web},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957047},
doi = {10.1145/957013.957047},
abstract = {In this paper, we describe a generic image classification system with an automatic knowledge acquisition mechanism from the World-Wide Web. Due to the recent spread of digital imaging devices, the demand for image recognition of various kinds of real world scenes becomes greater. For realizing it, visual knowledge on various kinds of scenes is required. Then, we propose gathering visual knowledge on real world scenes for generic image classification from the World-Wide Web. Our system gathers a large number of images from the Web automatically and makes use of them as training images for generic image classification. It consists of three modules, which are an image-gathering module, an image-learning module and an image classification module. The image-gathering module gathers images related to given class keywords from the Web automatically. The learning module extracts image features from gathered images and associates them with each class. The image classification module classifies an unknown image into one of the classes corresponding to the class keywords by using the association between image features and classes. In the experiments, we achieved a classification rate 44.6% for generic images by using images gathered from the World-Wide Web automatically as training images.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {167–176},
numpages = {10},
keywords = {image classification, web image mining, image gathering},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957049,
author = {Bennett, Eric P. and McMillan, Leonard},
title = {Proscenium: A Framework for Spatio-Temporal Video Editing},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957049},
doi = {10.1145/957013.957049},
abstract = {We present an approach to video editing where movie sequences are treated as spatio-temporal volumes that can be sheered and warped under user control. This simple capability enables new video editing operations that support complex postproduction modifications, such as object removal and/or changes in camera motion. Our methods do not rely on complicated and error-prone image analysis or computer vision methods. Moreover, they facilitate an editing approach to video that is similar to standard image-editing tasks. Central to our system is a movie representation framework called Proscenium that supports efficient queries and operations on spatio-temporal volumes while maintaining the original source content. We have adopted a graph-based lazy-evaluation model in order to support interactive visualizations, complex data modifications, and efficient processing of large spatio-temporal volumes.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {177–184},
numpages = {8},
keywords = {video layers, video editing, feature selection, video stabilization, feature removal, special effects, multimedia framework},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957050,
author = {Kum, Sang-Uok and Mayer-Patel, Ketan and Fuchs, Henry},
title = {Real-Time Compression for Dynamic 3D Environments},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957050},
doi = {10.1145/957013.957050},
abstract = {The goal of tele-immersion has long been to enable people at remote locations to share a sense of presence. A tele-immersion system acquires the 3D representation of a collaborator's environment remotely and sends it over the network where it is rendered in the user's environment. Acquisition, reconstruction, transmission, and rendering all have to be done in real-time to create a sense of presence. With added commodity hardware resources, parallelism can increase the acquisition volume and reconstruction data quality while maintaining real-time performance. However this is not as easy for rendering since all of the data need to be combined into a single display.In this paper we present an algorithm to compress data from such 3D environments in real-time to solve this imbalance. We expect the compression algorithm to scale comparably to the acquisition and reconstruction, reduce network transmission bandwidth, and reduce the rendering requirement for real-time performance. We have tested the algorithm using a synthetic office data set and have achieved a 5 to 1 compression for 22 depth streams.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {185–194},
numpages = {10},
keywords = {real-time compression, virtual reality, tele-immersion, K-Means algorithm, K-Means initialization},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957051,
author = {Li, Beitao and Goh, Kingshy},
title = {Confidence-Based Dynamic Ensemble for Image Annotation and Semantics Discovery},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957051},
doi = {10.1145/957013.957051},
abstract = {Providing accurate and scalable solutions to map low-level perceptual features to high-level semantics is critical for multimedia information organization and retrieval. In this paper, we propose a confidence-based dynamic ensemble (CDE) to overcome the shortcomings of the traditional static classifiers. In contrast to the traditional models, CDE can make dynamic adjustments to accommodate new semantics, to assist the discovery of useful low-level features, and to improve class-prediction accuracy. We depict two key components of CDE: a multi-level function that asserts class-prediction confidence, and the dynamic ensemble method based upon the confidence function. Through theoretical analysis and empirical study, we demonstrate that CDE is effective in annotating large-scale, real-world image datasets.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {195–206},
numpages = {12},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957053,
author = {Adams, Brett and Venkatesh, Svetha},
title = {Weaving Stories in Digital Media: When Spielberg Makes Home Movies},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957053},
doi = {10.1145/957013.957053},
abstract = {In this paper we describe research aimed at enabling amateur video makers to improve both the technical quality and communicative capacity of their work. Motivated by the recognition that untold hours of home video are simply abandoned after capture, we have formulated the problem as one of defining the what and how of footage capture.We have implemented a framework that answers the first problem, the what, by means of the age-old communicative powers of Story; the second problem, the how, is addressed by means of well documented aesthetic principles that constitute the film profession, which impact both technical and cinematic considerations for a given project.We provide a brief overview of the process, beginning with the narrative template, embodying a chosen story, through the principal phases of generating a storyboard, directing, and editing, resulting in the finished product. We demonstrate the interplay of narrative, purpose for the production, and aesthetic agents, and their influence on the automatically generated storyboard with examples.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {207–210},
numpages = {4},
keywords = {cinematography, narrative structure, video editing, video analysis, media aesthetics, home movies},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957054,
author = {Biatov, Konstantin and Koehler, Joachim},
title = {An Audio Stream Classification and Optimal Segmentation for Multimedia Applications},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957054},
doi = {10.1145/957013.957054},
abstract = {In this paper we investigate on-line zero-crossing based audio stream segmentation and classification into speech and other segments. We consider such segments as applause, noise of the auditorium, and silence. We demonstrate that the features extracted from zero-crossing are stable and valid to be used for speech and other signal discrimination and classification and don't require large amount of data for the training. We describe the optimal segmentation of unlimited audio signals using results of the frames classification. We demonstrate that using optimal segmentation is better than using traditional sliding window technique.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {211–214},
numpages = {4},
keywords = {optimal segmentation, classification, audio stream analysis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957055,
author = {Boston, Jeff and Kim, Michelle and Luken, William and So, Edward and Wood, Steve},
title = {Interleaving Media Data for MPEG-4 Presentations},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957055},
doi = {10.1145/957013.957055},
abstract = {A composite multimedia presentation may be represented by a sequence of virtual media data packets. An algorithm is presented for ordering these virtual media data packets so as to minimize the initial delay required to transfer the composite stream from a media server to a client. This algorithm has been implemented as part of the IBM Toolkit for MPEG-4.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {215–218},
numpages = {4},
keywords = {streaming media, multimedia, composite multimedia, MPEG-4},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957056,
author = {Cai, Rui and Lu, Lie and Zhang, Hong-Jiang},
title = {Using Structure Patterns of Temporal and Spectral Feature in Audio Similarity Measure},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957056},
doi = {10.1145/957013.957056},
abstract = {Although statistical characteristics of audio features are widely used for similarity measure in most of current audio analysis systems and have been proved to be effective, they only utilized the averaged feature variations over time, and thus lead to inaccuracy in some cases. In this paper, structure pattern, which describes the representative structure characteristics of both temporal and spectral features, is proposed to improve the similarity measure for audio effects. Three kind structure patterns are proposed and utilized in current work, including energy contour pattern, harmonicity pattern and pitch contour pattern. Evaluations on a content-based audio retrieval system indicate that structure patterns can improve the performance pretty much.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {219–222},
numpages = {4},
keywords = {audio similarity measure, structure pattern, audio retrieval},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957057,
author = {Chai, Wei and Vercoe, Barry},
title = {Music Thumbnailing via Structural Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957057},
doi = {10.1145/957013.957057},
abstract = {Music thumbnailing (or music summarization) aims at finding the most representative part of a song, which can be used for web browsing, web searching and music recommendation. Three strategies are proposed in this paper for automatically generating the thumbnails of music. All the strategies are based on the results of music structural analysis, which identifies the recurrent structure of musical signals. Instead of being evaluated subjectively, the generated thumbnails are evaluated by several criteria, mainly based on previous human experiments on music thumbnailing and the properties of thumbnails used for commercial web sites. Additionally, the performance of the structural analysis is demonstrated visually using figures for qualitative evaluation, and by three novel structural similarity metrics for quantitative evaluation. The preliminary results obtained using a corpus of Beatles' songs demonstrate the promise of our method and suggest that different thumbnailing strategies might be proper for different applications.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {223–226},
numpages = {4},
keywords = {music information retrieval, music thumbnailing, pattern matching, music segmentation, structural analysis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957058,
author = {Chambers, Chris and Feng, Wu-chi and Feng, Wu-chang and Saha, Debanjan},
title = {A Geographic Redirection Service for On-Line Games},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957058},
doi = {10.1145/957013.957058},
abstract = {For many on-line games, user experience is impacted significantly by network latency. As on-line games and on-line game servers proliferate, the ability to discover and connect to nearby servers is essential for maintaining user satisfaction. In this paper, we present a redirection service for on-line games based on the geographic location of players relative to servers. As our results show, the service better meets client demand, saving each client and the Internet as a whole, thousands of miles of networking inefficiency.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {227–230},
numpages = {4},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957059,
author = {Cheong, Won-Sik and Kim, Kyuheon and Park, Gwang Hoon},
title = {A New Scanning Method for H.264 Based Fine Granular Scalable Video Coding},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957059},
doi = {10.1145/957013.957059},
abstract = {In this paper, we introduce a new scanning method for H.264 based Fine Granular Scalable video coding, which can significantly improve the subjective picture quality of a decoded scalable video. Since the network condition is fluctuated, it is often happened that the important part of the streaming data, especially video sequences, cannot be transmitted, and thus a viewer watches less interesting parts of the sequences or poorer quality of pictures in important regions. Therefore, this paper presents a new scanning method, called water ring scan method, for improving the subjective picture quality of the decoded scalable video by encoding and transmitting the visually important region most-preferentially as a watering is expanded in the lake from the location where gravel falls. The water ring scan method presented in this paper is to encode and decode video sequences at the location designated by a user. From the simulation results, it has been found that the proposed scan method can achieve significantly improved picture quality, especially on the region of interest as being compared with the traditional FGS scheme.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {231–234},
numpages = {4},
keywords = {video coding, FGS, MPEG-4, water ring, AVC, H.264},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957060,
author = {Cherkasova, Ludmila and Tang, Wenting},
title = {Capacity Planning Tool for Streaming Media Services},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957060},
doi = {10.1145/957013.957060},
abstract = {The goal of the proposed capacity planning tool is to provide the best cost/performance configuration for support of a known media service workload. There are two essential components in our capacity planning tool: i) the capacity measurements of different h/w and s/w solutions using a specially designed set of media benchmarks and ii) a media service workload profiler, called MediaProf, which extracts a set of quantitative and qualitative parameters characterizing the service demand. The capacity planning tool matches the requirements of the media service workload profile, SLAs and configuration constraints to produce the best available cost/performance solution.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {235–238},
numpages = {4},
keywords = {capacity planning, media server capacity, measurements, SLAs, workload profiling, media server benchmarks},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957061,
author = {Ding, Dawei and Li, Qing and Feng, Bo and Wenyin, Liu},
title = {A Semantic Model for Flash Retrieval Using Co-Occurrence Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957061},
doi = {10.1145/957013.957061},
abstract = {Flash is experiencing a breathtaking growth and has become one of the prevailing media formats on the Web. Our goal is to exploit the enormous Flash resources by developing a model of content-based Flash retrieval. Towards this end, we introduce a novel approach for discovering semantic relationships among the co-occurrence patterns of elements in Flash movies. The proposed approach includes a three-layered structure to index the Flash movie, a query expansion procedure to improve the recall performance, and a relevance ranking procedure utilizing link analysis to improve the precision performance of Flash retrieval. Experiments show the potential of leveraging co-occurrence analysis of elements in the context of scenes for improving the performance of Flash retrieval.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {239–242},
numpages = {4},
keywords = {query expansion, flash retrieval, relevance ranking, co-occurrence analysis, composition matrix},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957062,
author = {Duan, Ling-Yu and Xu, Min and Tian, Qi and Xu, Chang-sheng},
title = {Nonparametric Color Characterization Using Mean Shift},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957062},
doi = {10.1145/957013.957062},
abstract = {Color is very useful in locating and recognizing objects that occur in artificial environments. The color histogram has shown its efficiency and advantages as a general tool for various applications, such as content-based image retrieval and video browsing, object indexing and location, and video segmentation. However, due to the lack of any spatial and context information, the histogram is not robust and effective for color characterization (e.g. dominant color) in large video databases. In this paper, we propose a nonparametric color characterization model using mean shift procedure, with an emphasis on spatio-temporal consistency. Experimental results suggest that the color characterization model is much more effective for video indexing and browsing, particularly in the domain of structured video (e.g. sports video).},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {243–246},
numpages = {4},
keywords = {video indexing, spatio-temporal consistency, color characterization, content analysis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@dataset{10.1145/review-957013.957062_R37348,
author = {Palomares, Jose Manuel M.},
title = {Review ID:R37348 for DOI: 10.1145/957013.957062},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-957013.957062_R37348}
}

@inproceedings{10.1145/957013.957063,
author = {Fan, Xin and Xie, Xing and Zhou, He-Qin and Ma, Wei-Ying},
title = {Looking into Video Frames on Small Displays},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957063},
doi = {10.1145/957013.957063},
abstract = {With the growing popularity of personal digital assistants and smart phones, people have become enthusiastic to watch videos through these mobile devices. However, a crucial challenge is to provide a better user experience for browsing videos on the limited and heterogeneous screen sizes. In this paper, we present a novel approach which allows users to overcome the display constraints by zooming into video frames while browsing. An automatic approach for detecting the focus regions is introduced to minimize the amount of user interaction. In order to improve the quality of output stream, virtual camera control is employed in the system. Preliminary evaluation shows that this approach is an effective way for video browsing on small displays.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {247–250},
numpages = {4},
keywords = {virtual camera control, adaptive content delivery, mobile device, form factor, video adaptation},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957064,
author = {Hollfelder, Silvia and Fankhauser, Peter and Neuhold, Erich J.},
title = {Observation Based vs. Model Based Admission Control for Interactive Multimedia Sessions},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957064},
doi = {10.1145/957013.957064},
abstract = {Interactive multimedia sessions cause high variations in workload due to presenting different media streams at different points in time. As users interact, the workload variations can not be predicted precisely. But admission control mechanisms need to at least estimate the workload in order to provide Quality of Service. In this paper, we investigate two approaches to estimate workloads and on this basis introduce admission control mechanisms that give stochastic QoS guarantees for such sessions. The observation based approach uses bookkept workload data for the prediction. The model based approach employs knowledge about usage patterns and media streams. For both approaches we introduce a uniform stochastic admission control criterion. Furthermore, we illustrate the relative benefits of these approaches for various session scenarios by means of simulations.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {251–254},
numpages = {4},
keywords = {session based admission control, continuous time markov chains, interactive multimedia applications},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957065,
author = {Iyengar, G. and Nock, H. J.},
title = {Discriminative Model Fusion for Semantic Concept Detection and Annotation in Video},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957065},
doi = {10.1145/957013.957065},
abstract = {In this paper we describe a general information fusion algorithm that can be used to incorporate multimodal cues in building user-defined semantic concept models. We compare this technique with a Bayesian Network-based approach on a semantic concept detection task. Results indicate that this technique yields superior performance. We demonstrate this approach further by building classifiers of arbitrary concepts in a score space defined by a pre-deployed set of multimodal concepts. Results show annotation for user-defined concepts both in and outside the pre-deployed set is competitive with our best video-only models on the TREC Video 2002 corpus.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {255–258},
numpages = {4},
keywords = {digital video annotation and indexing, ACM proceedings, semantic concept detection},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957066,
author = {Kang, Hang-Bong},
title = {Affective Content Detection Using HMMs},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957066},
doi = {10.1145/957013.957066},
abstract = {This paper discusses a new technique for detecting affective events using Hidden Markov Models(HMM). To map low level features of video data to high level emotional events, we perform empirical study on the relationship between emotional events and low-level features. After that, we compute simple low-level features that represent emotional characteristics and construct a token or observation vector by combining low level features. The observation vector sequence is tested to detect emotional events through HMMs. We create two HMM topologies and test both topologies. The affective events are detected from our proposed models with good accuracy.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {259–262},
numpages = {4},
keywords = {content analysis, hidden Markov models, emotional event},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957067,
author = {Lienhart, Rainer and Kozintsev, Igor and Wehr, Stefan},
title = {Universal Synchronization Scheme for Distributed Audio-Video Capture on Heterogeneous Computing Platforms},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957067},
doi = {10.1145/957013.957067},
abstract = {We propose a universal synchronization scheme for distributed audio-video capture on heterogeneous computing devices such as laptops, tablets, PDAs, cellular phones, audio recorders, and camcorders. These devices typically possess sensors such as microphones and possibly cameras. In order to combine them wirelessly into a distributed sensing and computing system, it is necessary to provide relative time synchronization among the distributed sensors. In this work we propose a setup and an algorithm that provide synchronization between sampling times for a network of distributed multi-channel audio sensors connected to general purpose computing (GPC) platforms. Extensive experimental results on distributed acoustic Blind Source Separation (BSS) algorithms validate the performance of our synchronization scheme.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {263–266},
numpages = {4},
keywords = {distributed audio-video processing, distributed audio-video synchronization, distributed microphone array},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957068,
author = {Lin, Cheng-Yuan and Jang, J.-S. Roger and Hsu, Mao-Yuan},
title = {An Automatic Singing Voice Rectifier Design},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957068},
doi = {10.1145/957013.957068},
abstract = {This paper proposes a new approach to automatic singing voice rectification. There are two components in the rectifier; one is the recognizer based on dynamic time warping and the other is the synthesizer based PSOLA (Pitch Synchronous Overlap and Add) for pitch shifting. The purpose of the recognizer is to identify the locations of off-key parts of the user's acoustic input. Then with the target music score, the synthesizer tries to correct the off-key parts by appropriate pitch shifting to match the give music score. We also attempt some singing and listening experiments for evaluating the feasibility of the rectifier and the results exhibit the satisfactory performance.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {267–270},
numpages = {4},
keywords = {singing voice rectifier, dynamic time warping, pitch synchronous overlap and add},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957069,
author = {Matsumura, Kinji and Usui, Kazuya and Kai, Kenjiro and Ishikawa, Koichi},
title = {Location-Aware Data Broadcasting: An Application for Digital Mobile Broadcasting in Japan},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957069},
doi = {10.1145/957013.957069},
abstract = {Terrestrial digital broadcasting that uses the ISDB-T (Integrated Services Digital Broadcasting-Terrestrial) system is scheduled for launch in Japan in December 2003. This system also enables mobile broadcasting service, which will be offered a few years later. We are developing a Location-Aware Data Broadcasting Service as a remarkably new type of interactive mobile broadcasting service. In this paper, we describe the service application, information filtering method, and presentation techniques for this location-aware data service. We also discuss a further experiment that uses scalable vector graphics.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {271–274},
numpages = {4},
keywords = {location-aware, GPS, terrestrial digital broadcasting, ISDB, mobile reception, data broadcasting, BML},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957070,
author = {Monay, Florent and Gatica-Perez, Daniel},
title = {On Image Auto-Annotation with Latent Space Models},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957070},
doi = {10.1145/957013.957070},
abstract = {Image auto-annotation, i.e., the association of words to whole images, has attracted considerable attention. In particular, unsupervised, probabilistic latent variable models of text and image features have shown encouraging results, but their performance with respect to other approaches remains unknown. In this paper, we apply and compare two simple latent space models commonly used in text analysis, namely Latent Semantic Analysis (LSA) and Probabilistic LSA (PLSA). Annotation strategies for each model are discussed. Remarkably, we found that, on a 8000-image dataset, a classic LSA model defined on keywords and a very basic image representation performed as well as much more complex, state-of-the-art methods. Furthermore, non-probabilistic methods (LSA and direct image matching) outperformed PLSA on the same dataset.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {275–278},
numpages = {4},
keywords = {semantic indexing, automatic annotation of images},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957071,
author = {Nack, Frank and Manniesing, Amit and Hardman, Lynda},
title = {Colour Picking: The Pecking Prder of Form and Function},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957071},
doi = {10.1145/957013.957071},
abstract = {Multimedia presentation generation has to be able to balance the functional aspects of a presentation that address the information needs of the user and its aesthetic form. We demonstrate our approach using automatic colour design for which we integrate relevant aspects of colour theory. We do not provide a definition of the relative importance of form versus function, but seek to explore the roles of subjective elements in the generation process.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {279–282},
numpages = {4},
keywords = {colour harmonisation, style-driven multimedia presentation generation, multimedia semantics, automatic colour design},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957072,
author = {Ngo, Chong-Wah},
title = {A Robust Dissolve Detector by Support Vector Machine},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957072},
doi = {10.1145/957013.957072},
abstract = {In this paper, we propose a novel approach for the robust detection and classification of dissolve sequences in videos. Our approach is based on the multi-resolution representation of temporal slices extracted from 3D image volume. At the low-resolution (LR) scale, the problem of dissolve detection is reduced as cut transition detection. At the high-resolution (HR) space, Gabor wavelet features are computed for regions that surround the cuts located at LR scale. The computed features are then input to support vector machines for pattern classification. Encouraging results have been obtained through experiments.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {283–286},
numpages = {4},
keywords = {support vector machine, dissolve detector, temporal slices},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957073,
author = {Phung, Dinh Q. and Venkatesh, Svetha and Dorai, Chitra},
title = {Hierarchical Topical Segmentation in Instructional Films Based on Cinematic Expressive Functions},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957073},
doi = {10.1145/957013.957073},
abstract = {In this paper, we propose a novel solution for segmenting an instructional video into hierarchical topical sections. Incorporating the knowledge of education-oriented film theory with our previous study of expressive functions namely the content density and the thematic functions, we develop an algorithm to effectively structuralize an instructional video into a two-tiered hierarchy of topical sections at the main and sub-topic levels. Our experimental results on a set of ten industrial instructional videos demonstrate the validity of the detection scheme.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {287–290},
numpages = {4},
keywords = {topic detection/segmentation, instructional films, media aesthetics, cinematic expressive functions, narrative structure},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957074,
author = {Rojas, Juan Carlos and Leeser, Miriam},
title = {Programming Portable Optimized Multimedia Applications},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957074},
doi = {10.1145/957013.957074},
abstract = {Multimedia computer architectures can speed-up applications significantly when programmed manually. Optimized programs have been non-portable up to now, because of differences in instruction sets, register lengths, alignment requirements and programming styles. We solve all these problems by using a library of C pre-processor macros called MMM. We implemented three examples from video compression in MMM, and automatically translated them into optimized code for four distinct multimedia processors. Their performance is comparable, and in several cases better, than equivalent examples optimized by the processor vendors.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {291–294},
numpages = {4},
keywords = {portability, multimedia, MMX, SSE, SSE2, TriMedia, optimization, AltiVec, SIMD},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957075,
author = {Rui, Yong and Liu, Zicheg},
title = {ARTiFACIAL: Automated Reverse Turing Test Using FACIAL Features},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957075},
doi = {10.1145/957013.957075},
abstract = {Web services designed for human users are being abused by computer programs (bots). The bots steal thousands of free email accounts in a minute; participate in online polls to skew results; and irritate people by joining online chat rooms. These real-world issues have recently generated a new research area called Human Interactive Proofs (HIP), whose goal is to defend services from malicious attacks by differentiating bots from human users. In this paper, we propose a new HIP algorithm based on detecting human face and facial features. Human faces are the most familiar object to humans, rendering it possibly the best candidate for HIP. We conducted user studies and showed the ease of use of our system to human users. We designed attacks using the best existing face detectors and demonstrated the difficulty to bots.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {295–298},
numpages = {4},
keywords = {human interactive proof (HIP), face and facial feature detection, CAPTCHA, turing test, web services security},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957076,
author = {Salway, Andrew and Graham, Mike},
title = {Extracting Information about Emotions in Films},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957076},
doi = {10.1145/957013.957076},
abstract = {We present a method being developed to extract information about characters' emotions in films. It is suggested that this information can help describe higher levels of multimedia semantics relating to narrative structures. Our method extracts information from audio description that is provided for the visually-impaired with an increasing number of films. The method is based on a cognitive theory of emotions that links a character's emotional states to the events in their environment. In this paper the method is described along with some preliminary evaluation and discussions about the kinds of novel video retrieval and browsing applications it may support.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {299–302},
numpages = {4},
keywords = {audio description, narrative, video retrieval, semantic video content, emotions, film, video browsing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957077,
author = {Takemae, Yoshinao and Otsuka, Kazuhiro and Mukawa, Naoki},
title = {Video Cut Editing Rule Based on Participants' Gaze in Multiparty Conversation},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957077},
doi = {10.1145/957013.957077},
abstract = {This paper proposes a video cut editing rule based on participants' gaze for extracting and conveying the flow of conversation in multiparty conversation. Systems that record meetings and those that support teleconferences are attracting considerable interest. Conventional systems use a fixed-viewpoint camera and simple camera selection based on participants' utterances. However, conventional systems fail to convey a sufficient amount of nonverbal information about the participants and the flow of conversation. We focus on participants' gaze since it is a good indicator of the participants' intent and emotion, conversational attention etc. We propose a video cut editing rule based on the convergence of participants' gaze direction. We conduct an experiment to evaluate the effectiveness of the proposed method. The results indicate that the proposed method can successfully convey who is taking to whom, which is a key indicator of the flow of conversation.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {303–306},
numpages = {4},
keywords = {teleconferencing, recording meetings, multiple cameras, multiparty conversation, gaze, video cut editing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957078,
author = {Venkatramani, Chitra and Westerink, Peter and Verscheure, Olivier and Frossard, Pascal},
title = {Securing Media for Adaptive Streaming},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957078},
doi = {10.1145/957013.957078},
abstract = {This paper describes the ARMS system which enables secure and adaptive rich media streaming to a large-scale, heterogeneous client population. The secure streaming algorithms ensure end-to-end security while the content is adapted and streamed via intermediate, potentially untrusted servers. ARMS streaming is completely standards compliant and to our knowledge is the first such end-to-end MPEG-4-based system.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {307–310},
numpages = {4},
keywords = {MPEG-4, video server, scalability, streaming, adaptive, encrypted},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957079,
author = {Wan, Kongwah and Yan, Xin and Yu, Xinguo and Xu, Changsheng},
title = {Real-Time Goal-Mouth Detection in MPEG Soccer Video},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957079},
doi = {10.1145/957013.957079},
abstract = {We report our work in real-time detection of goal-mouth appearances in MPEG soccer video. Processing on sub-optimal quality images after MPEG-decoding, the system constrains the Hough Transform-based line-mark detection to only the dominant green regions typically seen in soccer video. The vertical goal-posts and horizontal goal-bar are then isolated by color-based region (pole)-growing. We demonstrate its application for quick video browsing and virtual content insertion. Extensive test over a large data set of about 15 hours of MPEG-1 soccer video @1.15Mbps, CIF-resolution, shows the robustness of our method.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {311–314},
numpages = {4},
keywords = {video summarization, virtual content insertion},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957080,
author = {Wang, Feng and Ngo, Chong-Wah and Pong, Ting-Chuen},
title = {Synchronization of Lecture Videos and Electronic Slides by Video Text Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957080},
doi = {10.1145/957013.957080},
abstract = {An essential goal of structuring lecture videos captured in live presentation is to provide a synchronized view of video clips and electronic slides. This paper presents an automatic approach to match video clips and slides based on the analysis of text embedded in lecture videos. We describe a method to reconstruct high-resolution video texts from multiple keyframes for robust OCR recognition. A two-stage matching algorithm based on the title and content similarity measures between video clips and slides is also proposed.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {315–318},
numpages = {4},
keywords = {video text analysis, synchronization, lecture videos},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957081,
author = {Wang, Jun and Kankanhalli, Mohan S.},
title = {Experience Based Sampling Technique for Multimedia Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957081},
doi = {10.1145/957013.957081},
abstract = {We present a novel experience based sampling or experiential sampling technique which has the ability to focus on the analysis's task by making use of the contextual information from the environment. In this technique, sensor samples are used to gather information about the current environment and attention samples are used to represent the current state of attention. The task-attended samples are inferred from experience and maintained by a sampling based dynamical system. The multimedia analysis task can then focus on the attention samples only. Moreover, past experiences and the current environment can be used to adaptively correct and tune the attention. Experimental results have been presented to demonstrate the efficacy of our technique.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {319–322},
numpages = {4},
keywords = {attention, dynamical systems, experiential computing, sampling},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957082,
author = {Wang, Yuhang and Makedon, Fillia},
title = {R-Histogram: Quantitative Representation of Spatial Relations for Similarity-Based Image Retrieval},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957082},
doi = {10.1145/957013.957082},
abstract = {Representation of relative spatial relations between objects is required in many multimedia database applications. Quantitative representation of spatial relations taking into account shape, size, orientation and distance is often required. This cannot be accomplished by assimilating an object to elementary entities such as the centroid or the minimum bounding rectangle. Thus many authors have proposed numerous representations based on the notion of histograms of angles. However, they can only represent directional relations, but not the topological spatial relations "inside" and "overlap." Moreover, distance information is not explicitly taken into account. To address these issues, we propose in this paper a new histogram representation called R-Histogram that extends the histogram of angles by incorporating both angles and labeled distances. Dissimilarity between images is then defined by the distance between corresponding R-Histograms. A prototype Query By Example (QBE) system using the R-Histogram has been implemented. The effectiveness of our algorithm is demonstrated with experiments on two databases of 2000 synthetic images.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {323–326},
numpages = {4},
keywords = {similarity search, R-Histogram, spatial relations, image retrieval},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957083,
author = {Wang, Zhiheng and Banerjee, Sujata and Jamin, Sugih},
title = {Studying Streaming Video Quality: From an Application Point of View},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957083},
doi = {10.1145/957013.957083},
abstract = {An important aspect of improving streaming application performance is the streaming quality evaluation process. In this paper we introduce a set of alternative objective streaming video quality metrics, which are suitable for large scale deployment. Derived from an existent media application, our metrics are designed to capture the application behaviors disrupting the streaming video quality. We also present a set of experiments to demonstrate the effectiveness of these metrics.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {327–330},
numpages = {4},
keywords = {streaming video, video quality evaluation},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957084,
author = {Wu, Xiaomeng and Zhang, Wenli and Kamijo, Shunsuke and Sakauchi, Masao},
title = {Construction of Interactive Video Information System by Applying Results of Object Recognition},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957084},
doi = {10.1145/957013.957084},
abstract = {Although numerous attempts have been made to determine algorithms and approaches for building up a video information system, not many practical applications have been proposed. In this paper, a novel interactive video information system called the Drama Characters' Popularity Voting System (DCPVS) is constructed by applying the results of off-line object recognition. The system's purpose is to provide description annotation, retrieval, and statistics in the video associated with an object, such as a character as a basic unit, over the Internet. By using the proposed system, multiple users in a network can enjoy the same video and can vote for the characters they like in it. The voting information is collected and stored in the server, which then provides the statistics regarding the popularity of different characters or the voting rates within different periods of the video.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {331–334},
numpages = {4},
keywords = {interactive application, multimedia database, object recognition, video information utilization},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957085,
author = {Wyse, Lonce and Wang, Ye and Zhu, Xinglei},
title = {Application of a Content-Based Percussive Sound Synthesizer to Packet Loss Recovery in Music Streaming},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957085},
doi = {10.1145/957013.957085},
abstract = {This paper presents a novel method to recover lost packets in music streaming using a synthesizer to generate percussive sounds. As an improvement of the state-of-the-art system that uses a content-based audio codebook, the new method can greatly reduce the redundant information needed to recover perceptually critical lost packets.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {335–338},
numpages = {4},
keywords = {packet error recovery, music streaming, sound synthesis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957086,
author = {Yan, Rong and Hauptmann, Alexander G.},
title = {The Combination Limit in Multimedia Retrieval},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957086},
doi = {10.1145/957013.957086},
abstract = {Combining search results from multimedia sources is crucial for dealing with heterogeneous multimedia data, particularly in multimedia retrieval where a final ranked list of items of interest is returned sorted by confidence or relevance. However, relatively little attention has been given to combination functions, especially their upper bound performance limits. This paper presents a theoretical framework for studying upper bounds for two types of combination functions. A general upper bound and two approximations are proposed for monotonic combination functions. We also studied the upper bounds for linear combination functions using a global optimization technique. Our experimental results show that the choice of combination functions has a considerable influence to retrieval performance.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {339–342},
numpages = {4},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957087,
author = {Yan, Rong and Hauptmann, Alexander G. and Jin, Rong},
title = {Negative Pseudo-Relevance Feedback in Content-Based Video Retrieval},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957087},
doi = {10.1145/957013.957087},
abstract = {Video information retrieval requires a system to find information relevant to a query which may be represented simultaneously in different ways through a text description, audio, still images and/or video sequences. We present a novel approach that uses pseudo-relevance feedback from retrieved items that are NOT similar to the query items without further inquiring user feedback. We provide insight into this approach using a statistical model and suggest a score combination scheme via posterior probability estimation. An evaluation on the 2002 TREC Video Track queries shows that this technique can improve video retrieval performance on a real collection. We believe that negative pseudo-relevance feedback shows great promise for very difficult multimedia retrieval tasks, especially when combined with other different retrieval algorithms.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {343–346},
numpages = {4},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957088,
author = {Yonemoto, Satoshi and Nakano, Hiroshi and Taniguchi, Rin-ichiro},
title = {Avatar Motion Control by User Body Postures},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957088},
doi = {10.1145/957013.957088},
abstract = {This paper describes an avatar motion control by body postures. Our goal is to do seamless mapping of human motion in the real world into virtual environments. We hope that the idea of direct human motion sensing will be used on future interfaces. With the aim of making computing systems suited for users, we have developed a computer vision based avatar motion control. The human motion sensing is based on skin-color blob tracking. Our method can generate realistic avatar motion from the sensing data. We address our framework to use virtual scene context as a priori knowledge. We assume that virtual objects in virtual environments can afford avatar's action, that is, the virtual environments provide action information for the avatar. Avatar's motion is controlled, based on simulating the idea of affordance extended into the virtual environments.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {347–350},
numpages = {4},
keywords = {perceptual user interfaces, avatar, virtual environments},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957089,
author = {Yotsukura, Tatsuo and Morishima, Shigeo and Nakamura, Satoshi},
title = {Model-Based Talking Face Synthesis for Anthropomorphic Spoken Dialog Agent System},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957089},
doi = {10.1145/957013.957089},
abstract = {Towards natural human-machine communication, interface technologies by way of speech and image information have been intensively developed. An anthropomorphic dialog agent is an ideal system, which integrates spoken dialog and natural facial expressions. This paper reports on our project aiming to create a general-purpose toolkit for building an easily customizable anthropomorphic agent. There have been almost no tools so far such as intuitive, easy to understand, fully interactive, and open source. Our anthropomorphic agent is designed to fulfill these requirements. This toolkit consists four modules, multi modal dialog integration, speech recognition, speech synthesis, and face image synthesis. These modules are highly modularized and interlinked by a simple communication protocols.In this paper, we focus on the construction of an agent's face image synthesis. For this part lip movement control synchronous to the speech signal and facial emotion expression are the most important parts. We developed the face image synthesis module (FSM) that only requires one frontal face image, and can be used by any skill level of users. A user's original agent can be generated by easy adjustment of the frontal face image and the generic wire-frame model. The paper describes overall system diagram and specifically the agent's face image synthesis part.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {351–354},
numpages = {4},
keywords = {lip synchronization, face image synthesis, anthropomorphic dialog agent, facial animation},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957090,
author = {Zhang, Lei and Chen, Longbin and Li, Mingjing and Zhang, Hongjiang},
title = {Automated Annotation of Human Faces in Family Albums},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957090},
doi = {10.1145/957013.957090},
abstract = {Automatic annotation of photographs is one of the most desirable needs in family photograph management systems. In this paper, we present a learning framework to automate the face annotation in family photograph albums. Firstly, methodologies of content-based image retrieval and face recognition are seamlessly integrated to achieve automated annotation. Secondly, face annotation is formulated in a Bayesian framework, in which the face similarity measure is defined as maximum a posteriori (MAP) estimation. Thirdly, to deal with the missing features, marginal probability is used so that samples which have missing features are compared with those having the full feature set to ensure a non-biased decision. The experimental evaluation has been conducted within a family album of few thousands of photographs and the results show that the proposed approach is effective and efficient in automated face annotation in family albums.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {355–358},
numpages = {4},
keywords = {content-based image retrieval, face annotation, face recognition},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957091,
author = {Zhu, Yongwei and Kankanhalli, Mohan},
title = {Music Scale Modeling for Melody Matching},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957091},
doi = {10.1145/957013.957091},
abstract = {Several time series matching techniques have been proposed for content-based music retrieval. These techniques have shown to be robust and effective for music retrieval by acoustic inputs, such as query-by-humming. However, due to the key transposition issue, all the current methods need to search a large space for the proper key in melody matching. This computation can be prohibitive for a practical music retrieval system with a large database.In this paper, we present a music scale modeling technique for melody matching. The root note of music scale (Major or Minor) of a melody is estimated by fitting the notes to a music scale model. The estimated root note can then be used as the key in melody matching. To the best of our knowledge, this is the first approach that utilizes music scale knowledge for retrieval. In our experiments, 96% of the songs in the database (3000 melodies) can fit into the music scale model. Promising results for query-by-humming retrieval have been obtained by using this novel approach.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {359–362},
numpages = {4},
keywords = {music scale, query-by-humming, content-based music retrieval},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957093,
author = {Cooper, Matthew and Foote, Jonathan and Girgensohn, Andreas and Wilcox, Lynn},
title = {Temporal Event Clustering for Digital Photo Collections},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957093},
doi = {10.1145/957013.957093},
abstract = {We present similarity-based methods to cluster digital photos by time and image content. The approach is general, unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present results for the algorithm based solely on temporal similarity, and jointly on temporal and content-based similarity. We also describe a supervised algorithm based on learning vector quantization. Finally, we include experimental results for the proposed algorithms and several competing approaches on two test collections.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {364–373},
numpages = {10},
keywords = {digital photo organization, temporal media indexing and segmentation},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957094,
author = {Ma, Yu-Fei and Zhang, Hong-Jiang},
title = {Contrast-Based Image Attention Analysis by Using Fuzzy Growing},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957094},
doi = {10.1145/957013.957094},
abstract = {Visual attention analysis provides an alternative methodology to semantic image understanding in many applications such as adaptive content delivery and region-based image retrieval. In this paper, we propose a feasible and fast approach to attention area detection in images based on contrast analysis. The main contributions are threefold: 1) a new saliency map generation method based on local contrast analysis is proposed; 2) by simulating human perception, a fuzzy growing method is used to extract attended areas or objects from the saliency map; and 3) a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points. This framework facilitates visual analysis tools or vision systems to automatically extract attentions from images in a manner like human perception. User study results indicate that the proposed approach is effective and practicable.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {374–381},
numpages = {8},
keywords = {contrast analysis, attention detection, visual attention model, image analysis, fuzzy growing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957095,
author = {Yu, Bin and Ma, Wei-Ying and Nahrstedt, Klara and Zhang, Hong-Jiang},
title = {Video Summarization Based on User Log Enhanced Link Analysis},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957095},
doi = {10.1145/957013.957095},
abstract = {Efficient video data management calls for intelligent video summarization tools that automatically generate concise video summaries for fast skimming and browsing. Traditional video summarization techniques are based on low-level feature analysis, which generally fails to capture the semantics of video content. Our vision is that users unintentionally embed their understanding of the video content in their interaction with computers. This valuable knowledge, which is difficult for computers to learn autonomously, can be utilized for video summarization process. In this paper, we present an intelligent video browsing and summarization system that utilizes previous viewers' browsing log to facilitate future viewers. Specifically, a novel ShotRank notion is proposed as a measure of the subjective interestingness and importance of each video shot. A ShotRank computation framework is constructed to seamlessly unify low-level video analysis and user browsing log mining. The resulting ShotRank is used to organize the presentation of video shots and generate video skims. Experimental results from user studies have strongly confirmed that ShotRank indeed represents the subjective notion of interestingness and importance of each video shot, and it significantly improves future viewers' browsing experience.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {382–391},
numpages = {10},
keywords = {video summarization, skimming, video content analysis, log mining, user behavior, link analysis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957096,
author = {Shipman, Frank and Girgensohn, Andreas and Wilcox, Lynn},
title = {Generation of Interactive Multi-Level Video Summaries},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957096},
doi = {10.1145/957013.957096},
abstract = {In this paper, we describe how a detail-on-demand representation for interactive video is used in video summarization. Our approach automatically generates a hypervideo composed of multiple video summary levels and navigational links between these summaries and the original video. Viewers may interactively select the amount of detail they see, access more detailed summaries, and navigate to the source video through the summary. We created a representation for interactive video that supports a wide range of interactive video applications and Hyper-Hitchcock, an editor and player for this type of interactive video. Hyper-Hitchcock employs methods to determine (1) the number and length of levels in the hypervideo summary, (2) the video clips for each level in the hypervideo, (3) the grouping of clips into composites, and (4) the links between elements in the summary. These decisions are based on an inferred quality of video segments and temporal relations those segments.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {392–401},
numpages = {10},
keywords = {video editing, hypervideo, link generation, video summarization},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957098,
author = {Militzer, Michael and Suchomski, Maciej and Meyer-Wegener, Klaus},
title = {Improved P-Domain Rate Control and Perceived Quality Optimizations for MPEG-4 Real-Time Video Applications},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957098},
doi = {10.1145/957013.957098},
abstract = {The paper describes bit rate control for a one-pass MPEG-4 video encoding algorithm in order to make it suitable for real-time applications. The proposed control method is of low computational complexity and more accurate than previous approaches. In result, the rate-control buffer size which highly influences the latency between a video sender and receiver can be decreased significantly. Additionally, a solution is proposed for increasing the perceived quality by introducing an advanced bit allocation scheme and by exploiting activity masking. The proposed algorithm has been implemented in the XVID codec, a representative of the MPEG-4 standard. Experiments prove that the proposed algorithm is highly accurate and provides improved perceived visual quality. Moreover, the implementation outperforms other up-to-now bit rate control algorithms.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {402–411},
numpages = {10},
keywords = {"live streaming", "real-time", "video encoding", "quality optimization", "bit rate control", "MPEG-4", "p-domain"},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957099,
author = {Wang, Ye and Ahmaniemi, Ali and Isherwood, David and Huang, Wendong},
title = {Content-Based UEP: A New Scheme for Packet Loss Recovery in Music Streaming},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957099},
doi = {10.1145/957013.957099},
abstract = {Bandwidth efficiency and error robustness are two essential and conflicting requirements for streaming media content over error-prone channels, such as wireless channels. This paper describes a new scheme called content-based unequal error protection (C-UEP), which aims to improve the user-perceived QoS in the case of packet loss. We use music streaming as an example to show the effectiveness of the new concept. C-UEP requires only a small fraction of the redundancy used in existing forward error correction (FEC) methods. C-UEP classifies every audio segment (e.g. an encoding frame) into different classes to improve encoding efficiency. Salient transients such as drumbeats and note onsets are encoded with more redundancy in a secondary bitstream used to recover lost packets by the receiver. Formal perceptual evaluations show that our scheme improves audio quality significantly over simple muting and packet repetition baselines. This improvement is achieved with a negligible amount of redundancy, which is transmitted to the receiver ahead of playback.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {412–421},
numpages = {10},
keywords = {prioritized resource allocation, packet loss recovery, error robustness, user-perceived QoS, content-based unequal error protection (C-UEP), audio coding and streaming},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957100,
author = {Chakareski, J. and Han, S. and Girod, B.},
title = {Layered Coding vs. Multiple Descriptions for Video Streaming over Multiple Paths},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957100},
doi = {10.1145/957013.957100},
abstract = {In this paper, we examine the performance of specific implementations of multiple description coding and of layered coding for video streaming over error-prone packet switched networks. We compare their performance using different transmission schemes with and without network path diversity. It is shown that given the specific implementations there is a large variation in relative performance between multiple description coding and layered coding depending on the employed transmission scheme. For scenarios where the packet transmission schedules can be optimized in a rate-distortion sense, layered coding provides a better performance. The converse is true for scenarios where the packet schedules are not rate-distortion optimized.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {422–431},
numpages = {10},
keywords = {packet path diversity, layered coding, automatic repeat request (ARQ), optimized streaming, multiple description coding, rate-distortion},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957101,
author = {Peng, Cheng and Deng, Robert H. and Wu, Yongdong and Shao, Weizhong},
title = {A Flexible and Scalable Authentication Scheme for JPEG2000 Image Codestreams},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957101},
doi = {10.1145/957013.957101},
abstract = {JPEG2000 is an emerging standard for still image compression and is becoming the solution of choice for many digital imaging fields and applications. An important aspect of JPEG2000 is its "compress once, decompress many ways" property [1], i. e., it allows extraction of various sub-images (e.g., images with various resolutions, pixel fidelities, tiles and components) all from a single compressed image codestream. In this paper, we present a flexible and scalable authentication scheme for JPEG2000 images based on the Merkle hash tree and digital signature. Our scheme is fully compatible with JPEG2000 and possesses a "sign once, verify many ways" property. That is, it allows users to verify the authenticity and integrity of different sub-images extracted from a single compressed codestream protected with a single digital signature.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {433–441},
numpages = {9},
keywords = {merkle hash tree, JPEG2000, authentication, message digest, data integrity, one-way hash function, digital signature, image compression},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957103,
author = {Agnihotri, Lalitha and Dimitrova, Nevenka and Kender, John and Zimmerman, John},
title = {Music Videos Miner},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957103},
doi = {10.1145/957013.957103},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {442–443},
numpages = {2},
keywords = {video analysis, user needs analysis, music video summarization, multimedia content analysis, chorus detection, music databases},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957104,
author = {Burges, Chris J. C. and Platt, John C. and Goldstein, Jonathan},
title = {Identifying Audio Clips with <i>RARE</i>},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957104},
doi = {10.1145/957013.957104},
abstract = {In this paper, we describe RARE (Robust Audio Recognition Engine): a system for identifying audio streams and files. RARE can be used in a variety of applications: from enhancing the consumer listening experience to cleaning large audio databases. RARE was designed with two key qualities in mind: robustness to distortion of the audio, and lookup speed. RARE identifies audio clips in a stream against a database of 1/4 million songs in real time using approximately 10% CPU on an 850 MHz P3, and with a measured false positive rate of 1.5x10-8 per clip, per database entry, at a false negative rate of 0.2% per clip. We demo RARE in real-time on a stream and on distorted files.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {444–445},
numpages = {2},
keywords = {fast indexing, audio fingerprinting, robust lookup},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957105,
author = {Chen, Shu-Ching and Shyu, Mei-Ling and Zhao, Na and Zhang, Chengcui},
title = {An Affinity-Based Image Retrieval System for Multimedia Authoring and Presentation},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957105},
doi = {10.1145/957013.957105},
abstract = {In this demonstration, we present an image retrieval system to support multimedia authoring and presentation. An affinity-based mechanism, Markov Model Mediator (MMM), is used as the search engine for the system, which utilizes both the low-level image features and the learned high-level concepts via user access patterns and access frequencies. This system is one of the major components of MediaManager, a distributed multimedia management system developed by us. Both retrieval and learning facilities are supported in this system. The retrieval system also provides input information to the Multimedia Augmented Transition Network (MATN) environment for multimedia authoring and presentation.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {446–447},
numpages = {2},
keywords = {content-based image retrieval, mediamanager, markov model mediator},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957106,
author = {Chew, Elaine and Francois, Alexandre R.J.},
title = {MuSA.RT: Music on the Spiral Array. Real-Time},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957106},
doi = {10.1145/957013.957106},
abstract = {We present MuSA.RT, Opus 1, a multimodal interactive system for music analysis and visualization using the Spiral Array model. Real-time MIDI input from a live performance is processed, analyzed and mapped to the 3D model, revealing tonal structures such as pitches, chords and keys. A user can concurrently navigate through the Spiral Array space using a gamepad or set the camera control to automatic pilot. The interaction among and concurrent processing of the different data streams is made possible through the Modular Flow Scheduling Middleware.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {448–449},
numpages = {2},
keywords = {SAI, MFSM, spiral array, music visualization, system implementation, music analysis},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957107,
author = {H\"{u}rst, Wolfgang},
title = {Indexing, Searching, and Skimming of Multimedia Documents Containing Recorded Lectures and Live Presentations},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957107},
doi = {10.1145/957013.957107},
abstract = {This demonstration illustrates different ways to support users dealing with recorded live presentations in order to improve the usability of the corresponding documents. It highlights different problems in this context and presents solutions and alternative approaches for both, multimedia indexing and query processing as well as user interface issues in order to support users who are skimming or browsing such documents in search for information.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {450–451},
numpages = {2},
keywords = {skimming and browsing of continuous multimedia data, presentation recording, multimedia indexing and retrieval},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957108,
author = {Jang, Jyh-Shing Roger and Jang, Yung-Sen},
title = {Microcontroller Implementation of Melody Recognition: A Prototype},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957108},
doi = {10.1145/957013.957108},
abstract = {This demo presents a 16-bit microcontroller implementation of a content-based music retrieval system that can take a user's acoustic input (5-second clip of singing or humming) and then retrieve the intended song from 20 candidate songs. Performance evaluation based on 192 clips shows that the system has a satisfactory top-1 recognition rate of 92%. This system demonstrates the feasibility of microcontroller based melody recognition for music retrieval, which can be used in consumer electronics such as melody-activated interactive toys, query engines for MP3 players or karaoke machines, and so on.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {452–453},
numpages = {2},
keywords = {microcontroller, content-based music retrieval, query by singing, melody recognition, query by humming, audio signal processing, embedded system, dynamic time warping},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957109,
author = {Kerne, Andruid and Sundaram, Vikram and Wang, Jin and Khandelwal, Madhur and Mistrot, J. Michael},
title = {Human + Agent: Creating Recombinant Information},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957109},
doi = {10.1145/957013.957109},
abstract = {combinFormation is a tool that enables browsing and collecting information elements in a generative space. By generative, we mean that the tool is an agent that automatically retrieves information elements and visually composes them. A combinFormation session presents a dynamic, evolving recombination of information elements from different sources. The elements are manipulable in the information space. Recombination is the process of taking previously unconnected elements, and combining them to create new configurations.One purpose of this space is to support the formation of ideas, through more and less focused processes of foraging. While ideas are forming, the criteria that underlie information foraging activities may not be well defined. Collecting the specific subset of related information elements is challenging. Cognitive scientists have established that combinations of images and textual elements are examples of preinventive structures that can lead to the emergence of new ideas. These preinventive structures often combine existing representations.Our program generates recombinant visualizations that develop interrelationships between the information elements. The generative visualization is based on a procedural model of the information, and the user's interests. The user model reflects interactions in which s/he explicitly expresses interest. The agent retrieves information based on the evolving model. The visual composition is also developed to emphasize the user's evolving sense of what is important. This involves solving problems in the dynamic visualization of dynamic, heterogeneous collections. In our novel interaction model, the human being shares control of the evolving information space with the agent. The user can express interest in information elements as they stream in, and design the visual space, using interactive tools. Expressions feed back through the model to drive the program's retrieval and visual composition decisions.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {454–455},
numpages = {2},
keywords = {procedural visual composition, generative hypertext, granularity of browsing, recombinant information},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957110,
author = {Lee, Alison and Hanson, Vicki},
title = {Enhancing Web Accessibility},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957110},
doi = {10.1145/957013.957110},
abstract = {This demonstration will illustrate the key technical and user interface aspects of the Web Adaptation Technology. Various transformations underlying the system will be shown that illustrate how this approach enables a wide range of users with reduced visual, cognitive, and motor abilities to access a large proportion of Web pages using a standard browser.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {456–457},
numpages = {2},
keywords = {accessibility, the Web, image enlargement, page segmentation, enlarge text, page linearization, speak text},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957111,
author = {Pestoni, Florian and Drews, Clemens},
title = {EXtensible Content Protection},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957111},
doi = {10.1145/957013.957111},
abstract = {This paper describes a proof of concept implementation of xCP, a content protection scheme for home networks.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {458–459},
numpages = {2},
keywords = {peer-to-peer computing, home networking, digital media, content protection},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957112,
author = {Pinhanez, Claudio and Kjeldsen, Rick and Tang, Lijun and Levas, Anthony and Podlaseck, Mark and Sukaviriya, Noi and Pingali, Gopal},
title = {Creating Touch-Screens Anywhere with Interactive Projected Displays},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957112},
doi = {10.1145/957013.957112},
abstract = {We demonstrate a system that combines steerable projection and computer vision technologies to create "touch-screen" style interactive displays on any flat surface in a space. A high-end version of the system -- the Everywhere Display (ED) -- combines an LCD projector with motorized focus and zoom, a computer controlled pan-tilt mirror, and a pan-tilt zoom camera to enable steering of interactive projections around space. A low-end version (ED-lite) enables creation of interactive displays using a portable projector and camera attached to a laptop computer. Unlike traditional augmented reality systems, the ED systems enable delivery of interactive multimedia content on ordinary objects without requiring users to wear head mounted displays or carry special input devices.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {460–461},
numpages = {2},
keywords = {steerable, interfaces, vision, smart spaces, graphics, ubiquitous},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957113,
author = {Rui, Yong and Liu, Zicheg},
title = {Excuse Me, but Are You Human?},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957113},
doi = {10.1145/957013.957113},
abstract = {Web services designed for human users are being abused by computer programs (bots). The bots steal thousands of free email accounts in a minute; participate in online polls to skew results; and irritate people by joining online chat rooms. These real-world issues have recently generated a new research area called Human Interactive Proofs (HIP), whose goal is to defend services from malicious attacks by differentiating bots from human users. We propose a new HIP algorithm based on detecting human face and facial features. Human faces are the most familiar object to humans, rendering it possibly the best candidate for HIP. We conducted user studies and showed the ease of use of our system to human users. We designed attacks using the best existing face detectors and demonstrated the difficulty to bots.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {462–463},
numpages = {2},
keywords = {human interactive proof (HIP), turing test, CAPTCHA, web services security, face and facial feature detection},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957114,
author = {Jun, Shen and Rong, Yan and Pei, Sun and Song, Song},
title = {Interactive Multimedia Messaging Service Platform},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957114},
doi = {10.1145/957013.957114},
abstract = {An interactive multimedia messaging service platform and two demos are described in this paper.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {464–465},
numpages = {2},
keywords = {interactive messaging, MMS, H.264/MPEG-4 AVC},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957115,
author = {Tosa, Naoko and Matsuoka, Seigo and Miyazaki, Koji},
title = {Interactive Storytelling System Using Behavior-Based Non-Verbal Information: ZENetic Computer},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957115},
doi = {10.1145/957013.957115},
abstract = {We have developed an interactive storytelling system that aims to help us "recreate" our conscious selves by calling on Buddhist principles, Asian philosophy, and traditional Japanese culture through the inspirational media of ink painting, kimono and haiku. "Recreating ourselves" means the process of making the consciousness of our 'daily self' meet that of our 'hidden self'. through stimulation of activity deep within us. Ultimately, this may meld our consciousness and unconsciousness in complete harmony. It is difficult to achieve this through traditional logic-based interactions. Our system is a new approach to reaching this goal by incorporating traditional media and methods in an interactive computer system.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {466–467},
numpages = {2},
keywords = {narrative technology, expression technology, editorial engineering, cognitive consciousness},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957116,
author = {WAN, Kongwah and YAN, Xin and YU, Xinguo and XU, Changsheng},
title = {Robust Goal-Mouth Detection for Virtual Content Insertion},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957116},
doi = {10.1145/957013.957116},
abstract = {In this paper, we describe a working system that detects and segments goal-mouth appearances of soccer video in real-time. Processing on sub-optimal quality images after MPEG-decoding, the system constrains the Hough Transform-based line-mark detection to only the dominant green regions. The vertical goal-posts and horizontal goal-bar are then isolated by color-based region (pole)-growing. We demonstrate its application for quick video browsing and virtual content insertion.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {468–469},
numpages = {2},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957118,
author = {Baker, H. Harlyn and Bhatti, Nina and Tanguay, Donald and Sobel, Irwin and Gelb, Dan and Goss, Michael E. and MacCormick, John and Yuasa, Kei and Culbertson, W. Bruce and Malzbender, Thomas},
title = {Computation and Performance Issues In Coliseum: An Immersive Videoconferencing System},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957118},
doi = {10.1145/957013.957118},
abstract = {Coliseum is a multiuser immersive remote teleconferencing system designed to provide collaborative workers the experience of face-to-face meetings from their desktops. Five cameras are attached to each PC display and directed at the participant. From these video streams, view synthesis methods produce arbitrary-perspective renderings of the participant and transmit them to others at interactive rates, currently about 15 frames per second. Combining these renderings in a shared synthetic environment gives the appearance of having all participants interacting in a common space. In this way, Coliseum enables users to share a virtual world, with acquired-image renderings of their appearance replacing the synthetic representations provided by more conventional avatar-populated virtual worlds. The system supports virtual mobility--participants may move around the shared space--and reciprocal gaze, and has been demonstrated in collaborative sessions of up to ten Coliseum workstations, and sessions spanning two continents. This paper summarizes the technology, and reports on issues related to its performance.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {470–479},
numpages = {10},
keywords = {videoconferencing, view synthesis, telepresence},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957119,
author = {Hosseini, Mojtaba and Georganas, Nicolas D.},
title = {Design of a Multi-Sender 3D Videoconferencing Application over an End System Multicast Protocol},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957119},
doi = {10.1145/957013.957119},
abstract = {Videoconferencing in the context of 3D virtual environments promises better spatial consistency and mutual awareness for its participants. However, in the absence of IP Multicast and limited upload bandwidth of today's DSL connections, the feasibility of such systems in supporting even a small group of users is in question. This paper presents the design and implementation of an awareness driven 3D videoconferencing application that runs on a peer-to-peer architecture and our own End System Multicast protocol. The paper highlights the unique requirements of multiparty videoconferencing applications and presents a solution that can support 4-10 bandwidth-limited users without the need for IP Multicast capability.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {480–489},
numpages = {10},
keywords = {peer-to-peer, awareness management, 3D videoconferencing, end system multicast},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957121,
author = {HUA, Xian-Sheng and LU, Lie and ZHANG, Hong-Jiang},
title = {AVE: Automated Home Video Editing},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957121},
doi = {10.1145/957013.957121},
abstract = {In this paper, we present a system that automates home video editing. This system automatically extracts a set of highlight segments from a set of raw home videos and aligns them with user supplied incidental music based on the content of the video and incidental music. We developed an approach for extracting temporal structure and determining the importance of a video segment in order to facilitate the selection of highlight segments. Additionally we extract temporal structure, beats and tempos from the incidental music. In order to create more professional-looking results, the selected highlight segments satisfy a set of editing rules and are matched to the content of the incidental music. This task is formulated as a non-linear 0-1 programming problem and the rules are embedded as constraints. The output video is rendered by connecting the selected highlight video segments with transition effects and the incidental music.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {490–497},
numpages = {8},
keywords = {video content analysis, audio segmentation, optimization, video skimming, music analysis, video segmentation, video editing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957122,
author = {Erol, Berna and Hull, Jonathan J. and Lee, Dar-Shyang},
title = {Linking Multimedia Presentations with Their Symbolic Source Documents: Algorithm and Applications},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957122},
doi = {10.1145/957013.957122},
abstract = {An algorithm is presented that automatically matches images of presentation slides to the symbolic source file (e.g., PowerPoint™ or Acrobat™) from which they were generated. The images are captured either by tapping the video output from a laptop connected to a projector or by taking a picture of what's displayed on the screen in a conference room. The matching algorithm extracts features from the image data, including OCR output, edges, projection profiles, and layout and determines the symbolic file that contains the most similar collection of features. This algorithm enables several unique applications for enhancing a meeting in real-time and accessing the audio and video that were recorded while a presentation was being given. These applications include the simultaneous translation of presentation slides during a meeting, linking video clips inside a PowerPoint file that show how each slide was described by the presenter, and retrieving presentation recordings using digital camera images as queries.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {498–507},
numpages = {10},
keywords = {synchronization, e-learning, document linking, multimedia meeting room, presentation recording, meeting recording},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957124,
author = {DeMenthon, Daniel and Doermann, David},
title = {Video Retrieval Using Spatio-Temporal Descriptors},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957124},
doi = {10.1145/957013.957124},
abstract = {This paper describes a novel methodology for implementing video search functions such as retrieval of near-duplicate videos and recognition of actions in surveillance video. Videos are divided into half-second clips whose stacked frames produce 3D space-time volumes of pixels. Pixel regions with consistent color and motion properties are extracted from these 3D volumes by a threshold-free hierarchical space-time segmentation technique. Each region is then described by a high-dimensional point whose components represent the position, motion and, when possible, color of the region. In the indexing phase for a video database, these points are assigned labels that specify their video clip of origin. All the labeled points for all the clips are stored into a single binary tree for efficient $k$-nearest neighbor retrieval. The retrieval phase uses video segments as queries. Half-second clips of these queries are again segmented to produce sets of points, and for each point the labels of its nearest neighbors are retrieved. The labels that receive the largest numbers of votes correspond to the database clips that are the most similar to the query video segment. We illustrate this approach for video indexing and retrieval and for action recognition. First, we describe retrieval experiments for dynamic logos, and for video queries that differ from the indexed broadcasts by the addition of large overlays. Then we describe experiments in which office actions (such as pulling and closing drawers, taking and storing items, picking up and putting down a phone) are recognized. Color information is ignored to insure independence to people's appearance. One of the distinct advantages of using this approach for action recognition is that there is no need for detection or recognition of body parts.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {508–517},
numpages = {10},
keywords = {space-time segmentation, object motion, video retrieval of near-duplicates, spatio-temporal descriptors, content-based indexing and retrieval, action recognition},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957125,
author = {Rao, Cen and Shah, Mubarak and Syeda-Mahmood, Tanveer},
title = {Invariance in Motion Analysis of Videos},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957125},
doi = {10.1145/957013.957125},
abstract = {In this paper, we propose an approach that retrieves motion of objects from the videos based on the dynamic time warping of view invariant characteristics. The motion is represented as a sequence of dynamic instants and intervals, which are automatically computed using the spatiotemporal curvature of the trajectory of moving object in the videos. Dynamic Time Warping (DTW) method matches trajectories using a view invariant similarity measure. Our system is able to incrementally learn different actions without any initialization mode, therefore it can work in an unsupervised manner. The retrieval of relevant videos can be easily performed by computing a simple distance metric. This paper makes two fundamental contribution to view invariant video retrieval: (1) Dynamic Instant detection in trajectories of moving objects acquired from video. (2) View-invariant Dynamic Time Warping to measure similarity between two trajectories of actions performed by different persons and from different viewpoints. Although the learning algorithm is relatively simple in our approach, we can achieve high recognition rate because of the view-invariant representation and the similarity measure using DTW.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {518–527},
numpages = {10},
keywords = {view-invariant action representation, view-invariant measure, human actions, spatiotemporal curvature, view-invariant dynamic time warping, learning},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957126,
author = {Wu, Gang and Wu, Yi and Jiao, Long and Wang, Yuan-Fang and Chang, Edward Y.},
title = {Multi-Camera Spatio-Temporal Fusion and Biased Sequence-Data Learning for Security Surveillance},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957126},
doi = {10.1145/957013.957126},
abstract = {We present a framework for multi-camera video surveillance. The framework consists of three phases: detection, representation, and recognition. The detection phase handles multi-source spatio-temporal data fusion for efficiently and reliably extracting motion trajectories from video. The representation phase summarizes raw trajectory data to construct hierarchical, invariant, and content-rich descriptions of the motion events. Finally, the recognition phase deals with event classification and identification on the data descriptors. Because of space limits, we describe only briefly how we detect and represent events, but we provide in-depth treatment on the third phase: event recognition. For effective recognition, we devise a sequence-alignment kernel function to perform sequence data learning for identifying suspicious events. We show that when the positive training instances (i.e., suspicious events) are significantly outnumbered by the negative training instances (benign events), then SVMs (or any other learning methods) can suffer a high incidence of errors. To remedy this problem, we propose the kernel boundary alignment (KBA) algorithm to work with the sequence-alignment kernel. Through empirical study in a parking-lot surveillance setting, we show that our spatio-temporal fusion scheme and biased sequence-data learning method are highly effective in identifying suspicious events.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {528–538},
numpages = {11},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957128,
author = {Hans, Mat C. and Smith, Mark T.},
title = {Interacting with Audio Streams for Entertainment and Communication},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957128},
doi = {10.1145/957013.957128},
abstract = {We present a new model of interactive audio for entertainment and communication. A new device called the DJammer and its associated technologies are described. The DJammer introduces the idea of provisioning mobile users to interact cooperatively with digital audio streams. Users can augment the audio in real time and communicate the result in several ways resulting in a new form of multimedia communication across diverse devices and multiple networks. This paper describes the technologies incorporated into the DJammer, and discusses the actual implementation of the prototype DJammer. Future enhancements are also described.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {539–545},
numpages = {7},
keywords = {peer-to-peer, mobility, distributed multimedia communication, distributed systems, digital content, context awareness, connectivity, network aggregation, rich media},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957129,
author = {Liao, Chunyuan and Liu, Qiong and Kimber, Don and Chiu, Patrick and Foote, Jonathan and Wilcox, Lynn},
title = {Shared Interactive Video for Teleconferencing},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957129},
doi = {10.1145/957013.957129},
abstract = {We present a system that allows remote and local participants to control devices in a meeting environment using mouse or pen based gestures "through" video windows. Unlike state-of-the-art device control interfaces that require interaction with text commands, buttons, or other artificial symbols, our approach allows users to interact with devices through live video of the environment. This naturally extends our video supported pan/tilt/zoom (PTZ) camera control system, by allowing gestures in video windows to control not only PTZ cameras, but also other devices visible in video images. For example, an authorized meeting participant can show a presentation on a screen by dragging the file on a personal laptop and dropping it on the video image of the presentation screen. This paper presents the system architecture, implementation tradeoffs, and various meeting control scenarios.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {546–554},
numpages = {9},
keywords = {video enabled device control, video conferencing, gesture based device control, panoramic video, video communication, collaborative device control, distance learning},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957130,
author = {Chen, Milton},
title = {Visualizing the Pulse of a Classroom},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957130},
doi = {10.1145/957013.957130},
abstract = {Effective classroom teaching often requires an instructor to be acutely aware of every student. The instructor must rapidly look from student to student to catch fleeting gestures or facial expressions. To facilitate the tracking of communicative actions in a remote classroom, we built a multiparty videoconferencing system that automatically determine whether students are speaking, making gestures, or moving in their seats. These activity indicators are displayed over the video such that the instructor can see into the recent past. The activity indicators are also grouped into a visualization of the classroom interaction dynamics, thereby providing a measure of the pulse of the classroom.We conducted a user study where teachers used our system in a simulated class. The teachers found that the activity indicators to be a useful teaching aid during class; however, the indicators are most useful as a record of the class. In a student survey, we found that if audio, video, or activity indicators must be recorded, students overwhelmingly prefer activity indicators since the indicators mask the content of the communication and thus are less intrusive to the students' privacy.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {555–561},
numpages = {7},
keywords = {activity history, visualization of communication pattern},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957132,
author = {Feng, Wu-chi and Code, Brian and Kaiser, Ed and Shea, Mike and Feng, Wu-chang and Bavoil, Louis},
title = {Panoptes: Scalable Low-Power Video Sensor Networking Technologies},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957132},
doi = {10.1145/957013.957132},
abstract = {Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This paper describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance. We describe a video sensor platform that can deliver high-quality video over 802.11 networks with a power requirement of approximately 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {562–571},
numpages = {10},
keywords = {video streaming, JPEG, MPEG, sensors, video sensors},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957133,
author = {Raykar, Vikas C. and Kozintsev, Igor and Lienhart, Rainer},
title = {Position Calibration of Audio Sensors and Actuators in a Distributed Computing Platform},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957133},
doi = {10.1145/957013.957133},
abstract = {In this paper, we present a novel approach to automatically determine the positions of sensors and actuators in an ad-hoc distributed network of general purpose computing platforms. The formulation and solution accounts for the limited precision in temporal synchronization among multiple platforms. The theoretical performance limit for the sensor positions is derived via the Cramer-Rao bound. We analyze the sensitivity of localization accuracy with respect to the number of sensors and actuators as well as their geometry. Extensive Monte Carlo simulation results are reported together with a discussion of the real-time system. In a test platform consisting of 4 speakers and 4 microphones, the sensors' and actuators' three dimensional locations could be estimated with an average bias of 0.08 cm and average standard deviation of 3.8 cm.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {572–581},
numpages = {10},
keywords = {sensor networks, self-localization, position calibration, Cramer-Rao bound, multidimensional scaling, microphone array calibration},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957134,
author = {Mohapatra, Shivajit and Cornea, Radu and Dutt, Nikil and Nicolau, Alex and Venkatasubramanian, Nalini},
title = {Integrated Power Management for Video Streaming to Mobile Handheld Devices},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957134},
doi = {10.1145/957013.957134},
abstract = {Optimizing user experience for streaming video applications on handheld devices is a significant research challenge. In this paper, we propose an integrated power management approach that unifies low level architectural optimizations (CPU, memory, register), OS power-saving mechanisms (Dynamic Voltage Scaling) and adaptive middleware techniques (admission control, optimal transcoding, network traffic regulation). Specifically, we identify interaction parameters between the different levels and optimize them to significantly reduce power consumption. With knowledge of device configurations, dynamic device parameters and changing system conditions, the middleware layer selects an appropriate video quality and fine tunes the architecture for optimized delivery of video. Our performance results indicate that architectural optimizations that are cognizant of user level parameters(e.g. transcoded video quality) can provide energy gains as high as 57.5% for the CPU and memory. Middleware adaptations to changing network noise levels can save as much as 70% of energy consumed by the wireless network interface. Furthermore, we demonstrate how such an integrated framework, that supports tight coupling of inter-level parameters can enhance user experience on a handheld substantially.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {582–591},
numpages = {10},
keywords = {low-power, cross-layer adaptation, multimedia streaming},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957136,
author = {HUA, Xian-Sheng and LU, Lie and ZHANG, Hong-Jiang},
title = {Photo2Video},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957136},
doi = {10.1145/957013.957136},
abstract = {To exploit rich content embedded in a single photograph, a system named Photo2Video was developed to automatically convert a photographic series into a video by simulating camera motions, set to incidental music of the user's choice. For a chosen photographic series, an appropriate camera motion pattern is selected for each photograph to generate a corresponding motion photograph clip. Then, the final output video is rendered by connecting a series of motion photograph clips with specific transitions, and aligning with the selected incidental music. Photo2Video provides a novel way to browse a series of images and can be regarded as a system exploring the new medium between photograph and video.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {592–593},
numpages = {2},
keywords = {integer programming, attention detection, face detection, audio segmentation, image clustering, image content analysis, optimization},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957137,
author = {Tryfonas, Christos and Schumacher, James},
title = {Essistants},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957137},
doi = {10.1145/957013.957137},
abstract = {One of the challenges the service providers currently face is the ability to introduce a variety of services at a minimal cost and impact to their customers. These services often become personalized as more and more content becomes available. The natural progression to the service/content explosion is a seamless user interface that remains consistent across the various services and devices. The Essistant architecture attempts to provide personalized services to the end users through a seamless multi-modal user interface and a systematization of the backend services. This paper describes the overall architecture of the Essistant project. The associated video demonstrates the functionality of Essistant for a set of services that have been implemented in a lab environment. The services include video-on-demand using an automated price broker, broadcast video over IP multicast, personalized news, and horoscope, and interaction with the physical space by acting as a proxy to a robot.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {594–595},
numpages = {2},
keywords = {multimedia presentation, service discovery, avatars, multimodal interfaces},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957138,
author = {Yip, Sam and Leu, Eugenia and Howe, Hunter},
title = {The Automatic Video Editor},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957138},
doi = {10.1145/957013.957138},
abstract = {More and more home videos are being produced with the increasing popularity of digital video camcorders. Yet the resulting home videos tend to be very long and boring to watch. The precious memories within those videos are ultimately lost. The problem is that the average home videographer does not have the time, or the editing skills to edit their home videos. It is a shame to let all those precious moments go to waste.There are editing software in the market that allow the user to edit their own home videos. For example, Apple's iMovie [1], Microsoft's MovieMaker [2], and the Hitchcock editing system [3]. But they still demand time, skill and effort from the user.With the above issue in mind, we developed the Automatic Video Editor, an application that analyzes a home video and edit it automatically into a condensed and interesting mini-movie.In our video, we will first show a clip of an unedited home video. Afterwards, we will show the output video created by our Automatic Video Editor.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {596–597},
numpages = {2},
keywords = {editor, automatic, video, home videos},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957139,
author = {Adcock, John and Cooper, Matthew and Doherty, John and Foote, Jonathan and Girgensohn, Andreas and Wilcox, Lynn},
title = {Managing Digital Memories with the FXPAL Photo Application},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957139},
doi = {10.1145/957013.957139},
abstract = {The FXPAL Photo Application is designed to faciliate the organization of digital images from digital cameras and other sources through automated organization and intuitive user interfaces.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {598–599},
numpages = {2},
keywords = {automatic event detection, user-centered design, digital photo collections},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957140,
author = {Doherty, John and Girgensohn, Andreas and Helfman, Jonathan and Shipman, Frank and Wilcox, Lynn},
title = {Detail-on-Demand Hypervideo},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957140},
doi = {10.1145/957013.957140},
abstract = {We demonstrate the use of detail-on-demand hypervideo in interactive training and video summarization. Detail-on-demand video allows viewers to watch short video segments and to follow hyperlinks to see additional detail. The player for detail-on-demand video displays keyframes indicating what links are available at each point in the video. The Hyper-Hitchcock authoring tool helps users create hypervideo by automatically dividing video into clips that can be combined in a direct manipulation interface. Clips can be grouped into composites and hyperlinks can be placed between clips and composites. A summarization algorithm creates multi-level hypervideo summaries from linear video by automatically selecting clips and placing links between them.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {600–601},
numpages = {2},
keywords = {video summarization, link generation, video editing, hypervideo},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957141,
author = {Davis, Marc},
title = {Active Capture: Automatic Direction for Automatic Movies},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957141},
doi = {10.1145/957013.957141},
abstract = {Current consumer media production is laborious, tedious, and produces unsatisfying results. To address this problem, Active Capture leverages media production knowledge, computer vision and audition algorithms, and user interaction techniques to automate direction and cinematography and thus enables the automatic production of annotated, high quality, reusable media assets. Active Capture is part of a new computational media production paradigm that transforms media production from a manual mechanical process into an automated computational one that can produce mass customized and personalized media integrating video of non-actors. The implemented system automates the process of capturing a non-actor performing two simple reusable actions ("screaming" and "turning her head to look at the camera") and automatically integrates those shots into various commercials and movie trailers.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {602–603},
numpages = {2},
keywords = {active capture, automatic movies, metadata, automatic direction, human-in-the-loop, video capture, automated direction, recognition},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957143,
author = {Li, Dongge and Dimitrova, Nevenka and Li, Mingkun and Sethi, Ishwar K.},
title = {Multimedia Content Processing through Cross-Modal Association},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957143},
doi = {10.1145/957013.957143},
abstract = {Multimodal information processing has received considerable attention in recent years. The focus of existing research in this area has been predominantly on the use of fusion technology. In this paper, we suggest that cross-modal association can provide a new set of powerful solutions in this area. We investigate different cross-modal association methods using the linear correlation model. We also introduce a novel method for cross-modal association called Cross-modal Factor Analysis (CFA). Our earlier work on Latent Semantic Indexing (LSI) is extended for applications that use off-line supervised training. As a promising research direction and practical application of cross-modal association, cross-modal information retrieval where queries from one modality are used to search for content in another modality using low-level features is then discussed in detail. Different association methods are tested and compared using the proposed cross-modal retrieval system. All these methods achieve significant dimensionality reduction. Among them CFA gives the best retrieval performance. Finally, this paper addresses the use of cross-modal association to detect talking heads. The CFA method achieves 91.1% detection accuracy, while LSI and Canonical Correlation Analysis (CCA) achieve 66.1% and 73.9% accuracy, respectively. As shown by experiments, cross-modal association provides many useful benefits, such as robust noise resistance and effective feature selection. Compared to CCA and LSI, the proposed CFA shows several advantages in analysis performance and feature usage. Its capability in feature selection and noise resistance also makes CFA a promising tool for many multimedia analysis applications.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {604–611},
numpages = {8},
keywords = {cross-modal information retrieval, cross-modal factor analysis (CFA), talking head analysis, cross-modal association},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957144,
author = {Wu, Hong and Lu, Hanqing and Ma, Songde},
title = {A Practical SVM-Based Algorithm for Ordinal Regression in Image Retrieval},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957144},
doi = {10.1145/957013.957144},
abstract = {Most current learning algorithms for image retrieval are based on dichotomy relevance judgement (relevant and non-relevant), though this measurement of relevance is too coarse. To better identify the user needs and preference, a good retrieval system should be able to handle multilevel relevance judgement. In this paper, we focus on relevance feedback with multilevel relevance judgment, where the relevance feedback is considered as an ordinal regression problem. Herbrich has proposed a support vector learning algorithm for ordinal regression based on the Linear Utility Model. His algorithm is intrinsically to train a SVM on a new derived training set, whose size increases rapidly when the original training set gets bigger. This property limits its applicability in relevance feedback, due to real-time requirement of the interactive process. By thoroughly analyzing Herbrich's algorithm, we first propose a new model for ordinal regression, called Cascade Linear Utility Model, then a practical SVM-based algorithm for image retrieval upon it. Our new algorithm is tested on a real-world image database, and compared with other three algorithms capable to handle multilevel relevance judgment. The experimental results show that the retrieval performance of our algorithm is comparable with that of Herbrich's algorithm but with only a fraction of its computational time, and apparently outperform the other methods.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {612–621},
numpages = {10},
keywords = {relevance feedback, ordinal regression, support vector machine, cascade linear model},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957145,
author = {Yu, Kai and Ma, Wei-Ying and Tresp, Volker and Xu, Zhao and He, Xiaofei and Zhang, HongJiang and Kriegel, Hans-Peter},
title = {Knowing a Tree from the Forest: Art Image Retrieval Using a Society of Profiles},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957145},
doi = {10.1145/957013.957145},
abstract = {This paper aims to address the problem of art image retrieval (AIR), which aims to help users find their favorite painting images. AIR is of great interests to us because of its application potentials and interesting research challenges---the retrieval is not only based on painting contents or styles, but also heavily based on user preference profiles. This paper describes the collaborative ensemble learning, a novel statistical learning approach to this task. It at first applies probabilistic support vector machines (SVMs) to model each individual user's profile based on given examples, i.e. liked or disliked paintings. Due to the high complexity of profile modelling, the SVMs can be rather weak in predicting preferences for new paintings. To overcome this problem, we combine a society of users' profiles, represented by their respective SVM models, to predict a given user's preferences for painting images. We demonstrate that the combination scheme is embedded in a Bayesian framework and retains intuitive interpretations---like-minded users are likely to share similar preferences. We report extensive empirical studies based on two experimental settings. The first one includes some controlled simulations performed on 4533 painting images. In the second setting, we report evaluations based on user preferences collected through an online web-based survey. Both experiments demonstrate that the proposed approach achieves excellent performance in terms of capturing a user's diverse preferences.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {622–631},
numpages = {10},
keywords = {collaborative ensemble learning, art image retrieval},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957146,
author = {Yang, Hui and Chaisorn, Lekha and Zhao, Yunlong and Neo, Shi-Yong and Chua, Tat-Seng},
title = {VideoQA: Question Answering on News Video},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957146},
doi = {10.1145/957013.957146},
abstract = {When querying a news video archive, the users are interested in retrieving precise answers in the form of a summary that best answers the query. However, current video retrieval systems, including the search engines on the web, are designed to retrieve documents instead of precise answers. This research explores the use of question answering (QA) techniques to support personalized news video retrieval. Users interact with our system, VideoQA, using short natural language questions with implicit constraints on contents, context, duration, and genre of expected videos. VideoQA returns short precise news video summaries as answers. The main contributions of this research are: (a) the extension of QA technology to support QA in news video; and (b) the use of multi-modal features, including visual, audio, textual, and external resources, to help correct speech recognition errors and to perform precise question answering. The system has been tested on 7 days of news video and has been found to be effective.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {632–641},
numpages = {10},
keywords = {video retrieval, video summarization, transcript error correction, video question answering},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957148,
author = {Hefeeda, Mohamed M.},
title = {A Framework for Cost-Effective Peer-to-Peer Content Distribution},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957148},
doi = {10.1145/957013.957148},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {642–643},
numpages = {2},
keywords = {economics of peer-to-peer systems, multimedia streaming, peer-to-peer systems, distributed trust management},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957149,
author = {Ott, David E. and Mayer-Patel, Ketan},
title = {Transport-Level Protocol Coordination in Distributed Multimedia Applications},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957149},
doi = {10.1145/957013.957149},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {644–645},
numpages = {2},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957150,
author = {Yu, Bin and Nahrstedt, Klara},
title = {A Scalable Overlay Video Mixing Service Model},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957150},
doi = {10.1145/957013.957150},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {646–647},
numpages = {2},
keywords = {overlay, service, video mixing, multiparty video conferencing},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957152,
author = {Barry, Barbara},
title = {The Mindful Camera: Common Sense for Documentary Videography},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957152},
doi = {10.1145/957013.957152},
abstract = {Cameras with story understanding can help videographers reflect on their process of content capture during documentary construction. This paper describes a set of tools that use common sense knowledge to support documentary videography.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {648–649},
numpages = {2},
keywords = {documentary videography, story understanding},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957153,
author = {Ratnaike, A. Viranga and Srinivasan, Bala and Nepal, Surya},
title = {Making Sense of Video Content},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957153},
doi = {10.1145/957013.957153},
abstract = {Our aim in this research is to make sense of scenes in video. We expect this will enable us to identify different scenes sharing the same semantic, even if they do not share any multimedia cues. Our approach is based on emergence, and involves classification and reasoning. We use patterns of cues to synthesize semantic classifications. These classifications need to be consistent with the observed cues, and suggest other elements that might be present. The suggestions might be based on sets of related patterns, ontology, or other knowledge bases. With an idea of what we are looking for, we can re-examine the scenes for multimedia elements, which support our hypothesis, or at least are not inconsistent. We expect that sufficiently detailed semantic descriptions can be generated, by cycling through these steps.},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {650–651},
numpages = {2},
keywords = {semantic gap, synthesis, semantic modeling, scene analysis, video semantics},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

@inproceedings{10.1145/957013.957154,
author = {Song, Dezhen},
title = {Algorithms and Systems for Shared Access to a Robotic Streaming Video Camera},
year = {2003},
isbn = {1581137222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/957013.957154},
doi = {10.1145/957013.957154},
booktitle = {Proceedings of the Eleventh ACM International Conference on Multimedia},
pages = {652–653},
numpages = {2},
keywords = {webcam, collaborative, teleoperation, internet robot, videoconferencing, control},
location = {Berkeley, CA, USA},
series = {MULTIMEDIA '03}
}

