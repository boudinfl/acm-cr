@inproceedings{10.1145/2072298.2072300,
author = {Chang, Shih-Fu},
title = {Content Based Multimedia Retrieval: Lessons Learned from Two Decades of Research},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072300},
doi = {10.1145/2072298.2072300},
abstract = {In the past two decades, we have witnessed bourgeoning research on content based multimedia information retrieval, covering a wide range of topics such as feature extraction, content matching, structure parsing, semantic annotation, multimodal analysis, 3D content retrieval, and user-in-the-loop interaction. More than ten years have also passed since the publication of the influential survey paper by Smeulders et al on content based image retrieval. Recently, exciting solutions are emerging in several practical contexts such as mobile media search, augmented reality, and Web-scale copy detection. However, many fundamental problems remain open, including but not limited to large-scale semantic annotation, multimedia ontological organization, and human-machine interaction for searching complex events. In this talk, I will discuss lessons learned from our past research, drawing from successes and failures in developing and deploying a few image/video search systems in different domains, and then share views about promising future directions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1–2},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072301,
author = {Yu, Felix X. and Ji, Rongrong and Chang, Shih-Fu},
title = {Active Query Sensing for Mobile Location Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072301},
doi = {10.1145/2072298.2072301},
abstract = {While much exciting progress is being made in mobile visual search, one important question has been left unexplored in all current systems. When the first query fails to find the right target (up to 50% likelihood), how should the user form his/her search strategy in the subsequent interaction? In this paper, we propose a novel Active Query Sensing system to suggest the best way for sensing the surrounding scenes while forming the second query for location search. We accomplish the goal by developing several unique components -- an offline process for analyzing the saliency of the views associated with each geographical location based on score distribution modeling, predicting the visual search precision of individual views and locations, estimating the view of an unseen query, and suggesting the best subsequent view change. Using a scalable visual search system implemented over a NYC street view data set (0.3 million images), we show a performance gain as high as two folds, reducing the failure rate of mobile location search to only 12% after the second query. This work may open up an exciting new direction for developing interactive mobile media applications through innovative exploitation of active sensing and query formulation.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {3–12},
numpages = {10},
keywords = {mobile visual search, active query sensing, mobile location recognition, content-based image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072302,
author = {Wu, Wanmin and Arefin, Ahsan and Kurillo, Gregorij and Agarwal, Pooja and Nahrstedt, Klara and Bajcsy, Ruzena},
title = {Color-plus-Depth Level-of-Detail in 3D Tele-Immersive Video: A Psychophysical Approach},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072302},
doi = {10.1145/2072298.2072302},
abstract = {This paper presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Detail peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just Noticeable Degradation and Just Unacceptable Degradation thresholds on the factor. In light of the results, we describe the design and implementation of a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage while considerably enhancing the overall perceived visual quality.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {13–22},
numpages = {10},
keywords = {perception, adaptation, psychophysics, level-of-detail, color-plus-depth, tele-immersive video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072303,
author = {Garg, Ravi and Varna, Avinash L. and Wu, Min},
title = {"Seeing" ENF: Natural Time Stamp for Digital Video via Optical Sensing and Signal Processing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072303},
doi = {10.1145/2072298.2072303},
abstract = {Electric Network Frequency (ENF) fluctuates slightly over time from its nominal value of 50 Hz/60 Hz. The fluctuations in the ENF remain consistent across the entire power grid even when measured at physically distant locations. The near-invisible flickering of fluorescent lights connected to the power mains reflect these fluctuations present in the ENF. In this paper, mechanisms using optical sensors and video cameras to record and validate the presence of the ENF fluctuations in fluorescent lighting are presented. Signal processing techniques are applied to demonstrate a high correlation between the fluctuations in the ENF signal captured from fluorescent lighting and the ENF signal captured directly from power mains supply. The proposed technique is then used to demonstrate the presence of the ENF signal in video recordings taken in various geographical areas. Experimental results show that the ENF signal can be used as a natural timestamp for optical sensor recordings and video surveillance recordings from indoor environments under fluorescent lighting. Application of the ENF signal analysis to tampering detection of surveillance video recordings is also demonstrated.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {23–32},
numpages = {10},
keywords = {electric network frequency, video authentication, information forensics, timestamp},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072305,
author = {Subramanian, Ramanathan and Yanulevskaya, Victoria and Sebe, Nicu},
title = {Can Computers Learn from Humans to See Better? Inferring Scene Semantics from Viewers' Eye Movements},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072305},
doi = {10.1145/2072298.2072305},
abstract = {This paper describes an attempt to bridge the semantic gap between computer vision and scene understanding employing eye movements. Even as computer vision algorithms can efficiently detect scene objects, discovering semantic relationships between these objects is as essential for scene understanding. Humans understand complex scenes by rapidly moving their eyes (saccades) to selectively focus on salient entities (fixations). For 110 social scenes, we compared verbal descriptions provided by observers against eye movements recorded during a free-viewing task. Data analysis confirms (i) a strong correlation between task-explicit linguistic descriptions and task-implicit eye movements, both of which are influenced by underlying scene semantics and (ii) the ability of eye movements in the form of fixations and saccades to indicate salient entities and entity relationships mentioned in scene descriptions.We demonstrate how eye movements are useful for inferring the meaning of social (everyday scenes depicting human activities) and affective (emotion-evoking content like expressive faces, nudes) scenes. While saliency has always been studied through the prism of fixations, we show that saccades are particularly useful for (i) distinguishing mild and high-intensity facial expressions and (ii) discovering interactive actions between scene entities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {33–42},
numpages = {10},
keywords = {scene semantics, salient entities and interactions, eye movements, fixations and saccades},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072306,
author = {Carlier, Axel and Ravindra, Guntur and Charvillat, Vincent and Ooi, Wei Tsang},
title = {Combining Content-Based Analysis and Crowdsourcing to Improve User Interaction with Zoomable Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072306},
doi = {10.1145/2072298.2072306},
abstract = {This paper introduces a new paradigm for interacting with zoomable video. Our interaction technique reduces the number of zooms and pans required by providing recommended viewports to the users, and replaces multiple zoom and pan actions with a simple click on the recommended viewport. The usefulness of our technique is visible in the quality of the recommended viewport, which needs to match the user intention, track movement in the scene, and properly frame the scene in the video. To this end, we propose a hybrid method where content analysis is complimented by the implicit feedback of a community of users in order to recommend viewports. We first compute preliminary sets of recommended viewports by analyzing the content of the video. These viewports allow tracking of moving objects in the scene, and are framed without violating basic aesthetic rules. To improve the relevance of the recommended viewports, we collect viewing statistics as users view a video, and use the viewports they select to reinforce the importance of certain recommendations and penalize others. New recommendations that are not previously recognized by content analysis may also emerge. The resulting recommended viewports converge towards the regions in the video that are relevant to users. A user study involving 70 participants shows that an user interface incorporating with our paradigm leads to more number of zooms, into more informative regions with fewer interactions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {43–52},
numpages = {10},
keywords = {interaction techniques, content-analysis, zoomable video, crowdsourcing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072307,
author = {Xie, Lexing and Natsev, Apostol and Kender, John R. and Hill, Matthew and Smith, John R.},
title = {Visual Memes in Social Media: Tracking Real-World News in YouTube Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072307},
doi = {10.1145/2072298.2072307},
abstract = {We propose visual memes, or frequently reposted short video segments, for tracking large-scale video remix in social media. Visual memes are extracted by novel and highly scalable detection algorithms that we develop, with over 96% precision and 80% recall. We monitor real-world events on YouTube, and we model interactions using a graph model over memes, with people and content as nodes, and meme postings as links. This allows us to define several measures of influence. These abstractions, using more than two million video shots from several large-scale event datasets, enable us to quantify and efficiently extract several important observations: over half of the videos contain re-mixed content, which appears rapidly; video view counts, particularly high ones, are poorly correlated with the virality of content; the influence of traditional news media versus citizen journalists varies from event to event; iconic single images of an event are easily extracted; and content that will have long lifespan can be predicted within a day after it first appears. Visual memes can be applied to a number of social media scenarios: brand monitoring, social buzz tracking, ranking content and users, among others.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {53–62},
numpages = {10},
keywords = {video analysis, social media, youtube, memes},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072308,
author = {Geng, Bo and Yang, Linjun and Xu, Chao and Hua, Xian-Sheng and Li, Shipeng},
title = {The Role of Attractiveness in Web Image Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072308},
doi = {10.1145/2072298.2072308},
abstract = {Existing web image search engines are mainly designed to optimize topical relevance. However, according to our user study, attractiveness is becoming a more and more important factor for web image search engines to satisfy users' search intentions. Important as it can be, web image attractiveness from the search users' perspective has not been sufficiently recognized in both the industry and the academia. In this paper, we present a definition of web image attractiveness with three levels according to the end users' feedback, including perceptual quality, aesthetic sensitivity and affective tune. Corresponding to each level of the definition, various visual features are investigated on their applicability to attractiveness estimation of web images. To further deal with the unreliability of visual features induced by the large variations of web images, we propose a contextual approach to integrate the visual features with contextual cues mined from image EXIF information and the associated web pages. We explore the role of attractiveness by applying it to various stages of a web image search engine, including the online ranking and the interactive reranking, as well as the offline index selection. Experimental results on three large-scale web image search datasets demonstrate that the incorporation of attractiveness can bring more satisfaction to 80% of the users for ranking/reranking search results and 30.5% index coverage improvement for index selection, compared to the conventional relevance based approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {63–72},
numpages = {10},
keywords = {image attractiveness, web image search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072310,
author = {Wang, Yang and Mei, Tao and Wang, Jingdong and Li, Houqiang and Li, Shipeng},
title = {JIGSAW: Interactive Mobile Visual Search with Multimodal Queries},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072310},
doi = {10.1145/2072298.2072310},
abstract = {The traditional text-based visual search has not been sufficiently improved over the years to accommodate the new emerging demand of mobile users. While on the go, searching on one's phone is becoming pervasive. This paper presents an innovative application for mobile phone users to facilitate their visual search experience. By taking advantage of smart phone functionalities such as multi-modal and multi-touch interactions, users can more conveniently formulate their search intent, and thus search performance can be significantly improved. The system, called JIGSAW (Joint search with ImaGe, Speech, And Words), represents one of the first attempts to create an interactive and multi-modal mobile visual search application. The key of JIGSAW is the composition of an exemplary image query generated from the raw speech via multi-touch user interaction, as well as the visual search based on the exemplary image. Through JIGSAW, users can formulate their search intent in a natural way like playing a jigsaw puzzle on the phone screen: 1) a user speaks a natural sentence as the query, 2) the speech is recognized and transferred to text which is further decomposed to keywords through entity extraction, 3) the user selects preferred exemplary images that can visually represent his/her intent and composes a query image via multi-touch, and 4) the composite image is then used as a visual query to search similar images. We have deployed JIGSAW on a real-world phone system, evaluated the performance on one million images, and demonstrated that it is an effective complement to existing mobile visual search applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {73–82},
numpages = {10},
keywords = {mobile visual search, query formulation, user interface},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072311,
author = {Cheng, An-Jung and Chen, Yan-Ying and Huang, Yen-Ta and Hsu, Winston H. and Liao, Hong-Yuan Mark},
title = {Personalized Travel Recommendation by Mining People Attributes from Community-Contributed Photos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072311},
doi = {10.1145/2072298.2072311},
abstract = {Leveraging community-contributed data (e.g., blogs, GPS logs, and geo-tagged photos) for travel recommendation is one of the active researches since there are rich contexts and trip activities in such explosively growing data. In this work, we focus on personalized travel recommendation by leveraging the freely available community-contributed photos. We propose to conduct personalized travel recommendation by further considering specific user profiles or attributes (e.g., gender, age, race). In stead of mining photo logs only, we argue to leverage the automatically detected people attributes in the photo contents. By information-theoretic measures, we will demonstrate that such people attributes are informative and effective for travel recommendation -- especially providing a promising aspect for personalization. We effectively mine the demographics for different locations (or landmarks) and travel paths. A probabilistic Bayesian learning framework which further entails mobile recommendation on the spot is introduced. We experiment on four million photos collected for eight major worldwide cities. The experiments confirm that people attributes are promising and orthogonal to prior works using travel logs only and can further improve prior travel recommendation methods especially in difficult predictions by further leveraging user contexts in mobile devices.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {83–92},
numpages = {10},
keywords = {personalized tourist recommendation, route planning, geo-tagged photos},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072312,
author = {Shen, Zhijie and Arslan Ay, Sakire and Kim, Seon Ho and Zimmermann, Roger},
title = {Automatic Tag Generation and Ranking for Sensor-Rich Outdoor Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072312},
doi = {10.1145/2072298.2072312},
abstract = {Video tag annotations have become a useful and powerful feature to facilitate video search in many social media and web applications. The majority of tags assigned to videos are supplied by users - a task which is time consuming and may result in annotations that are subjective and lack precision. A number of studies have utilized content-based extraction techniques to automate tag generation. However, these methods are compute-intensive and challenging to apply across domains. Here, we describe a complementary approach for generating tags based on the geographic properties of videos. With today's sensor-equipped smartphones, the location and orientation of a camera can be continuously acquired in conjunction with the captured video stream. Our novel technique utilizes these sensor meta-data to automatically tag outdoor videos in a two step process. First, we model the viewable scenes of the video as geometric shapes by means of its accompanied sensor data and determine the geographic objects that are visible in the video by querying geo-information databases through the viewable scene descriptions. Subsequently we extract textual information about the visible objects to serve as tags. Second, we define six criteria to score the tag relevance and rank the obtained tags based on these scores. Then we associate the tags with the video and the accurately delimited segments of the video. To evaluate the proposed technique we implemented a prototype tag generator and conducted a user study. The results demonstrate significant benefits of our method in terms of automation and tag utility.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {93–102},
numpages = {10},
keywords = {video tags, mobile video, location sensors, geospatial},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072313,
author = {Shi, Shu and Hsu, Cheng-Hsin and Nahrstedt, Klara and Campbell, Roy},
title = {Using Graphics Rendering Contexts to Enhance the Real-Time Video Coding for Mobile Cloud Gaming},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072313},
doi = {10.1145/2072298.2072313},
abstract = {The emerging cloud gaming service has been growing rapidly, but not yet able to reach mobile customers due to many limitations, such as bandwidth and latency. We introduce a 3D image warping assisted real-time video coding method that can potentially meet all the requirements of mobile cloud gaming. The proposed video encoder selects a set of key frames in the video sequence, uses the 3D image warping algorithm to interpolate other non-key frames, and encodes the key frames and the residues frames with an H.264/AVC encoder. Our approach is novel in taking advantage of the run-time graphics rendering contexts (rendering viewpoint, pixel depth, camera motion, etc.) from the 3D game engine to enhance the performance of video encoding for the cloud gaming service. The experiments indicate that our proposed video encoder has the potential to beat the state-of-art x264 encoder in the scenario of real-time cloud gaming. For example, by implementing the proposed method in a 3D tank battle game, we experimentally show that more than 2 dB quality improvement is possible.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {103–112},
numpages = {10},
keywords = {cloud gaming, mobile devices, 3d image warping, real-time video coding},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072315,
author = {Zhuang, Jinfeng and Mei, Tao and Hoi, Steven C.H. and Hua, Xian-Sheng and Li, Shipeng},
title = {Modeling Social Strength in Social Media Community via Kernel-Based Learning},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072315},
doi = {10.1145/2072298.2072315},
abstract = {Modeling continuous social strength rather than conventional binary social ties in the social network can lead to a more precise and informative description of social relationship among people. In this paper, we study the problem of social strength modeling (SSM) for the users in a social media community, who are typically associated with diverse form of data. In particular, we take Flickr---the most popular online photo sharing community---as an example, in which users are sharing their experiences through substantial amounts of multimodal contents (e.g., photos, tags, geo-locations, friend lists) and social behaviors (e.g., commenting and joining interest groups). Such heterogeneous data in Flickr bring opportunities yet challenges to the research community for SSM. One of the key issues in SSM is how to effectively explore the heterogeneous data and how to optimally combine them to measure the social strength. In this paper, we present a kernel-based learning to rank framework for inferring the social strength of Flickr users, which involves two learning stages. The first stage employs a kernel target alignment algorithm to integrate the heterogeneous data into a holistic similarity space. With the learned kernel, the second stage rectifies the pair-wise learning to rank approach to estimating the social strength. By learning the social strength graph, we are able to conduct collaborative recommendation and collective classification. The promising results show that the learning-based approach is effective for SSM. Despite being focused on Flickr, our technique can be applied to model social strength of users in any other social media community.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {113–122},
numpages = {10},
keywords = {social networks, learning to rank, kernel-based learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072316,
author = {Jiang, Wei and Loui, Alexander C.},
title = {Audio-Visual Grouplet: Temporal Audio-Visual Interactions for General Video Concept Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072316},
doi = {10.1145/2072298.2072316},
abstract = {We investigate general concept classification in unconstrained videos by joint audio-visual analysis. A novel representation, the Audio-Visual Grouplet (AVG), is extracted by studying the statistical temporal audio-visual interactions. An AVG is defined as a set of audio and visual codewords that are grouped together according to their strong temporal correlations in videos. The AVGs carry unique audio-visual cues to represent the video content, based on which an audio-visual dictionary can be constructed for concept classification. By using the entire AVGs as building elements, the audio-visual dictionary is much more robust than traditional vocabularies that use discrete audio or visual codewords. Specifically, we conduct coarse-level foreground/background separation in both audio and visual channels, and discover four types of AVGs by exploring mixed-and-matched temporal audio-visual correlations among the following factors: visual foreground, visual background, audio foreground, and audio background. All of these types of AVGs provide discriminative audio-visual patterns for classifying various semantic concepts. We extensively evaluate our method over the large-scale Columbia Consumer Video set. Experiments demonstrate that the AVG-based dictionaries can achieve consistent and significant performance improvements compared with other state-of-the-art approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {123–132},
numpages = {10},
keywords = {video concept classification, audio-visual grouplet, temporal audio-visual interaction, foreground/background separation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072317,
author = {Li, Zechao and Wang, Meng and Liu, Jing and Xu, Changsheng and Lu, Hanqing},
title = {News Contextualization with Geographic and Visual Information},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072317},
doi = {10.1145/2072298.2072317},
abstract = {In this paper, we investigate the contextualization of news documents with geographic and visual information. We propose a matrix factorization approach to analyze the location relevance for each news document. We also propose a method to enrich the document with a set of web images. For location relevance analysis, we first perform toponym extraction and expansion to obtain a toponym list from news documents. We then propose a matrix factorization method to estimate the location-document relevance scores while simultaneously capturing the correlation of locations and documents. For image enrichment, we propose a method to generate multiple queries from each news document for image search and then employ an intelligent fusion approach to collect a set of images from the search results. Based on the location relevance analysis and image enrichment, we introduce a news browsing system named NewsMap which can support users in reading news via browsing a map and retrieving news with location queries. The news documents with the corresponding enriched images are presented to help users quickly get information. Extensive experiments demonstrate the effectiveness of our approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {133–142},
numpages = {10},
keywords = {news location relevance, newsmap, image enrichment},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072318,
author = {Fu, Zhenyong and Ip, Horace H.S. and Lu, Hongtao and Lu, Zhiwu},
title = {Multi-Modal Constraint Propagation for Heterogeneous Image Clustering},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072318},
doi = {10.1145/2072298.2072318},
abstract = {This paper presents a multi-modal constraint propagation approach to exploiting pairwise constraints for constrained clustering tasks on multi-modal datasets. Pairwise constraint propagation methods have previously been designed primarily for single modality data and cannot be directly applied to multi-modal data or a dataset with multiple representations. In this paper, we provide an effective solution to the multi-modal constraint propagation problem by decomposing it into a set of independent multi-graph based two-class label propagation subproblems which are then merged into a unified problem and solved by quadratic optimization. We also show that such a formulation yields a closed-form solution. Our approach allows the initial pairwise constraints to be propagated throughout the entire multi-modal dataset. The propagated constraints are further used to refine the similarities between the objects for subsequent clustering tasks. The proposed method has been tested in constrained clustering tasks on two real-life multi-modal image datasets and shown to achieve significant improvements with respect to the single modality methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {143–152},
numpages = {10},
keywords = {pairwise constraint propagation, multi-modal analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072320,
author = {Mansilla, Wendy Ann and Perkis, Andrew and Ebrahimi, Touradj},
title = {Implicit Experiences as a Determinant of Perceptual Quality and Aesthetic Appreciation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072320},
doi = {10.1145/2072298.2072320},
abstract = {Since the Dadaist refusal of the conventional standards in art, followed by Fluxus` rejection of art as a commodity, and recently, the popularity of Internet and technology in art, artworks have become difficult to recognize as artworks in themselves. Modern works of art are no longer readily only seen today, more often fully experienced. The processing of an aesthetic experience needs a new understanding in terms of the changing context of art and the experiential perspective of art recipients. In the multimedia arena, the valid assumption is that, evaluations of aesthetic experiences are mostly based on the accessible information on the surface of the medium. Several research groups in psychology question the singularity of exterior-level assumptions demonstrating that there are modulating factors that affect aesthetic experiences and one of these is implicit experience. In this paper, we review the significance of empirical aesthetics in psychology and from the artistic point of view combined with a technical and experiential perspective. We also discuss our approach of considering the implications of implicit experiences to the modelling of Quality of Experience (QoE) where this is used as a measurement of perception and aesthetic judgment of contemporary and modern works of art.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {153–162},
numpages = {10},
keywords = {implicit experiences, empirical aesthetic, aesthetics of digital error, aesthetic experience, art, QoE},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072321,
author = {Roman-Rangel, Edgar and Pallan Gayol, Carlos and Odobez, Jean-Marc and Gatica-Perez, Daniel},
title = {Searching the Past: An Improved Shape Descriptor to Retrieve Maya Hieroglyphs},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072321},
doi = {10.1145/2072298.2072321},
abstract = {Archaeologists often spend significant time looking at traditional printed catalogs to identify and classify historical images. Our collaborative efforts between archaeologists and multimedia researchers seek to develop a tool to retrieve two specific types of ancient Maya visual information: hieroglyphs and iconographic elements. Towards that goal we present two contributions in this paper. The first one is the introduction and analysis of a new dataset of 3400+ Maya hieroglyphs, whose compilation involved manual search, annotation and segmentation by experts. This dataset presents several challenges for visual description and automatic retrieval as it is rich in complex visual details. The second and main contribution is the in-depth analysis of the Histogram Of Orientation Shape Context (HOOSC), and more precisely, the development of 4 improvements that were designed to handle the visual complexity of Maya hieroglyphs: open contours, mixture of thick and thin lines, hatches, large instance variability, and a variety of internal details. Experiments demonstrate that the adequate combination of our improvements to retrieve Maya hieroglyphs, provides results with roughly 20% more precision compared to the original HOOSC descriptor. Complementary results with the MPEG-7 shape dataset validate (or not) the proposed improvements, showing that the design of appropriate descriptors depends on the nature of the shapes one deals with.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {163–172},
numpages = {10},
keywords = {hieroglyphs, shape, archaeology, maya, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072322,
author = {Jacquemin, Christian and Gagner\'{e}, Georges and Lahoz, Beno\^{\i}t},
title = {Shedding Light on Shadow: Real-Time Interactive Artworks Based on Cast Shadows or Silhouettes},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072322},
doi = {10.1145/2072298.2072322},
abstract = {Digital shadowing is a source of interest in immersive and interactive artworks because it enhances the feeling of presence, and because it is a very intuitive and engaging interaction "device." After a presentation of some major pieces in shadow-based artwork along two categories (silhouette-based digital shadows and cast shadow-based digital shadows), we propose a real-time software platform that can produce these two types of shadows from video capture and image processing. The processing is divided into image analysis for shadow extraction, calibration, and special effects. It is made highly flexible and parameterized, so that it can fit various configurations. Two specific configurations are illustrated because of their genericity: one for infrared light silhouette-based shadow and live performance, and one for both digital shadowing categories and interactive art installations with visible light. We then present some of the applications of such a platform through various live performances made by two theater companies involved in a collaborative project with scientists. The examples confirm the wide variety of scenographic setups in which shadow can be involved as a key component.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {173–182},
numpages = {10},
keywords = {digital shadow, video-scenography, digital art, real-time image processing, augmented reality, interactive art},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072324,
author = {Zhu, Minhui and Mondet, Sebastien and Morin, G\'{e}raldine and Ooi, Wei Tsang and Cheng, Wei},
title = {Towards Peer-Assisted Rendering in Networked Virtual Environments},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072324},
doi = {10.1145/2072298.2072324},
abstract = {This paper introduces a new technique, called peer-assisted rendering, that aims to enable interactive navigation in a 3D networked virtual environment using a resource-constrained device, by speeding up the rendering. A resource-constrained client requests part of the rendered scenes from other peers with similar viewpoints within the virtual environment, and merges the rendered parts into its own view. This approach is more scalable than previous solutions based on server-based pre-rendering. The goal of this paper is to make a strong case for the feasibility of peer-assisted rendering through the following two messages. First, by analyzing a large number of user traces from a popular virtual world called Second Life, we show that there are surprisingly many users with similar viewpoints and encompass large number of common objects in their viewing areas, indicating that a client can potentially find multiple other peers that can assist in rendering. Second, by combining three different rendering methods, each contributing to rendering of different classes of objects in the scene, we show that it is possible for a client to render the scene efficiently with little visual artifacts.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {183–192},
numpages = {10},
keywords = {peer-assisted rendering, image-based rendering, networked virtual environments, resource-constrained devices},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072325,
author = {Liu, Yao and Li, Fei and Guo, Lei and Guo, Yang and Chen, Songqing},
title = {BlueStreaming: Towards Power-Efficient Internet P2P Streaming to Mobile Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072325},
doi = {10.1145/2072298.2072325},
abstract = {P2P streaming applications are very popular on the Internet today. However, a mobile device in P2P streaming not only needs to continuously receive streaming data from other peers for its playback, but also needs to continuously exchange control information (e.g., buffermaps and file chunk requests) with neighboring peers and upload the downloaded streaming data to them. These lead to excessive battery power consumption on the mobile device.In this paper, we first conduct Internet experiments to study in-depth the impact of control traffic and uploading traffic on battery power consumption with several popular Internet P2P streaming applications. Motivated by measurement results, we design and implement a system called BlueStreaming that effectively utilizes the commonly existing Bluetooth interface on mobile devices. Instead of activating WiFi and Bluetooth interfaces alternatively, BlueStreaming keeps Bluetooth active all the time to transmit delay-sensitive control traffic while using WiFi for streaming data traffic. BlueStreaming trades Bluetooth's power consumption for much more significant energy saving from shaped WiFi traffic. To evaluate the performance of BlueStreaming, we have implemented prototypes on both Windows and Mac to access existing popular Internet P2P streaming services. The experimental results show that BlueStreaming can save up to 46% battery power compared to the commodity PSM scheme.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {193–202},
numpages = {10},
keywords = {P2P, power saving, bluetooth, internet mobile streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072326,
author = {Liang, Ke and Zimmermann, Roger and Ooi, Wei Tsang},
title = {Peer-Assisted Texture Streaming in Metaverses},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072326},
doi = {10.1145/2072298.2072326},
abstract = {User-extensible metaverses need an effective way to disseminate massive and dynamic 3D contents (i.e., meshes, textures, animations, etc.) to end users, and at the same time maintain a low consumption of server bandwidth. Peer-to-peer (or peer-assisted) technologies have been widely considered as a desirable complementary solution to efficaciously offload servers in large-scale media streaming applications. However, due to both the bandwidth constraints of heterogeneous users and unpredictable access patterns of latency-sensitive 3D contents, it is very challenging to cut the server bandwidth cost in metaverses. In this paper, we propose a peer-assisted texture streaming architecture to minimize the server bandwidth consumption without degrading the end-user satisfaction. We propose a game-theoretic peer selection strategy which achieves a good trade-off between performance and complexity. Our algorithm is light-weight, and can efficiently utilize the bandwidth of users in a fully decentralized manner by enabling each peer (i.e., user) to quickly select its content providers who can satisfy the requests of the peer within the latency constraint of the content. We evaluate our algorithm through an extensive comparison study based on simulations using realistic data (i.e., avatar mobility traces and textures) collected from Second Life. The simulation results show that the proposed algorithm can effectively reduce the server bandwidth consumption without degrading the user experience.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {203–212},
numpages = {10},
keywords = {game-theoretic algorithm, peer-assisted streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072327,
author = {Huang, Yan and Li, Zhenhua and Liu, Gang and Dai, Yafei},
title = {Cloud Download: Using Cloud Utilities to Achieve High-Quality Content Distribution for Unpopular Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072327},
doi = {10.1145/2072298.2072327},
abstract = {Video content distribution dominates the Internet traffic. The state-of-the-art techniques generally work well in distributing popular videos, but do not provide satisfactory content distribution service for unpopular videos due to low data health or low data transfer rate. In recent years, the worldwide deployment of cloud utilities provides us with a novel perspective to consider the above problem. We propose and implement the cloud download scheme, which achieves high-quality video content distribution by using cloud utilities to guarantee the data health and enhance the data transfer rate. Specifically, a user sends his video request to the cloud which subsequently downloads the video from the Internet and stores it in the cloud cache. Then the user can usually retrieve his requested video (whether popular or unpopular) from the cloud with high data rate in any place at any time, via the intra-cloud data transfer acceleration. Running logs of our real deployed commercial system (named VideoCloud) confirm the effectiveness and efficiency of cloud download. The users' average data transfer rate of unpopular videos exceeds 1.6 Mbps, and over 80% of the data transfer rates are more than 300 Kbps which is the basic playback rate of online videos. Our study provides practical experiences and valuable heuristics for making use of cloud utilities to achieve efficient Internet services.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {213–222},
numpages = {10},
keywords = {data transfer rate., unpopular videos, data health, content distribution, cloud download},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072329,
author = {Piacenza, Alberto and Guerrini, Fabrizio and Adami, Nicola and Leonardi, Riccardo and Porteous, Julie and Teutenberg, Jonathan and Cavazza, Marc},
title = {Generating Story Variants with Constrained Video Recombination},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072329},
doi = {10.1145/2072298.2072329},
abstract = {We present a novel approach to the automatic generation of filmic variants within an implemented Video-Based Storytelling (VBS) system that successfully integrates video segmentation with stochastically controlled re-ordering techniques and narrative generation via AI planning. We have introduced flexibility into the video recombination process by sequencing video shots in a way that maintains local video consistency and this is combined with exploitation of shot polysemy to enable shot reuse in a range of valid semantic contexts. Results of evaluations on output narratives using a shared set of video data show consistency in terms of local video sequences and global causality with no loss of generative power.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {223–232},
numpages = {10},
keywords = {logical story unit, interactive storytelling, Markov chains, narrative modeling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072330,
author = {Li, Xirong and Gavves, Efstratios and Snoek, Cees G.M. and Worring, Marcel and Smeulders, Arnold W.M.},
title = {Personalizing Automated Image Annotation Using Cross-Entropy},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072330},
doi = {10.1145/2072298.2072330},
abstract = {Annotating the increasing amounts of user-contributed images in a personalized manner is in great demand. However, this demand is largely ignored by the mainstream of automated image annotation research. In this paper we aim for personalizing automated image annotation by jointly exploiting personalized tag statistics and content-based image annotation. We propose a cross-entropy based learning algorithm which personalizes a generic annotation model by learning from a user's multimedia tagging history. Using cross-entropy-minimization based Monte Carlo sampling, the proposed algorithm optimizes the personalization process in terms of a performance measurement which can be flexibly chosen. Automatic image annotation experiments with 5,315 realistic users in the social web show that the proposed method compares favorably to a generic image annotation method and a method using personalized tag statistics only. For 4,442 users the performance improves, where for 1,088 users the absolute performance gain is at least 0.05 in terms of average precision. The results show the value of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {233–242},
numpages = {10},
keywords = {cross-entropy optimization, personalization, automated image annotation, personal multimedia tagging history},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072331,
author = {Tan, Song and Ngo, Chong-Wah and Tan, Hung-Khoon and Pang, Lei},
title = {Cross Media Hyperlinking for Search Topic Browsing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072331},
doi = {10.1145/2072298.2072331},
abstract = {With the rapid growth of social media, there are plenty of information sources freely available online for use. Nevertheless, how to synchronize and leverage these diverse forms of information for multimedia applications remains a problem yet to be seriously studied. This paper investigates the synchronization of multiple media content in the physical form of hyperlinking them. The ultimate goal is to develop browsing systems that author search results with rich media information mined from various knowledge sources. The authoring enables the vivid visualization and exploration of different information landscapes inherent in search results. Several key techniques are studied in this paper for developing these browsing features. These techniques include content mining and selection from web videos, space-time alignment of multiple media, and augmenting of search result with when and what information. We conduct both quantitative and user studies on a large video dataset for performance evaluation. Comparison with traditional techniques including storyboard summarization and video skimming are also presented.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {243–252},
numpages = {10},
keywords = {media content synchronization, video browsing, visual summarization, event extraction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072332,
author = {Liu, Chun-Wei and Huang, Tz-Huan and Chang, Ming-Hsu and Lee, Ken-Yi and Liang, Chia-Kai and Chuang, Yung-Yu},
title = {3D Cinematography Principles and Their Applications to Stereoscopic Media Processing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072332},
doi = {10.1145/2072298.2072332},
abstract = {This paper introduces 3D cinematography principles to the field of multimedia and illustrates their usage in stereoscopic media processing applications. These principles include (1) maintaining coordination among views, (2) having a continuous depth chart, (3) placing rest areas between strong 3D shots, (4) using a shallow depth of field for shots with excessive depth brackets, and (5) being careful about the stereoscopic window. Taking these principles into account, we propose designs for stereoscopic extensions of two popular 2D media applications---video stabilization and photo slideshow---to provide a better 3D viewing experience. User studies show that by incorporating 3D cinematography principles, the proposed methods yield more comfortable and enjoyable 3D viewing experiences than those delivered using naive extensions of conventional 2D methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {253–262},
numpages = {10},
keywords = {stereoscopy, stereoscopic media, 3D cinematography principles, photo slideshow, video stabilization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072334,
author = {Chen, Xiangyu and Yuan, Xiaotong and Yan, Shuicheng and Tang, Jinhui and Rui, Yong and Chua, Tat-Seng},
title = {Towards Multi-Semantic Image Annotation with Graph Regularized Exclusive Group Lasso},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072334},
doi = {10.1145/2072298.2072334},
abstract = {To bridge the semantic gap between low level feature and human perception, most of the existing algorithms aim mainly at annotating images with concepts coming from only one semantic space, e.g. cognitive or affective. The naive combination of the outputs from these spaces will implicitly force the conditional independence and ignore the correlations among the spaces. In this paper, to exploit the comprehensive semantic of images, we propose a general framework for harmoniously integrating the above multiple semantics, and investigating the problem of learning to annotate images with training images labeled in two or more correlated semantic spaces, such as fascinating nighttime, or exciting cat. This kind of semantic annotation is more oriented to real world search scenario. Our proposed approach outperforms the baseline algorithms by making the following contributions. 1) Unlike previous methods that annotate images within only one semantic space, our proposed multi-semantic annotation associates each image with labels from multiple semantic spaces. 2) We develop a multi-task linear discriminative model to learn a linear mapping from features to labels. The tasks are correlated by imposing the exclusive group lasso regularization for competitive feature selection, and the graph Laplacian regularization to deal with insufficient training sample issue. 3) A Nesterov-type smoothing approximation algorithm is presented for efficient optimization of our model. Extensive experiments on NUS-WIDEEmotive dataset (56k images) with 8\texttimes{}81 emotive cognitive concepts and Object&amp;Scene datasets from NUS-WIDE well validate the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {263–272},
numpages = {10},
keywords = {exclusive group lasso, multi-semantic image annotation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072335,
author = {Larson, Martha and Kofler, Christoph and Hanjalic, Alan},
title = {Reading between the Tags to Predict Real-World Size-Class for Visually Depicted Objects in Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072335},
doi = {10.1145/2072298.2072335},
abstract = {Multimedia information retrieval stands to benefit from the availability of additional information about tags and how they relate to the content visually depicted in images. We propose a generic approach that contributes to improving the informativeness of image tags by combining generalizations about the distributional tendencies of physical objects in the real world and statistics of natural language use patterns that have been mined from the Web. The approach, which we refer to as 'Reading between the Tags,' provides for each tag associated with an image, first, a prediction concerning corporeality, i.e., whether or not the tag denotes a physical entity, and, then, concerning the real-world size of that entity, i.e., large, medium or small. Mining takes place using a set of Language Use Frames (LUFs) that are composed of natural language neighborhoods characteristic of tag classes. We validate our approach with a series of experiments on a set of images from the MIRFLICKR data set using ground truth created with standard crowdsourcing techniques. The main experiments demonstrate the effectiveness of our approach for size-class prediction. A further experiment shows that size-class prediction can be improved and made image-specific using general and relatively small sets of visual concepts. A final experiment confirms that the set of LUFs can also be chosen automatically via statistical feature selection.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {273–282},
numpages = {10},
keywords = {crowdsourcing, real-world scale, lexico-syntactic patterns, size, selectional restrictions, image annotation, user-contributed tags},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072336,
author = {Ma, Zhigang and Yang, Yi and Nie, Feiping and Uijlings, Jasper and Sebe, Nicu},
title = {Exploiting the Entire Feature Space with Sparsity for Automatic Image Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072336},
doi = {10.1145/2072298.2072336},
abstract = {The explosive growth of digital images requires effective methods to manage these images. Among various existing methods, automatic image annotation has proved to be an important technique for image management tasks, e.g., image retrieval over large-scale image databases. Automatic image annotation has been widely studied during recent years and a considerable number of approaches have been proposed. However, the performance of these methods is yet to be satisfactory, thus demanding more effort on research of image annotation. In this paper, we propose a novel semi supervised framework built upon feature selection for automatic image annotation. Our method aims to jointly select the most relevant features from all the data points by using a sparsity-based model and exploiting both labeled and unlabeled data to learn the manifold structure. Our framework is able to simultaneously learn a robust classifier for image annotation by selecting the discriminating features related to the semantic concepts. To solve the objective function of our framework, we propose an efficient iterative algorithm. Extensive experiments are performed on different real-world image datasets with the results demonstrating the promising performance of our framework for automatic image annotation.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {283–292},
numpages = {10},
keywords = {image annotation, sparse feature selection, manifold learning, semi-supervised learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072337,
author = {Wang, Ju-Chiang and Shih, Yu-Chin and Wu, Meng-Sung and Wang, Hsin-Min and Jeng, Shyh-Kang},
title = {Colorizing Tags in Tag Cloud: A Novel Query-by-Tag Music Search System},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072337},
doi = {10.1145/2072298.2072337},
abstract = {This paper presents a novel content-based query-by-tag music search system for an untagged music database. We design a new tag query interface that allows users to input multiple tags with multiple levels of preference (denoted as an MTML query) by colorizing desired tags in a web-based tag cloud interface. When a user clicks and holds the left mouse button (or presses and holds his/her finger on a touch screen) on a desired tag, the color of the tag will change cyclically according to a color map (from dark blue to bright red), which represents the level of preference (from 0 to 1). In this way, the user can easily organize and check the query of multiple tags with multiple levels of preference through the colored tags. To effect the MTML content-based music retrieval, we introduce a probabilistic fusion model (denoted as GMFM), which consists of two mixture models, namely a Gaussian mixture model and a multinomial mixture model. GMFM can jointly model the auditory features and tag labels of a song. Two indexing methods and their corresponding matching methods, namely pseudo song-based matching and tag affinity-based matching, are incorporated into the pre-learned GMFM. We evaluate the proposed system on the MajorMiner and CAL-500 datasets. The experimental results demonstrate the effectiveness of GMFM and the potential of using MTML queries to search music from an untagged music database.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {293–302},
numpages = {10},
keywords = {probabilistic fusion model, MTML query, content-based music information retrieval, tag cloud-based music query interface},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072339,
author = {Laiola Guimar\~{a}es, Rodrigo and Cesar, Pablo and Bulterman, Dick C.A. and Zsombori, Vilmos and Kegel, Ian},
title = {Creating Personalized Memories from Social Events: Community-Based Support for Multi-Camera Recordings of School Concerts},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072339},
doi = {10.1145/2072298.2072339},
abstract = {The wide availability of relatively high-quality cameras makes it easy for many users to capture video fragments of social events such as concerts, sports events or community gatherings. The wide availability of simple sharing tools makes it nearly as easy to upload individual fragments to on-line video sites. Current work on video mashups focuses on the creation of a video summary based on the characteristics of individual media fragments, but it fails to address the interpersonal relationships and time-variant social context within a community of diverse (but related) users. The aim of this paper is to reformulate the research problem of video authoring, by investigating the social relationships of the media 'authors' relative to the performers. Based on a 10-month evaluation process, we specify a set of guidelines for the design and implementation of socially-aware video editing and sharing tools. Our contributions have been realized and evaluated in a prototype software that enables community-based users to navigate through a large common content space and to generate highly personalized video compilations of targeted interest within a social circle. According to the results, a system like ours is a valid alternative for social interactions when apart. We hope that our insights can stimulate future research on socially-aware multimedia tools and applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {303–312},
numpages = {10},
keywords = {privacy, web-mediated communication, video sharing and information management, video authoring},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072340,
author = {Nack, Frank and Ide, Ichiro},
title = {Why Did the Prime Minister Resign? Generation of Event Explanations from Large News Repositories},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072340},
doi = {10.1145/2072298.2072340},
abstract = {One of the common parts of news is to provide the background for a current event, such as the resignation of a Prime Minister. This paper addresses a framework that facilitates semi-automated authoring of explanatory audio-visual news topics in a retrospective style for the domain of politics based on already edited new stories available in the repository of the news corporation. The aim is to facilitate a journalist with an audio-visual body based on which he/she can finalize the explanatory piece. The proposed framework enhances current state of the art video summarization by allowing the combination of different news stories into one coherent explanation about a topic of the current news. The framework introduces techniques that exploit demoscopic data in form of polls for the development of the general story outline; the automatic retrieval of relevant material by using a combination of event templates and automatic news summarization over topic threads; and the generation of the final video by applying a set of trimming rules. Example generations are presented and discussed and an outline of future work is presented.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {313–322},
numpages = {10},
keywords = {automated video editing, video archive, topic, template, topic thread, story generation, event, multimedia authoring},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072341,
author = {Lino, Christophe and Christie, Marc and Ranon, Roberto and Bares, William},
title = {The Director's Lens: An Intelligent Assistant for Virtual Cinematography},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072341},
doi = {10.1145/2072298.2072341},
abstract = {We present the Director's Lens, an intelligent interactive assistant for crafting virtual cinematography using a motion-tracked hand-held device that can be aimed like a real camera. The system employs an intelligent cinematography engine that can compute, at the request of the filmmaker, a set of suitable camera placements for starting a shot. These suggestions represent semantically and cinematically distinct choices for visualizing the current narrative. In computing suggestions, the system considers established cinema conventions of continuity and composition along with the filmmaker's previous selected suggestions, and also his or her manually crafted camera compositions, by a machine learning component that adapts shot editing preferences from user-created camera edits. The result is a novel workflow based on interactive collaboration of human creativity with automated intelligence that enables efficient exploration of a wide range of cinematographic possibilities, and rapid production of computer-generated animated movies.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {323–332},
numpages = {10},
keywords = {virtual camera planning, virtual cinematography, motion-tracked virtual cameras},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072342,
author = {Singh, Vivek K. and Luo, Jiebo and Joshi, Dhiraj and Lei, Phoury and Das, Madirakshi and Stubler, Peter},
title = {Reliving on Demand: A Total Viewer Experience},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072342},
doi = {10.1145/2072298.2072342},
abstract = {Billions of people worldwide use images and videos to capture various events in their lives. The primary purpose of the proposed media sharing application is digital re-living of those events by the photographers and their families and friends. The most popular tools for achieving this today are still static slide-shows (SSS) which primarily focus on visual effects rather than understanding the semantics of the media assets being used, or allowing different viewers (e.g. friends, family, who have different relationships, interests, time availabilities, and familiarities) any control over the flow of the show. We present a novel system that generates an aesthetically appealing and semantically drivable audio-visual media show based on several reliving dimensions of events, people, locations, and time. We allow each viewer to interact with the default presentation to 'on-the-fly' redirect the flow of reliving as desired from their individual perspectives. Moreover, each reliving session is logged and can be shared with other people over a wide array of platforms and devices, allowing sharing experience to go beyond the sharing of the media assets themselves. From a detailed analysis of the logged sessions across different user categories, we have obtained many interesting findings on the reliving needs, behaviors and patterns, which in turn validate our design motivations and principles.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {333–342},
numpages = {10},
keywords = {interaction, slide-show, sharing, reliving, spatio-temporal navigation, semantic},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072344,
author = {Zhong, Sheng-hua and Liu, Yan and Liu, Yang},
title = {Bilinear Deep Learning for Image Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072344},
doi = {10.1145/2072298.2072344},
abstract = {Image classification is a well-known classical problem in multimedia content analysis. This paper proposes a novel deep learning model called bilinear deep belief network (BDBN) for image classification. Unlike previous image classification models, BDBN aims to provide human-like judgment by referencing the architecture of the human visual system and the procedure of intelligent perception. Therefore, the multi-layer structure of the cortex and the propagation of information in the visual areas of the brain are realized faithfully. Unlike most existing deep models, BDBN utilizes a bilinear discriminant strategy to simulate the "initial guess" in human object recognition, and at the same time to avoid falling into a bad local optimum. To preserve the natural tensor structure of the image data, a novel deep architecture with greedy layer-wise reconstruction and global fine-tuning is proposed. To adapt real-world image classification tasks, we develop BDBN under a semi-supervised learning framework, which makes the deep model work well when labeled images are insufficient. Comparative experiments on three standard datasets show that the proposed algorithm outperforms both representative classification models and existing deep learning techniques. More interestingly, our demonstrations show that the proposed BDBN works consistently with the visual perception of humans.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {343–352},
numpages = {10},
keywords = {image classification, deep learning, bilinear discriminant projection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072345,
author = {Wang, Dayong and Hoi, Steven C.H. and He, Ying and Zhu, Jianke},
title = {Retrieval-Based Face Annotation by Weak Label Regularized Local Coordinate Coding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072345},
doi = {10.1145/2072298.2072345},
abstract = {This paper investigates a retrieval-based annotation paradigm of mining web facial images for automated face annotation. In general, there are two key challenges for such an annotation paradigm. The first challenge is how to efficiently retrieve a short list of most similar facial images from facial image databases, and the second challenge is how to effectively perform annotation by exploiting these similar facial images and their weak labels which are often noisy and incomplete. In this paper, we mainly focus on tackling the second challenge of the retrieval-based face annotation paradigm. In particular, we propose an effective Weak Label Regularized Local Coordinate Coding (WLRLCC) technique, which exploits the local coordinate coding principle in learning sparse features, and at the same time employs the graph-based weak label regularization principle to enhance the weak labels of the short list of similar facial images. We present an efficient optimization algorithm to solve the WLRLCC task, and develop an effective sparse reconstruction scheme to perform the final face name annotation. We conduct a set of extensive empirical studies on a large-scale facial image database withatotal of 6,000 persons and over 600,000 web facial images, in which encouraging results show that the proposed WLRLCC algorithm significantly boosts the performance of the regular retrieval-based face annotation approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {353–362},
numpages = {10},
keywords = {auto face annotation, web facial images, unsupervised learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072346,
author = {Tian, Xinmei and Lu, Yijuan and Yang, Linjun and Tian, Qi},
title = {Learning to Judge Image Search Results},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072346},
doi = {10.1145/2072298.2072346},
abstract = {Given the explosive growth of the Web and the popularity of image sharing Web sites, image retrieval plays an increasingly important role in our daily lives. Search engines aim to provide beneficial image search results to users in response to queries. The quality of image search results depends on many factors: chosen search algorithms, ranking functions, indexing features, the base image database, etc. Applying different settings for these factors generates search result lists with varying levels of quality. Previous research has shown that no setting can always perform optimally for all queries. Therefore, given a set of search result lists generated by different settings, it is crucial to automatically determine which result list is the best in order to present it to users. This paper proposes a novel method to automatically identify the best search result list from a number of candidates. There are three main innovations in this paper. First, we propose a preference learning model to quantitatively study the best image search result identification problem. Second, we propose a set of valuable preference learning related features by exploring the visual characters of returned images. Third, our method shows promising potential in applications such as reranking ability assessment and optimal search engine selection. Experiments on two image search datasets show that our method achieves about 80% prediction accuracy for reranking ability assessment, and selects optimal search engine for about 70% queries correctly.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {363–372},
numpages = {10},
keywords = {search results performance comparison, image retrieval, reranking ability assessment},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072347,
author = {Le Meur, Olivier and Baccino, Thierry and Roumy, Aline},
title = {Prediction of the Inter-Observer Visual Congruency (IOVC) and Application to Image Ranking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072347},
doi = {10.1145/2072298.2072347},
abstract = {This paper proposes an automatic method for predicting the inter-observer visual congruency (IOVC). The IOVC reflects the congruence or the variability among different subjects looking at the same image. Predicting this congruence is of interest for image processing applications where the visual perception of a picture matters such as website design, advertisement, etc. This paper makes several new contributions. First, a computational model of the IOVC is proposed. This new model is a mixture of low-level visual features extracted from the input picture where model's parameters are learned by using a large eye-tracking database. Once the parameters have been learned, it can be used for any new picture. Second, regarding low-level visual feature extraction, we propose a new scheme to compute the depth of field of a picture. Finally, once the training and the feature extraction have been carried out, a score ranging from 0 (minimal congruency) to 1 (maximal congruency) is computed. A value of 1 indicates that observers would focus on the same locations and suggests that the picture presents strong locations of interest. A second database of eye movements is used to assess the performance of the proposed model. Results show that our IOVC criterion outperforms the Feature Congestion measure cite{Rosenholtz2007}. To illustrate the interest of the proposed model, we have used it to automatically rank personalized photograph.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {373–382},
numpages = {10},
keywords = {visual dispersion, eye tracking, congruency, images ranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072349,
author = {Quynh, Dao T.P. and He, Ying and Chen, Xiaoming and Xia, Jiazhi and Sun, Qian and Hoi, Steven C.H.},
title = {Modeling 3D Articulated Motions with Conformal Geometry Videos (CGVs)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072349},
doi = {10.1145/2072298.2072349},
abstract = {3D articulated motions are widely used in entertainment, sports, military, and medical applications. Among various techniques for modeling 3D motions, geometry videos (GVs) are a compact representation in that each frame is parameterized to a 2D domain, which captures the 3D geometry (x, y, z) to a pixel (r, g, b) in the image domain. As a result, the widely studied image/video processing techniques can be directly borrowed for 3D motion. This paper presents conformal geometry videos (CGVs), a novel extension of the traditional geometry videos by taking into the consideration of the isometric nature of 3D articulated motions. We prove that the 3D articulated motion can be uniquely (up to rigid motion) represented by (»,H), where » is the conformal factor characterizing the intrinsic property of the 3D motion, and H the mean curvature characterizing the extrinsic feature (i.e., embedding or appearance). Furthermore, the conformal factor » is pose-invariant. Thus, in sharp contrast to the GVs which capture 3D motion by three channels, CGVs take only one channel of mean curvature H and the first frame of the conformal factor », i.e., approximately 1/3 the storage of the GVs. In addition, CGVs have strong spatial and temporal coherence, which favors various well studied video compression techniques. Thus, CGVs can be highly compressed by using the state-of the-art video compression techniques, such as H.264/AVC. Our experimental results on real-world 3D motions show that CGVs are a highly compact representation for 3D articulated motions, i.e., given CGVs and GVs of the same file size, CGVs show much better visual quality than GVs.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {383–392},
numpages = {10},
keywords = {isometric transformation, h.264/avc, conformal geometry videos, 3d articulated motion, geometry videos, video compression, deformable objects, conformal parameterization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072350,
author = {Xu, Qianqian and Jiang, Tingting and Yao, Yuan and Huang, Qingming and Yan, Bowei and Lin, Weisi},
title = {Random Partial Paired Comparison for Subjective Video Quality Assessment via Hodgerank},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072350},
doi = {10.1145/2072298.2072350},
abstract = {Subjective visual quality evaluation provides the groundtruth and source of inspiration in building objective visual quality metrics. Paired comparison is expected to yield more reliable results; however, this is an expensive and timeconsuming process. In this paper, we propose a novel framework of HodgeRank on Random Graphs (HRRG) to achieve efficient and reliable subjective Video Quality Assessment (VQA). To address the challenge of a potentially large number of combinations of videos to be assessed, the proposed methodology does not require the participants to perform the complete comparison of all the paired videos. Instead, participants only need to perform a random sample of all possible paired comparisons, which saves a great amount of time and labor. In contrast to the traditional deterministic incomplete block designs, our random design is not only suitable for traditional laboratory and focus-group studies, but also fit for crowdsourcing experiments on Internet where the raters are distributive over Internet and it is hard to control with precise experimental designs.Our contribution in this work is three-fold: 1) a HRRG framework is proposed to quantify the quality of video; 2) a new random design principle is investigated to conduct paired comparison based on Erdos-Renyi random graph theory; 3) Hodge decomposition is introduced to derive, from incomplete and imbalanced data, quality scores of videos and inconsistency of participants'judgments. We demonstrate the effectiveness of the proposed framework on LIVE Database. Equipped with random graph theory and HodgeRank, our scheme has the following advantages over the traditional ones: 1) data collection is simple and easy to handle, and thus is more suitable for crowdsourcing on Internet; 2) workload on participants is lower and more flexible; 3) the rating procedure is efficient, labor-saving, and more importantly, without jeopardizing the accuracy of the results.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {393–402},
numpages = {10},
keywords = {subjective video quality assessment, random graphs, persistence homology, hodgerank},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072351,
author = {Song, Wei and Tjondronegoro, Dian and Docherty, Michael},
title = {Saving Bitrate vs. Pleasing Users: Where is the Break-Even Point in Mobile Video Quality?},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072351},
doi = {10.1145/2072298.2072351},
abstract = {This paper presents a comprehensive study to find the most efficient bitrate requirement to deliver mobile video that optimizes bandwidth, while at the same time maintains good user viewing experience. In the study, forty participants were asked to choose the lowest quality video that would still provide for a comfortable and long-term viewing experience, knowing that higher video quality is more expensive and bandwidth intensive. This paper proposes the lowest pleasing bitrates and corresponding encoding parameters for five different content types: cartoon, movie, music, news and sports. It also explores how the lowest pleasing quality is influenced by content type, image resolution, bitrate, and user gender, prior viewing experience, and preference. In addition, it analyzes the trajectory of users' progression while selecting the lowest pleasing quality. The findings reveal that the lowest bitrate requirement for a pleasing viewing experience is much higher than that of the lowest acceptable quality. Users' criteria for the lowest pleasing video quality are related to the video's content features, as well as its usage purpose and the user's personal preferences. These findings can provide video providers guidance on what quality they should offer to please mobile users.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {403–412},
numpages = {10},
keywords = {mobile video, user profile, bitrate, acceptability, content type, encoding parameters},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072352,
author = {Zheng, Peijia and Huang, Jiwu},
title = {Implementation of the Discrete Wavelet Transform and Multiresolution Analysis in the Encrypted Domain},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072352},
doi = {10.1145/2072298.2072352},
abstract = {Signal processing in the encrypted domain is a new technology for protecting valuable signals from insecure signal processing. Although there has been some research in the area, this field of research is still in its infancy.In this paper, we propose a scheme for implementing discrete wavelet transform (DWT) and multi-resolution analysis (MRA) in a homomorphic encrypted domain. We first suggest a framework for performing DWT and inverse discrete wavelet transform (IDWT) in the encrypted domain and then conduct an analysis of data expansion and quantization errors on the framework. In order to solve the problem of data expansion, which is very important in practical applications, we then present a novel method for reducing data expansion in the case that both DWT and IDWT are performed. With the proposed method, multi-level DWT/IDWT can be performed with less data expansion in a homomorphic encrypted domain. We address the case of a two-dimensional Haar wavelet transform and give some experiments to demonstrate the advantages of our improved method. To the best of our knowledge, this is the first paper on the implementation of DWT and MRA in the encrypted domain.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {413–422},
numpages = {10},
keywords = {data expansion, multiresolution analysis (mra), discrete wavelet transform(dwt), secure signal processing., signal processing the encrypted domain, homomorphic encryption},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072354,
author = {Song, Jingkuan and Yang, Yi and Huang, Zi and Shen, Heng Tao and Hong, Richang},
title = {Multiple Feature Hashing for Real-Time Large Scale near-Duplicate Video Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072354},
doi = {10.1145/2072298.2072354},
abstract = {Near-duplicate video retrieval (NDVR) has recently attracted lots of research attention due to the exponential growth of online videos. It helps in many areas, such as copyright protection, video tagging, online video usage monitoring, etc. Most of existing approaches use only a single feature to represent a video for NDVR. However, a single feature is often insufficient to characterize the video content. Besides, while the accuracy is the main concern in previous literatures, the scalability of NDVR algorithms for large scale video datasets has been rarely addressed. In this paper, we present a novel approach - Multiple Feature Hashing (MFH) to tackle both the accuracy and the scalability issues of NDVR. MFH preserves the local structure information of each individual feature and also globally consider the local structures for all the features to learn a group of hash functions which map the video keyframes into the Hamming space and generate a series of binary codes to represent the video dataset. We evaluate our approach on a public video dataset and a large scale video dataset consisting of 132,647 videos, which was collected from YouTube by ourselves. The experiment results show that the proposed method outperforms the state-of-the-art techniques in both accuracy and efficiency.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {423–432},
numpages = {10},
keywords = {video retrieval, indexing, near-duplicate, hashing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072355,
author = {Liu, Xianming and Yao, Hongxun and Ji, Rongrong and Xu, Pengfei and Sun, Xiaoshuai and Tian, Qi},
title = {Learning Heterogeneous Data for Hierarchical Web Video Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072355},
doi = {10.1145/2072298.2072355},
abstract = {Web videos such as YouTube are hard to obtain sufficient precisely labeled training data and analyze due to the complex ontology. To deal with these problems, we present a hierarchical web video classification framework by learning heterogeneous web data, and construct a bottom-up semantic forest of video concepts by learning from meta-data. The main contributions are two-folds: firstly, analysis about middle-level concepts' distribution is taken based on data collected from web communities, and a concepts redistribution assumption is made to build effective transfer learning algorithm. Furthermore, an AdaBoost-Like transfer learning algorithm is proposed to transfer the knowledge learned from Flickr images to YouTube video domain and thus it facilitates video classification. Secondly, a group of hierarchical taxonomies named Semantic Forest are mined from YouTube and Flickr tags which reflect better user intention on the semantic level. A bottom-up semantic integration is also constructed with the help of semantic forest, in order to analyze video content hierarchically in a novel perspective. A group of experiments are performed on the dataset collected from Flickr and YouTube. Compared with state-of-the-arts, the proposed framework is more robust and tolerant to web noise.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {433–442},
numpages = {10},
keywords = {hierarchical taxonomy, transfer learning, semantic forest, video annotation, heterogeneous data, video search, web video classification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072356,
author = {Wei, Xiao-Yong and Yang, Zhen-Qun},
title = {Coached Active Learning for Interactive Video Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072356},
doi = {10.1145/2072298.2072356},
abstract = {Active learning with uncertainty sampling has been popularly employed in implementing interactive video search, due to its promise to reduce labeling efforts. However, since the ultimate goal of interactive search is to find as many relevant shots as possible, the purely explorative learning strategy always places conventional active learning in a dilemma whether to explore uncertain areas for a better understanding of query distribution or to harvest in certain areas for more relevant instances. In this paper, we propose a novel paradigm of active learning, where a coaching process is introduced to guide the leaner by jointly consulting an estimated prior query distribution and a posterior query distribution indicated by current classifier outcomes. To bypass the difficulty of estimating the prior query distribution from a limited number of labeled relevant instances, we propose to estimate the distribution using a set of semantic distributions which are statistically from the same distributions as the labeled relevant instances. With the coaching of both prior and posterior query distributions, the learning can be conducted and scheduled with a global perspective, and thus can explicitly balance the trade-off between exploitation and exploration. The results of the experiments on TRECVID 2005--2009 datasets validate the efficiency and effectiveness of our approach, which outperforms the conventional active learning methods with uncertainty sampling and also shows superiority to several state-of-the art interactive video search systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {443–452},
numpages = {10},
keywords = {interactive search, querying function, coached active learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072357,
author = {Yuan, Jin and Zha, Zheng-Jun and Zheng, Yao-Tao and Wang, Meng and Zhou, Xiangdong and Chua, Tat-Seng},
title = {Learning Concept Bundles for Video Search with Complex Queries},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072357},
doi = {10.1145/2072298.2072357},
abstract = {Classifiers for primitive visual concepts like "car", "sky" have been well developed and widely used to support video search on simple queries. However, it is usually ineffective for complex queries like "one or more people at a table or desk with a computer visible", as they carry semantics far more complex and different from simply aggregating the meanings of their constituent primitive concepts. To facilitate video search of complex queries, we propose a higher-level semantic descriptor named "concept bundle", which integrates multiple primitive concepts, such as "(soccer, fighting)", "(lion, hunting, zebra)" etc, to describe the visual representation of the complex semantics. The proposed approach first automatically selects informative concept bundles. It then builds a novel concept bundle classifier based on multi-task learning by exploiting the relatedness between concept bundle and its primitive concepts. To model a complex query, it proposes an optimal selection strategy to select related primitive concepts and concept bundles by considering both their classifier performance and semantic relatedness with respect to the query. The final results are generated by fusing the individual results from these selected primitive concepts and concept bundles. Extensive experiments are conducted on two video datasets: TRECVID 2008 and YouTube datasets. The experimental results indicate that: (a) our concept bundle learning approach outperforms the state-of-the-art methods by at least 19% and 29% on TRECVID 2008 and YouTube datasets, respectively; and (b) the use of concept bundles can improve the search performance for complex queries by at least 37.5% on TRECVID 2008 and 52% on YouTube datasets.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {453–462},
numpages = {10},
keywords = {multi-task learning, video search, concept bundle, primitive concept, complex query},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072359,
author = {Ni, Pengpeng and Eg, Ragnhild and Eichhorn, Alexander and Griwodz, Carsten and Halvorsen, P\r{a}l},
title = {Flicker Effects in Adaptive Video Streaming to Handheld Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072359},
doi = {10.1145/2072298.2072359},
abstract = {Streaming video over the Internet requires mechanisms that limit the streams' bandwidth consumption within its fair share. TCP streaming guarantees this and provides lossless streaming as a side-effect. Adaptation by packet drop does not occur in the network, and excessive startup latency and stalling must be prevented by adapting the bandwidth consumption of the video itself. However, when the adaptation is performed during an ongoing session, it may influence the perceived quality of the entire video and result in improved or reduced visual quality of experience. We have investigated visual artifacts that are caused by adaptive layer switching -- we call them flicker effects -- and present our results for handheld devices in this paper. We considered three types of flicker, namely noise, blur and motion flicker. The perceptual impact of flicker is explored through subjective assessments. We vary both the intensity of quality changes (amplitude) and the number of quality changes per second (frequency). Users' ability to detect and their acceptance of variations in the amplitudes and frequencies of the quality changes are explored across four content types. Our results indicate that multiple factors influence the acceptance of different quality variations. Amplitude plays the dominant role in delivering satisfactory video quality, while frequency can also be adjusted to relieve the annoyance of flicker artifacts.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {463–472},
numpages = {10},
keywords = {video adaptation, layer switching, subjective video quality},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072360,
author = {Liu, Yao and Guo, Lei and Li, Fei and Chen, Songqing},
title = {An Empirical Evaluation of Battery Power Consumption for Streaming Data Transmission to Mobile Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072360},
doi = {10.1145/2072298.2072360},
abstract = {Internet streaming applications are becoming increasingly popular on mobile devices. However, receiving streaming services on mobile devices is often constrained by their limited battery power supply. Various techniques have been proposed to save battery power consumption on mobile devices, mainly focusing on how much data to transmit and how to transmit.In this paper, we conduct an experiment-based study with 11 Internet streaming applications using different streaming protocols. Our goal is to empirically investigate the battery power consumption on the wireless network interface for receiving streaming data via different approaches. Through measurement and analysis, we find that (1) the Chunk-based streaming is widely used in practice and it is most power-efficient because the traffic shaping technique is adopted to utilize PSM on mobile devices to save battery power consumption; however, it may cause quality degradation from time to time; (2) reducing streaming data transmission (by switching to a lower streaming quality) can marginally help save battery power consumption in RTSP, Pseudo streaming, and Chunk-based streaming applications; but it is effective for P2P streaming applications; (3) P2P streaming to mobile devices is not power-efficient because of the additional transmission of control traffic and uploading traffic; and reducing upload alone does not help for battery power saving. Our investigation provides new insights and some guidelines for the current Internet mobile streaming services and calls for further research on more power-efficient and scalable Internet mobile streaming protocols.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {473–482},
numpages = {10},
keywords = {data transmission, power saving, battery power consumption, internet mobile streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072361,
author = {Lai, Jui-Hsin and Chen, Chieh-Li and Wu, Po-Chen and Kao, Chieh-Chi and Chien, Shao-Yi},
title = {Tennis Real Play: An Interactive Tennis Game with Models from Real Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072361},
doi = {10.1145/2072298.2072361},
abstract = {Tennis Real Play (TRP) is an interactive tennis game system constructed with models extracted from videos of real matches. The key techniques proposed for TRP include player modeling and video-based player/court rendering. For player model creation, we propose a database normalization process and a behavioral transition model of tennis players, which might be a good alternative for motion capture in the conventional video games. For player/court rendering, we propose a framework for rendering vivid game characters and providing the real-time ability. We can say that image-based rendering leads to a more interactive and realistic rendering. Experiments show that video games with vivid viewing effects and characteristic players can be generated from match videos without much user intervention. Because the player model can adequately record the ability and condition of a player in the real world, it can then be used to roughly predict the results of real tennis matches in the next days. The results of a user study reveal that subjects like the increased interaction, immersive experience, and enjoyment from playing TRP.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {483–492},
numpages = {10},
keywords = {interactive game, video rendering, video analysis, video game},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072362,
author = {Chen, Xiangwen and Chen, Minghua and Li, Baochun and Zhao, Yao and Wu, Yunnan and Li, Jin},
title = {Celerity: A Low-Delay Multi-Party Conferencing Solution},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072362},
doi = {10.1145/2072298.2072362},
abstract = {In this paper, we attempt to revisit the problem of multi-party conferencing from a practical perspective, and to rethink the design space involved in this problem. We believe that an emphasis on low end-to-end delays between any two parties in the conference is a must, and the source sending rate in a session should adapt to bandwidth availability and congestion. We present Celerity, a multi-party conferencing solution specifically designed to achieve our objectives. It is entirely Peer-to-Peer (P2P), and as such eliminating the cost of maintaining centrally administered servers. It is designed to deliver video with low end-to-end delays, at quality levels commensurate with available network resources over arbitrary network topologies where bottlenecks can be anywhere in the network. This is in contrast to commonly assumed P2P scenarios where bandwidth bottlenecks reside only at the edge of the network. The highlight in our design is a distributed and adaptive rate control protocol, that can discover and adapt to arbitrary topologies and network conditions quickly, converging to efficient link rate allocations allowed by the underlying network. In accordance with adaptive link rate control, source video encoding rates are also dynamically controlled to optimize video quality. We have implemented Celerity in a prototype system, and demonstrate its superior performance over existing solutions in a local experimental testbed and over the Internet.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {493–502},
numpages = {10},
keywords = {multi-party video conferencing, peer-to-peer, arbitrary network topology, low delay, utility maximization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072364,
author = {Tang, Wenbin and Cai, Rui and Li, Zhiwei and Zhang, Lei},
title = {Contextual Synonym Dictionary for Visual Object Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072364},
doi = {10.1145/2072298.2072364},
abstract = {In this paper, we study the problem of visual object retrieval by introducing a dictionary of contextual synonyms to narrow down the semantic gap in visual word quantization. The basic idea is to expand a visual word in the query image with its synonyms to boost the retrieval recall. Unlike the existing work such as soft-quantization, which only focuses on the Euclidean (l2) distance in descriptor space, we utilize the visual words which are more likely to describe visual objects with the same semantic meaning by identifying the words with similar contextual distributions (i.e. contextual synonyms). We describe the contextual distribution of a visual word using the statistics of both co-occurrence and spatial information averaged over all the image patches having this visual word, and propose an efficient system implementation to construct the contextual synonym dictionary for a large visual vocabulary. The whole construction process is unsupervised and the synonym dictionary can be naturally integrated into a standard bag-of-feature image retrieval system. Experimental results on several benchmark datasets are quite promising. The contextual synonym dictionary-based expansion consistently outperforms the l2 distance-based soft-quantization, and advances the state-of-the-art performance remarkably.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {503–512},
numpages = {10},
keywords = {object retrieval, query expansion, visual synonym dictionary, bag-of-word},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072365,
author = {Lu, Wenhao and Wang, Jingdong and Hua, Xian-Sheng and Wang, Shengjin and Li, Shipeng},
title = {Contextual Image Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072365},
doi = {10.1145/2072298.2072365},
abstract = {In this paper, we propose a novel image search scheme, contextual image search. Different from conventional image search schemes that present a separate interface (e.g., text input box) to allow users to submit a query, the new search scheme enables users to search images by only masking a few words when they are reading through Web pages or other documents. Rather than merely making use of the explicit query input that is often not sufficient to express user's search intent, our approach explores the context information to better understand the search intent with two key steps: query augmenting and search results reranking using context, and expects to obtain better search results. Beyond contextual Web search, the context in our case is much richer and includes images besides texts. In addition to this type of search scheme, called contextual image search with text input, we also present another type of scheme, called contextual image search with image input, to allow users to select an image as the search query from Web pages or other documents they are reading. The key idea is to use the search-to-annotation technique and the contextual textual query mining scheme to determine the corresponding textual query, to finally get semantically similar search results. Experiments show that the proposed schemes make image search more convenient and the search results are more relevant to user intention.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {513–522},
numpages = {10},
keywords = {contextual query augmentation, image search, contextual reranking, textual and visual context},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072366,
author = {Liu, Zhang and Wang, Chaokun and Bai, Yiyuan and Wang, Hao and Wang, Jianmin},
title = {MUSIZ: A Generic Framework for Music Resizing with Stretching and Cropping},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072366},
doi = {10.1145/2072298.2072366},
abstract = {Content-aware music adaption, i.e. music resizing, in temporal constraints starts drawing attention from multimedia communities because of the need of real-world scenarios, e.g. animation production and radio advertisement production. The goal of music resizing is to change the length of a music track to a user preferred length using a series of basic operations, e.g. compression, prolonging, cropping and repeating. The only existing music resizing approach so far, called LyDAR, suffers from some limitations. For example, it cannot support prolonging a music track and cannot compress music pieces with very small stretch rates. In this paper, we propose MUSIZ, a generic framework for MUsic reSIZing. Observing the diversity of quality degradation for different segments, we propose the concept of stretch-resistance to measure the degree of quality degradation after a segment is stretched. MUSIZ stretches high stretch-resistance segments intensively and relieves low stretch-resistance segments to reduce the negative impact on the stretched music piece. For short length resizing requests, we develop a contiguity-preservative cropping algorithm to remove segments before stretching, while smoothing the abrupt change at the joint between two segments. Comprehensive experimental results show that MUSIZ is superior to the existing approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {523–532},
numpages = {10},
keywords = {music retargeting, stretch-resistance, music resizing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072367,
author = {Morioka, Nobuyuki and Wang, Jingdong},
title = {Robust Visual Reranking via Sparsity and Ranking Constraints},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072367},
doi = {10.1145/2072298.2072367},
abstract = {Visual reranking has become a widely-accepted method to improve traditional text-based image search engines. Its basic principle is that visually similar images should have similar ranking scores. While existing methods are different in specifics, almost all of them are based on explicit or implicit pseudo-relevance feedback (PRF). Explicit PRF-based approaches, including classification-based and clustering-based reranking, suffer from the difficulty of selecting reliable positive and negative samples. Implicit PRF-based approaches, such as graph-based and Bayesian visual reranking, deal with such unreliability by making use of the initial ranking in a soft manner, but have limited capability of promoting relevant images and lowering down irrelevant images.In this paper, we propose l1 square loss optimization based on sparsity and ranking constraints to detect confident samples which are most likely to be relevant to a query. Based on the discovered confident samples, we present an adaptive kernel-based scheme to rerank the images. The success of our proposed method comes from another important observation that irrelevant images, whether initially positioned at the top or bottom, are usually less-popular and more diverse than relevant images. Therefore, it is robust against outlier images and suitable when relevant images are multi-modally distributed. The experimental results demonstrate significant improvement of our method over several existing reranking approaches on both MSRA-MM V1.0 and Web Queries datasets.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {533–542},
numpages = {10},
keywords = {non-negativity, l1 loss, image search reranking, sparsity},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072369,
author = {McDaniel, Troy and Goldberg, Morris and Villanueva, Daniel and Viswanathan, Lakshmie Narayan and Panchanathan, Sethuraman},
title = {Motor Learning Using a Kinematic-Vibrotactile Mapping Targeting Fundamental Movements},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072369},
doi = {10.1145/2072298.2072369},
abstract = {In this paper, we present a novel approach for teaching motor skills through the use of vibrotactile stimulation. We propose a kinematic-vibrotactile mapping that targets fundamental movements (basic building blocks of human motion) using saltatory vibration patterns where vibrations are delivered and interpreted as movement through a conceptual mapping. Two conceptual mappings are explored: the "follow me" concept and the push/pull metaphor. A user study, approved by a local ethics committee, was conducted to explore how these conceptual mappings affect learnability, recognition accuracy, response time and naturalness. Results show the approach to work effectively with a combination of vibration patterns under each conceptual mapping providing the most useful design.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {543–552},
numpages = {10},
keywords = {saltation, fundamental movements, kinematic, vibrotactile},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072370,
author = {Xiang, Xiaohong and Kankanhalli, Mohan S.},
title = {Affect-Based Adaptive Presentation of Home Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072370},
doi = {10.1145/2072298.2072370},
abstract = {In recent times, the proliferation of multimedia devices and reduced costs of data storage have enabled people to easily record and collect a large number of home videos; furthermore, this collection is growing with time. With the popularity of participatory media such as YouTube and facebook, problems are encountered when people intend to share their home videos with others. The first problem is that different people might be interested in different video content. Given the numbers of home videos, it is a time-consuming and hard task to manually select proper content for people with different interests. Secondly, as short videos are becoming more and more popular in media sharing applications, people need to manually cut and edit home videos which is again a tedious task. In this paper, we propose a method that employs affective analysis to automatically create video presentations from home videos. Our novel method adaptively creates presentations based on three properties: emotional tone, local main character and global main character. A novel sparsity-based affective labeling method is proposed to identify the emotional content of the videos. The local and global main characters are determined by applying face recognition in each shot. To demonstrate the proposed method, three kinds of presentations are created for family, acquaintance and outsider. Experimental results show that our method is very effective in video sharing and the users are satisfied with the videos generated by our method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {553–562},
numpages = {10},
keywords = {home video, adaptive presentation, affect},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072371,
author = {Nitta, Naoko and Babaguchi, Noboru},
title = {Example-Based Video Remixing Support System},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072371},
doi = {10.1145/2072298.2072371},
abstract = {Video remixes are generally created by sequentially arranging selected video clips and mixing them with other media streams such as audio clips and transition effects. Especially, mixing music clips often effectively improves the expressive quality of video clips. This paper proposes an example-based system for supporting average users in the 3 steps in video remixing: I)video shot sequence creation, II)music clip selection, and III)audio volume adjustment. The proposed system creates a template for a video remix and gives suggestions to users on an interface such as which video clips should be selected to create a video shot sequence and which music clips should be mixed to the created video shot sequence based on professionally created video remix examples. Then, the audio volume of each video shot is automatically adjusted based on its audio content so that the sounds in the video shots and the music clips would not interfere with each other. Experiments have verified that our system was able to create a video shot sequence whose quality was improved equally as the professionally created one by mixing the selected music clips, doubling the subjective scores from 1.7 to 3.7 on a scale of 1-5. Automatic audio volume adjustment improved the subjective scores by approximately 0.5 points on average. Further, the suggestions provided on the interface was evaluated useful by 5 subjects when creating a video remix by selecting 32 video clips and 4 music clips from 265 video clips and 180 music clips.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {563–572},
numpages = {10},
keywords = {video remixing, music mixing, user interface, examples},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072372,
author = {Ji, Rongrong and Duan, Ling-Yu and Chen, Jie and Yao, Hongxun and Rui, Yong and Chang, Shih-Fu and Gao, Wen},
title = {Towards Low Bit Rate Mobile Visual Search with Multiple-Channel Coding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072372},
doi = {10.1145/2072298.2072372},
abstract = {In this paper, we propose a multiple-channel coding scheme to extract compact visual descriptors for low bit rate mobile visual search. Different from previous visual search scenarios that send the query image, we make use of the ever growing mobile computational capability to directly extract compact visual descriptors at the mobile end. Meanwhile, stepping forward from the state-of-the-art compact descriptor extractions, we exploit the rich contextual cues at the mobile end (such as GPS tags for mobile visual search and 2D barcodes or RFID tags for mobile product search), together with the visual statistics at the reference database, to learn multiple coding channels. Therefore, we describe the query with one of many forms of high-dimensional visual signature, which is subsequently mapped to one or more channels and compressed. The compression function within each channel is learnt based on a novel robust PCA scheme, with specific consideration to preserve the retrieval ranking capability of the original signature. We have deployed our scheme on both iPhone4 and HTC DESIRE 7 to search ten million landmark images in a low bit rate setting. Quantitative comparisons to the state-of-the-arts demonstrate our significant advantages in descriptor compactness (with orders of magnitudes improvement) and retrieval mAP in mobile landmark, product, and CD/book cover search.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {573–582},
numpages = {10},
keywords = {data compression, compact descriptor, mobile visual search, wireless communication, contextual learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072374,
author = {Pentland, Alex},
title = {Honest Signals: How Social Networks Shape Human Behavior},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072374},
doi = {10.1145/2072298.2072374},
abstract = {How did humans coordinate before we had sophisticated language capabilities? Pre-linguistic social species coordinate by signaling, and in particular 'honest signals' which actually cause changes in the listener. I will describe examples of human behaviors that are honest signals, and how they can be used to accurately predict and shape the outcomes of interactions (medical compliance, negotiation, trust assessment, depression screening, etc.). Understanding how human decision making is influenced by these pre-linguistic patterns of signaling also leads to very different ways to build incentives to change. In a recent trial we were able to change population behaviors using a social signaling strategy, and achieved twice the efficiency of standard behavior change schemes.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {583–584},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072375,
author = {Bell, Genevieve},
title = {"U Are Happy Life": Telling the Future's Stories},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072375},
doi = {10.1145/2072298.2072375},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {585–586},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072376,
author = {Robert, Arnaud},
title = {Digital Media Distribution: The Future},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072376},
doi = {10.1145/2072298.2072376},
abstract = {Consumer devices and technologies are evolving faster than ever, allowing for rich, interactive user experiences. But more importantly, media consumption behavior and expectations are changing and the promise of digital media remains to be fulfilled. In this presentation, we will explore major technology trends, consumer trends, and how they intersect with the future digital media experiences and business models.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {587–588},
numpages = {2},
keywords = {technology and consumer trends, digital media},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072378,
author = {Nahrstedt, Klara and Venkatesh, Svetha and Venkatasubramanian, Nalini and Ponceleon, Dulce and Zemankova, Maria and Boll, Susanne},
title = {Networking of Multimedia Women Event beyond Epsilon Science: Where to Look and How to Realize New Opportunities},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072378},
doi = {10.1145/2072298.2072378},
abstract = {"Networking of Multimedia Women" event is a continuation of an on-going conversation in the multimedia research community and efforts by the ACM SIGMM to engage and promote female researchers in multimedia community, enable networking of junior and senior female researchers, and give insights towards successful professional careers based on examples.This year, the event will have a theme, called "Beyond Epsilon Science", where preeminent senior female researchers from academia, industry and government, Svetha Venkatesh, Nalini Venkatasubramanian, Dulce Ponceleon, Susanne Boll, and Maria Zemankova will present and discuss how to go beyond epsilon science, where to look for big ideas with high social impact, as well as how to obtain funding to realize these ideas, innovations and opportunities. Their current research projects and funding efforts, and their personal experiences will drive the event's discussions, awareness of major research and funding initiatives, answers to open questions and insights into successful professional careers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {589–592},
numpages = {4},
keywords = {multimedia interfaces, intervention technologies for autism, multimedia applications, autism, multimedia scalable systems, multimedia information gathering, emergency response, multi-modal situational awareness technologies},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072379,
author = {Tinapple, David and Ingalls, Todd},
title = {ACM Multimedia Interactive Art Program: Interaction Stations},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072379},
doi = {10.1145/2072298.2072379},
abstract = {The Interaction Stations Exhibit features screen-based, interactive works that align with the new conference themes, and integrate into the physical setting of the conference center. In this paper we describe our motivation for this format, as well as the works selected and larger connections.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {593–594},
numpages = {2},
keywords = {culture, multimedia arts, technology},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072381,
author = {El Ssaddik, Abdulmotaleb},
title = {Serious Games},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072381},
doi = {10.1145/2072298.2072381},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {595–596},
numpages = {2},
keywords = {entertainment, serious games},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072382,
author = {Cesar, Pablo and Ooi, Wei Tsang and Moskowitz, Ben and Babin, Zohar and Bulterman, Dick and Lienhart, Rainer and Richter, Robert},
title = {Towards Synergy between the Open Source and the Research Multimedia Communities},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072382},
doi = {10.1145/2072298.2072382},
abstract = {This panel extends current efforts from the ACM Multimedia 2011 Organization Committee in taking an important step towards open source projects. The panelists include speakers who are among the leading figures from the open source community. The goal is to provide a shared space for discussion and interaction among consolidated and new open source projects and multimedia researchers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {597–598},
numpages = {2},
keywords = {open source software, multimedia},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072383,
author = {Lin, Yu-Ru and Oria, Vincent and Candan, K. Selcuk and Kennedy, Lyndon and Dulce Ponceleon, Dulce and Sundaram, Hari and Yan, Rong and Zimmerman, Roger},
title = {Job Opportunities and Career Perspective for Fresh Graduates of the Multimedia Community},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072383},
doi = {10.1145/2072298.2072383},
abstract = {The future of multimedia community depends on how the community effectively and efficiently recruits, nurtures and retains young talents. Students tends to decide on their majors based on job opportunities and the main question in every student mind while finishing a degree is "which jobs are out there for me?" In this panel, we have gathered people from both academia and industry to discuss job opportunities and career perceptive. The panel will try to basically answer two main questions: (1) Which are the jobs for the fresh graduates of our community? (2) What are the carrier paths in both academia and industry?},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {599–600},
numpages = {2},
keywords = {academia industry, tenure, career},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072384,
author = {El-Maleh, Khaled and Wang, Haohong and Wee, Susie and Yu, Heather and Johnston, James D. and Zhang, Zhengyou},
title = {Innovating the Multimedia Experience},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072384},
doi = {10.1145/2072298.2072384},
abstract = {In this panel, each panelist will present their view of the current state-of-the-art of research and product innovations in the three major areas of multimedia experience: visual, auditory and gaming. We will discuss examples of innovation that enhance the consumption and sharing of multimedia (video, audio, graphics etc.) and thus increase quality of user experience. Another major focus of this panel is to open the discussion on how to innovate new multimedia user experiences.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {601–602},
numpages = {2},
keywords = {multimedia experience, gaming, visual media, auditory media, user experience, sharing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072386,
author = {Liem, Cynthia C.S. and M\"{u}ller, Meinard and Eck, Douglas and Tzanetakis, George},
title = {1st International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072386},
doi = {10.1145/2072298.2072386},
abstract = {The 1st International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM) at ACM Multimedia was proposed in order to gather experts from the Music and Multimedia Information Retrieval communities, as well as other neighboring fields. The workshop aims to provide a high-profile platform for presenting current work on Music Information Retrieval, with strong focus on user-centered and multimodal approaches. These focus areas are not only relevant to the Music Information Retrieval field, but equally recognized as emerging and relevant in the Multimedia domain. This way, a cross-disciplinary dialogue on open challenges can be initiated, facilitating new bridging opportunities and increased exchanges of expertise between communities. In this summary, we provide an overview of the 1st MIRUM workshop. After a description of the rationale and focus areas of the workshop, the accepted submissions and other program elements are summarized.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {603–604},
numpages = {2},
keywords = {user-centered design, multimodal music processing, music information retrieval, cross-domain methodology transfer},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072387,
author = {Battiato, Sebastiano and Emmanuel, Sabu and Ulges, Adrian and Worring, Marcel},
title = {Third ACM International Workshop on Multimedia in Forensics and Intelligence (MiFor 2011)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072387},
doi = {10.1145/2072298.2072387},
abstract = {This paper introduces the context of the workshop and the associated papers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {605–606},
numpages = {2},
keywords = {privacy, multimedia search, forensic mining, image forgery, pattern recognition, computer vision},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072388,
author = {Berrani, Sid-Ahmed and Messina, Alberto},
title = {AIEMPro 2011: The 4th International Workshop on Automated Media Analysis and Production for Novel TV Services},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072388},
doi = {10.1145/2072298.2072388},
abstract = {The ACM AIEMPro 2011 workshop presents research on automated media content analysis and production for, amongst others, the development of novel TV services. The program of the workshop has two sessions. The first one is composed of three papers on video and TV content structuring and indexing. The second session is also composed of three papers on media production and retrieval systems and applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {607–608},
numpages = {2},
keywords = {tv services., automatic media content analysis and indexing, media production},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072389,
author = {Hoi, Steven Chu-Hong and Jacovi, Michal and Kompatsiaris, Ioannis and Luo, Jiebo and Tserpes, Konstantinos},
title = {WSM2011: Third ACM Workshop on Social Media},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072389},
doi = {10.1145/2072298.2072389},
abstract = {The Third Workshop on Social Media (WSM2011) continues the series of Workshops on Social Media in 2009 and 2010 and has been established as a platform for the presentation and discussion of the latest, key research issues in social media analysis, exploration, search, mining, and emerging new social media applications. It is held in conjunction with the ACM International Multimedia Conference (MM'11) at Scottsdale, Arizona, USA, 2011 and has attracted contributions on various aspects of social media including data mining from social media, content organization, geo-localization, personalization, recommendation systems, user experience, machine learning and social media approaches and architectures for large-scale data processing.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {609–610},
numpages = {2},
keywords = {social media, social web and networks, social media search, media analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072390,
author = {Ramzan, Naeem and Wang, Fei and Patrikakis, Charalampos Z. and Cui, Peng and Doulamis, Nikolaos and Yang, Shiqiang and Sun, Gordon},
title = {ACM International Workshop on Social and Behavioral Networked Media Access (SBNMA'11)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072390},
doi = {10.1145/2072298.2072390},
abstract = {In an endeavour to speak and prevail over some of the open problems that obstruct efficient networked media, this workshop will fetch together folks from a number of research communities, including but not limited to Multimedia Distribution and Access, Social Network Analysis, Multimedia Content Analysis, Behavioral Analysis, User Modelling Adaptation and Personalization. It is our credence that a synergetic approach involving the above mentioned research areas can surpass their individual potentials, leading to improved networked media access. The main objective of this workshop is to provide a forum to disseminate work that explicitly exploit the synergy between multimedia content analysis, behavioral modelling, personalisation, and next generation networking and community aspects of social networks. This synergetic methodology could produce high quality of experience for personalized multimedia access in networking environment.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {611–612},
numpages = {2},
keywords = {social, interaction, adaptation, personalization, access, behavioral},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072391,
author = {Mezaris, Vasileios and Scherp, Ansgar and Jain, Ramesh and Kankanhalli, Mohan and Zhou, Huiyu and Zhang, Jianguo and Wang, Liang and Zhang, Zhengyou},
title = {Modeling and Representing Events in Multimedia},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072391},
doi = {10.1145/2072298.2072391},
abstract = {This paper presents an overview of the Joint Workshop on Modeling and Representing Events (JMRE), which is held as part of ACM Multimedia 2011. JMRE is concerned with the understanding of events from multimedia, and with using events in order to better organize and consume multimedia.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {613–614},
numpages = {2},
keywords = {event detection, sparse representation, multimedia, objects, event-based applications, event models, events},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072392,
author = {Pantic, Maja and Pentland, Alex and Vinciarelli, Alessandro and Cucchiara, Rita and Daoudi, Mohamed and Del Bimbo, Alberto},
title = {Joint ACM Workshop on Human Gesture and Behavior Understanding: (J-HGBU'11)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072392},
doi = {10.1145/2072298.2072392},
abstract = {The ability to understand social signals of a person we are communicating with is the core of social intelligence. Social Intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. At the same time, human-centric multimedia applications for humans and about humans are becoming increasingly important. 3D modeled human-objects, like bodies, heads and faces are exploited for animation, security, and human computer interaction, while three dimensional motion of arms, legs and local body features is used for more complete human gesture, activity and behavior analysis. The Joint Human Gesture and Behavior Understanding (J-HGBU) workshop event consists of two parts focusing on these complementary challenges: the Workshop on Multimedia Access to 3D Human Objects (MA3HO'11) and the Workshop on Social Signal Processing (SSPW'11).},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {615–616},
numpages = {2},
keywords = {human gestures, social signals, behavior understanding, multimedia experience, 3d human objects},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072393,
author = {Cao, Yu and Kalpathy-Cramer, Jayashree and \"{U}nay, Devrim},
title = {Medical Multimedia Analysis and Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072393},
doi = {10.1145/2072298.2072393},
abstract = {Advances in sensor technology, processing speed, high-speed networking, and the massive digital storages are being incorporated into today's healthcare practice. Tremendous amounts of medical multimedia data are captured and recorded in digital format during the daily clinical practice, medical research, and education. Intelligent medical knowledge discovery and retrieval from medical multimedia data is very useful and highly desirable. Motivated by the huge potential benefits, we have organized the ACM International Workshop on Medical Multimedia Analysis and Retrieval (MMAR2011), held in conjunction with The Annual ACM International Conference on Multimedia (ACM Multimedia 2011). In this paper, we first introduce the background and overview of the workshop. Then we introduce the workshop review process, followed with a brief introduction of the accepted papers in this workshop, as well as the conclusion.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {617–618},
numpages = {2},
keywords = {medical information system, multimedia databases, multimedia information system},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072394,
author = {Shirehjini, Ali Asghar Nazari and Albayrak, Sahin and Yassine, Abdulsalam},
title = {Ubi-MUI 2011 ACM Workshop Summary},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072394},
doi = {10.1145/2072298.2072394},
abstract = {Intelligent Environments have the vision of enhancing our everyday environment and interaction with its objects by sensing, computing, and communication capabilities. The major characteristics of such environments are the increasing number of embedded intelligent devices (ubiquity) into the background (transparency). These devices are expected to disappear or blend into the background and will be invisible to the user. However, because of this transparency, users fail to develop an adequate mental model for interaction with such environments. The Ubiquitous Meta User Interfaces (Ubi-MUI) ACM workshop provides a venue for the development of highly intuitive, multimedia supported meta user interfaces that bring transparency, predictability, and control into intelligent environments.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {619–620},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072395,
author = {Lau, Rynson W.H. and Shih, Timothy K. and Li, Frederick W.B. and Yen, Neil Y.},
title = {The Third ACM International Workshop on Multimedia Technologies for Distance Learning (MTDL 2011)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072395},
doi = {10.1145/2072298.2072395},
abstract = {The MTDL 2011 workshop in its third edition aims to continue in the contribution and evaluation of the impact of multimedia technologies to e-Learning. This workshop is held in conjunction with the ACM Multimedia 2011 Conference in Scottsdale, Arizona, U.S.A. As a cover paper of this workshop, we briefly summarize important issues to be addressed in e-learning in the first section, followed by a discussion of important issues proposed in the 5 papers accepted to the workshop (among the 9 submissions), plus 3 invited papers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {621–622},
numpages = {2},
keywords = {multimedia technologies, e-learning, distance learning, learning methodologies, game-based learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072396,
author = {Luo, Jiebo and Shan, Caifeng and Shao, Ling and Etoh, Minoru},
title = {ACM International Workshop on Interactive Multimedia on Mobile and Portable Devices (IMMPD'11)},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072396},
doi = {10.1145/2072298.2072396},
abstract = {With the mobile and portable devices become ubiquitous for people's daily life, how to design user interfaces of these products that enable natural, intuitive and fun interaction is one of the main challenges the multimedia community is facing. Following several successful events, the ACM International workshop on Interactive Multimedia on Mobile and Portable Devices (IMMPD'11) aims to bring together researchers from both academia and industry in domains including computer vision, audio and speech processing, machine learning, pattern recognition, communications, human-computer interaction, and media technology to share and discuss recent advances in interactive multimedia.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {623–624},
numpages = {2},
keywords = {human-computer interaction, consumer electronics, computer vision, pattern recognition., multimedia},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072398,
author = {Hanjalic, Alan and Larson, Martha},
title = {Frontiers in Multimedia Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072398},
doi = {10.1145/2072298.2072398},
abstract = {This outline summarizes our tutorial "Frontiers in Multimedia Search", whose goal is to provide insights into the most recent developments in the field of multimedia retrieval and to identify the issues and bottlenecks that could determine the directions of research focus for the coming years. We present an overview of new algorithms and techniques, particularly concentrating on those innovative approaches that are informed by neighboring fields including information retrieval, speech and language processing and network analysis. We also discuss evaluation of new algorithms, in particular, making use of crowdsourcing for the development of the necessary data sets.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {625–626},
numpages = {2},
keywords = {tutorial, multimedia, retrieval, search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072399,
author = {Mei, Tao and Zhang, Ruofei and Hua, Xian-Sheng},
title = {Internet Multimedia Advertising: Techniques and Technologies},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072399},
doi = {10.1145/2072298.2072399},
abstract = {The explosive growth of multimedia data on the Internet creates huge opportunities for multimedia advertising. In this tutorial, we present the techniques and technologies for Internet multimedia advertising. The tutorial aims at bringing together recent insights from the research on multimedia advertising that addresses the theoretical fundamentals, solution concepts, and the issues related to the development of modern multimedia advertising schemes.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {627–628},
numpages = {2},
keywords = {online advertising, multimedia advertising},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072400,
author = {Snoek, Cees G.M. and Smeulders, Arnold W.M.},
title = {Internet Video Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072400},
doi = {10.1145/2072298.2072400},
abstract = {In this tutorial, we focus on the challenges in internet video search, present methods how to achieve state-of-the-art performance while maintaining efficient execution, and indicate how to obtain improvements in the near future. Moreover, we give an overview of the latest developments and future trends in the field on the basis of the TRECVID competition - the leading competition for video search engines run by NIST - where we have achieved consistent top performance over the past years, including the 2008, 2009 and 2010 editions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {629–630},
numpages = {2},
keywords = {information visualization, video retrieval, visual categorization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072401,
author = {Santini, Simone},
title = {Semantic Computing in Multimedia},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072401},
doi = {10.1145/2072298.2072401},
abstract = {This short overview describes the contents of the tutorial Semantic computing in multimedia, which was offered to the participants of ACM Multimedia 2011.Given the impossibility of summarizing properly the contents of the tutorial in just two pages, the purpose of this overview is mainly to introduce the reader to the relevant bibliography.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {631–632},
numpages = {2},
keywords = {multimedia semantics, logic, semantics, context},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072402,
author = {Richard, Ga\"{e}l},
title = {Tutorial on Multimedia Music Signal Processing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072402},
doi = {10.1145/2072298.2072402},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {633–634},
numpages = {2},
keywords = {music source separation, audio signal representations, decomposition models, multimodal music signal processing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072403,
author = {Friedland, Gerald},
title = {Acoustic and Multimodal Processing for Multimedia Content Analysis},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072403},
doi = {10.1145/2072298.2072403},
abstract = {This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2011.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {635–636},
numpages = {2},
keywords = {multimodal, video, audio, multimedia content analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072404,
author = {Zhang, Xiao-Ping and Liu, Zhu},
title = {Graphical Probabilistic Modeling and Applications in Multimedia Content Analysis},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072404},
doi = {10.1145/2072298.2072404},
abstract = {Graphical probabilistic models play an important role in modern machine learning and pattern recognition. In this half-day tutorial, we introduce the fundamentals of graphical probabilistic modeling, including Bayesian networks, Markov random fields, hidden conditional Random field, etc. We also discuss some applications of graphical probabilistic modeling in multimedia content analysis, for example, video segmentation, video event detection, video sequence matching, and image labeling. The audience will learn the basic of graphical models, and get familiar with the state of the art in multimedia content analysis systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {637–638},
numpages = {2},
keywords = {video segmentation., hidden conditional random field model, multimedia content analysis, graphical probabilistic modeling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072405,
author = {Shen, Jialie and Wang, Meng and Yan, Shuicheng and Hua, Xian-Sheng},
title = {Multimedia Tagging: Past, Present and Future},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072405},
doi = {10.1145/2072298.2072405},
abstract = {The tags have proved to be a very crucial mechanism to facilitate the effective sharing and organization of large scale of multimedia information. As a result, technical developments on intelligent multimedia tagging have attracted a substantial amount of efforts involving experts from information retrieval, multimedia computing and artificial intelligence (particularly computer vision). The truly interdisciplinary research has resulted in many algorithmic and methodological developments. Meanwhile, many commercial web systems (e.g., Youtube, Last.fm and Flickr) have successfully introduced a variety of toolkits to assist different users in discovering and exploring media content using tags. This tutorial aims to provide a comprehensive coverage on the evolution of research for developing multimedia tagging technologies and identify a range of major challenges for the further scholarly study in the coming years.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {639–640},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072406,
author = {Katti, Harish and Kankanhalli, Mohan},
title = {Eye-Tracking Methodology and Applications to Images and Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072406},
doi = {10.1145/2072298.2072406},
abstract = {Our tutorial introduces eye-tracking as an exciting, non-intrusive method of capturing user attention during human interaction with digital images and videos. We believe eye-gaze can play a valuable role in understanding and processing (a) huge volumes of image and video content generated as a result of human experiences and interaction with the environment (b) Personalization and in human-media interaction, having access to individual preferences and behavioral patterns would be a key component of such a system. Recent possibilities to seamlessly integrate eye-tracking into laptops and mobile devices opens up a plethora of possibilities for applications that can respond to user's visual attention strategies. (c) Visual content design such as in advertising often employs techniques that guide user attention to produce visual impact and elements of surprise and emotion. Eye-gaze has been used as a tool to evaluate different choices of visual elements and their placement. (d) Affective analysis of images and videos is an ongoing and challenging area in multimedia research, show recent results on how eye-gaze and accompanying pupillary dilation information can aid affective analysis.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {641–642},
numpages = {2},
keywords = {visual attention, eye-tracking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072408,
author = {Kofler, Christoph and Larson, Martha and Hanjalic, Alan},
title = {Alice's Worlds of Wonder: Exploiting Tags to Understand Images in Terms of Size and Scale},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072408},
doi = {10.1145/2072298.2072408},
abstract = {The 'Wonderlands of Size and Scale' system extends Flickr search functionality with a mechanism that exploits information implicit in image tag sets to filter images on the basis of characteristics related to size and scale. The innovative contribution of the 'Wonderlands' system is threefold: first, its use of an understanding of real-world physical entities depicted in images in terms of size and scale to filter images for display to the user; second, its application of our recently proposed 'Reading between the Tags' approach, which infers the real-world size of physical objects depicted in images by combining user-assigned image tags and natural language statistics mined from the Web; and, third, its trim realization of an engaging application that is implemented in a light-weight manner such that it can be executed as a live extension to the Flickr search engine. Results of a simple system-oriented evaluation support the conclusion that 'Wonderlands' reaches its aim of presenting users with images sorted with respect to the size- and scale-characteristics of their visually depicted content.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {643–646},
numpages = {4},
keywords = {scale, tags, image understanding, real-world size},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072409,
author = {Wu, Guan-Long and Su, Yu-Chuan and Chiu, Tzu-Hsuan and Hsieh, Liang-Chi and Hsu, Winston H.},
title = {Scalable Mobile Video Question-Answering System with Locally Aggregated Descriptors and Random Projection},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072409},
doi = {10.1145/2072298.2072409},
abstract = {We present a scalable mobile video Question-Answering system with locally aggregated descriptors and random projection using user-generated videos all around the world for ACM Multimedia 2011 Technicolor challenge: "precise event recognition and description from video excerpts." Our proposed system takes a video excerpt as a query and explores its canonical semantics of that. We collect a public events video dataset containing 7 topics with 1963 YouTube videos to evaluate our proposed system. The experiment results show that our proposed video feature representation outperforms a state-of-the-art near-duplicate retrieval based on color histogram. The signature generated by random projection not only ensures real-time efficiency but also achieves a competitive MAP performance with original feature.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {647–650},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072410,
author = {Lei, Yu-Heng and Chen, Yan-Ying and Iida, Lime and Chen, Bor-Chun and Su, Hsiao-Hang and Hsu, Winston H.},
title = {Photo Search by Face Positions and Facial Attributes on Touch Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072410},
doi = {10.1145/2072298.2072410},
abstract = {With the explosive growth of camera devices, people can freely take photos to capture moments of life, especially the ones accompanied with friends and family. Therefore, a better solution to organize the increasing number of personal or group photos is highly required. In this paper, we propose a novel way to search for face images according facial attributes and face similarity of the target persons. To better match the face layout in mind, our system allows the user to graphically specify the face positions and sizes on a query "canvas," where each attribute or identity is defined as an "icon" for easier representation. Moreover, we provide aesthetics filtering to enhance visual experience by removing candidates of poor photographic qualities. The scenario has been realized on a touch device with an intuitive user interface. With the proposed block-based indexing approach, we can achieve near real-time retrieval (0.1 second on average) in a large-scale dataset (more than 200k faces in Flickr images).},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {651–654},
numpages = {4},
keywords = {block-based indexing, face attributes, face retrieval, touch-based user interface},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072411,
author = {Tan, Chun Chet and Jiang, Yu-Gang and Ngo, Chong-Wah},
title = {Towards Textually Describing Complex Video Contents with Audio-Visual Concept Classifiers},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072411},
doi = {10.1145/2072298.2072411},
abstract = {Automatically generating compact textual descriptions of complex video contents has wide applications. With the recent advancements in automatic audio-visual content recognition, in this paper we explore the technical feasibility of the challenging issue of precisely recounting video contents. Based on cutting-edge automatic recognition techniques, we start from classifying a variety of visual and audio concepts in video contents. According to the classification results, we apply simple rule-based methods to generate textual descriptions of video contents. Results are evaluated by conducting carefully designed user studies. We find that the state-of-the-art visual and audio concept classification, although far from perfect, is able to provide very useful clues indicating what is happening in the videos. Most users involved in the evaluation confirmed the informativeness of our machine-generated descriptions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {655–658},
numpages = {4},
keywords = {textual descriptions of video content, audio-visual concept classification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072412,
author = {Alexiadis, Dimitrios S. and Kelly, Philip and Daras, Petros and O'Connor, Noel E. and Boubekeur, Tamy and Moussa, Maher Ben},
title = {Evaluating a Dancer's Performance Using Kinect-Based Skeleton Tracking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072412},
doi = {10.1145/2072298.2072412},
abstract = {In this work, we describe a novel system that automatically evaluates dance performances against a gold-standard performance and provides visual feedback to the performer in a 3D virtual environment. The system acquires the motion of a performer via Kinect-based human skeleton tracking, making the approach viable for a large range of users, including home enthusiasts. Unlike traditional gaming scenarios, when the motion of a user must by kept in synch with a pre-recorded avatar that is displayed on screen, the technique described in this paper targets online interactive scenarios where dance choreographies can be set, altered, practiced and refined by users. In this work, we have addressed some areas of this application scenario. In particular, a set of appropriate signal processing and soft computing methodologies is proposed for temporally aligning dance movements from two different users and quantitatively evaluating one performance against another.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {659–662},
numpages = {4},
keywords = {signal processing, skeleton tracking, microsoft kinect},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072413,
author = {Tsai, Tsung-Hung and Cheng, Wen-Huang and Hsieh, Yung-Huan},
title = {Dynamic Social Network for Narrative Video Analysis},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072413},
doi = {10.1145/2072298.2072413},
abstract = {Narrative video analysis has attracted much research attention, for narrative scenes can provide meaningful representations of multimedia contents. To go beyond the limitations of content based appraoches, social network techniques was introduced in the literature to explore the high-level narrative structures by mining the relations between video characters. Taking into account the fact that such a social network is not static but changes over time as the video narrative evolves, in this work, we develop a novel social network model, namely dynamic social network, for capturing the spatiotemporal dynamics in the social network of video characters so as to enable the automatic segmentation of a video into a sequence of narrative scenes. The proposed approach is experimented with various genres of movies and the results demonstrate our effectiveness.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {663–666},
numpages = {4},
keywords = {video segmentation, social network, narrative analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072414,
author = {Gowing, Marc and Kell, Philip and O'Connor, Noel E. and Concolato, Cyril and Essid, Slim and Lefeuvre, Jean and Tournemenne, Robin and Izquierdo, Ebroul and Kitanovski, Vlado and Lin, Xinyu and Zhang, Qianni},
title = {Enhanced Visualisation of Dance Performance from Automatically Synchronised Multimodal Recordings},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072414},
doi = {10.1145/2072298.2072414},
abstract = {The Huawei/3DLife Grand Challenge Dataset provides multimodal recordings of Salsa dancing, consisting of audiovisual streams along with depth maps and inertial measurements. In this paper, we propose a system for augmented reality-based evaluations of Salsa dancer performances. An essential step for such a system is the automatic temporal synchronisation of the multiple modalities captured from different sensors, for which we propose efficient solutions. Furthermore, we contribute modules for the automatic analysis of dance performances and present an original software application, specifically designed for the evaluation scenario considered, which enables an enhanced dance visualisation experience, through the augmentation of the original media with the results of our automatic analyses.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {667–670},
numpages = {4},
keywords = {audio, synchronisation, multimodal processing, video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072415,
author = {Teixeira, Bruno do Nascimento and Oliveira, J\'{u}lia Epischina Engr\'{a}cia de and Souza, Fillipe Dias Moreira de and Cunha, Tiago Oliveira and Ara\'{u}jo, Arnaldo de Albuquerque and Okamoto, Christiane and Figueiredo, Lucas and Silva, Vin\'{\i}cius de Oliveira and Oliveira, Igor Calil Loures de},
title = {News Browsing System: Multimodal Analysis},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072415},
doi = {10.1145/2072298.2072415},
abstract = {This paper reports a system developed for video browsing based on multimodal analysis. Our multimodal approach performs audio transcription for shot categorization (sports, weather, politics and economy) combining audio and visual information for theme categorization. Its main features include static and dynamic summaries, segmentation using face detection, classification into Indoor/Outdoor scenes based on Support Vector Machine (SVM) and audio transcription for theme keyword search. Keywords are selected to represent the subjects, followed by a simple text search. We conduct a set of experiments for evaluating the effectiveness of the shot subject categorization using audio transcription information.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {671–674},
numpages = {4},
keywords = {multimodal analysis, multimedia user interfaces and interaction, video browsing, static and dynamic summarizations, face detection, video segmentation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072416,
author = {Essid, Slim and Grenier, Yves and Maazaoui, Mounira and Richard, Ga\"{e}l and Tournemenne, Robin},
title = {An Audio-Driven Virtual Dance-Teaching Assistant},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072416},
doi = {10.1145/2072298.2072416},
abstract = {This work addresses the Huawei/3Dlife Grand challenge proposing a set of audio tools for a virtual dance-teaching assistant. These tools are meant to help the dance student develop a sense of rhythm to correctly synchronize his/her movements and steps to the musical timing of the choreographies to be executed. They consist of three main components, namely a music (beat) analysis module, a source separation and remastering module and a dance step segmentation module. These components enable to create augmented tutorial videos highlighting the rhythmic information using, for instance, a synthetic dance teacher voice, but also videos highlighting the steps executed by a student to help in the evaluation of his/her performance.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {675–678},
numpages = {4},
keywords = {music analysis, grand challenge, source separation, remastering, audio, 3DLife},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072417,
author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Understanding Images with Natural Sentences},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072417},
doi = {10.1145/2072298.2072417},
abstract = {We propose a novel system which generates sentential captions for general images. For people to use numerous images effectively on the web, technologies must be able to explain image contents and must be capable of searching for data that users need. Moreover, images must be described with natural sentences based not only on the names of objects contained in an image but also on their mutual relations. The proposed system uses general images and captions available on the web as training data to generate captions for new images. Furthermore, because the learning cost is independent from the amount of data, the system has scalability, which makes it useful with large-scale data.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {679–682},
numpages = {4},
keywords = {probabilistic canonical correlation analysis, multi-stack decoding, similarity measure},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072418,
author = {Wu, Wanmin and Arefin, Ahsan and Kurillo, Gregorij and Agarwal, Pooja and Nahrstedt, Klara and Bajcsy, Ruzena},
title = {A Psychophysical Approach for Real-Time 3D Video Processing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072418},
doi = {10.1145/2072298.2072418},
abstract = {This paper presents a psychophysical approach to control a new factor called Color-plus-Depth Level-of-Detail in polygon-based 3D tele-immersive video. Based on our psychophysical study that demonstrates the existence of perceptual thresholds on the factor, we present a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage while considerably enhancing the overall perceived visual quality.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {683–686},
numpages = {4},
keywords = {tele-immersive video, adaptation, psychophysics, perception, level-of-detail, color-plus-depth},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072419,
author = {Kuo, Yin-Hsi and Lee, Wen-Yu and Hsu, Winston H. and Cheng, Wen-Huang},
title = {Augmenting Mobile City-View Image Retrieval with Context-Rich User-Contributed Photos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072419},
doi = {10.1145/2072298.2072419},
abstract = {With the growth of mobile devices, the needs for location-based services are emerging. Taking the advantage of the GPS information, we can roughly estimate a user's location. However, it is necessary to leverage extra information (e.g., photos) to precisely locate the object of interest through mobile devices for further applications such as mobile search. Users can simply take a picture (with GPS enabled) of an interesting target to retrieve the building information. Therefore, the raise of real-time building recognition or retrieval system becomes a challenging problem. The most recent approaches are to recognize buildings by the street-view images; however, the query photos from mobile devices usually contain different lighting conditions. In order to provide a more robust city-view image retrieval system, we propose to augment the visual diversity of database images by integrating the context-rich user-contributed photos from social media. Preliminary experimental results show that the street-view images can provide different angles of the target whereas the user-contributed photos can enhance the diversity of the target. Besides, for the real-time retrieval system, we also combine both visual and GPS constraints in the retrieval process on inverted indexing so that we can achieve a real-time retrieval system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {687–690},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072421,
author = {Hare, Jonathon S. and Samangooei, Sina and Dupplaw, David P.},
title = {OpenIMAJ and ImageTerrier: Java Libraries and Tools for Scalable Multimedia Analysis and Indexing of Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072421},
doi = {10.1145/2072298.2072421},
abstract = {OpenIMAJ and ImageTerrier are recently released open-source libraries and tools for experimentation and development of multimedia applications using Java-compatible programming languages. OpenIMAJ (the Open toolkit for Intelligent Multimedia Analysis in Java) is a collection of libraries for multimedia analysis. The image libraries contain methods for processing images and extracting state-of-the-art features, including SIFT. The video and audio libraries support both cross-platform capture and processing. The clustering and nearest-neighbour libraries contain efficient, multi-threaded implementations of clustering algorithms. The clustering library makes it possible to easily create BoVW representations for images and videos. OpenIMAJ also incorporates a number of tools to enable extremely-large-scale multimedia analysis using distributed computing with Apache Hadoop.ImageTerrier is a scalable, high-performance search engine platform for content-based image retrieval applications using features extracted with the OpenIMAJ library and tools. The ImageTerrier platform provides a comprehensive test-bed for experimenting with image retrieval techniques. The platform incorporates a state-of-the-art implementation of the single-pass indexing technique for constructing inverted indexes and is capable of producing highly compressed index data structures.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {691–694},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072422,
author = {Esteban, Isaac and Dijk, Judith and Groen, Frans C.A.},
title = {From Images to 3d Models Made Easy},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072422},
doi = {10.1145/2072298.2072422},
abstract = {FIT3D is a Toolbox built for Matlab that aims at unifying and distributing a set of tools that will allow the researcher to obtain a complete 3D model from a set of calibrated images. In this paper we motivate and present the structure of the toolbox in a tutorial and example based approach. Given its exibility and scope we believe that FIT3D represents an exciting opportunity for researchers that want to develop or test one particular method with real data without the need for extensive additional programming.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {695–698},
numpages = {4},
keywords = {3D modeling, reconstruction, vision, matlab, structure from motion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072423,
author = {Cazenave, Fabien and Quint, Vincent and Roisin, C\'{e}cile},
title = {Timesheets.Js: Tools for Web Multimedia},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072423},
doi = {10.1145/2072298.2072423},
abstract = {Timesheets.js is a JavaScript library for publishing multimedia web documents that take advantage of the new features of HTML5 and CSS3. The library allows web developers to extend their skills to synchronized multimedia contents. This technology has been experimented in a class where students had to implement an XSLT transformation for converting OpenOffice Impress presentations into web formats. The resulting slideshows run in web browsers thanks to the timesheets.js library.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {699–702},
numpages = {4},
keywords = {smil, declarative languages, html5, timesheets, multimedia web applications},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072424,
author = {Brooks, Christopher A. and Ketterl, Markus and Hochman, Adam and Holtzman, Josh and Stern, Judy and Wunden, Tobias and Amundson, Kristofor and Logan, Greg and Lui, Kenneth and McKenzie, Adam and Meyer, Denis and Moormann, Markus and Rihtar, Matjaz and Rolf, Ruediger and Skofic, Nejc and Sutton, Micah and Vazquez, Ruben Perez and Wulff, Benjamin},
title = {OpenCast Matterhorn 1.1: Reaching New Heights},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072424},
doi = {10.1145/2072298.2072424},
abstract = {This paper gives a short overview of the Opencast Matterhorn system. Built by an open community of individuals and institutions, Matterhorn provides a lecture capture platform for both research and production environments. Matterhorn is comprehensive and scalable, and includes components for the acquisition, processing, and playback of content. Matterhorn is licensed under the liberal Educational Community License (ECL 2.0), a flexible OSI approved open source license, and the Opencast community is free for all institutions, corporations, or individuals to join.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {703–706},
numpages = {4},
keywords = {and educational aspects of multimedia, multimedia systems, human, social, middleware},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072425,
author = {Park, Sung Hee and Adams, Andrew and Talvala, Eino-Ville},
title = {The FCam API for Programmable Cameras},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072425},
doi = {10.1145/2072298.2072425},
abstract = {The FCam API is an open-source camera control library, enabling precise control over a camera's imaging pipeline. Intended for researchers and students in the field of computational photography, it allows easy implementation of novel algorithms and applications. Currently implemented on the Nokia N900 smartphone, and a custom-built "Frankencamera", it has been used in teaching at universities around the world, and is freely available for download for the N900. This paper describes the architecture underlying the API, the design of the API itself, several applications built on top of it, and some examples of its use in education.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {707–710},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072426,
author = {Gorin, J\'{e}r\^{o}me and Yviquel, Herv\'{e} and Pr\^{e}teux, Fran\c{c}oise and Raulet, Micka\"{e}l},
title = {Just-in-Time Adaptive Decoder Engine: A Universal Video Decoder Based on MPEG RVC},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072426},
doi = {10.1145/2072298.2072426},
abstract = {In this paper, we introduce the Just-In-Time Adaptive Decoder Engine (Jade) project, which is shipped as part of the Open RVC-CAL Compiler (Orcc) project. Orcc provides a set of open-source software tools for managing decoders standardized within MPEG by the Reconfigurable Video Coding (RVC) experts. In this framework, Jade acts as a Virtual Machine for any decoder description that uses the MPEG RVC paradigm. Jade dynamically generates a native decoder representation suitable for X86, ARM and CELL platforms with a possibility of exploiting multi-core CPUs. Thus, according to the MPEG RVC decoder description coupled with a video coded stream, Jade can create, configure and re-configure video decompression algorithms adapting to the video content.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {711–714},
numpages = {4},
keywords = {scalable execution, decoder reconfiguration, adaptive decoding, mpeg rvc, multimedia application, dataflow programs},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072427,
author = {Le Feuvre, Jean and Concolato, Cyril and Dufourd, Jean-Claude and Bouqueau, Romain and Moissinac, Jean-Claude},
title = {Experimenting with Multimedia Advances Using GPAC},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072427},
doi = {10.1145/2072298.2072427},
abstract = {Multimedia applications are challenging software and require collaboration of very different components such as networking, rendering, or scripting to provide a nice user experience. GPAC is an open source multimedia framework that implements a vast number of components and helps experimenting with different types of multimedia applications. It provides a multimedia packager, some servers and a player for dynamic and interactive multimedia content. In this paper, we present the recent additions to the project which allow experimenting with new applications such as dynamic home networking or rich broadcasting of interactive content on the latest devices, including with 3D displays. Such experiments can be useful in research work and, as illustrated, in academic environment for educational purposes.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {715–718},
numpages = {4},
keywords = {Web3D, stereoscopy, streaming, broadcasting, home networking, BIFS, MPEG, interactivity, multimedia, SVG},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072428,
author = {Halawa, Sherif and Pang, Derek and Cheung, Ngai-Man and Girod, Bernd},
title = {ClassX: An Open Source Interactive Lecture StreamingSystem},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072428},
doi = {10.1145/2072298.2072428},
abstract = {The ClassX open source project is a free experimental interactive video streaming platform designed for educators, researchers and software developers. With minimal infra-structure set-up, ClassX offers educational communities a cost-effective solution for online lecture delivery. Our goal is to encourage contributions from other researchers, developers and educators in building an open, cost-effective and state-of-the-art online education video viewing system for the general public.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {719–722},
numpages = {4},
keywords = {interactive region-of-interest video streaming, electronic slide matching, auto-matic lecturer tracking, education},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072429,
author = {M\"{u}ller, Christopher and Timmerer, Christian},
title = {A VLC Media Player Plugin Enabling Dynamic Adaptive Streaming over HTTP},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072429},
doi = {10.1145/2072298.2072429},
abstract = {This paper describes the implementation of a VLC media player plugin enabling Dynamic Adaptive Streaming over HTTP (DASH). DASH is an emerging ISO/IEC MPEG and 3GPP standard for HTTP streaming. It aims to standardize formats enabling segmented progressive download by exploiting existing Internet infrastructure as such. Our implementation of these formats as described in this paper is based on the well-known VLC. Hence, it is fully integrated into the VLC structure and has been also submitted to the VLC development team for consideration in future releases of VLC. Therefore, it is licensed under the GNU Lesser General Public License (LGPL). The plugin provides a very flexible structure that could be easily extended with respect to different adaptation logics or profiles of the DASH standard. As a consequence, the plugin enables the integration of a variety of adaptation logics and comparison thereof, making it attractive for the research community.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {723–726},
numpages = {4},
keywords = {DASH, video, MPEG, 3GPP, dynamic adaptive streaming over HTTP, HTTP streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072430,
author = {Barrios, Andr\'{e}s and Barrios, Mat\'{\i}as and De Vera, Daniel and Rodr\'{\i}guez-Bocca, Pablo and Rostagnol, Claudia},
title = {GoalBit: A Free and Open Source Peer-to-Peer Streaming Network},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072430},
doi = {10.1145/2072298.2072430},
abstract = {This paper presents the GoalBit Starter platform. GoalBit is an open source peer-to-peer distribution system of real-time video streams over Internet. The main advantage of a P2P architecture is the possibility of using available upload bandwidth in the hosts connected. The main difficulty is that these hosts are typically highly dynamic, they continuously enter and leave the network. To deal with this problem a mesh connectivity approach is used (Bittorrent-like) where the stream is decomposed into several pieces and shared between different peers. Nowadays, the GoalBit platform is used by operators and by final users to broadcast their live contents. To illustrate its potential, we present some empirical results measured in an emulation of a GoalBit P2P streaming, with more than 300 peers concurrently connected.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {727–730},
numpages = {4},
keywords = {video, streaming, peer-to-peer},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072431,
author = {Bailer, Werner and F\"{u}rntratt, Hermann and Schallauer, Peter and Thallinger, Georg and Haas, Werner},
title = {A C++ Library for Handling MPEG-7 Descriptions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072431},
doi = {10.1145/2072298.2072431},
abstract = {We present a C++ library implementing part 2, 3, 4 and 5 of the MPEG 7 multimedia content description standard, including the updated version finalized in 2004. The library supports handling of MPEG-7 descriptions as trees of typed objects, supporting (de)serialization from/to XML. It has convenient and powerful features such as creation of subtrees by XPath statements and is extensible at runtime. The library is available for Windows, Linux/Unix and Mac OS X. It has been provided under a free use license for several years, downloaded more than 5,200 times and used in a large number of projects. It has been published under GNU LGPL in 2009. This paper discusses the key functionalities of the library as well as some exemplary applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {731–734},
numpages = {4},
keywords = {description scheme, library, api, metadata, MPEG-7, xpath},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072432,
author = {Lux, Mathias},
title = {Content Based Image Retrieval with LIRe},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072432},
doi = {10.1145/2072298.2072432},
abstract = {LIRe (Lucene Image Retrieval) is an open source library for content based image retrieval. Besides providing multiple common and state of the art retrieval mechanisms it allows for easy use on multiple platforms. LIRe is actively used for research, teaching and commercial applications. Due to its modular nature it can be used on process level (e.g. index images and search) as well as on image feature level. Developers and researchers can easily extend and modify LIRe to adapt it to their needs.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {735–738},
numpages = {4},
keywords = {image features, image search, image indexing, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072433,
author = {Zeilemaker, Niels and Capot\u{a}, Mihai and Bakker, Arno and Pouwelse, Johan},
title = {Tribler: P2P Media Search and Sharing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072433},
doi = {10.1145/2072298.2072433},
abstract = {Tribler is an open-source software project facilitating search, streaming and sharing content using P2P technology. Over 800,000 people have used Tribler since the project started in 2005. The Tribler P2P core supports BitTorrent-compatible downloading, video on demand and live streaming. Aside from a regular desktop GUI that runs on multiple OSes, it can be installed as a browser plug-in, currently used by Wikipedia. Aditionally, it runs on a 450~MHz processor, showcasing future TV support. We continuously work on extensions and test out novel research ideas within our user base, resulting in sub-second content search, a reputation system for rewarding upload, and channels for content publishing and spam prevention. Presently, 1200 channels have been created, enabling rich multimedia communities without requiring any server.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {739–742},
numpages = {4},
keywords = {Tribler, file sharing, gossip, video on demand, BitTorrent},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072434,
author = {Bresson, Jean and Agon, Carlos and Assayag, G\'{e}rard},
title = {OpenMusic: Visual Programming Environment for Music Composition, Analysis and Research},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072434},
doi = {10.1145/2072298.2072434},
abstract = {OpenMusic is an open source environment dedicated to music composition. The core of this environment is a full-featured visual programming language based on Common Lisp and CLOS (Common Lisp Object System) allowing to design processes for the generation or manipulation of musical material. This language can also be used for general purpose visual programming and other (possibly extra-musical) applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {743–746},
numpages = {4},
keywords = {visual programming, music, computer-aided composition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072436,
author = {Law-To, Julien and Grefenstette, Gregory},
title = {VOVALEAD: A Scalable Video Search Engine Based on Content},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072436},
doi = {10.1145/2072298.2072436},
abstract = {Most news organizations provide immediate access to topical news broadcasts through RSS streams or podcasts. Until recently, applications have not permitted a user to perform content based search within a longer spoken broadcast to find the segment that might interest them. Recent progress in both automatic speech recognition (ASR) and natural language processing (NLP) has produced robust tools that allow us to now provide users with quicker and more focused access to relevant segments of news broadcast videos. Our public online demonstrator of the Voxalead application currently indexes daily broadcast news content from 50 sources in English, French, Chinese, Arabic, Spanish, Dutch, Italian and Russian.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {747–748},
numpages = {2},
keywords = {podcast search engine, content based search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072437,
author = {Blouet, Rapha\"{e}l and Juan, Charlotte},
title = {MMSI Talk: An Applicative Use Case of Quaero Media Monitoring &amp; Social Impact},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072437},
doi = {10.1145/2072298.2072437},
abstract = {MMSI (Media Monitoring: Social impact) is an applicative project that is part of the European project Quaero. MMSI aims at developing a cross-media analysis and monitoring platform that enables to measure and understand information life-cycle within a set of radio, tv, newspaper and web sources. MMSI allows to quickly build ad-hoc web applications that implements study according to customers needs. We here present MMSI Talk, a platform for quantitative and qualitative analysis of political talks. MMSI Talk is a web interface that allows a global monitoring of broadcasted political content over a set of french radio. MMSI Talk allows intuitive search through queries by media, named entities, political party and/or politician. It includes access to talks, cumulative measure of speech duration at the politician or political party level. Moreover MMSI Talk provides a set of statistical and visualization tools that allow an advance understanding of political communication strategies. We first describe the context and motivation of the MMSI Talk talk platform. We then describe the global architecture of the platform before presenting the user interface through several concrete use cases.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {749–750},
numpages = {2},
keywords = {information retrieval, media monitoring, speech transcription},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072438,
author = {Wang, Jingdong and Hua, Xian-Sheng},
title = {Web-Scale Image Search by Color Sketch},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072438},
doi = {10.1145/2072298.2072438},
abstract = {Most existing image search engines rely on the associated texts or tags with images to index and retrieval images, which results in limited ability on searching images with visual requirement. In this demonstration, we present an image search system, which enables consumers to find images on the requirement of how the colors are spatially distributed. It is a well-designed trade-off between scalability and feasibility. To the best knowledge, this system is the first one to scale up to Web-scale images. The interface is very intuitive and requires users to only scribble a few color strokes or drag an image and mask a few regions of interest, to express the search intent.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {751–752},
numpages = {2},
keywords = {color sketch, image search, web-scale},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072439,
author = {Lenoir, Arthur and Landais, R\'{e}mi},
title = {MuMa: A Scalable Music Search Engine Based on Content Analysis},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072439},
doi = {10.1145/2072298.2072439},
abstract = {Existing music search engines process music as they would process text, ignoring the richness of this media. We introduce here MuMa1 (MUsic MAshup), a web application which makes the most of audio contents (the signal itself) to provide the user with new ways of browsing music. MuMa allows the user to search for particular chords sequences, to generate smart playlists and to extend its musical universe.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {753–754},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072440,
author = {Wengert, Christian and Jaeggli, Tobias and Messmer, Philippe and Quack, Till and Cech, Peter and Prodan, Cristi and Carnecky, Tomas and Sebregondi, Franco and Wisti, David},
title = {Kooaba Interactive Posters},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072440},
doi = {10.1145/2072298.2072440},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {755–756},
numpages = {2},
keywords = {image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072442,
author = {Gong, Boqing and Liu, Jianzhuang and Wang, Xiaogang and Tang, Xiaoou},
title = {3D Object Retrieval with Semantic Attributes},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072442},
doi = {10.1145/2072298.2072442},
abstract = {Humans are capable of describing objects using attributes, such as "the object looks circular and is man-made". Motivated by these high-level descriptions, we build a user-friendly 3D object retrieval system, where the user can browse the database and search for targeted objects using semantic attributes. The main advantage of our system is that it does not require the user to find or sketch a 3D object as the query for 3D object retrieval. Besides, to the best of our knowledge, our system has obtained the best retrieval performance on three popular benchmarks.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {757–758},
numpages = {2},
keywords = {attribute, 3D object retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072443,
author = {Ren, Zhou and Meng, Jingjing and Yuan, Junsong and Zhang, Zhengyou},
title = {Robust Hand Gesture Recognition with Kinect Sensor},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072443},
doi = {10.1145/2072298.2072443},
abstract = {Hand gesture based Human-Computer-Interaction (HCI) is one of the most natural and intuitive ways to communicate between people and machines, since it closely mimics how human interact with each other. In this demo, we present a hand gesture recognition system with Kinect sensor, which operates robustly in uncontrolled environments and is insensitive to hand variations and distortions. Our system consists of two major modules, namely, hand detection and gesture recognition. Different from traditional vision-based hand gesture recognition methods that use color-markers for hand detection, our system uses both the depth and color information from Kinect sensor to detect the hand shape, which ensures the robustness in cluttered environments. Besides, to guarantee its robustness to input variations or the distortions caused by the low resolution of Kinect sensor, we apply a novel shape distance metric called Finger-Earth Mover's Distance (FEMD) for hand gesture recognition. Consequently, our system operates accurately and efficiently. In this demo, we demonstrate the performance of our system in two real-life applications, arithmetic computation and rock-paper-scissors game.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {759–760},
numpages = {2},
keywords = {finger-earth mover's distance, human-computer-interaction, hand gesture recognition, kinect sensor},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072444,
author = {Shen, Zhijie and Arslan Ay, Sakire and Kim, Seon Ho},
title = {SRV-TaGS: An Automatic TAGging and Search System for Sensor-Rich Outdoor Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072444},
doi = {10.1145/2072298.2072444},
abstract = {Tagging facilitates video search in many social media and web applications. While manual tagging is time consuming, subjective and sometimes inaccurate, auto-tagging facilitated by content-based techniques is compute-intensive and challenging to apply across domains. We have developed a complementary system, named SRV-TAGS, to automatically generate tags for outdoor videos based on their geographic properties, to index the videos based on their generated tags and to provide textual search services. The system works with our geo-referenced video management web portal, enabling users to manage, search and watch videos.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {761–762},
numpages = {2},
keywords = {video tags, location sensors, geospatial, mobile video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072445,
author = {Otsuka, Kazuhiro and Mucha, Kamil Sebastian and Kumano, Shiro and Mikami, Dan and Matsuda, Masafumi and Yamato, Junji},
title = {A System for Reconstructing Multiparty Conversation Field Based on Augmented Head Motion by Dynamic Projection},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072445},
doi = {10.1145/2072298.2072445},
abstract = {A novel system is presented for reconstructing, in the real world, multiparty face-to-face conversation scenes; it uses dynamics projection to augment human head motion. This system aims to display and playback pre-recorded conversations to the viewers as if the remote people were taking in front of them. This system consists of multiple projectors and transparent screens. Each screen separately displays the life-size face of one meeting participant, and are spatially arranged to recreate the actual scene. The main feature of this system is dynamics projection, screen pose is dynamically controlled to emulate the head motions of the participants, especially rotation around the vertical axis, that are typical of shifts in visual attention, i.e. turning gaze from one to another. This recreation of head motion by physical screen motion, in addition to image motion, aims to more clearly express the interactions involving visual attention among the participants. The minimal design, frameless-projector-screen, with augmented head motion is expected to create a feeling that the remote participants are actually present in the same room. This demo presents our initial system and discusses its potential impact on future visual communications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {763–764},
numpages = {2},
keywords = {telepresence, face-to-face conversation, visual attention, multimodal interaction, visual communication, projector system},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072446,
author = {Zingerle, Andreas and Freeman, Tyler},
title = {Enabling the VJ as Performer with Rhythmic Wearable Interfaces},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072446},
doi = {10.1145/2072298.2072446},
abstract = {This paper proposes an experimental wearable controller for live video performances called the VJacket. The VJacket can be worn by the performer or visual artist (VJ) to control video effects and transitions, trigger clips or scratch frames with the output of the integrated sensor system. The sensors detect body movements like bending, touching or hitting, and can send OpenSoundControl or MIDI messages wirelessly to the VJ program of your choice. The VJacket brings the rhythmic movement of dance to computer interaction, so the VJ won't have to fumble for knobs and buttons or look at the screen to be sure he's clicking on the right thing - he will be free to control the video using his body movements alone.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {765–766},
numpages = {2},
keywords = {real-time image control, vj, entertainment computing, wearable sensors, live video performance, mobility},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072447,
author = {Sadlier, David and Ferguson, Paul and Zhang, Dian and O'Connor, Noel E. and Lee, Hyowon},
title = {InSPeCT: Integrated Surveillance for Port Container Traffic},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072447},
doi = {10.1145/2072298.2072447},
abstract = {This paper describes a fully-operational content-indexing and management system, designed for monitoring and profiling freight-based vehicular traffic in a seaport environment. The 'InSPeCT' system captures video footage of passing vehicles and uses tailored OCR to index the footage according to vehicle license plates and freight codes. In addition to real-time functionality such as alerting, the system provides advanced search techniques for the efficient retrieval of records, where each vehicle is profiled according to multi-angled video, context information, and links to external information sources. Currently being piloted at a busy national seaport, the feedback from port officials indicates the system to be extremely useful in supplementing their existing transportation-security structures.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {767–768},
numpages = {2},
keywords = {indexing, OCR, search and retrieval, content analysis, HCI},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072448,
author = {Venkatesh, Svetha and Greenhill, Stewart and Phung, Dinh and Adams, Brett},
title = {Cognitive Intervention in Autism Using Multimedia Stimulus},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072448},
doi = {10.1145/2072298.2072448},
abstract = {We demonstrate an open multimedia-based system for delivering early intervention therapy for autism. Using flexible multi-touch interfaces together with principled ways to access rich content and tasks, we show how a syllabus can be translated into stimulus sets for early intervention. Media stimuli are able to be presented agnostic to language and media modality due to a semantic network of concepts and relations that are fundamental to language and cognitive development, which enable stimulus complexity to be adjusted to child performance. Being open, the system is able to assemble enough media stimuli to avoid children over-learning, and is able to be customised to a specific child which aids with engagement. Computer-based delivery enables automation of session logging and reporting, a fundamental and time-consuming part of therapy.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {769–770},
numpages = {2},
keywords = {autism, assistive technology, multimedia stimulus, therapy},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072449,
author = {Yamada, Takayuki and Gohshi, Seiichi and Echizen, Isao},
title = {ICabinet: Stand-Alone Implementation of a Method for Preventing Illegal Recording of Displayed Content by Adding Invisible Noise Signals},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072449},
doi = {10.1145/2072298.2072449},
abstract = {A previously proposed method for preventing the illegal recording of displayed content by adding invisible noise signals to the recorded content during recording to make it unrecognizable has now been implemented in a stand-alone cabinet. Testing using the prototype stand-alone implementation demonstrated that it can effectively prevent illegal recording of displayed content by completely corrupting the recorded content. This illegal recording prevention method is applicable not only to content displayed on a screen but also to other things that should be protected against illegal recording such as confidential documents in a public office and art objects in a museum.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {771–772},
numpages = {2},
keywords = {illegal recording, infrared LED, copyright violation, information leakage, content display},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072450,
author = {Dong, Jian and Ni, Yuzhao and Feng, Jiashi and Yan, Shuicheng},
title = {Purposive Hidden-Object Game (P-HOG) towards Imperceptible Human Computation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072450},
doi = {10.1145/2072298.2072450},
abstract = {This demonstration presents a novel purposive web game on iOS to collect object locations, named Purposive Hidden-Object-Game(P-HOG). Having sufficient training images with known object locations is crucial for many multimedia tasks. P-HOG imperceptibly embeds localizing objects into the gaming process so that P-HOG preserves attractiveness to common players. During the game, players need to localize both automatically inserted known items and unknown objects (which are aimed to localize) to gain scores. The unknown objects are indicated by the refined user-provided tags from photo sharing websites. The difficulty mainly lies in how to insert known items naturally and thus preserve the game's playability. The P-HOG can be applied for constructing a large database, which contains located objects and may benefit general learning-based algorithms for multimedia tasks. The comprehensive experiments shows that P-HOG appeals to general players and can easily perform quality control, and hence effective for collecting massive object locations with high accuracy.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {773–774},
numpages = {2},
keywords = {gwap, human computing, hidden object game},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072451,
author = {Guan, Genliang and Wang, Zhiyong and Hua, Xian-Sheng and Feng, Dagan},
title = {<i>StoryImaging</i>: A Media-Rich Presentation System for Textual Stories},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072451},
doi = {10.1145/2072298.2072451},
abstract = {In this demo, we develop the StoryImaging system to illustrate a textual story with both images harvested from the Web and synthesized speech. At the backend, a story is firstly processed to identify key terms such as named entities and to obtain the story summary. With the aid of commercial search engines, images are then collected from the Web for those key terms and re-ranked by taking the summary as context. At last, images are clustered to provide an overview of the story. At the web-based frontend, the user interface has been tailored to both improve information comprehension and provide engaging and explorative experiences for users by closely bridging textual and visual modalities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {775–776},
numpages = {2},
keywords = {story imaging, clustering, cross-media, story illustration, re-ranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072452,
author = {Zhang, Ning and Mei, Tao and Hua, Xian-Sheng and Guan, Ling and Li, Shipeng},
title = {TapTell: Understanding Visual Intents on-the-Go},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072452},
doi = {10.1145/2072298.2072452},
abstract = {This demonstration presents a mobile-based visual recognition and recommendation application on Windows Phone 7 called TapTell. This is different from other mobile-based visual search mechanisms which merely focus on the search process. TapTell firstly discovers and understands users' visual intents via a circle based natural user interaction called "O" gestures. Following, a Tap action is operated to choose the "O" gestured regions. The context-aware visual search mechanism is utilized for recognizing the intents and associating them with indexed metadata. Finally, the "Tell" action recommends relevant entities utilizing contextual information. The TapTell system has been evaluated at different scenarios on million scale images.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {777–778},
numpages = {2},
keywords = {mobile visual search, mobile recommendation, visual intent},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072453,
author = {Meixner, Britta and K\"{o}stler, Johannes and Kosch, Harald},
title = {A Mobile Player for Interactive Non-Linear Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072453},
doi = {10.1145/2072298.2072453},
abstract = {With rapid improvements in available bandwidth and processing power of smart phones, the presentation of interactive non-linear video became possible. Web players for these form of video already exist, they usually have a video area and one or more annotation areas. Most of them can not be used for mobile phones because of their space consuming representation. This work presents a player to display interactive nonlinear videos on Android smart phones. The small display size and the possibility to switch between portrait and landscape mode have been taken into account. The player also provides functions to manage a library of downloaded interactive videos for off line use as well as bookmarks.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {779–780},
numpages = {2},
keywords = {mobile video player, video annotations, interactive video, non-linear video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072454,
author = {Velardo, Carmelo and Dugelay, Jean-Luc},
title = {Real Time Extraction of Body Soft Biometric from 3D Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072454},
doi = {10.1145/2072298.2072454},
abstract = {In this technical demonstration, we show the application of our research on body soft biometrics. Exploiting a 3D video sensor we are able to extract semantic information that describes subjects standing in front of a camera. Semantic analysis and tracking is performed, a series of anthropometric measures are extracted and used to compute subjects' height, weight, and gender information. Possible applications of such research fall in the medical domain for monitoring elderly people; in the gaming industry for automatic avatar creation, or in smart billboards which collects demographics of the public interested by the commercial. Our algorithm allows the estimation of all these parameters in real time without requiring the computational complexity of a 3D model fitting approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {781–782},
numpages = {2},
keywords = {anthropometric measures, user profiling, kinect, soft biometrics},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072455,
author = {Arefin, Ahsan and Huang, Zixia and Rivas, Raoul and Shi, Shu and Wu, Wanmin and Nahrstedt, Klara},
title = {Tele-Immersive Gaming for Everybody},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072455},
doi = {10.1145/2072298.2072455},
abstract = {In this demonstration, we present two 3D tele-immersive games: light-saber dual and block fencing that merge 3D video representations of participants in real-time to enable remote interactions in a virtual world. The light-saber dual arranges participants in a symmetric setup where both participants interact with each other in a virtual world with similar goals. On the other hand, the block fencing creates an asymmetric setup where participants interact with virtual objects having different goals. Using these two setups, we address the challenges and novelty of our solutions in portable environment setup, data acquisition, multi-stream synchronization, multi-stream session management, mobile device rendering, and overlay communication in the design and implementation of advanced 3D tele-immersive systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {783–784},
numpages = {2},
keywords = {tele-immersion, portability, gaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072456,
author = {Siwiak, Diana and Lehrer, Nicole and Baran, Michael and Chen, Yinpeng and Duff, Margaret and Ingalls, Todd and Rikakis, Thanassis},
title = {A Home-Based Adaptive Mixed Reality Rehabilitation System},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072456},
doi = {10.1145/2072298.2072456},
abstract = {This paper presents an interactive home-based adaptive mixed reality system (HAMRR) for upper extremity stroke rehabilitation. This home-based system is an extension of a previously designed and currently implemented clinical system. The goal of HAMRR is to restore motor function to chronic stroke survivors by providing an engaging long-term reaching task therapy at home. The HAMMR system tracks movement of the wrist and torso, and provides real-time, post-trial, and post-set multimodal feedback to encourage the stroke survivor to self-assess his or her movement and engage in active learning of new movement strategies. This experiential media system uses a computational adaptation scheme to create a continuously challenging and unique multi-year therapy experience through the use of multiple, integrated audio and visual feedback streams. Novel design features include creating an over-arching story for the participant, the ability of the system to adapt the feedback over multiple time scales, and the ability for this system to integrate into any home.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {785–786},
numpages = {2},
keywords = {affective multimedia, self-evaluation, long-term usage, mixed reality, adaptation, interactive, home healthcare, rehabilitation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072457,
author = {Pang, Derek and Halawa, Sherif and Cheung, Ngai-Man and Girod, Bernd},
title = {ClassX Mobile: Region-of-Interest Video Streaming to Mobile Devices with Multi-Touch Interaction},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072457},
doi = {10.1145/2072298.2072457},
abstract = {Small screen sizes, limited bandwidth, and low computational power often prohibit streaming of high-resolution videos to mobile devices over a wireless network. Recent advances in interactive region-of-interest (IRoI) video streaming technology enable users to interactively control pan/tilt/ zoom, while provide bit-rate and complexity savings. One recent application of IRoI video streaming is ClassX developed at Stanford University. It offers an open-source experimental platform for interactive online lecture streaming. In this technical demonstration, we present ClassX Mobile, which extends the current ClassX system and delivers high-quality interactive video to smartphones and tablets with multi-touch screens.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {787–788},
numpages = {2},
keywords = {mobile video, iroi video streaming, online lecture delivery},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072458,
author = {Wang, Changhu and Zhang, Jun and Yang, Bruce and Zhang, Lei},
title = {Sketch2Cartoon: Composing Cartoon Images by Sketching},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072458},
doi = {10.1145/2072298.2072458},
abstract = {In this paper, we introduce the Sketch2Cartoon system, which is an automatic cartoon making system by leveraging a novel sketch-based clipart image search engine. Different from existing work, most of which either limited users to the pre-prepared characters or only used keyword queries to search materials, Sketch2Cartoon enables users to sketch major curves of characters and props in their mind, and real-time search results from millions of clipart images could be selected to compose the cartoon images. The selected components are vectorized and thus could be further edited. By enabling sketch-based input, the cartoon image making process becomes more natural, and even a child who is too young to read or write can draw whatever he/she imagines and get interesting cartoon images.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {789–790},
numpages = {2},
keywords = {sketch2cartoon, search, clipart, sketching},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072459,
author = {Seo, Beomjoo and Hao, Jia and Wang, Guanfeng},
title = {Sensor-Rich Video Exploration on a Map Interface},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072459},
doi = {10.1145/2072298.2072459},
abstract = {Result presentations from searches into video repositories is still a challenging problem. Current systems usually display a ranked list that shows the first frame of each video. Users then explore the videos one-by-one. In our recent work we have investigated the fusion of captured video with a continuous stream of sensor meta-data. These so-called sensor-rich videos can conveniently be captured with today's smartphones. Importantly, the recorded sensor-data streams enable processing and result resentation in novel and useful ways.In this demonstration we present a system that provides an integrated solution to present videos based on keyframe extraction and interactive, map-based browsing. As a key feature, the system automatically computes popular places based on the collective information from all the available videos. For each video it then extracts keyframes and renders them at their proper location on the map synchronously with the video playback. All the processing is performed in real-time, which allows for an interactive exploration of all the videos in a geographic area.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {791–792},
numpages = {2},
keywords = {web services, keyframe extraction, hotspot estimation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072460,
author = {Au, Andrew and Liang, Jie},
title = {Ztitch: A Mobile Phone Application for 3D Scene Creation, Navigation, and Sharing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072460},
doi = {10.1145/2072298.2072460},
abstract = {Modern smartphones provide an excellent platform for creating 3D scenes from photos. While there already exists many mobile applications that can stitch a set of photos to create a single, panoramic landscape photo, this paper proposes the creation of panoramic scenes where multiple photos are projected in a 3D space using the pinhole camera model, so that a realistic perspective of the scene is maintained. Our application allows users to automatically create a panoramic 3D scene in real-time using the live images from the phone's camera, and to manually fine-tune each photo's position via the touchscreen if the default position is inaccurate. The application enables scenes to be easily shared to other users, and was developed using the Silverlight framework so that it can run across multiple platforms.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {793–794},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072461,
author = {Myodo, Emi and Ueno, Satoshi and Takagi, Koichi and Sakazawa, Shigeyuki},
title = {Automatic Comic-like Image Layout System Preserving Image Order and Important Regions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072461},
doi = {10.1145/2072298.2072461},
abstract = {In this paper, we present a practical and automatic comic-like image layout system for event photos or event movies. We focus on the comic-like layout with cropping. The comic-like layouts allow viewers to track events in time sequential order like comic books, in which event images are cropped and arranged in panels from top left to bottom right on a screen so that the resultant layout has variety of image shapes and gives a pleasant impression. The main drawback of the conventional method is that an important region such as a face or a salient object area is sometimes cropped. In order to avoid excessive cropping of important regions while maintaining the image order, our system achieves adaptive layout to input images by generating all possible templates suitable for comic and deciding appropriate templates based on our proposed layout evaluation. In the demo, we will show that our full automatic system generates comic-like layouts immediately. Furthermore, the system allows a user to select his/her favorite layout from the multiple ones with different looks.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {795–796},
numpages = {2},
keywords = {image layout, viewing system, tree enumeration, comic, cropping},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072463,
author = {Bouillet, Eric and Gasparini, Luca and Verscheure, Olivier},
title = {Towards a Real Time Public Transport Awareness System: Case Study in Dublin},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072463},
doi = {10.1145/2072298.2072463},
abstract = {In this paper we discuss our experience with the design of a public transport awareness application developed for the city of Dublin. The application is capable to ingest, analyze and visualize in real-time high volumes of traffic data coming from a variety of sources. We address the challenges encountered during the design of the application and propose novel solutions to tackle those challenges.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {797–798},
numpages = {2},
keywords = {transport},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072464,
author = {Golovchinksy, Gene and Carter, Scott and Dunnigan, Anthony},
title = {ARA: The Active Reading Application},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072464},
doi = {10.1145/2072298.2072464},
abstract = {The Active Reading Application (ARA) brings the familiar experience of writing on paper to the tablet. The application augments paper-based practices with audio, the ability to review annotations, and sharing. It is designed to make it easier to review, annotate, and comment on documents by individuals and groups. ARA incorporates several patented technologies and draws on several years of research and experimentation.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {799–800},
numpages = {2},
keywords = {annotation, mobile, reading},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072465,
author = {Biehl, Jacob T. and Turner, Thea and van Melle, William and Girgensohn, Andreas},
title = {MyUnity: A New Platform to Support Communication in the Modern Workplace},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072465},
doi = {10.1145/2072298.2072465},
abstract = {Modern office work practices increasingly breach traditional boundaries of time and place, making it difficult to interact with colleagues. To address these problems, we developed myUnity, a software and sensor platform that enables rich workplace awareness and coordination. myUnity is an integrated platform that collects information from a set of independent sensors and external data aggregators to report user location, availability, tasks, and communication channels. myUnity's sensing architecture is component-based, allowing channels of awareness information to be added, updated, or removed at any time. Our current system includes a variety of sensor and data input, including camera-based activity classification, wireless location trilateration, and network activity monitoring. These and other input channels are combined and composited into a single, highlevel presence state. Early studies of a myUnity deployment have demonstrated that use of the platform allows quick access to core awareness information and show its utility in supporting communication and collaboration in the modern workplace.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {801–802},
numpages = {2},
keywords = {mobile communication, workplace studies, awareness},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072467,
author = {Pang, Lei and Tan, Song and Tan, Hung Khoon and Ngo, Chong Wah},
title = {Galaxy Browser: Exploratory Search of Web Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072467},
doi = {10.1145/2072298.2072467},
abstract = {Most search engines return a ranked list of items in response to a query. The list however tells very little about the relationship among items. For videos especially, users often read to spend significant amount of time to navigate the search result. Exploratory search presents a new paradigm for browsing where the browser takes up the role of information exploring and presents a well-organized browsing structure for users to navigate. The proposed interface Galaxy Browser adopts the recent advances in near-duplicate detection and then synchronizes the detected near-duplicate information with comprehensive background knowledge derived from online external resources. The result is a topic structure on which users can easily browse and explore.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {803–804},
numpages = {2},
keywords = {information visualization, summarization, video browsing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072468,
author = {Yu, Felix X. and Ji, Rongrong and Zhang, Tongtao and Chang, Shih-Fu},
title = {A Mobile Location Search System with Active Query Sensing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072468},
doi = {10.1145/2072298.2072468},
abstract = {How should the second query be taken once the first query fails in mobile location search based on visual recognition? In this demo, we describe a mobile search system with a unique Active Query Sensing (AQS) function to intelligently guide the mobile user to take a successful second query. This suggestion is built upon a scalable visual matching system covering over 0.3 million street view reference images in New York City, where each location is associated with multiple surrounding views and panorama. In online search, once the initial search result fails, the system will perform online analysis and suggest the mobile user to turn to the most discriminative viewing angle to take the second visual query, from which the search performance is expected to greatly improve. The AQS suggestion is based on both offline salient view discovery and online viewing angle prediction and intelligent turning decision. Our experiments show our AVS can improve the mobile location search with a performance gain as high as 100%, reducing the failure rate to only 12% after taking the second visual query.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {805–806},
numpages = {2},
keywords = {mobile visual search, mobile location recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072469,
author = {Mehm, Florian and Hardy, Sandro and G\"{o}bel, Stefan and Steinmetz, Ralf},
title = {Collaborative Authoring of Serious Games for Health},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072469},
doi = {10.1145/2072298.2072469},
abstract = {The efficient production of Serious Games typically requires the collaboration of technical and game development experts, i.e. game developers and domain experts such as pedagogues or sports experts. For the use case of exergames with educational aspects, we demonstrate how an authoring tool for Serious Games can be specialized for collaborative authoring by defining roles of users and providing different views on the created game to each user group carrying out different tasks in the production of the game.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {807–808},
numpages = {2},
keywords = {serious games, authoring tool, collaborative authoring, multi-user software, exergame},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072470,
author = {Li, Haojie and Yi, Lei and Tang, Jinhui and Wang, Xiaohui},
title = {Capturing a Great Photo via Learning from Community-Contributed Photo Collections},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072470},
doi = {10.1145/2072298.2072470},
abstract = {We present a novel system, named PhotoReference, to help users, especially amateur photographers, to interactively learn to improve photograph skills by leveraging the available web image collections. The idea behind is based on the observation that the quality of a photo is mainly determined with the important camera parameters set by users when the photo is taken. In the proposed system, given a user-provided photo and its associated metadata, the system first retrieves sets of high-quality community-contributed photos with similar visual content and shoot settings to the input photo. Then users are allowed to interactively explore the returned photos as per different camera parameters, such that they can easily share and learn the photograph experiences of other people; and more importantly, users are able to get intuitive knowledge on the relationship between camera parameters and the resulted shooting effects and thus can use it to guide their future photograph activities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {809–810},
numpages = {2},
keywords = {view categorization, photography, EXIF},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072471,
author = {Piacenza, Alberto and Guerrini, Fabrizio and Adami, Nicola and Leonardi, Riccardo and Teutenberg, Jonathan and Porteous, Julie and Cavazza, Marc},
title = {Changing Video Arrangement for Constructing Alternative Stories},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072471},
doi = {10.1145/2072298.2072471},
abstract = {Currently, automatic generation of filmic variants faces a number of key technical issues and thus it usually resorts to the shooting of multiple versions of alternative scenes. However, recent advancements in video analysis has made this objective feasible, though semantic consistency must be somehow preserved. This demo presents a video-based storytelling (VBS) system that successfully integrates video processing with narrative generation by means of a shared semantic description. The novel filmic variants are constructed through a flexible video recombination process that takes advantage of the polysemy of baseline video segments. The short output video clips shown in this demo prove how the generated narratives are semantically consistent while keeping generative power intact.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {811–812},
numpages = {2},
keywords = {interactive storytelling, narrative modeling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072472,
author = {Go\"{e}au, Herv\'{e} and Joly, Alexis and Selmi, Souheil and Bonnet, Pierre and Mouysset, Elise and Joyeux, Laurent and Molino, Jean-Fran\c{c}ois and Birnbaum, Philippe and Bathelemy, Daniel and Boujemaa, Nozha},
title = {Visual-Based Plant Species Identification from Crowdsourced Data},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072472},
doi = {10.1145/2072298.2072472},
abstract = {This demo presents a crowdsourcing web application dedicated to the access of botanical knowledge through automated identification of plant species by visual content. Inspired by citizen sciences, our aim is to speed up the collection and integration of raw botanical observation data, while providing to potential users an easy and efficient access to this botanical knowledge. The result presented during the demo is an enjoying application where anyone can play to shoot fresh cut leaves and observe the relevance of species suggested in spite of various visual difficult queries.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {813–814},
numpages = {2},
keywords = {collaborative data collection, educational aspects of multimedia, plant species identification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072473,
author = {Singh, Vivek K. and Luo, Jiebo and Joshi, Dhiraj and Das, Madirakshi and Lei, Phoury and Stubler, Peter},
title = {Dynamic Media Show Drivable by Semantics},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072473},
doi = {10.1145/2072298.2072473},
abstract = {We demonstrate a system to generate dynamic media shows that are significantly richer than static slide shows, which are currently the most popular form of photo playback. The goal is to enable media reliving experiences that are aesthetically pleasing, interactive, and semantically drivable as they center on people, locations, time, and events discovered in a media collection. Dynamic shows allow for better sharing of one's media collections in diverse social networks because people have different time availabilities and perspectives, and hence may want to interact, customize, and reroute the media flow per their individual needs.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {815–816},
numpages = {2},
keywords = {semantic, slide-show, reliving, interaction, sharing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072474,
author = {Hoi, Steven C.H. and Wu, Pengcheng},
title = {SIRE: A Social Image Retrieval Engine},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072474},
doi = {10.1145/2072298.2072474},
abstract = {With the explosive growth of social media applications on the internet, billions of social images have been made available in many social media web sites nowadays. This has presented an open challenge of web-scale social image search. Unlike existing commercial web search engines that often adopt text based retrieval, in this demo, we present a novel web-based multimodal paradigm for large-scale social image retrieval, termed "Social Image Retrieval Engine" (SIRE), which effectively exploits both textual and visual contents to narrow down the semantic gap between high-level concepts and low-level visual features. A relevance feedback mechanism is also equipped to learn with user's feedback to refine the search results interactively. Our live demo is available at http://msm.cais.ntu.edu.sg/SIRE, and a video is available athttp://www.youtube.com/user/msmntu.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {817–818},
numpages = {2},
keywords = {image retrieval, social media, multi-modal image search, social images, relevance feedback},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072475,
author = {Beskow, Paul B. and Stensland, H\r{a}kon K. and Espeland, H\r{a}vard and Kristiansen, Espen A. and Olsen, Preben N. and Kristoffersen, St\r{a}le and Griwodz, Carsten and Halvorsen, P\r{a}l},
title = {Processing of Multimedia Data Using the P2G Framework},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072475},
doi = {10.1145/2072298.2072475},
abstract = {In this demo, we present the P2G framework designed for processing distributed real-time multimedia data. P2G supports arbitrarily complex dependency graphs with cycles, branches and deadlines. P2G is implemented to scale transparently with available resources, i.e., a concept familiar from the cloud computing paradigm. Additionally, P2G supports heterogeneous computing resources, such as x86 and GPU processing cores. We have implemented an interchangeable P2G kernel language which is meant to expose fundamental concepts of the P2G programming model and ease the application development. Here, we demonstrate the P2G execution node using a MJPEG encoder as an example workload when dynamically adding and removing processing cores.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {819–820},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072476,
author = {Wang, Qia and Lobzhanidze, Alex and Roy, Suman Deb and Zeng, Wenjun and Shang, Yi},
title = {Positionit: An Image-Based Remote Target Localization System on Smartphones},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072476},
doi = {10.1145/2072298.2072476},
abstract = {We present PositionIt, a novel, low-cost, image-based remote target localization system using commodity smartphones. By leveraging smartphones' built-in sensors such as camera, digital compass, GPS, etc., the system can estimate the distance/position of remote targets. The system also takes advantage of the user-friendly interface of the smartphones to facilitate low complexity implementation (e.g., to locate target of interest and restrict the search area for matching).The feasibility of both single-image and two-image based systems are demonstrated.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {821–822},
numpages = {2},
keywords = {multi-view geometry, position estimation, android application, remote target},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072477,
author = {Monaghan, David and O'Sullivan, James and O'Connor, Noel E. and Kelly, Bridget and Kazmierczak, Olivier and Comer, Lorraine},
title = {Low-Cost Creation of a 3D Interactive Museum Exhibition},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072477},
doi = {10.1145/2072298.2072477},
abstract = {We present a demonstration of a multi-sensory virtual experience that accurately reflects the experience of a real visit to a museum. We have developed a 3D interactive digital museum exhibition corresponding to a photo-realistic 3D reconstruction of the Iconic Treasures exhibition at the National Museum of Ireland's archaeology building. Users can explore the 3D virtual environment and interact with the displays by accessing high resolution photographs and historical descriptions of the artefacts. This demo will showcase the experience on a 3D laptop with the use of active shutter LCD glasses to create an immersive experience. A key aspect of the demonstration is that it was developed using low-cost or freely available OTS tools wherever possible putting the creation of such digital exhibitions within reach of even modest budgets.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {823–824},
numpages = {2},
keywords = {museum exhibition, 3D reconstruction, 3D vision},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072478,
author = {Mori, Koichi and Ballagas, Rafael and Revelle, Glenda and Raffle, Hayes and Horii, Hiroshi and Spasojevic, Mirjana},
title = {Interactive Rich Reading: Enhanced Book Reading Experience with a Conversational Agent},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072478},
doi = {10.1145/2072298.2072478},
abstract = {In this work, we introduce Interactive Rich Reading, a new enhanced book experience designed to run on smartphones and a tablet device. Interactive Rich Reading is characterized by a video-based conversational agent that asks questions or makes comments about the current page and is specifically designed to promote engagement with the contents of children's books. We use video compositing techniques to overlay the conversational agent directly over the book contents, creating a magical experience for children by bringing the book to life. We describe technical issues related to enabling this experience on mobile platforms for easier adoption of this technique by other researchers and practitioners.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {825–826},
numpages = {2},
keywords = {mobile application, conversational agent, interactive e-book},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072479,
author = {Schoeffmann, Klaus and Fabro, Manfred del},
title = {Hierarchical Video Browsing with a 3D Carousel},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072479},
doi = {10.1145/2072298.2072479},
abstract = {We present a video browsing tool that combines advantages of the hierarchical browsing concept with 3D projection and multi-threaded programming in order to provide a convenient and efficient interface. The tool allows for instantaneous hierarchical browsing of video and uses a dynamic approach (i.e. tree of playable video segments instead of static key frames)that also supports parallel playback.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {827–828},
numpages = {2},
keywords = {video browsing, video exploration, video navigation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072480,
author = {Carlier, Axel and Shafiei, Arash and Badie, Julien and Bensiali, Salim and Ooi, Wei Tsang},
title = {COZI: Crowdsourced and Content-Based Zoomable Video Player},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072480},
doi = {10.1145/2072298.2072480},
abstract = {We present a new user interface designed to allow easy yet effective zooming and panning into high-definition videos for playback on low resolution displays. Our system first applies state-of-the-art video analysis algorithms to detect salient regions of interest and recommends them to users. These recommendations help users to quickly identify important regions in the video and zoom into the regions with a single mouse click. The salient regions may moves according to the movement of track objects, further reducing the need for users to manually pan to track an object of interests. To further improve the relevance of the recommended regions, users' interactions are logged and analyzed. The actual regions selected and viewed by users serve as a feedback and is integrated into the system to improve the recommendations. We have implemented a Web-based version of the user interface, running on modern browsers supporting HTML5. We describe the algorithms and optimizations used to implement and improve the system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {829–830},
numpages = {2},
keywords = {zoomable video, crowdsourcing, interaction techniques, content analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072481,
author = {Lino, Christophe and Christie, Marc and Ranon, Roberto and Bares, William},
title = {A Smart Assistant for Shooting Virtual Cinematography with Motion-Tracked Cameras},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072481},
doi = {10.1145/2072298.2072481},
abstract = {This demonstration shows how an automated assistant encoded with knowledge of cinematography practice can offer suggested viewpoints to a filmmaker operating a hand-held motion-tracked virtual camera device. Our system, called Director's Lens, uses an intelligent cinematography engine to compute, at the request of the filmmaker, a set of suitable camera placements for starting a shot that represent semantically and cinematically distinct choices for visualizing the current narrative. Editing decisions and hand-held camera compositions made by the user in turn influence the system's suggestions for subsequent shots. The result is a novel virtual cinematography workflow that enhances the filmmaker's creative potential by enabling efficient exploration of a wide range of computer-suggested cinematographic possibilities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {831–832},
numpages = {2},
keywords = {virtual cinematography, motion-tracked virtual cameras, virtual camera planning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072482,
author = {Friedland, Gerald and Choi, Jaeyoung and Janin, Adam},
title = {Video2GPS: A Demo of Multimodal Location Estimation on Flickr Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072482},
doi = {10.1145/2072298.2072482},
abstract = {The following article describes our demo of an approach to determine the geo-coordinates of the recording place of Flickr videos based on both textual metadata and visual cues. The underlying system has been tested on the MediaEval 2010 Placing Task evaluation data, which consists of 5091 unfiltered test videos is able to classify 14% of the videos to within an accuracy of 10m.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {833–834},
numpages = {2},
keywords = {location estimation, tagging, content analysis, multimodal, video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072483,
author = {Fesnin, Alexis and Gouet-Brunet, Valerie and Kominen, Scott and Oria, Vincent and Sun, Jichao},
title = {Towards a Privacy Preserving Personal Photo Album Manager with Semantic Classification, Indexing and Querying Capabilities},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072483},
doi = {10.1145/2072298.2072483},
abstract = {This paper presents the prototype of a personal photo album manager that we build over three years. It integrates state of the art techniques of both database systems and computer vision to propose to publishers a privacy preserving sharable photo album manager where the publisher decides what each user is allowed to see. In addition, the photo album manager provides tools for an easy semantic annotation, classification and querying of the images.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {835–836},
numpages = {2},
keywords = {personal photo album, privacy, CBIR},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072484,
author = {Cai, Yang and Yang, Linjun and Ping, Wei and Wang, Fei and Mei, Tao and Hua, Xian-Sheng and Li, Shipeng},
title = {Million-Scale near-Duplicate Video Retrieval System},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072484},
doi = {10.1145/2072298.2072484},
abstract = {In this paper, we present a novel near-duplicate video retrieval system serving one million web videos. To achieve both the effectiveness and efficiency, a visual word based approach is proposed, which quantizes each video frame into a word and represents the whole video as a bag of words. The system can respond to a query in 41ms with 78.4% MAP on average.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {837–838},
numpages = {2},
keywords = {video retrieval, near-duplicate, large scale},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072485,
author = {He, Junfeng and Lin, Tai-Hsu and Feng, Jinyuan and Chang, Shih-Fu},
title = {Mobile Product Search with Bag of Hash Bits},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072485},
doi = {10.1145/2072298.2072485},
abstract = {The advent of smart phones has provided an excellent plat- form for mobile visual search. Most of previous mobile visual search systems adopt the framework of "bag of words",in which words indicate quantized codes of visual features. In this work, we propose a novel mobile visual search system based on "bag of hash bits". Using new ideas for hash bit selection, multi-hash table generation, and hamming-distance soft scoring, we overcome the problem of bit inefficiency affecting the traditional hashing approaches, and achieve promising accuracy outperforming state of the art. The framework is also general in that general feature type can be used for generating the hash bits. Demos and experiments over a large scale product image set demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {839–840},
numpages = {2},
keywords = {mobile, visual search, retrieval, image search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072487,
author = {Romberg, Stefan},
title = {From Local Features to Local Regions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072487},
doi = {10.1145/2072298.2072487},
abstract = {Local features are omnipresent in computer vision applications and an important building block for applications like object recognition and image retrieval. Such applications often involve feature matching or use an inverted index to efficiently retrieve similar local features for a given query.In such cases it is commonly known that local features are often not informative yielding mismatches and false positives when used for feature matching (see Figure 1)or retrieval. As a result retrieval systems employ expensive post-retrieval verification steps. These observations underline the importance of embedding spatial information of a local image region directly within the index to decrease the number of false positives upon retrieval.We present our latest method for embedding spatial information into an index by bundling local feature triples for logo recognition. We give an overview of our feature representation, describe currently ongoing research and pose open questions for discussion.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {841–844},
numpages = {4},
keywords = {local features, region representation, feature bundling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072488,
author = {Huang, Zixia},
title = {Synchronized Dissemination Framework for Supporting High-Quality Tele-Immersive Shared Activity},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072488},
doi = {10.1145/2072298.2072488},
abstract = {My Ph.D. thesis aims to develop a synchronized dissemination framework for supporting high-quality tele-immersive shared activity (TISA). Synchronization in TISA is complicated by (a) the multi-modal, bandwidth-savvy and timing-dependent media streaming over shared network resources, and (b) the delay-sensitive and interaction-critical nature of TISA in the real two-site and multi-site applications. This paper provides an outline of my thesis which includes four major contributions: (1) proposing a generalized layered framework for multi-stream synchronization sourced at one or multiple sites, (2) presenting an adaptive media packet scheduling scheme based on multi-stream timing correlations under Internet dynamics, (3) proposing a synchronized multicast topology based on diverse user interests, and (4) studying human subjective satisfactions to guide the system adaptation. This study is expected to output research results significant for next-generation systems, where timing-dependent media multi-modality is critical.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {845–848},
numpages = {4},
keywords = {synchronization, tele-immersive shared activity},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072489,
author = {Arefin, Ahsan},
title = {Session Management of Correlated Multi-Stream 3D Tele-Immersive Environments},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072489},
doi = {10.1145/2072298.2072489},
abstract = {Quality control and resource optimization are challenging problems in 3D tele-immersive (3DTI) environments due to their large scale, multi-stream dependencies and dynamic peer (viewer) behavior. Such systems are also prone to performance degradation due to undesired behavior in the event of drastic demand changes, such as view change and large-scale simultaneous viewer arrivals or departures. Therefore, it is crucial to localize undesired behavior inside the system and re-organize the streaming overlay structures accordingly. Doing this accurately for a large scale is even more challenging and it requires to capture all events effecting the data plan and control plan of the system. Moreover, to do this, we need to understand the desired behavior of the application first, which is defined by the dependency patterns of performance and configuration metadata at each participating peers. To assist that, we propose a learning framework that discovers metadata dependency patterns from the time series metadata and uses an online profiler to detect undesired behavior of the system during run-time. Such universal protocol also enables the prediction of large scale performance degradation due to irregular dependencies. Finally an adaptation is proposed that reallocates the resources and rearranges overlay structures to overcome the undesired behavior. In summary, our goal is to provide a universal session monitoring and management framework for complex multi-stream 3DTI environments to support large number of concurrent viewers. We consider the difficulty in overlay construction, collecting metadata, answering queries, learning patterns, detecting undesired behavior at the participating peers and finally overlay adaptation considering multi-stream dependencies.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {849–852},
numpages = {4},
keywords = {monitoring, learning, streaming, resource allocation, adaptation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072490,
author = {Sipiran Mendoza, Ivan Anselmo},
title = {Local Features for Partial Shape Matching and Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072490},
doi = {10.1145/2072298.2072490},
abstract = {This PhD thesis proposal is focused on proposing solutions to the problem of Partial Shape Retrieval which is part of the Three-Dimensional Object Retrieval problem and represents a very challenging problem. Given a shape (or a part of a shape) as query, one wants to retrieve from a collection of 3D models those objects which contain parts visually similar to the query. Difficulties can arise due to the need of representing a model in a compact way with local information, where the extent of the locality is unknown a priori. In addition, the matching becomes an expensive task because of the large amount of possible memberships of the partial query in the shapes. Typically, assumptions are made to constrain the problem in order to solve it effectively. Therefore, partial shape retrieval is a challenging and open problem. In this document, we provide proposals to address this problem and furthermore, we discuss the open problems and the important issues that should be tackled in order to solve the problem of Partial Shape Matching and Retrieval.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {853–856},
numpages = {4},
keywords = {3D local features, shape retrieval, shape matching},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072492,
author = {Yang, Xiaodong},
title = {Recognizing Clothes Patterns for Blind People by Confidence Margin Based Feature Combination},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072492},
doi = {10.1145/2072298.2072492},
abstract = {Clothes pattern recognition is a challenging task for blind or visually impaired people. Automatic clothes pattern recognition is also a challenging problem in computer vision due to the large pattern variations. In this paper, we present a new method to classify clothes patterns into 4 categories: stripe, lattice, special, and patternless. While existing texture analysis methods mainly focused on textures varying with distinctive pattern changes, they cannot achieve the same level of accuracy for clothes pattern recognition because of the large intra-class variations in each clothes pattern category. To solve this problem, we extract both structural feature and statistical feature from image wavelet subbands. Furthermore, we develop a new feature combination scheme based on the confidence margin of a classifier to combine the two types of features to form a novel local image descriptor in a compact and discriminative format. The recognition experiment is conducted on a database with 627 clothes images of 4 categories of patterns. Experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art texture analysis methods in the context of clothes pattern recognition.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {857–858},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072493,
author = {Shi, Shu},
title = {Building Low-Latency Remote Rendering Systems for Interactive 3D Graphics Rendering on Mobile Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072493},
doi = {10.1145/2072298.2072493},
abstract = {The recent explosion of mobile devices is changing people's computing behaviors and more and more applications are ported to mobile platforms. However, some applications, such as 3D video tele-immersion and 3D video gaming that require intensive computation or network bandwidth are not capable of running on mobile devices yet. Remote rendering is a simple but effective solution. A workstation with enough computation and network bandwidth resources (e.g., cloud server) is served as the rendering server. It receives and renders the source contents (e.g., 3D graphics or 3D video), and sends the rendering results (2D images) to one or multiple clients. The client simply receives and displays the result images. Using remote rendering for 3D game or 3D video rendering on mobile devices can solve both computation and bandwidth problems.However, there are some issues with remote rendering 3D video/3D games for mobile devices: (1) Interaction Latency: The interaction latency is defined as the time from the generation of user interaction request till the appearance of the first updated rendering frame on the mobile client. In our application scenario, the interaction latency is mainly determined by the round trip time of wireless networks. (2) Bandwidth: Since we are considering the mobile devices using wireless mobile networks as the client in our remote rendering system, the available network bandwidth is very limited compared with the wired networks. The system design should consider the bandwidth limitation and try to minimize the network bandwidth usage. (3) Real Time: There are real-time requirements in our system design. The rendering rate of 3D video is determined by the recording 3D camera and the frame rate of 3D gaming depends on the game motion. The processing operations of every frame should be completed before the start of next frame.My researches propose novel remote rendering designs to enhance the interactive experience of 3D graphics rendering on mobile devices by reducing the interaction latency. Different from conventional approaches, the proposed system applies no restriction on the network latency to provide low latency rendering services. Instead, the rendering server sends an image-based representation of the current 3D scene/model to the client. Once any viewpoint change interaction happens, the mobile client can directly synthesize the appropriate image using image-based rendering techniques. The research problems can be summarized as the follows: What image-based representation of the 3D scene/model should the rendering server generate in order to reduce interaction latency?Given the real-time requirement, how does the rendering server generate the image-based representation of every frame efficiently?What encoding scheme can be applied to compress the generated image-based representation to meet the limited wireless bandwidth requirement?How can the remote rendering system with latency reduction enhancements be evaluated effectively?This research started in 2009 and progresses of all topics have been made. For problem (1), our full paper in MM'091 proposed to generate two depth images as the image-based representation and use 3D image warping algorithm to synthesize the view at the new rendering viewpoint on the mobile client. For problem (2), our full paper in MM'10 introduced several computation efficient algorithms in how to select reference frames for 3D image warping. For problem (3), we have studied a novel video coding method by integrating H.264/AVC together with 3D image warping algorithm and this new coding method can potentially beat the state of art x264 in terms of compression efficiency in the scenario of real-time 3D game video encoding. The full paper with the latest research results has been accepted by MM'11. For the last problem, we developed a new metric: DOL (Distortion Over Latency) to evaluate the interactive performance of remote rendering systems by combining both latency and rendering quality in one score and the paper has been presented in ICME'11.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {859–860},
numpages = {2},
keywords = {remote rendering, interaction latency},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072494,
author = {Yu, Felix X.},
title = {Intelligent Query Formulation for Mobile Visual Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072494},
doi = {10.1145/2072298.2072494},
abstract = {While much progress is being made in mobile visual search, most efforts are on how to improve search performance (precision, recall, speed) given queries. How to help the user form a good query has generally left unexplored. Successful mobile search should keep users in the loop and have the machine and user work closely as a team to solve the difficult problem - user provide fast feedback on the fly and machine does the data intensive analysis. Therefore, helping the user to form a good query has a great potential in improving the search performance. We describe a novel framework, Active Query Sensing, to provide interactive query formulation solutions for mobile location search. We also discuss new research directions addressing the important open issues of interactive query formulation for mobile visual search.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {861–862},
numpages = {2},
keywords = {mobile location recognition, content-based image retrieval, query formulation, mobile visual search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072495,
author = {Li, Xia},
title = {Robust Image Representation for Efficient Recognition and Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072495},
doi = {10.1145/2072298.2072495},
abstract = {Image representation plays an essential role in image categorization and retrieval applications. Images span on visual and spatial space. Encoding both visual and spatial information for effective and efficient image matching remains a fundamental problem in computer vision. The objective of my research is to construct a robust image representation for efficient recognition and retrieval.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {863–864},
numpages = {2},
keywords = {visual codebook, and object classification., spatial transformation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072496,
author = {Sawant, Neela},
title = {Modeling Tagged Photos for Automatic Image Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072496},
doi = {10.1145/2072298.2072496},
abstract = {A semantic concept can be a physical object (e.g., 'car', 'zebra-fish'), an activity (e.g., 'demonstration', 'running') or an obscure category (e.g., 'historical', 'autumn'). In automatic image annotation, machines are taught to infer such semantic concepts by training with hundreds of manually selected images that contain those concepts. So far, remarkable progress has been made in the areas of visual feature extraction and machine learning algorithms that relate features to concepts. Now the new challenge is to scale the inference and annotation capacity to thousands of semantic concepts in the real world.The key to large-scale machine annotation lies in automatic training data selection from user-tagged photos shared on social websites. To learn an annotation model for a concept such as 'car', one can exploit the fact that 'car' is also a common tag associated with photos of day-to-day activities. We may consider the concept to be illustrated by the collection of all photos thus annotated. The photos can be retrieved through a text-search interface and readily used to train the annotation model. While this seems like a straightforward idea, a few ramifications have to be considered: Substandard images: Tagging is an uncontrolled activity not geared towards scientific computation, but guided by personal motivations and communal influences. Tags may be incorrect or incomplete and some images may be poor examples of the target concept. Such substandard training images may hamper the annotation performance.Modeling constraints: As tags as well as visual features determine the quality of training data, a joint analysis of visual-textual features is necessary. Additionally, a diverse set of features need to be effectively combined even if some features are incapable of modeling specific concepts in the large annotation vocabulary. Efficient computing techniques and infrastructure is necessary to maintain scalability and adaptability to new concepts and training data.We recently surveyed the application of tagged photographs for image annotation [2]. The literature was organized to address four main annotation types: (a) general-purpose concepts, (b) names of people, (c) names of locations, and (d) events. An important observation was that a majority of studies resorted to semi-supervised learning to annotate an unlabeled (or partially labeled) image. Specifically, content based retrieval techniques were used to identify similar images and a tag-ranking model was developed to transfer the annotations of the retrieved results on to the query image. Such data-driven techniques can potentially access an arbitrarily large annotation vocabulary. However, they do not conform to the idea of model-based vision and only work if large labeled image sets can be analyzed at run-time.We adopted a supervised learning approach to large-scale image annotation where training data was selected from Flickr images [3]. The selection process was designed to reject substandard images. Annotation models were trained and stored for 1000 words, so that only pixel information of test images needed to be analyzed at run-time. The time required to select training data for a single concept from 10,000 images (634-dimensional feature) using a single CPU of 2.66 GHz speed and 24.4 GB memory was under 5 minutes. Also, the results as compared to a state-of-the-art annotation system showed marked diversity and accuracy.The supervised annotation approach can only predict the concepts used in its training. In this sense, the approach does not cater to the preferred vocabularies of users. To address this issue, we developed a personalized tagging extension [1]. We proposed a transfer learning model to translate the set of machine annotations to a user's vocabulary using a Na\"{\i}ve Bayes formulation. The highlight of the technique was the computation of the translation model from the collective tagging behavior of the user's local social network.In conclusion, the proposed research harnesses user-tagged images and social interactions on photo sharing websites to develop a practical image annotation system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {865–866},
numpages = {2},
keywords = {social interactions, image annotation, automatic training data selection, user-tagged photos},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072497,
author = {Rivas, Raoul},
title = {Acropolis: Operating System for Real-Time Multi-Streaming Systems},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072497},
doi = {10.1145/2072298.2072497},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {867–868},
numpages = {2},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072498,
author = {Wu, Wanmin and Nahrstedt, Klara},
title = {Human-Centric Control of Video Functions and Underlying Resources in 3D Tele-Immersive Systems},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072498},
doi = {10.1145/2072298.2072498},
abstract = {3D tele-immersion (3DTI) has the potential of enabling virtual reality interaction among remote people with real-time 3D video. However, today's 3DTI systems still suffer from various performance issues, limiting their broader deployment, due to the enormous demand on temporal (computing) and spatial (networking) resources. Past research focused on system-centric approaches for technical optimization, without taking human users into the loop. We argue that human factors (including user preferences, semantics, limitations, etc.) are an important and integral part of the cyber-physical 3DTI systems, and should not be neglected. This thesis proposes a comprehensive, human-centric framework for managing video data and functions across the 3DTI pipeline. Our approach is comprehensive because it involves all components of the video function pipeline. It is also comprehensive in the sense that both temporal and spatial resource challenges are considered and tackled.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {869–870},
numpages = {2},
keywords = {adaptation, video, resources, tele-immersion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072499,
author = {Merler, Michele},
title = {Analysis, Indexing and Visualization of Presentation Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072499},
doi = {10.1145/2072298.2072499},
abstract = {The exponential diffusion of unstructured multimedia content on sites such as YouTube has lately fostered a rapid growth of interest in the multimedia community toward a new line of systems to recognize visual content "in the wild", that is, in less structured, unconstrained and more realistic domains. Due to the lack of structure and to the low quality of the data, algorithms and paradigms designed for professional content often cannot directly be applied to the aforementioned domains, thus presenting a new challenge.One particular instance of such "wild videos" is represented by unstructured presentation videos, which are tools nowadays employed in a large variety of systems for different purposes, spanning from distance or e-learning to generation of conference proceedings, from corporate talks to student presentations.We are interested in helping users efficiently and effectively access visual information, in particular technical information, and learn from such online multimedia sources. Our proposed system aims at summarizing, indexing, cross- referencing and browsing unstructured ("wild") presentation videos, with a focus on the quality of the produced indexes as perceived by the end users.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {871–872},
numpages = {2},
keywords = {interactive user studies, presentation video, multi-modal indexing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072500,
author = {Liu, Yao},
title = {Towards Efficient Resource Utilization in Internet Mobile Streaming},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072500},
doi = {10.1145/2072298.2072500},
abstract = {Internet video streaming to mobile devices is challenging because of device heterogeneity, resource constraints, and limited battery power supply of mobile devices. From mobile users' perspective, we examine the power efficiency of existing streaming protocols, and propose to reduce the power consumed by data transmission. Moreover, from the service provider's perspective, we propose to efficiently utilize resources at server side to better serve mobile users.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {873–874},
numpages = {2},
keywords = {internet mobile streaming, power saving},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072501,
author = {Singh, Vivek K.},
title = {From Multimedia Data to Situation Detection},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072501},
doi = {10.1145/2072298.2072501},
abstract = {We are witnessing a phenomenal increase in multimodal human and device sensing to measure and report parameters such as temperature, vehicle speed, visual experiences, flu cases, and people happiness. Soon we expect these heterogeneous datasets (e.g. images, videos, weather sensors, check-ins and tweets) to become available in real-time in the Cloud for reasoning and decision making.Important human decisions however cannot be undertaken by piecemeal treatment of these individual data points. Rather we need computational tools to integrate and abstract these data points into higher level actionable representations.This underscores the need for computational tools to model and detect situations from large heterogeneous spatio-temporal data sets. In this thesis, we computationally define the notion of situations and propose a methodology to bridge the semantic gap between the widely available low level spatio-temporal data and actionable situation inference needed for decision making.We define a situation as: "An actionable abstraction of observed spatio-temporal descriptors".This definition underscores our viewpoint of computationally defining situations based on statistical descriptors (as opposed to say situation-calculus or recognition-by-parts), a focus on spatio-temporal data (which is indeed the most common connotation associated with situations), scoping of problem only to observable (via human/device sensors) data, and a focus on actionable abstractions (as defined explicitly by human domain experts).The problem of modeling and detecting situations from (STT) i.e. spatio-temporal-thematic data is relevant in multiple domains like traffic, weather, healthcare, business analysis, emergency response, and political decision making.Situation Modeling STT data spreads across very disparate application domains as well as data types. However, focusing on the commonalities and not the differences, we realize that there is a core set of operations which is central to defining spatio-temporal situations across different applications. Once a domain expert defines a situation of interest (e.g. a 'flu pandemic', 'hurricane advise') based on data sources, core operations, and user parameters, the same situation model can act as a standing query on realtime data streams and provide 'mass-personalization' to billions of end-users.Just like E/R modeling, or UML we merely provide the basic building blocks. It is each domain expert's responsibility to define actionable situations by combining these building blocks. These building blocks are designed to be computable, modular and explicit and hence translatable into executable code once the modeling is complete.Approach Our approach for integrating and characterizing heterogeneous spatio-temporal data is based on the concept of social pixels. We simply organize spatio-temporal values related to any theme on a two dimensional data grid. Such a grid provides heat-map like intuitive visualization, and also an image like computational data structure. Hence multiple spatio-temporal situational descriptors can be implemented as off-the-shelf image and video processing operations (Refer Fig 1).Current status We have made progress in terms of identifying the generic set of situation detection operations [1]. We have run multiple experiments with STT data sets to answer situational queries like 'what recommendation to give to user indicating flu-like symptoms' [2] and 'where to open a new iphone store'[1]. We are currently implementing the core STT analysis engine which will allow modeling and detection of multiple situation queries across applications. We are also finalizing a methodology to guide domain experts when they model situations in terms of building blocks like data sources, characterizations operators, and user parameters.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {875–876},
numpages = {2},
keywords = {social networks, situation modeling, events, detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072502,
author = {McDaniel, Troy and Panchanathan, Sethuraman},
title = {The Building Blocks of Somatic Information Delivery},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072502},
doi = {10.1145/2072298.2072502},
abstract = {With each technological breakthrough, our visual and auditory modalities are becoming increasingly overloaded with information. In this work, we explore the sense of touch as an alternative channel for communication for multimedia applications. We propose a novel theoretical framework, inspired by natural language, for the design of somatic information delivery systems, and validate this framework in applications of motor learning and accessible movies for individuals who are blind.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {877–878},
numpages = {2},
keywords = {communication, somatemes, haptics, somatic phonemes},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072503,
author = {Tiwari, Richa and Zhang, Chengcui},
title = {Video Genre Detection Using a Multimodality Approach},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072503},
doi = {10.1145/2072298.2072503},
abstract = {In this abstract, we introduce the idea of our new multimodality approach towards video genre detection. The two novelties of this algorithm are that firstly it is based on identifying relationships between semantic concepts and genres in videos, which can later help in distinguishing videos in different genres. Secondly, we apply topic level genre detection such as a 'talk show' genre can be further categorized as 'political talk show' or 'economic talk show', etc., which is a more precise categorization.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {879–880},
numpages = {2},
keywords = {machine learning, multimodality approach, video genre detection, video classification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072504,
author = {Chen, Wei-Bang and Zhang, Chengcui},
title = {Multiple Object Retrieval in Image Databases Using Hierarchical Segmentation Tree},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072504},
doi = {10.1145/2072298.2072504},
abstract = {With the rapid growth of information, efficient and robust information retrieval techniques have become increasingly more important. Multiple object retrieval remains challenging due to the complex nature of this problem. The proposed research, unlike most existing works that are designed for single object retrieval or adopt heuristic multiple object matching scheme, aims at contributing to this field through the development of an image retrieval system that adopts a hierarchical region-tree representation of image, and enables effective and efficient multiple object retrieval and automatic discovery of the objects of interest through users' relevance feedback. We believe this is the first systematic attempt to formulate a comprehensive, intelligent, and interactive framework for multiple object retrieval in image databases that makes use of a hierarchical region-tree representation.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {881–882},
numpages = {2},
keywords = {content-based image retrieval, multi-object retrieval, hierarchical region-tree, multi-resolution image segmentation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072505,
author = {Zhong, Shenghua and Liu, Yan and Liu, Yang},
title = {Bilinear Deep Learning for Image Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072505},
doi = {10.1145/2072298.2072505},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {883–884},
numpages = {2},
keywords = {image classification., deep learning, bilinear discriminant projection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072506,
author = {Tan, Song},
title = {Beyond Video Searching: Summarization and Exploration},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072506},
doi = {10.1145/2072298.2072506},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {885–886},
numpages = {2},
keywords = {visual summarization, video browsing, media content synchronization, event extraction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072507,
author = {Li, Xirong},
title = {Image Search 2.0},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072507},
doi = {10.1145/2072298.2072507},
abstract = {This abstract sketches my PhD research towards establishing a generic mechanism for exploiting social intelligence for next-generation image search.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {887–888},
numpages = {2},
keywords = {big social data, image search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071898,
author = {Duan, Guifang and Sawant, Neela and Wang, James Z. and Snow, Dean and Ai, Danni and Chen, Yen-Wei},
title = {Analysis of Cypriot Icon Faces Using ICA-Enhanced Active Shape Model Representation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071898},
doi = {10.1145/2072298.2071898},
abstract = {Religious iconography is an integral component of the cultural heritage of Cyprus, which was once a part of the great Byzantine empire. On one hand, icons exhibit strict adherence to conventional symbols, poses and apparel. On the other hand, there is a great variety in the style of depiction that can be attributed to different schools and periods. This paper proposes an active shape model (ASM) based technique for icon face representation that can be used for style comparison and attribution. For centuries-old icons suffering from loss of paint, cracks and added noise from digitization artifacts, we apply an independent component analysis (ICA) technique to enhance the paintings' original work. The experimental results show that our method can effectively characterize Cypriot icons.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {901–904},
numpages = {4},
keywords = {byzantine, cyprus, active shape model, independent component analysis, iconography},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071899,
author = {Cabral, Diogo and Valente, Jo\~{a}o and Silva, Jo\~{a}o and Arag\~{a}o, Ur\^{a}ndia and Fernandes, Carla and Correia, Nuno},
title = {A Creation-Tool for Contemporary Dance Using Multimodal Video Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071899},
doi = {10.1145/2072298.2071899},
abstract = {This paper presents a video annotator that supports multimodal annotation and is applied to contemporary dance as a creation tool. The Creation-Tool was conceived and designed to assist the creative processes of choreographers, working as a digital notebook for personal annotations. The prototype, developed for Tablet PCs, is a real-time multimodal video annotator based on keyboard, pen and voice inputs. In addition, a remote control for mobile devices was added to the system. Two types of annotations were defined: annotation marks and regular annotations. The annotations marks are defined by a keyword and an icon, in contrast to regular annotations that do not have a pre-defined structure. Motion tracking defines the dynamic behavior of the annotations and voice input complements the other modalities.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {905–908},
numpages = {4},
keywords = {annotations, contemporary dance, pen-based video annotations, tablet pcs, video annotations},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071900,
author = {Monaci, Gianluca and Gritti, Tommaso and Vignoli, Fabio and Walmink, Wouter and Hendriks, Maarten},
title = {Flower Power},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071900},
doi = {10.1145/2072298.2071900},
abstract = {Flower Power is an interactive installation that creates immersive light atmospheres by capturing the colors of flowers and re-projecting them in the environment. A camera is suspended over a flower bed and its position depends on the positions of the visitors around the installation, which are detected using presence sensors. The image captured by the camera is rendered into the surrounding environment as a suggestive, saturated light scene. The participants can thus give the power to their favorite flower to set the overall light experience. In this paper we describe the motivation and the realization of the installation, and we discuss insights and reactions collected during the 2010 STRP Festival.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {909–912},
numpages = {4},
keywords = {ambient interaction, light feedback, interactive installation, real-time video rendering},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071901,
author = {Freiburg, Bauke and Kamps, Jaap and Snoek, Cees G.M.},
title = {Crowdsourcing Visual Detectors for Video Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071901},
doi = {10.1145/2072298.2071901},
abstract = {In this paper, we study social tagging at the video fragment-level using a combination of automated content understanding and the wisdom of the crowds. We are interested in the question whether crowdsourcing can be beneficial to a video search engine that automatically recognizes video fragments on a semantic level. To answer this question, we perform a 3-month online field study with a concert video search engine targeted at a dedicated user-community of pop concert enthusiasts. We harvest the feedback of more than 500 active users and perform two experiments. In experiment 1 we measure user incentive to provide feedback, in experiment 2 we determine the tradeoff between feedback quality and quantity when aggregated over multiple users. Results show that users provide sufficient feedback, which becomes highly reliable when a crowd agreement of 67% is enforced.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {913–916},
numpages = {4},
keywords = {video retrieval, information visualization, semantic indexing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071902,
author = {Reben, Alexander and Paradiso, Joseph},
title = {A Mobile Interactive Robot for Gathering Structured Social Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071902},
doi = {10.1145/2072298.2071902},
abstract = {Documentaries are typically captured in a very structured way, using teams to film and interview people. We developed an autonomous method for capturing structured cin\'{e}ma v\'{e}rit\'{e} style documentaries through an interactive robotic camera, which was used as a mobile physical agent to facilitate interaction and story gathering within a ubiquitous media framework. We sent this robot out to autonomously gather human narrative about its environment. The robot had a specific story capture goal and leveraged humans to attain that goal. The robot collected a 1st person view of stories unfolding in real life, and as it engaged with its subjects via a preset dialog, these media clips were intrinsically structured. We evaluated this agent by way of determining "complete" vs. "incomplete" interactions. "Complete" interactions were those that generated viable and interesting videos, which could be edited together into a larger narrative. It was found that 30% of the interactions captured were "complete" interactions. Our results suggested that changes in the system would only produce incrementally more "complete" interactions, as external factors like natural bias or busyness of the user come into play. The types of users who encountered the robot were fairly polar; either they wanted to interact or did not - very few partial interactions went on for more than 1 minute. Users who partially interacted with the robot were found to treat it rougher than those who completed the full interaction. It was also determined that this type of limited-interaction system is best suited for short-term encounters. At the end of the study, a short cin\'{e}ma v\'{e}rit\'{e} documentary showcasing the people and activity in our building was easily produced from the structured videos that were captured, indicating the utility of this approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {917–920},
numpages = {4},
keywords = {automatic documentary, social robotics, HRI, interaction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071903,
author = {Danielescu, Andreea and Spicer, Ryan P. and Tinapple, David and Kelliher, Aisling and Nikkila, Shawn and Burdick, Sean},
title = {Abstract Rendering of Human Activity in a Dynamic Distributed Learning Environment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071903},
doi = {10.1145/2072298.2071903},
abstract = {Contemporary distributed enterprises present challenges in terms of demonstrating community activity awareness and coherence across individuals and teams in collaborating networks. Building with a Memory is an experiential media system that captures and represents human activity in a distributed workplace over time. The system senses and analyzes movement in two workspaces in a mixed-use building with the results rendered in an informative ambient display in the building entryway. We describe the design and development of the system, together with insights from two studies of the installation and promising future directions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {921–924},
numpages = {4},
keywords = {motion tracking, ambient presence, memory},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071904,
author = {Mann, Steve and Janzen, Ryan and Huang, Jason},
title = {"WaterTouch": An Aquatic Interactive Multimedia Sensory Table Based on Total Internal Reflection in Water},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071904},
doi = {10.1145/2072298.2071904},
abstract = {We propose a water-based multitouch multimedia user-interface based on total-internal reflection as viewed by an underwater camera. The underwater camera is arranged so that nothing above the water surface is visible until a user touches the water, at which time anything that penetrates the water's surface becomes clearly visible. Our contribution is twofold: (1) computer vision using underwater cameras aided by total internal reflection; (2) hyperacoustic signal processing (frequency shifting) to capture the natural, acoustically-originating sounds of water rather than using synthetic sounds.Using water itself as a touch screen creates a fun and playful user interface medium that captures the fluidity of the water's ebb and flow. In one application, a musical instrument is created in which acoustic disturbances in the water (received by underwater microphones or hydrophones) are frequency-shifted to musical notes corresponding to the location in which the water is touched, as determined by the underwater computer vision system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {925–928},
numpages = {4},
keywords = {tangible user interface, organic user interface, human computer interaction, musical instrument, waterplay, total internal reflection, hydraulophone, touch screen},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071905,
author = {Bhattacharya, Abhishek and Wu, Wanmin and Yang, Zhenyu},
title = {Quality of Experience Evaluation of Voice Communication Systems Using Affect-Based Approach},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071905},
doi = {10.1145/2072298.2071905},
abstract = {The voice communication industry is undergoing a rapid phase change with continuously evolving technologies such as cellular, mobile and Internet telephony. Effective evaluation of system performance is becoming critical, which will serve as an important instrument for service providers to monitor and manage quality. Recently, Quality of Experience (QoE) metrics are found to be more valuable quality assessment mechanism since they are more closely related to human perception as compared to traditional QoS-based techniques. In this paper, we investigate a new affect-based approach for QoE evaluation. Our hypothesis states that QoE in voice communication is correlated to user's affective behavior, which will vary across networking conditions. We conduct extensive user study based on natural human-human voice communication. The experimental results indicate very promising prospect of this approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {929–932},
numpages = {4},
keywords = {quality of experience, voice communication, affect computing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071906,
author = {Nicolaou, Mihalis A. and Gunes, Hatice and Pantic, Maja},
title = {A Multi-Layer Hybrid Framework for Dimensional Emotion Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071906},
doi = {10.1145/2072298.2071906},
abstract = {This paper investigates dimensional emotion prediction and classification from naturalistic facial expressions. Similarly to many pattern recognition problems, dimensional emotion classification requires generating multi-dimensional outputs. To date, classification for valence and arousal dimensions has been done separately, assuming that they are independent. However, various psychological findings suggest that these dimensions are correlated. We therefore propose a novel, multi-layer hybrid framework for emotion classification that is able to model inter-dimensional correlations. Firstly, we derive a novel geometric feature set based on the (a)symmetric spatio-temporal characteristics of facial expressions. Subsequently, we use the proposed feature set to train a multi-layer hybrid framework composed of a tem- poral regression layer for predicting emotion dimensions, a graphical model layer for modeling valence-arousal correlations, and a final classification and fusion layer exploiting informative statistics extracted from the lower layers. This framework (i) introduces the Auto-Regressive Coupled HMM (ACHMM), a graphical model specifically tailored to accommodate not only inter-dimensional correlations but also to exploit the internal dynamics of the actual observations, and (ii) replaces the commonly used Maximum Likelihood principle with a more robust final classification and fusion layer. Subject-independent experimental validation, performed on a naturalistic set of facial expressions, demonstrates the effectiveness of the derived feature set, and the robustness and flexibility of the proposed framework.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {933–936},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071907,
author = {Vuong, Catherine H. and Ingalls, Todd and Abbas, James J.},
title = {Transforming Clinical Rehabilitation into Interactive Multimedia},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071907},
doi = {10.1145/2072298.2071907},
abstract = {There has been an increased interest in using interactive multimedia as a mechanism for physical rehabilitation, including the direct usage of off-the-shelf games and gaming systems. Although, these games have been shown to have physical and psychosocial benefits in the elderly; they were limited when participants had significant physical and/or cognitive disabilities. The limited success in the use of gaming systems to promote rehabilitation may largely be due to the fact that the games being played on them were not designed with rehabilitation in mind and thus they do not necessarily target specific the symptoms or conditions of the disorder. However, if designed correctly and for rehabilitation, interactive multimedia and gaming systems have significant rehabilitation potential. We believe for a system to effective in promoting rehabilitation, its approach must be based in the clinical techniques used to treat the symptoms and the principles of neural science behind the disorder. In this paper, we describe and approach to Parkinson's rehabilitation using this methodology.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {937–940},
numpages = {4},
keywords = {parkinson's rehabilitation, interactive multimedia, visual cueing, visual feedback, optical flow},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071908,
author = {Tadayon, Ramin and Amresh, Ashish and Burleson, Winslow},
title = {Socially Relevant Simulation Games: A Design Study},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071908},
doi = {10.1145/2072298.2071908},
abstract = {Socially Relevant Simulation Games (SRSG), a new medium for social interaction, based on real-world skills and skill development, creates a single gaming framework that connects both serious and casual players. Through a detailed case study this paper presents a design process and framework for SRSG, in the context of mixed-reality golf swing simulations. The SRSG, entitled "World of Golf", utilizes a real-time expert system to capture, analyze, and evaluate golf swing metrics. The game combines swing data with players' backgrounds, e.g., handicaps, to form individual profiles. These profiles are then used to implement a golf simulation game using artificially controlled agents who inherit the skill levels of their corresponding human users. The simulation and assessment modules provide the serious player with tools to build golf skills while allowing casual players to engage within a simulated social world. A framework that incorporates simulated golf competitions among these social agents is presented and validated by comparing the usage statistics of 10 PGA Golf Management (PGM) students with 10 non-professional students.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {941–944},
numpages = {4},
keywords = {background assessment, real-time motion capture, social gaming, golf swing analysis, simulation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071909,
author = {Yao, Ting and Ngo, Chong-Wah and Mei, Tao},
title = {Context-Based Friend Suggestion in Online Photo-Sharing Community},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071909},
doi = {10.1145/2072298.2071909},
abstract = {With the popularity of social media, web users tend to spend more time than before for sharing their experience and interest in online photo-sharing sites. The wide variety of sharing behaviors generate different metadata which pose new opportunities for the discovery of communities. We propose a new approach, named context-based friend suggestion, to leverage the diverse form of contextual cues for more effective friend suggestion in the social media community. Different from existing approaches, we consider both visual and geographical cues, and develop two user-based similarity measurements, i.e., visual similarity and geo similarity for characterizing user relationship. The problem of friend suggestion is casted as a contextual graph modeling problem, where users are nodes and the edges between them are weighted by geo similarity. Meanwhile, the graph is initialized in a way that users with higher visual similarity to a given query have better chance to be recommended. Experimental results on a dataset of 13,876 users and ~1.5 million of their shared photos demonstrated that the proposed approach is consistent with human perception and outperforms other works.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {945–948},
numpages = {4},
keywords = {friend suggestion, user similarity, social media},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071910,
author = {Gumulia, Anastasia and Puzon, BartBomiej and Kosugi, Naoko},
title = {Music Visualization: Predicting the Perceived Speed of a Composition -- Misual Project --},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071910},
doi = {10.1145/2072298.2071910},
abstract = {This paper discusses how people feel the speed of tunes and proposes a way to express it with images. In Misual Project, the research is done to provide people with a way to learn/know music other than by listening to it. This paper focuses on the speed feeling of music, since it is an important musical characteristic for listeners. Although tempo itself is an important indicator of the speed of music, we found that the number of notes per time unit and rhythms (combination of regular and dotted notes) are also important. How and how much these factors contribute to the speed feeling is investigated in listening tests with simple tunes. The factors are visualized so that people can intuitively imagine how fast a tune is. A selection of famous tunes is visualized and the advantage of music visualization is discussed. The visualization allows us to look at a tune as a whole and to compare tunes.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {949–952},
numpages = {4},
keywords = {music speed, music psychology, music vizualization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071911,
author = {Zhou, Xiuzhuang and Hu, Junlin and Lu, Jiwen and Shang, Yuanyuan and Guan, Yong},
title = {Kinship Verification from Facial Images under Uncontrolled Conditions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071911},
doi = {10.1145/2072298.2071911},
abstract = {In this paper, we present an automatic kinship verification system based on facial image analysis under uncontrolled conditions. While a large number of studies on human face analysis have been performed in the literature, there are a few attempts on automatic face analysis for kinship verification, possibly due to lacking of such publicly available databases and great challenges of this problem. To this end, we collect a kinship face database by searching 400+ pairs of public figures and celebrities from the internet, and automatically detect them with the Viola-Jones face detector. Then, we propose a new spatial pyramid learning-based (SPLE) feature descriptor for face representation and apply support vector machine (SVM) for kinship verification. The proposed system has the following three characteristics: 1) no manual human annotation of face landmarks is required and the kinship information is automatically obtained from the original pair of images; 2) both local appearance information and global spatial information have been effectively utilized in the proposed SPLE feature descriptor, and better performance can be obtained than state-of-the-art feature descriptors in our application; 3) the performance of our proposed system is comparable to that of human observers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {953–956},
numpages = {4},
keywords = {face analysis, image descriptors, kinship verification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071912,
author = {Morris, Mitchell J. and Kender, John R.},
title = {VastMM-Tag: A Semantic Tagging Browser for Unstructured Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071912},
doi = {10.1145/2072298.2071912},
abstract = {Quickly accessing the contents of a video is challenging for users, particularly for unstructured video, which contains no intentional shot boundaries, no chapters, and no apparent edited format. We approach this problem in the domain of lecture videos using machine learning and semantic display techniques. We extend an existing video browser, through a display of these machine-learned semantic labelings to provide the user with a multi-timeline semantic view. Each timeline corresponds to one semantic label and indicates the label's probable presence or absence in the associated frames. We also provide a full Boolean algebra over these labels, in order to accommodate more complex queries, such as 'text or code, but no instructor'. Finally, we quantify the effectiveness of our features and our browser through user studies on various tasks. We find that users follow a nearly fixed pattern of access, alternating between the use of these tags and keyframes, and also between the use of 'word bubbles' and the player. We show that the tag algebra is integral to the time efficient use of tag timelines, saving up to 27% of the time for various retrieval tasks.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {957–960},
numpages = {4},
keywords = {machine learning of tags, semantic tagging, unstructured video, tagging user interface, tag algebra},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071913,
author = {Tung, Qiyam and Swaminathan, Ranjini and Efrat, Alon and Barnard, Kobus},
title = {Expanding the Point: Automatic Enlargement of Presentation Video Elements},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071913},
doi = {10.1145/2072298.2071913},
abstract = {We present a system that assists users in viewing videos of lectures on small screen devices, such as cell phones. It automatically identifies semantic units on the slides, such as bullets, groups of bullets, and images. As the participant views the lecture, the system magnifies the appropriate semantic unit while it is the focus of the discussion. The system makes this decision based on cues from laser pointer gestures and spoken words that are read off the slide. It then magnifies the semantic element using the slide image and the homography between the slide image and the video frame. Experiments suggest that the semantic units of laser-based events identified by our algorithm closely match those identified by humans. In the case of identifying bullets through spoken words, results are more limited but are a good starting point for more complex methods. Finally, we show that this kind of magnification has potential for improving learning of technical content from video lectures when the resolution of the video is limited, such as when being viewed on hand held devices.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {961–964},
numpages = {4},
keywords = {video, magnification, speech, lecture, mobile, learning, laser},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071914,
author = {Kumar, Senthil and Menon, Sreedal and Zane, Francis},
title = {Sharing Rectangular Objects in a Video Conference},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071914},
doi = {10.1145/2072298.2071914},
abstract = {This paper presents a novel method for sharing rectangular objects in a video conference. During these sessions, users often want to share a printed document or other planar rectangular objects (a photograph, the whiteboard on the wall, etc.) with the other participants. If the object is simply held in front of the camera (or if the camera is turned towards the object), the object will undergo perspective distortion and a rectangular object will appear in the video as a quadrilateral. In addition, a handheld document will not be steady and it will be difficult for other users to read its contents. In this paper, we present an algorithm that uses real-time predictive keystone correction to automatically lock on to object and present it with the proper orientation and shape.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {965–968},
numpages = {4},
keywords = {keystone correction, remote collaboration},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071915,
author = {Ji, Hao and Su, Fei},
title = {Biased Metric Learning for Person-Independent Head Pose Estimation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071915},
doi = {10.1145/2072298.2071915},
abstract = {In this paper, a new biased metric learning (BML) method is proposed for human head pose estimation problem. Traditional approaches focus on modeling a smooth low-dimensional manifold embedded in the high dimensional feature space. Such manifold-embedding methods, linear or nonlinear, suffer from one common drawback, that all neighbors are identified based on the Euclidean distance in the original feature space. However, the nature local structure of data is always corrupted by various factors in this original feature space. The proposed BML method aims at obtaining a global optimal linear transformation from the input feature space into a new semantic space which is characterized by pose angles. The metric is trained with the goal that local semantic structure of data with same label is preserved while the biased distance of differently labeled data is maximized. The learning process also reduces to a convex optimization by formulating it as a semidefinite problem (SDP). Numerous experiments demonstrate the superiority of our BML method over several current states of art approaches on publicly available dataset.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {969–972},
numpages = {4},
keywords = {metric learning, semidefinite programming, human head pose estimation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071916,
author = {Liu, Dong and Yan, Shuicheng and Zhang, Hong-Jiang},
title = {Next Photo Please: Towards Visually Consistent Sequential Photo Browsing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071916},
doi = {10.1145/2072298.2071916},
abstract = {Sequential photo browsing has become the most important function in the desktop and online image repository management systems, where existing systems typically display the photos in default orders such as the lexicographic order of the photo filename or the chronological order of the photo taken time. However, these browsing orders, especially when the browsing speed is fast, ignore the vision persistency characteristic of human visual systems, which results in inconsistent visual experience for photo viewers. To address this issue, we construct a photo relationship graph based on various kinds of visual features that complementarily reflect human visual perception. Then the seeking of visually consistent photo browsing sequence is cast into a traveling salesman problem which seeks an optimal path with minimum visual distance within the graph structure. Experiment results on sequential browsing of Flickr photo groups indicate that the proposed method clearly beats the other sequential photo browsing methods in terms of visual consistency.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {973–976},
numpages = {4},
keywords = {sequential photo browsing, TSP, vision persistency},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071917,
author = {Khiem, Ngo Quang Minh and Ravindra, Guntur and Ooi, Wei Tsang},
title = {Towards Understanding User Tolerance to Network Latency in Zoomable Video Streaming},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071917},
doi = {10.1145/2072298.2071917},
abstract = {We conducted a user study with 35 participants viewing 5 video clips to understand user tolerance to network latency when zooming and panning in zoomable video streams. With zooming or panning, unseen spatial regions in a frame are revealed and momentarily in an unknown state until data arrive from the server. To handle such unknown state, two common concealment schemes are used, namely Black scheme and Low-Res scheme. Black scheme renders the newly revealed region as black pixels, while Low-Res covers the unknown part with data from a low resolution video stream, which is additionally streamed by the server. In the context of these schemes, our study based on the simulation of delays shows that users are more tolerable to delay in Low-Res scheme. Up to 94% of participants can tolerate 1 second delay and 80% can tolerate up to a delay of 2 seconds in Low-Res scheme, while only 77% of participants can tolerate 1 second delay in Black scheme. The tolerable delay in zoomable video streaming is higher than thresholds found in some high interactive multimedia applications},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {977–980},
numpages = {4},
keywords = {network latency, delay tolerance, zoomable video, concealment scheme, region-of-interest streaming},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071918,
author = {Ursu, Marian and Torres, Pedro and Zsombori, Vilmos and Franztis, Michael and Kaiser, Rene},
title = {Socialising through Orchestrated Video Communication},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071918},
doi = {10.1145/2072298.2071918},
abstract = {We report on the development of a video communication medium through which groups of people situated in different physical locations can naturally talk to each other, see and hear each other, and engage in social entertaining activities. Participants are free to move within their space and behave in a manner closer to collocated experiences. Essentially, this is implemented as a multi-location, multi-camera, hands-free video conferencing system between groups, with integrated support for entertaining activities. In this paper we focus on automatic orchestration, the reasoning process that applies screen grammar to best support the communication. We present a formal model for representing orchestration rules and discuss initial evaluation results.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {981–984},
numpages = {4},
keywords = {social entertainment, multi-location videoconferencing, orchestration},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071919,
author = {Brandi, Fernanda and Steinbach, Eckehard},
title = {Perceptual Coding of Recorded Telemanipulation Sessions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071919},
doi = {10.1145/2072298.2071919},
abstract = {Efficient recording and replay of telemanipulation sessions require the compression of the occurring visual-haptic signals. To this end, we propose two distinct approaches which take into consideration known limitations of human visual and haptic perception. We compare both schemes to the state-of-the-art approaches and show that a substantial additional data reduction can be achieved while maintaining good overall haptic and visual performances.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {985–988},
numpages = {4},
keywords = {data reduction, perceptual coding, signal compression, haptics, telemanipulation, playback, signal recording},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071920,
author = {Staiano, Jacopo and Lepri, Bruno and Subramanian, Ramanathan and Sebe, Nicu and Pianesi, Fabio},
title = {Automatic Modeling of Personality States in Small Group Interactions},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071920},
doi = {10.1145/2072298.2071920},
abstract = {In this paper, we target the automatic recognition of personality states in a meeting scenario employing visual and acoustic features. The social psychology literature has coined the name personality state to refer to a specific behavioral episode wherein a person behaves as more or less introvert/extrovert, neurotic or open to experience, etc. Personality traits can then be reconstructed as density distributions over personality states. Different machine learning approaches were used to test the effectiveness of the selected features in modeling the dynamics of personality states.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {989–992},
numpages = {4},
keywords = {personality states, modeling dynamics, modeling, big five traits},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071921,
author = {Liu, Si and Chen, Qiang and Dong, Jian and Yan, Shuicheng and Xu, Changsheng and Lu, Hanqing},
title = {Snap &amp; Play: Auto-Generate Personalized Find-the-Difference Mobile Game},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071921},
doi = {10.1145/2072298.2071921},
abstract = {According to the year 2010 report of the Entertainment Software Association [5], 42% of USA heads of households reported playing games on mobile devices, rising quickly from the 20% in 2002 and bringing huge market for mobile games. In this paper, by taking the popular game, Find-the-Difference (FiDi), as a concrete example, we explore new mobile game design principles and techniques for enhancing player's gaming experience in personalized, automatic, and dynamic aspects. Unlike traditional FiDi game, where image pairs (source image vs. target image) with M different patches are manually produced by game developer and players may feel boring or cheat after practicing all image pairs, our proposed Personalized FiDi (P-FiDi) mobile game may be played under a new Snap &amp; Play mode. The player may first take photos with one s mobile device (or select from one's own albums). Then, these photos serve as source images, and the P-FiDi system automatically generates the counterpart target images by sequential operations of aesthetic image quality enhancement, image patch and differentiating style joint selection, music adaptation, dynamic difficulty level determination, and ultimate automatic image editing with a rich set of popular differentiating styles used in traditional FiDi game. Finally, the player enjoys the unique gaming with one's own (instant) photos and music, and the freedom to have new gaming image pairs any time. The user studies show that the P-FiDi mobile game is satisfying in terms of player experience.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {993–996},
numpages = {4},
keywords = {personalized, games with a purpose, advertisement, snap &amp; play, automatic, dynamic, edutainment},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071922,
author = {Brahmachari, Aveek Shankar and Sarkar, Sudeep},
title = {Fast Detection of Noisy GPS and Magnetometer Tags in Wide-Baseline Multi-Views},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071922},
doi = {10.1145/2072298.2071922},
abstract = {We propose an algorithm for detection of noisy GPS and magnetometer tags in wide-baseline camera views. Our algorithm neither needs densely sampled views nor does it need a single visually connected path through all the views in the dataset. We use vision-based estimates of mutual rotation and translation between cameras to compute a measure of confidence on the correctness of the associated GPS and magnetometer tags. The vision algorithm can find the epipolar geometry between two wide-baseline images without needing pre-specified correspondences. We have two versions of our approach; one that requires geometric pose estimation between all pairs of images and a faster version that uses a pre-filter based on photometric comparison of images to quickly reject non-overlapping views from further geometric consideration. We show qualitative and quantitative results on the Nokia Grand Challenge 2010 Dataset. We find that magnetometer readings are more accurate than GPS readings.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {997–1000},
numpages = {4},
keywords = {wide-baseline images, image collection, epipolar geometry},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071923,
author = {Yang, Linjun and Cai, Yang and Hanjalic, Alan and Hua, Xian-Sheng and Li, Shipeng},
title = {Video-Based Image Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071923},
doi = {10.1145/2072298.2071923},
abstract = {Likely variations in the capture conditions (e.g. light, blur, scale, occlusion) and in the viewpoint between the query image and the images in the collection are the factors due to which image retrieval based on the Query-by-Example (QBE) principle is still not reliable enough. In this paper, we propose a novel QBE-based image retrieval system where users are allowed to submit a short video clip as a query to improve the retrieval reliability. Improvement is achieved by integrating the information about different viewpoints and conditions under which object and scene appearances can be captured across different video frames. Rich information extracted from a video can be exploited to generate a more complete query representation than in the case of a single-image query and to improve the relevance of the retrieved results. Our experimental results show that video-based image retrieval (VBIR) is significantly more reliable than the retrieval using a single image as a query.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1001–1004},
numpages = {4},
keywords = {content-based image retrieval, video-based image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071924,
author = {Yus, Roberto and Mena, Eduardo and Bernad, Jorge and Ilarri, Sergio and Illarramendi, Arantza},
title = {Location-Aware System Based on a Dynamic 3D Model to Help in Live Broadcasting of Sport Events},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071924},
doi = {10.1145/2072298.2071924},
abstract = {Broadcasting sport events in live is a challenging task because obtaining the best views requires taking into account many dynamic factors, such as: the location and movement of interesting objects, all the views provided by cameras in the scenario (some of them wireless, mobile, or attached to moving objects), possible occlusions, etc. Therefore, a technical director needs to manage a great amount of continuously changing information to quickly select the camera whose view should be broadcasted.In this paper, we present a location-aware system that helps technical directors in the broadcasting task, using a 3D model updated continuously with real-time location data retrieved from the scenario. They can indicate in run-time their interest in certain moving objects and the system is in charge of selecting the cameras that provide the kind of views required.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1005–1008},
numpages = {4},
keywords = {3d model, content selection in run-time, multiple camera management},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071925,
author = {Tang, Ziying and Ozbek, Orkun and Guo, Xiaohu},
title = {Real-Time 3D Interaction with Deformable Model on Mobile Devices},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071925},
doi = {10.1145/2072298.2071925},
abstract = {Mobile-based augmented reality is an emerging technology that provides immersive experiences over wireless networks. Its applications, such as 3D streaming, have become more and more popular on mobile devices. However, most of these applications do not support real-time 3D interactions. Consequently, mobile users can only watch or browse 3D contents passively instead of actively interacting with 3D objects. In this paper, we propose a novel method that allows mobile users to change a model's shape and motions through interactions via mobile touch screen and obtain feedbacks in real-time. To accelerate computational speed and reduce communication load, we compute 3D deformations using a spectral representation. Moreover, a progressive deformation streaming technique is proposed to reduce the effect of interaction delay between the server and mobile clients. Our experimental results indicate that our method provides real-time interaction feedback, offering satisfactory user experiences.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1009–1012},
numpages = {4},
keywords = {progressive streaming, mobile application, 3D model},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071926,
author = {Hao, Jia and Wang, Guanfeng and Seo, Beomjoo and Zimmermann, Roger},
title = {Keyframe Presentation for Browsing of User-Generated Videos on Map Interfaces},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071926},
doi = {10.1145/2072298.2071926},
abstract = {To present user-generated videos that relate to geographic areas for easy access and browsing it is often natural to use maps as interfaces. A common approach is to place thumbnail images of video keyframes in appropriate locations. Here we consider the challenge of determining which keyframes to select and where to place them on the map. Our proposed technique leverages sensor-collected meta-data which are automatically acquired as a continuous stream together with the video. Our approach is able to detect interesting regions and objects (hotspots) and their distances from the camera in a fully automated way. Meaningful keyframes are adaptively selected based on the popularity of the hotspots. Our experiments show very promising results and demonstrate excellent utility for the users.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1013–1016},
numpages = {4},
keywords = {hotspot detection, keyframe extraction, visible distance},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071927,
author = {Yu, Xunyi and Ganz, Aura},
title = {Detecting and Identifying People in Mobile Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071927},
doi = {10.1145/2072298.2071927},
abstract = {In this paper, we propose a system capable of detecting and identifying people in videos captured by smart phones. We discuss the challenges to extend existing location aware multimedia applications from annotating static landmarks in distance to annotating dynamic people in close range with significant pose variations. We propose to use a hybrid video and RF tracking system to enable accurate observer and target localization, and extract part models comprised of Maximally Stable Color Regions for each target. The model can efficiently detect possible positions of targets in the video, which are then used as dynamic landmarks to calibrate the camera orientation. Finally, positions of all targets in the video are jointly estimated using both visual features and spatial constraints. Experiments show that our approach can locate identified targets in video with significantly higher accuracy than back-projection using camera orientation estimations from accelerometers and magnetometers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1017–1020},
numpages = {4},
keywords = {camera calibration, people detection, feature fusion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071928,
author = {Lin, Fang-Erh and Kuo, Yin-Hsi and Hsu, Winston H.},
title = {Multiple Object Localization by Context-Aware Adaptive Window Search and Search-Based Object Recognition},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071928},
doi = {10.1145/2072298.2071928},
abstract = {Multiple object localization and recognition has been an important problem in recent years not only because of its difficulty to be time efficient but also due to many different schemes of widespread applications. In many previous works, only a limited amount of object models contribute to less computational time. However, they tend to not work efficiently together with large-scale database. In this paper, we propose a context-aware adaptive window search algorithm and search-based object recognition system to recognize and localize multiple objects in an image with a large-scale database. Since we tackle this problem with the idea that users can get brief information of an item immediately after taking only a snapshot, a low response time is also taken into account. We implement the algorithm within a large-scale book recognition system and present experimental results that demonstrate the efficiency of our algorithm in terms of detection recall, precision, and speed compared to the baseline and efficient subwindow search (ESS) approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1021–1024},
numpages = {4},
keywords = {image retrieval, window estimation, multiple object localization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071929,
author = {Cooper, Matthew L.},
title = {Clustering Geo-Tagged Photo Collections Using Dynamic Programming},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071929},
doi = {10.1145/2072298.2071929},
abstract = {This paper describes methods for clustering photos that possess both time stamps and geographical coordinates as metadata. We present a two part method that first analyzes photos' time and location information to independently partition the photos into multiple clusterings. A subset of the detected clusters is then selected for the final photo clustering using an efficient dynamic programming procedure that optimizes a clustering fitness score. We propose fitness measures to produce clusterings that are coherent in space, time, or both. One group of scores directly measures within-cluster inter-photo distances. A second set of scores measures clusters' consistency with the reference clusterings. We present experiments that validate our method using multiple data sets.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1025–1028},
numpages = {4},
keywords = {digital photo organization, digital libraries, event clustering},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071930,
author = {Tsai, Sam S. and Chen, David and Chen, Huizhong and Hsu, Cheng-Hsin and Kim, Kyu-Han and Singh, Jatinder P. and Girod, Bernd},
title = {Combining Image and Text Features: A Hybrid Approach to Mobile Book Spine Recognition},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071930},
doi = {10.1145/2072298.2071930},
abstract = {Despite the successful use of local image features for large-scale object recognition, they are not effective in recognizing book spines on bookshelves. This is because some book spines contain only text components that do not yield distinguishing image features. To overcome this issue, we develop a new approach that combines a text-based spine recognition pipeline with an image feature-based spine recognition pipeline. The text within the book spine image is recognized and used as keywords to search a book spine text database. The image features of the book spine image are searched through a book spine image database. The search results of the two approaches are then carefully combined to form the final result. We implement the proposed hybrid book recognition pipeline used in a book inventory management system, and conduct extensive experiments to evaluate its performance. The experimental results show that while text-based or image feature-based systems only achieve a recall of 72%, the proposed hybrid system achieves a recall of ~91%.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1029–1032},
numpages = {4},
keywords = {visual search, text recognition, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071931,
author = {Houle, Michael E. and Oria, Vincent and Satoh, Shin'ichi and Sun, Jichao},
title = {Knowledge Propagation in Large Image Databases Using Neighborhood Information},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071931},
doi = {10.1145/2072298.2071931},
abstract = {The aim of this paper is to reduce to a minimum the level of human intervention in the semantic annotation process of images. Ideally, only one copy of each object of interest would be labeled manually, and the labels would then be propagated automatically to all other occurrences of the objects in the database. To that end, we propose a neighbor-based influence propagation approach KProp which builds a voting model and propagates the knowledge associated to some objects to similar objects. We show that KProp can perform efficiently through matrix computations and achieve better performance with fewer labeled examples per object.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1033–1036},
numpages = {4},
keywords = {image annotation, classification, neighborhood},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071932,
author = {Zhou, Qiang and Chen, Shifeng and Liu, Jianzhuang and Tang, Xiaoou},
title = {Edge-Preserving Single Image Super-Resolution},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071932},
doi = {10.1145/2072298.2071932},
abstract = {This paper proposes a novel approach to single image super-resolution. First, an image up-sampling scheme is proposed which takes the advantages of both bilateral filtering and mean shift image segmentation. Then we use a shock filter to enhance strong edges in the initial up-sampling result and obtain an intermediate high-resolution image. Finally, we enforce a reconstruction constraint on the high-resolution image so that fine details can be inferred by back projection. Since strong edges in the intermediate result are enhanced, ringing artifacts can be suppressed in the back projection step. We compare our algorithm with several state-of-the-art image super-resolution algorithms. Qualitative and quantitative experimental results demonstrate that our approach performs the best.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1037–1040},
numpages = {4},
keywords = {edge-preserving, image super-resolution},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071933,
author = {Cao, Chen and Chen, Shifeng and Zhang, Wei and Tang, Xiaoou},
title = {Automatic Motion-Guided Video Stylization and Personalization},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071933},
doi = {10.1145/2072298.2071933},
abstract = {Video stylization transfers a source video into an artistic version while maintaining temporal coherence between adjacent frames. In this paper, we formulate the unsupervised example-based video stylization with Markov random field model. In our algorithm, we implement an improved optical flow algorithm to maintain temporal coherence while improve the accuracy of estimation along motion boundaries. We also extend our algorithm to the application of video personalization, in which human faces keep clear and distinguishable. A series of techniques are fused in video personalization, including face detection and alignment, motion flow, skin detection, and illumination blending. Given a source video and a style template image, our algorithm produces the stylized and/or personalized video(s) automatically. Experimental results demonstrate that our algorithm performs excellently in both video stylization and personalization.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1041–1044},
numpages = {4},
keywords = {video stylization, non-photorealistic rendering, video personalization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071934,
author = {Zhu, Shiai and Ngo, Chong-Wah and Jiang, Yu-Gang},
title = {On the Pooling of Positive Examples with Ontology for Visual Concept Learning},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071934},
doi = {10.1145/2072298.2071934},
abstract = {A common obstacle in effective learning of visual concept classifiers is the scarcity of positive training examples due to expensive labeling cost. This paper explores the sampling of weakly tagged web images for concept learning without human assistance. In particular, ontology knowledge is incorporated for semantic pooling of positive examples from ontologically neighboring concepts. This effectively widens the coverage of the positive samples with visually more diversified content, which is important for learning a good concept classifier. We experiment with two learning strategies: aggregate and incremental. The former strategy re-trains a new classifier by combining existing and newly collected examples, while the latter updates the existing model using the new samples incrementally. Extensive experiments on NUS-WIDE and VOC 2010 datasets show very encouraging results, even when comparing with classifiers learnt using expert labeled training examples.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1045–1048},
numpages = {4},
keywords = {visual concepts, training set construction, semantic pooling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071935,
author = {Fang, Yuming and Chen, Zhenzhong and Lin, Weisi and Lin, Chia-Wen},
title = {Saliency-Based Image Retargeting in the Compressed Domain},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071935},
doi = {10.1145/2072298.2071935},
abstract = {In this paper, we propose a novel image retargeting algorithm to resize images based on the extracted saliency information from the compressed domain. Firstly, we utilize DCT coefficients in JPEG bitstream to perform saliency detection with the consideration of the human visual sensitivity. The obtained saliency information is used to determine the relative visual importance of each 8 x 8 block for the image. Furthermore, we propose a new adaptive block-level seam removal operation for connected blocks to resize the image. Thanks to the directly derived saliency information from the compressed domain, the proposed image retargeting algorithm effectively preserves the objects of attention, efficiently removes the less crucial regions, and therefore significantly outperforms the relevant state-of-the-art algorithms, as demonstrated with the careful analysis and in the extensive experiments.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1049–1052},
numpages = {4},
keywords = {compressed domain, human visual sensitivity, image retargeting, saliency detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071936,
author = {Yi, Jian and Peng, Yuxin and Xiao, Jianguo},
title = {Mining Concept Relationship in Temporal Context for Effective Video Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071936},
doi = {10.1145/2072298.2071936},
abstract = {We propose a new method to boost the performance of video annotation by exploiting concept relationship in temporal context. The motivation of our idea mainly comes from the fact that temporally continuous shots in video are generally with consistent content, so that concepts in these shots should be semantically relevant. We utilize a temporal model to describe the contributions of relevant concepts to the presence of a target concept. By connecting our model with conditional random field and adopting the learning and inference approaches of it, we could obtain the refined probability of a concept occurring in the shot, which is the leverage of temporal context and initial output of video annotation. Experimental results on the widely used TRECVID dataset exhibit the effectiveness of our method for improving video annotation accuracy.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1053–1056},
numpages = {4},
keywords = {context mining, conditional random field, video annotation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071937,
author = {Zhou, Xiangmin and Chen, Lei and Zhou, Xiaofang},
title = {Structure Tensor Series-Based Matching for near-Duplicate Video Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071937},
doi = {10.1145/2072298.2071937},
abstract = {Near duplicate video retrieval has attracted much attention due to its wide spectrum of applications including copyright detection, commercial monitoring and news video tracking. In recent years, there has been significant research effort on efficiently identifying near duplicates from large video collections. However, existing approaches for large video databases suffer from low accuracy due to the serious information loss. In this paper, we propose a practical solution based on 3D structure tensor model for this problem. We first propose a novel video representation scheme, adaptive structure video tensor series (ASVT series), together with a robust similarity measure, to improve the retrieval effectiveness. Then, we prove the effectiveness of the proposed method by extensive experiments on hundreds hours real video data.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1057–1060},
numpages = {4},
keywords = {near duplicates, similarity measure, ASVT series},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071938,
author = {Jiang, Junfeng and Zhang, Xiao-Ping},
title = {A Smart Video Player with Content-Based Fast-Forward Playback},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071938},
doi = {10.1145/2072298.2071938},
abstract = {In this paper, we develop a video player to allow the users to do fast-forward playback based on the semantic video content. The whole system has two modules, processing and playing. In the processing part, we present a video time density function (VTDF) to describe the temporal dynamics of video data first. A VTDF-based temporal quantization method is then developed to find the best quanta and partition in the time domain. The optimal quanta are used to extract key frames. The optimal number of key frames is determined by a temporal mean square error (TMSE)-based criterion. In the playing module, we combine the key frame sequence and a set of parameters together and feed them into a triangle-based transition function to generate the sampled frames in a non-uniform way. A built video player will play all sampled frames in its intelligent fast-forward mode for a given fast-forward speed factor. The implementation of video player demonstrates the feasibility of proposed method in practice.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1061–1064},
numpages = {4},
keywords = {video time density function, video content analysis, temporal quantization.},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071939,
author = {Cai, Hongyuan and Zheng, Jiang Yu},
title = {Video Anatomy: Cutting Video Volume for Profile},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071939},
doi = {10.1145/2072298.2071939},
abstract = {This work takes a new aspect view to profile a video volume to a video track as a digest for video preview. Our projected video profile contains both spatial and temporal information inclusively in a 2D image scroll that is continuous, compact, scalable, and indexing to each frame. To profile various types of video clips, we investigate the global flow field under all camera actions, and propose a uniformed scheme that uses a sampling line to cut the video volume across the major optical flow field. The resulting profile obtains an intrinsic scene space less influenced by the camera actions, and can be displayed in a video track to guide the access to video frames, and facilitate video browsing, editing, and retrieval.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1065–1068},
numpages = {4},
keywords = {video indexing, video summary, profile of video, spatial-temporal slice, camera motion, optical flow},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071940,
author = {Baker, Harlyn and Chang, Nelson L. and Paruchuri, Arun},
title = {Capture and Display for Live Immersive 3D Entertainment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071940},
doi = {10.1145/2072298.2071940},
abstract = {We describe methods, accomplishments, and direction in a laboratory-corporate collaborative project over this past year and a half. With the goal of taking new immersive technologies to a growing customer base through in-situ demonstrations and experiments, we are combining novel capture and display capabilities in delivering life-sized immersive 3D entertainment experiences. We describe the adaptations and innovations needed in delivering immersive life sized 3D to a large audience.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1069–1072},
numpages = {4},
keywords = {3D entertainment, stereo display/capture, immersive experience},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071941,
author = {Shi, Junhao and Zhang, Mingmin and Pan, Zhigeng},
title = {A Real-Time Bimanual 3D Interaction Method Based on Bare-Hand Tracking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071941},
doi = {10.1145/2072298.2071941},
abstract = {We present a vision-based gesture interaction technique that is capable of tracking both hands in real-time. By only recovering features calculated from images, large amount of computation for 3D reconstructing and features tracking in 3D voxel grid is avoided. Thus, our system becomes an ideal tool for bimanual 3D gesture interaction, which needs neither markers nor gloves. Experimental results show that our method can complete the same pipeline with much less computational resource compared to existing ones. What's more, the application interface shows the effectiveness and intuitiveness of our bimanual tracking method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1073–1076},
numpages = {4},
keywords = {computer vision, 3D interaction, bare-hand interaction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071942,
author = {Zhang, Xinming and Zha, Zheng-Jun and Xu, Changsheng},
title = {Learning "Verb-Object" Concepts for Semantic Image Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071942},
doi = {10.1145/2072298.2071942},
abstract = {In real-world image understanding and retrieval applications, there exists a large number of images containing "verb-object" semantic. The most existing image annotation approaches which mainly focus on annotating images with "object" concepts may not well describe the image semantics. In this paper, we propose a novel image annotation approach by learning "verb-object" concepts. The "verb-object" concept learning method is developed based on the assumption that the classifiers of the "verb-object" concepts which contain the same object usually share a common structure. We formulate each "verb-object" concept classifier as a combination of a private part and a common part shared by all the "verb-object" concepts containing the same object. These classifiers are learned simultaneously through a joint optimization process. Experiments on a Web image data set containing 22,812 images with 28 concepts demonstrate that the proposed approach achieved promising performance compared to the baseline method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1077–1080},
numpages = {4},
keywords = {"verb-object" concept, semantic image annotation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071943,
author = {Sun, Yongqing and Kojima, Akira},
title = {A Novel Method for Semantic Video Concept Learning Using Web Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071943},
doi = {10.1145/2072298.2071943},
abstract = {In recent years, exploring the rich web image resources has been offering promising solutions to the problem of how to perform low-manual-cost concept learning. However, concept classifiers trained using web images perform poorly when they are directly applied to video concept detection. We propose a novel scheme to address video concept learning using web images, one that includes the selection of web training data and the transfer of subspace learning within a unified framework. Starting with a small set of video keyframes related to a video concept, we select web training data of good quality from the web by referring to the content of video keyframes. Then, by exploiting both the selected dataset and video keyframes, we train a robust concept classifier by means of a transfer subspace learning method. Experiment results demonstrate the robustness and effectiveness of our method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1081–1084},
numpages = {4},
keywords = {web image mining, visual concept modeling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071944,
author = {Imura, Jun and Fujisawa, Teppei and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Efficient Multi-Modal Retrieval in Conceptual Space},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071944},
doi = {10.1145/2072298.2071944},
abstract = {In this paper, we propose a new, efficient retrieval system for large-scale multi-modal data including video tracks. With large-scale multi-modal data, the huge data size and various contents cause degradation of efficiency and precision of retrieval results. Recent research on image annotation and retrieval shows that image features based on the Bag-of-Visual Words approach with local descriptors such as SIFT perform surprisingly well with large-scale image datasets. Those powerful descriptors tend to be high-dimensional, imposing a high computational cost for approximate nearest neighbor searching in raw feature space. Our video retrieval method is focused on the correlation between image, sound, and location information recorded simultaneously, and to learn conceptual space describing the contents of the data to realize efficient searching. Experiments show good performance of our retrieval system with low memory usage and temporal complexity.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1085–1088},
numpages = {4},
keywords = {product quantization, generalized canonical correlation analysis, video retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071945,
author = {Wang, Xiangyu and Rui, Yong and Kankanhalli, Mohan S.},
title = {Up-Fusion: An Evolving Multimedia Decision Fusion Method},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071945},
doi = {10.1145/2072298.2071945},
abstract = {The amount of multimedia data available on the Internet has increased exponentially in the past few decades and is likely to keep on increasing. Given that a multimedia system has multiple information sources, fusion methods are critical for its analysis and understanding. However, most of the traditional fusion methods are static with respect to time. To address this, in recent years, several evolving fusion methods have been proposed. However, they can only be used in limited scenarios. For example, the context aware fusion methods need the context information to update the fusion model, but the context may not always be available in many applications. In this paper, a new evolving fusion method is proposed based on the online portfolio selection theory. The proposed method takes the correlation among different information sources into account, and evolves the fusion model when new multimedia data is added. It can deal with either crisp or soft decisions without requiring additional context information. Extensive experiments on concept detection task over TRECVID dataset have been conducted, and promising results have been obtained.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1089–1092},
numpages = {4},
keywords = {evolving, online portfolio selection theory, multimedia fusion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071946,
author = {Ren, Zhou and Yuan, Junsong and Zhang, Zhengyou},
title = {Robust Hand Gesture Recognition Based on Finger-Earth Mover's Distance with a Commodity Depth Camera},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071946},
doi = {10.1145/2072298.2071946},
abstract = {The recently developed depth sensors, e.g., the Kinect sensor, have provided new opportunities for human-computer interaction (HCI). Although great progress has been made by leveraging the Kinect sensor, e.g. in human body tracking and body gesture recognition, robust hand gesture recognition remains an open problem. Compared to the entire human body, the hand is a smaller object with more complex articulations and more easily affected by segmentation errors. It is thus a very challenging problem to recognize hand gestures. This paper focuses on building a robust hand gesture recognition system using the Kinect sensor. To handle the noisy hand shape obtained from the Kinect sensor, we propose a novel distance metric for hand dissimilarity measure, called Finger-Earth Mover's Distance (FEMD). As it only matches fingers while not the whole hand shape, it can better distinguish hand gestures of slight differences. The extensive experiments demonstrate the accuracy, efficiency, and robustness of our hand gesture recognition system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1093–1096},
numpages = {4},
keywords = {human-computer-interaction, kinect sensor, finger-earth mover's distance, hand gesture recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071947,
author = {Yang, Xiaodong and Yuan, Shuai and Tian, YingLi},
title = {Recognizing Clothes Patterns for Blind People by Confidence Margin Based Feature Combination},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071947},
doi = {10.1145/2072298.2071947},
abstract = {Clothes pattern recognition is a challenging task for blind or visually impaired people. Automatic clothes pattern recognition is also a challenging problem in computer vision due to the large pattern variations. In this paper, we present a new method to classify clothes patterns into 4 categories: stripe, lattice, special, and patternless. While existing texture analysis methods mainly focused on textures varying with distinctive pattern changes, they cannot achieve the same level of accuracy for clothes pattern recognition because of the large intra-class variations in each clothes pattern category. To solve this problem, we extract both structural feature and statistical feature from image wavelet subbands. Furthermore, we develop a new feature combination scheme based on the confidence margin of a classifier to combine the two types of features to form a novel local image descriptor in a compact and discriminative format. The recognition experiment is conducted on a database with 627 clothes images of 4 categories of patterns. Experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art texture analysis methods in the context of clothes pattern recognition.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1097–1100},
numpages = {4},
keywords = {computer vision, clothes pattern, visually impaired, recognition, blind},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071948,
author = {Redzic, Milan and Brennan, Conor and O'Connor, Noel E.},
title = {Dual-Sensor Fusion for Indoor User Localisation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071948},
doi = {10.1145/2072298.2071948},
abstract = {In this paper we address the automatic identification of indoor locations using a combination of WLAN and image sensing. Our motivation is the increasing prevalence of wearable cameras, some of which can also capture WLAN data. We propose to use image-based and WLAN-based localization individually and then fuse the results to obtain better performance overall. We demonstrate the effectiveness of our fusion algorithm for localisation to within a 8.9m2 room on very challenging data both for WLAN and image-based algorithms. We envisage the potential usefulness of our approach in a range of ambient assisted living applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1101–1104},
numpages = {4},
keywords = {WLAN, fusion, indoor localisation, SURF vocabulary tree},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071949,
author = {Li, Zhonghua and Zhang, Bingjun and Wang, Ye},
title = {Document Dependent Fusion in Multimodal Music Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071949},
doi = {10.1145/2072298.2071949},
abstract = {In this paper, we propose a novel multimodal fusion framework, document dependent fusion (DDF), which derives the optimal combination strategy for each individual document in the fusion process. For each document, we derive a document weight vector by estimating the descriptive abilities of its different modalities. The document weight vector also enables our framework to be easily integrated with existing multimodal fusion schemes, and achieve a better combination strategy for each document given a query. Experiments are conducted on a 17174-song music database to compare the retrieval accuracy of traditional query independent fusion and query dependent fusion approaches, and that obtained after integrating DDF with them. Experimental results indicate that DDF can significantly improve the retrieval performance of current fusion approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1105–1108},
numpages = {4},
keywords = {descriptive ability, document dependent fusion, multimodal search, music, query dependent fusion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071950,
author = {Rudinac, Stevan and Hanjalic, Alan and Larson, Martha},
title = {Finding Representative and Diverse Community Contributed Images to Create Visual Summaries of Geographic Areas},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071950},
doi = {10.1145/2072298.2071950},
abstract = {This paper presents an automatic approach that uses community-contributed images to create representative and diverse visual summaries of specific geographic areas. Complex relations between images, extracted visual features, text associated with the images as well as users and their social network are modeled using a multimodal graph. To compute affinities between nodes in the graph we rely on the proven concept of random walk with restarts. The novelty of our approach lies in its use of the multimodal graph to create a diverse, yet representative, image set. Further, we introduce an edge-weighting mechanism for the fusion of heterogeneous modalities. We evaluate our summaries with a new protocol that tests for representativeness and diversity using image geo-coordinates and is independent of the need for human evaluators. The experiments, performed on a set of Flickr images, demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1109–1112},
numpages = {4},
keywords = {image set diversity, social media, random walk with restarts, visual summaries of geographic areas, image set representativeness},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071951,
author = {Del Pero, Luca and Lee, Philip and Magahern, James and Hartley, Emily and Barnard, Kobus and Wang, Ping and Kanaujia, Atul and Haering, Niels},
title = {Fusing Object Detection and Region Appearance for Image-Text Alignment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071951},
doi = {10.1145/2072298.2071951},
abstract = {We present a method for automatically aligning words to image regions that integrates specific object classifiers (e.g., "car" detectors) with weak models based on appearance features. Previous strategies have largely focused on the latter, and thus have not exploited progress on object category recognition. Hence, we augment region labeling with object detection, which simplifies the problem by reliably identifying a subset of the labels, and thereby reducing correspondence ambiguity overall. Comprehensive testing on the SAIAPR TC dataset shows that principled integration of object detection improves the region labeling task.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1113–1116},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071952,
author = {Han, Biao and Zhu, Hao and Ding, Youdong},
title = {Bottom-up Saliency Based on Weighted Sparse Coding Residual},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071952},
doi = {10.1145/2072298.2071952},
abstract = {The guidance of attention helps the human vision system (HVS) to detect and recognize objects rapidly. In this paper, we propose a bottom-up saliency algorithm based on sparse coding theory. Sparse coding decomposes the inputs into two parts, codes and residual. From the viewpoint of biological vision and information theory, the coding length is closely related to the local complexity while the residual is closely related to the uncertainty. The proposed algorithm defines the weighted residual using sparse coding length as saliency. By multiplying the L0 norm of sparse codes and the residual, a saliency map is obtained. The performance of the proposed method is evaluated using ROC curves with two different scale datasets and is compared with state-of-the-art models. Our algorithm outperforms all other methods and the results indicate a robust and accurate saliency.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1117–1120},
numpages = {4},
keywords = {visual attention, bottom-up saliency, sparse coding},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071953,
author = {Ni, Yuzhao and Dong, Jian and Feng, Jiashi and Yan, Shuicheng},
title = {Purposive Hidden-Object-Game: Embedding Human Computation in Popular Game},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071953},
doi = {10.1145/2072298.2071953},
abstract = {Having sufficient training images with known locations of objects is critical for modern image annotation, image retrieval, and object detection tasks. However, it is typically extremely labor-intensive to collect such data, as the process usually involves tedious manual-cropping and hand-labeling. In this work, following the principle of games with a purpose (GWAP), we design a so-called purposive hidden-object-game (P-HOG), which seamlessly embeds object localization into the enjoyable HOG gaming process. As indicated by its large number of online players and downloads, HOG is very popular and P-HOG thus possesses great potentials in aggregating massive informative annotations. For each gaming image, besides identifying the known hidden objects inserted automatically towards semantic and visual naturalness, players also imperceptibly locate the spatial positions of the unknown objects, which are indicated by the refined user-provided tags from Flickr.com or other photo sharing websites. We conduct a pilot study of the game prototype and the comprehensive experiments show that the P-HOG appeals to general players, and is effective for collecting massive object locations with satisfying accuracy.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1121–1124},
numpages = {4},
keywords = {human computing, GWAP, hidden object game},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071954,
author = {Bansal, Mayank and Sawhney, Harpreet S. and Cheng, Hui and Daniilidis, Kostas},
title = {Geo-Localization of Street Views with Aerial Image Databases},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071954},
doi = {10.1145/2072298.2071954},
abstract = {We study the feasibility of solving the challenging problem of geolocalizing ground level images in urban areas with respect to a database of images captured from the air such as satellite and oblique aerial images. We observe that comprehensive aerial image databases are widely available while complete coverage of urban areas from the ground is at best spotty. As a result, localization of ground level imagery with respect to aerial collections is a technically important and practically significant problem. We exploit two key insights: (1) satellite image to oblique aerial image correspondences are used to extract building facades, and (2) building facades are matched between oblique aerial and ground images for geo-localization. Key contributions include: (1) A novel method for extracting building facades using building outlines; (2) Correspondence of building facades between oblique aerial and ground images without direct matching; and (3) Position and orientation estimation of ground images. We show results of ground image localization in a dense urban area.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1125–1128},
numpages = {4},
keywords = {street view, aerial view, air-ground matching, geo-localization, self-similarity},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071956,
author = {Sang, Jitao and Liu, Jing and Xu, Changsheng},
title = {Exploiting User Information for Image Tag Refinement},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071956},
doi = {10.1145/2072298.2071956},
abstract = {Photo sharing websites allow users to describe images with freely chosen tags. The user-generated tags not only facilitate the users in sharing and organizing images, but also provide large scale meaningful data for image retrieval and management. Extensive studies on improving the quality of user-generated tags for tag-based applications focused on exploiting the image-tag, image-image and tag-tag binary relationships. Considering that user is the originator of the tagging activity and user involves with image and tag in many aspects, in this paper we tackle the problem of tag refinement by leveraging user information. We propose a Tensor Decomposition framework to jointly model the ternary user-image-tag interrelation and respective intra-relations. The users, images and tags are represented in the corresponding latent subspaces. For a given image, the tags with the highest cross-space associations are reserved as the final annotation. The proposed method is validated on a large-scale real-world dataset.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1129–1132},
numpages = {4},
keywords = {factor analysis, social images, tag refinement},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071957,
author = {Wan, Kong-Wah and Zheng, Yan-Tao and Chaisorn, Lekha},
title = {Known-Item Video Search via Query-to-Modality Mapping},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071957},
doi = {10.1145/2072298.2071957},
abstract = {We introduce a novel query-to-modality mapping approach to the TRECVid 2010 known-Item video search (KIS) task. To search for a specific target video, a KIS query is verbose with many multi-modal attributes. Issuing all search terms to a retrieval engine will confuse the search criteria in different modalities and result in "topic drift". We propose decomposing a KIS query into a set of short uni-modal subqueries and issue them to the search index of the corresponding modality features, such as text-based metadata, visualbased high-level features. To do so, we introduce novel syntactic query features and cast the query-to-modality mapping as a classification problem. Retrieval results on the TRECVid 2010 KIS dataset shows that our approach outperforms existing methods by a significant margin.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1133–1136},
numpages = {4},
keywords = {multimedia search, query segmentation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071958,
author = {Yang, Yang and Yang, Yi and Huang, Zi and Shen, Heng Tao},
title = {Transfer Tagging from Image to Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071958},
doi = {10.1145/2072298.2071958},
abstract = {Nowadays massive amount of web video datum has been emerging on the Internet. To achieve an effective and efficient video retrieval, it is critical to automatically assign semantic keywords to the videos via content analysis. However, most of the existing video tagging methods suffer from the problem of lacking sufficient tagged training videos due to high labor cost of manual tagging. Inspired by the observation that there are much more well-labeled data in other yet relevant types of media (e.g. images), in this paper we study how to build a "cross-media tunnel" to transfer external tag knowledge from image to video. Meanwhile, the intrinsic data structures of both image and video spaces are well explored for inferring tags. We propose a Cross-Media Tag Transfer (CMTT) paradigm which is able to: 1) transfer tag knowledge between image and video by minimizing their distribution difference; 2) infer tags by revealing the underlying manifold structures embedded within both image and video spaces. We also learn an explicit mapping function to handle unseen videos. Experimental results have been reported and analyzed to illustrate the superiority of our proposal.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1137–1140},
numpages = {4},
keywords = {tag transfer, video tagging, tag inference, cross-media},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071959,
author = {Sengamedu, Srinivasan H. and Sanyal, Subhajit and Satish, Sriram},
title = {Detection of Pornographic Content in Internet Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071959},
doi = {10.1145/2072298.2071959},
abstract = {Pornographic image detection is an important and challenging problem. Detection of pornography on the Internet is even more challenging because of the scale (billions of images) and diversity (small to very large images, graphic, grey scale images, etc.) of image content. The performance requirements (precision, recall, and speed) are also very stringent. Because of this, no single technique provides the required performance. In this paper, we describe a framework for detecting images with pornographic content. The framework combines various techniques based on object-level and pixel-level analysis of image content. To enable high-precision, we detect body parts (including faces) in images. For high-recall, low-level techniques like color and texture features are used. For adaptation to new datasets, we also support learning of appropriate color models from weakly-labeled datasets. In addition to image-based analysis, both text-based and site-level analysis are performed. Unlike many adult detection techniques, we explicitly leverage techniques like texture analysis and face detection for non-adult content identification. The multiple cues are combined in a systematic manner using ROC analysis and boosting. Evaluations on real world web data indicate that the system has the best performance among the systems compared.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1141–1144},
numpages = {4},
keywords = {body part detection, adult image detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071960,
author = {Yang, Chunlei and Shen, Jialie and Fan, Jianping},
title = {Effective Summarization of Large-Scale Web Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071960},
doi = {10.1145/2072298.2071960},
abstract = {In this paper, we present a novel framework to achieve effective summarization of large-scale web images by treating the problem of automatic image summarization as the problem of dictionary learning for sparse coding, e.g., the summary of a given image set can be treated as a sparse representation of the given image set (i.e., sparse dictionary for the given image set). For a given semantic category (i.e., certain object class or image concept), we build a sparsity model to reconstruct all its relevant images by using a subset of most representative images (i.e., image summary); and a stepwise basis selection algorithm is developed to learn such sparse dictionary (i.e., image summary) by minimizing an explicit optimization function. By investigating their reconstruction ability, the reconstruction Mean Square Error (MSE) is adapted to objectively measure the performance of various algorithms for automatic image summarization. Our experimental results demonstrate that our dictionary learning for sparse representation algorithm can obtain more accurate summary as compared with other baseline algorithms for automatic image summarization.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1145–1148},
numpages = {4},
keywords = {sparse coding, dictionary learning, automatic image summarization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071961,
author = {Yu, Gang and Yuan, Junsong and Liu, Zicheng},
title = {Real-Time Human Action Search Using Random Forest Based Hough Voting},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071961},
doi = {10.1145/2072298.2071961},
abstract = {Many existing techniques in content based video retrieval treat a video sequence as a whole to match it against a query video or to assign a text label. Such an approach has serious limitations when applied to human action retrieval because an action may occur only in a sub-region and last for a small portion of the video length. In situations like this, we essentially need to match the subvolumes of the video sequences against the query video. A naive exhaustive search is impractical due to large number of possible subvolumes for each video sequence. In this paper, we propose a novel framework for action retrieval which performs pattern matching at subvolume level and is very efficient in handling large corpus of videos. We construct an unsupervised random forest to index the video database, generate a score volume with Hough voting and then employ a max sub-path strategy to quickly search for the temporal and spatial positions of all the video sequences in the database. We present action search experiments on challenging datasets to validate the efficiency and effectiveness of our system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1149–1152},
numpages = {4},
keywords = {random forest indexing, action search, max sub-path search, hough voting},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071962,
author = {Xu, Xin-Shun and Xue, Xiangyang and Zhou, Zhi-Hua},
title = {Ensemble Multi-Instance Multi-Label Learning Approach for Video Annotation Task},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071962},
doi = {10.1145/2072298.2071962},
abstract = {Automatic video annotation is an important ingredient for video indexing, browsing, and retrieval. Traditional studies represent one video clip with a flat feature vector; however, video data usually has natural structure. Moreover, a video clip is generally relevant to multiple concepts. Indeed, the video annotation task is inherently a Multi-Instance Multi-Label (MIML) learning problem. In this paper, we propose the En-MIMLSVM approach for the video annotation task. It considers the class imbalance and long time training problems of most video annotation tasks. In addition, a temporally consistent weighted multi-instance kernel is developed to take into account both the temporal consistency in video data and the significance of instances of different levels in pyramid representation. The En-MIMLSVM is evaluated on TRECVID 2005 data set, and the results show that it outperforms several state-of-the-art methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1153–1156},
numpages = {4},
keywords = {multi-instance multi-label learning, video annotation, ensemble methods},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071963,
author = {Li, Ce and Xue, Jianru and Zheng, Nanning and Tian, Zhiqiang},
title = {Nonparametric Bottom-up Saliency Detection Using Hypercomplex Spectral Contrast},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071963},
doi = {10.1145/2072298.2071963},
abstract = {Saliency detection is an useful technique for image semantic analysis such as auto image segmentation, image retargeting, advertising design and image compression. Inspired by two existing saliency detection algorithms, named spectral residual (SR) and phase spectrum of quaternion Fourier transform (PQFT), we propose a new bottom-up saliency detection method which is featured with the introduction of hypercomplex spectral contrast (HSC) in saliency detection. The proposed HSC algorithm introduces the HSV color image vector space in hypercomplex number, and is better comprehensive to consider amplitude spectral contrast into saliency model as well as phase spectral contrast. Meanwhile, we also incorporate the human vision nonuniform sampling into our model, which is a common phenomenon that directs visual attention to the logarithmic center of image in natural scenes. Experimental results on two public saliency detection datasets show that our approach performs better than four state-of-the art approaches remarkably.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1157–1160},
numpages = {4},
keywords = {hypercomplex fourier transform, nonuniform sampling, spectral contrast, visual saliency},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071964,
author = {Wang, LiMin and Wu, Yirui and Lu, Tong and Chen, Kang},
title = {Multiclass Object Detection by Combining Local Appearances and Context},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071964},
doi = {10.1145/2072298.2071964},
abstract = {In this paper, we present a novel approach for multiclass object detection by combining local appearances and contextual constraints. We first construct a multiclass Hough forest of local patches, which can well deal with multiclass object deformations and local appearance variations, due to randomization and discrimination of the forest. Then, in the object hypothesis space, a new multiclass context model is proposed to capture relative location constraints, disambiguating appearance inputs in multiclass object detection. Finally, multiclass objects are detected with a greedy search algorithm efficiently. Experimental evaluations on two image data sets show that the combination of local appearances and context achieves state-of-the-art performance in multiclass object detection.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1161–1164},
numpages = {4},
keywords = {multiclass hough forest, context model, object detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071965,
author = {Luo, Yong and Tao, Dacheng and Geng, Bo and Xu, Chao and Maybank, Stephen},
title = {Shared Feature Extraction for Semi-Supervised Image Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071965},
doi = {10.1145/2072298.2071965},
abstract = {Multi-task learning (MTL) plays an important role in image analysis applications, e.g. image classification, face recognition and image annotation. That is because MTL can estimate the latent shared subspace to represent the common features given a set of images from different tasks. However, the geometry of the data probability distribution is always supported on an intrinsic image sub-manifold that is embedded in a high dimensional Euclidean space. Therefore, it is improper to directly apply MTL to multiclass image classification. In this paper, we propose a manifold regularized MTL (MRMTL) algorithm to discover the latent shared subspace by treating the high-dimensional image space as a sub-manifold embedded in an ambient space. We conduct experiments on the PASCAL VOC'07 dataset with 20 classes and the MIR dataset with 38 classes by comparing MRMTL with conventional MTL and several representative image classification algorithms. The results suggest that MRMTL can properly extract the common features for image representation and thus improve the generalization performance of the image classification models.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1165–1168},
numpages = {4},
keywords = {image classification, manifold regularization, multi-task learning, semi-supervised},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071966,
author = {Li, Yangxi and Geng, Bo and Zha, Zheng-Jun and Tao, Dacheng and Yang, Linjun and Xu, Chao},
title = {Difficulty Guided Image Retrieval Using Linear Multiview Embedding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071966},
doi = {10.1145/2072298.2071966},
abstract = {Existing image retrieval systems suffer from a radical performance variance for different queries. The bad initial search results for "difficult" queries may greatly degrade the performance of their subsequent refinements, especially the refinement that utilizes the information mined from the search results, e.g., pseudo relevance feedback based reranking. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which selectively performs reranking according to the estimated query difficulty. To improve the performance of both reranking and difficulty estimation, we apply multiview embedding (ME) to images represented by multiple different features for integrating a joint subspace by preserving the neighborhood information in each feature space. However, existing ME approaches suffer from both "out of sample" and huge computational cost problems, and cannot be applied to online reranking or offline large-scale data processing for practical image retrieval systems. Therefore, we propose a linear multiview embedding algorithm which learns a linear transformation from a small set of data and can effectively infer the subspace features of new data. Empirical evaluations on both Oxford and 500K ImageNet datasets suggest the effectiveness of the proposed difficulty guided retrieval system with LME.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1169–1172},
numpages = {4},
keywords = {query difficulty estimation, reranking, spectral embedding},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071967,
author = {Chen, Tianlong and Jiang, Shuqiang and Chu, Lingyang and Huang, Qingming},
title = {Detection and Location of Near-Duplicate Video Sub-Clips by Finding Dense Subgraphs},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071967},
doi = {10.1145/2072298.2071967},
abstract = {Robust and fast near-duplicate video detection is an important task with many potential applications. Most existing systems focus on the comparison between full copy videos or partial near-duplicate videos. While it is more challenging to find similar content for videos containing multiple near-duplicate segments at random locations with various connections. In this paper, we propose a new graph based method to detect complex near-duplicate video sub-clips. First, we develop a new succinct video descriptor for keyframe match. Then a graph is established to exploit temporal consistency of matched keyframes. The nodes of the graph are the matched frame pairs; the edge weights are computed from the temporal alignment and frame pair similarities. In this way, the validly matched keyframes would form a dense subgraph whose nodes are strongly connected. This graph model also preserves the complex connections of sub-clips. Thus detecting complex near-duplicate sub-clips is transformed to the problem of finding all the dense subgraphs. We employ the optimization method of graph shift to solve this problem due to its robust performance. The experiments are conducted on the dataset with various transformations and complex temporal relations. The results demonstrate the effectiveness and efficiency of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1173–1176},
numpages = {4},
keywords = {near-duplicate video detection, graph shift, sub-clip location},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071968,
author = {Li, Yingfei and Geng, Bo and Zha, Zheng-jun and Li, Yangxi and Tao, Dacheng and Xu, Chao},
title = {Query Expansion by Spatial Co-Occurrence for Image Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071968},
doi = {10.1145/2072298.2071968},
abstract = {The well-known bag-of-features (BoF) model is widely utilized for large scale image retrieval. However, BoF model lacks the spatial information of visual words, which is informative for local features to build up meaningful visual patches. To compensate for the spatial information loss, in this paper, we propose a novel query expansion method called Spatial Co-occurrence Query Expansion (SCQE), by utilizing the spatial co-occurrence information of visual words mined from the database images to boost the retrieval performance. In offline phase, for each visual word in the vocabulary, we treat the visual words that are frequently co-occurred with it in the database images as neighbors, base on which a spatial co-occurrence graph is built. In online phase, a query image can be expanded with some spatial co-occurred but unseen visual words according to the spatial co-occurrence graph, and the retrieval performance can be improved by expanding these visual words appropriately. Experimental results demonstrate that, SCQE achieves promising improvements over the typical BoF baseline on two datasets comprising 5K and 505K images respectively.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1177–1180},
numpages = {4},
keywords = {query expansion, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071969,
author = {Sun, Aixin and Bhowmick, Sourav S. and Chong, Jun-An},
title = {Social Image Tag Recommendation by Concept Matching},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071969},
doi = {10.1145/2072298.2071969},
abstract = {Tags associated with social images are valuable information source for superior image search and retrieval experiences. In this paper, we propose a novel tag recommendation technique that exploits the user-given tags associated with images. Each candidate tag to be recommended is described by a few tag concepts derived from the collective knowledge embedded in the tag co-occurrence pairs. Each concept, represented by a few tags with high co-occurrences among themselves, is indexed as a textual document. Then user-given tags of an image is represented as a text query and the matching concepts are retrieved from the index. The candidate tags associated with the matching concepts are then recommended. Leverages on the well studied Information Retrieval (IR) techniques, the proposed approach leads to superior tag recommendation accuracy and lower execution time compared to the state-of-the-art.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1181–1184},
numpages = {4},
keywords = {flickr, tag recommendation, social image, tag concept},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071970,
author = {Zhang, Wei and Lu, Yao and Xue, Xiangyang and Fan, Jianping},
title = {Automatic Image Annotation with Weakly Labeled Dataset},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071970},
doi = {10.1145/2072298.2071970},
abstract = {It is very attractive to exploit weakly-labeled image dataset for multi-label annotation applications. In our paper the meaning of the terminology weakly labeled is threefold: i) only a small subset of the available images are labeled; ii) even for the labeled image, the given labels may be uncorrect or incomplete; iii) the given labels do not provide the exact object locations in the images. A novel method is developed to predict the multiple labels for images and to provide region-level labels for the objects. We cluster the image regions to learn several region-exemplars and predict the label vector for each image region as a locally weighted average of the label vectors on exemplars. By investigating the label confidence matrix for the region-exemplars from different perspectives (column picture and row picture), we sufficiently leverage the visual contexts, the semantic contexts, and the consistency between similarities in the visual feature space and semantic label space. Experimental results on real web images demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1185–1188},
numpages = {4},
keywords = {weakly-labeled dataset, region-level labels, multi-label image annotation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071971,
author = {Zhang, Wei and Gao, Ke and Zhang, Yongdong and Li, Jintao},
title = {Efficient Approximate Nearest Neighbor Search with Integrated Binary Codes},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071971},
doi = {10.1145/2072298.2071971},
abstract = {Nearest neighbor search in Euclidean space is a fundamental problem in multimedia retrieval. The difficulty of exact nearest neighbor search has led to approximate solutions that sacrifice precision for efficiency. Among such solutions, approaches that embed data into binary codes in Hamming space have gained significant success for their efficiency and practical memory requirements. However, binary code searching only finds a big and coarse set of similar neighbors in Hamming space, and hence expensive Euclidean distance based ranking of the coarse set is needed to find nearest neighbors. Therefore, to improve nearest neighbor search efficiency, we proposed a novel binary code method called Integrated Binary Code (IBC) to get a compact set of similar neighbors. Experiments on public datasets show that our method is more efficient and effective than state-of-the-art in approximate nearest neighbor search.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1189–1192},
numpages = {4},
keywords = {approximate nearest neighbor search, similarity search},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071972,
author = {Yang, Peng and Li, Hui and Liu, Qingshan and Zhong, Lin and Metaxas, Dimitris},
title = {Content Quality Based Image Retrieval with Multiple Instance Boost Ranking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071972},
doi = {10.1145/2072298.2071972},
abstract = {Most previous works treated image retrieval as a classification problem or a similarity measurement problem. In this paper, we propose a new idea for image retrieval, in which we regard image retrieval as a ranking issue by evaluating image content quality. Based on the content preference between the images, the image pairs are organized to build the data set for rank learning. Because image content generally is disclosed by image patches with meaningful objects, each image is looked as one bag, and the regions inside are the corresponding instances. In order to save the computation cost, the instances in the image are the rectangle regions and the integral histogram is applied to speed up histogram feature extraction. Due to the feature dimension is high, we propose a boost-based multiple instance learning for image retrieval. Based on different assumptions in multiple instance setting, Mean, Max and TopK ranking models are developed with Boost learning. Experiments on the real-world images from Flickr, Pisca, and Google shows that the power of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1193–1196},
numpages = {4},
keywords = {multiple instance, image retrieval, ranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071973,
author = {Zheng, Ying and Gu, Steve and Tomasi, Carlo},
title = {Detecting Motion Synchrony by Video Tubes},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071973},
doi = {10.1145/2072298.2071973},
abstract = {Motion synchrony, i.e., the coordinated motion of a group of individuals, is an interesting phenomenon in nature or daily life. Fish swim in schools, birds fly in flocks, soldiers march in platoons, etc. Our goal is to detect motion synchrony that may be present in the video data, and to track the group of moving objects as a whole. This opens the door to novel algorithms and applications. To this end, we model individual motions as video tubes in space-time, define motion synchrony by the geometric relation among video tubes, and track a whole set of tubes by dynamic programming. The resulting algorithm is highly efficient in practice. Given a video clip of T frames of resolution XxY, we show that finding the K spatially correlated video tubes and determining the presence of synchrony can be solved optimally in O(XYTK) time. Preliminary experiments show that our method is both effective and efficient. Typical running times are 30 - 100 VGA-resolution frames per second after feature extraction, and the accuracy for the detection of synchrony is more than 90% as evaluated in our annotated data set.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1197–1200},
numpages = {4},
keywords = {motion analysis, synchronization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071974,
author = {Bracamonte, Teresa and Poblete, Barbara},
title = {Automatic Image Tagging through Information Propagation in a Query Log Based Graph Structure},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071974},
doi = {10.1145/2072298.2071974},
abstract = {Annotating or tagging multimedia objects is an important task for enhancing multimedia information retrieval processes. In the context of the Web, automatic tagging deals with many issues, such as loosely tagged images and huge collections of images with no textual data at all. Recently, graph representations have been shown useful for modeling relationships between images and their associated semantics. Using these types of graphs, it is possible to represent images and their textual labels as nodes, and the relationship between them as edges, under the assumption that visual similarity implies semantic similarity. In this work, we present an algorithm for automatic tag propagation in such a graph structure, called the visual-semantic graph. This graph has been used in prior work only for the task of image retrieval re-ranking. The goal of our work, is to show how the visual-semantic graph can be used for efficient tag propagation to unlabeled images. More specifically, our contributions are: (1) An algorithm to propagate tags automatically based on the breadth-first traversal and (2) A set of heuristics for pruning this approach for large size collections.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1201–1204},
numpages = {4},
keywords = {tag propagation, query log analysis, automatic tagging, web mining},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071975,
author = {Kennedy, Lyndon and Slaney, Malcolm},
title = {Identifying Authoritative Sources of Multimedia Content: Mining Specificity and Expertise from Large-Scale Multimedia Databases},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071975},
doi = {10.1145/2072298.2071975},
abstract = {We present a framework for identifying authoritative sources (such as web sites or individual users) that are likely to produce high-quality or interesting images. We construct a directed graph across sources based on the propensity of one source to "cite" the content from another. A graph-centrality measure scores the authority for each source, which could then be applied for retrieval purposes. We apply this method to web image retrieval, where web sites are the sources, and citations are found via copy detection; and on a photo sharing site, where individuals are the sources and citations are users' favorites. We are able to identify primary or influential sources of media while avoiding the computational cost of other approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1205–1208},
numpages = {4},
keywords = {image citation, multimedia retrieval, source ranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071976,
author = {Yang, Kuiyuan and Zhang, Lei and Wang, Meng and Zhang, Hong-Jiang},
title = {Semantic Point Detector},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071976},
doi = {10.1145/2072298.2071976},
abstract = {Local features are the building blocks of many visual systems, and local point detector is usually the first component for local feature extraction. Existing local point detector are designed with target for matching and it may not perform well when applied in image content representation. Actually many existing studies demonstrate that the simple dense sampling strategy can achieve better performance than many local point detection methods in image classification tasks. In this paper, we propose a novel point detector named semantic point detector, which detects a set of semantically meaningful patches from each image and yields more compact and complete image representation. It is learned from an set of images with concepts from a large ontology. We conduct extensive experiments based on the proposed detector, and the experimental results demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1209–1212},
numpages = {4},
keywords = {semantic point detector},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071977,
author = {Su, Hsiao-Hang and Chen, Tse-Wei and Kao, Chieh-Chi and Hsu, Winston H. and Chien, Shao-Yi},
title = {Scenic Photo Quality Assessment with Bag of Aesthetics-Preserving Features},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071977},
doi = {10.1145/2072298.2071977},
abstract = {In this paper, an aesthetic modeling method for scenic photographs is proposed. A bottom-up approach is developed to construct an aesthetic library with bag-of-aesthetics preserving features instead of top-down methods that implement the heuristic guidelines (rule-specific features) listed in the photography literature, which is employed in previous works. The proposed method can cover both implicit and explicit aesthetic features with a learning process. The experimental results show that the proposed features in the library (92.06% in accuracy) outperform the state-of-the-art rule-specific features (83.63% in accuracy) significantly in the aesthetic quality assessment for scenic photos, and the rule-specific features are also proved to be encompassed by the proposed features. Meanwhile, it is observed from experiments that the features extracted for contrast information are more effective than those for absolute information, which is consistent with the properties of human visual systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1213–1216},
numpages = {4},
keywords = {photo aesthetic quality assessment, aesthetic modeling},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071978,
author = {Xu, Hao and Wang, Jingdong and Hua, Xian-Sheng and Li, Shipeng},
title = {Hybrid Image Summarization},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071978},
doi = {10.1145/2072298.2071978},
abstract = {In this paper, we address a problem of managing tagged images with hybrid summarization. We formulate this problem as finding a few image exemplars to represent the image set semantically and visually and solve it in a hybrid way by exploiting both visual and textual information associated with images. We propose a novel approach, called Homogeneous and Heterogeneous Message Propagation (H2MP), which extends affinity propagation that only works over homogeneous relations to heterogeneous relations. The summary obtained by our approach is both visually and semantically satisfactory. The experimental results demonstrate the effectiveness and efficiency of the proposed approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1217–1220},
numpages = {4},
keywords = {affinity propagation, image summarization, homogeneous and heterogeneous message propagation, image presentation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071979,
author = {Anguera, Xavier and Barrios, Juan Manuel and Adamek, Tomasz and Oliver, Nuria},
title = {Multimodal Fusion for Video Copy Detection},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071979},
doi = {10.1145/2072298.2071979},
abstract = {Content-based video copy detection algorithms (CBCD) focus on detecting video segments that are identical or transformed versions of segments in a known video. In recent years some systems have proposed the combination of orthogonal modalities (e.g. derived from audio and video) to improve detection performance, although not always achieving consistent results. In this paper we propose a fusion algorithm that is able to combine as many modalities as available at the decision level. The algorithm is based on the weighted sum of the normalized scores, which are modified depending on how well they rank in each modality. This leads to a virtually parameter-free fusion algorithm. We performed several tests using 2010 TRECVID VCD datasets and obtain up to 46% relative improvement in min-NDCR while also improving the F1 metric on the fused results in comparison to just using the best single modality.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1221–1224},
numpages = {4},
keywords = {content-based video copy detection, multimodal, fusion},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071980,
author = {Junqiang, Wang and Ma, Huadong},
title = {Pedestrian Detection with Geometric Context from a Single Image},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071980},
doi = {10.1145/2072298.2071980},
abstract = {We address the problem of pedestrian detection in still images. Many current pedestrian detection systems limit their performance by ignoring the underlying 3D geometric context in the image. We can estimate the geometric context by learning appearance-based method. We propose a novel context feature and local integrable features. These features are used for building many candidate weak classifiers by using linear SVM. Finally, MPL-Boost method selects the best weak classifiers suited for detection and construct the rejector-based cascade detector. We provide a thorough quantitative evaluation of our method on TUD-Brussels dataset and demonstrate that it outperforms the state-of-the-art pedestrian detector in recall rate, meanwhile, shows faster speed than other context incorporation method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1225–1228},
numpages = {4},
keywords = {geometric context, multi-pose learning boost, pedestrian detection, intersection kernel SVM},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071981,
author = {Niu, Di and Xu, Hong and Li, Baochun and Zhao, Shuqiao},
title = {Risk Management for Video-on-Demand Servers Leveraging Demand Forecast},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071981},
doi = {10.1145/2072298.2071981},
abstract = {Video-on-demand (VoD) servers are usually over-provisioned for peak demands, incurring a low average resource efficiency. However, bandwidth shortage may still occur for individual videos as they share and contend for server resources. In this position paper, we propose a predictive workload management system for VoD servers targeting bandwidth. The system draws belief about future demand as well as demand volatility based on demand history using time series forecasting techniques. The prediction enables dynamic and efficient server bandwidth reservation with QoS guarantees. More importantly, we use a hedging technique similar to investment portfolio management and distribute workloads to multiple servers exploiting demand anti-correlation. The proposed system consolidates the workloads, enhances resource utilization, while in the meantime effectively controlling risk of server overload. The proposed methods are evaluated based on real-world VoD traces.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1229–1232},
numpages = {4},
keywords = {video-on-demand, demand prediction, risk management, bandwidth reservation, hedging},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071982,
author = {Wang, Zhi and Sun, Lifeng and Yang, Shiqiang and Zhu, Wenwu},
title = {Prefetching Strategy in Peer-Assisted Social Video Streaming},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071982},
doi = {10.1145/2072298.2071982},
abstract = {Online social network has emerged as the most popular approach for people to directly access multimedia contents. Among these contents, video sharing is a challenging task due to the demand on a large amount of uplink bandwidth at the dedicated server. We leverage a P2P paradigm to alleviate the server to distribute shared videos. By investigating traces obtained from a popular online social network in China, we observe that users' preferences can be predicted. We design a user preference guided prefetching strategy to reduce video startup delays, enabling smooth playback. Simulation experiments show that our design achieves high prefetch accuracy and short startup delay with conservative storage and bandwidth capacities at peers.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1233–1236},
numpages = {4},
keywords = {streaming, prefetching, social network, p2p},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071983,
author = {Miao, Dan and Zhu, Wenwu and Luo, Chong and Chen, Chang Wen},
title = {Resource Allocation for Cloud-Based Free Viewpoint Video Rendering for Mobile Phones},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071983},
doi = {10.1145/2072298.2071983},
abstract = {Free viewpoint video (FVV) / Free viewpoint TV (FTV) on mobile devices over cellular networks is very challenging due to the requirement for large bandwidth and limitations in computation and battery life on mobile phones. To address such challenges, in this paper we propose a cloud-based FVV / FTV rendering framework for mobile devices over cellular networks. In this framework, cloud performs rendering for mobile devices. In order to achieve maximum QoE (Quality of Experience) for mobile users, we propose a novel resource allocation scheme, which jointly considers rendering allocation between cloud and client based on user's QoE and rate allocation among texture, depth, and channel rate based on rate-distortion analysis. We formulate this resource allocation scheme as an optimization problem which can then be transformed into a convex optimization for the given rate ratio. Experimental results demonstrate that the proposed cloud-based FVV rendering solution can substantially improve video quality on mobile devices comparing with traditional approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1237–1240},
numpages = {4},
keywords = {FVV, resource allocation, cloud computing, mobile devices},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071984,
author = {Zhao, Zhen Wei and Ooi, Wei Tsang},
title = {APRICOD: A Distributed Caching Middleware for Fast Content Discovery of Non-Continuous Media Access},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071984},
doi = {10.1145/2072298.2071984},
abstract = {We propose an access pattern-driven distributed caching middleware named APRICOD, which can be built on top of any existing content discovery system. APRICOD caters for fast and scalable content discovery in peer-to-peer media streaming systems, especially when user interactions that leads to non-continuous media access (such as random seek in video and teleportation in virtual environment) are present. APRICOD caches query results based correlations among media objects accessed by users, reducing the content discovery time. Our evaluation using a VoD access trace shows that close to 70% of non-continuous access queries can be resolved with a single hop using APRICOD.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1241–1244},
numpages = {4},
keywords = {caching, content discovery system, non-continuous media access, peer-to-peer, user interaction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071985,
author = {Battiato, Sebastiano and Farinella, Giovanni Maria and Messina, Enrico and Puglisi, Giovanni},
title = {Robust Image Registration and Tampering Localization Exploiting Bag of Features Based Forensic Signature},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071985},
doi = {10.1145/2072298.2071985},
abstract = {The distribution of digital images with the classic and newest technologies available on Internet (e.g., emails, social networks, digital repositories) has induced a growing interest on systems able to protect the visual content against malicious manipulations that could be performed during their transmission. One of the main problems addressed in this context is the authentication of the image received in a communication. This task is usually performed by localizing the regions of the image which have been tampered. To this aim the received image should be first registered with the one at the sender by exploiting the information provided by a specific component of the forensic hash associated with the image. In this paper we propose a robust alignment method which makes use of an image signature based on the Bag of Features paradigm. The alignment is based on a voting procedure in the parameter space of the model used to recover the geometric transformation occurred into the manipulated image. Experiments show that the proposed approach obtains good margin in terms of performances with respect to state-of-the art methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1245–1248},
numpages = {4},
keywords = {image authentication, image registration, forensic hash, image forensics, bag of features, tampering localization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071986,
author = {Xue, Xiangyang and Li, Wei and Yin, Yue},
title = {Towards Content-Based Audio Fragment Authentication},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071986},
doi = {10.1145/2072298.2071986},
abstract = {Audio authentication is a technique to protect the integrity and originality of audio signals. Due to the long duration, it is often desirable to authenticate only a segment of audio signal. To the authors' knowledge, this important issue has not been seriously researched so far. In this paper, a novel authentication algorithm is proposed for the purpose of audio fragment authentication. SIFT descriptor originated from computer vision field is introduced and calculated on audio spectrogram to accomplish the tasks of fragment alignment, time-domain blocking, cropping and inserting identification etc. Experiments show reliable discrimination between admissible and malicious manipulations, precise tamper localization and classification.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1249–1252},
numpages = {4},
keywords = {spectrogram, audio, fragment authentication, SIFT},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071987,
author = {Min, Rui and Dugelay, Jean-Luc},
title = {Cap Detection for Moving People in Entrance Surveillance},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071987},
doi = {10.1145/2072298.2071987},
abstract = {While there has been an enormous amount of research on face recognition under pose/illumination changes and image degradations, problems caused by occlusions are mostly overlooked. Moreover, most of the existing approaches of face recognition under occlusion conditions focus on overcoming facial occlusion problems due to sunglasses and scarf. To the best of our knowledge, occlusion due to cap has never been studied in the literature, but the importance of this problem should be emphasized since it is known that bank robbers and football hooligans take advantage of it for hiding their faces. This paper presents a solution to this newly identified face occlusion problem -- the time-variant occlusion due to cap in entrance surveillance, in the context of face biometrics in video surveillance. The proposed approach consists of two parts: detection and tracking of occluded faces in complex surveillance videos; detecting the presence of cap by exploiting temporal information. The detection and tracking part is based upon body silhouette and elliptical head tracker. The classification of cap/non-cap faces utilizes dynamic time warping (DTW) and agglomerative hierarchical clustering. The proposed algorithm is evaluated on several surveillance videos and yields good detection rates.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1253–1256},
numpages = {4},
keywords = {occlusion detection, security management, entrance surveillance, face recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071988,
author = {Cheng, Han-Ping and Shen, Yun-Chung and Wu, Ja-Ling and Aizawa, Kiyoharu},
title = {High Efficient Distributed Video Coding with Parallelized Design for Cloud Computing},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071988},
doi = {10.1145/2072298.2071988},
abstract = {In this work, by combining coding tools developed in recent literatures on transform domain WZ video coding with some newly developed modules on both encoding and decoding sides, an efficient and practical WZ video coding architecture, dubbed as DIStributed video coding with PArallelized design for Cloud computing (DISPAC), is proposed to better the corresponding rate-distortion (RD) performance. Another unique feature of DISPAC, lies in the parallelizability of the modules used by its WZ decoder which increased the decoding speed largely. Experimental results conducted on an emulated Could computing environment reveal that DISPAC codec can gain up to 3.6 dB in the RD measures and 60.97 times faster in the decoding speed as compared with the-state-of-art WZ video codec, respectively},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1257–1260},
numpages = {4},
keywords = {multi-core, cloud computing, distributed video coding, side information refinement, CUDA, wyner-ziv video coding, block mode selection, parallel},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071989,
author = {Su, Tse-Chung and Shen, Yun-Chung and Wu, Ja-Ling},
title = {Real-Time Decoding for LDPC Based Distributed Video Coding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071989},
doi = {10.1145/2072298.2071989},
abstract = {Wyner-Ziv (WZ) video coding -- a particular case of distributed video coding (DVC), is well known for its low-complexity encoding and high-complexity decoding characteristics. Although some works have been made in recent years, especially for improving the coding efficiency, most reported WZ codecs have high time delay in the decoder, which hinders its practical values for applications with critical timing constraint. In this paper, a fully parallelized sum-product algorithm (SPA) for low density parity check accumulate (LDPCA) codes is proposed and realized through Compute Unified Device Architecture (CUDA) based on General-Purpose Graphics Processing Unit (GPGPU). Simulation results show that, through our work, QCIF (surveillance) videos can be decoded in real-time with extremely high quality and without rate-distortion (RD) performance loss.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1261–1264},
numpages = {4},
keywords = {DVC, parallel computing, CUDA, SPA, LPDC, GPGPU},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071990,
author = {Uchihashi, Shingo and Tanzawa, Tsutomu},
title = {Mixing Remote Locations Using Shared Screen as Virtual Stage},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071990},
doi = {10.1145/2072298.2071990},
abstract = {This paper introduces a visual communication system that integrates images displayed on a shared screen and people in front of the screen using depth information obtained from a stereo camera. Conventional video conferencing systems convey PC screen contents and video of local and remote participants in different channels, making interactions among them difficult. The proposed system maps persons in the foreground onto the screen preserving relative position and size. A series of preliminary experiments is conducted to compare the performance of the proposed system against conventional systems.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1265–1268},
numpages = {4},
keywords = {remote collaboration, mixed reality},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071991,
author = {Chen, Kuan-Ta and Chang, Yu-Chun and Tseng, Po-Han and Huang, Chun-Ying and Lei, Chin-Laung},
title = {Measuring the Latency of Cloud Gaming Systems},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071991},
doi = {10.1145/2072298.2071991},
abstract = {Cloud gaming, i.e., real-time game playing via thin clients, relieves players from the need to constantly upgrade their computers and deal with compatibility issues when playing games. As a result, cloud gaming is generating a great deal of interest among entrepreneurs and the public. However, given the large design space, it is not yet known which platforms deliver the best quality of service and which design elements constitute a good cloud gaming system.This study is motivated by the question: How good is the real-timeliness of current cloud gaming systems? To address the question, we analyze the response latency of two cloud gaming platforms, namely, OnLive and StreamMyGame. Our results show that the streaming latency of OnLive is reasonable for real-time cloud gaming, while that of StreamMyGame is almost twice the former when the StreamMyGame server is provisioned using an Intel Core i7-920 PC. We believe that our measurement approach can be generally applied to PC-based cloud gaming platforms, and that it will further the understanding of such systems and lead to improvements.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1269–1272},
numpages = {4},
keywords = {thin client, onlive, quality of service, cloud rendering, games-on-demand, cloud computing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071992,
author = {Tseng, Hsiao-Yun and Shen, Yun-Chung and Wu, Ja-Ling},
title = {Distributed Video Coding with Compressive Measurements},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071992},
doi = {10.1145/2072298.2071992},
abstract = {This paper presents a novel distributed video coding (DVC) scheme using compressive sensing (CS) that achieves low-complexity for encoding and efficient signal sensing. Most CS recovery algorithms rely only on signal sparsity. Yet, under DVC architecture, additional statistical characterization of the signal is available, which offers the potential for more precise CS recovery. First, a set of random measurements are acquired and transmitted to the decoder. The decoder then exploits the statistical characterization of the signal and generates the side information (SI). Finally, utilizing the SI, a Bayesian inference using belief propagation (BP) decoding is performed for signal recovery. The proposed CS-DVC system offers a more direct way of signal acquisition and the potential for more precise estimation of the signal from random measurements. Experimental results indicate that SI can improve the signal reconstruction quality in comparison with a CS recovery algorithm that relies only on the sparsity.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1273–1276},
numpages = {4},
keywords = {side information, Wyner-Ziv video coding, sparse representations, compressive sampling, distributed video coding},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071993,
author = {Huang, Lei and Xia, Tian and Wan, Ji and Zhang, Yongdong and Lin, Shouxun},
title = {Personalized Portraits Ranking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071993},
doi = {10.1145/2072298.2071993},
abstract = {Portraits, also known as images of people, constitute an important part of consumer photos. Existing methods manage portraits based on either explicit objectives, e.g., a specified person or event, or aesthetics, i.e., the aesthetic quality of portraits. This paper presents a novel system for personalized portraits ranking. First, four kinds of personalized features, i.e., composition, clothing style, affection and social relationship are proposed to quantify users' intent. Then, example-based and sketch-based user interfaces (UI) are developed, which are capable of capturing users' personal intent hardly described by queries or aesthetics. Finally, portraits ranking is implemented by combing these features together with the developed user interfaces. Experimental results show that the system performs well in providing personalized preferences and the proposed features are effective for portraits ranking. From the user study, our system gets promising results.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1277–1280},
numpages = {4},
keywords = {clothing style features, personalized ranking, affection features, example-based ranking, sketch-based ranking, portraits ranking, social relationship features},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071994,
author = {Lee, Jong-Seok and Goldmann, Lutz and Ebrahimi, Touradj},
title = {A New Analysis Method for Paired Comparison and Its Application to 3D Quality Assessment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071994},
doi = {10.1145/2072298.2071994},
abstract = {Among various subjective quality evaluation methodologies, paired comparison has the advantage of improved simplicity of the subjects' evaluation task due to simplified rating scales and direct comparison of two stimuli. Thus, it may lead to more reliable results when individual quality levels are difficult to define, quality differences between stimuli are small or multiple quality factors are involved. This paper proposes a new method to analyze results of paired comparison-based subjective tests. By assuming that ties convey information about significant differences between two stimuli being compared, the confidence intervals for the quality scores are estimated using a maximum likelihood criterion, which enables us to intuitively examine the significance of quality score differences. We describe the complete test methodology including the test procedure, outlier detection and score analysis applied to quality assessment of 3D images acquired using varying camera distances. Experimental results demonstrate the usefulness of the proposed analysis method, as well as the enhanced quality discriminability of the paired comparison methodology in comparison to the conventional single stimulus methodology.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1281–1284},
numpages = {4},
keywords = {quality of experience, maximum likelihood estimation, stereoscopic image, statistical significance, paired comparison},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071995,
author = {Gritti, Tommaso and Monaci, Gianluca},
title = {ImagiLight: A Vision Approach to Lighting Scene Setting},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071995},
doi = {10.1145/2072298.2071995},
abstract = {The advent of integrated lighting installations, consisting of individually controllable lamps with advanced rendering capabilities, is fundamentally transforming lighting. This brings a need for an intuitive control capable of fully exploiting the rendering capabilities of the complete lighting infrastructure. In this paper we present a new method to automatically create lighting atmospheres in any type of environment, that allows for an natural interaction with the lighting system and generates unique, suggestive effects. To prove the effectiveness and versatility of the proposed solution, we deploy the system in several application scenarios, and discuss the results.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1285–1288},
numpages = {4},
keywords = {lighting design, sparse image approximation, solid state lighting, light atmosphere creation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071996,
author = {Schweiger, Florian and Schroth, Georg and Eichhorn, Michael and Steinbach, Eckehard and Fahrmair, Michael},
title = {Consensus-Based Cross-Correlation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071996},
doi = {10.1145/2072298.2071996},
abstract = {Cross-correlation is a classical similarity measure with broad applications in multimedia signal processing. While it is robust against uncorrelated noise in the input signals, it is severely affected by systematic disturbances which lead to biased results. To overcome this limitation, we propose in this paper consensus-based cross-correlation (ConCor) to deal with heavily corrupted signal parts that derail regular cross-correlation. ConCor builds upon the widely adopted RANSAC algorithm to reliably identify and eliminate corrupt signal parts at limited additional complexity. Our approach is universal in that it can be combined with existing cross-correlation variants. We apply ConCor in two example applications, namely video synchronization and template matching. Our experimental results demonstrate the improved robustness and accuracy when compared to classical cross-correlation.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1289–1292},
numpages = {4},
keywords = {synchronization, robust correlation, template matching},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071997,
author = {You, Junyong and Ebrahimi, Touradj and Perkis, Andrew},
title = {Modeling Motion Visual Perception for Video Quality Assessment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071997},
doi = {10.1145/2072298.2071997},
abstract = {Contrast sensitivity of Human Visual System (HVS) plays an important role in perceiving visual stimuli, and consequently, it has a significant impact on the perceived video quality. This paper proposes a visual perception model based on foveated vision and motion perception. The reference and the distorted video sequences are processed by the visual perception model to generate the perceived stimuli in HVS. The perceived difference of the processed sequences is measured in spatial and temporal domains considering the visual sensitivity. An advanced pooling scheme is proposed based on the visual attention mechanism, eye movement type, and influence of temporal quality variation, in order to estimate the perceived video quality. Experimental results demonstrate that the proposed metric significantly outperforms state-of-the-art quality models with respect to a combined eye-tracking and subjective video quality assessment data set.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1293–1296},
numpages = {4},
keywords = {foveated vision, contrast sensitivity, visual attention, quality of experience, motion perception},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071998,
author = {Yu, Zhiding and Xu, Chunjing and Liu, Jianzhuang and Au, Oscar C. and Tang, Xiaoou},
title = {Automatic Object Segmentation from Large Scale 3D Urban Point Clouds through Manifold Embedded Mode Seeking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071998},
doi = {10.1145/2072298.2071998},
abstract = {This paper presents a system that can automatically segment objects in large scale 3D point clouds obtained from urban ranging images. The system consists of three steps: The first one involves a ground detection process that can detect relatively complex terrain and separate it from other objects. The second step superpixelizes the remaining objects to speed up the segmentation process. In the final step, a manifold embedded mode seeking method is adopted to segment the point clouds. Even though the segmentation of urban objects is a challenging problem in terms of accuracy and problem scale, our system can efficiently generate very good segmentation results. The proposed manifold learning effectively improves the segmentation performance due to the fact that continuous artificial objects often have manifold-like structures.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1297–1300},
numpages = {4},
keywords = {manifold, mode seeking, 3D point cloud, clustering},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2071999,
author = {Tian, Zhiqiang and Xue, Jianru and Lan, Xuguang and Li, Ce and Zheng, Nanning},
title = {Key Object-Based Static Video Summarization},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2071999},
doi = {10.1145/2072298.2071999},
abstract = {In this paper, we present a system for object-based video summarization facilitated by an efficient video object segmentation system. We eliminate the redundancy not only from spatial and temporal domain, but also from content domain. First, we detect shot boundaries and extract video objects by a 3D graph-based algorithm. Once the objects are obtained, the shape of the objects need to be represented. The key objects are extracted in a global manner by K-means clustering of shapes. Experimental results on the proposed object-based scheme combined with efficient video object segmentation show desirable summarization.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1301–1304},
numpages = {4},
keywords = {video object segmentation, video object summarization, graph cuts, key actions},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072000,
author = {Tang, Nick C. and Hsu, Chiou-Ting and Lin, Tsung-Yi and Liao, Hong-Yuan Mark},
title = {Example-Based Human Motion Extrapolation Based on Manifold Learning},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072000},
doi = {10.1145/2072298.2072000},
abstract = {In this paper, we propose a new framework to synthesize human motions based on only one single posture given in the input image. To generate visually pleasing motion sequences, the proposed framework consists of two key techniques. One is motion retrieval, which retrieves reference motions from a human motion database on a low-dimensional motion manifold. Another one is human motion extrapolation, which first generates new postures by deforming the shape of the input posture according to the retrieved motions and then synthesizes the corresponding motion sequence. To demonstrate the efficacy of the proposed method, we generate several human motion sequences using input images with different postures and show that the results are indeed visually pleasing.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1305–1308},
numpages = {4},
keywords = {human motion synthesis, shape matching, manifold learning, mesh deformation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072001,
author = {Lu, Jiangbo and Nguyen, Viet Anh and Niu, Zeping and Singh, Bhavdeep and Luo, Zhiping and Do, Minh N.},
title = {CuteChat: A Lightweight Tele-Immersive Video Chat System},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072001},
doi = {10.1145/2072298.2072001},
abstract = {This paper presents a lightweight tele-immersive video chat system named CuteChat. Based on our recently developed video object cutout technology, the CuteChat system is designed and optimized to provide a radically new video chat experience by merging each participant in the same shared space, allowing them to interact more naturally in an integrated manner. With the goal to make the system easily accessible by massive consumers, we address the challenges in the whole pipeline of video processing, coding, communication, composition, and playback. Extensive experiments have shown that the proposed CuteChat system runs reliably and comfortably in real time on one's laptop or desktop PC, and it needs only a commodity webcam for video acquisition and just public Internet for tele-immersive video conferencing. With such a really minimal deployment requirement, we present a variety of interesting applications and user experiences created by the CuteChat system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1309–1312},
numpages = {4},
keywords = {video chat, tele-immersive system, video object cutout},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072002,
author = {Ma, Haiyang and Gangadharan, Deepak and Venkatasubramanian, Nalini and Zimmermann, Roger},
title = {Energy-Aware Complexity Adaptation for Mobile Video Calls},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072002},
doi = {10.1145/2072298.2072002},
abstract = {High energy consumption has become a challenge for multimedia applications on mobile platforms. We propose a cross layer framework that integrates complexity adaptation and energy conservation for mobile video calls. First we select the most utility-aware encoding and decoding parameters for videos of different motion levels through extensive offline profiling and analysis. Then we design a feedback algorithm to adaptively apply different coding parameters while monitoring the system performance online during a video call. To minimize energy consumption, we utilize Dynamic Voltage and Frequency Scaling (DVFS) for the CPU and buffered transmissions for the network. Our experimental results show an effective saving on energy consumption, with on average 52% savings on the CPU and 30% on the wireless network, while still maintaining high quality service.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1313–1316},
numpages = {4},
keywords = {energy-use optimization, adaptation, mobile video calls},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072003,
author = {Li, Yanjie and Sun, Lifeng and Xue, Tianfan},
title = {Fast Frame-Rate up-Conversion of Depth Video via Video Coding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072003},
doi = {10.1145/2072298.2072003},
abstract = {Recent development of depth sensors has facilitated the progress of 2D-plus-depth methods for 3D video representation, for which frame-rate up-conversion (FRUC) of depth video is a critical step. However, due to the computational cost of state-of-the-art FRUC methods, real time applications of 2D-plus-depth is still limited. In this paper, we present a method of speeding up the FRUC of the depth video by treating it as part of a video coding process, combined with a novel color-mapping algorithm is adopted to improve the quality of temporal upsampling. Experiments show that the proposed systems saves up to 99.5% of the frame interpolation time, while achieving virtually identical reconstructed depth video as state-of-the-art methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1317–1320},
numpages = {4},
keywords = {motion-compensated frame interpolation (MCFI), depth video, video coding, frame-rate up-conversion (FRUC)},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072004,
author = {Mu, Mu and Ishmael, Johnathan and Mitchell, Keith and Race, Nicholas and Mauthe, Andreas},
title = {Multimodal QoE Evaluation in P2P-Based IPTV Systems},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072004},
doi = {10.1145/2072298.2072004},
abstract = {Peer-to-Peer (P2P) technologies provide efficient and low-cost delivery for commercial and user-generated content. Although a number of audio-visual content distribution services have been developed using P2P-based mechanisms, it is still a challenge to guarantee satisfactory user experience on high quality video streaming services due to the dynamic nature of P2P distribution. An objective assessment service is the key enabler of service quality monitoring and management. However, this complex task can not be achieved by any individual objective model that only captures a certain aspect of service quality. This paper introduces a multimodal quality evaluation framework that is specifically designed and implemented for the assessment of video streaming services in P2P-based IPTV systems. Results of initial experiments show the effectiveness of this framework.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1321–1324},
numpages = {4},
keywords = {quality of experience, P2P-based IPTV},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072005,
author = {Mann, Steve and Huang, Jason and Janzen, Ryan and Lo, Raymond and Rampersad, Valmiki and Chen, Alexander and Doha, Taqveer},
title = {Blind Navigation with a Wearable Range Camera and Vibrotactile Helmet},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072005},
doi = {10.1145/2072298.2072005},
abstract = {We present a way finding system that uses a range camera and an array of vibrotactile elements which we built into a helmet.The range camera is a Kinect 3D sensor from Microsoft that is meant to be kept stationary, and used to watch the user (i.e., to detect the person's gestures). Rather than using the camera to look at the user, we reverse the situation, by putting the Kinect range camera on a helmet for being worn by the user. In our case, the Kinect is in motion rather than stationary.Whereas stationary cameras have previously been used for gesture recognition, which the Kinect does very well, in our new modality, we take advantage of the Kinect's resilience against rapidly changing background scenery, where the background in our case is now in motion (i.e., a conventional wearable camera would be presented with a constantly changing background that is difficult to manage by mere background subtraction).The goal of our project is collision avoidance for blind or visually impaired individuals, and for workers in harsh environments such as industrial environments with significant 3-dimensional obstacles, as well as for use in low-light environments.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1325–1328},
numpages = {4},
keywords = {depth sensor, personal safety devices, collision avoidance, microsoft kinect, vibrotactile helmet, blind navigation, vibrotactile array},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072006,
author = {Mukherjee, Snehasis and Biswas, Sujoy Kumar and Mukherjee, Dipti Prasad},
title = {Recognizing Interaction between Human Performers Using 'Key Pose Doublet'},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072006},
doi = {10.1145/2072298.2072006},
abstract = {In this paper, we propose a graph theoretic approach for recognizing interactions between two human performers present in a video clip. We watch primarily the human poses of each performer and derive descriptors that capture the motion patterns of the poses. From an initial dictionary of poses (visual words), we extract key poses (or key words) by ranking the poses on the centrality measure of graph connectivity. We argue that the key poses are graph nodes which share a close semantic relationship (in terms of some suitable edge weight function) with all other pose nodes and hence are said to be the central part of the graph. We apply the same centrality measure on all possible combinations of the key poses of the two performers to select the set of 'key pose doublets' that best represent the corresponding action. The results on standard interaction recognition dataset show the robustness of our approach when compared to the present state of the art method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1329–1332},
numpages = {4},
keywords = {graph centrality, human poses, bag of words, human interaction, key pose doublet},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072007,
author = {Su, Huayou and Zhang, Chunyuan and Chai, Jun and Wen, Mei and Wu, Nan and Ren, Ju},
title = {High-Efficient Software Parallel CAVLC Encoder Based on Programmable Stream Processor},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072007},
doi = {10.1145/2072298.2072007},
abstract = {This article presents an efficient software parallel CAVLC encoder based on programmable stream processors (Storm- SP16 and GPU). For static processor Storm SP16, a block-based 16 ways parallel CAVLC is presented with streaming processing. A component-oriented CAVLC encoder is proposed aiming at dynamic stream processor GPU. Experiments results show that, compared to the CPU version, more than 70 times of speedup can be obtained for the CAVLC based on Storm and over 50 times for GPU-based component-oriented CAVLC encoder. The throughput of the presented CAVLC encoder is more than 10 times higher over that of published software CAVLC encoders on DSP and multi-core platforms.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1333–1336},
numpages = {4},
keywords = {HD, software parallel, real-time, stream processor, CAVLC},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072008,
author = {Wang, Pengjie and Lau, Rynson W.H and Zhang, Mingmin and Wang, Jiang and Song, Haiyu and Pan, Zhigeng},
title = {A Real-Time Database Architecture for Motion Capture Data},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072008},
doi = {10.1145/2072298.2072008},
abstract = {Due to the popularity of motion capture data in many applications, such as games, movies and virtual environments, huge collections of motion capture data are now available. It is becoming important to store these data in compressed form while being able to retrieve them without much overhead. However, there is little work that addresses both issues together. In this paper, we address these two issues by proposing a novel database architecture. First, we propose a lossless compression algorithm to compress the motion clips, which is based on a novel Alpha Parallelogram Predictor (APP) to estimate the degree of freedom (DOF) of each child joint from its immediate neighbors and parents that have already been processed. Second, we propose to store selected eigenvalues and eigenvectors of each motion clip, which only require a very small amount of memory overheads, for faster filtering of irrelevant motions. Based on this architecture, real-time queries become a three-step process. In the first two steps, we perform a quick filtering to identify relevant motion clips in the database through a two-level indexing structure. In the third step, only a small number of candidate clips are uncompressed and accurately matched with a Dynamic Time Warping algorithm. Our results show that users can efficiently search clips from this losslessly compressed motion database.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1337–1340},
numpages = {4},
keywords = {motion compression, motion indexing, motion capture data},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072009,
author = {Geagan, Jay and Ponceleon, Dulce},
title = {Once upon a Time, i Bought a Movie and It Played Everywhere in My Home},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072009},
doi = {10.1145/2072298.2072009},
abstract = {As digital media replaces analog, consumers are beginning to discover that their devices no longer interoperate in the way they are accustomed to. For example, they may find it impossible to transfer a DVR-recorded show onto a portable media player from a different manufacturer. Content creators are weary of supporting myriad ecosystems, each with different rules, restrictions, DRM systems, and file formats. To address these issues, IBM has developed the Advanced Secure Content Cluster Technology (ASCCT), which provides the highest levels of DRM protections for content owners, while at the same time being completely invisible to the user, allowing them to freely move, play, and backup content from any and all ASCCT devices within their home network. We describe the plug-and-play automatic construction of an interoperable secure cluster of heterogeneous devices using a minimal message protocol.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1341–1344},
numpages = {4},
keywords = {ascct, security classes, broadcast encryption, digital media, interoperability, nnl, content protection, audio/video, consumer electronics, home networking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072011,
author = {Wang, Hua and Nie, Feiping and Huang, Heng and Yang, Yi},
title = {Learning Frame Relevance for Video Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072011},
doi = {10.1145/2072298.2072011},
abstract = {Traditional video classification methods typically require a large number of labeled training video frames to achieve satisfactory performance. However, in the real world, we usually only have sufficient labeled video clips (such as tagged online videos) but lack labeled video frames. In this paper, we formalize the video classification problem as a Multi-Instance Learning (MIL) problem, an emerging topic in machine learning in recent years, which only needs bag (video clip) labels. To solve the problem, we propose a novel Parameterized Class-to-Bag (P-C2B) Distance method to learn the relative importance of a training instance with respect to its labeled classes, such that the instance level labeling ambiguity in MIL is tackled and the frame relevances of training video data with respect to the semantic concepts of interest are given. Promising experimental results have demonstrated the effectiveness of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1345–1348},
numpages = {4},
keywords = {video classification, multi-instance learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072012,
author = {Zhou, Wengang and Li, Houqiang and Lu, Yijuan and Tian, Qi},
title = {Large Scale Image Search with Geometric Coding},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072012},
doi = {10.1145/2072298.2072012},
abstract = {Bag-of-Visual-Words model is popular in large-scale image search. However, traditional Bag-of-Visual-Words model does not capture the geometric context among local features in images. To fully explore geometric context of all visual words in images, efficient global geometric verification methods are demanded. In this paper, we propose a novel geometric coding algorithm to encode the spatial context among local features of an image for large scale partial duplicate image retrieval. Our approach is not only computationally efficient, but also can effectively detect duplicate images with rotation, scale changes, occlusion, and background clutter with low computational cost. Experiments show the promising results of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1349–1352},
numpages = {4},
keywords = {partial-duplicate, geometric fan coding., large scale, rotation-invariant, geometric square coding, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072013,
author = {Wang, Xianwang and Zhang, Tong},
title = {Clothes Search in Consumer Photos via Color Matching and Attribute Learning},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072013},
doi = {10.1145/2072298.2072013},
abstract = {Automatic clothes search in consumer photos is not a trivial problem as photos are usually taken under completely uncontrolled realistic imaging conditions. In this paper, a novel framework is presented to tackle this issue by leveraging low-level features (e.g., color) and high-level features (attributes) of clothes. First, a content-based image retrieval(CBIR) approach based on the bag-of-visual-words (BOW) model is developed as our baseline system, in which a codebook is constructed from extracted dominant color patches. A reranking approach is then proposed to improve search quality by exploiting clothes attributes, including the type of clothes, sleeves, patterns, etc. The experiments on photo collections show that our approach is robust to large variations of images taken in unconstrained environment, and the reranking algorithm based on attribute learning significantly improves retrieval performance in combination with the proposed baseline.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1353–1356},
numpages = {4},
keywords = {attribute learning, reranking, clothes search, color matching},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072014,
author = {Inoue, Nakamasa and Shinoda, Koichi},
title = {A Fast MAP Adaptation Technique for Gmm-Supervector-Based Video Semantic Indexing Systems},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072014},
doi = {10.1145/2072298.2072014},
abstract = {We propose a fast maximum a posteriori (MAP) adaptation technique for a GMM-supervectors-based video semantic indexing system.The use of GMM supervectors is one of the state-of-the-art methods in which MAP adaptation is needed for estimating the distribution of local features extracted from video data. The proposed method cuts the calculation time of the MAP adaptation step. With the proposed method, a tree-structured GMM is constructed to quickly calculate posterior probabilities for each mixture component of a GMM. The basic idea of the tree-structured GMM is to cluster Gaussian components and approximate them with a single Gaussian. Leaf nodes of the tree correspond to the mixture components, and each non-leaf node has a single Gaussian that approximates its descendant Gaussian distributions. Experimental evaluation on the TRECVID 2010 dataset demonstrates the effectiveness of the proposed method. The calculation time of the MAP adaptation step is reduced by 76.2% compared to that of a conventional method and resulting accuracy (in terms of Mean average precision) was 10.2%.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1357–1360},
numpages = {4},
keywords = {video semantic indexing, map adaptation, gmm supervectors},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072015,
author = {Tsai, Shen-Fu and Cao, Liangliang and Tang, Feng and Huang, Thomas S.},
title = {Compositional Object Pattern: A New Model for Album Event Recognition},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072015},
doi = {10.1145/2072298.2072015},
abstract = {In this paper, we study the problem of recognizing events in personal photo albums. In consumer photo collections or online photo communities, photos are usually organized in albums according to their events. However, interpreting photo albums is more complicated than the traditional problem of understanding single photos, because albums generally exhibit much more varieties than single image. To solve this challenge, we propose a novel representation, called Compositional Object Pattern, which characterizes object level pattern conveying much richer semantic than low level visual feature. To interpret the rich semantics in albums, we mine frequent object patterns in the training set, and then rank them by their discriminating power. The album feature is then set as the frequencies of these frequent and discriminative patterns, called Compositional Object Pattern Frequency(COPF). We show with experimental result that our algorithm is capable of recognizing holidays with accuracy higher than the baseline method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1361–1364},
numpages = {4},
keywords = {compositional object pattern, event recognition, photo albums},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072016,
author = {Yang, Linjun and Hanjalic, Alan},
title = {Learning from Search Engine and Human Supervision for Web Image Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072016},
doi = {10.1145/2072298.2072016},
abstract = {Visual reranking aims at improving the precision of text-based Web image search. In this paper we propose to combine two learning strategies for deriving the reranking model: learning from search engine and learning from human supervision. The first strategy learns the reranking model in a pseudo-supervised fashion by interpreting parts of the initial text-based search result as pseudo-relevant. The second strategy involves manual relevance labeling of the text-based search results obtained for a limited number of representative queries. While learning from search engine is query dependent and can therefore adapt better to individual queries, it is essentially unsupervised and noisy. While human supervision can better relate the search results to true relevance criteria, it needs to be deployed in a way to keep the reranking scalable. A combination of the two is expected to benefit from their respective advantages and reduce the impact of their individual deficiencies. We propose a two-stage learning approach to visual reranking, where in the online stage multiple query-relative meta rerankers are learned in a pseudo-supervised fashion from the search results and in the offline stage human supervision is used to derive the final reranking function based on these meta rerankers. The experimental results demonstrate that the proposed method significantly outperforms the existing reranking approaches.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1365–1368},
numpages = {4},
keywords = {image search reranking, learning to rerank, visual reranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072017,
author = {Chen, Bor-Chun and Kuo, Yin-Hsi and Chen, Yan-Ying and Chu, Kuan-Yu and Hsu, Winston},
title = {Semi-Supervised Face Image Retrieval Using Sparse Coding with Identity Constraint},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072017},
doi = {10.1145/2072298.2072017},
abstract = {We aim to develop a scalable face image retrieval system which can integrate with partial identity information to improve the retrieval result. To achieve this goal, we first apply sparse coding on local features extracted from face images combining with inverted indexing to construct an efficient and scalable face retrieval system. We then propose a novel coding scheme that refines the representation of the original sparse coding by using identity information. Using the proposed coding scheme, face images with large intra-class variances will still be quantized into similar visual words if they share the same identity. Experimental results show that our system can achieve salient retrieval results on LFW dataset (13K faces) and outperform linear search methods using well known face recognition feature descriptors.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1369–1372},
numpages = {4},
keywords = {retrieval, sparse coding, identity, face, semi-supervised},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072018,
author = {Lu, Hong and Wei, Renzhong and Shen, Yanran and Xue, Xiangyang},
title = {Level Influence of Spatial Pyramid Matching in Object Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072018},
doi = {10.1145/2072298.2072018},
abstract = {In this paper we propose to effectively consider the shape and size variations for object classification. Specifically, a novel image matching method is proposed to incorporate the image segmentation with Spatial Pyramid Matching (SPM), and test our method on flower classification. A Level Influence Factor (LIF) is introduced to represent weights of different pyramid levels based on the statistical information of each segmented image. Then the images are classified based on the LIF weighted spatial pyramid bag-of-visual-words feature, and some levels with weight values zeros are not needed to be compared further. Also, in SPM matching stage, the block in one image is compared with not only its corresponding block in another image, but also the spatially neighboring blocks of the corresponding blocks to find the best match. This fuzzy matching method can incorporate some translation of objects. Experiments are performed on a flower dataset containing 1360 images from 17 different categories. And experimental results demonstrate that our proposed method has better time efficiency than traditional SPM and outperforms the state-of-art flower classification methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1373–1376},
numpages = {4},
keywords = {spatial pyramid, adaptive matching, flower classification, level influence},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072019,
author = {Xu, Xin-Shun and Jiang, Yuan and Peng, Liang and Xue, Xiangyang and Zhou, Zhi-Hua},
title = {Ensemble Approach Based on Conditional Random Field for Multi-Label Image and Video Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072019},
doi = {10.1145/2072298.2072019},
abstract = {Multi-label image/video annotation is a challenging task that allows to correlate more than one high-level semantic keyword with an image/video-clip. Previously, a single model is usually used for the annotation task, with relatively large variance in performance. The correlation among the annotation keywords should also be considered. In this paper, to reduce the performance variance and exploit the correlation between keywords, we propose the En-CRF (Ensemble based on Conditional Random Field) method. In this method, multiple models are first trained for each keyword, then the predictions of these models and the correlations between keywords are incorporated into a conditional random field. Experimental results on benchmark data set, including Corel5k and TRECVID 2005, show that the En-CRF method is superior or highly competitive to several state-of-the-art methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1377–1380},
numpages = {4},
keywords = {ensemble methods, image annotation, conditional random field, video annotation, multi-label learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072020,
author = {Zheng, Yingbin and Wei, Renzhong and Lu, Hong and Xue, Xiangyang},
title = {Refining Local Descriptors by Embedding Semantic Information for Visual Categorization},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072020},
doi = {10.1145/2072298.2072020},
abstract = {Local descriptor extraction and vector quantization are the important components of widely-used Bag-of-Features (BoF) model for visual categorization. This paper proposes a simple and efficient approach to refine the local descriptors for vector quantization by embedding semantic information. The original local descriptors are integrated by a sequence of category-independent and category-dependent basis. Particularly, the category-dependent basis is learned by minimizing the joint loss minimization over local descriptors from different categories with a shared regularization penalty, which can be formulated as a linear programming problem. The transferred descriptors are further quantized and aggregated to the visual vocabulary. Experiments are performed on PASCAL VOC 2007 benchmark and the quantitative comparisons with several state-of-the-art approaches demonstrate the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1381–1384},
numpages = {4},
keywords = {semantic information, local descriptor, visual categorization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072021,
author = {Xie, Hongtao and Gao, Ke and Zhang, Yongdong and Li, Jintao and Ren, Huamin},
title = {Common Visual Pattern Discovery via Graph Matching},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072021},
doi = {10.1145/2072298.2072021},
abstract = {Discovering common visual patterns (CVPs) between two images is a challenging problem, due to the significant photometric and geometric transformations, and the high computational cost. In this paper, we formulate CVPs discovery as a graph matching problem, depending on pairwise geometric compatibility between feature correspondences. To efficiently find all CVPs, we propose two algorithms--Preliminary Initialization Optimization (PIO) and Post Agglomerative Combining (PAC). PIO reduces the search space of CVPs discovery based on the internal homogeneity of CVPs, while PAC refines the discovery result in an agglomerative way. Experiments on object recognition and near-duplicate image re-trieval validate the effectiveness and efficiency of our method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1385–1388},
numpages = {4},
keywords = {graph matching, common visual pattern},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072022,
author = {Su, Feng and Yang, Li and Lu, Tong and Wang, Gongyou},
title = {Environmental Sound Classification for Scene Recognition Using Local Discriminant Bases and HMM},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072022},
doi = {10.1145/2072298.2072022},
abstract = {Analysis and classification of auditory scenes or contexts play important roles in content-based indexing and retrieval of multimedia databases and context-aware applications. In this paper, we propose an environmental sound and auditory scene recognition scheme that focuses on efficient feature representation and classfication of the unstructured composition of a scene (for example, restaurant, street, beach, etc.). We propose to use the local discriminant bases (LDB) technique to identify the discriminatory time-frequency subspace for environmental sounds and then use it for corresponding feature extraction. Based on LDB, we present two recognition models, with or without explicit sound event modeling, for auditory scenes, in which the hidden Markov model (HMM) is used to depict the characteristics and correlations among various events that constitute the scene. The experimental results demonstrate the effectiveness of the proposed approach for auditory scene classification.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1389–1392},
numpages = {4},
keywords = {HMM, local discriminant bases, auditory scene recognition, environmental sound},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072023,
author = {Liu, Yang and Liu, Yan and Zhong, Shenghua and Chan, Keith C.C.},
title = {Semi-Supervised Manifold Ordinal Regression for Image Ranking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072023},
doi = {10.1145/2072298.2072023},
abstract = {In this paper, we present a novel algorithm called manifold ordinal regression (MOR) for image ranking. By modeling the manifold information in the objective function, MOR is capable of uncovering the intrinsically nonlinear structure held by the image data sets. By optimizing the ranking information of the training data sets, the proposed algorithm provides faithful rating to the new coming images. To offer more general solution for the real-word tasks, we further provide the semi-supervised manifold ordinal regression (SS-MOR). Experiments on various data sets validate the effectiveness of the proposed algorithms.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1393–1396},
numpages = {4},
keywords = {manifold ordinal regression, ordinal regression, semi-supervised learning, semi-supervised manifold ordinal regression, manifold learning, image ranking},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072024,
author = {Su, Bolan and Lu, Shijian and Tan, Chew Lim},
title = {Blurred Image Region Detection and Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072024},
doi = {10.1145/2072298.2072024},
abstract = {Many digital images contain blurred regions which are caused by motion or defocus. Automatic detection and classification of blurred image regions are very important for different multimedia analyzing tasks. This paper presents a simple and effective automatic image blurred region detection and classification technique. In the proposed technique, blurred image regions are first detected by examining singular value information for each image pixels. The blur types (i.e. motion blur or defocus blur) are then determined based on certain alpha channel constraint that requires neither image deblurring nor blur kernel estimation. Extensive experiments have been conducted over a dataset that consists of 200 blurred image regions and 200 image regions with no blur that are extracted from 100 digital images. Experimental results show that the proposed technique detects and classifies the two types of image blurs accurately. The proposed technique can be used in many different multimedia analysis applications such as image segmentation, depth estimation and information retrieval.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1397–1400},
numpages = {4},
keywords = {alpha channel map, blurred region detection and classification, image blur, singular value decomposition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072025,
author = {Cheng, Zhongwei and Qin, Lei and Huang, Qingming and Jiang, Shuqiang and Yan, Shuicheng and Tian, Qi},
title = {Human Group Activity Analysis with Fusion of Motion and Appearance Information},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072025},
doi = {10.1145/2072298.2072025},
abstract = {Human activity analysis is an important and challenging task in video content analysis and understanding. In this paper, we focus on the activity of small human group, which involves countable persons and complex interactions. To cope with the variant number of participants and inherent interactions within the activity, we propose a hierarchical model with three layers to depict the characteristics at different granularities. In traditional methods, group activity is represented mainly based on motion information, such as human trajectories, but ignoring discriminative appearance information, e.g. the rough sketch of a pose style. In our approach, we take advantage of both the motion and the appearance information in the spatiotemporal activity context under the hierarchical model. These features are inhomogeneous. Therefore, we employ multiple kernel learning methods to fuse the features for group activity recognition. Experiments on a surveillance-like human group activity database demonstrate the validity of our approach and the recognition performance is promising.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1401–1404},
numpages = {4},
keywords = {human group activity, feature fusion, activity analysis},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072026,
author = {Liu, Lingqiao and Wang, Lei},
title = {Exploring Latent Class Information for Image Retrieval Using the Bag-of-Feature Model},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072026},
doi = {10.1145/2072298.2072026},
abstract = {Recently, the Bag-of-Feature (BoF) model has shown promising performance in object and generic image retrieval. The similarity between two images is typically measured by the distance between the two histograms. Due to the imperfection of local descriptor and quantization error, visually similar image patches can be wrongly quantized into different visual words, making this distance-based measure less accurate. To address this issue, this paper explores the information of latent class, which is formed by all the database images that share the same visual concept with the one being compared to a given query. We then cast image similarity as the probability of the query and a database image belonging to a same latent class. Considering that a class of images together can better depict a visual concept, the shift from image-to-image to image-to-class comparison is expected to bring a more robust similarity measure. Because the ground truth of the latent class is not accessible in image retrieval, we define a latent class prior in our probabilistic model and derive its marginal distribution. This gives rise to a novel and efficient image similarity measure. It can significantly improve retrieval performance without prolonging retrieval process. Experimental study on multiple benchmark data sets demonstrates its advantages.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1405–1408},
numpages = {4},
keywords = {similarity, neighborhood, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072027,
author = {Lu, Zhiwu and Peng, Yuxin},
title = {Combining Latent Semantic Learning and Reduced Hypergraph Learning for Semi-Supervised Image Categorization},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072027},
doi = {10.1145/2072298.2072027},
abstract = {This paper presents a novel framework that can combine latent semantic learning and reduced hypergraph learning for semi-supervised image categorization. To improve the traditional bag-of-features representation, we first propose a semantics-aware representation which can learn latent semantics automatically from a large vocabulary of abundant visual keywords through contextual spectral embedding. The learnt latent semantics can be readily used to define a histogram intersection kernel. Based on this semantics-aware kernel, we further develop a reduced hypergraph-based semi-supervised learning method to exploit both labeled and unlabeled images for image categorization. Experimental results have shown that the proposed framework can achieve significant improvements with respect to the state of the arts.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1409–1412},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072028,
author = {Chakraborty, Shayok and Balasubramanian, Vineeth and Panchanathan, Sethuraman},
title = {Optimal Batch Selection for Active Learning in Multi-Label Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072028},
doi = {10.1145/2072298.2072028},
abstract = {Multi-label classification is a generalization of conventional classification, where it is possible for a single data point to have multiple labels. Manual annotation of a multi-label data point requires a human oracle to consider the presence/absence of every possible class separately, which involves significant labor. Active learning techniques are effective in reducing human labeling effort to induce a classification model. When exposed to large quantities of unlabeled data, such algorithms automatically select the salient and representative instances for manual annotation. Further, to address the high redundancy in data such as image or video sequences as well as the availability of multiple labeling agents, there have been recent attempts towards a batch mode form of active learning, where a batch of data points is selected simultaneously from an unlabeled set. In this work, we propose a novel optimization based batch mode active learning strategy to minimize human labeling effort in multi-label classification problems. To the best of our knowledge, this is the first attempt to develop such a scheme primarily intended for the multi-label context. The proposed framework is computationally simple, easy to implement and can be suitably modified to perform batch mode active learning in other formulations, such as single-label classification or problems involving hierarchical label spaces. Our results corroborate the efficacy of the proposed algorithm and certify the potential of the framework in being used for real world applications.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1413–1416},
numpages = {4},
keywords = {multi-label classification, batch mode active learning, optimization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072029,
author = {Nakashima, Yuta and Babaguchi, Noboru},
title = {Extracting Intentionally Captured Regions Using Point Trajectories},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072029},
doi = {10.1145/2072298.2072029},
abstract = {When camera persons take videos with mobile video cameras, they usually have capture intentions, i.e., what they want to express in their videos, and there are intentionally captured regions (ICRs) in the video frames that are essential for the capture intentions. Extracting ICRs is thus beneficial for wide range of applications such as video summarization and video adaptation for small displays. In this paper, we present a novel method for automatically extracting ICRs. A camera person usually moves his/her camera so that ICRs can be arranged in appropriate positions in video frames; therefore, ICRs can yield specific motion. This observation indicates that such specific motion is a vital cue for extracting ICRs. The proposed method represents motion by point trajectories, which are long-term trajectories of spatially dense points in video frames, and extracts ICRs using an ICR model based on the point trajectories. We experimentally evaluate the proposed method to demonstrate its potential applicability.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1417–1420},
numpages = {4},
keywords = {point trajectory, intentionally captured region, capture intention},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072030,
author = {Chen, Chih-Fan and Wang, Yu-Chiang Frank},
title = {Exploring Self-Similarities of Bag-of-Features for Image Classification},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072030},
doi = {10.1145/2072298.2072030},
abstract = {The use of bag-of-features (BOF) models has been a popular technique for image classification and retrieval. In order to better represent and discriminate images from different classes, we advance BOF and explore the self-similarities of visual words for improved performance. The proposed self-similarity hypercubes (SSH) model, which observes the concurrent occurrences of visual words in an image, is able to describe the structural information of the BOF in an image. Our experiments confirm that our SSH provides additional and complementary information to BOF and thus results in improved classification performance. Unlike most prior methods requiring extraction or integration of multiple types of features for similar improvements, our SSH works in the same domain as the BOF does. Moreover, we do not limit the use of our SSH to any particular type of image descriptors, and its generalization is also verified.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1421–1424},
numpages = {4},
keywords = {bag-of-features, self-similarity, image classification},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072031,
author = {Li, Pengjie and Ma, Huadong and Ming, Anlong},
title = {Non-Rigid 3D Model Retrieval Using Multi-Scale Local Features},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072031},
doi = {10.1145/2072298.2072031},
abstract = {The number of available non-rigid 3D models in various areas increases steadily. The local features are more effective than global features for the search of these non-rigid 3D models. Global descriptors fail to consistently compensate for the intra-class variability of non-rigid 3D models. To solve this problem, we propose a non-rigid 3D model retrieval method based on multi-scale local features. Firstly, we extract keypoints at multiple scales automaticlly. Then, the Heat Kernel Signature (HKS) local descriptors are computed for each keypoint. However, the HKS descriptors are sensitive to scale. In order to solve this problem, the HKS descriptors are put into the Bag-of-Features (BOF) framework. In the BOF framework, we use a kind of histogram equalization technique to make our feature descriptor robust to model scaling. Experimental results on two public benchmarks show that our algorithm can achieve satisfactory retrieval performance for the non-rigid 3D models.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1425–1428},
numpages = {4},
keywords = {scale invariant, heat kernel signature(HKS), local features, multi-scale, non-rigid 3D model retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072032,
author = {Redi, Miriam and Merialdo, Bernard},
title = {Marginal-Based Visual Alphabets for Local Image Descriptors Aggregation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072032},
doi = {10.1145/2072298.2072032},
abstract = {Bag of Words (BOW) models are nowadays one of the most effective methods for visual categorization. They use visual dictionaries to aggregate the set of local descriptors extracted from a given image. Despite their high discriminative ability, one of the major drawbacks of BOW still remains the computational cost of the visual dictionary, built by clustering in the high dimensional feature space. In this paper we introduce a fast, effective method for local image descriptors aggregation that is based on marginal approximations, i.e. the approximation of each descriptor component distribution. We quantize each dimension of the feature space, obtaining a visual alphabet that we use to map the image descriptors in a fixed-length visual signature. Experimental results show that our new method outperforms the traditional BOW model in both accuracy and efficiency for the scene recognition task. Moreover, we discover that the marginal-based aggregation provides complementary information with respect to BOW, by combining the two models in a video retrieval system based on TRECVID 2010.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1429–1432},
numpages = {4},
keywords = {feature extraction, cbir, scene recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072033,
author = {Beecks, Christian and Ivanescu, Anca Maria and Kirchhoff, Steffen and Seidl, Thomas},
title = {Modeling Multimedia Contents through Probabilistic Feature Signatures},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072033},
doi = {10.1145/2072298.2072033},
abstract = {We introduce a new family of flexible feature representations for content-based multimedia retrieval: probabilistic feature signatures. While conventional feature histograms and feature signatures aggregate the multimedia objects' feature distributions exhibited in some feature space according to a partitioning, probabilistic feature signatures model these feature distributions by means of discrete or continuous probability distributions. In this way, they combine the advantages of high expressiveness and compactness, for instance through Gaussian mixture models. In this paper, we introduce the concept of probabilistic feature signatures and provide the empirical evidence of high retrieval performance when using this feature representation type. We show that probabilistic feature signatures are able to outperform conventional feature signatures.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1433–1436},
numpages = {4},
keywords = {probabilistic feature signature, content-based multimedia retrieval, signature quadratic form distance},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072034,
author = {Wengert, Christian and Douze, Matthijs and J\'{e}gou, Herv\'{e}},
title = {Bag-of-Colors for Improved Image Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072034},
doi = {10.1145/2072298.2072034},
abstract = {This paper investigates the use of color information when used within a state-of-the-art large scale image search system. We introduce a simple yet effective and efficient color signature generation procedure. It is used either to produce global or local descriptors. As a global descriptor, it outperforms several state-of-the-art color description methods, in particular the bag-of-words method based on color SIFT. As a local descriptor, our signature is used jointly with SIFT descriptors (no color) to provide complementary information. This significantly improves the recognition rate, outperforming the state of the art on two image search benchmarks. We provide an open source package of our signature (http://www.kooaba.com/en/learnmore/labs/).},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1437–1440},
numpages = {4},
keywords = {color descriptor, image retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072035,
author = {Jain, Mihir and J\'{e}gou, Herv\'{e} and Gros, Patrick},
title = {Asymmetric Hamming Embedding: Taking the Best of Our Bits for Large Scale Image Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072035},
doi = {10.1145/2072298.2072035},
abstract = {This paper proposes an asymmetric Hamming Embedding scheme for large scale image search based on local descriptors. The comparison of two descriptors relies on an vector-to-binary code comparison, which limits the quantization error associated with the query compared with the original Hamming Embedding method. The approach is used in combination with an inverted file structure that offers high efficiency, comparable to that of a regular bag-of-features retrieval system. The comparison is performed on two popular datasets. Our method consistently improves the search quality over the symmetric version. The trade-off between memory usage and precision is evaluated, showing that the method is especially useful for short binary signatures.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1441–1444},
numpages = {4},
keywords = {image search, hamming embedding},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072036,
author = {Cheng, Xiangang and Chia, Liang-Tien},
title = {Spatially-Coherent Pyramid Matching Based on Max-Pooling},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072036},
doi = {10.1145/2072298.2072036},
abstract = {This paper presents a method of max-pooling spatially-coherent pyramid matching (MpScPM). Higher-layer representations are generated from lower-layer subregions, by a biologically-inspired max pooling strategy. Second, instead of reshaping the pyramid representation into a vector (used in generic SPM), the layer and location information of each subregion are kept and weak geometrical correspondences between matched subregions are explored to enhance our pyramid matching method. To enhance the possibility of finding the best matches at different scales and locations, cross-layer region similarities are computed, while the correspondences (either spatial neighbors or adjacent layers) are also incorporated. We evaluate our proposed MpScPM method on several existing benchmark datasets and it achieves excellent performances.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1445–1448},
numpages = {4},
keywords = {max pooling, pyramid matching, spatial coherent},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072037,
author = {Vreeswijk, Daan T.J. and Huurnink, Bouke and Smeulders, Arnold W.M.},
title = {Text and Image Subject Classifiers: Dense Works Better},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072037},
doi = {10.1145/2072298.2072037},
abstract = {We investigate the feasibility of training visual concept detectors for such abstract subject categories as biology and history with the aim of employing these for full-text to image linking. We show that using dense sampling methods can lead to image classifiers that perform well enough for interactive search. Echoing this dense sampling in the image domain, we also show that using term frequencies as text features outperforms using a topic abstraction method. Finally, we use these monomodal classifiers for the task of linking texts to images, improving more than 50% over the state-of-the-art, thereby showing that dense is better.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1449–1452},
numpages = {4},
keywords = {feature representation, multimedia linking, multimedia subject classification, retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072038,
author = {Borth, Damian and Ulges, Adrian and Breuel, Thomas Michael},
title = {Automatic Concept-to-Query Mapping for Web-Based Concept Detector Training},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072038},
doi = {10.1145/2072298.2072038},
abstract = {Nowadays, online platforms like YouTube provide massive content for training of visual concept detectors. However, it remains a difficult challenge to retrieve the right training content from such platforms since the underlying query construction can be arbitrarily complex. In this paper we present an approach, which offers an automatic concept-to-query mapping for training data acquisition from such platforms. Queries are automatically constructed by a keyword selection and a category assignment using ImageNet and Google Sets as external sources. Our results demonstrate that the proposed method is able to reach retrieval results comparable to queries constructed by humans providing 76% more relevant content for detector training than a one-to-one mapping of concept names to retrieval queries would do.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1453–1456},
numpages = {4},
keywords = {query refinement, web video, concept detection, query expansion, query mapping},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072039,
author = {Zhuang, Yueting and Liu, Yang and Wu, Fei and Zhang, Yin and Shao, Jian},
title = {Hypergraph Spectral Hashing for Similarity Search of Social Image},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072039},
doi = {10.1145/2072298.2072039},
abstract = {The development of social media brings great challenges to image retrieval on both efficiency and accuracy. In addition to achieving fast similarity search over large scale data, it is very crucial to represent the complex and high-order relationships among the social contents to improve the semantic understanding of social images.In this paper, unified hypergraph is implemented to model the various relationships among images and other contexts in social media. Moreover, we extend traditional spectral hashing to hypergraph to accelerate similarity search of social images by mapping semantically related vertices into similar binary codes within a short Hamming distance. Furthermore, the proposed HSH approach is extended to out-of-sample data in a supervised manner. We evaluated our approach on the dataset crawled from Flickr and the experiment results indicate that our proposed HSH approach is both efficient and effective.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1457–1460},
numpages = {4},
keywords = {hypergraph, social media, hashing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072040,
author = {Merler, Michele and Kender, John R.},
title = {Selecting the Best Faces to Index Presentation Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072040},
doi = {10.1145/2072298.2072040},
abstract = {We propose a system to select the most representative faces in unstructured presentation videos with respect to two criteria: to optimize matching accuracy between pairs of face tracks, and to select humanly preferred face icons for indexing purposes. We first extract face tracks using state-of-the-art face detection and tracking. A small subset of images are then selected per track in order to maximize matching accuracy between tracks. Finally, representative images are extracted for each speaker in order to build a face index of the video. We tested our approach on 3 unstructured presentation videos of approximately 45 minutes each, for a total of a quarter million frames. Compared to the standard min-min approach, our method achieves higher track matching accuracy (94.22%), while using 6% of the running time. Using an optimal combination of 3 user preference measures, we were able to build face indexes containing 54 speakers (out of the 58 present in the videos) indexing into 795 detected tracks.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1461–1464},
numpages = {4},
keywords = {presentation video, speaker face indexing},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072041,
author = {He, Sheng and Han, Junwei and Hu, Xintao and Xu, Ming and Guo, Lei and Liu, Tianming},
title = {A Biologically Inspired Computational Model for Image Saliency Detection},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072041},
doi = {10.1145/2072298.2072041},
abstract = {Image saliency detection provides a powerful tool for predicting where human tends to look at in an image, which has been a long attempt for the computer vision community. In this paper, we propose a biologically-inspired model for computing image saliency. At first, a set of basis functions that accords with visual responses to natural stimuli is learned by using eye-fixation patches from an eye-tracking dataset. Three features are then derived based on the learned basis functions including continuity, clutter contrast, and local contrast. Finally, these three features are combined into the saliency map. The proposed approach is easy to implement and can be used in many image and video content analysis applications. Experiments on a large-scale benchmark dataset and comparisons with a number of the state-of-the-art approaches demonstrate its superiority.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1465–1468},
numpages = {4},
keywords = {sparse coding, biologically-inspired, image saliency detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072042,
author = {Sun, Xiaoshuai and Yao, Hongxun and Ji, Rongrong and Liu, Xianming and Xu, Pengfei},
title = {Unsupervised Fast Anomaly Detection in Crowds},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072042},
doi = {10.1145/2072298.2072042},
abstract = {In this paper, we proposed a fast and robust unsupervised framework for anomaly detection and localization in crowed scenes. Our method avoids modeling the normal state of the crowds which is a very complex task due to the large within class variance of the normal target appearance and motion patterns. For each video frame, we extract the spatial temporal features of 3D blocks and generate the saliency map using a block-based center-surround difference operator. Then, motion vector matrix is obtained by adaptive rood pattern search block-matching algorithm and distance normalization. Attractive motion disorder descriptor is proposed to measure the global intensity of anomalies in the scene. Finally, we classify the frames into normal and anomalous ones by a binary classifier. In the experiments, we compared our method against several state-of-the-art approaches on UCSD dataset which is a widely used anomaly detection and localization benchmark. As the only unsupervised approach, our method outputs competitive results with near real-time processing speed},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1469–1472},
numpages = {4},
keywords = {attractive motion disorder descriptor, motion estimation, unsupervised anomaly detection},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072043,
author = {Zhao, Sicheng and Yao, Hongxun and Sun, Xiaoshuai and Xu, Pengfei and Liu, Xianming and Ji, Rongrong},
title = {Video Indexing and Recommendation Based on Affective Analysis of Viewers},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072043},
doi = {10.1145/2072298.2072043},
abstract = {Most previous works on video indexing and recommendation were only based on the content of video itself, without considering the affective analysis of viewers, which is an efficient and important way to reflect viewers' attitudes, feelings and evaluations of videos. In this paper, we propose a novel method to index and recommend videos based on affective analysis, mainly on facial expression recognition of viewers. We first build a facial expression recognition classifier by embedding the process of building compositional Haar-like features into hidden conditional random fields (HCRFs). Then we extract viewers' facial expressions frame by frame through the videos, collected from the camera when viewers are watching videos, to obtain the affections of viewers. Finally, we draw the affective curve which tells the process of affection changes. Through the curve, we segment each video into affective sections, give the indexing result of the videos, and list recommendation points from views' aspect. Experiments on our collected database from the web show that the proposed method has a promising performance.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1473–1476},
numpages = {4},
keywords = {affective analysis, segmentation, video indexing, viewers, recommendation, facial expression recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072044,
author = {Adams, Brett and Phung, Dinh and Venkatesh, Svetha},
title = {Eventscapes: Visualizing Events over Time with Emotive Facets},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072044},
doi = {10.1145/2072298.2072044},
abstract = {The scale and dynamicity of social media, and interaction between traditional news sources and online communities, has created challenges to information retrieval approaches. Users may have no clear information need or be unable to express it in the appropriate idiom, requiring instead to be oriented in an unfamiliar domain, to explore and learn. We present a novel data-driven visualization, termed Eventscape, that combines time, visual media, mood, and controversy. Formative evaluation highlights the value of emotive facets for rapid evaluation of mixed news and social media topics, and a role for such visualizations as pre-cursors to deeper search.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1477–1480},
numpages = {4},
keywords = {event, sentiment, social media, visualization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072045,
author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and De Natale, Francesco G.B.},
title = {Signature-Image-Based Event Analysis for Personal Photo Albums},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072045},
doi = {10.1145/2072298.2072045},
abstract = {Quick reorganizing and draft annotating personal photo albums under event scheme is an emerging trend. In this research, a method has been developed to meet such requirements using the idea of gist and mosaic art so that viewers could understand the meaning of a whole scene without paying much attention in individual details. First, given a photo album, all chronologically ordered images are normalized to a smaller size, and then mosaicked side-by-side to create a signature image representing for that album. Next, by integrating the optimized linear programming with the color descriptor of the signature image, not only the event-type of the album but also all sub-event-types of the sub-sequence photos are decided. More than 19,000 images of five varied event-types have been used to evaluate the proposed method. Experimental results show that the proposed method could detect events towards annotation and re-organization of personal photo albums with high accuracy at a rapid speed.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1481–1484},
numpages = {4},
keywords = {mosaic art, gist, signature image base, optimization linear programming, event analysis, personal photo album},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072046,
author = {Pang, Lin and Cao, Juan and Zhang, Yongdong and Lin, Shouxun},
title = {Leveraging Collective Wisdom for Web Video Retrieval through Heterogeneous Community Discovery},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072046},
doi = {10.1145/2072298.2072046},
abstract = {With the exponential growth of social media, web video retrieval based on contextual information associated with videos has attracted wide attention recently. However, state-of-the-art methods mainly focus on limited kinds of context cues and lack of unified exploration towards multiple heterogeneous contexts. In this paper, we propose a novel web video ranking framework called CommunityRank by leveraging the collective wisdom from a community perspective. Firstly, it formulizes various social relations among users, videos and tags in a heterogeneous context network and further detects its latent community structure. Then the algorithm maps videos into the community space and performs a community-oriented re-ranking through a bipartite graph model. By aggregating the multiple relations, CommunityRank can make the most of textual, visual and social contexts and leads to better search results. The encouraging performances of the proposed method on YouTube video collection demonstrate that the discovered communities reveal topics of interest emerging in collective behaviors and can facilitate web video retrieval.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1485–1488},
numpages = {4},
keywords = {heterogeneous community discovery, video retrieval, social media},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072047,
author = {Hoashi, Keiichiro and Ono, Chihiro and Ishii, Daisuke and Watanabe, Hiroshi},
title = {Automatic Preview Generation of Comic Episodes for Digitized Comic Search},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072047},
doi = {10.1145/2072298.2072047},
abstract = {This research proposes a novel method to present "thumbnails" of episodes of digitized comics, in order to improve the efficiency of comic search. Comic episode thumbnails are generated based on image analysis technologies developed especially for comic images. Namely, the following procedures are developed for our system: automatic comic frame segmentation, text balloon extraction, and a linear regression based model to calculate the importance score of each extracted frame. The system then selects frames from each episode with high importance score, and aligns the selected frames to create the episode thumbnail, which is presented to the system user as a compact preview of the episode. User experiments conducted with actual Japanese comic images prove that the proposed method significantly decreases the time necessary to search for specific episodes from a large scaled comic data collection.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1489–1492},
numpages = {4},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072048,
author = {Yu, Xiangqian and Oria, Vincent and Gouton, Pierre and Jomier, Genevi\`{e}ve},
title = {2D Geon Based Generic Object Recognition},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072048},
doi = {10.1145/2072298.2072048},
abstract = {The Recognition by Components(RBC) is a theory in Psychology introduced by Biederman in the late 80s, by which humans perceive scenes through simple 3D objects with regular shapes such as spheres, cubes, cylinders, cones, or wedges, called Geons (geometric ions). Extracting geons from 2D images is a very challenging task as it requires a good segmentation and the recognition of the 3D geons in a 2D space. In this paper, we propose a novel approach for extracting 2D geons from 2D images. The process is composed of three major parts: image preprocessing which includes image background removal and segmentation, arc-geon detection, and polygon-geon detection. We also propose a general procedure for matching the extracted 2D geons to given models for object recognition. Experiment results show that our approach is competitive compared to existing object recognition methodologies in general.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1493–1496},
numpages = {4},
keywords = {levenshtein distance, segmentation, geon extraction, object recognition},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072049,
author = {Yuan, Ying and Wu, Fei and Zhuang, Yueting and Shao, Jian},
title = {Image Annotation by Composite Kernel Learning with Group Structure},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072049},
doi = {10.1145/2072298.2072049},
abstract = {We can obtain more and more kinds of heterogeneous features (such as color, shape and texture) in images which can be extracted to describe various aspects of visual characteristics. Those high-dimensional heterogeneous visual features are intrinsically embedded in a non-linear space. In order to effectively utilize these heterogeneous features, this paper proposes an approach, called Composite Kernel Learning with Group Structure (CKLGS), to select groups of discriminative features for image annotation. For each image label, the CKLGS method embeds the nonlinear image data with discriminative features into different Reproducing Kernel Hilbert Spaces (RKHS), and then composes these kernels to select groups of discriminative features. Thus a classification model can be trained for image annotation. By the comparisons with other image annotation algorithms, experiments show that the proposed CKLGS for image annotation achieves a better performance.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1497–1500},
numpages = {4},
keywords = {image annotation, feature selection, group structure, composite kernel learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072050,
author = {Zhu, Xiaofeng and Huang, Zi and Shen, Heng Tao},
title = {Video-to-Shot Tag Allocation by Weighted Sparse Group Lasso},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072050},
doi = {10.1145/2072298.2072050},
abstract = {Traditional shot tagging techniques are focused on learning and propagating the tags at the same level, that is from labeled training shots to the unknown test shots. Due to the lack of sufficient labeled video shots, effective shot tagging remains challenging. By observing that video-level tags are more widely provided, we design a novel approach to propagate video-level tags to the test shots. A weighted sparse group lasso method (WSGL) is proposed for shot reconstruction, which well preserves the structural sparsity to reduce the noise in tag propagation. Meanwhile, it simultaneously considers the spatial-temporal information within the video corpus to enhance the tagging performance. Extensive experiments are conducted on two public video datasets to demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1501–1504},
numpages = {4},
keywords = {structure sparsity, video tagging},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072051,
author = {Wang, Zheshen and Kumar, Mrityunjay and Luo, Jiebo and Li, Baoxin},
title = {Extracting Key Frames from Consumer Videos Using Bi-Layer Group Sparsity},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072051},
doi = {10.1145/2072298.2072051},
abstract = {Compared to well-edited videos with predefined structures (e.g., news or sports videos), extracting key frames from unconstrained consumer videos remains a much more challenging problem due to their extremely diverse contents (no pre-imposed structure) and uncontrolled video quality (e.g., due to poor lighting or camera shake). In order to exploit spatio-temporal correlation present in the video for key frame extraction, we propose a bi-layer group sparse representation in which the input video frames are first segmented into homogeneous patches and group sparsity is imposed at two levels simultaneously: (i) patch-to-frame, and (ii) frame-to-sequence. The grouped sparse coefficients are further combined with frame quality scores to generate key frames. Extensive experiments are performed on videos from actual end users. Results obtained by the proposed approach compare favorably with existing methods to confirm its effectiveness.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1505–1508},
numpages = {4},
keywords = {group sparsity, key frame extraction, consumer video},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072052,
author = {Li, Xia and Song, Yan and Lu, Yijuan and Tian, Qi},
title = {Spatial Pooling for Transformation Invariant Image Representation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072052},
doi = {10.1145/2072298.2072052},
abstract = {Spatial Pyramid Matching (SPM) [2] has been proposed to extend the Bag-of-Word (BoW) model for object classification. By re-serving the finer level information, it makes image matching more accurate. However, for not well-aligned images, where the object is rotated, flipped or translated, SPM may lose its discrimination power. To tackle this problem, we propose novel spatial pooling layouts to address various transformations, and generate a more general image representation. To evaluate the effectiveness of the proposed approach, we conduct extensive experiments on three transformation emphasized datasets for object classification task. Experimental results demonstrate its superiority over the state-of-the-arts. Besides, the proposed image representation is compact and consistent with the BoW model, which makes it applicable to image retrieval task as well.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1509–1512},
numpages = {4},
keywords = {spatial transformation, image representation, and object classification.},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072053,
author = {Zhang, Rui and Zhang, Lei and Wang, Xin-Jing and Guan, Ling},
title = {Multi-Feature PLSA for Combining Visual Features in Image Annotation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072053},
doi = {10.1145/2072298.2072053},
abstract = {We study in this paper the problem of combining low-level visual features for image region annotation. The problem is tackled with a novel method that combines texture and color features via a mixture model of their joint distribution. The structure of the presented model can be considered as an extension of the probabilistic latent semantic analysis (pLSA) in that it handles data from two different visual feature domains by attaching one more leaf node to the graphical structure of the original pLSA. Therefore, the proposed approach is referred to as multi-feature pLSA (MF-pLSA). The supervised paradigm is adopted to classify a new image region into one of a few pre-defined object categories using the MF-pLSA. To evaluate the performance, the VOC2009 and LabelMe databases were employed in our experiments, along with various experimental settings in terms of the number of visual words and mixture components. Evaluated based on the average recall and precision, the MF-pLSA is demonstrated superior to seven other approaches, including other schemes for visual feature combination.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1513–1516},
numpages = {4},
keywords = {descriptor combination, image annotation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072054,
author = {Gao, Yue and Wang, Meng and Luan, Huanbo and Shen, Jialie and Yan, Shuicheng and Tao, Dacheng},
title = {Tag-Based Social Image Search with Visual-Text Joint Hypergraph Learning},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072054},
doi = {10.1145/2072298.2072054},
abstract = {Tag-based social image search has attracted great interest and how to order the search results based on relevance level is a research problem. Visual content of images and tags have both been investigated. However, existing methods usually employ tags and visual content separately or sequentially to learn the image relevance. This paper proposes a tag-based image search with visual-text joint hypergraph learning. We simultaneously investigate the bag-of-words and bag-of-visual-words representations of images and accomplish the relevance estimation with a hypergraph learning approach. Each textual or visual word generates a hyperedge in the constructed hypergraph. We conduct experiments with a real-world data set and experimental results demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1517–1520},
numpages = {4},
keywords = {tag-based image search, hypergraph learning, visual-text},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072055,
author = {Zhao, Xiaojian and Li, Guangda and Wang, Meng and Yuan, Jin and Zha, Zheng-Jun and Li, Zhoujun and Chua, Tat-Seng},
title = {Integrating Rich Information for Video Recommendation with Multi-Task Rank Aggregation},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072055},
doi = {10.1145/2072298.2072055},
abstract = {Video recommendation is an important approach for helping people to access interesting videos. In this paper, we propose a scheme to integrate rich information for video recommendation. We regard video recommendation as a ranking problem and generate multiple ranking lists by exploring different information sources. A multi-task rank aggregation approach is proposed to integrate the ranking lists for different users in a joint manner. Our scheme is flexible and can easily incorporate other methods by adding their generated ranking lists into our multi-task learning algorithm. We conduct experiments with 76 users and more than 10,000 videos. The results demonstrate the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1521–1524},
numpages = {4},
keywords = {multi-task rank aggregation, video recommendation},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072056,
author = {Monaghan, David S. and Kelly, Philip and O'Connor, Noel},
title = {Quantifying Human Reconstruction Accuracy for Voxelcarving in a Sporting Environment},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072056},
doi = {10.1145/2072298.2072056},
abstract = {Whilst voxel carving approaches exist that allow non-invasive 3D human reconstruction, their performance is heavily dependent on the number of cameras used and the placement of these cameras around the subject. We present a technique to quantify the fall-off in accuracy of spatially carved volumetric representations of humans based on real world constraints. We describe an example of such a quantitative evaluation using a synthetic dataset of typical sports motion in a tennis court scenario, created using computer graphics techniques and motion capture data. Experiments are performed using a baseline voxel carving technique that includes player tracking, background subtraction and player voxel carving. This type of quantitative evaluation could be used by amateur sporting clubs without a sophisticated capture infrastructure to understand how best to instrument a camera network in order to obtain a good trade-off between reconstruction accuracy and installation cost.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1525–1528},
numpages = {4},
keywords = {space carving, image processing, 3D reconstruction},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072057,
author = {Dovgalecs, Vladislavs and M\'{e}gret, R\'{e}mi and Berthoumieu, Yannick},
title = {Time-Aware Co-Training for Indoors Localization in Visual Lifelogs},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072057},
doi = {10.1145/2072298.2072057},
abstract = {In this paper we address the problem of location recognition from visual lifelogs by leveraging visual features and temporal information in an unified framework. The proposed method features a co-training approach that takes advantage of both labeled and unlabeled data using a confidence measure we propose for this task. It exploits jointly two SVM classifiers on two types of visual features as well as the temporal continuity of the video through temporal accumulation scheme. We demonstrate experimentally on the publicly available IDOL2 dataset that the algorithm yields performance improvement due to its ability to exploit jointly multiple cues, time and unlabeled data.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1529–1532},
numpages = {4},
keywords = {temporal information, co-training, lifelog indexing, semi-supervised learning},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072058,
author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Automatic Sentence Generation from Images},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072058},
doi = {10.1145/2072298.2072058},
abstract = {For the overwhelming amounts of multimedia used on the Web, methods of search and understanding with sentences are necessary. Representing the contents not only using labels but also using sentences including labels' relations enables users to search with a story and to understand multimedia deeply. However, few existing works describe such sentences because obtaining objects' relations and grammar is difficult. We specifically examine captions of images that are similar to an input image. They are expected to explain the input image to some degree. Therefore, we propose a novel approach to generate a sentential caption for the input image by summarizing those captions. Our experiment using a dataset consisting of images and text demonstrates that the proposed method can generate sentential captions.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1533–1536},
numpages = {4},
keywords = {probabilistic canonical correlation analysis, multi-stack decoding, similarity measure},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072059,
author = {Manohar, Vasant and Tsakalidis, Stavros and Natarajan, Pradeep and Prasad, Rohit and Natarajan, Prem},
title = {Audio-Visual Fusion Using Bayesian Model Combination for Web Video Retrieval},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072059},
doi = {10.1145/2072298.2072059},
abstract = {Combining features from multiple, heterogeneous, audio visual sources can significantly improve retrieval performance in consumer domain videos. However, such videos often contain unrelated overlaid audio content, or have significant camera motion to reliably extract visual features. We present an approach, which overcomes errors in individual feature streams by combining classifiers trained on multiple, heterogeneous feature streams using Bayesian model combination (BAYCOM). We demonstrate our method, by combining low-level audio and visual features, for classification of a large 200 hour web video corpus. The combined models outperform any of the individual features by 10%. Further, BAYCOM consistently outperforms traditional early and late fusion methods.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1537–1540},
numpages = {4},
keywords = {audio-visual fusion, bayesian model combination, web video retrieval},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072060,
author = {Ballan, Lamberto and Bertini, Marco and Del Bimbo, Alberto and Serra, Giuseppe},
title = {Enriching and Localizing Semantic Tags in Internet Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072060},
doi = {10.1145/2072298.2072060},
abstract = {Tagging of multimedia content is becoming more and more widespread as web 2.0 sites, like Flickr and Facebook for images, YouTube and Vimeo for videos, have popularized tagging functionalities among their users. These user-generated tags are used to retrieve multimedia content, and to ease browsing and exploration of media collections, e.g.~using tag clouds. However, not all media are equally tagged by users: using the current browsers is easy to tag a single photo, and even tagging a part of a photo, like a face, has become common in sites like Flickr and Facebook; on the other hand tagging a video sequence is more complicated and time consuming, so that users just tag the overall content of a video. In this paper we present a system for automatic video annotation that increases the number of tags originally provided by users, and localizes them temporally, associating tags to shots. This approach exploits collective knowledge embedded in tags and Wikipedia, and visual similarity of keyframes and images uploaded to social sites like YouTube and Flickr.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1541–1544},
numpages = {4},
keywords = {tag refinement, tag relevance learning, social video retrieval, internet videos},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072061,
author = {Sawai, Kazuki and Takahashi, Tomokazu and Deguchi, Daisuke and Ide, Ichiro and Murase, Hiroshi},
title = {Scene Segmentation of Wedding Party Videos by Scenario-Based Matching with Example Videos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072061},
doi = {10.1145/2072298.2072061},
abstract = {We propose a method for scene segmentation of a wedding party video. Recently, it has become popular to take videos of a wedding ceremony and its party. Especially, because of its length, each scene of a wedding party video needs to be indexed with each event for efficient browsing. The proposed method segments a wedding party video into scenes of events by scenario- based matching with example videos that are synthesized by combining scenes from other wedding party videos according to a scenario.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1545–1548},
numpages = {4},
keywords = {scene segmentation, wedding party, dynamic time warping},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072062,
author = {Tian, Aibo and Zhang, Xuemei and Tretter, Daniel R.},
title = {Content-Aware Photo-on-Photo Composition for Consumer Photos},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072062},
doi = {10.1145/2072298.2072062},
abstract = {We propose an approach to automatically generate photo-on-photo compositions using only a consumer's own image collection, without imposing a strong theme constraint dictated by typical background art designs. Our goal is to ensure adequate foreground and background photo sizing, avoid occlusion of important background photo content, and respect simple aesthetic rules in photo arrangement. We propose a novel image content redundancy measure to extract the informative zone, which is a rectangular region of the background photo that should not be occluded by foreground content. Background photo layout and foreground layout regions are then calculated based on a placement algorithm that deals with possible aspect ratio mismatch of the canvas and background photo, as well as cases of insufficient foreground photo layout area.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1549–1552},
numpages = {4},
keywords = {content redundancy, photo composition, salience map, informative zone},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072063,
author = {Li, Cheng-Te and Hsieh, Hsun-Ping and Lin, Shou-De},
title = {PhotoFeel: Feeling Your Photo Collection with Graph-Based Audiovisual Flocking},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072063},
doi = {10.1145/2072298.2072063},
abstract = {This paper proposes an audiovisual presentation system, called PhotoFeel, to not only give users pleasant browsing atmosphere but also deliver a quick sense conveyed by given photo collection. While conventional photo display systems aim at improving the styles of presentation, we explore the visual and semantic feelings to create a space exhibiting the feelings from photographers. This is achieved by simulating the interactions among photos to emerge some flocking behaviors, where each photo is regarded as a simulated agent. To present the feelings of photos on the flocking, we construct two graphs by investigating the visual contents and tag semantics respectively. In addition, to enhance the diverse feelings of a photo collection, three audiovisual effects are composed to have rich presentation of feelings. Experimental results show that our PhotoFeel truly exhibits potential feelings for given photos and people comparatively favor our system.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1553–1556},
numpages = {4},
keywords = {audiovisual presentation, photo feelings, photo-based artwork, flocking simulation, visualization},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072064,
author = {Park, Minwoo and Luo, Jiebo and Gallagher, Andrew and Rabbani, Majid},
title = {Learning to Produce 3D Media from a Captured 2D Video},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072064},
doi = {10.1145/2072298.2072064},
abstract = {Due to the advances in display technologies and the commercial success of 3D motion pictures in recent years, there is renewed interest in enabling consumers to create 3D content. While new 3D content can be created using more advanced capture devices (i.e., stereo cameras), most people still use 2D capture devices. Furthermore, enormously large collections of captured media exist only in 2D. We present a system for producing stereo images from captured 2D videos. Our system detects "good" stereo frames from a 2D video, which was captured a priori without any constraints on camera motion or content. We use a trained classifier to detect pairs of video frames that are suitable for constructing stereo images. In particular, for a given frame It at time t, we determine if t̂ exists such that It+t̂ and It can form an acceptable stereo image. We verify the performance of our method for producing stereo media from captured 2D videos in a psychovisual evaluation using both professional movie clips and amateur home videos. To the best of our knowledge, detecting good stereo pairs from a captured 2D video has been adequately addressed in the literature.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1557–1560},
numpages = {4},
keywords = {2D to 3D, stereo image},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072065,
author = {Girgensohn, Andreas and Shipman, Frank and Wilcox, Lynn and Liu, Qiong and Liao, Chunyuan and Oneda, Yuichi},
title = {A Tool for Authoring Unambiguous Links from Printed Content to Digital Media},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072065},
doi = {10.1145/2072298.2072065},
abstract = {Embedded Media Markers (EMMs) are nearly transparent icons printed on paper documents that link to associated digital media. By using the document content for retrieval, EMMs are less visually intrusive than barcodes and other glyphs while still providing an indication for the presence of links. An initial implementation demonstrated good overall performance but exposed difficulties in guaranteeing the creation of unambiguous EMMs. We developed an EMM authoring tool that supports the interactive authoring of EMMs via visualizations that show the user which areas on a page may cause recognition errors and automatic feedback that moves the authored EMM away from those areas. The authoring tool and the techniques it relies on have been applied to corpora with different visual characteristics to explore the generality of our approach.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1561–1564},
numpages = {4},
keywords = {authoring tool, document retrieval, vision-based paper interface},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072066,
author = {Yeh, Jung-Yu and Hu, Min-Chun and Cheng, Wen-Huang and Wu, Ja-Ling},
title = {Interactive Digital Scrapbook Generation for Travel Photos Based on Design Principles of Typography},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072066},
doi = {10.1145/2072298.2072066},
abstract = {To facilitate the photo management and sharing tasks, many application tools have been developed to generate pleasant photo slideshows, collages, or scrapbooks by applying simple templates/layouts and visual effects. In this work, we propose a convenient digital scrapbook generating system for travel photos, named as IS-Scrapbook, which keeps the virtues while dismisses the drawbacks of conventional digital photo presentation styles. The IS-Scrapbook system has three friendly attributes compared to other photo presentation tools. First, aiming to attract the audience, we highlight objects more meaningful or familiar to the viewer, e.g. the landmark and the protagonist in the photos. Second, five basic design principles of typography, i.e. proximity, contrast, balance, color harmony and repetition, are applied to produce more vivacious layouts. Third, the system automatically generates a digital scrapbook with a default layout, and the user can further adjust photo positions and enrich each page by sketching or inserting dialog bubbles with the aid of the developed user interface. User study shows that the proposed work enhances the experiences of photo browsing and gives a brand-new way of photo sharing.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1565–1568},
numpages = {4},
keywords = {image content analysis, photo browsing, slideshow, design principle, photo sharing, scrapbook},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072067,
author = {Wang, Patricia and Tong, Xiaofeng and Du, Yangzhou and Li, Jianguo and Hu, Wei and Zhang, Yimin},
title = {Augmented Makeover Based on 3D Morphable Model},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072067},
doi = {10.1145/2072298.2072067},
abstract = {Avatar is the virtual representation of user's facial, body, and motion characteristics in computer game, social network, and augmented reality. Facial modeling needs enormous efforts to achieve immersive experience in applications like avatar chatting or online makeover. Great challenge exists in robust detection of 2D facial prominent points and mapping them to 3D models in a parameterized manner. Another challenge is how to characterize semantic components of eyes, mouth, nose, and cheek rather than low level mesh geometries. In this paper, we proposed an augmented makeover framework to deal with aforementioned challenges. Aiming to provide amateurs with flexible customizations, morphable model is constructed from a set of scanned 3D face data set. Appearance personalization is carried out in the offline phase where single image and multiple views are discussed respectively to generate deformative shape in a progressive manner. Augmentation is implemented in the online phase where a fast and robust 3D tracking is used to balance the tradeoff between accuracy and real-time requirements. By this means, immersive Human Computer Interaction such as virtual makeover and photo-realistic avatar chatting could be achieved.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1569–1572},
numpages = {4},
keywords = {structure analysis, virtual makeover, augmented reality, morphable model, surface reconstruction, face landmark},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

@inproceedings{10.1145/2072298.2072068,
author = {Li, Yingbo and Merialdo, Bernard and Rouvier, Mickael and Linares, Georges},
title = {Static and Dynamic Video Summaries},
year = {2011},
isbn = {9781450306164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2072298.2072068},
doi = {10.1145/2072298.2072068},
abstract = {Currently there are a lot of algorithms for video summarization; however most of them only represent visual information. In this paper, we propose two approaches for the construction of the summary using both video and text. One approach focuses on static summaries, where the summary is a set of selected keyframes and keywords, to be displayed in a fixed area. The second approach addresses dynamic summaries where video segments are selected based on both their visual and textual content to compose a new video sequence of predefined duration. Our approaches rely on an existing summarization algorithm, Video Maximal Marginal Relevance (Video-MMR), and its extension Text Video Maximal Marginal Relevance (TV-MMR) proposed by us. We describe the details of those approaches and present experimental results.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimedia},
pages = {1573–1576},
numpages = {4},
keywords = {dynamic summary, video summarization, static summary},
location = {Scottsdale, Arizona, USA},
series = {MM '11}
}

