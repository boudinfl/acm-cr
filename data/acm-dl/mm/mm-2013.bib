@inproceedings{10.1145/3245284,
author = {Shamma, David A.},
title = {Session Details: Keynote Address},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245284},
doi = {10.1145/3245284},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2512088,
author = {Churchill, Elizabeth},
title = {Multimedia Framed},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2512088},
doi = {10.1145/2502081.2512088},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1–2},
numpages = {2},
keywords = {multimedia, multimedia art, design, use analytics, art, theatre, social media, performance art, multimodality, wearable multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245285,
author = {Zimmerman, Roger F.},
title = {Session Details: Best Paper Session},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245285},
doi = {10.1145/3245285},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502126,
author = {Liu, Luoqi and Xu, Hui and Xing, Junliang and Liu, Si and Zhou, Xi and Yan, Shuicheng},
title = {"Wow! You Are so Beautiful Today!"},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502126},
doi = {10.1145/2502081.2502126},
abstract = {Beauty e-Experts, a fully automatic system for hairstyle and facial makeup recommendation and synthesis, is developed in this work. Given a user-provided frontal face image with short/bound hair and no/light makeup, the Beauty e-Experts system can not only recommend the most suitable hairdo and makeup, but also show the synthetic effects. To obtain enough knowledge for beauty modeling, we build the Beauty e-Experts Database, which contains 1,505 attractive female photos with a variety of beauty attributes and beauty-related attributes annotated. Based on this Beauty e-Experts Dataset, two problems are considered for the Beauty e-Experts system: what to recommend and how to wear, which describe a similar process of selecting hairstyle and cosmetics in our daily life. For the what-to-recommend problem, we propose a multiple tree-structured super-graphs model to explore the complex relationships among the high-level beauty attributes, mid-level beauty-related attributes and low-level image features, and then based on this model, the most compatible beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear problem, an effective and efficient facial image synthesis module is designed to seamlessly synthesize the recommended hairstyle and makeup into the user facial image. Extensive experimental evaluations and analysis on testing images of various conditions well demonstrate the effectiveness of the proposed system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {3–12},
numpages = {10},
keywords = {beauty synthesis, multiple tree-structured super-graphs model, beauty recommendation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502088,
author = {Fang, Quan and Sang, Jitao and Xu, Changsheng},
title = {GIANT: Geo-Informative Attributes for Location Recognition and Exploration},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502088},
doi = {10.1145/2502081.2502088},
abstract = {This paper considers the problem of automatically discovering geo-informative attributes for location recognition and exploration. The attribute is expected to be both discriminative and representative, which corresponds to a distinctive visual pattern and associates with semantic interpretation. For solution, we analyze the attribute at region level. Each segmented region in the training set is assigned a binary latent variable indicating its discriminative capability. A latent learning framework is proposed for discriminative region detection and geo-informative attribute discovery. Moreover, we use user-generated content to obtain the semantic interpretation for the discovered visual attribute. The proposed approach are evaluated on one challenging dataset including GoogleStreetView and Flickr photos. Experimental results show that: (1) geo-informative attribute are discriminative and useful for location recognition; (2) the discovered semantic interpretation is meaningful and can be exploited for further explorations.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {13–22},
numpages = {10},
keywords = {geo-informative attributes, latent model, location recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502103,
author = {Zhao, Xin and Li, Xue and Pang, Chaoyi and Zhu, Xiaofeng and Sheng, Quan Z.},
title = {Online Human Gesture Recognition from Motion Data Streams},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502103},
doi = {10.1145/2502081.2502103},
abstract = {Online human gesture recognition has a wide range of applications in computer vision, especially in human-computer interaction applications. Recent introduction of cost-effective depth cameras brings on a new trend of research on body-movement gesture recognition. However, there are two major challenges: i) how to continuously recognize gestures from unsegmented streams, and ii) how to differentiate different styles of a same gesture from other types of gestures. In this paper, we solve these two problems with a new effective and efficient feature extraction method that uses a dynamic matching approach to construct a feature vector for each frame and improves sensitivity to the features of different gestures and decreases sensitivity to the features of gestures within the same class. Our comprehensive experiments on MSRC-12 Kinect Gesture and MSR-Action3D datasets have demonstrated a superior performance than the stat-of-the-art approaches.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {23–32},
numpages = {10},
keywords = {depth camera, feature extraction, gesture recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502093,
author = {Zhang, Hanwang and Zha, Zheng-Jun and Yang, Yang and Yan, Shuicheng and Gao, Yue and Chua, Tat-Seng},
title = {Attribute-Augmented Semantic Hierarchy: Towards Bridging Semantic Gap and Intention Gap in Image Retrieval},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502093},
doi = {10.1145/2502081.2502093},
abstract = {This paper presents a novel Attribute-augmented Semantic Hierarchy (A2 SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in Content-based Image Retrieval (CBIR). A2 SH organizes the semantic concepts into multiple semantic levels and augments each concept with a set of related attributes, which describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. A hierarchical semantic similarity function is learnt to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedbacks on attributes and images. These feedbacks are then used to refine the search results based on A2 SH. We develop a content-based image retrieval system based on the proposed A2 SH. We conduct extensive experiments on a large-scale data set of over one million Web images. Experimental results show that the proposed A2 SH can characterize the semantic affinities among images accurately and can shape user search intent precisely and quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {33–42},
numpages = {10},
keywords = {image retrieval, attribute, semantic hierarchy},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245286,
author = {Joshi, Dhiraj},
title = {Session Details: Experience},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245286},
doi = {10.1145/3245286},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502083,
author = {Xu, Qianqian and Xiong, Jiechao and Huang, Qingming and Yao, Yuan},
title = {Robust Evaluation for Quality of Experience in Crowdsourcing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502083},
doi = {10.1145/2502081.2502083},
abstract = {Strategies exploiting crowdsourcing are increasingly being applied in the area of Quality of Experience (QoE) for multimedia. They enable researchers to conduct experiments with a more diverse set of participants and at a lower economic cost than conventional laboratory studies. However, a major challenge for crowdsourcing tests is the detection and control of outliers, which may arise due to different test conditions, human errors or abnormal variations in context. For this purpose, it is desired to develop a robust evaluation methodology to deal with crowdsourceable data, which are possibly incomplete, imbalanced, and distributed on a graph. In this paper, we propose a robust rating scheme based on robust regression and Hodge Decomposition on graphs, to assess QoE using crowdsourcing. The scheme shows that the removal of outliers in crowdsourcing experiments would be helpful for purifying data and could provide us with more reliable results. The effectiveness of the proposed scheme is further confirmed by experimental studies on both simulated examples and real-world data.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {43–52},
numpages = {10},
keywords = {paired comparison, hodge decomposition, lasso, outlier detection, random graph, robust evaluation, quality of experience (QoE), crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502102,
author = {Chu, Wei-Ta and Chen, Yu-Kuang and Chen, Kuan-Ta},
title = {Size Does Matter: How Image Size Affects Aesthetic Perception?},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502102},
doi = {10.1145/2502081.2502102},
abstract = {There is no doubt that an image's content determines how people assess the image aesthetically. Previous works have shown that image contrast, saliency features, and the composition of objects may jointly determine whether or not an image is perceived as aesthetically pleasing. In addition to an image's content, the way the image is presented may affect how much viewers appreciate it. For example, it may be assumed that a picture will always look better when it is displayed in a larger size. Is this "the-bigger-the-better" rule always valid? If not, in what situations is it invalid? In this paper, we investigate how an image's resolution (pixels) and physical dimensions (inches) affect viewers' appreciation of it. Based on a large-scale aesthetic assessments of 100 images displayed in a variety of resolutions and physical dimensions, we show that an image's size significantly affects its aesthetic rating in a complicated way. Normally a picture looks better when it is bigger, but it may look worse depending on its content. We develop a set of regression models to predict a picture's absolute and relative aesthetic levels at a given display size based on its content and compositional features. In addition, we analyze the essential features that lead to the size-dependent property of image aesthetics. It is hoped that this work will motivate further research by showing that both content and presentation should be considered when evaluating an image's aesthetic appeals.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {53–62},
numpages = {10},
keywords = {crowdsourcing, human percpetion, image aesthetics, size-dependent aesthetics, quality assessment},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502106,
author = {Li, Zhonghua and Wang, Ju-Chiang and Cai, Jingli and Duan, Zhiyan and Wang, Hsin-Min and Wang, Ye},
title = {Non-Reference Audio Quality Assessment for Online Live Music Recordings},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502106},
doi = {10.1145/2502081.2502106},
abstract = {Immensely popular video sharing websites such as YouTube have become the most important sources of music information for Internet users and the most prominent platform for sharing live music. The audio quality of this huge amount of live music recordings, however, varies significantly due to factors such as environmental noise, location, and recording device. However, most video search engines do not take audio quality into consideration when retrieving and ranking results. Given the fact that most users prefer live music videos with better audio quality, we propose the first automatic, non-reference audio quality assessment framework for live music video search online. We first construct two annotated datasets of live music recordings. The first dataset contains 500 human-annotated pieces, and the second contains 2,400 synthetic pieces systematically generated by adding noise effects to clean recordings. Then, we formulate the assessment task as a ranking problem and try to solve it using a learning-based scheme. To validate the effectiveness of our framework, we perform both objective and subjective evaluations. Results show that our framework significantly improves the ranking performance of live music recording retrieval and can prove useful for various real-world music applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {63–72},
numpages = {10},
keywords = {audio quality assessment, learning-to-rank, live music videos, music information retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502110,
author = {Su, Yu-Chuan and Chiu, Tzu-Hsuan and Chen, Yan-Ying and Yeh, Chun-Yen and Hsu, Winston H.},
title = {Enabling Low Bitrate Mobile Visual Recognition: A Performance versus Bandwidth Evaluation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502110},
doi = {10.1145/2502081.2502110},
abstract = {The rapid development of technologies in both hardware and software have made content-based multimedia services feasible on mobile devices such as smartphones and tablets; and the strong needs for mobile visual search and recognition have been emerging. While many real applications of visual recognition require a large scale recognition systems, the same technologies that support server-based scalable visual recognition may not be feasible on mobile devices due to the resource constraints. Although the client-server framework ensures the scalability, the real-time response subjects to the limitation on network bandwidth. Therefore, the main challenge for mobile visual recognition system should be the recognition bitrate, which is the amount of data transmission under the same recognition performance. For this work, we exploit and compare various strategies such as compact features, feature compression, feature signatures by hashing, image scaling, etc., to enable low bitrate mobile visual recognition. We argue that thumbnail image is a competitive candidate for low bitrate visual recognition because it carries multiple features at once and multi-feature fusion is important as the size of semantic space increases. Our evaluations on two subsets of ImageNet, both contain more than 10,000 images with 19 and 137 categories, verify the efficacy of thumbnail images. We further suggest a new strategy that combines single (local) feature signature and the thumbnail image, which achieves significant bitrate reduction from (average) 102,570 to 4,661 bytes with merely (overall) 10% performance degradation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {73–82},
numpages = {10},
keywords = {mobile image recognition, multi-modal fusion, thumbnail image, bitrate},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245287,
author = {Liem, Cynthia},
title = {Session Details: Music &amp; Play},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245287},
doi = {10.1145/3245287},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502115,
author = {Mour\~{a}o, Andr\'{e} and Magalh\~{a}es, Jo\~{a}o},
title = {Competitive Affective Gaming: Winning with a Smile},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502115},
doi = {10.1145/2502081.2502115},
abstract = {Human-computer interaction (HCI) is expanding towards natural modalities of human expression. Gestures, body movements and other affective interaction techniques can change the way computers interact with humans. In this paper, we propose to extend existing interaction paradigms by including facial expression as a controller in videogames. NovaEm\"{o}tions is a multiplayer game where players score by acting an emotion through a facial expression. We designed an algorithm to offer an engaging interaction experience using the facial expression. Despite the novelty of the interaction method, our game scoring algorithm kept players engaged and competitive. A user study done with 46 users showed the success and potential for the usage of affective-based interaction in videogames, i.e., the facial expression as the sole controller in videogames. Moreover, we released a novel facial expression dataset with over 41,000 images. These face images were captured in a novel and realistic setting: users playing games where a player's facial expression has an impact on the game score.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {83–92},
numpages = {10},
keywords = {competitive games, videogames, affective interaction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502120,
author = {H\"{u}rst, Wolfgang and Dekker, Joris},
title = {Tracking-Based Interaction for Object Creation in Mobile Augmented Reality},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502120},
doi = {10.1145/2502081.2502120},
abstract = {In this work, we evaluate the feasibility of tracking-based interaction using a mobile phone's or tablet's camera in order to create and edit 3D objects in augmented reality applications. We present a feasibility study investigating if and how gestures made with your finger can be used to create such objects. A revised interface design is evaluated in a user study with 24 subjects that reveals a high usability and entertainment value, but also identifies issues such as ergonomic discomfort and imprecise input for complex tasks. Hence, our results suggest huge potential for this type of interaction in the entertainment, edutainment, and leisure domain, but limited usefulness for serious applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {93–102},
numpages = {10},
keywords = {hand gesture interaction, mobile interaction, user evaluation, augmented reality},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502101,
author = {Percival, Graham and Bailey, Nicholas and Tzanetakis, George},
title = {Physical Modelling and Supervised Training of a Virtual String Quartet},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502101},
doi = {10.1145/2502081.2502101},
abstract = {This work improves the realism of synthesis and performance of string quartet music by generating audio through physical modelling of the violins, viola, and cello. To perform music with the physical models, virtual musicians interpret the musical score and generate actions which control the physical models. The resulting audio and haptic signals are examined with support vector machines, which adjust the bowing parameters in order to establish and maintain a desirable timbre. This intelligent feedback control is trained with human input, but after the initial training is completed, the virtual musicians perform autonomously. The system can synthesize and control different instruments of the same type (e.g., multiple distinct violins) and has been tested on two distinct string quartets (total of 8 violins, 2 violas, 2 cellos). In addition to audio, the system creates a video animation of the instruments performing the sheet music.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {103–112},
numpages = {10},
keywords = {intelligent feedback control, physical modelling, autonomous music performance, violin synthesis, machine learning},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502124,
author = {Smith, Jordan B.L. and Chew, Elaine},
title = {Using Quadratic Programming to Estimate Feature Relevance in Structural Analyses of Music},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502124},
doi = {10.1145/2502081.2502124},
abstract = {To identify repeated patterns and contrasting sections in music, it is common to use self-similarity matrices (SSMs) to visualize and estimate structure. We introduce a novel application for SSMs derived from audio recordings: using them to learn about the potential reasoning behind a listener's annotation. We use SSMs generated by musically-motivated audio features at various timescales to represent contributions to a structural annotation. Since a listener's attention can shift among musical features (e.g., rhythm, timbre, and harmony) throughout a piece, we further break down the SSMs into section-wise components and use quadratic programming (QP) to minimize the distance between a linear sum of these components and the annotated description. We posit that the optimal section-wise weights on the feature components may indicate the features to which a listener attended when annotating a piece, and thus may help us to understand why two listeners disagreed about a piece's structure. We discuss some examples that substantiate the claim that feature relevance varies throughout a piece, using our method to investigate differences between listeners' interpretations, and lastly propose some variations on our method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {113–122},
numpages = {10},
keywords = {quadratic programming, music cognition, repetition, music perception, music structure analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245288,
author = {Rui, Yong},
title = {Session Details: Similarity Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245288},
doi = {10.1145/3245288},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502091,
author = {Zhang, Lei and Zhang, Yongdong and Tang, Jinhui and Gu, Xiaoguang and Li, Jintao and Tian, Qi},
title = {Topology Preserving Hashing for Similarity Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502091},
doi = {10.1145/2502081.2502091},
abstract = {Binary hashing has been widely used for efficient similarity search. Learning efficient codes has become a research focus and it is still a challenge. In many cases, the real-world data often lies on a low-dimensional manifold, which should be taken into account to capture meaningful neighbors with hashing. The importance of a manifold is its topology, which represents the neighborhood relationships between its subregions and the relative proximities between the neighbors of each subregion, e.g. the relative ranking of neighbors of each subregion. Most existing hashing methods try to preserve the neighborhood relationships by mapping similar points to close codes, while ignoring the neighborhood rankings. Moreover, most hashing methods lack in providing a good ranking for query results since they use Hamming distance as the similarity metric, and in practice, there are often a lot of results sharing the same distance to a query. In this paper, we propose a novel hashing method to solve these two issues jointly. The proposed method is referred to as Topology Preserving Hashing (TPH). TPH is distinct from prior works by preserving the neighborhood rankings of data points in Hamming space. The learning stage of TPH is formulated as a generalized eigendecomposition problem with closed form solutions. Experimental comparisons with other state-of-the-art methods on three noted image benchmarks demonstrate the efficacy of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {123–132},
numpages = {10},
keywords = {approximate nearest neighbor search, binary hashing, similarity search, topology preserving hashing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502100,
author = {Wang, Jianfeng and Wang, Jingdong and Yu, Nenghai and Li, Shipeng},
title = {Order Preserving Hashing for Approximate Nearest Neighbor Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502100},
doi = {10.1145/2502081.2502100},
abstract = {In this paper, we propose a novel method to learn similarity-preserving hash functions for approximate nearest neighbor (NN) search. The key idea is to learn hash functions by maximizing the alignment between the similarity orders computed from the original space and the ones in the hamming space. The problem of mapping the NN points into different hash codes is taken as a classification problem in which the points are categorized into several groups according to the hamming distances to the query. The hash functions are optimized from the classifiers pooled over the training points. Experimental results demonstrate the superiority of our approach over existing state-of-the-art hashing techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {133–142},
numpages = {10},
keywords = {order preserving hashing, learning to hash, approximate nearest neighbor search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502107,
author = {Zhu, Xiaofeng and Huang, Zi and Shen, Heng Tao and Zhao, Xin},
title = {Linear Cross-Modal Hashing for Efficient Multimedia Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502107},
doi = {10.1145/2502081.2502107},
abstract = {Most existing cross-modal hashing methods suffer from the scalability issue in the training phase. In this paper, we propose a novel cross-modal hashing approach with a linear time complexity to the training data size, to enable scalable indexing for multimedia search across multiple modals. Taking both the intra-similarity in each modal and the inter-similarity across different modals into consideration, the proposed approach aims at effectively learning hash functions from large-scale training datasets. More specifically, for each modal, we first partition the training data into $k$ clusters and then represent each training data point with its distances to $k$ centroids of the clusters. Interestingly, such a k-dimensional data representation can reduce the time complexity of the training phase from traditional O(n2) or higher to O(n), where $n$ is the training data size, leading to practical learning on large-scale datasets. We further prove that this new representation preserves the intra-similarity in each modal. To preserve the inter-similarity among data points across different modals, we transform the derived data representations into a common binary subspace in which binary codes from all the modals are "consistent" and comparable. nThe transformation simultaneously outputs the hash functions for all modals, which are used to convert unseen data into binary codes. Given a query of one modal, it is first mapped into the binary codes using the modal's hash functions, followed by matching the database binary codes of any other modals. Experimental results on two benchmark datasets confirm the scalability and the effectiveness of the proposed approach in comparison with the state of the art.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {143–152},
numpages = {10},
keywords = {multimedia search, cross-modal, hashing, index},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502112,
author = {Wu, Pengcheng and Hoi, Steven C.H. and Xia, Hao and Zhao, Peilin and Wang, Dayong and Miao, Chunyan},
title = {Online Multimodal Deep Similarity Learning with Application to Image Retrieval},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502112},
doi = {10.1145/2502081.2502112},
abstract = {Recent years have witnessed extensive studies on distance metric learning (DML) for improving similarity search in multimedia information retrieval tasks. Despite their successes, most existing DML methods suffer from two critical limitations: (i) they typically attempt to learn a linear distance function on the input feature space, in which the assumption of linearity limits their capacity of measuring the similarity on complex patterns in real-world applications; (ii) they are often designed for learning distance metrics on uni-modal data, which may not effectively handle the similarity measures for multimedia objects with multimodal representations. To address these limitations, in this paper, we propose a novel framework of online multimodal deep similarity learning (OMDSL), which aims to optimally integrate multiple deep neural networks pretrained with stacked denoising autoencoder. In particular, the proposed framework explores a unified two-stage online learning scheme that consists of (i) learning a flexible nonlinear transformation function for each individual modality, and (ii) learning to find the optimal combination of multiple diverse modalities simultaneously in a coherent process. We conduct an extensive set of experiments to evaluate the performance of the proposed algorithms for multimodal image retrieval tasks, in which the encouraging results validate the effectiveness of the proposed technique.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {153–162},
numpages = {10},
keywords = {online learning, image retrieval, distance metric learning, deep learning, similarity learning},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245289,
author = {Huber, Jochen},
title = {Session Details: Art, Performance, and Sports},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245289},
doi = {10.1145/3245289},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502092,
author = {Foote, Eric and Carr, Peter and Lucey, Patrick and Sheikh, Yaser and Matthews, Iain},
title = {One-Man-Band: A Touch Screen Interface for Producing Live Multi-Camera Sports Broadcasts},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502092},
doi = {10.1145/2502081.2502092},
abstract = {Generating live broadcasts of sporting events requires a coordinated crew of camera operators, directors, and technical personnel to control and switch between multiple cameras to tell the evolving story of a game. In this paper, we present an unimodal interface concept that allows one person to cover live sporting action by controlling multiple cameras and and determining which view to broadcast. The interface exploits the structure of sports broadcasts which typically switch between a zoomed out game-camera view (which records the strategic team-level play), and a zoomed in iso-camera view (which captures the animated adversarial relations between opposing players). The operator simultaneously controls multiple pan-tilt-zoom cameras by pointing at a location on the touch screen, and selects which camera to broadcast using one or two points of contact. The image from the selected camera is superimposed on top of a wide-angle view captured from a context-camera which provides the operator with periphery information (which is useful for ensuring good framing while controlling the camera). We show that by unifying directorial and camera operation functions, we can achieve comparable broadcast quality to a multi-person crew, while reducing cost, logistical, and communication complexities.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {163–172},
numpages = {10},
keywords = {multiple cameras, camera control, touch screen},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502125,
author = {Kobayashi, Hiroki and Hirose, Michitaka and Fujiwara, Akio and Nakamura, Kazuhiko and Sezaki, Kaoru and Saito, Kaoru},
title = {Tele Echo Tube: Beyond Cultural and Imaginable Boundaries},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502125},
doi = {10.1145/2502081.2502125},
abstract = {Currently, human-computer interaction (HCI) is primarily focused on human-centric interactions; however, people experience many nonhuman-centric interactions during the course of a day. Interactions with nature, such as experiencing the sounds of birds or trickling water, can imprint the beauty of nature in our memories. In this context, this paper presents an interface of such nonhuman interactions to observe people's reaction to the interactions through an imaginable interaction with a mythological creature. Tele Echo Tube (TET) is a speaking tube interface that acoustically interacts with a deep mountain echo through the slightly vibrating lampshade-like interface. TET allows users to interact with the mountain echo in real time through an augmented echo-sounding experience with the vibration over a satellite data network. This novel interactive system can create an imaginable presence of the mythological creature in the undeveloped natural locations beyond our cultural and imaginable boundaries. The results indicate that users take the reflection of the sound as a cue that triggers the nonlinguistic believability in the form of the mythological metaphor of the mountain echo. This echo-like experience of believable interaction in an augmented reality between a human and nature gave the users an imaginable presence of the mountain echo with a high degree of excitement. This paper describes the development and integration of nonhuman-centric design protocols, requirements, methods, and context evaluation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {173–182},
numpages = {10},
keywords = {sustainability, HCBI (human-computer-biosphere interaction), nature interface, sustainable design},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502104,
author = {Lin, Min and Hu, Zhenzhen and Liu, Si and Wang, Meng and Hong, Richang and Yan, Shuicheng},
title = {EHeritage of Shadow Puppetry: Creation and Manipulation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502104},
doi = {10.1145/2502081.2502104},
abstract = {To preserve the precious traditional heritage Chinese shadow puppetry, we propose the puppetry eHeritage, including a creator module and a manipulator module. The creator module accepts a frontal view face image and a profile face image of the user as input, and automatically generates the corresponding puppet, which looks like the original person and meanwhile has some typical characteristics of traditional Chinese shadow puppetry. In order to create the puppet, we first extract the central profile curve and warp the reference puppet eye and eyebrow to the shape of the frontal view eye and eyebrow. Then we transfer the puppet texture to the real face area. The manipulator module can accept the script provided by the user as input and automatically generate the motion sequences. Technically, we first learn atomic motions from a set of shadow puppetry videos. A scripting system converts the user's input to atomic motions, and finally synthesizes the animation based on the atomic motion instances. For better visual effects, we propose the sparsity optimization over simplexes formulation to automatically assemble weighted instances of different atomic actions into a smooth shadow puppetry animation sequence. We evaluate the performance of the creator module and the manipulator module sequentially. Extensive experimental results on the creation of puppetry characters and puppetry plays well demonstrate the effectiveness of the proposed system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {183–192},
numpages = {10},
keywords = {sparsity optimization over simplex, face rendering, animation, chinese shadow puppetry},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502086,
author = {Carr, Peter and Mistry, Michael and Matthews, Iain},
title = {Hybrid Robotic/Virtual Pan-Tilt-Zom Cameras for Autonomous Event Recording},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502086},
doi = {10.1145/2502081.2502086},
abstract = {We present a method to generate aesthetic video from a robotic camera by incorporating a virtual camera operating on a delay, and a hybrid controller which uses feedback from both the robotic and virtual cameras. Our strategy employs a robotic camera to follow a coarse region-of-interest identified by a realtime computer vision system, and then resamples the captured images to synthesize the video that would have been recorded along a smooth, aesthetic camera trajectory. The smooth motion trajectory is obtained by operating the virtual camera on a short delay so that perfect knowledge of immediate future events is known. Previous autonomous camera installations have employed either robotic cameras or stationary wide-angle cameras with subregion cropping. Robotic cameras track the subject using realtime sensor data, and regulate a smoothness-latency trade-off through control gains. Fixed cameras post-process the data and suffer significant reductions in image resolution when the subject moves freely over a large area.Our approach provides a solution for broadcasting events from locations where camera operators cannot easily access. We can also offer broadcasters additional actuated camera angles without the overhead of additional human operators. Experiments on our prototype system for college basketball illustrate how our approach better mimics human operators compared to traditional robotic control approaches, while avoiding the loss in resolution that occurs from fixed camera system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {193–202},
numpages = {10},
keywords = {camera, planning, control, tracking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245290,
author = {Luo, Jiebo},
title = {Session Details: Brave New Topics: Social and Cognitive Aspects},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245290},
doi = {10.1145/3245290},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502279,
author = {Gupta, Amarnath and Jain, Ramesh},
title = {Social Life Networks: A Multimedia Problem?},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502279},
doi = {10.1145/2502081.2502279},
abstract = {Connecting people to the resources they need is a fundamental task for any society. We present the idea of a technology that can be used by the middle tier of a society so that it uses people's mobile devices and social networks to connect the needy with providers. We conceive of a world observatory called the Social Life Network (SLN) that connects together people and things and monitors for people's needs as their life situations evolve. To enable such a system we need SLN to register and recognize situations by combining people's activities and data streaming from personal devices and environment sensors, and based on the situations make the connections when possible. But is this a multimedia problem? We show that many pattern recognition, machine learning, sensor fusion and information retrieval techniques used in multimedia-related research are deeply connected to the SLN problem. We sketch the functional architecture of such a system and show the place for these techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {203–212},
numpages = {10},
keywords = {social computing, social networks, event-based system, situation recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502280,
author = {Cristani, Marco and Vinciarelli, Alessandro and Segalin, Cristina and Perina, Alessandro},
title = {Unveiling the Multimedia Unconscious: Implicit Cognitive Processes and Multimedia Content Analysis},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502280},
doi = {10.1145/2502081.2502280},
abstract = {One of the main findings of cognitive sciences is that automatic processes of which we are unaware shape, to a significant extent, our perception of the environment. The phenomenon applies not only to the real world, but also to multimedia data we consume every day. Whenever we look at pictures, watch a video or listen to audio recordings, our conscious attention efforts focus on the observable content, but our cognition spontaneously perceives intentions, beliefs, values, attitudes and other constructs that, while being outside of our conscious awareness, still shape our reactions and behavior. So far, multimedia technologies have neglected such a phenomenon to a large extent. This paper argues that taking into account cognitive effects is possible and it can also improve multimedia approaches. As a supporting proof-of-concept, the paper shows not only that there are visual patterns correlated with the personality traits of 300 Flickr users to a statistically significant extent, but also that the personality traits (both self-assessed and attributed by others) of those users can be inferred from the images these latter post as "favourite".},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {213–222},
numpages = {10},
keywords = {flickr, feature extraction, personality traits, image representation, cognitive processes},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502282,
author = {Borth, Damian and Ji, Rongrong and Chen, Tao and Breuel, Thomas and Chang, Shih-Fu},
title = {Large-Scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502282},
doi = {10.1145/2502081.2502282},
abstract = {We address the challenge of sentiment analysis from visual content. In contrast to existing methods which infer sentiment or emotion directly from visual low-level features, we propose a novel approach based on understanding of the visual concepts that are strongly related to sentiments. Our key contribution is two-fold: first, we present a method built upon psychological theories and web mining to automatically construct a large-scale Visual Sentiment Ontology (VSO) consisting of more than 3,000 Adjective Noun Pairs (ANP). Second, we propose SentiBank, a novel visual concept detector library that can be used to detect the presence of 1,200 ANPs in an image. The VSO and SentiBank are distinct from existing work and will open a gate towards various applications enabled by automatic sentiment analysis. Experiments on detecting sentiment of image tweets demonstrate significant improvement in detection accuracy when comparing the proposed SentiBank based predictors with the text-based approaches. The effort also leads to a large publicly available resource consisting of a visual sentiment ontology, a large detector library, and the training/testing benchmark for visual sentiment analysis.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {223–232},
numpages = {10},
keywords = {concept detection, sentiment prediction, ontology, social multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502281,
author = {Sun, Xinghai and Wang, Changhu and Xu, Chao and Zhang, Lei},
title = {Indexing Billions of Images for Sketch-Based Retrieval},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502281},
doi = {10.1145/2502081.2502281},
abstract = {Because of the popularity of touch-screen devices, it has become a highly desirable feature to retrieve images from a huge repository by matching with a hand-drawn sketch. Although searching images via keywords or an example image has been successfully launched in some commercial search engines of billions of images, it is still very challenging for both academia and industry to develop a sketch-based image retrieval system on a billion-level database. In this work, we systematically study this problem and try to build a system to support query-by-sketch for two billion images. The raw edge pixel and Chamfer matching are selected as the basic representation and matching in this system, owning to the superior performance compared with other methods in extensive experiments. To get a more compact feature and a faster matching, a vector-like Chamfer feature pair is introduced, based on which the complex matching is reformulated as the crossover dot-product of feature pairs. Based on this new formulation, a compact shape code is developed to represent each image/sketch by projecting the Chamfer features to a linear subspace followed by a non-linear source coding. Finally, the multi-probe Kmedoids-LSH is leveraged to index database images, and the compact shape codes are further used for fast reranking. Extensive experiments show the effectiveness of the proposed features and algorithms in building such a sketch-based image search system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {233–242},
numpages = {10},
keywords = {billion scale, chamfer matching, indexing, product quantization, sketch-based image retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502283,
author = {Hua, Xian-Sheng and Yang, Linjun and Wang, Jingdong and Wang, Jing and Ye, Ming and Wang, Kuansan and Rui, Yong and Li, Jin},
title = {Clickage: Towards Bridging Semantic and Intent Gaps via Mining Click Logs of Search Engines},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502283},
doi = {10.1145/2502081.2502283},
abstract = {The semantic gap between low-level visual features and high-level semantics has been investigated for decades but still remains a big challenge in multimedia. When "search" became one of the most frequently used applications, "intent gap", the gap between query expressions and users' search intents, emerged. Researchers have been focusing on three approaches to bridge the semantic and intent gaps: 1) developing more representative features, 2) exploiting better learning approaches or statistical models to represent the semantics, and 3) collecting more training data with better quality. However, it remains a challenge to close the gaps. In this paper, we argue that the massive amount of click data from commercial search engines provides a data set that is unique in the bridging of the semantic and intent gap. Search engines generate millions of click data (a.k.a. image-query pairs), which provide almost "unlimited" yet strong connections between semantics and images, as well as connections between users' intents and queries. To study the intrinsic properties of click data and to investigate how to effectively leverage this huge amount of data to bridge semantic and intent gap is a promising direction to advance multimedia research. In the past, the primary obstacle is that there is no such dataset available to the public research community. This changes as Microsoft has released a new large-scale real-world image click data to public. This paper presents preliminary studies on the power of large-scale click data with a variety of experiments, such as building large-scale concept detectors, tag processing, search, definitive tag detection, intent analysis, etc., with the goal to inspire deeper researches based on this dataset.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {243–252},
numpages = {10},
keywords = {image search, dataset, image understanding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502284,
author = {Yuan, Zhaoquan and Sang, Jitao and Liu, Yan and Xu, Changsheng},
title = {Latent Feature Learning in Social Media Network},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502284},
doi = {10.1145/2502081.2502284},
abstract = {The current trend in social media analysis and application is to use the pre-defined features and devoted to the later model development modules to meet the end tasks. In this work, we claim that representation is critical to the end tasks and contributes much to the model development module. We provide evidence that specially learned feature well addresses the diverse, heterogeneous and collective characteristics of social media data. Therefore, we propose to transfer the focus from the model development to latent feature learning, and present a general feature learning framework based on the popular deep architecture. In particular, following the proposed framework, we design a novel relational generative deep learning model to test the idea on link analysis tasks in the social media networks. We show that the derived latent features well embed both the media content and their observed links, leading to improvement in social media tasks of user recommendation and social image annotation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {253–262},
numpages = {10},
keywords = {link analysis, feature learning, deep learning, social media},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245291,
author = {Hsu, Winston},
title = {Session Details: Action and Event Recognition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245291},
doi = {10.1145/3245291},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502089,
author = {Liang, Xiaodan and Lin, Liang and Cao, Liangliang},
title = {Learning Latent Spatio-Temporal Compositional Model for Human Action Recognition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502089},
doi = {10.1145/2502081.2502089},
abstract = {Action recognition is an important problem in multimedia understanding. This paper addresses this problem by building an expressive compositional action model. We model one action instance in the video with an ensemble of spatio-temporal compositions: a number of discrete temporal anchor frames, each of which is further decomposed to a layout of deformable parts. In this way, our model can identify a Spatio-Temporal And-Or Graph (STAOG) to represent the latent structure of actions emph{e.g.} triple jumping, swinging and high jumping. The STAOG model comprises four layers: (i) a batch of leaf-nodes in bottom for detecting various action parts within video patches; (ii) the or-nodes over bottom, i.e. switch variables to activate their children leaf-nodes for structural variability; (iii) the and-nodes within an anchor frame for verifying spatial composition; and (iv) the root-node at top for aggregating scores over temporal anchor frames. Moreover, the contextual interactions are defined between leaf-nodes in both spatial and temporal domains. For model training, we develop a novel weakly supervised learning algorithm which iteratively determines the structural configuration (e.g. the production of leaf-nodes associated with the or-nodes) along with the optimization of multi-layer parameters. By fully exploiting spatio-temporal compositions and interactions, our approach handles well large intra-class action variance (emph{e.g.} different views, individual appearances, spatio-temporal structures). The experimental results on the challenging databases demonstrate superior performance of our approach over other methods.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {263–272},
numpages = {10},
keywords = {action recognition, and-or graph, structural learning, video understanding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502094,
author = {Zhao, Xu and Liu, Yuncai and Fu, Yun},
title = {Exploring Discriminative Pose Sub-Patterns for Effective Action Classification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502094},
doi = {10.1145/2502081.2502094},
abstract = {Articulated configuration of human body parts is an essential representation of human motion, therefore is well suited for classifying human actions. In this work, we propose a novel approach to exploring the discriminative pose sub-patterns for effective action classification. These pose sub-patterns are extracted from a predefined set of 3D poses represented by hierarchical motion angles. The basic idea is motivated by the two observations: (1) There exist representative sub-patterns in each action class, from which the action class can be easily differentiated. (2) These sub-patterns frequently appear in the action class. By constructing a connection between frequent sub-patterns and the discriminative measure, we develop the SSPI, namely, the Support Sub-Pattern Induced learning algorithm for simultaneous feature selection and feature learning. Based on the algorithm, discriminative pose sub-patterns can be identified and used as a series of "magnetic centers" on the surface of normalized super-sphere for feature transform. The "attractive forces" from the sub-patterns determine the direction and step-length of the transform. This transformation makes a feature more discriminative while maintaining dimensionality invariance. Comprehensive experimental studies conducted on a large scale motion capture dataset demonstrate the effectiveness of the proposed approach for action classification and the superior performance over the state-of-the-art techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {273–282},
numpages = {10},
keywords = {sub-pattern mining, action classification, feature transform},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502099,
author = {Gupta, Raj and Chia, Alex Yong-Sang and Rajan, Deepu},
title = {Human Activities Recognition Using Depth Images},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502099},
doi = {10.1145/2502081.2502099},
abstract = {We present a new method to classify human activities by leveraging on the cues available from depth images alone. Towards this end, we propose a descriptor which couples depth and spatial information of the segmented body to describe a human pose. Unique poses (i.e. codewords) are then identified by a spatial-based clustering step. Given a video sequence of depth images, we segment humans from the depth images and represent these segmented bodies as a sequence of codewords. We exploit unique poses of an activity and the temporal ordering of these poses to learn subsequences of codewords which are strongly discriminative for the activity. Each discriminative subsequence acts as a classifier and we learn a boosted ensemble of discriminative subsequences to assign a confidence score for the activity label of the test sequence. Unlike existing methods which demand accurate tracking of 3D joint locations or couple depth with color image information as recognition cues, our method requires only the segmentation masks from depth images to recognize an activity. Experimental results on the publicly available Human Activity Dataset (which comprises 12 challenging activities) demonstrate the validity of our method, where we attain a precision/recall of 78.1%/75.4% when the person was not seen before in the training set, and 94.6%/93.1% when the person was seen before.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {283–292},
numpages = {10},
keywords = {human activity detection, depth image segmentation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502119,
author = {Ma, Zhigang and Yang, Yi and Xu, Zhongwen and Sebe, Nicu and Hauptmann, Alexander G.},
title = {We Are Not Equally Negative: Fine-Grained Labeling for Multimedia Event Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502119},
doi = {10.1145/2502081.2502119},
abstract = {Multimedia event detection (MED) is an effective technique for video indexing and retrieval. Current classifier training for MED treats the negative videos equally. However, many negative videos may resemble the positive videos in different degrees. Intuitively, we may capture more informative cues from the negative videos if we assign them fine-grained labels, thus benefiting the classifier learning. Aiming for this, we use a statistical method on both the positive and negative examples to get the decisive attributes of a specific event. Based on these decisive attributes, we assign the fine-grained labels to negative examples to treat them differently for more effective exploitation. The resulting fine-grained labels may be not accurate enough to characterize the negative videos. Hence, we propose to jointly optimize the fine-grained labels with the knowledge from the visual features and the attributes representations, which brings mutual reciprocality. Our model obtains two kinds of classifiers, one from the attributes and one from the features, which incorporate the informative cues from the fine-grained labels. The outputs of both classifiers on the testing videos are fused for detection. Extensive experiments on the challenging TRECVID MED 2012 development set have validated the efficacy of our proposed approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {293–302},
numpages = {10},
keywords = {attribute representation, multimedia event detection (MED), attribute selection, fine-grained labeling, multi-source attributes},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245292,
author = {Smeaton, Alan},
title = {Session Details: Streaming and Synchronization},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245292},
doi = {10.1145/3245292},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502090,
author = {Zhao, Zhen Wei and Ooi, Wei Tsang},
title = {Joserlin: Joint Request and Service Scheduling for Peer-to-Peer Non-Linear Media Access},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502090},
doi = {10.1145/2502081.2502090},
abstract = {A peer-to-peer non-linear media streaming system needs to schedule both on-demand and prefetch requests carefully so as to reduce the server load and ensure good user experience. In this work, we propose, Joserlin, a joint request and service scheduling solution that not only alleviates request contentions (requests compete for limited service capacity), but also schedules the prefetch requests by considering their contributions to potential reduction of server load. In particular, we propose a novel request binning algorithm to prevent self-contention among on-demand requests issued from the same peer. A service and rejection policy is devised to resolve contention among on-demand requests issued from different neighbors. More importantly, Joserlin employs a gain function to prioritize prefetch requests at both requesters and responders, and a prefetch request issuing algorithm to fully utilize available upload bandwidth. Evaluation with traces collected from a popular networked virtual environment shows that Joserlin leads to 20%~60% reduction in server load.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {303–312},
numpages = {10},
keywords = {non-linear access pattern, service scheduling, scheduling, prefetch request, peer-to-peer, on-demand request},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502122,
author = {Ryu, Moonkyung and Ramachandran, Umakishore},
title = {FlashStream: A Multi-Tiered Storage Architecture for Adaptive HTTP Streaming},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502122},
doi = {10.1145/2502081.2502122},
abstract = {Video streaming on the Internet is popular and the need to store and stream video content using CDNs is continually on the rise thanks to services such as Hulu and Netflix. Adaptive HTTP streaming using the deployed CDN infrastructure has become the de facto standard for meeting the increasing demand for video streaming on the Internet. The storage architecture that is used for storing and streaming the video content is the focus of this study. Hard-disk as the storage medium has been the norm for enterprise-class storage servers for the longest time. More recently, multi-tiered storage servers (incorporating SSDs) such as Sun's ZFS and Facebook's flashcache offer an alternative to disk-based storage servers for enterprise applications. Both these systems use the SSD as a cache between the DRAM and the hard disk. The thesis of our work is that the current-state-of-the art in multi-tiered storage systems, architected for general-purpose enterprise workloads, do not cater to the unique needs of adaptive HTTP streaming. We present FlashStream, a multi-tiered storage architecture that addresses the unique needs of adaptive HTTP streaming. Like ZFS and flashcache, it also incorporates SSDs as a cache between the DRAM and the hard disk. The key architectural elements of FlashStream include optimal write granularity to overcome the write amplification effect of flash memory SSDs and a QoS-sensitive caching strategy that monitors the activity of the flash memory SSDs to ensure that video streaming performance is not hampered by the caching activity. We have implemented FlashStream and experimentally compared it with ZFS and flashcache for adaptive HTTP streaming workloads. We show that FlashStream outperforms both these systems for the same hardware configuration. Specifically, it is better by a factor of two compared to its nearest competitor, namely ZFS. In addition, we have compared FlashStream with a traditional two-level storage architecture (DRAM + HDDs), and have shown that, for the same investment cost, FlashStream provides 33% better performance and 94% better energy efficiency.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {313–322},
numpages = {10},
keywords = {adaptive http streaming, flash memory, video-on-demand, storage, solid-state drive},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502114,
author = {Montagud, Mario and Boronat, Fernando and Stokking, Hans},
title = {Early Event-Driven (EED) RTCP Feedback for Rapid IDMS},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502114},
doi = {10.1145/2502081.2502114},
abstract = {Inter-Destination Media Synchronization (IDMS) is essential in the emerging media consumption paradigm, which is radically evolving from passive and isolated services towards dynamic and interactive group shared experiences. This paper concentrates on improving a standardized RTP/RTCP-based solution for IDMS. In particular, novel Early Event-Driven (EED) RTCP feedback reporting mechanisms are designed to overcome latency issues and to enable higher flexibility, dynamism and accuracy when using RTP/RTCP for IDMS. The faster reaction on dynamic situations (e.g., detection of asynchrony or channel change delays) and a finer granularity for synchronizing media-related events, while preserving the RTCP bandwidth bounds, are validated through simulation tests.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {323–332},
numpages = {10},
keywords = {RTP/RTCP, synchronization, IDMS, event-driven, simulation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502118,
author = {Ursu, Marian F. and Groen, Martin and Falelakis, Manolis and Frantzis, Michael and Zsombori, Vilmos and Kaiser, Rene},
title = {Orchestration: Tv-like Mixing Grammars Applied to Video-Communication for Social Groups},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502118},
doi = {10.1145/2502081.2502118},
abstract = {This paper reports research into video-mediated synchronous communication within social groups. The ultimate aim of the research is to create a more natural medium for interaction, aware of the context in which it operates, able to continuously adapt itself to the communication needs and optimise the way in which it captures and transmits aspects of the communication. This, is hypothesised, can be achieved by equipping each of the various locations involved in the communication with multiple controllable video cameras and microphones, and mixing the resulting content through techniques similar to those used in television?a process referred to as "orchestration". Through orchestration, each location should be able to receive the appropriate perspectives and levels of detail, thus generating experiences in which the spatial separation between participants is minimised. The paper defines the concept of orchestration and presents two major evaluation experiments that provide supporting evidence for the main assumption and motivate further research, in richer interaction contexts, into this concept.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {333–342},
numpages = {10},
keywords = {conferencing, orchestration, group, video, interaction, social, communication, mediated, telepresence},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245293,
author = {Boujemaa, Nozha},
title = {Session Details: Keynote Address},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245293},
doi = {10.1145/3245293},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2509857,
author = {Guibas, Leonidas J.},
title = {The Space between the Images},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2509857},
doi = {10.1145/2502081.2509857},
abstract = {Multimedia content has become a ubiquitous presence on all our computing devices, spanning the gamut from live content captured by device sensors such as smartphone cameras to immense databases of images, audio and video stored in the cloud. As we try to maximize the utility and value of all these petabytes of content, we often do so by analyzing each piece of data individually and foregoing a deeper analysis of the relationships between the media. Yet with more and more data, there will be more and more connections and correlations, because the data captured comes from the same or similar objects, or because of particular repetitions, symmetries or other relations and self-relations that the data sources satisfy. This is particularly true for media of a geometric character, such as GPS traces, images, videos, 3D scans, 3D models, etc.In this talk we focus on the "space between the images", that is on expressing the relationships between different mutlimedia data items. We aim to make such relationships explicit, tangible, first-class objects that themselves can be analyzed, stored, and queried -- irrespective of the media they originate from. We discuss mathematical and algorithmic issues on how to represent and compute relationships or mappings between media data sets at multiple levels of detail. We also show how to analyze and leverage networks of maps and relationships, small and large, between inter-related data. The network can act as a regularizer, allowing us to to benefit from the "wisdom of the collection" in performing operations on individual data sets or in map inference between them.We will illustrate these ideas using examples from the realm of 2D images and 3D scans/shapes -- but these notions are more generally applicable to the analysis of videos, graphs, acoustic data, biological data such as microarrays, homeworks in MOOCs, etc. This is an overview of joint work with multiple collaborators, as will be discussed in the talk.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {343–344},
numpages = {2},
keywords = {geometric data analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245294,
author = {Kompatsiaris, Yiannis and O'Hare, Neil},
title = {Session Details: Multimedia Grand Challenge},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245294},
doi = {10.1145/3245294},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508115,
author = {Che, Xiaoyin and Yang, Haojin and Meinel, Christoph},
title = {Lecture Video Segmentation by Automatically Analyzing the Synchronized Slides},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508115},
doi = {10.1145/2502081.2508115},
abstract = {In this paper we propose a solution which segments lecture video by analyzing its supplementary synchronized slides. The slides content derives automatically from OCR (Optical Character Recognition) process with an approximate accuracy of 90%. Then we partition the slides into different subtopics by examining their logical relevance. Since the slides are synchronized with the video stream, the subtopics of the slides indicate exactly the segments of the video. Our evaluation reveals that the average length of segments for each lecture is ranged from 5 to 15 minutes, and 45% segments achieved from test datasets are logically reasonable.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {345–348},
numpages = {4},
keywords = {slides content analysis, video segmentation, OCR},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508116,
author = {Chen, Shannon and Xia, Pengye and Nahrstedt, Klara},
title = {Activity-Aware Adaptive Compression: A Morphing-Based Frame Synthesis Application in 3DTI},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508116},
doi = {10.1145/2502081.2508116},
abstract = {In view of the different demands on quality of service of different user activities in the 3D Tele-immersive (3DTI) environment, we combine activity recognition and real-time morphing-based compression and present the Activity-Aware Adaptive Compression. We implement this scheme on our 3DTI platform: the TEEVE Endpoint, which is a runtime engine to handle the creation, transmission and rendering of 3DTI data. User study as well as objective evaluation of the scheme show that it can achieve 25% more bandwidth saving compared to conventional 3D data compression as zlib without perceptible degradation in the user experience.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {349–352},
numpages = {4},
keywords = {adaptation, frame rate, compression, morphing, 3D tele-immersion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508117,
author = {Su, Yu-Chuan and Chiu, Tzu-Hsuan and Wu, Guan-Long and Yeh, Chun-Yen and Wu, Felix and Hsu, Winston},
title = {Flickr-Tag Prediction Using Multi-Modal Fusion and Meta Information},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508117},
doi = {10.1145/2502081.2508117},
abstract = {We present our evaluation and analysis on Yahoo! Large-scale Flickr-tag Image Classification dataset. Our evaluations show that combining multi-features and different classification models, the MAP of tag prediction can be significantly improve over ordinary linear classification. Further analysis shows that some tags are given not because of the visual content but the meta information of images. Our experiments show that we can make more accurate prediction on certain tags using meta information without any training process, compared with visual content based classifiers. Combine the meta information, multi-features and multi-models fusion, we achieve significantly better performance than simple linear classification. We also evaluate the performance of various mid-level feature, and the results suggest that "Concept Bank" feature may be a promising direction for the task.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {353–356},
numpages = {4},
keywords = {multi-features fusion, meta data, concept bank},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508118,
author = {Jou, Brendan and Li, Hongzhi and Ellis, Joseph G. and Morozoff-Abegauz, Daniel and Chang, Shih-Fu},
title = {Structured Exploration of Who, What, When, and Where in Heterogeneous Multimedia News Sources},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508118},
doi = {10.1145/2502081.2508118},
abstract = {We present a fully automatic system from raw data gathering to navigation over heterogeneous news sources, including over 18k hours of broadcast video news, 3.58M online articles, and 430M public Twitter messages. Our system addresses the challenge of extracting "who," "what," "when," and "where" from a truly multimodal perspective, leveraging audiovisual information in broadcast news and those embedded in articles, as well as textual cues in both closed captions and raw document content in articles and social media. Performed over time, we are able to extract and study the trend of topics in the news and detect interesting peaks in news coverage over the life of the topic. We visualize these peaks in trending news topics using automatically extracted keywords and iconic images, and introduce a novel multimodal algorithm for naming speakers in the news. We also present several intuitive navigation interfaces for interacting with these complex topic structures over different news sources.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {357–360},
numpages = {4},
keywords = {speaker diarization, topic linking, multimedia analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508119,
author = {Bhattacharya, Subhabrata and Nojavanasghari, Behnaz and Chen, Tao and Liu, Dong and Chang, Shih-Fu and Shah, Mubarak},
title = {Towards a Comprehensive Computational Model Foraesthetic Assessment of Videos},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508119},
doi = {10.1145/2502081.2508119},
abstract = {In this paper we propose a novel aesthetic model emphasizing psycho-visual statistics extracted from multiple levels in contrast to earlier approaches that rely only on descriptors suited for image recognition or based on photographic principles. At the lowest level, we determine dark-channel, sharpness and eye-sensitivity statistics over rectangular cells within a frame. At the next level, we extract Sentibank features (1,200 pre-trained visual classifiers) on a given frame, that invoke specific sentiments such as "colorful clouds", "smiling face" etc. and collect the classifier responses as frame-level statistics. At the topmost level, we extract trajectories from video shots. Using viewer's fixation priors, the trajectories are labeled as foreground, and background/camera on which statistics are computed. Additionally, spatio-temporal local binary patterns are computed that capture texture variations in a given shot. Classifiers are trained on individual feature representations independently. On thorough evaluation of 9 different types of features, we select the best features from each level -- dark channel, affect and camera motion statistics. Next, corresponding classifier scores are integrated in a sophisticated low-rank fusion framework to improve the final prediction scores. Our approach demonstrates strong correlation with human prediction on 1,000 broadcast quality videos released by NHK as an aesthetic evaluation dataset.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {361–364},
numpages = {4},
keywords = {affect features, cinematography, video aesthetics, low rank late fusion, camera motion features},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508120,
author = {Bhatt, Chidansh Amitkumar and Popescu-Belis, Andrei and Habibi, Maryam and Ingram, Sandy and Masneri, Stefano and McInnes, Fergus and Pappas, Nikolaos and Schreer, Oliver},
title = {Multi-Factor Segmentation for Topic Visualization and Recommendation: The MUST-VIS System},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508120},
doi = {10.1145/2502081.2508120},
abstract = {This paper presents the MUST-VIS system for the MediaMixer/VideoLectures.NET Temporal Segmentation and Annotation Grand Challenge. The system allows users to visualize a lecture as a series of segments represented by keyword clouds, with relations to other similar lectures and segments. Segmentation is performed using a multi-factor algorithm which takes advantage of the audio (through automatic speech recognition and word-based segmentation) and video (through the detection of actions such as writing on the blackboard). The similarity across segments and lectures is computed using a content-based recommendation algorithm. Overall, the graph-based representation of segment similarity appears to be a promising and cost-effective approach to navigating lecture databases.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {365–368},
numpages = {4},
keywords = {lecture recommendation, lecture segmentation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508121,
author = {Wang, Yanran and Dai, Qi and Feng, Rui and Jiang, Yu-Gang},
title = {Beauty is Here: Evaluating Aesthetics in Videos Using Multimodal Features and Free Training Data},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508121},
doi = {10.1145/2502081.2508121},
abstract = {The aesthetics of videos can be used as a useful clue to improve user satisfaction in many applications such as search and recommendation. In this paper, we demonstrate a computational approach to automatically evaluate the aesthetics of videos, with particular emphasis on identifying beautiful scenes. Using a standard classification pipeline, we analyze the effectiveness of a comprehensive set of features, ranging from low-level visual features, mid-level semantic attributes, to style descriptors. In addition, since there is limited public training data with manual labels of video aesthetics, we explore freely available resources with a simple assumption that people tend to share more aesthetically appealing works than unappealing ones. Specifically, we use images from DPChallenge and videos from Flickr as positive training data and the Dutch documentary videos as negative data, where the latter contain mostly old materials of low visual quality. Our extensive evaluations show that combining multiple features is helpful, and very promising results can be obtained using the noisy but annotation-free training data. On the NHK Multimedia Challenge dataset, we attain a Spearman's rank correlation coefficient of 0.41.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {369–372},
numpages = {4},
keywords = {attributes, beautiful scenes, video aesthetics, free training data, multimodal features},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508122,
author = {Wan, Kong-Wah and Yau, Wei-Yun and Roy, Sujoy},
title = {Metadata Enrichment for News Video Retrieval: A Graph-Based Propagation Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508122},
doi = {10.1145/2502081.2508122},
abstract = {This paper summarizes our contribution to the Technicolor Rich Multimedia Retrieval from Input Videos Grand Challenge. We hold the view that semantic analysis of a given news video is best performed in the text domain. Starting with a noisy text obtained from applying Automatic Speech Recognition (ASR), a graph-based approach is then used to enrich the text by propagating labels from visually similar videos culled from parallel (YouTube) News sources. From the enriched text, we next extract salient keywords to form a query to a news video search engine, retrieving a larger corpus of related news video. Compared to a baseline method that only uses the ASR text, significant improvement in precision has been obtained, indicating that retrieval has benefited from the ingestion of the external labels. Capitalizing on the enriched metadata, we find that videos are more amenable to the Wikipedia-based Explicit Semantic Analysis (ESA), resulting in better support for subtopic news video retrieval. We apply our methods to an in-house live news search portal, and report on several best practices.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {373–376},
numpages = {4},
keywords = {multimedia search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508123,
author = {Hao, Zongbo and Zhang, Qianni and Ezquierdo, Ebroul and Sang, Nan},
title = {Human Action Recognition by Fast Dense Trajectories},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508123},
doi = {10.1145/2502081.2508123},
abstract = {In this paper, we propose the fast dense trajectories algorithm for human action recognition. Dense trajectories are robust to fast irregular motions and outperform other state-of-the-art descriptors such as KLT tracker or SIFT descriptors. However, the use of dense trajectories is time consuming. To improve the efficiency, we extract feature trajectories in the ROI rather than in the whole frames, and we use the temporal pyramids to achieve adaptable mechanism for different action speed. We evaluate the method on the dataset of Huawei/3DLife -- 3D human reconstruction and action recognition Grand Challenge in ACM Multimedia 2013. Experimental results show a significant improvement over the dense trajectories descriptor in real-time, and adaptable to different speed.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {377–380},
numpages = {4},
keywords = {adaptable to different speed, real-time, fast dense trajectories, human action recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508124,
author = {Mantziou, Eleni and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
title = {Scalable Training with Approximate Incremental Laplacian Eigenmaps and PCA},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508124},
doi = {10.1145/2502081.2508124},
abstract = {The paper describes the approach, the experimental settings, and the results obtained by the proposed methodology at the ACM Yahoo! Multimedia Grand Challenge. Its main contribution is the use of fast and efficient features with a highly scalable semi-supervised learning approach, the Approximate Laplacian Eigenmaps (ALEs), and its extension, by computing the test set incrementally for learning concepts in time linear to the number of images (both labelled and unlabelled). A combination of two local visual features combined with the VLAD feature aggregation method and PCA is used to improve the efficiency and time complexity. Our methodology achieves somewhat better accuracy compared to the baseline (linear SVM) in small training sets, but improves the performance as the training data increase. Performing ALE fusion on a training set of 50K/concept resulted in a MiAP score of 0.4223, which was among the highest scores of the proposed approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {381–384},
numpages = {4},
keywords = {algorithms, performance, measurement, experimentation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508125,
author = {Yildirim, G\"{o}khan and Shaji, Appu and S\"{u}sstrunk, Sabine},
title = {Estimating Beauty Ratings of Videos Using Supervoxels},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508125},
doi = {10.1145/2502081.2508125},
abstract = {The major low-level perceptual components that influence the beauty ratings of video are color, contrast, and motion. To estimate the beauty ratings of the NHK dataset, we propose to extract these features based on supervoxels, which are a group of pixels that share similar color and spatial information through the temporal domain. Recent beauty methods use frame-level processing for visual features and disregard the spatio-temporal aspect of beauty. In this paper, we explicitly model this property by introducing supervoxel-based visual and motion features. In order to create a beauty estimator, we first identify 60 videos (either beautiful or not beautiful) in the NHK dataset. We then train a neural network regressor using the supervoxel-based features and binary beauty ratings. We rate the 1000 videos in the NHK dataset and rank them according to their ratings. When comparing our rankings with the actual rankings of the NHK dataset, we obtain a Spearman correlation coefficient of 0.42.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {385–388},
numpages = {4},
keywords = {video beauty, supervoxel, video ranking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508126,
author = {Sun, Litian and Aizawa, Kiyoharu},
title = {Action Recognition Using Invariant Features under Unexampled Viewing Conditions},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508126},
doi = {10.1145/2502081.2508126},
abstract = {A great challenge in real-world applications of action recognition is the lack of sufficient label information because of variance in the recording viewpoint and differences between individuals. A system that can adapt itself according to these variances is required for practical use. We present a generic method for extracting view-invariant features from skeleton joints. These view-invariant features are further refined using a stacked, compact autoencoder. To model the challenge of real-world applications, two unexampled test settings (NewView and NewPerson) are used to evaluate the proposed method. Experimental results with these test settings demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {389–392},
numpages = {4},
keywords = {3Dlife, autoencoder, unexampled test setting, action recognition, grand challenge, view-invariant feature},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508127,
author = {Wu, Chun-Che and Chu, Kuan-Yu and Kuo, Yin-Hsi and Chen, Yan-Ying and Lee, Wen-Yu and Hsu, Winston H.},
title = {Search-Based Relevance Association with Auxiliary Contextual Cues},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508127},
doi = {10.1145/2502081.2508127},
abstract = {In this work, we target at solving the Bing challenge provided by Microsoft. The task is to design an effective and efficient measurement of query terms in describing the images (image-query pairs) crawled from the web. We observe that the provided image-query pairs (e.g., text-based image retrieval results) are usually related to their surrounding text; however, the relationship between image content seems to be ignored. Hence, we attempt to integrate the visual information for better ranking results. In addition, we found that plenty of query terms are related to people (e.g., celebrity) and user might have similar queries (click logs) in the search engine. Therefore, in this work, we propose a relevance association by investigating the effectiveness of different auxiliary contextual cues (i.e., face, click logs, visual similarity). Experimental results show that the proposed method can have 16% relative improvement compared to the original ranking results. Especially, for people-related queries, we can further have 45.7% relative improvement.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {393–396},
numpages = {4},
keywords = {name detection, identity bank, image retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508128,
author = {Pan, Yingwei and Yao, Ting and Yang, Kuiyuan and Li, Houqiang and Ngo, Chong-Wah and Wang, Jingdong and Mei, Tao},
title = {Image Search by Graph-Based Label Propagation with Image Representation from DNN},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508128},
doi = {10.1145/2502081.2508128},
abstract = {Our objective is to estimate the relevance of an image to a query for image search purposes. We address two limitations of the existing image search engines in this paper. First, there is no straightforward way of bridging the gap between semantic textual queries as well as users' search intents and image visual content. Image search engines therefore primarily rely on static and textual features. Visual features are mainly used to identify potentially useful recurrent patterns or relevant training examples for complementing search by image reranking. Second, image rankers are trained on query-image pairs labeled by human experts, making the annotation intellectually expensive and time-consuming. Furthermore, the labels may be subjective when the queries are ambiguous, resulting in difficulty in predicting the search intention. We demonstrate that the aforementioned two problems can be mitigated by exploring the use of click-through data, which can be viewed as the footprints of user searching behavior, as an effective means of understanding query. The correspondences between an image and a query are determined by whether the image was searched and clicked by users under the query in a commercial image search engine. We therefore hypothesize that the image click counts in response to a query are as their relevance indications. For each new image, our proposed graph-based label propagation algorithm employs neighborhood graph search to find the nearest neighbors on an image similarity graph built up with visual representations from deep neural networks and further aggregates their clicked queries/click counts to get the labels of the new image. We conduct experiments on MSR-Bing Grand Challenge and the results show consistent performance gain over various baselines. In addition, the proposed approach is very efficient, completing annotation of each query-image pair within just 15 milliseconds on a regular PC.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {397–400},
numpages = {4},
keywords = {click-through data, deep neural networks, image search, neighborhood graph search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245295,
author = {Anguera, Xavier and Yang, Yi},
title = {Session Details: Demos},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245295},
doi = {10.1145/3245295},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502240,
author = {Lu, Chia-Ju and Hsu, Chih-Fan and Yeh, Mei-Chen},
title = {Real-Time Salient Object Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502240},
doi = {10.1145/2502081.2502240},
abstract = {Salient object detection techniques have a variety of multimedia applications of broad interest. However, the detection must be fast to truly aid in these processes. There exist many robust algorithms tackling the salient object detection problem but most of them are computationally demanding. In this demonstration we show a fast salient object detection system implemented in a conventional PC environment. We examine the challenges faced in the design and development of a practical system that can achieve accurate detection in real-time.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {401–402},
numpages = {2},
keywords = {object detection, saliency},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502241,
author = {Korpi, Kiia and Aizawa, Kiyoharu},
title = {Kanji Snap: An OCR-Based Smartphone Application for Learning Japanese Kanji Characters},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502241},
doi = {10.1145/2502081.2502241},
abstract = {As optical character recognition techniques improve, new opportunities to improve existing systems open up. There are for example OCR reading aids for the visually impaired and applications for translating foreign text by capturing photos with smartphones. But one field that hasn't made use of the advancing technology is the learning environment. This demo concentrates on incorporating optical character recognition into a smartphone application for learning Japanese kanji characters. With the application users can take photos of kanji in their everyday environment and look up detailed information and translations easily. Users can practice those kanji with vocabulary lists and quizzes and track their study progress.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {403–404},
numpages = {2},
keywords = {Japanese, learning, OCR, mobile multimedia, kanji},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502242,
author = {Hudelist, Marco A. and Schoeffmann, Klaus and Boeszoermenyi, Laszlo},
title = {Mobile Video Browsing with the ThumbBrowser},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502242},
doi = {10.1145/2502081.2502242},
abstract = {In this work we propose an early prototype of a video browser for mobile devices with touchscreens. We concentrate on utilizing the thumbs because of the natural posture used with the devices when watching videos in landscape mode. The controls are only displayed when the user touches the screen and automatically rearrange themselves depending on the position of the thumbs. A combination of a radial menu and an extended seeker control with hierarchical browsing and bookmarking features enables the user to navigate quickly through videos.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {405–406},
numpages = {2},
keywords = {natural user interfaces, video browsing, mobile devices, touchscreens},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502243,
author = {Hsu, Che-Hao and Hua, Kai-Lung and Cheng, Wen-Huang},
title = {Physiognomy Master: A Novel Personality Analysis System Based on Facial Features},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502243},
doi = {10.1145/2502081.2502243},
abstract = {In this demo, we present the proposed "Physiognomy Master." It is a novel practical personality analysis system based on facial features. We first design five facial features that are essential for face reading. We then construct a database to record the facial features' values from a number of volunteers. In the meantime, the volunteers are also invited to fill out a professional personality test. The relations between the facial features and the personality traits are then learned. Given a test subject or an input frontal face image, the proposed system will produce the associated personality report by fusing the personality scores from the people who have similar facial features in the constructed database. The fusing mechanism is based on the idea that people with similar facial features possess similar personality characteristics. The proposed system is a powerful tool in numerous kinds of social interactions, such as personnel selection, team composition, and marriage matching.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {407–408},
numpages = {2},
keywords = {personality analysis, physiognomy, face reading, facial features},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502244,
author = {Liu, Wu and Yang, Feibin and Zhang, Yongdong and Huang, Qinghua and Mei, Tao},
title = {LAVES: An Instant Mobile Video Search System Based on Layered Audio-Video Indexing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502244},
doi = {10.1145/2502081.2502244},
abstract = {This demonstration presents an innovative instant mobile video search system based on layered audio-video indexing, called "LAVES." Through the system, users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. Unlike most existing mobile video search applications which simply send the original video query to the cloud, the proposed mobile system is one of the first attempts towards instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is able to index large-scale video data using the layered audio-video indexing technique on the cloud, as well as extract light-weight joint audio-video signatures in real time and perform bipartite-graph-based progressive search process on the devices. On a 600 hours video dataset, the system can outperform the state-of-the-arts by achieving 90.79% precision when the query video is less than 10 seconds.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {409–410},
numpages = {2},
keywords = {instant search, layered audio-video indexing, audio-video signatures, progressive query process, mobile video search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502245,
author = {Font, Frederic and Roma, Gerard and Serra, Xavier},
title = {Freesound Technical Demo},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502245},
doi = {10.1145/2502081.2502245},
abstract = {Freesound is an online collaborative sound database where people with diverse interests share recorded sound samples under Creative Commons licenses. It was started in 2005 and it is being maintained to support diverse research projects and as a service to the overall research and artistic community. In this demo we want to introduce Freesound to the multimedia community and show its potential as a research resource. We begin by describing some general aspects of Freesound, its architecture and functionalities, and then explain potential usages that this framework has for research applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {411–412},
numpages = {2},
keywords = {online databases, freesound, audio clips, sound},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502246,
author = {You, Yu and Mattila, Ville-Veikko},
title = {Visualizing Web Mash-Ups for in-Situ Vision-Based Mobile AR Applications},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502246},
doi = {10.1145/2502081.2502246},
abstract = {Augmented reality applications are gaining popularity due to increased capabilities of modern mobile devices. Creating AR content however is tedious and traditionally done on desktop environments by professionals, with extensive knowledge and/or even programming skills required. In this demo, we demonstrate a complete mobile approach for creating vision-based AR in both indoor and outdoor environment. Using hyperlinks, Web mashups are built to dynamically augment the physical world by normal users without programing skills.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {413–414},
numpages = {2},
keywords = {mash-ups, augmented reality, authoring, www},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502247,
author = {Mayor, Oscar and Llimona, Quim and Marchini, Marco and Papiotis, Panos and Maestre, Esteban},
title = {RepoVizz: A Framework for Remote Storage, Browsing, Annotation, and Exchange of Multi-Modal Data},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502247},
doi = {10.1145/2502081.2502247},
abstract = {In this technical demo we present repoVizz (http://repovizz.upf.edu), an integrated online system capable of structural formatting and remote storage, browsing, exchange, annotation, and visualization of synchronous multi-modal, time-aligned data. Motivated by a growing need for data-driven collaborative research, repoVizz aims to resolve commonly encountered difficulties in sharing or browsing large collections of multi-modal data. At its current state, repoVizz is designed to hold time-aligned streams of heterogeneous data: audio, video, motion capture, physiological signals, extracted descriptors, annotations, et cetera. Most popular formats for audio and video are supported, while Broadcast WAVE or CSV formats are adopted for streams other than audio or video (e.g., motion capture or physiological signals). The data itself is structured via customized XML files, allowing the user to (re-) organize multi-modal data in any hierarchical manner, as the XML structure only holds metadata and pointers to data files. Datasets are stored in an online database, allowing the user to interact with the data remotely through a powerful HTML5 visual interface accessible from any standard web browser; this feature can be considered a key aspect of repoVizz since data can be explored, annotated, or visualized from any location or device. Data exchange and upload/download is made easy and secure via a number of data conversion tools and a user/permission management system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {415–416},
numpages = {2},
keywords = {multimodal},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502248,
author = {Letessier, Pierre and Herv\'{e}, Nicolas and Champ, Julien and Joly, Alexis and Olivier, Buisson and Hamzaoui, Amel},
title = {Small Objects Query Suggestion in a Large Web-Image Collection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502248},
doi = {10.1145/2502081.2502248},
abstract = {State-of-the-art visual search methods allow retrieving efficiently small rigid objects in very large image datasets (e.g. logos, paintings, etc.). User's perception of the classical query-by-window paradigm is however affected by the fact that many submitted queries actually return nothing or only junk results. We demonstrate in this demo that the perception can be radically different if the objects of interest are rather suggested to the user by pre-computing relevant clusters of instances. Impressive results involving very small objects discovered in a web collection of 110K images are demonstrated through a simple interactive GUI.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {417–418},
numpages = {2},
keywords = {small objects, mining, hashing, scalable},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502249,
author = {Habibian, Amirhossein and Snoek, Cees G.M.},
title = {Video2Sentence and Vice Versa},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502249},
doi = {10.1145/2502081.2502249},
abstract = {In this technical demonstration, we showcase a multimedia search engine that retrieves a video from a sentence, or a sentence from a video. The key novelty is our machine translation capability that exploits a cross-media representation for both the visual and textual modality using concept vocabularies. We will demonstrate the translations using arbitrary web videos and sentences related to everyday events. What is more, we will provide an automatically generated explanation, in terms of concept detectors, on why a particular video or sentence has been retrieved as the most likely translation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {419–420},
numpages = {2},
keywords = {concept vocabulary, event detection, multimedia translation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502250,
author = {Korinke, Christoph and Rabbath, Mohamad and Lamken, Dennis and Boll, Susanne},
title = {A Tool for Catching Back Your Preferred Videos from Physical Collages},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502250},
doi = {10.1145/2502081.2502250},
abstract = {Images and videos are easy to create by inexperienced users with intuitive devices like smartphones. We propose a system which enables users to extract panoramas from videos for the creation of collages which can be printed. Single frames used for panoramas are indexed on the basis of SURF features. As a result users can make a photo with the smartphone from a panorama within the printed collage or zoom into the digital version of a collage to select an area of interest and retrieve the related video using the SURF features index. Moreover users can pick a photo from a location where a video was previously recorded and retrieve a related video as well. Our approach supports the maintenance of a large number of videos.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {421–422},
numpages = {2},
keywords = {photo collage, descriptor matching, video analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502251,
author = {Go\"{e}au, Herv\'{e} and Bonnet, Pierre and Joly, Alexis and Baki\'{c}, Vera and Barbe, Julien and Yahiaoui, Itheri and Selmi, Souheil and Carr\'{e}, Jennifer and Barth\'{e}l\'{e}my, Daniel and Boujemaa, Nozha and Molino, Jean-Fran\c{c}ois and Duch\'{e}, Gr\'{e}goire and P\'{e}ronnet, Aur\'{e}lien},
title = {Pl@ntNet Mobile App},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502251},
doi = {10.1145/2502081.2502251},
abstract = {Pl@ntNet is an image sharing and retrieval application for the identification of plants, available on iPhone and iPad devices. Contrary to previous content-based identification applications it can work with several parts of the plant including flowers, leaves, fruits and bark. It also allows integrating user's observations in the database thanks to a collaborative workflow involving the members of a social network specialized on plants. Data collected so far makes it one of the largest mobile plant identification tool.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {423–424},
numpages = {2},
keywords = {iPhone, social network, ecology, crowdsourcing, collaborative, monitoring, identification, surveillance, computer vision, flower, plant, multi-organ, botanist, mobile, images, visual, retrieval, fruit, citizen science, bark, multimedia, leaf},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502252,
author = {Guthier, Benjamin and Ho, Kalun and Kopf, Stephan and Effelsberg, Wolfgang},
title = {Determining Exposure Values from HDR Histograms for Smartphone Photography},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502252},
doi = {10.1145/2502081.2502252},
abstract = {We present a novel system to assist users in choosing suitable exposure values for photography on smartphones. The user specifies the desired shape of the histogram of the captured image by adjusting the parameters of a function. The system then uses the smartphone's camera and varying exposure values to calculate a high dynamic range (HDR) histogram of the scene. It contains the entire brightness range from the darkest to the brightest area of the scene. For any exposure value, the HDR histogram can be used to synthesize a conventional histogram of a low dynamic range (LDR) image as if captured by this exposure value. This process is much faster than capturing the image itself. By comparing a number of synthesized LDR histograms to the user's desired histogram shape, the system determines an exposure value that best meets the user's preference.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {425–426},
numpages = {2},
keywords = {photography, HDR, smartphones},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502253,
author = {Law-To, Julien and Grefenstette, Gregory and Landais, R\'{e}mi},
title = {Semantic Dispatching of Multimedia News with MEWS},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502253},
doi = {10.1145/2502081.2502253},
abstract = {Online news comes in rich multimedia form: video, audio, photos, in addition to traditional text. Recent advances in semantically-rich text processing, in speech-to-text processing, and in image processing allows us to develop new ways of presenting and enriching news stories. Here we present MEWS, a Multimedia nEWS platform, which enriches news browsing according to media (text, images, and video) and to automatically detected type of news (music, general news, politics).},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {427–428},
numpages = {2},
keywords = {search engine, speech processing, image processing, semantic annotation, speech-to-text, news, multimedia indexing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502254,
author = {Wu, Peng and Vernica, Rares and Lin, Qian},
title = {Cloud Based Multimedia Analytic Platform},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502254},
doi = {10.1145/2502081.2502254},
abstract = {Multimedia Analytic Platform is a cloud based service to expose state-of-the-art multimedia technologies for mobile and web application development. As a product-quality service platform, it offers comprehensive API documentation, code example, service description and sandbox for trial for each multimedia technology. The utilization of the cloud storage and distributed computing framework allows the service platform to run with robustness and efficiency. The current technologies supported by the platform include face detection, face verification, face demographic estimation, feature extraction, image matching, and image collage. Since its initial public launch in October 2012, it has been adopted by universities and third party companies for course support and application development.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {429–430},
numpages = {2},
keywords = {analytic, platform, multimedia, cloud},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502255,
author = {Hardy, Sandro and G\"{o}bel, Stefan and Steinmetz, Ralf},
title = {Adaptable and Personalized Game-Based Training System for Fall Prevention},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502255},
doi = {10.1145/2502081.2502255},
abstract = {Digital Games which incorporate movements of the player`s body in their gameplay are becoming more and more popular. An increasing number of doctors and physical therapists use such games for training exercises, although these games are not designed to achieve predefined training goals. Various studies show that the training effects of these games are small in comparison with classic exercises. Therapists request more accessible and more flexible games. In this paper we present an adaptive game for fall prevention based on the adaptation and exergame analysis framework StoryTecRT which allows the adaptation of parameters which impact accessibility, acceptance and training load of a game. This paper includes an insight into the framework and the implementation as well as first evaluation results.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {431–432},
numpages = {2},
keywords = {sensors, exergames, health, adaptation, serious games, training, personalization},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502256,
author = {Dong, Chao and Chen, Shifeng and Tang, Xiaoou},
title = {AdVisual: A Visual-Based Advertising System},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502256},
doi = {10.1145/2502081.2502256},
abstract = {In this work, we present a visual-based contextual advertising system, AdVisual. It is designed for content service providers to effectively select relevant ads for online videos. First, it will analyze each video and extract high level semantic visual information including specific objects, people, and significant scenes. Then ads highly related to the visual concepts are associated with the corresponding shots. As AdVisual is a user interaction system, it allows users to select favorite ads relevant to the video. By saliency detection, selected ads will be displayed as an overlay window embedded at the non-intrusive part of the shot.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {433–434},
numpages = {2},
keywords = {visual-based contextual advertising, coarse-to-fine approach},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502257,
author = {Jin, Yichao and Xie, Tian and Wen, Yonggang and Xie, Haiyong},
title = {Multi-Screen Cloud Social TV: Transforming TV Experience into 21st Century},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502257},
doi = {10.1145/2502081.2502257},
abstract = {Nowadays, TV experience has been transformed from the traditional "laid-back" video watching experience to a "lean-forward" social and multi-screen experience. In this demo, we design and develop a multi-screen cloud social TV system in response to this trend. Our system is built upon two enabling technologies, including a cloud based back-end infrastructure and a multi-screen front-end application. We demonstrate two key features of our system based on real user scenarios, including a living-room video watching experience with remote viewers, and the video teleportation as an enhanced multi-screen experience.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {435–436},
numpages = {2},
keywords = {video teleportation, social TV, multi-screen, cloud},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502258,
author = {Liu, Luoqi and Xu, Hui and Liu, Si and Xing, Junliang and Zhou, Xi and Yan, Shuicheng},
title = {"Wow! You Are so Beautiful Today!"},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502258},
doi = {10.1145/2502081.2502258},
abstract = {In this demo, we present Beauty e-Experts, a fully automatic system for hairstyle and facial makeup recommendation and synthesis. Given a user-provided frontal facial image with short/bound hair and no/light makeup, the Beauty e-Experts system can not only recommend the most suitable hairstyle and makeup, but also show the synthesis effects. Two problems are considered for the Beauty e-Experts system: what to recommend and how to wear, which describe a similar process of selecting and applying hairstyle and cosmetics in our daily life. For the what-to-recommend problem, we propose a multiple tree-structured super-graphs model to explore the complex relationships among the beauty attributes, beauty-related attributes and image features, and then based on this model, the most suitable beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear problem, a facial image synthesis module is designed to seamlessly blend the recommended hairstyle and makeup into the user facial image. Extensive experimental evaluations and analysis on testing images well demonstrate the effectiveness of the proposed system.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {437–438},
numpages = {2},
keywords = {beauty synthesis, multiple tree-structured super-graphs model, beauty recommendation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502259,
author = {Wang, Guanfeng and Seo, Beomjoo and Yin, Yifang and Zimmermann, Roger and Shen, Zhijie},
title = {OSCOR: An Orientation Sensor Data Correction System for Mobile Generated Contents},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502259},
doi = {10.1145/2502081.2502259},
abstract = {In addition to positioning data, other sensor information -- such as orientation data, have become a useful and powerful contextual feature. Such auxiliary information can facilitate higher-level semantic description inferences in many multimedia applications, e.g., video tagging and video summarization. However, sensor data collected from current mobile devices is often not accurate enough for upstream multimedia analysis. An effective orientation data correction system for mobile multimedia content has been an elusive goal so far. Here we present a system, termed Oscor, which aims to improve the accuracy of noisy orientation sensor measurements generated by mobile devices during image and video recording. We provide a user-friendly camera interface to facilitate the gathering of additional information, which enables the correction process on the server-side. Geographic field-of-view (FOV) visualizations based on the original and corrected sensor data help users understand the corrected contextual information and how the erroneous data possibly may affect further processes.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {439–440},
numpages = {2},
keywords = {mobile multimedia, camera orientation, digital compass, data correction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502260,
author = {Herv\'{e}, Nicolas and Viaud, Marie-Luce and Thi\`{e}vre, J\'{e}r\^{o}me and Saulnier, Agn\`{e}s and Champ, Julien and Letessier, Pierre and Buisson, Olivier and Joly, Alexis},
title = {OTMedia: The French TransMedia News Observatory},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502260},
doi = {10.1145/2502081.2502260},
abstract = {Who said What, Where and How? How are images, video and stories spreading out? Who produces the information? OTMedia addresses these questions by collecting, enriching and analysing continuously more than 1500 streams of French media from TV Radio, Web, AFP, and Twitter. Two studies on media produced by end users with the OTMedia framework are presented.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {441–442},
numpages = {2},
keywords = {indexing retrieval, datamining, visual search, multimedia, natural language processing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502261,
author = {Xia, Pengye and Nahrstedt, Klara},
title = {TEEVE Endpoint: Towards the Ease of 3D Tele-Immersive Application Development},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502261},
doi = {10.1145/2502081.2502261},
abstract = {We present TEEVE Endpoint, which is a runtime engine to handle the creation, transmission and rendering of 3D Tele-immersive (3DTI) data and provides application programming interfaces (APIs) to developers to easily create 3DTI applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {443–444},
numpages = {2},
keywords = {3D tele-immersion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502285,
author = {Hu, Zhenzhen and Lin, Min and Liu, Si and Wang, Meng and Hong, Richang and Yan, Shuicheng},
title = {EHeritage of Shadow Puppetry: Creation and Manipulation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502285},
doi = {10.1145/2502081.2502285},
abstract = {In this demo, we propose the puppetry eHeritage, including a creator module and a manipulator module, to preserve the precious traditional heritage Chinese shadow puppetry. The creator module accepts a frontal face image and a profile face image of the user as input, and automatically generates the corresponding puppet, which looks like the original person and meanwhile preserves typical characteristics of traditional Chinese shadow puppetry. The manipulator module can accept the script provided by the user as input and automatically generate the motion sequences. For better visual effects, we propose the sparsity optimization over simplexes formulation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {445–446},
numpages = {2},
keywords = {face rendering, animation, chinese shadow puppetry},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502262,
author = {Fran\c{c}oise, Jules and Schnell, Norbert and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Gesture-Based Control of Physical Modeling Sound Synthesis: A Mapping-by-Demonstration Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502262},
doi = {10.1145/2502081.2502262},
abstract = {We address the issue of mapping between gesture and sound for gesture-based control of physical modeling sound synthesis. We propose an approach called mapping by demonstration, allowing users to design the mapping by performing gestures while listening to sound examples. The system is based on a multimodal model able to learn the relationships between gestures and sounds.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {447–448},
numpages = {2},
keywords = {sound synthesis, physical modeling, music, multimodal, gesture, HMM},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502263,
author = {Li, Hongzhi and Jou, Brendan and Ellis, Jospeh G. and Morozoff, Daniel and Chang, Shih-Fu},
title = {News Rover: Exploring Topical Structures and Serendipity in Heterogeneous Multimedia News},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502263},
doi = {10.1145/2502081.2502263},
abstract = {News stories are rarely understood in isolation. Every story is driven by key entities that give the story its context. Persons, places, times, and several surrounding topics can often succinctly represent a news event, but are only useful if they can be both identified and linked together. We introduce a novel architecture called News Rover for re-bundling broadcast video news, online articles, and Twitter content. The system utilizes these many multimodal sources to link and organize content by topics, events, persons and time. We present two intuitive interfaces for navigating content by topics and their related news events as well as serendipitously learning about a news topic. These two interfaces trade-off between user-controlled and serendipitous exploration of news while retaining the story context. The novelty of our work includes the linking of multi-source, multimodal news content to extracted entities and topical structures for contextual understanding, and visualized in intuitive active and passive interfaces.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {449–450},
numpages = {2},
keywords = {topic linking, news segmentation, multimedia analaysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502264,
author = {Bertini, Marco and Del Bimbo, Alberto and Ferracani, Andrea and Gelli, Francesco and Maddaluno, Daniele and Pezzatini, Daniele},
title = {A Novel Framework for Collaborative Video Recommendation, Interest Discovery and Friendship Suggestion Based on Semantic Profiling},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502264},
doi = {10.1145/2502081.2502264},
abstract = {Two important challenges for social networks are the creation of targeted and personalized content for their users, selecting the most interesting material from the huge amount of user-generated content, and keeping user engagement , e.g. through creation and curation of users' profiles. In this demo we show a system for video commenting, sharing and interest discovery that combines recommendation algorithms, clustering techniques, tools for video tagging and evaluation of semantic resources relatedness. Combining these tools and techniques it becomes possible to provide personalized multimedia services and to improve and propagate interests and inter-personal connections through the network.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {451–452},
numpages = {2},
keywords = {social video recommendation, internet videos, social video tagging},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502265,
author = {Bertini, Marco and Del Bimbo, Alberto and Ioannidis, George and Bijk, Emile and Trancoso, Isabel and Meinedo, Hugo},
title = {EuTV: A System for Media Monitoring and Publishing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502265},
doi = {10.1145/2502081.2502265},
abstract = {In this paper, we describe the euTV system, which provides a flexible approach to collect, manage, annotate and publish collections of images, videos and textual documents. The system is based on a Service Oriented Architecture that allows to combine and orchestrate a large set of web services for automatic and manual annotation, retrieval, browsing, ingestion and authoring of multimedia sources. euTV tools have been used to create several publicly available vertical applications, addressing different use cases. Positive results of user evaluations have shown that the system can be effectively used to create different types of applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {453–454},
numpages = {2},
keywords = {soa, semantic multimedia annotation, content-based multimedia retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502266,
author = {Armano, Giuliano and Giuliani, Alessandro and Messina, Alberto and Montagnuolo, Maurizio},
title = {CAMMA: Contextual Advertising System for Multimodal News Aggregations},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502266},
doi = {10.1145/2502081.2502266},
abstract = {This demo paper describes a system for contextual advertising on aggregations of multimodal news items. The prototype is intended to demonstrate how modern content analysis techniques can be profitably used to automate tasks commonly performed by humans such as the planning of the computer-assisted advertising content.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {455–456},
numpages = {2},
keywords = {contextual advertising, multimodal aggregation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502267,
author = {Del Bimbo, Alberto and Ferracani, Andrea and Pezzatini, Daniele},
title = {Flarty: Recommending Art Routes Using Check-Ins Latent Topics},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502267},
doi = {10.1145/2502081.2502267},
abstract = {In this demo we present Flarty, a mobile location-based social network for the dynamic construction and recommendation of art routes in the city of Florence, Italy, via item based similarity algorithms, places topic extraction and user interest modeling. To achieve this goal Flarty derives knowledge from users check-ins and combines clustering techniques and recommendation algorithms, as well as features such as geo-location, to define groups of similar artworks or POIs (Points Of Interest) and to compute the most efficient routes likely to meet user's interests. Model analysis takes into account ratings, topics extracted from textual features associated with the POIs, and users preferences computed exploiting collaborative filtering techniques on their past behavior.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {457–458},
numpages = {2},
keywords = {topic modeling, recommendation systems, location-based services, mobile},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502268,
author = {Borth, Damian and Chen, Tao and Ji, Rongrong and Chang, Shih-Fu},
title = {SentiBank: Large-Scale Ontology and Classifiers for Detecting Sentiment and Emotions in Visual Content},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502268},
doi = {10.1145/2502081.2502268},
abstract = {A picture is worth one thousand words, but what words should be used to describe the sentiment and emotions conveyed in the increasingly popular social multimedia? We demonstrate a novel system which combines sound structures from psychology and the folksonomy extracted from social multimedia to develop a large visual sentiment ontology consisting of 1,200 concepts and associated classifiers called SentiBank. Each concept, defined as an Adjective Noun Pair (ANP), is made of an adjective strongly indicating emotions and a noun corresponding to objects or scenes that have a reasonable prospect of automatic detection. We believe such large-scale visual classifiers offer a powerful mid-level semantic representation enabling high-level sentiment analysis of social multimedia. We demonstrate novel applications made possible by SentiBank including live sentiment prediction of social media and visualization of visual content in a rich intuitive semantic space.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {459–460},
numpages = {2},
keywords = {sentiment analysis, affect, ontology, concept detection, social multimedia, emotion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502269,
author = {Fu, Junsheng and Fan, Lixin and You, Yu and Roimela, Kimmo},
title = {Augmented and Interactive Video Playback Based on Global Camera Pose},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502269},
doi = {10.1145/2502081.2502269},
abstract = {This paper proposes a video playback system that allows user to expend the field of view to surrounding environments that are not visible in the original video frame, arbitrarily change the viewing angles, and see the superimposed point-of-interest (POIs) data in an augmented reality manner during the video playback. The processing consists of two main steps: in the first step, client uploads a video to the GeoVideo Engine, and then the GeoVideo Engine extracts the geo-metadata and returns them back to the client; in the second step, client requests POIs from server, and then the client renders the video with POIs.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {461–462},
numpages = {2},
keywords = {camera pose, augmented reality, video playback},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502270,
author = {Yu, Matt C. and Vajda, Peter and Chen, David M. and Tsai, Sam S. and Daneshi, Maryam and Araujo, Andre F. and Chen, Huizhong and Girod, Bernd},
title = {EigenNews: A Personalized News Video Delivery Platform},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502270},
doi = {10.1145/2502081.2502270},
abstract = {We demonstrate EigenNews, a personalized television news system. Upon visiting the EigenNews website, a user is shown a variety of news videos which have been automatically selected based on her individual preferences. These videos are extracted from 16 continually recorded television programs using a multimodal segmentation algorithm. Relevant metadata for each video are generated by linking videos to online news articles. Selected news videos can be watched in three different layouts and on various devices.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {463–464},
numpages = {2},
keywords = {personalization, story segmentation, news aggregation, story linking, mobile},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502271,
author = {Mour\~{a}o, Andr\'{e} and Magalh\~{a}es, Jo\~{a}o},
title = {NovaEm\"{o}Tions: Winning with a Smile},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502271},
doi = {10.1145/2502081.2502271},
abstract = {Human-computer interaction (HCI) is expanding towards natural modalities of human expression. Gestures, body movements and other affective interaction techniques can change the way computers interact with humans. In this demo, we display a fully playable version of NovaEm\"{o}tions, a competitive game where players score by acting an emotion through a facial expression. The game is designed to offer a competitive playing experience using only facial expressions. Despite the novelty of the interaction method, our game scoring algorithm kept players engaged and competitive.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {465–466},
numpages = {2},
keywords = {videogames, competitive games, affective interaction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502272,
author = {Chen, Jia and Jin, Qin and Zhang, Weipeng and Bao, Shenghua and Su, Zhong and Yu, Yong},
title = {Tell Me What Happened Here in History},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502272},
doi = {10.1145/2502081.2502272},
abstract = {This demo shows our system that takes a landmark image as input, recognizes the landmark from the image and returns historical events of the landmark with related photos. Different from existing landmark related researches, we focus on the temporal dimension of a landmark. Our system automatically recognizes the landmark, shows historical events chronologically and provides detailed photos for the events. To build these functions, we fuse information from multiple online resources.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {467–468},
numpages = {2},
keywords = {history photo, landmark},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502273,
author = {Wang, Xiaoyan and Sun, Lifeng and Wang, Shou},
title = {Group TV: A Cloud Based Social TV for Group Social Experience},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502273},
doi = {10.1145/2502081.2502273},
abstract = {Social TV allows people to meet the demand of social experience while watching videos. Existing solutions cannot balance interest among group and provide good viewing quality for all members with different network bandwidth and devices. Realizing that people in group will be more tolerant, trusting and willing to help each other, we developed a cloud based social TV for online group video service. We emphasize group behavior in our application and introduce the concept of tolerance and trust between users to balance their interest. Group Recommendation results we show in our system can obtain overall high satisfaction while provides diverse content. We also design a collaborative video distribution technology to optimize network resource utility for fluency and high quality viewing experience. The entire operation of the system emphasizes on simplicity, collaboration and smooth, providing excellent social TV experience.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {469–470},
numpages = {2},
keywords = {group recommendation, collaborative video delivery, social TV},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502274,
author = {Gao, Xingyu and Cao, Juan and Jin, Zhiwei and Li, Xin and Li, Jintao},
title = {GeSoDeck: A Geo-Social Event Detection and Tracking System},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502274},
doi = {10.1145/2502081.2502274},
abstract = {This demonstration presents a novel geo-social event detection and tracking system based on geographical pattern mining and content analysis, called "GeSoDeck". A user can capture what events happened by our system. Unlike most existing social event detection applications, GeSoDeck aims to detect events with high accuracy and efficiency, and track them as well. Given a geographical area, the system can not only detect diverse social events in this area using the geographical pattern mining and density-based K-means clustering, but also track the representative tweets of the detected event in real time, mining geographical diffusion trajectory on the map and temporal pattern of retweeting process. On a realistic dataset collected from Sina Weibo, the system can outperform the state-of-the-art methods.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {471–472},
numpages = {2},
keywords = {geo-social event, tracking, detection},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502275,
author = {Yang, You and Liu, Qiong and Gao, Yue and Xiong, Binbin and Yu, Li and Luan, Huanbo and Ji, Rongrong and Tian, Qi},
title = {Stereotime: A Wireless 2D and 3D Switchable Video Communication System},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502275},
doi = {10.1145/2502081.2502275},
abstract = {Mobile 3D video communication, especially with 2D and 3D compatible, is a new paradigm for both video communication and 3D video processing. Current techniques face challenges in mobile devices when bundled constraints such as computation resource and compatibility should be considered. In this work, we present a wireless 2D and 3D switchable video communication to handle the previous challenges, and name it as Stereotime. The methods of Zig-Zag fast object segmentation, depth cues detection and merging, and texture-adaptive view generation are used for 3D scene reconstruction. We show the functionalities and compatibilities on 3D mobile devices in WiFi network environment.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {473–474},
numpages = {2},
keywords = {video communication, scene reconstruction, visual content, view generation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502276,
author = {Sun, Xinghai and Wang, Changhu and Sud, Avneesh and Xu, Chao and Zhang, Lei},
title = {MagicBrush: Image Search by Color Sketch},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502276},
doi = {10.1145/2502081.2502276},
abstract = {In this paper, we showcase the MagicBrush system, a novel painting-based image search engine. This system enables users to draw a color sketch as a query to find images. Different from existing works on sketch-based image retrieval, most of which focus on matching the shape structure without carefully considering other important visual modalities, MagicBrush takes into account the indispensable value of "color" related to "shape", and explores to make use of both the shape and color expectations that users usually have when they're imaging or searching for an image. To achieve this, we 1) develop a user-friendly interface to allow users to easily "paint out" their colorful visual expectations; 2) design a compact feature "color-edge word" to encode both shape and color information in a organic way; and 3) develop a novel matching and index structure to support a real-time response in 6.4 million images. By taking into account both shape and color information, the MagicBrush system helps users to vividly present what they are imagining, and retrieve images in a more natural way.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {475–476},
numpages = {2},
keywords = {color sketch, magicbrush, painting-based image retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502277,
author = {Nguyen, Duong-Trung-Dung and Saini, Mukesh and Nguyen, Vu-Thanh and Ooi, Wei Tsang},
title = {Jiku Director: A Mobile Video Mashup System},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502277},
doi = {10.1145/2502081.2502277},
abstract = {In this technical demonstration, we demonstrate a Web-based application called Jiku Director that automatically creates a mashup video from event videos uploaded by users. The system runs an algorithm that considers view quality (shakiness, tilt, occlusion), video quality (blockiness, contrast, sharpness, illumination, burned pixels), and spatial-temporal diversity (shot angles, shot lengths) to create a mashup video with smooth shot transitions while covering the event from different perspectives.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {477–478},
numpages = {2},
keywords = {mobile video, virtual director, video mashup},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502278,
author = {Lin, Huijie and Jia, Jia and Liao, Hanyu and Cai, Lianhong},
title = {WeCard: A Multimodal Solution for Making Personalized Electronic Greeting Cards},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502278},
doi = {10.1145/2502081.2502278},
abstract = {In this demo, we build a practical system, WeCard, to generate personalized multimodal electronic greeting cards based on parametric emotional talking avatar synthesis technologies. Given user-input greeting text and facial image, WeCard intelligently and automatically generate the personalized speech with expressive lip-motion synchronized facial animation. Besides the parametric talking avatar synthesis, WeCard incorporates two key technologies: 1) automatical face mesh generation algorithm based on MPEG-4 FAPs (Facial Animation Parameters) extracted by the face alignment algorithm; 2) emotional audio-visual speech synchronization algorithm based on DBN. More specifically, WeCard merges the users? preferred electronic card scene with emotional talking avatar animation, turning the final content into flash or video file that can be easily shared with friends. By this way, WeCard can help you make your multimodal greetings to be more attractive, beautiful, and sincere.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {479–480},
numpages = {2},
keywords = {talking avatar, emotion, multimodal, personalized},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502129,
author = {Li, Xirong and Snoek, Cees G.M.},
title = {Classifying Tag Relevance with Relevant Positive and Negative Examples},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502129},
doi = {10.1145/2502081.2502129},
abstract = {Image tag relevance estimation aims to automatically determine what people label about images is factually present in the pictorial content. Different from previous works, which either use only positive examples of a given tag or use positive and random negative examples, we argue the importance of relevant positive and relevant negative examples for tag relevance estimation. We propose a system that selects positive and negative examples, deemed most relevant with respect to the given tag from crowd-annotated images. While applying models for many tags could be cumbersome, our system trains efficient ensembles of Support Vector Machines per tag, enabling fast classification. Experiments on two benchmark sets show that the proposed system compares favorably against five present day methods. Given extracted visual features, for each image our system can process up to 3,787 tags per second. The new system is both effective and efficient for tag relevance estimation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {485–488},
numpages = {4},
keywords = {image tag relevance, fast classification, relevant examples},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502130,
author = {Zhuo, Tao and Zhang, Yanning and Zhang, Peng and Huang, Wei and Sahli, Hichem},
title = {Non-Rigid Target Tracking Based on 'flow-Cut' in Pair-Wise Frames with Online Hough Forests},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502130},
doi = {10.1145/2502081.2502130},
abstract = {In conventional online learning based tracking studies, fixed-shape appearance modeling is often incorporated for training samples generation, as it is simple and convenient to be applied. However, for more general non-rigid and articulated object, this strategy may regard some background areas as foreground, which is likely to deteriorate the learning process. Recently published works utilize more than one patches to represent non-rigid object with foreground object segmentation, but most of these segmentation for target representation are performed only in single frame manner. Since the motion information between the consecutive frames was not considered by these approaches, when the backgrounds are similar to the target, accurate segmentation is hard to be achieved. In this work, we propose a novel model for non-rigid object segmentation by incorporating consecutive gradients flow between pair-wise frames into a Gibbs energy function. With help from motion information, the irregular target areas can be segmented more accurately during precise boundary convergence. The proposed segmentation model is incorporated into a semi-supervised online tracking framework for training samples generation. We test the proposed tracking on challenging videos involving heavy intrinsic variations and occlusions. As a result, the experiments demonstrate a significant improvement in tracking accuracy and robustness in comparison with other state-of-art tracking works.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {489–492},
numpages = {4},
keywords = {segmentation, online learning, hough forests, tracking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502131,
author = {Chen, Jingjing and Han, Yahong and Cao, Xiaochun and Tian, Qi},
title = {Object Coding on the Semantic Graph for Scene Classification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502131},
doi = {10.1145/2502081.2502131},
abstract = {In the scene classification, a scene can be considered as a set of object cliques. Objects inside each clique have semantic correlations with each other, while two objects from different cliques are relatively independent. To utilize these correlations for better recognition performance, we propose a new method - Object Coding on the Semantic Graph to address the scene classification problem. We first exploit prior knowledge by making statistics on a large number of labeled images and calculating the dependency degree between objects. Then, a graph is built to model the semantic correlations between objects. This semantic graph captures semantics by treating the objects as vertices and the objects affinities as the weights of edges. By encoding this semantic knowledge into the semantic graph, object coding is conducted to automatically select a set of object cliques that have strongly semantic correlations to represent a specific scene. The experimental results show that the Object Coding on semantic graph can improve the classification accuracy.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {493–496},
numpages = {4},
keywords = {semantic graph, object coding, scene classification},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502132,
author = {Zhang, Chunjie and Wang, Shuhui and Liang, Chao and Liu, Jing and Huang, Qingming and Li, Haojie and Tian, Qi},
title = {Beyond Bag of Words: Image Representation in Sub-Semantic Space},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502132},
doi = {10.1145/2502081.2502132},
abstract = {Due to the semantic gap, the low-level features are not able to semantically represent images well. Besides, traditional semantic related image representation may not be able to cope with large inter class variations and are not very robust to noise. To solve these problems, in this paper, we propose a novel image representation method in the sub-semantic space. First, examplar classifiers are trained by separating each training image from the others and serve as the weak semantic similarity measurement. Then a graph is constructed by combining the visual similarity and weak semantic similarity of these training images. We partition this graph into visually and semantically similar sub-sets. Each sub-set of images are then used to train classifiers in order to separate this sub-set from the others. The learned sub-set classifiers are then used to construct a sub-semantic space based representation of images. This sub-semantic space is not only more semantically meaningful but also more reliable and resistant to noise. Finally, we make categorization of images using this sub-semantic space based representation on several public datasets to demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {497–500},
numpages = {4},
keywords = {examplar classifier, object categorization, sub-semantic space, sparse coding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502133,
author = {Santani, Darshan and Gatica-Perez, Daniel},
title = {Speaking Swiss: Languages and Venues in Foursquare},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502133},
doi = {10.1145/2502081.2502133},
abstract = {Due to increasing globalization, urban societies are becoming more multicultural. The availability of large-scale digital mobility traces e.g. from tweets or checkins provides an opportunity to explore multiculturalism that until recently could only be addressed using survey-based methods. In this paper we examine a basic facet of multiculturalism through the lens of language use across multiple cities in Switzerland. Using data obtained from Foursquare over 330 days, we present a descriptive analysis of linguistic differences and similarities across five urban agglomerations in a multicultural, western European country.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {501–504},
numpages = {4},
keywords = {linguistic geography, multilingualism, foursquare},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502134,
author = {Mao, Zhendong and Zhang, Yongdong and Tian, Qi},
title = {What Are the Distance Metrics for Local Features?},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502134},
doi = {10.1145/2502081.2502134},
abstract = {Previous research has found that the distance metric for similarity estimation is determined by the underlying data noise distribution. The well known Euclidean(L2) and Manhattan (L1) metrics are then justified when the additive noise are Gaussian and Exponential, respectively. However, finding a suitable distance metric for local features is still a challenge when the underlying noise distribution is unknown and could be neither Gaussian nor Exponential. To address this issue, we introduce a modeling framework for arbitrary noise distributions and propose a generalized distance metric for local features based on this framework. We prove that the proposed distance is equivalent to the L1 or the L2 distance when the noise is Gaussian or Exponential. Furthermore, we justify the Hamming metric when the noise meets the given conditions. In that case, the proposed distance is a linear mapping of the Hamming distance. The proposed metric has been extensively tested on a benchmark data set with five state-of-the-art local features: SIFT, SURF, BRIEF, ORB and BRISK. Experiments show that our framework better models the real noise distributions and that more robust results can be obtained by using the proposed distance metric.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {505–508},
numpages = {4},
keywords = {distance metric, local feature, noise distribution},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502135,
author = {Luo, Ye and Yuan, Junsong},
title = {Salient Object Detection in Videos by Optimal Spatio-Temporal Path Discovery},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502135},
doi = {10.1145/2502081.2502135},
abstract = {Many consumer videos focus on and follow salient objects in a scene. Detecting such salient objects is thus of great interests to video analytics and search. Instead of detecting salient object in individual frames separately, we propose to detect and track salient object simultaneously by finding a spatio-temporal path of the highest saliency density in the video. As salient video objects usually appear in consecutive frames, leveraging the motion coherence of videos can detect salient object more robustly. Without any prior knowledge of the salient objects, our method can automatically detect the salient objects of different shapes and sizes, and is able to handle noisy saliency maps and moving cameras. Experimental results on two public datasets demonstrate the effectiveness of the proposed method on salient video object detection.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {509–512},
numpages = {4},
keywords = {optimal path discovery, temporal coherence, salient object detection},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502136,
author = {Fakeri-Tabrizi, Ali and Amini, Massih R. and Gallinari, Patrick},
title = {Multiview Semi-Supervised Ranking for Automatic Image Annotation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502136},
doi = {10.1145/2502081.2502136},
abstract = {Most photo sharing sites give their users the opportunity to manually label images. The labels collected that way are usually very incomplete due to the size of the image collections: most images are not labeled according to all the categories they belong to, and, conversely, many class have relatively few representative examples. Automated image systems that can deal with small amounts of labeled examples and unbalanced classes are thus necessary to better organize and annotate images. In this work, we propose a multiview semi-supervised bipartite ranking model which allows to leverage the information contained in unlabeled sets of images in order to improve the prediction performance, using multiple descriptions, or views of images. For each topic class, our approach first learns as many view-specific rankers as available views using the labeled data only. These rankers are then improved iteratively by adding pseudo-labeled pairs of examples on which all view-specific rankers agree over the ranking of examples within these pairs. We report on experiments carried out on the NUS-WIDE dataset, which show that the multiview ranking process improves predictive performances when a small number of labeled examples is available specially for unbalanced classes. We show also that our approach achieves significant improvements over a state-of-the art semi-supervised multiview classification model.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {513–516},
numpages = {4},
keywords = {image annotation, multiview semi-supervised ranking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502137,
author = {Vliegendhart, Raynor and Loni, Babak and Larson, Martha and Hanjalic, Alan},
title = {How Do We Deep-Link? Leveraging User-Contributed Time-Links for Non-Linear Video Access},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502137},
doi = {10.1145/2502081.2502137},
abstract = {This paper studies a new way of accessing videos in a non-linear fashion. Existing non-linear access methods allow users to jump into videos at points that depict specific visual concepts or that are likely to elicit affective reactions. We believe that deep-link comments, which occur unprompted on social video sharing platforms, offer a new opportunity beyond existing methods. With deep-link comments, viewers express themselves about a particular moment in a video by including a time-code. Deep-link comments are special because they reflect viewer perceptions of noteworthiness, that include, but extend beyond depicted conceptual content and induced affective reactions. Based on deep-link comments collected from YouTube, we develop a Viewer Expressive Reaction Variety (VERV) taxonomy that captures how viewers deep-link. We validate the taxonomy with a user study on a crowdsourcing platform and discuss how it extends conventional relevance criteria. We carry out experiments which show that deep-link comments can be automatically filtered and sorted into VERV categories.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {517–520},
numpages = {4},
keywords = {video retrieval, relevance criteria, non-linear video access, deep-links, comments, youtube},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502138,
author = {Zhuang, Xiaodan and Wu, Shuang and Natarajan, Pradeep},
title = {Compact Bag-of-Words Visual Representation for Effective Linear Classification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502138},
doi = {10.1145/2502081.2502138},
abstract = {Bag-of-words approaches have been shown to achieve state-of-the-art performance in large-scale multimedia event detection. However, the commonly used histogram representation of bag-of-words requires large codebook sizes and expensive nonlinear kernel based classifiers for optimal performance. To address these two issues, we present a two-part generative model for compact visual representation, based on the i-vector approach recently proposed for speech and audio modeling. First, we use a Gaussian mixture model (GMM) to model the joint distribution of local descriptors. Second, we use a low-dimensional factor representation that constrains the GMM parameters to a subspace that preserves most of the information. We further extend this method to incorporate overlapping spatial regions, forming a highly compact visual representation that achieves superior performance with fast linear classifiers. We evaluate the method on a large video dataset used in the TRECVID 2011 MED evaluation. With linear classifiers, the proposed representation, with one-tenth of the storage footprint, outperforms soft quantization histograms used in the top performing TRECVID 2011 MED systems.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {521–524},
numpages = {4},
keywords = {generative model, linear classifier, bag of words},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502139,
author = {Nga, Do Hang and Yanai, Keiji},
title = {Large-Scale Web Video Shot Ranking Based on Visual Features and Tag Co-Occurrence},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502139},
doi = {10.1145/2502081.2502139},
abstract = {In this paper, we propose a novel ranking method, VisualTextualRank, which extends [1] and [2]. Our method is based on random walk over bipartite graph to integrate visual information of video shots and tag information of Web videos effectively. Note that instead of treating the textual information as an additional feature for shot ranking, we explore the mutual reinforcement between shots and textual information of their corresponding videos to improve shot ranking. We apply our proposed method to the system of extracting automatically relevant video shots of specific actions from Web videos [3]. Based on our experimental results, we demonstrate that our ranking method can improve the performance of video shot retrieval.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {525–528},
numpages = {4},
keywords = {web video, ranking, bipartite graph},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502140,
author = {Pang, Shanmin and Xue, Jianru and Zheng, Nanning and Tian, Qi},
title = {Locality Preserving Verification for Image Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502140},
doi = {10.1145/2502081.2502140},
abstract = {Establishing correct correspondences between two images has a wide range of applications, such as 2D and 3D registration, structure from motion, and image retrieval. In this paper, we propose a new matching method based on spatial constraints. The proposed method has linear time complexity, and is efficient when applying it to image retrieval. The main assumption behind our method is that, the local geometric structure among a feature point and its neighbors, is not easily affected by both geometric and photometric transformations, and thus should be preserved in their corresponding images. We model this local geometric structure by linear coefficients that reconstruct the point from its neighbors. The method is flexible, as it can not only estimate the number of correct matches between two images efficiently, but also determine the correctness of each match accurately. Furthermore, it is simple and easy to be implemented. When applying the proposed method on re-ranking images in an image search engine, it outperforms the-state-of-the-art techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {529–532},
numpages = {4},
keywords = {image retrieval, local geometric structure, re-ranking, linear time complexity},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502141,
author = {Zhang, Chunjie and Zhang, Yifan and Wang, Shuhui and Pang, Junbiao and Liang, Chao and Huang, Qingming and Tian, Qi},
title = {Undo the Codebook Bias by Linear Transformation for Visual Applications},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502141},
doi = {10.1145/2502081.2502141},
abstract = {The bag of visual words model (BoW) and its variants have demonstrate their effectiveness for visual applications and have been widely used by researchers. The BoW model first extracts local features and generates the corresponding codebook, the elements of a codebook are viewed as visual words. The local features within each image are then encoded to get the final histogram representation. However, the codebook is dataset dependent and has to be generated for each image dataset. This costs a lot of computational time and weakens the generalization power of the BoW model. To solve these problems, in this paper, we propose to undo the dataset bias by codebook linear transformation. To represent every points within the local feature space using Euclidean distance, the number of bases should be no less than the space dimensions. Hence, each codebook can be viewed as a linear transformation of these bases. In this way, we can transform the pre-learned codebooks for a new dataset. However, not all of the visual words are equally important for the new dataset, it would be more effective if we can make some selection using sparsity constraints and choose the most discriminative visual words for transformation. We propose an alternative optimization algorithm to jointly search for the optimal linear transformation matrixes and the encoding parameters. Image classification experimental results on several image datasets show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {533–536},
numpages = {4},
keywords = {linear transformation, codebook bias, sparsity},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502142,
author = {Yan, Yan and Xu, Zhongwen and Liu, Gaowen and Ma, Zhigang and Sebe, Nicu},
title = {GLocal Structural Feature Selection with Sparsity for Multimedia Data Understanding},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502142},
doi = {10.1145/2502081.2502142},
abstract = {The selection of discriminative features is an important and effective technique for many multimedia tasks. Using irrelevant features in classification or clustering tasks could deteriorate the performance. Thus, designing efficient feature selection algorithms to remove the irrelevant features is a possible way to improve the classification or clustering performance. With the successful usage of sparse models in image and video classification and understanding, imposing structural sparsity in emph{feature selection} has been widely investigated during the past years. Motivated by the merit of sparse models, we propose a novel feature selection method using a sparse model in this paper. Different from the state of the art, our method is built upon $ell _{2,p}$-norm and simultaneously considers both the global and local (GLocal) structures of data distribution. Our method is more flexible in selecting the discriminating features as it is able to control the degree of sparseness. Moreover, considering both global and local structures of data distribution makes our feature selection process more effective. An efficient algorithm is proposed to solve the $ell_{2,p}$-norm sparsity optimization problem in this paper. Experimental results performed on real-world image and video datasets show the effectiveness of our feature selection method compared to several state-of-the-art methods.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {537–540},
numpages = {4},
keywords = {image and video classification, feature selection, l2, global and local, p-norm},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502143,
author = {Driedger, Jonathan and Grohganz, Harald and Pr\"{a}tzlich, Thomas and Ewert, Sebastian and M\"{u}ller, Meinard},
title = {Score-Informed Audio Decomposition and Applications},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502143},
doi = {10.1145/2502081.2502143},
abstract = {The separation of different sound sources from polyphonic music recordings constitutes a complex task since one has to account for different musical and acoustical aspects. In the last years, various score-informed procedures have been suggested where musical cues such as pitch, timing, and track information are used to support the source separation process. In this paper, we discuss a framework for decomposing a given music recording into notewise audio events which serve as elementary building blocks. In particular, we introduce an interface that employs the additional score information to provide a natural way for a user to interact with these audio events. By simply selecting arbitrary note groups within the score a user can access, modify, or analyze corresponding events in a given audio recording. In this way, our framework not only opens up new ways for audio editing applications, but also serves as a valuable tool for evaluating and better understanding the results of source separation algorithms.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {541–544},
numpages = {4},
keywords = {score-informed processing, alignment, music synchronization, audio editing, source separation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502144,
author = {Ren, Zhixiang and Chia, Liang-Tien and Rajan, Deepu and Gao, Shenghua},
title = {Background Subtraction via Coherent Trajectory Decomposition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502144},
doi = {10.1145/2502081.2502144},
abstract = {Background subtraction, the task to detect moving objects in a scene, is an important step in video analysis. In this paper, we propose an efficient background subtraction method based on coherent trajectory decomposition. We assume that the trajectories from background lie in a low-rank subspace, and foreground trajectories are sparse outliers in this background subspace. Meanwhile, the Markov Random Field (MRF) is used to encode the spatial coherency and trajectory consistency. With the low-rank decomposition and the MRF, our method can better handle videos with moving camera and obtain coherent foreground. Experimental results on a video dataset show our method achieves very competitive performance.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {545–548},
numpages = {4},
keywords = {background subtraction, sparse, low-rank, trajectory},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502145,
author = {Guo, Xiaojie and Li, Siyuan and Cao, Xiaochun},
title = {Motion Matters: A Novel Framework for Compressing Surveillance Videos},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502145},
doi = {10.1145/2502081.2502145},
abstract = {Currently, video surveillance plays a very important role in the fields of public safety and security. For storing the videos that usually contain extremely long sequences, it requires huge space. Video compression techniques can be used to release the storage load to some extent, such as H.264/AVC. However, the existing codecs are not sufficiently effective and efficient for encoding surveillance videos as they do not specifically consider the characteristic of surveillance videos, i.e. the background of surveillance video has intensive redundancy. This paper introduces a novel framework for compressing such videos. We first train a background dictionary based on a small number of observed frames. With the trained background dictionary, we then separate every frame into the background and motion (foreground), and store the compressed motion together with the reconstruction coefficient of the background corresponding to the background dictionary. The decoding is carried out on the encoded frame in an inverse procedure. The experimental results on extensive surveillance videos demonstrate that our proposed method significantly reduces the size of videos while gains much higher PSNR compared to the state of the art codecs.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {549–552},
numpages = {4},
keywords = {video surveillance, motion, video compression},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502146,
author = {Nguyen, Viet Anh and Zhao, Shengkui and Vu, Tien Dung and Jones, Douglas L. and Do, Minh N.},
title = {Spatialized Audio Multiparty Teleconferencing with Commodity Miniature Microphone Array},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502146},
doi = {10.1145/2502081.2502146},
abstract = {This paper presents a Spatialized Audio Multiparty Teleconferencing (SAMT) system with a radically new communication experience for group teleconferencing. The system includes our recently developed 3D audio technologies: 3D sound source localization (SSL) and 3D audio capture and reproduction using a low-cost and compact design microphone array. In essence, the SAMT system offers 3D audio capture capability and spatial audio perception with multiple participants at a site, which still falls short in teleconferencing solutions. In addition to being able to identify and automatically track the active speaker, the system allows more compelling visual presentation for effective communication. Requiring only a low-cost microphone array and a consumer depth camera, the proposed system runs reliably and comfortably in real time on a commodity laptop or desktop PC. With such a minimal deployment requirement, we present a variety of user experiences created by SAMT.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {553–556},
numpages = {4},
keywords = {3D sound localization, 3D spatialized audio, video conferencing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502147,
author = {Baltieri, Davide and Vezzani, Roberto and Cucchiara, Rita},
title = {Learning Articulated Body Models for People Re-Identification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502147},
doi = {10.1145/2502081.2502147},
abstract = {People re-identification is a challenging problem in surveillance and forensics and it aims at associating multiple instances of the same person which have been acquired from different points of view and after a temporal gap. Image-based appearance features are usually adopted but, in addition to their intrinsically low discriminability, they are subject to perspective and view-point issues. We propose to completely change the approach by mapping local descriptors extracted from RGB-D sensors on a 3D body model for creating a view-independent signature. An original bone-wise color descriptor is generated and reduced with PCA to compute the person signature. The virtual bone set used to map appearance features is learned using a recursive splitting approach. Finally, people matching for re-identification is performed using the Relaxed Pairwise Metric Learning, which simultaneously provides feature reduction and weighting. Experiments on a specific dataset created with the Microsoft Kinect sensor and the OpenNi libraries prove the advantages of the proposed technique with respect to state of the art methods based on 2D or non-articulated 3D body models.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {557–560},
numpages = {4},
keywords = {people re-identification, Microsoft Kinect, RGB-D sensors},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502148,
author = {Zhang, Zhanpeng and Zhang, Wei and Liu, Jianzhuang and Tang, Xiaoou},
title = {Facial Landmark Localization Based on Hierarchical Pose Regression with Cascaded Random Ferns},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502148},
doi = {10.1145/2502081.2502148},
abstract = {The main challenge of facial landmark localization in real-world application is that the large changes of head pose and facial expressions cause substantial image appearance variations. To avoid high dimensional regression in the 3D and 2D facial pose spaces simultaneously, we propose a hierarchical pose regression approach, estimating the head rotation, facial components and landmarks hierarchically. The regression process works in a unified cascaded fern framework. We present generalized gradient boosted ferns (GBFs) for the regression framework, which give better performance than traditional ferns. The framework also achieves real time performance. We verify our method on the latest benchmark datasets. The results show that it outperforms state-of-the-art methods in both accuracy and speed.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {561–564},
numpages = {4},
keywords = {pose regression, random fern, facial landmark localization},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502149,
author = {Kimura, Akisato and Ishiguro, Katsuhiko and Yamada, Makoto and Marcos Alvarez, Alejandro and Kataoka, Kaori and Murasaki, Kazuhiko},
title = {Image Context Discovery from Socially Curated Contents},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502149},
doi = {10.1145/2502081.2502149},
abstract = {This paper proposes a novel method of discovering a set of image contents sharing a specific context (attributes or implicit meaning) with the help of image collections obtained from social curation platforms. Socially curated contents are promising to analyze various kinds of multimedia information, since they are manually filtered and organized based on specific individual preferences, interests or perspectives. Our proposed method fully exploits the process of social curation: (1) How image contents are manually grouped together by users, and (2) how image contents are distributed in the platform. Our method reveals the fact that image contents with a specific context are naturally grouped together and every image content includes really various contexts that cannot necessarily be verbalized by texts.% A preliminary experiment with a small collection of a million of images yields a promising result.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {565–568},
numpages = {4},
keywords = {graph clustering, social curation, classification, contexts},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502150,
author = {Li, Lu and Xue, Jianru and Tian, Zhiqiang and Zheng, Nanning},
title = {Moment Feature Based Forensic Detection of Resampled Digital Images},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502150},
doi = {10.1145/2502081.2502150},
abstract = {Forensic detection of resampled digital images has become an important technology among many others to establish the integrity of digital visual content. This paper proposes a moment feature based method to detect resampled digital images. Rather than concentrating on the positions of characteristic resampling peaks, we utilize a moment feature to exploit the periodic interpolation characteristics in the frequency domain. Not only the positions of resampling peaks but also the amplitude distribution is taken into consideration. With the extracted moment feature, a trained SVM classifier is used to detect resampled digital images. Extensive experimental results show the validity and efficiency of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {569–572},
numpages = {4},
keywords = {resampling detection, moment feature, digital forensic},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502151,
author = {Popescu, Adrian and Shabou, Aymen},
title = {Towards Precise POI Localization with Social Media},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502151},
doi = {10.1145/2502081.2502151},
abstract = {Points of interest (POIs) are a core component of geographical databases and of location based services. POI acquisition was performed by domain experts but associated costs and access difficulties in many regions of the world reduce the coverage of manually built geographical databases. With the availability of large geotagged multimedia datasets on the Web, a sustained research effort was dedicated to automatic POI discovery and characterization. However, in spite of its practical importance, POI localization was only marginally addressed. To compute POI coordinates an assumption was made that the more data were available, the more precise the localization will be. Here we shift the focus of the process from data quantity to data quality. Given a set of geotagged Flickr photos associated to a POI, close-up classification is used to trigger a spatial clustering process. To evaluate the newly introduced method against different other localization schemes, we create an accurate ground truth. We show that significant localization error reductions are obtained compared to a coordinate averaging approach and to a X-Means clustering scheme.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {573–576},
numpages = {4},
keywords = {geographic information extraction, POI localization, flickr},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502152,
author = {Zhao, Wan-Lei and J\'{e}gou, Herv\'{e} and Gravier, Guillaume},
title = {Sim-Min-Hash: An Efficient Matching Technique for Linking Large Image Collections},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502152},
doi = {10.1145/2502081.2502152},
abstract = {One of the most successful method to link all similar images within a large collection is min-Hash, which is a way to significantly speed-up the comparison of images when the underlying image representation is bag-of-words. However, the quantization step of min-Hash introduces important information loss. In this paper, we propose a generalization of min-Hash, called Sim-min-Hash, to compare sets of real-valued vectors. We demonstrate the effectiveness of our approach when combined with the Hamming embedding similarity. Experiments on large-scale popular benchmarks demonstrate that Sim-min-Hash is more accurate and faster than min-Hash for similar image search. Linking a collection of one million images described by 2 billion local descriptors is done in 7 minutes on a single core machine.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {577–580},
numpages = {4},
keywords = {min-hash, large-scale, image linking/search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502153,
author = {Srinivasan, Ramya and Roy-Chowdhury, Amit and Rudolph, Conrad and Kohl, Jeanette},
title = {Recognizing the Royals: Leveraging Computerized Face Recognition for Identifying Subjects in Ancient Artworks},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502153},
doi = {10.1145/2502081.2502153},
abstract = {We present a work that explores the feasibility of automated face recognition technologies for analyzing identities in works of portraiture, and in the process provide additional evidence to settle some long-standing questions in art history. Works of portrait art bear the mark of visual interpretation of the artist. Moreover, the number of samples available to model these effects is often limited. From a set of portraiture of the Renaissance and Baroque periods, where the identities of subjects are known, we derive appropriate features that are based on domain knowledge of artistic renderings, and learn and validate statistical models for the distribution of the match and non-match scores, which we refer to as portrait feature space (PFS). Thereafter, we use this PFS on a number of cases that have been "open questions" to art historians. They are usually in the form of validating two portraits as belonging to the same person. Using statistical hypothesis tests on the PFS, we provide quantitative measures of similarity for each of these questions. It is, to the best of our knowledge, the first study that applies automated face recognition technologies to the analysis of portraits of multiple subjects in various forms - paintings, death masks, sculptures.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {581–584},
numpages = {4},
keywords = {portrait art, feature selection, face recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502154,
author = {Marzo, Asier and Ardaiz, Oscar},
title = {CollARt: A Tool for Creating 3D Photo Collages Using Mobile Augmented Reality},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502154},
doi = {10.1145/2502081.2502154},
abstract = {A collage is an artistic composition made by assembling different parts to create a new whole. This procedure can be applied for assembling tridimensional objects. In this paper we present CollARt, a Mobile Augmented Reality application which permits to create 3D photo collages. Virtual pieces are textured with pictures taken with the camera and can be blended with real objects. A preliminary user study (N=12) revealed that participants were able to create interesting works of art. The evaluation also suggested that the possibility of itinerantly mixing virtual pieces with the real world increases creativity.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {585–588},
numpages = {4},
keywords = {creativity, mobile, augmented reality, collages},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502155,
author = {Chen, Qiang and Cai, Yang and Brown, Lisa and Datta, Ankur and Fan, Quanfu and Feris, Rogerio and Yan, Shuicheng and Hauptmann, Alex and Pankanti, Sharath},
title = {Spatio-Temporal Fisher Vector Coding for Surveillance Event Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502155},
doi = {10.1145/2502081.2502155},
abstract = {We present a generic event detection system evaluated in the Surveillance Event Detection (SED) task of TRECVID 2012. We investigate a statistical approach with spatio-temporal features applied to seven event classes, which were defined by the SED task. This approach is based on local spatio-temporal descriptors, called MoSIFT and generated by pair-wise video frames. A Gaussian Mixture Model(GMM) is learned to model the distribution of the low level features. Then for each sliding window, the Fisher vector encoding [improvedFV] is used to generate the sample representation. The model is learnt using a Linear SVM for each event. The main novelty of our system is the introduction of Fisher vector encoding into video event detection. Fisher vector encoding has demonstrated great success in image classification. The key idea is to model the low level visual features as a Gaussian Mixture Model and to generate an intermediate vector representation for bag of features. FV encoding uses higher order statistics in place of histograms in the standard BoW. FV has several good properties: (a) it can naturally separate the video specific information from the noisy local features and (b) we can use a linear model for this representation. We build an efficient implementation for FV encoding which can attain a 10 times speed-up over real-time. We also take advantage of non-trivial object localization techniques to feed into the video event detection, e.g. multi-scale detection and non-maximum suppression. This approach outperformed the results of all other teams submissions in TRECVID SED 2012 on four of the seven event types.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {589–592},
numpages = {4},
keywords = {feature coding, system, video event detection},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502156,
author = {Wu, Lin and Wang, Yang and Shepherd, John},
title = {Efficient Image and Tag Co-Ranking: A Bregman Divergence Optimization Method},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502156},
doi = {10.1145/2502081.2502156},
abstract = {Ranking on image search has attracted considerable attentions. Many graph-based algorithms have been proposed to solve this problem. Despite their remarkable success, these approaches are restricted to their separated image networks. To improve the ranking performance, one effective strategy is to work beyond the separated image graph by leveraging fruitful information from manual semantic labeling (i.e., tags) associated with images, which leads to the technique of co-ranking images and tags, a representative method that aims to explore the reinforcing relationship between image and tag graphs. The idea of co-ranking is implemented by adopting the paradigm of random walks. However, there are two problems hidden in co-ranking remained to be open: the high computational complexity and the problem of out-of-sample. To address the challenges above, in this paper, we cast the co-ranking process into a Bregman divergence optimization framework under which we transform the original random walk into an equivalent optimal kernel matrix learning problem. Enhanced by this new formulation, we derive a novel extension to achieve a better performance for both in-sample and out-of-sample cases. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of our approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {593–596},
numpages = {4},
keywords = {bregman divergence, out-of-sample, co-ranking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502157,
author = {Chu, Kuan-Yu and Kuo, Yin-Hsi and Hsu, Winston H.},
title = {Real-Time Privacy-Preserving Moving Object Detection in the Cloud},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502157},
doi = {10.1145/2502081.2502157},
abstract = {With the advance of cloud computing, growing applications have been migrating to the cloud for its robustness and scalability. However, sending raw data to the cloud-based service providers will generally risk our privacy; especially for cloud-based surveillance system, where privacy is one of the major concerns as continuously recording daily life. Thus, privacy-preserving intelligent analytics are in dire needs. In this preliminary research, we investigate real-time privacy-preserving moving object detection in the encrypted cloud-based surveillance videos. Moving object detection is one of the core techniques and can further enable other applications (e.g., object tracking, action recognition, etc.). One possible approach is using homomorphic encryption which provides corresponding operations between unencrypted and encrypted data. However, homomorphic encryption is impractical in real case because of formidable computations and bulky storage consumption. In this paper, we propose an efficient and secure encryption framework, which entails real-time analytics (e.g., moving object detection) in encrypted video streams. Experiments confirm that the proposed method can achieve similar accuracy as detection on original raw frames.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {597–600},
numpages = {4},
keywords = {real-time detection, privacy-preserving surveillance},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502158,
author = {Huang, De-An and Wang, Yu-Chiang Frank},
title = {With One Look: Robust Face Recognition Using Single Sample per Person},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502158},
doi = {10.1145/2502081.2502158},
abstract = {In this paper, we address the problem of robust face recognition using single sample per person. Given only one training image per subject of interest, our proposed method is able to recognize query images with illumination or expression changes, or even the corrupted ones due to occlusion. In order to model the above intra-class variations, we advocate the use of external data (i.e., images of subjects not of interest) for learning an exemplar-based dictionary. This dictionary provides auxiliary yet representative information for handling intra-class variation, while the gallery set containing one training image per class preserves separation between different subjects for recognition purposes. Our experiments on two face datasets confirm the effectiveness and robustness of our approach, which is shown to outperform state-of-the-art sparse representation based methods.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {601–604},
numpages = {4},
keywords = {face recognition, sparse representation, low-rank matrix decomposition, affinity propagation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502159,
author = {Kanezaki, Asako and Kuniyoshi, Yasuo and Harada, Tatsuya},
title = {Weakly-Supervised Multi-Class Object Detection Using Multi-Type 3D Features},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502159},
doi = {10.1145/2502081.2502159},
abstract = {We propose a weakly-supervised learning method for object detection using color and depth images of a real environment attached with object labels. The proposed method applies Multiple Instance Learning to find proper instances of the objects in training images. This method is novel in the sense that it learns multiple objects simultaneously in a way to balance the scores of each training sample across all object classes. Moreover, we combine 3D features considering different properties, that is, color texture, grayscale texture, and surface curvature, to improve the performance. We show that our method surpasses a conventional method using color and depth images. Furthermore, we evaluate its performance with our new dataset consisting of color and depth images with weak labels of 100 objects and various backgrounds.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {605–608},
numpages = {4},
keywords = {online learning, multiple instance learning},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502160,
author = {Mazloom, Masoud and Habibian, Amirhossein and Snoek, Cees G.M.},
title = {Querying for Video Events by Semantic Signatures from Few Examples},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502160},
doi = {10.1145/2502081.2502160},
abstract = {We aim to query web video for complex events using only a handful of video query examples, where the standard approach learns a ranker from hundreds of examples. We consider a semantic signature representation, consisting of off-the-shelf concept detectors, to capture the variance in semantic appearance of events. Since it is unknown what similarity metric and query fusion to use in such an event retrieval setting, we perform three experiments on unconstrained web videos from the TRECVID event detection task. It reveals that: retrieval with semantic signatures using normalized correlation as similarity metric outperforms a low-level bag-of-words alternative, multiple queries are best combined using late fusion with an average operator, and event retrieval is preferred over event classification when less than eight positive video examples are available.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {609–612},
numpages = {4},
keywords = {multiple queries, fusion, video retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502161,
author = {Grosche, Peter and M\"{u}ller, Meinard and Serr\`{a}, Joan},
title = {Towards Cover Group Thumbnailing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502161},
doi = {10.1145/2502081.2502161},
abstract = {In this paper we investigate whether we can extract the commonalities shared by a group of cover songs or versions of the same musical piece. As a main contribution, we introduce the concept of cover group thumbnail, which is the most representative, essential subsequence for an entire group of versions. Opposed to previous approaches, we jointly consider all versions of a given song to compute a single cover group template, which then shows a high degree of robustness against version-specific aspects. To compute such a template, we introduce a modification of a recent audio thumbnailing technique. To evaluate the reliability of our conceptual contribution, we consider the task of template-based version identification, where we show comparable accuracies to existing systems.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {613–616},
numpages = {4},
keywords = {cover song, music retrieval, audio thumbnailing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502162,
author = {Gong, Dihong and Li, Zhifeng and Liu, Jianzhuang and Qiao, Yu},
title = {Multi-Feature Canonical Correlation Analysis for Face Photo-Sketch Image Retrieval},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502162},
doi = {10.1145/2502081.2502162},
abstract = {Automatic face photo-sketch image retrieval has attracted great attention in recent years due to its important applications in real life. The major difficulty in automatic face photo-sketch image retrieval lies in the fact that there exists great discrepancy between the different image modalities (photo and sketch). In order to reduce such discrepancy and improve the performance of automatic face photo-sketch image retrieval, we propose a new framework called multi-feature canonical correlation analysis (MCCA) to effectively address this problem. The MCCA is an extension and improvement of the canonical correlation analysis (CCA) algorithmusing multiple features combined with two different random sampling methods in feature space and sample space. In this framework, we first represent each photo or sketch using a patch-based local feature representation scheme, in which histograms of oriented gradients (HOG) and multi-scale local binary pattern (MLBP) serve as the local descriptors. Canonical correlation analysis (CCA) is then performed on a collection of random subspaces to construct an ensemble of classifiers for photo-sketch image retrieval. Extensive experiments on two public-domain face photo-sketch datasets (CUFS and CUFSF) clearly show that the proposed approach obtains a substantial improvement over the state-of-the-art.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {617–620},
numpages = {4},
keywords = {image retrieval, canonical correlation analysis, face photo-sketch matching},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502163,
author = {Lu, Zhihan and Lal Khan, Muhammad Sikandar and Ur R\'{e}hman, Shafiq},
title = {Hand and Foot Gesture Interaction for Handheld Devices},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502163},
doi = {10.1145/2502081.2502163},
abstract = {In this paper we present hand and foot based immersive multimodal interaction approach for handheld devices. A smart phone based immersive football game is designed as a proof of concept. Our proposed method combines input modalities (i.e. hand &amp; foot) and provides a coordinated output to both modalities along with audio and video. In this work, human foot gesture is detected and tracked using template matching method and Tracking-Learning-Detection (TLD) framework. We evaluated our system's usability through a user study in which we asked participants to evaluate proposed interaction method. Our preliminary evaluation demonstrates the efficiency and ease of use of proposed multimodal interaction approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {621–624},
numpages = {4},
keywords = {foot gesture, mobile, HCI, vibrotactile, immersive multimodal interaction, smart phone games},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502164,
author = {Lin, Shih-Yao and Shie, Chuen-Kai and Chen, Shen-Chi and Hung, Yi-Ping},
title = {AirTouch Panel: A Re-Anchorable Virtual Touch Panel},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502164},
doi = {10.1145/2502081.2502164},
abstract = {To achieve maximum mobility, device-less approaches for home appliance remote control have received increasing attention in recent years. In this paper, we propose a screen-less virtual touch panel, called AirTouch Panel, which can be positioned at any place with various orientations around users. The proposed virtual touch panel provides a potential ability to remotely control the home appliances, such as television, air conditioner, and so on. The proposed system allows users to anchor the panel at the place with comfortable poses. If the users want to change panel's position or orientation, they only need to re-anchor it, and then the panel will be reset. In this paper, our main contribution is to design a re-anchorable virtual panel for digital home remote control. Most importantly, we explore the design of such imaginary interface through two user studies. In our user studies, we analyze task completion time, satisfaction rate, and the number of miss-clicks. We are interested in the feasibility issues, for example, proper click gesture, panel size and button size, etc. Moreover, based on the AirTouch Panel, we also developed an intelligent TV to demonstrate the usability for controlling home appliance.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {625–628},
numpages = {4},
keywords = {remote control, imaginary interface, gesture},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502165,
author = {Walber, Tina and Neuhaus, Chantal and Staab, Steffen and Scherp, Ansgar and Jain, Ramesh},
title = {Creation of Individual Photo Selections: Read Preferences from the Users' Eyes},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502165},
doi = {10.1145/2502081.2502165},
abstract = {The automated selection of satisfying subsets from large collections of photos is a central challenge in multimedia research. Objective criteria like the depiction of persons or the photo quality are met by existing approaches. But it is difficult to know the users' personal interest, which plays an important role in the selection process. The expected spread of devices with eye tracking support in the near future allows us to measure this interest in a new way. In an experiment with 12 participants, we derive the most interesting photos of a collection for every person from gaze information recorded during the free viewing of the photos. We can show that the eye tracking information delivers valuable information about the users' preferences by comparing the results to a manual selection. The selection based on gaze information significantly outperforms baseline approaches and improves the results by up to 17%. For photo sets of personal interest this improvement is even up to 23%.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {629–632},
numpages = {4},
keywords = {personalized photo selection, personal interest, eye tracking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502166,
author = {Wang, Junqiang and Tang, Jinhui and Jiang, Yu-Gang},
title = {Strong Geometrical Consistency in Large Scale Partial-Duplicate Image Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502166},
doi = {10.1145/2502081.2502166},
abstract = {The state-of-the-art partial-duplicate image search systems reply heavily on the match of local features like SIFT. Independently matching local features across two images ignores the overall geometry structure and therefore may incur many false matches. To reduce such matches, several geometry verification methods have been proposed. This paper introduces a new geometry verification method named as Strong Geometry Consistency (SGC), which uses the orientation, scale and location information of the local feature points to accurately and quickly remove the false matches. We also propose a simple scale weighting (SW) strategy, which gives feature points with larger scales greater weights, based on the intuition that a larger-scale feature point tends to be more robust for image search as it occupies a larger area of an image. Extensive experiments performed on three popular datasets show that SGC significantly outperforms state-of-the-art geometry verification methods, and SW can further boost the performance with marginal additional computation.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {633–636},
numpages = {4},
keywords = {image search, partial-duplicate images, SGC, geometry verification, scale weighting},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502167,
author = {Kim, Ilseo and Oh, Sangmin and Vahdat, Arash and Cannons, Kevin and Perera, A.G. Amitha and Mori, Greg},
title = {Segmental Multi-Way Local Pooling for Video Recognition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502167},
doi = {10.1145/2502081.2502167},
abstract = {In this work, we address the problem of complex event detection on unconstrained videos. We introduce a novel multi-way feature pooling approach which leverages segment-level information. The approach is simple and widely applicable to diverse audio-visual features. Our approach uses a set of clusters discovered via unsupervised clustering of segment-level features. Depending on feature characteristics, not only scene-based clusters but also motion/audio-based clusters can be incorporated. Then, every video is represented with multiple descriptors, where each descriptor is designed to relate to one of the pre-built clusters. For classification, intersection kernel SVMs are used where the kernel is obtained by combining multiple kernels computed from corresponding per-cluster descriptor pairs. Evaluation on TRECVID'11 MED dataset shows a significant improvement by the proposed approach beyond the state-of-the-art.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {637–640},
numpages = {4},
keywords = {multimedia event recognition, representation, TRECVID},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502168,
author = {Peng, Peng and Cannons, Kevin and Li, Ze-Nian},
title = {Efficient Video Quality Assessment Based on Spacetime Texture Representation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502168},
doi = {10.1145/2502081.2502168},
abstract = {Most existing video quality metrics measure temporal distortions based on optical-flow estimation, which typically has limited descriptive power of visual dynamics and low efficiency. This paper presents a unified and efficient framework to measure temporal distortions based on a spacetime texture representation of motion. We first propose an effective motion-tuning scheme to capture temporal distortions along motion trajectories by exploiting the distributive characteristic of the spacetime texture. Then we reuse the motion descriptors to build a self-information based spatiotemporal saliency model to guide the spatial pooling. At last, a comprehensive quality metric is developed by combining the temporal distortion measure with spatial distortion measure. Our method demonstrates high efficiency and excellent correlation with the human perception of video quality.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {641–644},
numpages = {4},
keywords = {spatiotemporal oriented energy (SOE), video quality assessment, spacetime texture representation, visual attention},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502169,
author = {Wang, Yu and Tang, Sheng and Zhang, Yalin and Li, JinTao and Chen, DanYi},
title = {Fitted Spectral Hashing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502169},
doi = {10.1145/2502081.2502169},
abstract = {Spectral hashing (SpH) is an efficient and simple binary hashing method, which assumes that data are sampled from a multidimensional uniform distribution. However, this assumption is too restrictive in practice. In this paper we propose an improved method, Fitted Spectral Hashing, to relax this distribution assumption. Our work is based on the fact that one-dimensional data of any distribution could be mapped to a uniform distribution without changing the local neighbor relations among data items. We have found that this mapping on each PCA direction has certain regular pattern, and could fit data well by S-Curve function, Sigmoid function. With more parameters Fourier function also fit data well. Thus with Sigmoid function and Fourier function, we propose two binary hashing methods. Experiments show that our methods are efficient and outperform state-of-the-art methods.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {645–648},
numpages = {4},
keywords = {spectral hashing, sigmoid function, fourier function},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502170,
author = {Chen, Chih-Ming and Tsai, Ming-Feng and Liu, Jen-Yu and Yang, Yi-Hsuan},
title = {Using Emotional Context from Article for Contextual Music Recommendation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502170},
doi = {10.1145/2502081.2502170},
abstract = {This paper proposes a context-aware approach that recommends music to a user based on the user's emotional state predicted from the article the user writes. We analyze the association between user-generated text and music by using a real-world dataset with user, text, music tripartite information collected from the social blogging website LiveJournal. The audio information represents various perceptual dimensions of music listening, including danceability, loudness, mode, and tempo; the emotional text information consists of bag-of-words and three dimensional affective states within an article: valence, arousal and dominance. To combine these factors for music recommendation, a factorization machine-based approach is taken. Our evaluation shows that the emotional context information mined from user-generated articles does improve the quality of recommendation, comparing to either the collaborative filtering approach or the content-based approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {649–652},
numpages = {4},
keywords = {emotion-based music recommendation, listening context},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502171,
author = {Delhumeau, Jonathan and Gosselin, Philippe-Henri and J\'{e}gou, Herv\'{e} and P\'{e}rez, Patrick},
title = {Revisiting the VLAD Image Representation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502171},
doi = {10.1145/2502081.2502171},
abstract = {Recent works on image retrieval have proposed to index images by compact representations encoding powerful local descriptors, such as the closely related VLAD and Fisher vector. By combining such a representation with a suitable coding technique, it is possible to encode an image in a few dozen bytes while achieving excellent retrieval results. This paper revisits some assumptions proposed in this context regarding the handling of "visual burstiness", and shows that ad-hoc choices are implicitly done which are not desirable. Focusing on VLAD without loss of generality, we propose to modify several steps of the original design. Albeit simple, these modifications significantly improve VLAD and make it compare favorably against the state of the art.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {653–656},
numpages = {4},
keywords = {multimedia retrieval, VLAD, image search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502172,
author = {Soleymani, Mohammad and Kaltwang, Sebastian and Pantic, Maja},
title = {Human Behavior Sensing for Tag Relevance Assessment},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502172},
doi = {10.1145/2502081.2502172},
abstract = {Users react differently to non-relevant and relevant tags associated with content. These spontaneous reactions can be used for labeling large multimedia databases. We present a method to assess tag relevance to images using the non-verbal bodily responses, namely, electroencephalogram (EEG), facial expressions, and eye gaze. We conducted experiments in which 28 images were shown to 28 subjects once with correct and another time with incorrect tags. The goal of our system is to detect the responses to non-relevant tags and consequently filter them out. Therefore, we trained classifiers to detect the tag relevance from bodily responses. We evaluated the performance of our system using a subject independent approach. The precision at top 5% and top 10% detections were calculated and results of different modalities and different classifiers were compared. The results show that eye gaze outperforms the other modalities in tag relevance detection both overall and for top ranked results.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {657–660},
numpages = {4},
keywords = {facial expressions, EEG, implicit tagging, eye gaze},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502173,
author = {Chen, Jinhui and Ariki, Yasuo and Takiguchi, Tetsuya},
title = {Robust Facial Expressions Recognition Using 3D Average Face and Ameliorated Adaboost},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502173},
doi = {10.1145/2502081.2502173},
abstract = {One of the most crucial techniques associated with Computer Vision is technology that deals with facial recognition, especially, the automatic estimation of facial expressions. However, in real-time facial expression recognition, when a face turns sideways, the expressional feature extraction becomes difficult as the view of camera changes and recognition accuracy degrades significantly. Therefore, quite many conventional methods are proposed, which are based on static images or limited to situations in which the face is viewed from the front. In this paper, a method that uses Look-Up-Table (LUT) AdaBoost combining with the three-dimensional average face is proposed to solve the problem mentioned above. In order to evaluate the proposed method, the experiment compared with the conventional method was executed. These approaches show promising results and very good success rates. This paper covers several methods that can improve results by making the system more robust.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {661–664},
numpages = {4},
keywords = {facial expressions recognition, adaboost, 3D average face},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502174,
author = {Roshan Zamir, Amir and Dehghan, Afshin and Shah, Mubarak},
title = {Visual Business Recognition: A Multimodal Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502174},
doi = {10.1145/2502081.2502174},
abstract = {In this paper we investigate a new problem called visual business recognition. Automatic identification of businesses in images is an interesting task with plenty of potential applications especially for mobile device users. We propose a multimodal approach which incorporates business directories, textual information, and web images in a unified framework. We assume the query image is associated with a coarse location tag and utilize business directories for extracting an over complete list of nearby businesses which may be visible in the image. We use the name of nearby businesses as search keywords in order to automatically collect a set of relevant images from the web and perform image matching between them and the query. Additionally, we employ a text processing method customized for business recognition which is assisted by nearby business names; we fuse the information acquired from image matching and text processing in a probabilistic framework to recognize the businesses. We tested the proposed algorithm on a challenging set of user-uploaded and street view images with promising results for this new application.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {665–668},
numpages = {4},
keywords = {location based service, yelp, business recognition, multi-hypotheses, map, business review, storefront, scene text},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502175,
author = {Wolinski, David and Le Meur, Olivier and Gautier, Josselin},
title = {3D View Synthesis with Inter-View Consistency},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502175},
doi = {10.1145/2502081.2502175},
abstract = {In this paper, we propose a new pipeline to synthesize virtual views by extrapolation. It allows us to generate virtual views far away from each other, each presenting the exact same level of quality. This inter-view consistency is key to seamlessly navigate between viewpoints. Its computational cost is also lower than that of existing approaches. We compare the proposed approach with state-of-the-art methods and show the effectiveness of this new view synthesis pipeline.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {669–672},
numpages = {4},
keywords = {dibr, view synthesis, fvv, 3DTV},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502176,
author = {Tzelepis, Christos and Gkalelis, Nikolaos and Mezaris, Vasileios and Kompatsiaris, Ioannis},
title = {Improving Event Detection Using Related Videos and Relevance Degree Support Vector Machines},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502176},
doi = {10.1145/2502081.2502176},
abstract = {In this paper, a new method that exploits related videos for the problem of event detection is proposed, where related videos are videos that are closely but not fully associated with the event of interest. In particular, the Weighted Margin SVM formulation is modified so that related class observations can be effectively incorporated in the optimization problem. The resulting Relevance Degree SVM is especially useful in problems where only a limited number of training observations is provided, e.g., for the EK10Ex subtask of TRECVID MED, where only ten positive and ten related samples are provided for the training of a complex event detector. Experimental results on the TRECVID MED 2011 dataset verify the effectiveness of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {673–676},
numpages = {4},
keywords = {video event detection, relevance degree support vector machines, very few positive samples},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502177,
author = {Yan, Tao and He, Shengfeng and Lau, Rynson W.H. and Xu, Yun},
title = {Consistent Stereo Image Editing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502177},
doi = {10.1145/2502081.2502177},
abstract = {Stereo images and videos are very popular in recent years, and techniques for processing this media are attracting a lot of attention. In this paper, we extend the shift-map method for stereo image editing. Our method simultaneously processes the left and right images on pixel level using a global optimization algorithm. It enforces photo consistence between the two images and preserves 3D scene structures. It also addresses the occlusion and disocclusion problem, which may enable many stereo image editing functions, such as depth mapping, object depth adjustment and non-homogeneous image resizing. Our experiments show that the proposed method produces high quality results in various editing functions.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {677–680},
numpages = {4},
keywords = {stereo images editing, multi-label optimization, depth mapping},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502178,
author = {Bu, Shuhui and Liu, Zhenbao and Han, Junwei and Wu, Jun},
title = {Superpixel Segmentation Based Structural Scene Recognition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502178},
doi = {10.1145/2502081.2502178},
abstract = {This paper presents a novel structural model based scene recognition method. In order to resolve regular grid image division methods which cause low content discriminability for scene recognition in previous methods, we partition an image into a pre-defined set of regions by superpixel segmentation. And then classification is modelled by introducing a structural model which has the capability of organizing unordered features of image patches. In the implementation, CENTRIST which is robust to scene recognition is used as original image feature, and bag-of-words representation is used to capture the local appearances of an image. In addition, we incorporate adjacent superpixel's differences as edge features. Our models are trained using structural SVM. Two state-of-the-art scene datasets are adopted to evaluate the proposed method. The experiment results show that the recognition accuracy is significantly improved by the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {681–684},
numpages = {4},
keywords = {scene recognition, image segmentation, bag of words, superpixel, structural SVM},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502179,
author = {Wu, Song and Lew, Michael},
title = {Evaluation of Salient Point Methods},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502179},
doi = {10.1145/2502081.2502179},
abstract = {Processing visual content in images and videos is a challenging task associated with the development of modern computer vision. Because salient point approaches can represent distinctive and affine invariant points in images, many approaches have been proposed over the past decade. Each method has particular advantages and limitations and may be appropriate in different contexts. In this paper we evaluate the performance of a wide set of salient point detectors and descriptors. We begin by comparing diverse salient point algorithms (SIFT, SURF, BRIEF, ORB, FREAK, BRISK, STAR, GFTT and FAST) with regard to repeatability, recall and precision and then move to accuracy and stability in real-time video tracking.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {685–688},
numpages = {4},
keywords = {salient point methods, evaluation, video tracking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502180,
author = {Wang, Xikui and Liu, Yang and Wang, Donghui and Wu, Fei},
title = {Cross-Media Topic Mining on Wikipedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502180},
doi = {10.1145/2502081.2502180},
abstract = {As a collaborative wiki-based encyclopedia, Wikipedia provides a huge amount of articles of various categories. In addition to their text corpus, Wikipedia also contains plenty of images which makes the articles more intuitive for readers to understand. To better organize these visual and textual data, one promising area of research is to jointly model the embedding topics across multi-modal data (i.e, cross-media) from Wikipedia. In this work, we propose to learn the projection matrices that map the data from heterogeneous feature spaces into a unified latent topic space. Different from previous approaches, by imposing the l1 regularizers to the projection matrices, only a small number of relevant visual/textual words are associated with each topic, which makes our model more interpretable and robust. Furthermore, the correlations of Wikipedia data in different modalities are explicitly considered in our model. The effectiveness of the proposed topic extraction algorithm is verified by several experiments conducted on real Wikipedia datasets.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {689–692},
numpages = {4},
keywords = {wikipedia, sparsity, topic modeling, cross media},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502181,
author = {Tian, Yuan and Yang, Yin and Guo, Xiaohu and Prabhakaran, Balakrishnan},
title = {A Multigrid Approach for Bandwidth and Display Resolution Aware Streaming of 3D Deformations},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502181},
doi = {10.1145/2502081.2502181},
abstract = {In this paper, we propose a novel multimedia system adaptively streaming the animation according to display resolution and/or network bandwidth. A Multigrid-like technique is used in this framework to accelerate the converging rate of the optimization of the nonlinear deformation energy. The computation is performed from coarsest mesh at the top level to the finest mesh at the bottom level and then goes back to the top again. Such V-shape calculation provides great flexibility for the networked environment. Clients are able to receive the data streaming corresponding to its display resolution and network bandwidth. A more compact form of deformation data packaging is also used in this system such that a cube element only needs six parameters instead of 24 variables as used in regular mesh representation, which significantly reduces the network overhead for the streaming.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {693–696},
numpages = {4},
keywords = {3D streaming, deformation, multigrid},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502182,
author = {Zhu, Shiai and Wei, Xiao-Yong and Ngo, Chong-Wah},
title = {Error Recovered Hierarchical Classification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502182},
doi = {10.1145/2502081.2502182},
abstract = {Hierarchical classification (HC) is a popular and efficient way for detecting the semantic concepts from the images. However, the conventional HC, which always selects the branch with the highest classification response to go on, has the risk of propagating serious errors from higher levels of the hierarchy to the lower levels. We argue that the highest-response-first strategy is too arbitrary, because the candidate nodes are considered individually which ignores the semantic relationship among them. In this paper, we propose a novel method for HC, which is able to utilize the semantic relationship among candidate nodes and their children to recover the responses of unreliable classifiers of the candidate nodes, with the hope of providing the branch selection a more globally valid and semantically consistent view. The experimental results show that the proposed method outperforms the conventional HC methods and achieves a satisfactory balance between the accuracy and efficiency.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {697–700},
numpages = {4},
keywords = {large-scale hierarchy, error propagation, concept detection},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502183,
author = {Mironica, Ionut and Uijlings, Jasper and Rostamzadeh, Negar and Ionescu, Bogdan and Sebe, Nicu},
title = {Time Matters! Capturing Variation in Time in Video Using Fisher Kernels},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502183},
doi = {10.1145/2502081.2502183},
abstract = {In video global features are often used for reasons of computational efficiency, where each global feature captures information of a single video frame. But frames in video change over time, so an important question is: how can we meaningfully aggregate frame-based features in order to preserve the variation in time? In this paper we propose to use the Fisher Kernel to capture variation in time in video. While in this approach the temporal order is lost, it captures both subtle variation in time such as the ones caused by a moving bicycle and drastic variations in time such as the changing of shots in a documentary. Our work should not be confused with a Bag of Local Visual Features approach, where one captures the visual variation of local features in both time and space indiscriminately. Instead, each feature measures a complete frame hence we capture variation in time only.We show that our framework is highly general, reporting improvements using frame-based visual features, body-part features, and audio features on three diverse datasets: We obtain state-of-the-art results on the UCF50 human action dataset and improve the state-of-the-art on the MediaEval 2012 video-genre benchmark and on the ADL daily activity recognition dataset.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {701–704},
numpages = {4},
keywords = {video classification, variation in time, fisher kernels},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502184,
author = {Fran\c{c}oise, Jules and Schnell, Norbert and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {A Multimodal Probabilistic Model for Gesture--Based Control of Sound Synthesis},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502184},
doi = {10.1145/2502081.2502184},
abstract = {In this paper, we propose a multimodal approach to create the mapping between gesture and sound in interactive music systems. Specifically, we propose to use a multimodal HMM to conjointly model the gesture and sound parameters. Our approach is compatible with a learning method that allows users to define the gesture--sound relationships interactively. We describe an implementation of this method for the control of physical modeling sound synthesis. Our model is promising to capture expressive gesture variations while guaranteeing a consistent relationship between gesture and sound.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {705–708},
numpages = {4},
keywords = {HMM, gesture, music performance, music, multimodal, sound synthesis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502185,
author = {Serra, Giuseppe and Grana, Costantino and Manfredi, Marco and Cucchiara, Rita},
title = {Modeling Local Descriptors with Multivariate Gaussians for Object and Scene Recognition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502185},
doi = {10.1145/2502081.2502185},
abstract = {Common techniques represent images by quantizing local descriptors and summarizing their distribution in a histogram. In this paper we propose to employ a parametric description and compare its capabilities to histogram based approaches. We use the multivariate Gaussian distribution, applied over the SIFT descriptors, extracted with dense sampling on a spatial pyramid. Every distribution is converted to a high-dimensional descriptor, by concatenating the mean vector and the projection of the covariance matrix on the Euclidean space tangent to the Riemannian manifold. Experiments on Caltech-101 and ImageCLEF2011 are performed using the Stochastic Gradient Descent solver, which allows to deal with large scale datasets and high dimensional feature spaces.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {709–712},
numpages = {4},
keywords = {stochastic gradient descent, object recognition, image understanding, local features},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502186,
author = {Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},
title = {Anchor Concept Graph Distance for Web Image Re-Ranking},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502186},
doi = {10.1145/2502081.2502186},
abstract = {Web image re-ranking aims to automatically refine the initial text-based image search results by employing visual information. A strong line of work in image re-ranking relies on building image graphs that requires computing distances between image pairs. In this paper, we present Anchor Concept Graph Distance (ACG Distance), a novel distance measure for image re-ranking. For a given textual query, an Anchor Concept Graph (ACG) is automatically learned from the initial text-based search results. The nodes of the ACG (i.e., anchor concepts) and their correlations well model the semantic structure of the images to be re-ranked. Images are projected to the anchor concepts. The projection vectors undergo a diffusion process over the ACG, and then are used to compute the ACG distance. The ACG distance reduces the semantic gap and better represents distances between images. Experiments on the MSRA-MM and INRIA datasets show that the ACG distance consistently outperforms existing distance measures and significantly improves start-of-the-art methods in image re-ranking.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {713–716},
numpages = {4},
keywords = {anchor concept graph, semantic gap, web image search re-ranking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502187,
author = {Acar, Esra and Hopfgartner, Frank and Albayrak, Sahin},
title = {Violence Detection in Hollywood Movies by the Fusion of Visual and Mid-Level Audio Cues},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502187},
doi = {10.1145/2502081.2502187},
abstract = {Detecting violent scenes in movies is an important video content understanding functionality e.g., for providing automated youth protection services. One key issue in designing algorithms for violence detection is the choice of discriminative features. In this paper, we employ mid-level audio features and compare their discriminative power against low-level audio and visual features. We fuse these mid-level audio cues with low-level visual ones at the decision level in order to further improve the performance of violence detection. We use Mel-Frequency Cepstral Coefficients (MFCC) as audio and average motion as visual features. In order to learn a violence model, we choose two-class support vector machines (SVMs). Our experimental results on detecting violent video shots in Hollywood movies show that mid-level audio features are more discriminative and provide more precise results than low-level ones. The detection performance is further enhanced by fusing the mid-level audio cues with low-level visual ones using an SVM-based decision fusion.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {717–720},
numpages = {4},
keywords = {bag-of-audio-words, decision fusion, motion, mel-frequency cepstral coefficients, support vector machine},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502188,
author = {Raghuraman, Suraj and Venkatraman, Karthik and Wang, Zhanyu and Prabhakaran, Balakrishnan and Guo, Xiaohu},
title = {A 3D Tele-Immersion Streaming Approach Using Skeleton-Based Prediction},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502188},
doi = {10.1145/2502081.2502188},
abstract = {3D collaborative Tele-Immersive environments allow reconstruction of real world 3D scenes in the virtual world across multiple physical locations. This kind of reconstruction results in a lot of 3D data being transmitted over the internet in real time. The current systems allow for transmission at low frame rates due to the large volume of data and network bandwidth restrictions. In this paper we propose a prediction based approach that generates future frames by animating the live model based on few skeleton points. By doing so the magnitude of data transmitted is reduced to few hundred bytes. The prediction errors are corrected when an entire frame is received. This approach allows minimal amounts (few bytes) of data to be transmitted per frame, thus allowing for high frame rates and still maintain an acceptable visual quality of reconstruction at the receiver side.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {721–724},
numpages = {4},
keywords = {segmentation, tele-immersion, visualization, 3D reconstruction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502189,
author = {Tarashima, Shuhei and Irie, Go and Tsutsuguchi, Ken and Arai, Hiroyuki and Taniguchi, Yukinobu},
title = {Fast Image/Video Collection Summarization with Local Clustering},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502189},
doi = {10.1145/2502081.2502189},
abstract = {Image/video collection summarization is an emerging paradigm to provide an overview of contents stored in massive databases. Existing algorithms require at least O(N) time to generate a summary, which cannot be applied to online scenarios. Assuming that contents are represented as a sparse graph, we propose a fast image/video collection summarization algorithm using local graph clustering. After a query node is specified, our algorithm first finds a small sub-graph near the query without looking at the whole graph, and then selects fewer number of nodes diverse to each other. Our algorithm thus provides a summary in nearly constant time in the number of contents. Experimental results demonstrate that our algorithm is more than 1500 times faster than a state-of-the-art method, with comparable summarization quality.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {725–728},
numpages = {4},
keywords = {image/video collection, graph, summarization, multimodal, local clustering},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502190,
author = {Tasli, H. Emrah and van Gemert, Jan C. and Gevers, Theo},
title = {Spot the Differences: From a Photograph Burst to the Single Best Picture},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502190},
doi = {10.1145/2502081.2502190},
abstract = {With the rise of the digital camera, people nowadays typically take several near-identical photos of the same scene to maximize the chances of a good shot. This paper proposes a user-friendly tool for exploring a personal photo gallery for selecting or even creating the best shot of a scene between its multiple alternatives. This functionality is realized through a graphical user interface where the best viewpoint can be selected from a generated panorama of the scene. Once the viewpoint is selected, the user is able to go explore possible alternatives coming from the other images. Using this tool, one can explore a photo gallery efficiently. Moreover, additional compositions from other images are also possible. With such additional compositions, one can go from a burst of photographs to the single best one. Even funny compositions of images, where you can duplicate a person in the same image, are possible with our proposed tool.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {729–732},
numpages = {4},
keywords = {organizing personal photo albums},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502191,
author = {Yu, Qian and Liu, Jingen and Cheng, Hui and Divakaran, Ajay and Sawhney, Harpreet},
title = {Semantic Pooling for Complex Event Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502191},
doi = {10.1145/2502081.2502191},
abstract = {Complex event detection is very challenging in open source such as You-Tube videos, which usually comprise very diverse visual contents involving various object, scene and action concepts. Not all of them, however, are relevant to the event. In other words, a video may contain a lot of "junk" information which is harmful for recognition. Hence, we propose a semantic pooling approach to tackle this issue. Unlike the conventional pooling over the entire video or specific spatial regions of a video, we employ a discriminative approach to acquire abstract semantic "regions" for pooling. For this purpose, we first associate low-level visual words with semantic concepts via their co-occurrence relationship. We then pool the low-level features separately according to their semantic information. The proposed semantic pooling strategy also provides a new mechanism for incorporating semantic concepts for low-level feature based event recognition. We evaluate our approach on TRECVID MED [1] dataset and the results show that semantic pooling consistently improves the performance compared with conventional pooling strategies.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {733–736},
numpages = {4},
keywords = {complex event detection, semantic pooling, multimedia event representation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502192,
author = {Legrady, George and Bazo, Danny and Pinter, Marco},
title = {SwarmVision: Autonomous Aesthetic Multi-Camera Interaction},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502192},
doi = {10.1145/2502081.2502192},
abstract = {A platform of exploratory networked robotic cameras was created, utilizing an aesthetic approach to experimentation. Initiated by research in autonomous swarm robotic camera behavior, SwarmVision is an installation consisting of multiple Pan-Tilt-Zoom cameras on rails positioned above spectators in an exhibition space, where each camera behaves autonomously based on its own rules of computer vision and control. Each of the cameras is programmed to detect visual information of interest based on a different algorithm, and each negotiates with the other two, influencing what subject matter to study in a collective way. The emergent behaviors of the system illustrate an ongoing process of scene},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {737–740},
numpages = {4},
keywords = {robotics camera system, media arts},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502193,
author = {Pauwels, Johan and Peeters, Geoffroy},
title = {Segmenting Music through the Joint Estimation of Keys, Chords and Structural Boundaries},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502193},
doi = {10.1145/2502081.2502193},
abstract = {In this paper, we introduce a new approach to music structure segmentation that is based on the joint estimation of structural segments, keys and chords in one probabilistic framework. More precisely, the boundaries of a structure segment are determined by detecting key changes and by utilizing the difference in prior probability of chord transitions according to their position in a structural segment. In contrast to many of the recent approaches to structural segmentation, this system does not work with self-similarity matrices, although it has been designed to integrate this kind of approach into the framework at a later stage. However, just the current version of the system, using only the estimated harmony, is already producing encouraging results, especially with respect to the precise localization of the boundaries.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {741–744},
numpages = {4},
keywords = {key and chord estimation, music information retrieval, structural segmentation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502194,
author = {Jain, Aadhar and Arefin, Ahsan and Rivas, Raoul and Chen, Chien-nan and Nahrstedt, Klara},
title = {3D Teleimmersive Activity Classification Based on Application-System Metadata},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502194},
doi = {10.1145/2502081.2502194},
abstract = {Being able to detect and recognize human activities is essential for 3D collaborative applications for efficient quality of service provisioning and device management. A broad range of research has been devoted to analyze media data to identify human activity, which requires the knowledge of data format, application-specific coding technique and computationally expensive image analysis. In this paper, we propose a human activity detection technique based on application generated metadata and related system metadata. Our approach does not depend on specific data format or coding technique. We evaluate our algorithm with different cyber-physical setups, and show that we can achieve very high accuracy (above 97%) by using a good learning model.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {745–748},
numpages = {4},
keywords = {algorithms, activity, classification, design, performance, 3D tele-immersion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502195,
author = {Li, Yong and Liu, Jing and Li, Zechao and Liu, Yang and Lu, Hanqing},
title = {Object Co-Segmentation via Discriminative Low Rank Matrix Recovery},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502195},
doi = {10.1145/2502081.2502195},
abstract = {The goal of this paper is to simultaneously segment the object regions appearing in a set of images of the same object class, known as object co-segmentation. Different from typical methods, simply assuming that the regions common among images are the object regions, we additionally consider the disturbance from consistent backgrounds, and indicate not only common regions but salient ones among images to be the object regions. To this end, we propose a Discriminative Low Rank matrix Recovery (DLRR) algorithm to divide the over-completely segmented regions (i.e.,superpixels) of a given image set into object and non-object ones. In DLRR, a low-rank matrix recovery term is adopted to detect salient regions in an image, while a discriminative learning term is used to distinguish the object regions from all the super-pixels. An additional regularized term is imported to jointly measure the disagreement between the predicted saliency and the objectiveness probability corresponding to each super-pixel of the image set. For the unified learning problem by connecting the above three terms, we design an efficient optimization procedure based on block-coordinate descent. Extensive experiments are conducted on two public datasets, i.e., MSRC and iCoseg, and the comparisons with some state-of-the-arts demonstrate the effectiveness of our work.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {749–752},
numpages = {4},
keywords = {object co-segmentation, discriminative learning, low rank matrix recovery},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502196,
author = {Tang, Siliang and Wang, Hanqi and Shao, Jian and Wu, Fei and Chen, Ming and Zhuang, Yueting},
title = {πLDA: Document Clustering with Selective Structural Constraints},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502196},
doi = {10.1145/2502081.2502196},
abstract = {Segments, such as sentence boundaries in texts or annotated regions in images, can be considered as useful structural constraints (i.e., priors) for unsupervised topic modeling. However, some segment units (e.g., words in texts or visual words in images) inside a given segment may be irrelevant to the topic of this segment due to their characteristics. This paper proposes a model called πLDA, which introduces a latent variable π into LDA, a traditional topic model, to capture the characteristic of each segment unit. That is to say, the πLDA model is conducted to determine whether a segment unit is assigned (or selected) to the topic embedded in its corresponding segment. Compared with other approaches that assume all the segment units in one segment to share a common topic, our proposed πLDA has the selective ability to discover the discriminative segment units (e.g., informative words or visual words). Experimental results and interpretations of them are presented for demonstrating the promising performance of our method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {753–756},
numpages = {4},
keywords = {selective structural constraints, topic modeling, latent dirichlet allocation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502197,
author = {Karaoglu, Sezer and van Gemert, Jan C. and Gevers, Theo},
title = {Con-Text: Text Detection Using Background Connectivity for Fine-Grained Object Classification},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502197},
doi = {10.1145/2502081.2502197},
abstract = {This paper focuses on fine-grained classification by detecting photographed text in images. We introduce a text detection method that does not try to detect all possible foreground text regions but instead aims to reconstruct the scene background to eliminate non-text regions. Object cues such as color, contrast, and objectiveness are used in corporation with a random forest classifier to detect background pixels in the scene. Results on two publicly available datasets ICDAR03 and a fine-grained Building subcategories of ImageNet shows the effectiveness of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {757–760},
numpages = {4},
keywords = {fine-grained classification, scene text recognition, scene text detection, multimodal fusion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502198,
author = {Kim, Jongpil and Yoon, Sejong and Pavlovic, Vladimir},
title = {Relative Spatial Features for Image Memorability},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502198},
doi = {10.1145/2502081.2502198},
abstract = {Recent studies in image memorability showed that the memorability of an image is a measurable quantity and is closely correlated with semantic attributes. However, the intrinsic characteristics of memorability are not yet fully understood. It has been reported that in contrast to a popular belief unusualness or aesthetic beauty of the image may not be positively correlated with the image memorability. This counter-intuitive characteristic of memorability hinders a better understanding of image memorability and its applicability. In this paper, we investigate two new spatial features that are closely correlated with the image memorability yet intuitively explainable. We propose the Weighted Object Area (WOA) that jointly considers the location and size of objects and the Relative Area Rank (RAR) that captures the relative unusualness of the size of objects. We empirically demonstrate their useful correlation with the image memorability. Results show that both WOA and RAR can improve the memorability prediction. In addition, we provide evidence that the RAR can effectively capture object-centric unusualness of size.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {761–764},
numpages = {4},
keywords = {feature extraction, image memorability, relative correlation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502199,
author = {Franken, Morris and van Gemert, Jan C.},
title = {Automatic Egyptian Hieroglyph Recognition by Retrieving Images as Texts},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502199},
doi = {10.1145/2502081.2502199},
abstract = {In this paper we propose an approach for automatically recognizing ancient Egyptian hieroglyph from photographs. To this end we first manually annotated and segmented a large collection of nearly 4,000 hieroglyphs. In our automatic approach we localize and segment each individual hieroglyph, determine the reading order and subsequently evaluate 5 visual descriptors in 3 different matching schemes to evaluate visual hieroglyph recognition. In addition to visual-only cues, we use a corpus of Egyptian texts to learn language models that help re-rank the visual output.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {765–768},
numpages = {4},
keywords = {automatic recognition, Egyptian, hieroglyphs},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502200,
author = {Wang, Jialong and Deng, Cheng and Liu, Wei and Ji, Rongrong and Chen, Xiangyu and Gao, Xinbo},
title = {Query-Dependent Visual Dictionary Adaptation for Image Reranking},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502200},
doi = {10.1145/2502081.2502200},
abstract = {Although text-based image search engines are popular for ranking images of user's interest, the state-of-the-art ranking performance is still far from satisfactory. One major issue comes from the visual similarity metric used in the ranking operation, which depends solely on visual features. To tackle this issue, one feasible method is to incorporate semantic concepts, also known as image attributes, into image ranking. However, the optimal combination of visual features and image attributes remains unknown. In this paper, we propose a query-dependent image reranking approach by leveraging the higher level attribute detection among the top returned images to adapt the dictionary built over the visual features to a query-specific fashion. We start from offline learning transposition probabilities between visual codewords and attributes, then utilize the probabilities to online adapt the dictionary, and finally produce a query-dependent and semantics-induced metric for image ranking. Extensive evaluations on several benchmark image datasets demonstrate the effectiveness and efficiency of the proposed approach in comparison with state-of-the-arts.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {769–772},
numpages = {4},
keywords = {query dependent, image reranking, dictionary adaptation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502201,
author = {Nicolaou, Mihalis A. and Zafeiriou, Stefanos and Pantic, Maja},
title = {Correlated-Spaces Regression for Learning Continuous Emotion Dimensions},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502201},
doi = {10.1145/2502081.2502201},
abstract = {Adopting continuous dimensional annotations for affective analysis has been gaining rising attention by researchers over the past years. Due to the idiosyncratic nature of this problem, many subproblems have been identified, spanning from the fusion of multiple continuous annotations to exploiting output-correlations amongst emotion dimensions. In this paper, we firstly empirically answer several important questions which have found partial or no answer at all so far in related literature. In more detail, we study the correlation of each emotion dimension (i) with respect to other emotion dimensions, (ii) to basic emotions (e.g., happiness, anger). As a measure for comparison, we use video and audio features. Interestingly enough, we find that (i) each emotion dimension is more correlated with other emotion dimensions rather than with face and audio features, and similarly (ii) that each basic emotion is more correlated with emotion dimensions than with audio and video features. A similar conclusion holds for discrete emotions which are found to be highly correlated to emotion dimensions as compared to audio and/or video features. Motivated by these findings, we present a novel regression algorithm (Correlated-Spaces Regression, CSR), inspired by Canonical Correlation Analysis (CCA) which learns output-correlations and performs supervised dimensionality reduction and multimodal fusion by (i) projecting features extracted from all modalities and labels onto a common space where their inter-correlation is maximised and (ii) learning mappings from the projected feature space onto the projected, uncorrelated label space.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {773–776},
numpages = {4},
keywords = {continuous and dimensional emotion descriptions, feature selection, arousal, valence, component analysis, output-correlations, multi-modal fusion},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502202,
author = {Lin, Chien-Pang and Wang, Cheng-Yao and Chen, Hou-Ren and Chu, Wei-Chen and Chen, Mike Y.},
title = {RealSense: Directional Interaction for Proximate Mobile Sharing Using Built-in Orientation Sensors},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502202},
doi = {10.1145/2502081.2502202},
abstract = {We present RealSense, a technology that enables users to easily share media files with proximate users by performing directional gestures on mobile devices. RealSense leverages the natural human group behavior of forming a circle and facing the center of the group. By continuously monitoring the directional heading of each device using only built-in orientation sensors, RealSense can compute the relative direction between all the devices. It simplifies media sharing because users do not need to lookup and specify the user IDs and device IDs of the intended recipients. We first evaluated the feasibility and design of RealSense, including the orientation sensor error and the minimal arc degree for selecting recipients. We then compared RealSense with three other common sharing interactions: 1) linear menu, 2) pie menu, and 3) NFC. Our results show that participants preferred RealSense over other sharing interactions, especially for groups of participants who were unacquainted with each other.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {777–780},
numpages = {4},
keywords = {orientation sensor, relative direction, directional interaction, mobile devices, media sharing, proximate},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502203,
author = {Chen, Tao and Lu, Dongyuan and Kan, Min-Yen and Cui, Peng},
title = {Understanding and Classifying Image Tweets},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502203},
doi = {10.1145/2502081.2502203},
abstract = {Social media platforms now allow users to share images alongside their textual posts. These image tweets make up a fast-growing percentage of tweets, but have not been studied in depth unlike their text-only counterparts. We study a large corpus of image tweets in order to uncover what people post about and the correlation between the tweet's image and its text. We show that an important functional distinction is between visually-relevant and visually-irrelevant tweets, and that we can successfully build an automated classifier utilizing text, image and social context features to distinguish these two classes, obtaining a macro F1 of 70.5%.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {781–784},
numpages = {4},
keywords = {microblog, analysis, image tweets, classification},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502204,
author = {Yang, Yun and Cui, Peng and Zhu, Wenwu and Yang, Shiqiang},
title = {User Interest and Social Influence Based Emotion Prediction for Individuals},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502204},
doi = {10.1145/2502081.2502204},
abstract = {Emotions are playing significant roles in daily life, making emotion prediction important. To date, most of state-of-the-art methods make emotion prediction for the masses which are invalid for individuals. In this paper, we propose a novel emotion prediction method for individuals based on user interest and social influence. To balance user interest and social influence, we further propose a simple yet efficient weight learning method in which the weights are obtained from users' behaviors. We perform experiments in real social media network, with 4,257 users and 2,152,037 microblogs. The experimental results demonstrate that our method outperforms traditional methods with significant performance gains.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {785–788},
numpages = {4},
keywords = {social network, social influence, user interest, emotion prediction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502207,
author = {Rudovic, Ognjen and Petridis, Stavros and Pantic, Maja},
title = {Bimodal Log-Linear Regression for Fusion of Audio and Visual Features},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502207},
doi = {10.1145/2502081.2502207},
abstract = {One of the most commonly used audiovisual fusion approaches is feature-level fusion where the audio and visual features are concatenated. Although this approach has been successfully used in several applications, it does not take into account interactions between the features, which can be a problem when one and/or both modalities have noisy features. In this paper, we investigate whether feature fusion based on explicit modelling of interactions between audio and visual features can enhance the performance of the classifier that performs feature fusion using simple concatenation of the audio-visual features. To this end, we propose a log-linear model, named Bimodal Log-linear regression, which accounts for interactions between the features of the two modalities. The performance of the target classifiers is measured in the task of laughter-vs-speech discrimination, since both laughter and speech are naturally audiovisual events. Our experiments on the MAHNOB laughter database suggest that feature fusion based on explicit modelling of interactions between the audio-visual features leads to an improvement of 3% over the standard feature concatenation approach, when log-linear model is used as the base classifier. Finally, the most and least influential features can be easily identified by observing their interactions.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {789–792},
numpages = {4},
keywords = {audiovisual fusion, bimodalss log-linear regression-based fusion, laughter classification},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245296,
author = {Cucchiara, Rita},
title = {Session Details: Security and Forensics},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245296},
doi = {10.1145/3245296},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502121,
author = {Feng, Ranran and Prabhakaran, Balakrishnan},
title = {Facilitating Fashion Camouflage Art},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502121},
doi = {10.1145/2502081.2502121},
abstract = {Artists and fashion designers have recently been creating a new form of art -- Camouflage Art -- which can be used to prevent computer vision algorithms from detecting faces. This digital art technique combines makeup and hair styling, or other modifications such as facial painting to help avoid automatic face-detection. In this paper, we first study the camouflage interference and its effectiveness on several current state of art techniques in face detection/recognition; and then present a tool that can facilitate digital art design for such camouflage that can fool these computer vision algorithms. This tool can find the prominent or decisive features from facial images that constitute the face being recognized; and give suggestions for camouflage options (makeup, styling, paints) on particular facial features or facial parts. Testing of this tool shows that it can effectively aid the artists or designers in creating camouflage-thwarting designs. The evaluation on suggested camouflages applied on 40 celebrities across eight different face recognition systems (both non-commercial or commercial) shows that 82.5% ~ 100% of times the subject is unrecognizable using the suggested camouflage.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {793–802},
numpages = {10},
keywords = {fashion camouflage art, face recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502105,
author = {Zheng, Peijia and Huang, Jiwu},
title = {An Efficient Image Homomorphic Encryption Scheme with Small Ciphertext Expansion},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502105},
doi = {10.1145/2502081.2502105},
abstract = {The field of image processing in the encrypted domain has been given increasing attention for the extensive potential applications, for example, providing efficient and secure solutions for privacy-preserving applications in untrusted environment. One obstacle to the widespread use of these techniques is the ciphertext expansion of high orders of magnitude caused by the existing homomorphic encryptions. In this paper, we provide a way to tackle this issue for image processing in the encrypted domain. By using characteristics of image format, we develop an image encryption scheme to limit ciphertext expansion while preserving the homomorphic property. The proposed encryption scheme first encrypts image pixels with an existing probabilistic homomorphic cryptosystem, and then compresses the whole encrypted image in order to save storage space. Our scheme has a much smaller ciphertext expansion factor compared with the element-wise encryption scheme, while preserving the homomorphic property. It is not necessary to require additional interactive protocols when applying secure signal processing tools to the compressed encrypted image. We present a fast algorithm for the encryption and the compression of the proposed image encryption scheme, which speeds up the computation and makes our scheme much more efficient. The analysis on the security, ciphertext expansion ratio, and computational complexity are also conducted. Our experiments demonstrate the validity of the proposed algorithms. The proposed scheme is suitable to be employed as an image encryption method for the applications in secure image processing.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {803–812},
numpages = {10},
keywords = {efficient scheme, secure signal processing, image encryption, ciphertext expansion reducing, homomorphic property, image processing in the encrypted domain},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502082,
author = {Sethi, Ricky J. and Gil, Yolanda and Jo, Hyunjoon and Philpot, Andrew},
title = {Large-Scale Multimedia Content Analysis Using Scientific Workflows},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502082},
doi = {10.1145/2502081.2502082},
abstract = {Analyzing web content, particularly multimedia content, for security applications is of great interest. However, it often requires deep expertise in data analytics that is not always accessible to non-experts. Our approach is to use scientific workflows that capture expert-level methods to examine web content. We use workflows to analyze the image and text components of multimedia web posts separately, as well as by a multimodal fusion of both image and text data. In particular, we re-purpose workflow fragments to do the multimedia analysis and create additional components for the fusion of the image and text modalities. In this paper, we present preliminary work which focuses on a Human Trafficking Detection task to help deter human trafficking of minors by thus fusing image and text content from the web. We also examine how workflow fragments save time and effort in multimedia content analysis while bringing together multiple areas of machine learning and computer vision. We further export these workflow fragments using linked data as web objects.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {813–822},
numpages = {10},
keywords = {multimodal information fusion, big data, human trafficking detection, social media analysis, scientific workflows},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245297,
author = {Patras, Yiannis and Vedaldi, Andrea},
title = {Session Details: Open Source Software},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245297},
doi = {10.1145/3245297},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502221,
author = {Hildebrand, Michiel and Brinkerink, Maarten and Gligorov, Riste and van Steenbergen, Martijn and Huijkman, Johan and Oomen, Johan},
title = {Waisda? Video Labeling Game},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502221},
doi = {10.1145/2502081.2502221},
abstract = {The Waisda? video labeling game is a crowsourcing tool to collect user-generated metadata for video clips. It follows the paradigm of games-with-a-purpose, where two or more users play against each other by entering tags that describe the content of the video. Players score points by entering the same tags as one of the other players. As a result each video that is played in the game is annotated with tags that are anchored to a time point in the video. Waisda? has been deployed in two projects with videos from Dutch broadcasters. With the open source version of Waisda? crowdsourcing of video annotation becomes available for any online video collection.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {823–826},
numpages = {4},
keywords = {crowdsourcing, gwap, Waisda, video tagging},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502222,
author = {Huang, Chun-Ying and Chen, De-Yu and Hsu, Cheng-Hsin and Chen, Kuan-Ta},
title = {GamingAnywhere: An Open-Source Cloud Gaming Testbed},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502222},
doi = {10.1145/2502081.2502222},
abstract = {While cloud gaming opens new business opportunity, it also poses tremendous challenges as the Internet only provides best-effort service and gamers are hard to please. Although researchers have various ideas to improve cloud gaming systems, existing cloud gaming systems are closed and proprietary, and cannot be used to evaluate these ideas. We present GamingAnywhere, the first open-source cloud gaming system, which is extensible, portable, and configurable. GamingAnywhere may be used by: (i) researchers and engineers to implement and test their new ideas, (ii) service providers to develop cloud gaming services, and (iii) gamers to set up private cloud gaming systems. Details on GamingAnywhere are given in this paper. We firmly believe GamingAnywhere will stimulate future studies on cloud gaming and real-time interactive distributed systems.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {827–830},
numpages = {4},
keywords = {remote gaming, performance evaluation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502223,
author = {Wagner, Johannes and Lingenfelser, Florian and Baur, Tobias and Damian, Ionut and Kistler, Felix and Andr\'{e}, Elisabeth},
title = {The Social Signal Interpretation (SSI) Framework: Multimodal Signal Processing and Recognition in Real-Time},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502223},
doi = {10.1145/2502081.2502223},
abstract = {Automatic detection and interpretation of social signals carried by voice, gestures, mimics, etc. will play a key-role for next-generation interfaces as it paves the way towards a more intuitive and natural human-computer interaction. The paper at hand introduces Social Signal Interpretation (SSI), a framework for real-time recognition of social signals. SSI supports a large range of sensor devices, filter and feature algorithms, as well as, machine learning and pattern recognition tools. It encourages developers to add new components using SSI's C++ API, but also addresses front end users by offering an XML interface to build pipelines with a text editor. SSI is freely available under GPL at http://openssi.net.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {831–834},
numpages = {4},
keywords = {real-time pattern recognition, multimodal fusion, social signal processing, open source framework},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502224,
author = {Eyben, Florian and Weninger, Felix and Gross, Florian and Schuller, Bj\"{o}rn},
title = {Recent Developments in OpenSMILE, the Munich Open-Source Multimedia Feature Extractor},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502224},
doi = {10.1145/2502081.2502224},
abstract = {We present recent developments in the openSMILE feature extraction toolkit. Version 2.0 now unites feature extraction paradigms from speech, music, and general sound events with basic video features for multi-modal processing. Descriptors from audio and video can be processed jointly in a single framework allowing for time synchronization of parameters, on-line incremental processing as well as off-line and batch processing, and the extraction of statistical functionals (feature summaries), such as moments, peaks, regression parameters, etc. Postprocessing of the features includes statistical classifiers such as support vector machine models or file export for popular toolkits such as Weka or HTK. Available low-level descriptors include popular speech, music and video features including Mel-frequency and similar cepstral and spectral coefficients, Chroma, CENS, auditory model based loudness, voice quality, local binary pattern, color, and optical flow histograms. Besides, voice activity detection, pitch tracking and face detection are supported. openSMILE is implemented in C++, using standard open source libraries for on-line audio and video input. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. openSMILE 2.0 is distributed under a research license and can be downloaded from http://opensmile.sourceforge.net/.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {835–838},
numpages = {4},
keywords = {computational paralinguistics, affect recognition, video features, affective computing, multimedia analysis, openSMILE, acoustic features, feature extraction, machine learning, visual features, audio features},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502225,
author = {Tsampoulatidis, Ioannis and Ververidis, Dimitrios and Tsarchopoulos, Panagiotis and Nikolopoulos, Spiros and Kompatsiaris, Ioannis and Komninos, Nicos},
title = {ImproveMyCity: An Open Source Platform for Direct Citizen-Government Communication},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502225},
doi = {10.1145/2502081.2502225},
abstract = {ImproveMyCity is an open source platform that enables residents to directly report to their public administration local issues about their neighborhood such as discarded trash bins, faulty street lights, broken tiles on sidewalks, illegal advertising boards, etc. The reported issues are automatically transmitted to the appropriate office in public administration so as to schedule their settlement. Reporting is feasible both through a web- and a smartphone-based front-end that adopt a map-based visualization, which makes reporting a user-friendly and intriguing process. The management and routing of incoming issues is performed through a back-end infrastructure that serves as an integrated management system with easy to use interfaces. Apart from reporting a new issue, both front-ends allow the citizens to add comments or vote on existing issues, which adds a social dimension on the collected content. Finally, the platform makes also provision for informing the citizens about the progress status of the reported issue and in this way facilitate the establishment of a two-way dialogue between the citizen and public administration.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {839–842},
numpages = {4},
keywords = {egovernment, urban maintenance, open source, issue reporting, eparticipation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502226,
author = {Lux, Mathias},
title = {LIRE: Open Source Image Retrieval in Java},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502226},
doi = {10.1145/2502081.2502226},
abstract = {Content based image retrieval has been around for some time. There are lots of different test data sets, lots of published methods and techniques, and manifold retrieval challenges, where content based image retrieval is of interest. LIRE is a Java library, that provides a simple way to index and retrieve millions of images based on the images' contents. LIRE is robust and well tested and is not only recommended by the websites of ImageCLEF and MediaEval, but is also employed in industry. This paper gives an overview on LIRE, its use, capabilities and reports on retrieval and runtime performance.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {843–846},
numpages = {4},
keywords = {content based image retrieval},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502227,
author = {Tsochatzidis, Lazaros T. and Iakovidou, Chryssanthi and Chatzichristofis, Savvas A. and Boutalis, Yiannis S.},
title = {Golden Retriever: A Java Based Open Source Image Retrieval Engine},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502227},
doi = {10.1145/2502081.2502227},
abstract = {Golden Retriever Image Retrieval Engine (GRire) is an open source light weight Java library developed for Content Based Image Retrieval (CBIR) tasks, employing the Bag of Visual Words (BOVW) model. It provides a complete framework for creating CBIR system including image analysis tools, classifiers, weighting schemes etc., for efficient indexing and retrieval procedures. Its eminent feature is its extensibility, achieved through the open source nature of the library as well as a user-friendly embedded plug-in system. GRire is available on-line along with install and development documentation on http://www.grire.net and on its Google Code page http://code.google.com/p/grire. It is distributed either as a Java library or as a standalone Java application, both GPL licensed.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {847–850},
numpages = {4},
keywords = {image indexing, image search, bag-of-visual-words, visual words, image retrieval, open source},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502228,
author = {Aamulehto, Rami and Kuhna, Mikko and Tarvainen, Jussi and Oittinen, Pirkko},
title = {Stage Framework: An HTML5 and CSS3 Framework for Digital Publishing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502228},
doi = {10.1145/2502081.2502228},
abstract = {In this paper we present Stage Framework, an HTML5 and CSS3 framework for digital book, magazine and newspaper publishing. The framework offers publishers the means and tools for publishing editorial content in the HTML5 format using a single web application. The approach is cross-platform and is based on open web standards. Stage Framework serves as an alternative for platform-specific native publications using pure HTML5 to deliver book, magazine and newspaper content while retaining the familiar gesture interaction of native applications. Available gesture actions include for example the page swipe and kinetic scrolling. The magazine browsing view relies entirely on CSS3 3D Transforms and Transitions, thus utilizing hardware acceleration in most devices and platforms. The web application also features a magazine stand which, can be used to offer issues of multiple publications. Developed as a part of master's thesis research, the framework has been published under the GPL and MIT licenses and is available to everyone via the framework website (http://stageframework.com) and the GitHub repository (http://github.com/ralatalo/stage).},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {851–854},
numpages = {4},
keywords = {HTML5, CSS3, digital publishing, web application, browser technology, tablet magazine},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502229,
author = {Bogdanov, Dmitry and Wack, Nicolas and G\'{o}mez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jos\'{e} and Serra, Xavier},
title = {ESSENTIA: An Open-Source Library for Sound and Music Analysis},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502229},
doi = {10.1145/2502081.2502229},
abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {855–858},
numpages = {4},
keywords = {open source, music information retrieval, audio analysis, signal processing, sound and music computing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502230,
author = {Flynn, Carl and Monaghan, David and O'Connor, Noel E.},
title = {SCReen Adjusted Panoramic Effect: SCRAPE},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502230},
doi = {10.1145/2502081.2502230},
abstract = {A Cave Automatic Virtual Environment (CAVE) is an enclosed virtual reality room that uses multiple projectors to display images across its surfaces. It uses one or more computers to synchronise and combine the images and allows users to control virtual worlds using a host of interaction devices. Traditionally, a CAVE is used by a single user at any one time and by utilising some form of motion sensing, the user's head position can be tracked to allow for first virtual perception. The images are then displayed in stereographic 3D in order to complete the virtual reality effect. Professional CAVE installations are expensive and can cost upwards of several hundred thousand euros. This tends to act as a significant barrier to their propagation, however, as the reduction in cost of high specification computers, projectors and graphics cards continues apace, it has sparked a renewed interest in CAVE environments and given rise to the realistic possibility of setting up low cost, amateur CAVEs. Unfortunately, one of the greatest disadvantages of CAVE systems is the lack of inexpensive, easy to use, specialised software. In this paper we present an open source and easy to use CAVE software toolkit called SCReen Adjusted Panoramic Effect or SCRAPE for short. We believe that SCRAPE is the first major piece in a longer-term vision that aims to bring easy to setup, easy to use, portable CAVE systems to all types of non-expert users.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {859–862},
numpages = {4},
keywords = {processing, virtual reality, 3D, SCRAPE, cave},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502231,
author = {Yviquel, Herve and Lorence, Antoine and Jerbi, Khaled and Cocherel, Gildas and Sanchez, Alexandre and Raulet, Mickael},
title = {Orcc: Multimedia Development Made Easy},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502231},
doi = {10.1145/2502081.2502231},
abstract = {In this paper, we present Orcc, an open-source development environment that aims at enhancing multimedia development by offering all the advantages of dataflow programming: flexibility, portability and scalability. To do so, Orcc embeds two rich eclipse-based editors that provide an easy writing of dataflow applications, a simulator that allows quick validation of the written code, and a multi-target compiler that is able to translate any dataflow program, written in the RVC-CAL language, into an equivalent description in both hardware and software languages. Orcc has already been used to successfully write tens of multimedia applications, such as a video decoder supporting the new High Efficiency Video Coding standard, that clearly demonstrates the ability of the environment to develop complex applications. Moreover, results show scalable performances on multi-core platforms and achieve real-time decoding frame-rate on HD sequences.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {863–866},
numpages = {4},
keywords = {integrated development environment (IDE), mpeg reconfigurable media coding (RMC), multimedia development, dataflow programming, high efficiency video coding (HEVC)},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245298,
author = {Cooper, Matt},
title = {Session Details: Multimodal Analysis},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245298},
doi = {10.1145/3245298},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502123,
author = {Choi, Jaeyoung and Lei, Howard and Ekambaram, Venkatesan and Kelm, Pascal and Gottlieb, Luke and Sikora, Thomas and Ramchandran, Kannan and Friedland, Gerald},
title = {Human vs Machine: Establishing a Human Baseline for Multimodal Location Estimation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502123},
doi = {10.1145/2502081.2502123},
abstract = {Over the recent years, the problem of video location estimation (i.e., estimating the longitude/latitude coordinates of a video without GPS information) has been approached with diverse methods and ideas in the research community and significant improvements have been made. So far, however, systems have only been compared against each other and no systematic study on human performance has been conducted. Based on a human-subject study with 11,900 experiments, this article presents a human baseline for location estimation for different combinations of modalities (audio, audio/video, audio/video/text). Furthermore, this article compares state-of-the-art location estimation systems with the human baseline. Although the overall performance of humans' multimodal video location estimation is better than current machine learning approaches, the difference is quite small: For 41% of the test set, the machine's accuracy was superior to the humans. We present case studies and discuss why machines did better for some videos and not for others. Our analysis suggests new directions and priorities for future work on the improvement of location inference algorithms.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {867–876},
numpages = {10},
keywords = {multimodal, location estimation, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502097,
author = {Wu, Fei and Lu, Xinyan and Zhang, Zhongfei and Yan, Shuicheng and Rui, Yong and Zhuang, Yueting},
title = {Cross-Media Semantic Representation via Bi-Directional Learning to Rank},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502097},
doi = {10.1145/2502081.2502097},
abstract = {In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. Existing approaches take either one-to-one paired data or uni-directional ranking examples (i.e., utilizing only text-query-image ranking examples or image-query-text ranking examples) as training examples, which do not make full use of bi-directional ranking examples (bi-directional ranking means that both text-query-image and image-query-text ranking examples are utilized in the training period) to achieve a better performance. In this paper, we consider learning a cross-media representation model from the perspective of optimizing a listwise ranking problem while taking advantage of bi-directional ranking examples. We propose a general cross-media ranking algorithm to optimize the bi-directional listwise ranking loss with a latent space embedding, which we call Bi-directional Cross-Media Semantic Representation Model (Bi-CMSRM). The latent space embedding is discriminatively learned by the structural large margin learning for optimization with certain ranking criteria (mean average precision in this paper) directly. We evaluate Bi-CMSRM on the Wikipedia and NUS-WIDE datasets and show that the utilization of the bi-directional ranking examples achieves a much better performance than only using the uni-directional ranking examples.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {877–886},
numpages = {10},
keywords = {cross-media representation, bi-directional learning to rank, latent space embedding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502084,
author = {Liu, Wu and Mei, Tao and Zhang, Yongdong and Li, Jintao and Li, Shipeng},
title = {Listen, Look, and Gotcha: Instant Video Search with Mobile Phones by Layered Audio-Video Indexing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502084},
doi = {10.1145/2502081.2502084},
abstract = {Mobile video is quickly becoming a mass consumer phenomenon. More and more people are using their smartphones to search and browse video content while on the move. In this paper, we have developed an innovative instant mobile video search system through which users can discover videos by simply pointing their phones at a screen to capture a very few seconds of what they are watching. The system is able to index large-scale video data using a new layered audio-video indexing approach in the cloud, as well as extract light-weight joint audio-video signatures in real time and perform progressive search on mobile devices. Unlike most existing mobile video search applications that simply send the original video query to the cloud, the proposed mobile system is one of the first attempts at instant and progressive video search leveraging the light-weight computing capacity of mobile devices. The system is characterized by four unique properties: 1) a joint audio-video signature to deal with the large aural and visual variances associated with the query video captured by the mobile phone, 2) layered audio-video indexing to holistically exploit the complementary nature of audio and video signals, 3) light-weight fingerprinting to comply with mobile processing capacity, and 4) a progressive query process to significantly reduce computational costs and improve the user experience---the search process can stop anytime once a confident result is achieved. We have collected 1,400 query videos captured by 25 mobile users from a dataset of 600 hours of video. The experiments show that our system outperforms state-of-the-art methods by achieving 90.79% precision when the query video is less than 10 seconds and 70.07% even when the query video is less than 5 seconds.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {887–896},
numpages = {10},
keywords = {audio-video signatures, progressive query process, layered audio-video indexing, mobile video search, instant search},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502087,
author = {Mao, Xiangbo and Lin, Binbin and Cai, Deng and He, Xiaofei and Pei, Jian},
title = {Parallel Field Alignment for Cross Media Retrieval},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502087},
doi = {10.1145/2502081.2502087},
abstract = {Cross media retrieval systems have received increasing interest in recent years. Due to the semantic gap between low-level features and high-level semantic concepts of multimedia data, many researchers have explored joint-model techniques in cross media retrieval systems. Previous joint-model approaches usually focus on two traditional ways to design cross media retrieval systems: (a) fusing features from different media data; (b) learning different models for different media data and fusing their outputs. However, the process of fusing features or outputs will lose both low- and high-level abstraction information of media data. Hence, both ways do not really reveal the semantic correlations among the heterogeneous multimedia data. In this paper, we introduce a novel method for the cross media retrieval task, named Parallel Field Alignment Retrieval (PFAR), which integrates a manifold alignment framework from the perspective of vector fields. Instead of fusing original features or outputs, we consider the cross media retrieval as a manifold alignment problem using parallel fields. The proposed manifold alignment algorithm can effectively preserve the metric of data manifolds, model heterogeneous media data and project their relationship into intermediate latent semantic spaces during the process of manifold alignment. After the alignment, the semantic correlations are also determined. In this way, the cross media retrieval task can be resolved by the determined semantic correlations. Comprehensive experimental results have demonstrated the effectiveness of our approach.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {897–906},
numpages = {10},
keywords = {manifold alignment, parallel field, cross media, multimedia, text corpus and images},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245299,
author = {Kennedy, Lyndon and Gilbert, Eric},
title = {Session Details: Social Dynamics},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245299},
doi = {10.1145/3245299},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502117,
author = {Althoff, Tim and Borth, Damian and Hees, J\"{o}rn and Dengel, Andreas},
title = {Analysis and Forecasting of Trending Topics in Online Media Streams},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502117},
doi = {10.1145/2502081.2502117},
abstract = {Among the vast information available on the web, social media streams capture what people currently pay attention to and how they feel about certain topics. Awareness of such trending topics plays a crucial role in multimedia systems such as trend aware recommendation and automatic vocabulary selection for video concept detection systems. Correctly utilizing trending topics requires a better understanding of their various characteristics in different social media streams. To this end, we present the first comprehensive study across three major online and social media streams, Twitter, Google, and Wikipedia, covering thousands of trending topics during an observation period of an entire year. Our results indicate that depending on one's requirements one does not necessarily have to turn to Twitter for information about current events and that some media streams strongly emphasize content of specific categories. As our second key contribution, we further present a novel approach for the challenging task of forecasting the life cycle of trending topics in the very moment they emerge. Our fully automated approach is based on a nearest neighbor forecasting technique exploiting our assumption that semantically similar topics exhibit similar behavior.We demonstrate on a large-scale dataset of Wikipedia page view statistics that forecasts by the proposed approach are about 9-48k views closer to the actual viewing statistics compared to baseline methods and achieve a mean average percentage error of 45-19% for time periods of up to 14 days.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {907–916},
numpages = {10},
keywords = {social media analysis, twitter, wikipedia, lifecycle forecast, google, trending topics},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502098,
author = {Bhowmick, Sourav S. and Sun, Aixin and Truong, Ba Quan},
title = {Why Not, WINE? Towards Answering Why-Not Questions in Social Image Search},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502098},
doi = {10.1145/2502081.2502098},
abstract = {Despite considerable progress in recent years on Tag-based Social Image Retrieval (TagIR), state-of-the-art TagIR systems fail to provide a systematic framework for end users to ask why certain images are not in the result set of a given query and provide an explanation for such missing results. However, as humans, such why-not questions are natural when expected images are missing in the query results returned by a TagIR system. Clearly, it would be very helpful to users if they could pose follow-up why-not questions to seek clarifications on missing images in query results. In this work, we take the first step to systematically answer the why-not questions posed by end-users on TagIR systems. Our answer not only involves the reason why desired images are missing in the results but also suggestion on how the query can be altered so that the user can view these missing images in sufficient number. We present three explanation models, namely result reordering, query relaxation, and query substitution, that enable us to explain a variety of why-not questions. We present an algorithm called WINE (Why-not questIon aNswering Engine) that exploits these models to answer why-not questions efficiently. Experiments on NUS-WIDE dataset demonstrate effectiveness as well as benefits of WINE.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {917–926},
numpages = {10},
keywords = {social image, tag-based image search, explanation models, flickr, why-not questions},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502116,
author = {Yin, Wenyuan and Mei, Tao and Chen, Chang Wen},
title = {Automatic Generation of Social Media Snippets for Mobile Browsing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502116},
doi = {10.1145/2502081.2502116},
abstract = {The ongoing revolution in media consumption from traditional PCs to the pervasiveness of mobile devices is driving the adoption of social media in our daily lives. More and more people are using their mobile devices to enjoy social media content while on the move. However, mobile display constraints create challenges for presenting and authoring the rich media content on screens with limited display size. This paper presents an innovative system to automatically generate magazine-like social media visual summaries, which is called "snippet," for efficient mobile browsing. The system excerpts the most salient and dominant elements, i.e., a major picture element and a set of textual elements, from the original media content, and composes these elements into a text overlaid image by maximizing information perception. In particular, we investigate a set of aesthetic rules and visual perception principles to optimize the layout of the extracted elements by considering display constraints. As a result, browsing the snippet on mobile devices is just like quickly glancing at a magazine. To the best of our knowledge, this paper represents one of the first attempts at automatic social media snippet generation by studying aesthetic rules and visual perception principles. We have conducted experiments and user studies with social posts from news entities. We demonstrated that the generated snippets are effective at representing media content in a visually appealing and compact way, leading to a better user experience when consuming social media content on mobile devices.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {927–936},
numpages = {10},
keywords = {snippet presentation, social media, multimedia authoring, responsive design, mobile browsing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502096,
author = {Gan, Tian and Wong, Yongkang and Zhang, Daqing and Kankanhalli, Mohan S.},
title = {Temporal Encoded F-Formation System for Social Interaction Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502096},
doi = {10.1145/2502081.2502096},
abstract = {In the context of a social gathering, such as a cocktail party, the memorable moments are generally captured by professional photographers or by the participants. The latter case is often undesirable because many participants would rather enjoy the event instead of being occupied by the photo-taking task. Motivated by this scenario, we propose the use of a set of cameras to automatically take photos. Instead of performing dense analysis on all cameras for photo capturing, we first detect the occurrence and location of social interactions via F-formation detection. In the sociology literature, F-formation is a concept used to define social interactions, where each detection only requires the spatial location and orientation of each participant. This information can be robustly obtained with additional Kinect depth sensors. In this paper, we propose an extended F-formation system for robust detection of interactions and interactants. The extended F-formation system employs a heat-map based feature representation for each individual, namely Interaction Space (IS), to model their location, orientation, and temporal information. Using the temporally encoded IS for each detected interactant, we propose a best-view camera selection framework to detect the corresponding best view camera for each detected social interaction. The extended F-formation system is evaluated with synthetic data on multiple scenarios. To demonstrate the effectiveness of the proposed system, we conducted a user study to compare our best view camera ranking with human's ranking using real-world data.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {937–946},
numpages = {10},
keywords = {social interaction, social computing, behaviour modeling, video analytics, f-formation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245300,
author = {Caesar, Pablo},
title = {Session Details: Annotation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245300},
doi = {10.1145/3245300},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502127,
author = {Huang, Junshi and Liu, Hairong and Shen, Jialie and Yan, Shuicheng},
title = {Towards Efficient Sparse Coding for Scalable Image Annotation},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502127},
doi = {10.1145/2502081.2502127},
abstract = {Nowadays, content-based retrieval methods are still the development trend of the traditional retrieval systems. Image labels, as one of the most popular approaches for the semantic representation of images, can fully capture the representative information of images. To achieve the high performance of retrieval systems, the precise annotation for images becomes inevitable. However, as the massive number of images in the Internet, one cannot annotate all the images without a scalable and flexible (i.e., training-free) annotation method. In this paper, we particularly investigate the problem of accelerating sparse coding based scalable image annotation, whose off-the-shelf solvers are generally inefficient on large-scale dataset. By leveraging the prior that most reconstruction coefficients should be zero, we develop a general and efficient framework to derive an accurate solution to the large-scale sparse coding problem through solving a series of much smaller-scale subproblems. In this framework, an active variable set, which expands and shrinks iteratively, is maintained, with each snapshot of the active variable set corresponding to a subproblem. Meanwhile, the convergence of our proposed framework to global optimum is theoretically provable. To further accelerate the proposed framework, a sub-linear time complexity hashing strategy, e.g. Locality-Sensitive Hashing, is seamlessly integrated into our framework. Extensive empirical experiments on NUS-WIDE and IMAGENET datasets demonstrate that the orders-of-magnitude acceleration is achieved by the proposed framework for large-scale image annotation, along with zero/negligible accuracy loss for the cases without/with hashing speed-up, compared to the expensive off-the-shelf solvers.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {947–956},
numpages = {10},
keywords = {hash-accelerated sparsity induced scalable optimization, sparse coding, large-scale image annotation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502111,
author = {Li, Yingming and Qi, Zhongang and Zhang, Zhongfei (Mark) and Yang, Ming},
title = {Learning with Limited and Noisy Tagging},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502111},
doi = {10.1145/2502081.2502111},
abstract = {With the rapid development of social networks, tagging has become an important means responsible for such rapid development. A robust tagging method must have the capability to meet the two challenging requirements: limited labeled training samples and noisy labeled training samples. In this paper, we investigate this challenging problem of learning with limited and noisy tagging and propose a discriminative model, called SpSVM-MC, that exploits both labeled and unlabeled data through a semi-parametric regularization and takes advantage of the multi-label constraints into the optimization. While SpSVM-MC is a general method for learning with limited and noisy tagging, in the evaluations we focus on the specific application of noisy image tagging with limited labeled training samples on a benchmark dataset. Theoretical analysis and extensive evaluations in comparison with state-of-the-art literature demonstrate that SpSVM-MC outstands with a superior performance.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {957–966},
numpages = {10},
keywords = {semi-parametric regularization, noisy image tagging, multi-label constraint, limited labels, discriminative method},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502113,
author = {Xie, Lexing and He, Xuming},
title = {Picture Tags and World Knowledge: Learning Tag Relations from Visual Semantic Sources},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502113},
doi = {10.1145/2502081.2502113},
abstract = {This paper studies the use of everyday words to describe images. The common saying has it that 'a picture is worth a thousand words', here we ask which thousand? The proliferation of tagged social multimedia data presents a challenge to understanding collective tag-use at large scale -- one can ask if patterns from photo tags help understand tag-tag relations, and how it can be leveraged to improve visual search and recognition. We propose a new method to jointly analyze three distinct visual knowledge resources: Flickr, ImageNet/WordNet, and ConceptNet. This allows us to quantify the visual relevance of both tags learn their relationships. We propose a novel network estimation algorithm, Inverse Concept Rank, to infer incomplete tag relationships. We then design an algorithm for image annotation that takes into account both image and tag features. We analyze over 5 million photos with over 20,000 visual tags. The statistics from this collection leads to good results for image tagging, relationship estimation, and generalizing to unseen tags. This is a first step in analyzing picture tags and everyday semantic knowledge. Potential other applications include generating natural language descriptions of pictures, as well as validating and supplementing knowledge databases.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {967–976},
numpages = {10},
keywords = {folksonomy, knowledge graph, social media},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502085,
author = {Yao, Ting and Mei, Tao and Ngo, Chong-Wah and Li, Shipeng},
title = {Annotation for Free: Video Tagging by Mining User Search Behavior},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502085},
doi = {10.1145/2502081.2502085},
abstract = {The problem of tagging is mostly considered from the perspectives of machine learning and data-driven philosophy. A fundamental issue that underlies the success of these approaches is the visual similarity, ranging from the nearest neighbor search to manifold learning, to identify similar instances of an example for tag completion. The need to searching for millions of visual examples in high-dimensional feature space, however, makes the task computationally expensive. Moreover, the results can suffer from robustness problem, when the underlying data, such as online videos, are rich of semantics and the similarity is difficult to be learnt from low-level features. This paper studies the exploration of user searching behavior through click-through data, which is largely available and freely accessible by search engines, for learning video relationship and applying the relationship for economic way of annotating online videos. We demonstrated that, by a simple approach using co-click statistics, promising results were obtained in contrast to feature-based similarity measurement. Furthermore, considering the long tail effect that few videos dominate most clicks, a new method based on~polynomial~semantic indexing is proposed to learn a latent space~for alleviating the sparsity problem of click-through data. The proposed approaches are then applied for three major tasks in tagging: tag assignment, ranking, and enrichment. On~a bipartite graph constructed from click-through data with~over 15 million queries and 20 million video URL clicks,~we showed that annotation can be performed for free with competitive performance and minimum computing resource, representing a new and promising paradigm for video tagging in addition to machine learning and data-driven methodologies.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {977–986},
numpages = {10},
keywords = {video search, tag ranking, tag assignment, video tagging, tag enrichment, annotation, click-through data},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245301,
author = {Joshi, Dhiraj},
title = {Session Details: Scene Understanding},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245301},
doi = {10.1145/3245301},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502128,
author = {Nguyen, Tam V. and Xu, Mengdi and Gao, Guangyu and Kankanhalli, Mohan and Tian, Qi and Yan, Shuicheng},
title = {Static Saliency vs. Dynamic Saliency: A Comparative Study},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502128},
doi = {10.1145/2502081.2502128},
abstract = {Recently visual saliency has attracted wide attention of researchers in the computer vision and multimedia field. However, most of the visual saliency-related research was conducted on still images for studying static saliency. In this paper, we give a comprehensive comparative study for the first time of dynamic saliency (video shots) and static saliency (key frames of the corresponding video shots), and two key observations are obtained: 1) video saliency is often different from, yet quite related with, image saliency, and 2) camera motions, such as tilting, panning or zooming, affect dynamic saliency significantly. Motivated by these observations, we propose a novel camera motion and image saliency aware model for dynamic saliency prediction. The extensive experiments on two static-vs-dynamic saliency datasets collected by us show that our proposed method outperforms the state-of-the-art methods for dynamic saliency prediction. Finally, we also introduce the application of dynamic saliency prediction for dynamic video captioning, assisting people with hearing impairments to better entertain videos with only off-screen voices, e.g., documentary films, news videos and sports videos.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {987–996},
numpages = {10},
keywords = {dynamic saliency, camera motion, cinematography, static saliency},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502095,
author = {Liu, Li and Shao, Ling and Li, Xuelong},
title = {Building Holistic Descriptors for Scene Recognition: A Multi-Objective Genetic Programming Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502095},
doi = {10.1145/2502081.2502095},
abstract = {Real-world scene recognition has been one of the most challenging research topics in computer vision, due to the tremendous intraclass variability and the wide range of scene categories. In this paper, we successfully apply an evolutionary methodology to automatically synthesize domain-adaptive holistic descriptors for the task of scene recognition, instead of using hand-tuned descriptors. We address this as an optimization problem by using multi-objective genetic programming (MOGP). Specifically, a set of primitive operators and filters are first randomly assembled in theMOGP framework as tree-based combinations, which are then evaluated by two objective fitness criteria i.e., the classification error and the tree complexity. Finally, the best-so-far solution selected by MOGP is regarded as the (near-)optimal feature descriptor for scene recognition. We have evaluated our approach on three realistic scene datasets: MIT urban and nature, SUN and UIUC Sport. Experimental results consistently show that our MOGP-generated descriptors achieve significantly higher recognition accuracies compared with state-of-the-art hand-crafted and machine-learned features.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {997–1006},
numpages = {10},
keywords = {machine learning, feature extraction, multi-objective optimization, genetic programming, scene recognition},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502108,
author = {Mao, Junhua and Li, Houqiang and Zhou, Wengang and Yan, Shuicheng and Tian, Qi},
title = {Scale Based Region Growing for Scene Text Detection},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502108},
doi = {10.1145/2502081.2502108},
abstract = {Scene text is widely observed in our daily life and has many important multimedia applications. Unlike document text, scene text usually exhibits large variations in font and language, and suffers from low resolution, occlusions and complex background. In this paper, we present a novel scale-based region growing algorithm for scene text detection. We first distinguish SIFT features in text regions from those in background by exploring the inter- and intra-statistics of SIFT features. Then scene text regions in images are identified by scale-based region growing, which explores the geometric context of SIFT keypoints in local regions. Our algorithm is very effective to detect multilingual text in various fonts, sizes, and with complex background. In addition, it offers insights on efficiently deploying local features in numerous applications, such as visual search. We evaluate our algorithm on three datasets and achieve the state-of-the-art performance.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1007–1016},
numpages = {10},
keywords = {scene text, region growing, neural network, sift},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502109,
author = {Grabner, Helmut and Nater, Fabian and Druey, Michel and Van Gool, Luc},
title = {Visual Interestingness in Image Sequences},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502109},
doi = {10.1145/2502081.2502109},
abstract = {Interestingness is said to be the power of attracting or holding one's attention (because something is unusual or exciting, etc.). We, as humans, have the great capacity to direct our visual attention and judge the interestingness of a scene. Consider for example the image sequence in the figure on the right. The spider in front of the camera or the snow on the lens are examples of events that deviate from the context since they violate the expectations, and therefore are considered interesting. On the other hand, weather changes or a camera shift, do not raise human attention considerably, even though large regions of the image are influenced. In this work we firstly investigate what humans consider as "interesting" in image sequences. Secondly we propose a computer vision algorithm to automatically spot these interesting events. To this end, we integrate multiple cues inspired by cognitive concepts and discuss why and to what extent the automatic discovery of visual interestingness is possible.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1017–1026},
numpages = {10},
keywords = {computer vision, visual interestingness, image understanding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3245302,
author = {Hung, Hayley and Cristani, Marco},
title = {Session Details: Doctoral Symposium},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3245302},
doi = {10.1145/3245302},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502208,
author = {Chatzilari, Elisavet},
title = {Using Tagged Images of Low Visual Ambiguity to Boost the Learning Efficiency of Object Detectors},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502208},
doi = {10.1145/2502081.2502208},
abstract = {Motivated by the abundant availability of user-generated multimedia content, a data augmentation approach that enhances an initial manually labelled training set with regions from user tagged images is presented. Initially, object detection classifiers are trained using a small number of manually labelled regions as the training set. Then, a set of positive regions is automatically selected from a large number of loosely tagged images, pre-segmented by an automatic segmentation algorithm, to enhance the initial training set. In order to overcome the noisy nature of user tagged images and the lack of information about the pixel level annotations, the main contribution of this work is the introduction of the visual ambiguity term. Visual ambiguity is caused by the visual similarity of semantically dissimilar concepts with respect to the employed visual representation and analysis system (i.e. segmentation, feature space, classifier) and, in this work, is modelled so that the images where ambiguous concepts co-exist are penalized. Preliminary experimental results show that the employment of visual ambiguity guides the selection process away from the ambiguous images and, as a result, allows for better separation between the targeted true positive and the undesired negative regions.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1027–1030},
numpages = {4},
keywords = {social bootstrapping, user tagged images, multimedia data augmentation, semantic segmentation, bootstrapping, visual ambiguity},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502209,
author = {Scott, Michael James},
title = {Projective Identity and Procedural Rhetoric in Educational Multimedia: Towards the Enrichment of Programming Self-Concept and Growth Mindset with Fantasy Role-Play},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502209},
doi = {10.1145/2502081.2502209},
abstract = {There is a growing movement in the behavioral sciences towards exploring more situated, pragmatic and ontological accounts of human learning. Positive psychology shows that a reciprocal relationship may exist between self-concept and the development of expertise, while social psychology reveals that mindsets about the nature of personal traits can have profound impacts on practice behavior. Thus, nurturing psychological constructs through the learning environment may empower students, enabling them to learn more effectively. Educational multimedia is known to support learning in a range of contexts, but its role in facilitating such self-enrichment has seldom been explored. Consequently, it is not clear which designs can aid both self enhancement and skill development. This doctoral symposium paper proposes that an interplay between projective identity and procedural rhetoric, delivered in the form of a fantasy role-playing experience, could be one such practice. Early experiments in the area of introductory programming show promise, but raise questions about external validity, educationally relevant effect sizes and how multimedia elements within the tool could be utilized more effectively to enhance these effects.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1031–1034},
numpages = {4},
keywords = {role-play, programming, education, mindset, projective identity, fantasy, multimedia, implicit beliefs, procedural rhetoric, self-concept, self-enhancement, learning},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502210,
author = {Bhattacharya, Subhabrata},
title = {Recognition of Complex Events in Open-Source Web-Scale Videos: A Bottom up Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502210},
doi = {10.1145/2502081.2502210},
abstract = {Recognition of complex events in unconstrained Internet videos is a challenging research problem. In this symposium proposal, we present a systematic decomposition of complex events into hierarchical components and make an in-depth analysis of how existing research are being used to cater to various levels of this hierarchy. We also identify three key stages where we make novel contributions which are necessary to not only improve the overall recognition performance, but also develop richer understanding of these events. At the lowest level, our contributions include (a) compact covariance descriptors of appearance and motion features used in sparse coding framework to recognize realistic actions and gestures, and (b) a Lie-algebra based representation of dominant camera motion present in video shots which can be used as a complementary feature for video analysis. In the next level, we propose an (c) efficient maximum likelihood estimate based representation from low-level features computed from videos which demonstrates state of the art performance in large scale visual concept detection, and finally, we propose to (d) model temporal interactions between concepts detected in video shots through two new discriminative feature spaces derived from Linear dynamical systems which eventually boosts event recognition performance. In all cases, we conduct thorough experiments to demonstrate promising performance gains over some of the prominent approaches.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1035–1038},
numpages = {4},
keywords = {linear dynamical systems, riemannian manifolds, covariance matrices, maximum likelihood estimates, lie algebra, cinematographic techniques, shot classification, video descriptors, multimedia event detection, complex event recognition, block hankel matrices},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502211,
author = {Dutta, Tanima},
title = {Motion Compensated Compressed Domain Watermarking},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502211},
doi = {10.1145/2502081.2502211},
abstract = {The security has become an important issue in multimedia applications. The embedding of watermark bits in compressed domain is less computationally expensive as full decoding and re-encoding is not required. The motion coherency is an essential property to resist temporal frame averaging based attacks. The design of motion compensated embedding method in compressed domain is a challenging task. As far we know, no such embedding method is explored yet. In this paper, we propose a motion compensated compressed domain embedding method within a short video neighborhood that gives acceptable visual quality, embedding capacity, and robustness. The simulation results show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1039–1042},
numpages = {4},
keywords = {temporal frame averaging, video watermarking, compressed domain embedding, motion compensation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502212,
author = {Gan, Tian},
title = {Social Interaction Detection Using a Multi-Sensor Approach},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502212},
doi = {10.1145/2502081.2502212},
abstract = {In the context of a social gathering, such as a cocktail party, the memorable moments are often captured by professional photographers or the participants. The latter case is generally undesirable because many participants would rather enjoy the event instead of being occupied by the tedious photo capturing task. Motivated by this scenario, we propose an automated social event photo-capture framework for which, given the multiple sensor data streams and the information from the Web as input, will output the visually appealing photos of the social event. Our proposal consists of three components: (1) social attribute extraction from both the physical space and the cyberspace; (2) social attribute fusion; and (3) active camera control. Current work is presented and we conclude with expected contributions as well as future direction.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1043–1046},
numpages = {4},
keywords = {social interaction, social computing, behavior modeling, video analytics, f-formation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502213,
author = {Kaiser, Rene},
title = {Virtual Director Technology for Social Video Communication and Live Event Broadcast Production},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502213},
doi = {10.1145/2502081.2502213},
abstract = {This thesis investigates several aspects of Virtual Director technology, i.e. software capable of intelligent real-time selection of live media streams. It addresses several research questions in this interdisciplinary field with respect to how a generic Virtual Director framework can be constructed, and how its behavior can be modeled and formalized to realize professional applications with many parallel users within real-time constraints. Prototypes have been built for the applications of group videoconferencing and live event broadcast. The engine executes cinematic principles aiming to enhance the user experience. In group videoconferencing, a Virtual Director aims to support communication goals by selecting from multiple available streams, i.e. automating cuts between shots according to the communication situation. In event broadcast, it enables personalization by framing, animating and cutting virtual camera views as cropping from a high-resolution panorama. While the technical approach and framework has been evaluated in lab experiments, further evaluation involving potential users and cinematic professionals is ongoing.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1047–1050},
numpages = {4},
keywords = {live broadcast, video communication, virtual director},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502214,
author = {Fran\c{c}oise, Jules},
title = {Gesture--Sound Mapping by Demonstration in Interactive Music Systems},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502214},
doi = {10.1145/2502081.2502214},
abstract = {In this paper we address the issue of mapping between gesture and sound in interactive music systems. Our approach, we call mapping by demonstration, aims at learning the mapping from examples provided by users while interacting with the system. We propose a general framework for modeling gesture--sound sequences based on a probabilistic, multimodal and hierarchical model. Two orthogonal modeling aspects are detailed and we describe planned research directions to improve and evaluate the proposed models.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1051–1054},
numpages = {4},
keywords = {mapping, music performance, gesture, multimodal, hmm, sound synthesis, hierarchical modeling},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502215,
author = {Acar, Esra},
title = {Learning Representations for Affective Video Understanding},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502215},
doi = {10.1145/2502081.2502215},
abstract = {Among the ever growing available multimedia data, finding multimedia content which matches the current mood of users is a challenging problem. Choosing discriminative features for the representation of video segments is a key issue in designing video affective content analysis algorithms, where no dominant feature representation has emerged yet. Most existing affective content analysis methods either use low-level audio-visual features or generate hand-crafted higher level representations. In this work, we propose to use deep learning methods, in particular, convolutional neural networks (CNNs), in order to learn mid-level representations from automatically extracted raw features. We exploit only the audio modality in the current framework and employ Mel-Frequency Cepstral Coefficients (MFCC) features in order to build higher level audio representations. We use the learned representations for the affective classification of music video clips. We choose multi-class support vector machines (SVMs) for classifying video clips into affective categories. Preliminary results on a subset of the DEAP dataset show that a significant improvement is obtained when we learn higher level representations instead of using low-level features directly for video affective content analysis. We plan to further extend this work and include visual modality as well. We will generate mid-level visual representations using CNNs and fuse these visual representations with mid-level audio representations both at feature- and decision-level for video affective content analysis.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1055–1058},
numpages = {4},
keywords = {affect analysis, convolutional neural network, learning feature representations, support vector machine},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502216,
author = {Sarasua, Alvaro},
title = {Context-Aware Gesture Recognition in Classical Music Conducting},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502216},
doi = {10.1145/2502081.2502216},
abstract = {Body movement has received increasing attention in music technology research during the last years. Some new musical interfaces make use of gestures to control music in a meaningful and intuitive way. A typical approach is to use the orchestra conducting paradigm, in which the computer that generates the music would be a textit{virtual orchestra} conducted by the user. However, although conductors' gestures are complex and their meaning can vary depending on the musical context, this context-dependency is still to explore. We propose a method to study context-dependency of body and facial gestures of conductors in orchestral classical music based on temporal clustering of gestures into actions, followed by an analysis of the evolution of audio features after action occurrences. For this, multi-modal data (audio, video, motion capture) will be recorded in real live concerts and rehearsals situations using unobtrusive techniques.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1059–1062},
numpages = {4},
keywords = {conducting, music information retrieval, classical music, gesture},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502217,
author = {Centieiro, Pedro},
title = {Bringing the Sport Stadium Atmosphere to Remote Fans},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502217},
doi = {10.1145/2502081.2502217},
abstract = {While watching live sports broadcasts we do not feel so emotionally connected with the performers and the in-venue fans as if we were watching it live, where the event takes place. Moreover, both remote and in-venue fans do not share a social connection, resulting in defragmented social experiences. This work intends to establish a new paradigm that explores the use of mobile devices to enhance remote fans? interaction with a live event. This new paradigm will provide them with an emotional and social experience by bringing the stadium atmosphere, its immersion, and emotional levels, to remote supporters. As a result, remote fans will be more engaged in the broadcasted sports, and both, remote and in-venue, fans will all feel part of the same community.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1063–1066},
numpages = {4},
keywords = {social experiences, affective computing, remote interaction, sports fans, second screen, persuasive technology},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502218,
author = {Bosch, Juan J.},
title = {Automatic Melodic and Structural Analysis of Music Material for Enriched Concert Related Experiences},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502218},
doi = {10.1145/2502081.2502218},
abstract = {This PhD thesis proposal deals with the automatic analysis of musical audio, focusing on the estimation of the predominant melodic lines, which are used as a basis for extracting musical themes, and (along with other features) for structure recognition. The main focus is set on classical western music in large ensemble settings, which poses interesting research challenges to current state-of-the art algorithms. We will study the limitations of current approaches in this genre, and elaborate specific descriptors and methods, combining audio based analysis with further sources of knowledge and modalities. The creation of appropriate datasets will also be a main aspect, in order to properly evaluate the developed approaches. This work will be used to enrich musical concert related experiences, from music consumers to editors.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1067–1070},
numpages = {4},
keywords = {multipitch estimation, musical theme, structure recognition, music segmentation, melody extraction, classical music, musical voice},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502219,
author = {Montagud, Mario},
title = {Design, Development and Evaluation of an Adaptive and Standardized RTP/RTCP-Based IDMS Solution},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502219},
doi = {10.1145/2502081.2502219},
abstract = {Inter-Destination Media Synchronization (IDMS) is essential for enabling pleasant shared media experiences. The goal of my PhD thesis is to design, develop and evaluate an advanced RTP/RTCP-based IDMS solution fitting the requirements of the emerging distributed media consumption paradigm. In particular, standard compliant extensions to RTCP are being specified to allow for an accurate, adaptive and dynamic IDMS control when using RTP for streaming media. Moreover, the feasibility and suitability of several architectural schemes for exchanging the IDMS information, algorithms for allowing a dynamic IDMS monitoring and control, as well as adjustment techniques are being investigated. Objective and subjective testing are being conducted to validate the satisfactory performance of our IDMS solution and to provide insights about the users' tolerance on asynchrony levels in different IDMS scenarios.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1071–1074},
numpages = {4},
keywords = {IDMS, simulation, media synchronization, RTP/RTCP},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502220,
author = {Ventura, Carles},
title = {Visual Object Analysis Using Regions and Interest Points},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502220},
doi = {10.1145/2502081.2502220},
abstract = {This dissertation research will explore region-based and interest points based image representations, two of the most-used image models for object detection, image classification, and visual search among other applications. We will analyze the relationship between both representations with the goal of proposing a new hybrid representation that takes advantage of the strengths and overcomes the weaknesses of both approaches. More specifically, we will focus on the gPb-owt-ucm segmentation algorithm and the SIFT local features since they are the most contrasted techniques in their respective fields. Furthermore, using an object retrieval benchmark, this dissertation research will analyze three basic questions: (i) the usefulness of an interest points hierarchy based on a contour strength signal, (ii) the influence of the context on both interest points location and description, and (iii) the analysis of regions as spatial support for bundling interest points.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1075–1078},
numpages = {4},
keywords = {object retrieval, hierarchical segmentation, image representation, interest points},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2508459,
author = {Cavazza, Marc and Camurri, Antonio},
title = {The ACM Multimedia 2013 Art Exhibition},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2508459},
doi = {10.1145/2502081.2508459},
abstract = {The Art Exhibition of ACM Multimedia 2013 has attracted significant work from a variety of digital artists collaborating with research institutions. We have endeavored to select exhibits that achieved an interesting balance between technology and artistic intent. The techniques underpinning these artworks are relevant to several technical tracks of the conference, in particular those dealing with human-centered and interactive media. We briefly review how the various installations revisit current topics in Multimedia research, focusing more specifically on their approach to dynamic content generation, user experience, multimodality, and affective interfaces. Once again, the unique blend of technology and user experience does not limit itself to showcasing recent advances in interactive media, and should be of interest to all conference participants as well as the general public the exhibition space will be open to.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1079–1082},
numpages = {4},
keywords = {interactive art, digital arts},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503825,
author = {Doulamis, Anastasios and Doulamis, Nikolaos and Bertini, Marco and Gonzalez, Jordi and Moeslund, Thomas},
title = {4th ACM/IEEE ARTEMIS 2013 International Workshop on Analysis and Retrieval of Tracked Events and Motion in Imagery Streams},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503825},
doi = {10.1145/2502081.2503825},
abstract = {In this paper, we give a short summary of the papers proposed in ACMARTEMIS 2013 which is held in Barcelona Spain in conjunction with ACM Multimedia. The workshop handles the areas of features analysis both at low and high level for efficient events detection, retrieval of multimedia events and objects and video synchronization issues and also events and behavior recognition from visual data. All papers were classified into three session of a single track workshop. The first session named "Video Features and Scene Analysis" includes articles that handle low level and high level visual analysis appropriate for event detection. The second session entitled "Retrieval of Multimedia Objects/Events" applies schemes for media data retrieval and video synchronization. Finally the third session "Analysis of Visual Events" describes algorithms for detecting actions, behaviors and events in complex visual scenes.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1083–1084},
numpages = {2},
keywords = {analysis of events, retrieval of actions, tracking, video analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503826,
author = {Valstar, Michel and Schuller, Bj\"{o}rn and Krajewski, Jarek and Cowie, Roddy and Pantic, Maja},
title = {Workshop Summary for the 3rd International Audio/Visual Emotion Challenge and Workshop (AVEC'13)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503826},
doi = {10.1145/2502081.2503826},
abstract = {The third Audio-Visual Emotion Challenge and workshop AVEC 2013 will be held in conjunction ACM Multimedia'13. Like the 2012 edition of AVEC, the workshop/challenge addresses the interpretation of social signals represented in both audio and video in terms of the high-level continuous dimensions arousal and valence, but importantly this year the data is that of a large number of clinically depressed patients and controls, with a sub-challenge in self-reported severity of depression estimation. Like both previous AVECs, the aim is to bring together the audio and video analysis communities.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1085–1086},
numpages = {2},
keywords = {depression, social signal processing, affective computing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503827,
author = {Aizawa, Kiyoharu and Yamakata, Yoko and Funatomi, Takuya},
title = {Workshop Summary for the 5th International Workshop on Multimedia for Cooking and Eating Activities (CEA'13)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503827},
doi = {10.1145/2502081.2503827},
abstract = {This summary introduces the aim of the CEA'13 workshop and the list of papers presented in the workshop.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1087–1088},
numpages = {2},
keywords = {workshop, eating, overview, cooking},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503828,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha},
title = {ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503828},
doi = {10.1145/2502081.2503828},
abstract = {The topic "Crowdsourcing for Multimedia" encompasses the full range of techniques that combine human intelligence and a large number of individual contributors to advance the state of the art in multimedia research. The ACM Multimedia 2013 Workshop on Crowdsourcing for Multimedia (CrowdMM 2013) provided a forum for presenting new crowdsourcing techniques, exchanging innovative crowdsourcing ideas, and discussing crowdsourcing best practices for multimedia. The workshop program consisted of presented papers, a keynote speech and a panel discussion. A special feature of this year's workshop was the "Crowdsourcing for Multimedia Ideas Competition", the results of which were presented at the workshop.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1089–1090},
numpages = {2},
keywords = {multimedia, human computation, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503829,
author = {Cao, Liangliang and Friedland, Gerald and Kelm, Pascal},
title = {Second ACM Multimedia Workshop on Geotagging and Its Applications in Multimedia (GeoMM 2013)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503829},
doi = {10.1145/2502081.2503829},
abstract = {The Workshop on Geotagging and Its Applications in Multimedia (GeoMM 2013) focuses on new applications and methods of geotagging and in geo-location support systems. As the location based multimedia becomes more and more popular in the era of Web and mobile applications, the increase in the use of geotagging and improvements in geo-location support systems open up a new dimension for the description, organization and manipulation of multimedia data. This new dimension radically expands the usefulness of multimedia data both for daily users of the Internet and social networking sites as well as for experts in particular application scenarios. The workshop serves as a venue for the premier research in geotagging and multimedia, and continues to attract submissions from a diverse set of researchers, who address newly arising problems within this emerging field.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1091–1092},
numpages = {2},
keywords = {geotagging, geolocation, social media, location based service},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503830,
author = {Salah, Albert Ali and Hung, Hayley and Aran, Oya and Gunes, Hatice},
title = {Fourth International Workshop on Human Behavior Understanding (HBU 2013)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503830},
doi = {10.1145/2502081.2503830},
abstract = {With advances in pattern recognition and multimedia computing, it became possible to analyze human behavior via multimodal sensors, at different time-scales and at different levels of interaction and interpretation. This ability opens up enormous possibilities for multimedia and multimodal interaction, with a potential of endowing the computers with a capacity to attribute meaning to users' attitudes, preferences, personality, social relationships, etc., as well as to understand what people are doing, the activities they have been engaged in, their routines and lifestyles. This workshop gathers researchers dealing with the problem of modeling human behavior under its multiple facets with particular attention to interactions in arts, creativity, entertainment and edutainment.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1093–1094},
numpages = {2},
keywords = {computer analysis of human behavior, human behavior understanding},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503831,
author = {Chambel, Teresa and Bove, V. Michael and Strover, Sharon and Viana, Paula and Thomas, Graham},
title = {Immersive Media Experiences: Immersiveme 2013 Workshop at ACM Multimedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503831},
doi = {10.1145/2502081.2503831},
abstract = {Immersive media has the potential for strong impact on users' emotions and their sense of presence and engagement. The main objective of this workshop is to bring together researchers, students, media producers, service providers and industry players in the area of emergent immersive media. The workshop will provide a platform for a deep discussion on ongoing work, recent achievements and experiences. It is expected not only to consolidate experiences but also to identify aspects where strong collaboration among all the interested players is needed and to point towards future working directions.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1095–1096},
numpages = {2},
keywords = {immersive media, multisensory interaction, personalization, user participation},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503832,
author = {Luo, Jiebo and Shan, Caifeng and Shao, Ling and Etoh, Minoru},
title = {The Third ACM International Workshop on Interactive Multimedia on Mobile and Portable Devices (IMMPD'13)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503832},
doi = {10.1145/2502081.2503832},
abstract = {With the mobile and portable devices become ubiquitous for people's daily life, how to design user interfaces of these products that enable natural, intuitive and fun interaction is one of the main challenges the multimedia community is facing. Following previous successful events, the third ACM International workshop on Interactive Multimedia on Mobile and Portable Devices (IMMPD'13) aims to bring together researchers from both academia and industry in domains including computer vision, audio and speech processing, machine learning, pattern recognition, communications, human-computer interaction, and media technology to share and discuss recent advances in interactive multimedia.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1097–1098},
numpages = {2},
keywords = {pattern recognition, human-computer interaction, computer vision},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503833,
author = {Cesar, Pablo and Cooper, Matthew and Shamma, David A. and Williams, Doug},
title = {2nd International Workshop on Socially-Aware Multimedia (SAM'13)},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503833},
doi = {10.1145/2502081.2503833},
abstract = {Multimedia social communication is becoming commonplace. Television is becoming smart and social; media sharing applications are transforming the way we converse and recall events and videoconferencing is a common application on our computers, phones, tablets and even televisions. The confluence of computer-mediated interaction, social networking, and multimedia content are radically reshaping social communications, bringing new challenges and opportunities. This workshop, in its second edition, provides an opportunity to explore socially-aware multimedia, in which the social dimension of mediated interactions between people are considered to be as important as the characteristics of the media content. Even though this social dimension is implicitly addressed in some current solutions, further research is needed to better understand what makes multimedia socially-aware.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1099–1100},
numpages = {2},
keywords = {information retrieval, social communication, communication, micro-blogging, social media, social interaction},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503834,
author = {Spampinato, Concetto and Mezaris, Vasileios and van Ossenbruggen, Jacco},
title = {Summary Abstract for the 2nd ACM International Workshop on Multimedia Analysis for Ecological Data},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503834},
doi = {10.1145/2502081.2503834},
abstract = {The 2nd ACM International Workshop on Multimedia Analysis for Ecological Data (MAED'13) is held as part of ACM Multimedia 2013. MAED'13, following the first workshop of the MAED series (MAED'12) that was held as part of ACM Multimedia 2012, is concerned with the processing, interpretation, and visualization of ecology-related multimedia content with the aim to support biologists in their investigations for analyzing and monitoring natural environments.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1101–1102},
numpages = {2},
keywords = {ecological multimedia data retrieval, animal and plant identification, multimedia content analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503835,
author = {Benois-Pineau, Jenny and Briassouli, Alexia and Hauptmann, Alexander},
title = {ACM MM MIIRH 2013: Workshop on Multimedia Indexing and Information Retrieval for Healthcare},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503835},
doi = {10.1145/2502081.2503835},
abstract = {Healthcare systems are depending on increasingly sophisticated and ubiquitous technology, while telehealth is rapidly gaining importance with the advent of low-cost and effective technological solutions in medicine. The increase in the worldwide elderly population and the burden this is inflicting upon the workforce, societies and economies are making remote care and independent living at home a necessity. MIIRH is the first workshop on multimedia analysis for remote care of and assisted living solutions which enable people that are incapacitated in some regard to continue living independently at home and remain active members of society. The topics addressed in MIIRH are extremely timely, as multitudes of cost-effective and high quality care solutions are already being developed and used, rendering the examination of new medical, healthcare paradigms an absolute necessity.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1103–1104},
numpages = {2},
keywords = {assisted living, telecare, multimedia monitoring, multimedia processing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2503836,
author = {Singh, Vivek K. and Chua, Tat-Seng and Jain, Ramesh and Pentland, Alex Sandy},
title = {Summary Abstract for the 1st ACM International Workshop on Personal Data Meets Distributed Multimedia},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2503836},
doi = {10.1145/2502081.2503836},
abstract = {Multimedia data are now created at a macro, public scale as well as individual personal scale. While distributed multimedia streams (e.g. images, microblogs, and sensor readings) have recently been combined to understand multiple spatio-temporal phenomena like epidemic spreads, seasonal patterns, and political situations; personal data (via mobile sensors, quantified-self technologies) are now being used to identify user behavior, intent, affect, social connections, health, gaze, and interest level in real time. An effective combination of the two types of data can revolutionize multiple applications ranging from healthcare, to mobility, to product recommendation, to content delivery. Building systems at this intersection can lead to better orchestrated media systems that may also improve users' social, emotional and physical well-being. For example, users trapped in risky hurricane situations can receive personalized evacuation instructions based on their health, mobility parameters, and distance to nearest shelter. This workshop bring together researchers interested in exploring novel techniques that combine multiple streams at different scales (macro and micro) to understand and react to each user's needs.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1105–1106},
numpages = {2},
keywords = {sensor networks, personal data, situation recognition, distributed multimedia, social networks, reality mining},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502232,
author = {Scherp, Ansgar},
title = {Semantic Technologies for Multimedia Content: Foundations and Applications},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502232},
doi = {10.1145/2502081.2502232},
abstract = {Higher-level semantics for multimedia content is essential to answer questions like ``Give me all presentations of German Physicists of the 20th century''. The tutorial provides an introduction and overview to such semantics and the developments in multimedia metadata. It introduces current advancements for describing media on the web using Linked Open Data and other more expressive semantic technologies. The application of such technologies will be shown at concrete examples.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1107–1108},
numpages = {2},
keywords = {ontologies, multimedia metadata, linked open data},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502233,
author = {Shen, Jialie and Hua, Xian-Sheng and Sargin, Emre},
title = {Towards next Generation Multimedia Recommendation Systems},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502233},
doi = {10.1145/2502081.2502233},
abstract = {Empowered by advances in information technology, such as social media network, digital library and mobile computing, there emerges an ever-increasing amounts of multimedia data. As the key technology to address the problem of information overload, multimedia recommendation system has been received a lot of attentions from both industry and academia. This course aims to 1) provide a series of detailed review of state-of-the-art in multimedia recommendation; 2) analyze key technical challenges in developing and evaluating next generation multimedia recommendation systems from different perspectives and 3) give some predictions about the road lies ahead of us.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1109–1110},
numpages = {2},
keywords = {multimedia, next generation recommendation systems},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502234,
author = {Soleymani, Mohammad and Larson, Martha},
title = {Crowdsourcing for Multimedia Research},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502234},
doi = {10.1145/2502081.2502234},
abstract = {Crowdsourcing techniques make use of intelligent contributions of large number of human crowdmembers. This tutorial introduces researchers to the applications of crowdsourcing to multimedia analysis with the aim of allowing them to understand the potentials and limitations of crowdsourcing tools and techniques. We emphasize the fact that crowdsourcing represents a further development along a pre-existing continuum of techniques, and discuss the added advantages that new developments offer. We provide a basic overview of human computation, with an emphasis on example cases in which crowdsourcing has been applied to generate data sets, to improve automatic multimedia content analysis, and to elicit user needs or multimedia system requirements. Different techniques and considerations in using human computation methods to acquire high-quality data and annotations are discussed and demonstrated.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1111–1112},
numpages = {2},
keywords = {user studies, multimedia annotation, testing, crowdsourcing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502235,
author = {Smith, John R. and Cao, Liangliang},
title = {Massive-Scale Multimedia Semantic Modeling},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502235},
doi = {10.1145/2502081.2502235},
abstract = {Visual data is exploding! 500 billion consumer photos are taken each year world-wide, 633 million photos taken per year in NYC alone. 120 new video-hours are uploaded on YouTube per minute. The explosion of digital multimedia data is creating a valuable open source for insights. However, the unconstrained nature of 'image/video in the wild' makes it very challenging for automated computer-based analysis. Furthermore, the most interesting content in the multimedia files is often complex in nature reflecting a diversity of human behaviors, scenes, activities and events. To address these challenges, this tutorial will provide a unified overview of the two emerging techniques: Semantic modeling and Massive scale visual recognition, with a goal of both introducing people from different backgrounds to this exciting field and reviewing state of the art research in the new computational era.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1113–1114},
numpages = {2},
keywords = {semantic modeling, multimedia information retrieval, machine learning, content-based search, video analysis},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502236,
author = {Zimmermann, Roger and Yu, Yi},
title = {Social Interactions over Geographic-Aware Multimedia Systems},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502236},
doi = {10.1145/2502081.2502236},
abstract = {User-centric Internet multimedia scenes challenge us to discover more interesting events and topics based on matching users' needs associated with personal preferences, geographic interests and social norms. Geotagged multimedia contents from online social sites, e.g., Flickr and Twitter, provide large volumes of data about many given locations. Hence, location is one of the most important user-generated contexts and contains rich information about an individual's interests and behavior. Location-based social multimedia streams (e.g., tweets, videos, images) can provide us with socially complementary information to predict users' needs.By making use of geo-tagging, a cohesive set of social multimedia streams can be published to facilitate a more accurate analysis of user-centric big data information and further assess user tastes based on location activities. This tutorial delivers not only a better understanding of the basics of location-aware contextual descriptions and its relations to social multimedia scenes, but may also serve to highlight relationships that can be collaboratively applied to multimodal retrieval and recommendation technology.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1115–1116},
numpages = {2},
keywords = {location-based services, geographic sensor information, mobile videos, location recommendations, social interactions},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502237,
author = {Schedl, Markus and G\'{o}mez, Emilia and Goto, Masataka},
title = {Multimedia Information Retrieval: Music and Audio},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502237},
doi = {10.1145/2502081.2502237},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1117–1118},
numpages = {2},
keywords = {classification, multimodal music information retrieval, personalization, audio and music description, similarity},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502238,
author = {Tzanetakis, George and Fels, Sidney and Lyons, Michael},
title = {Blending the Physical and the Virtual in Music Technology: From Interface Design to Multi-Modal Signal Processing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502238},
doi = {10.1145/2502081.2502238},
abstract = {Recent years have seen a significant increase of interest in rich multi-modal user interfaces going beyond conventional mouse/keyboard/screen interaction. The new interface technologies are broadly impacting music technology and culture. New musical interfaces use a variety of sensing (and actuating) modalities to receive and present information to users, and often require techniques from signal processing and machine learning in order to extract and fuse high level information from noisy, high dimensional signals over time. Hence they pose many interesting signal processing challenges while offering fascinating possibilities for new research. At the same time the richness of possibilities for new forms of musical interaction requires a new approach to the design of musical technologies and has implications for performance aesthetics and music pedagogy. This tutorial begins with a general and gentle introduction to the theory and practice of the design of new technologies for musical creation and performance. It continues with an overview of signal processing and machine learning methods which are needed for more advanced work in new musical interface design.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1119–1120},
numpages = {2},
keywords = {musical instrument design, machine learning, signal processing, human computer interaction, multimodal interaction, new interfaces for musical expression, digital musical instruments},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/2502081.2502239,
author = {Friedland, Gerald},
title = {Privacy Concerns of Sharing Multimedia in Social Networks},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502239},
doi = {10.1145/2502081.2502239},
abstract = {This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2013.},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {1121–1122},
numpages = {2},
keywords = {multimedia content analysis, social concerns, privacy},
location = {Barcelona, Spain},
series = {MM '13}
}

