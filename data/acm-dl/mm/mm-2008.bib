@inproceedings{10.1145/3256764,
author = {Candan, Selcuk},
title = {Session Details: Best Paper Session},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256764},
doi = {10.1145/3256764},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459361,
author = {Mondet, Sebastien and Cheng, Wei and Morin, Geraldine and Grigoras, Romulus and Boudon, Frederic and Ooi, Wei Tsang},
title = {Streaming of Plants in Distributed Virtual Environments},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459361},
doi = {10.1145/1459359.1459361},
abstract = {Just as in the real world, plants are important objects in virtual world for creating pleasant and realistic environments, especially those involving natural scenes. As such, much effort has been made in realistic modeling of plants. As the trend moves towards networked and distributed virtual environment, however, the current models are inadequate as they are not designed for progressive transmissions. In this paper, we fill in this gap by proposing a progressive representation for plants based on generalized cylinders. To facilitate the transmission of the plants, we quantify the visual contribution of each branch and use this weight in packet scheduling. We show the efficiency of our representations and effectiveness of our packet scheduler through simulations.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1–10},
numpages = {10},
keywords = {streaming, plant models, networked virtual environment, progressive transmission, progressive coding, multiresolution},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459362,
author = {Cesar, Pablo and Bulterman, Dick C.A. and Geerts, David and Jansen, Jack and Knoche, Hendrik and Seager, William},
title = {Enhancing Social Sharing of Videos: Fragment, Annotate, Enrich, and Share},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459362},
doi = {10.1145/1459359.1459362},
abstract = {Media consumption is an inherently social activity, serving to communicate ideas and emotions across both small- and large-scale communities. The migration of the media experience to personal computers retains social viewing, but typically only via a non-social, strictly personal interface. This paper presents an architecture and implementation for media content selection, content (re)organization, and content sharing within a user community that is heterogeneous in terms of both participants and devices. In addition, our application allows the user to enrich the content as a differentiated personalization activity targeted to his/her peer-group. We describe the goals, architecture and implementation of our system in this paper. In order to validate our results, we also present results from two user studies involving disjoint sets of test participants.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {11–20},
numpages = {10},
keywords = {asynchronous media sharing, content enrichment},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459363,
author = {Knoche, Hendrik O. and Sasse, M Angela},
title = {The Sweet Spot: How People Trade off Size and Definition on Mobile Devices},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459363},
doi = {10.1145/1459359.1459363},
abstract = {Mobile TV can deliver up-to-date content to users on the move. But it is currently unclear how to best adapt higher resolution TV content. In this paper, we describe a laboratory study with 35 participants who watched short clips of different content and shot types on a 200ppi PDA display at a resolution of either 120x90 or 168x128. Participants selected their preferred size and rated the acceptability of the visual experience. The preferred viewing ratio depended on the resolution and had to be at least 9.8H. The minimal angular resolution people required and which limited the up-scaling factor was 14 pixels per degree. Extreme long shots were best when depicted actors were at least 0.7° high. A second study researched the ecological validity of previous lab results by comparing them to results from the field. Image size yielded more value for users in the field than was apparent from lab results. In conclusion, current prediction models based on preferred viewing distances for TV and large displays do not predict viewing preferences on mobile devices. Our results will help to further the understanding of multimedia perception and service designers to deliver both economically viable and enjoyable experiences.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {21–30},
numpages = {10},
keywords = {resolution, trade-off, mobile multimedia consumption, size},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459364,
author = {Wu, Lei and Hua, Xian-Sheng and Yu, Nenghai and Ma, Wei-Ying and Li, Shipeng},
title = {Flickr Distance},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459364},
doi = {10.1145/1459359.1459364},
abstract = {This paper presents Flickr distance, which is a novel measurement of the relationship between semantic concepts (objects, scenes) in visual domain. For each concept, a collection of images are obtained from Flickr, based on which the improved latent topic based visual language model is built to capture the visual characteristic of this concept. Then Flickr distance between different concepts is measured by the square root of Jensen-Shannon (JS) divergence between the corresponding visual language models. Comparing with WordNet, Flickr distance is able to handle far more concepts existing on the Web, and it can scale up with the increase of concept vocabularies. Comparing with Google distance, which is generated in textual domain, Flickr distance is more precise for visual domain concepts, as it captures the visual relationship between the concepts instead of their co-occurrence in text search results. Besides, unlike Google distance, Flickr distance satisfies triangular inequality, which makes it a more reasonable distance metric. Both subjective user study and objective evaluation show that Flickr distance is more coherent to human perception than Google distance. We also design several application scenarios, such as concept clustering and image annotation, to demonstrate the effectiveness of this proposed distance in image related applications.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {31–40},
numpages = {10},
keywords = {tagnet, flickr distance, visual distance, concept relationship, similarity measurement, visual concept net},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256765,
author = {Sebe, Nicu},
title = {Session Details: Content Track C1: Duplicate Detection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256765},
doi = {10.1145/3256765},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459366,
author = {Zhu, Jianke and Hoi, Steven C.H. and Lyu, Michael R. and Yan, Shuicheng},
title = {Near-Duplicate Keyframe Retrieval by Nonrigid Image Matching},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459366},
doi = {10.1145/1459359.1459366},
abstract = {Near-duplicate image retrieval plays an important role in many real-world multimedia applications. Most previous approaches have some limitations. For example, conventional appearance-based methods may suffer from the illumination variations and occlusion issue, and local feature correspondence-based methods often do not consider local deformations and the spatial coherence between two point sets. In this paper, we propose a novel and effective Nonrigid Image Matching (NIM) approach to tackle the task of near-duplicate keyframe retrieval from real-world video corpora. In contrast to previous approaches, the NIM technique can recover an explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data effectively. To make our technique applicable to large-scale applications, we suggest an effective multi-level ranking scheme that filters out the irrelevant results in a coarse-to-fine manner. In our ranking scheme, to overcome the extremely small training size challenge, we employ a semi-supervised learning method for improving the performance using unlabeled data. To evaluate the effectiveness of our solution, we have conducted extensive experiments on two benchmark testbeds extracted from the TRECVID2003 and TRECVID2004 corpora. The promising results show that our proposed method is more effective than other state-of-the-art approaches for near-duplicate keyframe retrieval.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {41–50},
numpages = {10},
keywords = {semi-supervised learning, near-duplicate keyframe, image copy detection, nonrigid image matching},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459367,
author = {Wu, Xiaomeng and Takimoto, Masao and Satoh, Shin'ichi and Adachi, Jun},
title = {Scene Duplicate Detection Based on the Pattern of Discontinuities in Feature Point Trajectories},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459367},
doi = {10.1145/1459359.1459367},
abstract = {The paper is aiming to detect and retrieve videos of the same scene (scene duplicates) from broadcast video archives. Scene duplicate is composed of different pieces of footage of the same scene, the same event, at the same time, but from the different viewpoints. Scene duplicate detection would be particularly useful to identify the same event reported in different programs from different broadcast stations. The approach should be invariant to viewpoint changes. We focused on object motion in videos and devised a video matching approach based on the temporal pattern of discontinuities obtained from feature point trajectories. We developed an acceleration method based on the discontinuity pattern, which is more robust to variations in camerawork and editing than conventional features, to dramatically reduce the computation burden. We compared our approach with an existing video matching method based on the local feature of keyframe. The spatial registration strategy of this method was also used with the proposed approach to cope with visually different unrelated video pairs. The performance and effectiveness of our approach was demonstrated on actual broadcasted videos.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {51–60},
numpages = {10},
keywords = {scene duplicate detection, time series analysis, feature point tracking, video matching},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459368,
author = {Poullot, S\'{e}bastien and Crucianu, Michel and Buisson, Olivier},
title = {Scalable Mining of Large Video Databases Using Copy Detection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459368},
doi = {10.1145/1459359.1459368},
abstract = {Mining the video content itself can bring to light important information regarding the internal structure of large video databases, compensating for a lasting absence of extensive and reliable annotations. Many valuable links between video segments can be identified by content-based copy detection methods, where "copies" are transformed versions of original video sequences. To make this approach viable for large video databases, we put forward a new mining method relying on the definition of a compact keyframe-level descriptor and of a specific index structure. The performance obtained in detecting links between video segments is evaluated with the help of a ground truth and several illustrations are given. The scalability of the approach is then demonstrated for databases of up to 10,000 hours of video.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {61–70},
numpages = {10},
keywords = {video mining, similarity join, content-based copy detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256766,
author = {Hauptmann, Alexander},
title = {Session Details: Content Track C2: Semantic Video Annotation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256766},
doi = {10.1145/3256766},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459370,
author = {Weng, Ming-Fang and Chuang, Yung-Yu},
title = {Multi-Cue Fusion for Semantic Video Indexing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459370},
doi = {10.1145/1459359.1459370},
abstract = {The huge amount of videos currently available poses a difficult problem in semantic video retrieval. The success of query-by-concept, recently proposed to handle this problem, depends greatly on the accuracy of concept-based video indexing. This paper describes a multi-cue fusion approach toward improving the accuracy of semantic video indexing. This approach is based on a unified framework that explores and integrates both contextual correlation among concepts and temporal dependency among shots. The framework is novel in two ways. First, a recursive algorithm is proposed to learn both inter-concept and inter-shot relationships from ground-truth annotations of tens of thousands of shots for hundreds of concepts. Second, labels for all concepts and all shots are solved simultaneously through optimizing a graphical model. Experiments on the widely used TRECVID 2006 data set show that our framework is effective for semantic concept detection in video, achieving around a 30% performance boost on two popular benchmarks, VIREO-374 and Columbia374, in inferred average precision.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {71–80},
numpages = {10},
keywords = {contextual correlation, temporal dependency, trecvid, semantic video indexing},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459371,
author = {Wei, Xiao-Yong and Ngo, Chong-Wah},
title = {Fusing Semantics, Observability, Reliability and Diversity of Concept Detectors for Video Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459371},
doi = {10.1145/1459359.1459371},
abstract = {Effective utilization of semantic concept detectors for large-scale video search has recently become a topic of intensive studies. One of main challenges is the selection and fusion of appropriate detectors, which considers not only semantics but also the reliability of detectors, observability and diversity of detectors in target video domains. In this paper, we present a novel fusion technique which considers different aspects of detectors for query answering. In addition to utilizing detectors for bridging the semantic gap of user queries and multimedia data, we also address the issue of "observability gap" among detectors which could not be directly inferred from semantic reasoning such as using ontology. To facilitate the selection of detectors, we propose the building of two vector spaces: semantic space (SS) and observability space (OS). We categorize the set of detectors selected separately from SS and OS into four types: anchor, bridge, positive and negative concepts. A multi-level fusion strategy is proposed to novelly combine detectors, allowing the enhancement of detector reliability while enabling the observability, semantics and diversity of concepts being utilized for query answering. By experimenting the proposed approach on TRECVID 2005-2007 datasets and queries, we demonstrate the significance of considering observability, reliability and diversity, in addition to the semantics of detectors to queries.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {81–90},
numpages = {10},
keywords = {concept-based video search, detector selection and fusion},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459372,
author = {Liu, Yanan and Wu, Fei and Zhuang, Yueting and Xiao, Jun},
title = {Active Post-Refined Multimodality Video Semantic Concept Detection with Tensor Representation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459372},
doi = {10.1145/1459359.1459372},
abstract = {In this paper, we resolve the problem of multi-modality video representation and semantic concept detection. Interaction and integration of multi-modality media types such as visual, audio and textual data in video are essential to video semantic analysis. Traditionally, videos are represented as vectors in the Euclidean space. Many learning algorithms are then taken to these vectors in a high dimensional space for dimension reduction, classification, clustering and so on. However, the multiple modalities in video not only have their own properties, but also have correlations among them; whereas the simple vector representation weakens the power of these relatively independent modalities and even ignores their relations to some extent. In this paper, we introduce a higher-order tensor framework for video analysis, in which we represent image, video and text three modalities in video shots as data points by the 3rd-order tensor called tensorshots. We propose a novel dimension reduction method that explicitly considers the manifold structure of the tensor space from multimodal media data which is temporal associated co-occurrence and then detect video semantic concepts through powerful classifiers which take tensor as input. Our algorithm preserves the intrinsic structure of the submanifold where tensorshots are sampled, and is also able to map out-of-sample data points directly. Moreover we apply an active learning based contextual and temporal post-refining strategy to enhance detection accuracy. Experiment results show that our method improves the performance of video semantic concept detection.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {91–100},
numpages = {10},
keywords = {contextual correlation, support tensor machines (stm), temporal dependency, hosvd, tensorshot, temporal associated cooccurrence (tac), active learning, multi-modality video semantic concept detection, dimension reduction},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256767,
author = {Luo, Jiebo},
title = {Session Details: Content Track C3: Image Annotation and Tagging},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256767},
doi = {10.1145/3256767},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459374,
author = {Magalh\~{a}es, Jo\~{a}o and Ciravegna, Fabio and R\"{u}ger, Stefan},
title = {Exploring Multimedia in a Keyword Space},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459374},
doi = {10.1145/1459359.1459374},
abstract = {We address the problem of searching multimedia by semantic similarity in a keyword space. In contrast to previous research we represent multimedia content by a vector of keywords instead of a vector of low-level features. This vector of keywords can be obtained through user manual annotations or computed by an automatic annotation algorithm. In this setting, we studied the influence of two aspects of the search by semantic similarity process: (1) accuracy of user keywords versus automatic keywords and (2) functions to compute semantic similarity between keyword vectors of two multimedia documents. We consider these two aspects to be crucial in the design of a keyword space that can exploit social-media information and can enrich applications such as Flickr and YouTube. Experiments were performed on an image and a video dataset with a large number of keywords, with different similarity functions and with two annotation methods. Surprisingly, we found that multimedia semantic similarity with automatic keywords performs as good as or better than 95% accurate user keywords.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {101–110},
numpages = {10},
keywords = {user keyword annotations, search, automatic keyword annotations, multimedia, keyword spaces},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459375,
author = {Weinberger, Kilian Quirin and Slaney, Malcolm and Van Zwol, Roelof},
title = {Resolving Tag Ambiguity},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459375},
doi = {10.1145/1459359.1459375},
abstract = {Tagging is an important way for users to succinctly describe the content they upload to the Internet. However, most tag-suggestion systems recommend words that are highly correlated with the existing tag set, and thus add little information to a user's contribution. This paper describes a means to determine the ambiguity of a set of (user-contributed) tags and suggests new tags that disambiguate the original tags. We introduce a probabilistic framework that allows us to find two tags that appear in different contexts but are both likely to co-occur with the original tag set. If such tags can be found, the current description is considered "ambiguous" and the two tags are recommended to the user for further clarification. In contrast to previous work, we only query the user when information is most needed and good suggestions are available. We verify the efficacy of our approach using geographical, temporal and semantic metadata, and a user study. We built our system using statistics from a large (100M) database of images and their tags.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {111–120},
numpages = {10},
keywords = {query expansion, ambiguity, tagging, photos},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459376,
author = {Cao, Liangliang and Luo, Jiebo and Huang, Thomas S.},
title = {Annotating Photo Collections by Label Propagation According to Multiple Similarity Cues},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459376},
doi = {10.1145/1459359.1459376},
abstract = {This paper considers the emerging problem of annotating personal photo collections that are taken by digital cameras and may have been subsequently organized by customers. Unlike the images from the web searching engine or commercial image banks (e.g. the Corel database), the photos in the same personal collection are related to each other in time, location, and content. Advanced technologies can record the GPS coordinates for each photo, and thus provide a richer source of context to model and enforce the correlation between the photos in the same collection. Recognizing the well-known limitations ("semantic gap") of visual recognition algorithms, we exploit the correlation between the photos to enhance the annotation performance. In our approach, high-confidence annotation labels are first obtained for certain photos and then propagated to the remaining photos in the same collection, according to time, location, and visual proximity (or similarity). A novel generative probabilistic model is employed, which outperforms the pervious linear propagation scheme. Experimental results have shown the advantages of the proposed annotation scheme.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {121–130},
numpages = {10},
keywords = {photo collection, sift, timestamp, gps, label propagation, color histogram, photo similarities},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256768,
author = {Worring, Marcel},
title = {Session Details: Content Track C4: Video Sp81-Wei.Pdfearch},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256768},
doi = {10.1145/3256768},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459378,
author = {Tian, Xinmei and Yang, Linjun and Wang, Jingdong and Yang, Yichen and Wu, Xiuqing and Hua, Xian-Sheng},
title = {Bayesian Video Search Reranking},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459378},
doi = {10.1145/1459359.1459378},
abstract = {Content-based video search reranking can be regarded as a process that uses visual content to recover the "true" ranking list from the noisy one generated based on textual information. This paper explicitly formulates this problem in the Bayesian framework, i.e., maximizing the ranking score consistency among visually similar video shots while minimizing the ranking distance, which represents the disagreement between the objective ranking list and the initial text-based. Different from existing point-wise ranking distance measures, which compute the distance in terms of the individual scores, two new methods are proposed in this paper to measure the ranking distance based on the disagreement in terms of pair-wise orders. Specifically, hinge distance penalizes the pairs with reversed order according to the degree of the reverse, while preference strength distance further considers the preference degree. By incorporating the proposed distances into the optimization objective, two reranking methods are developed which are solved using quadratic programming and matrix computation respectively. Evaluation on TRECVID video search benchmark shows that the performance improvement up to 21% on TRECVID 2006 and 61.11% on TRECVID 2007 are achieved relative to text search baseline.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {131–140},
numpages = {10},
keywords = {video search reranking, pair-wise, bayesian reranking},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459379,
author = {Hua, Xian-Sheng and Qi, Guo-Jun},
title = {Online Multi-Label Active Annotation: Towards Large-Scale Content-Based Video Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459379},
doi = {10.1145/1459359.1459379},
abstract = {Existing video search engines have not taken the advantages of video content analysis and semantic understanding. Video search in academia uses semantic annotation to approach content-based indexing. We argue this is a promising direction to enable real content-based video search. However, due to the complexity of both video data and semantic concepts, existing techniques on automatic video annotation are still not able to handle large-scale video set and large-scale concept set, in terms of both annotation accuracy and computation cost. To address this problem, in this paper, we propose a scalable framework for annotation-based video search, as well as a novel approach to enable large-scale semantic concept annotation, that is, online multi-label active learning. This framework is scalable to both the video sample dimension and concept label dimension. Large-scale unlabeled video samples are assumed to arrive consecutively in batches with an initial pre-labeled training set, based on which a preliminary multi-label classifier is built. For each arrived batch, a multi-label active learning engine is applied, which automatically selects and manually annotates a set of unlabeled sample-label pairs. And then an online learner updates the original classifier by taking the newly labeled sample-label pairs into consideration. This process repeats until all data are arrived. During the process, new labels, even without any pre-labeled training samples, can be incorporated into the process anytime. Experiments on TRECVID dataset demonstrate the effectiveness and efficiency of the proposed framework.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {141–150},
numpages = {10},
keywords = {video annotation, online learning, multi-label active learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459380,
author = {Bernardin, Keni and Stiefelhagen, Rainer and Waibel, Alex},
title = {Probabilistic Integration of Sparse Audio-Visual Cues for Identity Tracking},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459380},
doi = {10.1145/1459359.1459380},
abstract = {In the context of smart environments, the ability to track and identify persons is a key factor, determining the scope and flexibility of analytical components or intelligent services that can be provided. While some amount of work has been done concerning the camera-based tracking of multiple users in a variety of scenarios, technologies for acoustic and visual identification, such as face or voice ID, are unfortunately still subjected to severe limitations when distantly placed sensors have to be used. Because of this, reliable cues for identification can be hard to obtain without user cooperation, especially when multiple users are involved.In this paper, we present a novel technique for the tracking and identification of multiple persons in a smart environment using distantly placed audio-visual sensors. The technique builds on the opportunistic integration of tracking as well as face and voice identification cues, gained from several cameras and microphones, whenever these cues can be captured with a sufficient degree of confidence. A probabilistic model is used to keep track of identified persons and update the belief in their identities whenever new observations can be made. The technique has been systematically evaluated on the CLEAR Interactive Seminar database, a large audio-visual corpus of realistic meeting scenarios captured in a variety of smart rooms.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {151–158},
numpages = {8},
keywords = {smart environments, modality fusion, sensor fusion, human perception},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256769,
author = {Syeda-Mahmood, Tanveer},
title = {Session Details: Content Track C5: Multimedia Content Analysis and Applications},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256769},
doi = {10.1145/3256769},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459382,
author = {Mayer, Rudolf and Neumayer, Robert and Rauber, Andreas},
title = {Combination of Audio and Lyrics Features for Genre Classification in Digital Audio Collections},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459382},
doi = {10.1145/1459359.1459382},
abstract = {In many areas multimedia technology has made its way into mainstream. In the case of digital audio this is manifested in numerous online music stores having turned into profitable businesses. The widespread user adaption of digital audio both on home computers and mobile players show the size of this market. Thus, ways to automatically process and handle the growing size of private and commercial collections become increasingly important; along goes a need to make music interpretable by computers. The most obvious representation of audio files is their sound - there are, however, more ways of describing a song, for instance its lyrics, which describe songs in terms of content words. Lyrics of music may be orthogonal to its sound, and differ greatly from other texts regarding their (rhyme) structure. Consequently, the exploitation of these properties has potential for typical music information retrieval tasks such as musical genre classification; so far, there is a lack of means to efficiently combine these modalities. In this paper, we present findings from investigating advanced lyrics features such as the frequency of certain rhyme patterns, several parts-of-speech features, and statistic features such as words per minute (WPM). We further analyse in how far a combination of these features with existing acoustic feature sets can be exploited for genre classification and provide experiments on two test collections.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {159–168},
numpages = {10},
keywords = {feature selection, genre classification, audio features, feature fusion, lyrics processing, supervised learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459383,
author = {Wu, Wen and Yang, Jie},
title = {Object Fingerprints for Content Analysis with Applications to Street Landmark Localization},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459383},
doi = {10.1145/1459359.1459383},
abstract = {An object can be a basic unit for multimedia content analysis. Besides similarity among common objects, each object has its own unique characteristics which we cannot find in other surrounding objects in multimedia data. We call such unique characteristics object fingerprints. In this paper, we propose a novel approach to extract and match object fingerprints for multimedia content analysis. In particular, we focus on the problem of street landmark localization from images. Instead of modeling and matching a street landmark as a whole, our proposed approach extracts the landmark's object fingerprints in a given image and match to a new image or video in order to localize the landmark. We formulate matching the landmark's object fingerprints as a classification problem solved by a cascade of 1NN classifiers. We develop a street landmark localization system that combines salient region detection, segmentation, and object fingerprint extraction techniques for the purpose. To evaluate, we have compiled a novel dataset which consists of 15 U.S. street landmarks' images and videos. Our experiments on this dataset show superior performance to state-of-the-art recognition algorithms [20, 33]. The proposed approach can also be well generalized to other objects of interest and content analysis tasks. We demonstrate the feasibility through the application of our approach to refine web image search results and obtained encouraging results.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {169–178},
numpages = {10},
keywords = {street landmark localization, object fingerprint},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459384,
author = {Dong, Wei and Wang, Zhe and Charikar, Moses and Li, Kai},
title = {Efficiently Matching Sets of Features with Random Histograms},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459384},
doi = {10.1145/1459359.1459384},
abstract = {As the commonly used representation of a feature-rich data object has evolved from a single feature vector to a set of feature vectors, a key challenge in building a content-based search engine for feature-rich data is to match feature-sets efficiently. Although substantial progress has been made during the past few years, existing approaches are still inefficient and inflexible for building a search engine for massive datasets. This paper presents a randomized algorithm to embed a set of features into a single high-dimensional vector to simplify the feature-set matching problem. The main idea is to project feature vectors into an auxiliary space using locality sensitive hashing and to represent a set of features as a histogram in the auxiliary space. A histogram is simply a high dimensional vector, and efficient similarity measures like L1 and L2 distances can be employed to approximate feature-set distance measures.We evaluated the proposed approach under three different task settings, i.e. content-based image search, image object recognition and near-duplicate video clip detection. The experimental results show that the proposed approach is indeed effective and flexible. It can achieve accuracy comparable to the feature-set matching methods, while requiring significantly less space and time. For object recognition with Caltech 101 dataset, our method runs 25 times faster to achieve the same precision as Pyramid Matching Kernel, the state-of-the-art feature-set matching method.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {179–188},
numpages = {10},
keywords = {set of features, locality sensitive hashing, random histogram},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256770,
author = {Tian, Qi},
title = {Session Details: Content Track C6: Image Retrieval},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256770},
doi = {10.1145/3256770},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459386,
author = {Olivares, Ximena and Ciaramita, Massimiliano and van Zwol, Roelof},
title = {Boosting Image Retrieval through Aggregating Search Results Based on Visual Annotations},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459386},
doi = {10.1145/1459359.1459386},
abstract = {Online photo sharing systems, such as Flickr and Picasa, provide a valuable source of human-annotated photos. Textual annotations are used not only to describe the visual content of an image, but also subjective, spatial, temporal and social dimensions, complicating the task of keyword-based search. In this paper we investigate a method that exploits visual annotations, e.g. notes in Flickr, to enhance keyword-based systems retrieval performance. For this purpose we adopt the bag-of-visual-words approach for content-based image retrieval as our baseline. We then apply rank aggregation of the top 25 results obtained with a set of visual annotations that match the keyword-based query. The results on retrieval experiments show significant improvements in retrieval performance when comparing the aggregated approach with our baseline, which also slightly outperforms text-only search. When using a textual filter on the search space in combination with the aggregated approach an additional boost in retrieval performance is observed, which underlines the need for large scale content-based image retrieval techniques to complement the text-based search.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {189–198},
numpages = {10},
keywords = {rank aggregation, visual annotations, image retrieval},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459387,
author = {Yang, Yi Hsuan and Wu, Po Tun and Lee, Ching Wei and Lin, Kuan Hung and Hsu, Winston H. and Chen, Homer H.},
title = {ContextSeer: Context Search and Recommendation at Query Time for Shared Consumer Photos},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459387},
doi = {10.1145/1459359.1459387},
abstract = {The advent of media-sharing sites like Flickr has drastically increased the volume of community-contributed multimedia resources on the web. However, due to their magnitudes, these collections are increasingly difficult to understand, search and navigate. To tackle these issues, a novel search system, ContextSeer, is developed to improve search quality (by reranking) and recommend supplementary information (i.e., search-related tags and canonical images) by leveraging the rich context cues, including the visual content, high-level concept scores, time and location metadata. First, we propose an ordinal reranking algorithm to enhance the semantic coherence of text-based search result by mining contextual patterns in an unsupervised fashion. A novel feature selection method, wc-tf-idf is also developed to select informative context cues. Second, to represent the diversity of search result, we propose an efficient algorithm cannoG to select multiple canonical images without clustering. Finally, ContextSeer enhances the search experience by further recommending relevant tags. Besides being effective and unsupervised, the proposed methods are efficient and can be finished at query time, which is vital for practical online applications. To evaluate ContextSeer, we have collected 0.5 million consumer photos from Flickr and manually annotated a number of queries by pooling to form a new benchmark, Flickr550. Ordinal reranking achieves significant performance gains both in Flcikr550 and TRECVID search benchmarks. Through a subjective test, cannoG expresses its representativeness and excellence for recommending multiple canonical images.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {199–208},
numpages = {10},
keywords = {metadata, tag, visual word, search, canonical image, shared consumer photo, context, recommending, rerank},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459388,
author = {Joly, Alexis and Buisson, Olivier},
title = {A Posteriori Multi-Probe Locality Sensitive Hashing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459388},
doi = {10.1145/1459359.1459388},
abstract = {Efficient high-dimensional similarity search structures are essential for building scalable content-based search systems on feature-rich multimedia data. In the last decade, Locality Sensitive Hashing (LSH) has been proposed as indexing technique for approximate similarity search. Among the most recent variations of LSH, multi-probe LSH techniques have been proved to overcome the overlinear space cost drawback of common LSH. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by previous work on probabilistic similarity search structures and improves upon recent theoretical work on multi-probe and query adaptive LSH. Whereas these methods are based on likelihood criteria that a given bucket contains query results, we define a more reliable a posteriori model taking account some prior about the queries and the searched objects. This prior knowledge allows a better quality control of the search and a more accurate selection of the most probable buckets. We implemented a nearest neighbors search based on this paradigm and performed experiments on different real visual features datasets. We show that our a posteriori scheme outperforms other multi-probe LSH while offering a better quality control. Comparisons to the basic LSH technique show that our method allows consistent improvements both in space and time efficiency.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {209–218},
numpages = {10},
keywords = {locality sensitive hashing, multi probe, similarity search, high dimension},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459389,
author = {Huang, Zi and Shen, Heng Tao and Shao, Jie and R\"{u}ger, Stefan and Zhou, Xiaofang},
title = {Locality Condensation: A New Dimensionality Reduction Method for Image Retrieval},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459389},
doi = {10.1145/1459359.1459389},
abstract = {Content-based image similarity search plays a key role in multimedia retrieval. Each image is usually represented as a point in a high-dimensional feature space. The key challenge of searching similar images from a large database is the high computational overhead due to the "curse of dimensionality". Reducing the dimensionality is an important means to tackle the problem. In this paper, we study dimensionality reduction for top-k image retrieval. Intuitively, an effective dimensionality reduction method should not only preserve the close locations of similar images (or points), but also separate those dissimilar ones far apart in the reduced subspace. Existing dimensionality reduction methods mainly focused on the former. We propose a novel idea called Locality Condensation (LC) to not only preserve localities determined by neighborhood information and their global similarity relationship, but also ensure that different localities will not invade each other in the low-dimensional subspace. To generate non-overlapping localities in the subspace, LC first performs an elliptical condensation, which condenses each locality with an elliptical shape into a more compact hypersphere to enlarge the margins among different localities and estimate the projection in the subspace for overlap analysis. Through a convex optimization, LC further performs a scaling condensation on the obtained hyperspheres based on their projections in the subspace with minimal condensation degrees. By condensing the localities effectively, the potential overlaps among different localities in the low-dimensional subspace are prevented. Consequently, for similarity search in the subspace, the number of false hits (i.e., distant points that are falsely retrieved) will be reduced. Extensive experimental comparisons with existing methods demonstrate the superiority of our proposal.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {219–228},
numpages = {10},
keywords = {dimensionality reduction, top-k image retrieval, locality condensation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256771,
author = {Zhang, Zhengyou},
title = {Session Details: Content Track C7: Video Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256771},
doi = {10.1145/3256771},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459391,
author = {Zhou, Xi and Zhuang, Xiaodan and Yan, Shuicheng and Chang, Shih-Fu and Hasegawa-Johnson, Mark and Huang, Thomas S.},
title = {SIFT-Bag Kernel for Video Event Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459391},
doi = {10.1145/1459359.1459391},
abstract = {In this work, we present a SIFT-Bag based generative-to-discriminative framework for addressing the problem of video event recognition in unconstrained news videos. In the generative stage, each video clip is encoded as a bag of SIFT feature vectors, the distribution of which is described by a Gaussian Mixture Models (GMM). In the discriminative stage, the SIFT-Bag Kernel is designed for characterizing the property of Kullback-Leibler divergence between the specialized GMMs of any two video clips, and then this kernel is utilized for supervised learning in two ways. On one hand, this kernel is further refined in discriminating power for centroid-based video event classification by using the Within-Class Covariance Normalization approach, which depresses the kernel components with high-variability for video clips of the same event. On the other hand, the SIFT-Bag Kernel is used in a Support Vector Machine for margin-based video event classification. Finally, the outputs from these two classifiers are fused together for final decision. The experiments on the TRECVID 2005 corpus demonstrate that the mean average precision is boosted from the best reported 38.2% in [36] to 60.4% based on our new framework.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {229–238},
numpages = {10},
keywords = {video event recognition, sift-bag, within-class covariation normalization, kernel design},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459392,
author = {Wang, Feng and Jiang, Yu-Gang and Ngo, Chong-Wah},
title = {Video Event Detection Using Motion Relativity and Visual Relatedness},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459392},
doi = {10.1145/1459359.1459392},
abstract = {Event detection plays an essential role in video content analysis. However, the existing features are still weak in event detection because: i) most features just capture what is involved in an event or how the event evolves separately, and thus cannot completely describe the event; ii) to capture event evolution information, only motion distribution over the whole frame is used which proves to be noisy in unconstrained videos; iii) the estimated object motion is usually distorted by camera movement. To cope with these problems, in this paper, we propose a new motion feature, namely Expanded Relative Motion Histogram of Bag-of-Visual-Words (ERMH-BoW) to employ motion relativity and visual relatedness for event detection. In ERMH-BoW, by representing what aspect of an event with Bag-of-Visual-Words (BoW), we construct relative motion histograms between visual words to depict the object activities or how aspect of the event. ERMH-BoW thus integrates both what and how aspects for a complete event description. Instead of motion distribution features, local motion of visual words is employed which is more discriminative in event detection. Meanwhile, we show that by employing relative motion, ERMH-BoW is able to honestly describe object activities in an event regardless of varying camera movement. Besides, to alleviate the visual word correlation problem in BoW, we propose a novel method to expand the relative motion histogram. The expansion is achieved by diffusing the relative motion among correlated visual words measured by visual relatedness. To validate the effectiveness of the proposed feature, ERMH-BoW is used to measure video clip similarity with Earth Mover's Distance (EMD) for event detection. We conduct experiments for detecting LSCOM events in TRECVID 2005 video corpus, and performance is improved by 74% and 24% compared with existing motion distribution feature and BoW feature respectively.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {239–248},
numpages = {10},
keywords = {visual relatedness, video event detection, motion relativity},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459393,
author = {Wang, Gang and Chua, Tat-Seng and Zhao, Ming},
title = {Exploring Knowledge of Sub-Domain in a Multi-Resolution Bootstrapping Framework for Concept Detection in News Video},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459393},
doi = {10.1145/1459359.1459393},
abstract = {In this paper, we present a model based on a multi-resolution, multi-source and multi-modal (M3) bootstrapping framework that exploits knowledge of sub-domains for concept detection in news video. Because the characteristics and distributions of data in different sub-domains are different, we model and analyze the video in each sub-domain separately using a transductive framework. Along with this framework, we propose a "pseudo-Vapnik combined error bound" to tackle the problem of imbalanced distribution of training data in certain segments of sub-domains. For effective fusion of multi-modal features, we utilize multi-resolution inference and constraints to permit evidences from different modal features to support each other. Finally, we employ a bootstrapping technique to leverage unlabeled data to boost the overall system performance. We test our framework by detecting semantic concepts in the TRECVID 2004 dataset. Experimental results demonstrate that our approach is effective.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {249–258},
numpages = {10},
keywords = {bootstrapping, transductive learning, domain knowledge, multi-resolution analysis, unlabeled data, text semantics},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256772,
author = {Ooi, Wei Tsang},
title = {Session Details: Systems Track S1: Video Streaming},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256772},
doi = {10.1145/3256772},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459395,
author = {Tasaka, Shuji and Yoshimi, Hikaru and Hirashima, Akifumi and Nunome, Toshiro},
title = {The Effectiveness of a QoE-Based Video Output Scheme for Audio-Video Ip Transmission},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459395},
doi = {10.1145/1459359.1459395},
abstract = {This paper shows the effectiveness and feasibility of a video output scheme the authors proposed previously to enhance QoE (Quality of Experience) (i.e., perceptual QoS) of audio-video IP transmission. The scheme, SCS (Switching between error Concealment and frame Skipping), utilizes a tradeoff relation between spatial and temporal quality caused by video error concealment and video frame skipping, both of which cope with video packet loss. The scheme switches from error concealment to frame skipping when the percentage of video slices error-concealed in a frame exceeds a threshold value, which is a key to achieving high QoE with SCS. We propose a way of selecting appropriate threshold values on the basis of real-time estimation of QoE. For that purpose, we performed two experiments for four values of the threshold. One experiment is the measurement of application-level QoS and QoE, from which we confirm the effectiveness of SCS and derive multiple regression lines that estimate QoE from the application-level QoS. The other is the measurement of the percentage of the selected threshold value by the proposed way with the multiple regression lines. Examining the QoE measured by the first experiment when we adopt the selected threshold values in the second experiment, we find that the way gives appropriate selections in many cases and therefore the SCS with the way of threshold selection is feasible.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {259–268},
numpages = {10},
keywords = {perceptual qos, audio-video ip transmission, qos, qoe, video frame skipping, video error concealment},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459396,
author = {Feng, Chen and Li, Baochun},
title = {On Large-Scale Peer-to-Peer Streaming Systems with Network Coding},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459396},
doi = {10.1145/1459359.1459396},
abstract = {Live peer-to-peer (P2P) streaming has recently received much research attention, with successful commercial systems showing its viability in the Internet. Nevertheless, existing analytical studies of P2P streaming systems have failed to mathematically investigate and understand their critical properties, especially with a large scale and under extreme dynamics such as a flash crowd scenario. Even more importantly, there exists no prior analytical work that focuses on an entirely new way of designing streaming protocols, with the help of network coding. In this paper, we seek to show an in-depth analytical understanding of fundamental properties of P2P streaming systems, with a particular spotlight on the benefits of network coding. We show that, if network coding is used according to certain design principles, provably good performance can be guaranteed, with respect to high playback qualities, short initial buffering delays, resilience to peer dynamics, as well as minimal bandwidth costs on dedicated streaming servers. Our results are obtained with mathematical rigor, but without sacrificing realistic assumptions of system scale, peer dynamics, and upload capacities. For further insights, streaming systems using network coding are compared with traditional pull-based streaming in large-scale simulations, with a focus on fundamentals, rather than protocol details. The scale of our simulations throughout this paper exceeds 200,000 peers at times, which is in sharp contrast with existing empirical studies, typically with a few hundred peers involved.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {269–278},
numpages = {10},
keywords = {peer-to-peer streaming, performance analysis, network coding},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459397,
author = {Wang, Jiajun and Huang, Cheng and Li, Jin},
title = {On ISP-Friendly Rate Allocation for Peer-Assisted VoD},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459397},
doi = {10.1145/1459359.1459397},
abstract = {Peer-to-peer (P2P) content distribution is able to greatly reduce dependence on infrastructure servers and scale up to the demand of the Internet video era. However, the rapid growth of P2P applications has also created immense burden on service providers by generating significant ISP-unfriendly traffic, such as cross-ISP and inter-POP traffic. In this work, we consider the unique properties of peer-assisted Video-on-Demand (VoD) and design a distributed rate allocation algorithm, which can significantly cut down on ISP-unfriendly traffic without much impact on server load. Through extensive packet-level simulation with both synthetic and real-world traces, we show that the rate allocation algorithm can achieve substantial additional gain, on top of previously proposed schemes advocating ISP-friendly topologies.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {279–288},
numpages = {10},
keywords = {peer-to-peer, rate allocation, video-on-demand, isp-friendly},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459398,
author = {Turaga, Deepak S. and Foo, Brian and Verscheure, Olivier and Yan, Rong},
title = {Configuring Topologies of Distributed Semantic Concept Classifiers for Continuous Multimedia Stream Processing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459398},
doi = {10.1145/1459359.1459398},
abstract = {Real-time multimedia semantic concept detection requires instant identification of a set of concepts in streaming video or images. However, the potentially high data volumes of multimedia content, and high complexity associated with individual concept detectors, have hindered its practical deployment. In this paper, we present a new online concept detection system deployed on top of a distributed stream mining system. It uses a tree-topology of classifiers that are constructed on a semantic hierarchy of concepts of interest. We introduce a novel methodology for configuring such cascaded classifier topologies under constraints on the available resources. In our approach, we configure individual classifiers with optimized operating points after jointly and explicitly considering the misclassification cost of each end-to-end class of interest in the tree, the system imposed resource constraints, and the confidence level of each object that is classified. We describe the implemented application, system, and optimization algorithms, and verify that significant improvement in terms of accuracy of classification can be achieved through our approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {289–298},
numpages = {10},
keywords = {semantic concept detection, resource constrained mining, multimedia stream mining},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256773,
author = {Liu, Leslie},
title = {Session Details: Systems Track S2: Beyond 2D},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256773},
doi = {10.1145/3256773},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459400,
author = {Zimmermann, Roger and Liang, Ke},
title = {Spatialized Audio Streaming for Networked Virtual Environments},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459400},
doi = {10.1145/1459359.1459400},
abstract = {Networked virtual environments (NVE) are increasingly popular and represent a range of applications. Some online virtual worlds have a dedicated purpose, such as Massively Multiplayer Online Games (MMOG), while others implement more of foundational frameworks which are not necessarily applications per se, but form platforms to create applications. One of the premier examples of the latter is Second Life from Linden Lab. While such networked virtual environments show great potential for interesting and novel applications, many technical challenges remain. A significant shortcoming in current systems concerns the communication between virtual world participants. Early designs only supported text chat, while more recently voice chat has been introduced. However, often the voice communication paradigm still mimics traditional audio group conferencing without adaptation to the three-dimensional metaverse paradigm. In this study we present our design of an interactive audio streaming protocol that aims to enable the creation of an aural soundscape around the user that matches the visual experience. To make our design practical, scalable and to avoid further burdening the virtual world servers, our protocol is based on a peer-to-peer distribution topology. Simulation results are presented that show the feasibility and utility of our design.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {299–308},
numpages = {10},
keywords = {spatial audio, peer-to-peer streaming, proximity audio},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459401,
author = {Ay, Sakire Arslan and Zimmermann, Roger and Kim, Seon Ho},
title = {Viewable Scene Modeling for Geospatial Video Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459401},
doi = {10.1145/1459359.1459401},
abstract = {Video sensors are becoming ubiquitous and the volume of captured video material is very large. Therefore, tools for searching video databases are indispensable. Current techniques that extract features purely based on the visual signals of a video are struggling to achieve good results. By considering video related meta-information, more relevant and precisely delimited search results can be obtained. In this study we propose a novel approach for querying videos based on the notion that the geographical location of the captured scene in addition to the location of a camera can provide valuable information and may be used as a search criterion in many applications. This study provides an estimation model of the viewable area of a scene for indexing and searching and reports on a prototype implementation. Among our objectives is to stimulate a discussion of these topics in the research community as information fusion of different georeferenced data sources is becoming increasingly important. Initial results illustrate the feasibility of the proposed approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {309–318},
numpages = {10},
keywords = {georeferencing, gps, video search, meta-data},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459402,
author = {Lee, Kyoungwoo and Shrivastava, Aviral and Kim, Minyoung and Dutt, Nikil and Venkatasubramanian, Nalini},
title = {Mitigating the Impact of Hardware Defects on Multimedia Applications: A Cross-Layer Approach},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459402},
doi = {10.1145/1459359.1459402},
abstract = {Increasing exponentially with each technology generation, hardware-induced soft errors pose a significant threat for the reliability of mobile multimedia devices. Since traditional hardware error protection techniques incur significant power and performance overheads, this paper proposes a cooperative cross-layer approach that exploits existing error control schemes at the application layer to mitigate the impact of hardware defects. Specifically, we propose error detection codes in hardware, drop and forward recovery in middleware, and error-resilient video encoding at the application level to effectively and efficiently combat soft errors with minimal overheads. Experimental evaluation on standard test video streams demonstrates that our cooperative error-aware method for video encoding improves performance by 60% and energy consumption by 58% with even better reliability at the cost of only 3% quality degradation on average, as compared to an error correction code based hardware protection technique. Combining intelligent schemes to select a recovery mechanism can guide system designers to trade off multiple constraints such as performance, power, reliability, and QoS.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {319–328},
numpages = {10},
keywords = {cross-layer, error-awareness, video encoding, soft error},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256774,
author = {Muhlhauser, Max},
title = {Session Details: Applications Track A1: Tracing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256774},
doi = {10.1145/3256774},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459404,
author = {Liu, Feng and Hu, Yu-hen and Gleicher, Michael L.},
title = {Discovering Panoramas in Web Videos},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459404},
doi = {10.1145/1459359.1459404},
abstract = {While methods for stitching panoramas have been successful given proper source images, providing these source images still remains a burden. In this paper, we present a method to discover panoramic source images within widely available web videos. The challenge comes from the fact that many of these videos are not recorded intentionally for stitching panoramas. Our method aims to find segments within a video that work as panorama sources. Specifically, we determine a video segment to be a valid panorama source according to the following three criteria. First, its camera motion should cover a wide field-of-view of the scene. Second, its frames should be "mosaicable", which states that the inter-frame motion should observe the underlying conditions for stitching a panorama. Third, its frames should have good image quality. Based on these criteria, we formulate discovering panoramas in a video as an optimization problem that aims to find an optimal set of video segments as panorama sources. After discovering these panorama sources, we synthesize regular scene panoramas using them. When significant dynamics is detected in the sources, we fuse the dynamics into the scene panoramas to make activity synopses to convey the dynamics. Our experiment of querying panoramas from YouTube confirms the feasibility of using web videos as panorama sources and demonstrates the effectiveness of our method.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {329–338},
numpages = {10},
keywords = {discovering panorama, scene panorama, activity synopsis},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459405,
author = {Hopfgartner, Frank and Vallet, David and Halvey, Martin and Jose, Joemon},
title = {Search Trails Using User Feedback to Improve Video Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459405},
doi = {10.1145/1459359.1459405},
abstract = {In this paper we present an innovative approach for aiding users in the difficult task of video search. We use community based feedback mined from the interactions of previous users of our video search system to aid users in their search tasks. This feedback is the basis for providing recommendations to users of our video retrieval system. The ultimate goal of this system is to improve the quality of the results that users find, and in doing so, help users to explore a large and difficult information space and help them consider search options that they may not have considered otherwise. In particular we wish to make the difficult task of search for video much easier for users. The results of a user evaluation indicate that we achieved our goals, the performance of the users in retrieving relevant videos improved, and users were able to explore the collection to a greater extent.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {339–348},
numpages = {10},
keywords = {video, recommender, community, search, collaborative, user studies, feedback},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459406,
author = {Kennedy, Lyndon and Chang, Shih-Fu},
title = {Internet Image Archaeology: Automatically Tracing the Manipulation History of Photographs on the Web},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459406},
doi = {10.1145/1459359.1459406},
abstract = {We propose a system for automatically detecting the ways in which images have been copied and edited or manipulated. We draw upon these manipulation cues to construct probable parent-child relationships between pairs of images, where the child image was derived through a series of visual manipulations on the parent image. Through the detection of these relationships across a plurality of images, we can construct a history of the image, called the visual migration map (VMM), which traces the manipulations applied to the image through past generations. We propose to apply VMMs as part of a larger internet image archaeology system (IIAS), which can process a given set of related images and surface many interesting instances of images from within the set. In particular, the image closest to the "original" photograph might be among the images with the most descendants in the VMM. Or, the images that are most deeply descended from the original may exhibit unique differences and changes in the perspective being conveyed by the author. We evaluate the system across a set of photographs crawled from the web and find that many types of image manipulations can be automatically detected and used to construct plausible VMMs. These maps can then be successfully mined to find interesting instances of images and to suppress uninteresting or redundant ones, leading to a better understanding of how images are used over different times, sources, and contexts.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {349–358},
numpages = {10},
keywords = {image manipulation history, internet image mining, perspective discovery},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256775,
author = {Prabhakaran, B.},
title = {Session Details: Applications Track A2: Watch},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256775},
doi = {10.1145/3256775},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459408,
author = {Liu, Xu and Doermann, David and Li, Huiping},
title = {A Camera-Based Mobile Data Channel: Capacity and Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459408},
doi = {10.1145/1459359.1459408},
abstract = {In this paper we propose a novel application, color Video Code (V-Code) and analyze its data transmission capacity through camera-based mobile data channels. Users can use the camera on a mobile device (PDA or camera phone) as a passive and pervasive data channel to download data encoded as a sequence of color visual patterns. The color V-Code is animated on a display, acquired by the camera and decoded by the pre-embedded software in the mobile device. One interesting question is what is the data transmission capacity it can achieve, theoretically and practically. To answer this question we build a camera channel model to measure color degradation using information theory and show that the capacity of the camera channel can be improved with the optimized color selection through color calibration. After initialization color models are learned automatically as downloading proceeds. We address the problem of precise registration, and implemented a fast perspective correction method to accelerate the decoder in real-time on a resource constrained device. With the optimized color set and efficient implementation we achieve a transmission bit rate of 15.4kbps on a common iMate Jamin phone (200MHz CPU). This speed is faster than the average GPRS bit rate (12kbps).},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {359–368},
numpages = {10},
keywords = {image processing, camera phone, information theory},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459409,
author = {Leow, Wee Kheng and Chiang, Cheng-Chieh and Hung, Yi-Ping},
title = {Localization and Mapping of Surveillance Cameras in City Map},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459409},
doi = {10.1145/1459359.1459409},
abstract = {Many large cities have installed surveillance cameras to monitor human activities for security purposes. An important surveillance application is to track the motion of an object of interest, e.g., a car or a human, using one or more cameras, and plot the motion path in a city map. To achieve this goal, it is necessary to localize the cameras in the city map and to determine the correspondence mappings between the positions in the city map and the camera views. Since the view of the city map is roughly orthogonal to the camera views, there are very few common features between the two views for a computer vision algorithm to correctly identify corresponding points automatically. This paper proposes a method for camera localization and position mapping that requires minimum user inputs. Given approximate corresponding points between the city map and a camera view identified by a user, the method computes the orientation and position of the camera in the city map, and determines the mapping between the positions in the city map and the camera view. Both quantitative tests and practical application test have been performed. It can obtain the best-fit solutions even though the user-specified correspondence is inaccurate. The performance of the method is assessed in both quantitative tests and practical application. Quantitative test results show that the method is accurate and robust in camera localization and position mapping. Application test results are very encouraging, showing the usefulness of the method in real applications.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {369–378},
numpages = {10},
keywords = {surveillance, position mapping, camera localization},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459410,
author = {Liu, Huiying and Jiang, Shuqiang and Huang, Qingming and Xu, Changsheng},
title = {A Generic Virtual Content Insertion System Based on Visual Attention Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459410},
doi = {10.1145/1459359.1459410},
abstract = {This paper presents a generic Virtual Content Insertion (VCI) system based on visual attention analysis. VCI is an emerging application of video analysis and has been used in video augmentation and advertisement insertion. There are three critical issues for a VCI system: when (time), where (place) and how (method) to insert the Virtual Content (VC) into the video. Our system selects the insertion time and place by performing temporal and spatial attention analysis, which predicts the attention change along time and the attended region over space. In order to enable the inserted VC to be noticed by audience while not to interrupt the audience's viewing experience to the original content, the VC should be inserted at the time when the video content attracts much audience attention and at the place where attracts less. Dynamic insertion is performed by using Global Motion Estimation (GME) and affine transformation. Our VCI system is able to obtain an optimal balance between the notice of the VC by audience and disruption of viewing experience to the original content. Extensive subjective evaluations based on user study on the VCI result have verified the effectiveness of the system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {379–388},
numpages = {10},
keywords = {virtual content insertion, visual attention},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256776,
author = {Sundaram, Hari},
title = {Session Details: Applications Track A3: Photo},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256776},
doi = {10.1145/3256776},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459412,
author = {Yeh, Tom and Lee, John J. and Darrell, Trevor},
title = {Photo-Based Question Answering},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459412},
doi = {10.1145/1459359.1459412},
abstract = {Photo-based question answering is a useful way of finding information about physical objects. Current question answering (QA) systems are text-based and can be difficult to use when a question involves an object with distinct visual features. A photo-based QA system allows direct use of a photo to refer to the object. We develop a three-layer system architecture for photo-based QA that brings together recent technical achievements in question answering and image matching. The first, template-based QA layer matches a query photo to online images and extracts structured data from multimedia databases to answer questions about the photo. To simplify image matching, it exploits the question text to filter images based on categories and keywords. The second, information retrieval QA layer searches an internal repository of resolved photo-based questions to retrieve relevant answers. The third, human-computation QA layer leverages community experts to handle the most difficult cases. A series of experiments performed on a pilot dataset of 30,000 images of books, movie DVD covers, grocery items, and landmarks demonstrate the technical feasibility of this architecture. We present three prototypes to show how photo-based QA can be built into an online album, a text-based QA, and a mobile application.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {389–398},
numpages = {10},
keywords = {information retrieval, computer vision, question answering},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459413,
author = {Erol, Berna and Ant\'{u}nez, Emilio and Hull, Jonathan J.},
title = {HOTPAPER: Multimedia Interaction with Paper Using Mobile Phones},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459413},
doi = {10.1145/1459359.1459413},
abstract = {The popularity of camera phones enables many exciting multimedia applications. In this paper, we present a novel technology and several applications that allow users to interact with paper documents, books, and magazines. This interaction is in the form of reading and writing electronic information, such as images, web urls, video, and audio, to the paper medium by pointing a camera phone at a patch of text on a document. Our application does not require any special markings, barcodes, or watermarks on the paper document. Instead, we propose a document recognition algorithm that automatically determines the location of a patch of text in a large collection of document images given a small document image. This is very challenging because the majority of phone cameras lack autofocus and macro capabilities and they produce low quality images and video. We developed a novel algorithm, Brick Wall Coding (BWC), that performs image-based document recognition using the mobile phone video frames. Given a document patch image, BWC utilizes the layout, i.e. relative locations, of word boxes in order to determine the original file, page, and the location on the page. BWC runs real-time (4 frames per second) on a Treo 700w smartphone with a 312 MHz processor and 64MB RAM. Using our method we can recognize blurry document patch frames that contain as little as 4-5 lines of text and a video resolution as low as 176x144. We performed experiments by indexing 4397 document pages and querying this database with 533 document patches. Besides describing the basic algorithm, this paper also describes several applications that are enabled by mobile phone-paper interaction, such as inserting electronic annotations to paper, using paper as a tangible interface to collect and communicate multimedia data, and collaborative homework.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {399–408},
numpages = {10},
keywords = {markerless linking, mobile interaction, mobile imaging, linking paper to electronic data},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459414,
author = {Henze, Niels and Boll, Susanne},
title = {Snap and Share Your Photobooks},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459414},
doi = {10.1145/1459359.1459414},
abstract = {The sharing of photos with others, friends and family, has become even more popular with digital photography and Internet applications such as email or Web albums. At the same time, the physical touch of printed photos is still appreciated and customers use different services to print their photos on post cards, calendars or photobooks, often to give them as a present or to create a physical souvenir. Once printed, however, the sharing of photos with others becomes difficult as there is no link back from the now physical item to its digital counterpart. With Bookmarkr, we developed a system that employs a mobile camera phone to bridge the gap between the printed photo and its digital counterpart. The user takes an image of a photo in a photobook with the mobile phone's camera. The image is transmitted to a photobook server, which employs image analysis techniques to retrieve the corresponding photo. The photo is sent back to the user and establishes the digital-physical match. Bookmarkr allows the user to interact with printed photos in a similar way as interacting with digital photos by using a point-and-shoot metaphor. Our performance evaluation shows that the system is able to return the correct photo in up to 99% of all cases. A conducted user study revealed that the designed interaction is suitable for the participants and their printed photobooks.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {409–418},
numpages = {10},
keywords = {photo sharing, photobook, mobile interaction, image analysis, contextual bookmark},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459415,
author = {Hsieh, Chi-Chang and Cheng, Wen-Huang and Chang, Chia-Hu and Chuang, Yung-Yu and Wu, Ja-Ling},
title = {Photo Navigator},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459415},
doi = {10.1145/1459359.1459415},
abstract = {Nowadays, travel has become a popular activity for people to relax their body and mind. Taking photos is then often an inevitable and frequent event during one's trip for recording the enjoyable experience. To help people to relive the wonderful travel experience they had recorded in photos, this paper presents a system, Photo Navigator, for enhancing the photo browsing experience by creating a new browsing style with a realistic feel to users as being into the scenes and taking a trip back in time to revisit the place. The proposed system is characterized by two main features. First, it better reveals the spatial relations among photos and offers a strong sense of space by taking users to fly into the scenes. Second, it is fully automatic and makes plausible for novice users to utilize the 3D technologies that are traditionally complex to manipulate. The proposed system is compared with two other photo browsing tools, ACDSee's photo slideshow and Microsoft's PhotoStory. User studies show that people would comparatively favor the browsing style we offer and appreciate the ease to create such a style.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {419–428},
numpages = {10},
keywords = {fly-through, image-based modeling, automation, sense of space, photo browsing},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256777,
author = {Bulterman, Dick},
title = {Session Details: Applications Track A4: Context},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256777},
doi = {10.1145/3256777},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459417,
author = {Shao, Jie and Huang, Zi and Shen, Heng Tao and Shen, Jialie and Zhou, Xiaofang},
title = {Distribution-Based Similarity Measures for Multi-Dimensional Point Set Retrieval Applications},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459417},
doi = {10.1145/1459359.1459417},
abstract = {Effective and efficient method of similarity assessment continues to be one of the most fundamental problems in multimedia data analysis. In case of retrieving relevant items from a collection of objects based on series of multivariate observations (e.g., searching the similar video clips in a repository to a query example), satisfactory performance cannot be expected using many conventional similarity measures based on the aggregation of element pairwise comparisons. Some correlation information among the individual elements has also been investigated to characterize each set of multi-dimensional points for ranked retrieval, by making use of an unwarranted assumption that the underlying data distribution has a particular parametric form. Motivated by this observation, this paper introduces a novel collective gauge of relevance ranking by evaluating the probabilities that point sets are consistent with the same distribution of the query. Two non-parametric hypothesis tests in statistics are justified to exploit the distributional discrepancy of samples for assessing the similarity between two ensembles of points. While our methodology is mainly presented in the context of video similarity search, it enjoys great flexibility and can be easily adapted to other applications involving generic multi-dimensional point set representation for each object such as human gesture recognition.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {429–438},
numpages = {10},
keywords = {multi-dimensional point set, non-parametric, hypothesis tests, minimal spanning tree, reproducing kernel hilbert space, similarity measures},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459418,
author = {Mei, Tao and Hua, Xian-Sheng and Li, Shipeng},
title = {Contextual In-Image Advertising},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459418},
doi = {10.1145/1459359.1459418},
abstract = {The community-contributed media contents over the Internet have become one of the primary sources for online advertising. However, conventional ad-networks such as Google AdSense treat image and video advertising as general text advertising without considering the inherent characteristics of visual contents. In this work, we propose an innovative contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense, represents the first attempt towards contextual in-image advertising. The relevant ads are selected based on not only textual relevance but also visual similarity so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image saliency to minimize intrusiveness to the user. We evaluate ImageSense on three photo-sharing sites with around one million images and 100 Web pages collected from several major sites, and demonstrate the effectiveness of ImageSense.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {439–448},
numpages = {10},
keywords = {image saliency, web page segmentation, image advertising},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459419,
author = {Haubold, Alexander and Dutta, Promiti and Kender, John R.},
title = {Evaluation of Video Browser Features and User Interaction with VAST MM},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459419},
doi = {10.1145/1459359.1459419},
abstract = {In this paper, we present extensive user studies on browsing and information retrieval in the domain of unstructured videos using the VAST MM video library browser. Our studies were performed over a 3-year period with more than 1,000 participants in the university setting. The majority of students use the video library for retrieval of student presentations in a large engineering design course. Through iterative analysis of context-specific audio, visual, and textual cues, we are able to measure significant improvements on typical retrieval tasks, such as searching for unfamiliar content in a large database with over 300 hours of video. We also present user studies conducted in two videotaped core computer science courses to measure the usefulness of the VAST MM (Video Audio Structure Text MultiMedia) resource for final exam preparation. We find that students who use the lecture video library experience significant improvement in final exam scores.To better compare video browsers featuring rich content cues to standard video players without cues, we have performed a large experiment to collect measurable data on search tasks. In general, the lack of index cues can be described by an inverse relationship between amount of matching video content and time required to find it. When index cues are available, the relationship is constant, that is, rare content is found in the same time as common content. We evaluate this data and provide additional insight into two common user interaction techniques: audio-visual browsing and visual-only browsing. We show that user preference is uniform, but that audio-visual browsing is significantly more effective for search and retrieval of video data.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {449–458},
numpages = {10},
keywords = {text augmentation, speaker segmentation, visual segmentation, transcript analysis, automatic speech recognition, streaming video, presentation video, user studies, evaluation, structure in videos, speaker index, video library, measures},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256778,
author = {Atrey, Pradeep},
title = {Session Details: Applications Track A5/H3: Browsing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256778},
doi = {10.1145/3256778},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459421,
author = {Jia, Jimin and Yu, Nenghai and Hua, Xian-Sheng},
title = {Annotating Personal Albums via Web Mining},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459421},
doi = {10.1145/1459359.1459421},
abstract = {Nowadays personal albums are becoming more and more popular due to the explosive growth of digital image capturing devices. An effective automatic annotation system for personal albums is desired for both efficient browsing and search. Existing research on image annotation evolves through two stages: learning-based methods and web-based methods. Learning-based methods attempt to learn classifiers or joint probabilities between images and concepts, which are difficult to handle large-scale concept sets due to the lack of training data. Web-based methods leverage web image data to learn relevant annotations, which greatly expand the scale of concepts. However, they still suffer two problems: the query image lacks prior knowledge and the annotations are often noisy and incoherent. To address the above issues, we propose a web-based annotation approach to annotate a collection of photos simultaneously, instead of annotating them independently, by leveraging the abundant correlations among the photos. A multi-graph similarity propagation based semi-supervised learning (MGSP-SSL) algorithm is proposed to suppress the noises in the initial annotations from the Web. Experiments on real personal albums show that the proposed approach outperforms existing annotation methods.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {459–468},
numpages = {10},
keywords = {image annotation, similarity propagation, multi-graph, personal albums, semi-supervised learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459422,
author = {H\"{u}rst, Wolfgang and Meier, Konrad},
title = {Interfaces for Timeline-Based Mobile Video Browsing},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459422},
doi = {10.1145/1459359.1459422},
abstract = {Browsing video on mobile devices such as smartphones and PDAs requires new interface designs and interaction concepts because of their small screen sizes. In this paper, we present four different interfaces which enable users to skim video at different replay speed levels: An interface supporting flicking similar to text browsing on an iPhone™, an elastic slider, and two variations which also allow for interactive speed manipulation. Based on a heuristic evaluation with all four designs, revised versions of the two most promising ones have been implemented. A comparative user study proved the usefulness of the proposed designs. Both interfaces showed the same performance (measured in time needed to solve typical browsing tasks) but achieved different results in subjective user assessments.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {469–478},
numpages = {10},
keywords = {mobile video, elastic interfaces, flicking, video browsing, iphone},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459423,
author = {Yang, Yang and Zhu, Bin B. and Guo, Rui and Yang, Linjun and Li, Shipeng and Yu, Nenghai},
title = {A Comprehensive Human Computation Framework: With Application to Image Labeling},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459423},
doi = {10.1145/1459359.1459423},
abstract = {Image and video labeling is important for computers to understand images and videos and for image and video search. Manual labeling is tedious and costly. Automatically image and video labeling is yet a dream. In this paper, we adopt a Web 2.0 approach to labeling images and videos efficiently: Internet users around the world are mobilized to apply their "common sense" to solve problems that are hard for today's computers, such as labeling images and videos. We first propose a general human computation framework that binds problem providers, Web sites, and Internet users together to solve large-scale common sense problems efficiently and economically. The framework addresses the technical challenges such as preventing a malicious party from attacking others, removing answers from bots, and distilling human answers to produce high-quality solutions to the problems. The framework is then applied to labeling images. Three incremental refinement stages are applied. The first stage collects candidate labels of objects in an image. The second stage refines the candidate labels using multiple choices. Synonymic labels are also correlated in this stage. To prevent bots and lazy humans from selecting all the choices, trap labels are generated automatically and intermixed with the candidate labels. Semantic distance is used to ensure that the selected trap labels would be different enough from the candidate labels so that no human users would mistakenly select the trap labels. The last stage is to ask users to locate an object given a label from a segmented image. The experimental results are also reported in this paper. They indicate that our proposed schemes can successfully remove spurious answers from bots and distill human answers to produce high-quality image labels.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {479–488},
numpages = {10},
keywords = {human computation, distributed knowledge acquisition, common sense problems, humansense, image labeling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459424,
author = {Villa, Robert and Gildea, Nicholas and Jose, Joemon M.},
title = {FacetBrowser: A User Interface for Complex Search Tasks},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459424},
doi = {10.1145/1459359.1459424},
abstract = {With the rapid increase in online video services, multimedia retrieval systems are becoming increasingly important search tools to users in many different fields. In this paper we present a novel retrieval interface, "FacetBrowser", which supports the creation of multiple search "facets", to aid users carrying out complex search tasks involving multiple concepts. Each facet represents a different aspect of the search task: an assumption of this work is that search facets are best represented by sub-searches, providing users with flexibility in defining facets on the fly, rather than using pre-defined categories or metadata information as used in many other exploratory search interfaces [3, 8, 17]. Such facets can be organised into "stories" by users, facilitating users in building up sequences of related searches and material which together can be used to satisfy a work task.The interface allows more than one search to be executed and viewed simultaneously, and importantly, allows material to be reorganized between the facets, acknowledging the inter-relatedness which can often occur between search facets. The design of the FacetBrowser interface is presented, along with an experiment comparing it to a tabbed interface similar to that on modern web browsers. The results suggest that the FacetBrowser has the potential to aid users in exploring and structuring their searching effort when carrying out broad search tasks.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {489–498},
numpages = {10},
keywords = {exploratory search, video retrieval, multimedia retrieval},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256779,
author = {Steinbach, Eckehard},
title = {Session Details: HCM Track H1: Application},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256779},
doi = {10.1145/3256779},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459426,
author = {Chen, Shifeng and Tian, Yuandong and Wen, Fang and Xu, Ying-Qing and Tang, Xiaoou},
title = {Easytoon: An Easy and Quick Tool to Personalize a Cartoon Storyboard Using Family Photo Album},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459426},
doi = {10.1145/1459359.1459426},
abstract = {A family photo album based cartoon personalization system, EasyToon, is proposed in this paper. Using state of the art computer vision and graphics technologies and effective UI design, the interactive tool can quickly generate a personalized cartoon storyboard, which naturally blends a real face chosen from the family photo album into a cartoon picture. The personalized cartoon image is easily and quickly obtained in two main steps. First, the best face candidate is selected from the album interactively. Then a personalized cartoon image is automatically synthesized by blending the selected face into the interesting cartoon image. Experiments show that most users express great interest in our system. Without any art background, they can make a personalized cartoon of high quality using the EasyToon within minutes.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {499–508},
numpages = {10},
keywords = {family photo album, cartoon personalization},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459427,
author = {Xiao, Jun and Zhang, Xuemei and Cheatle, Phil and Gao, Yuli and Atkins, C. Brian},
title = {Mixed-Initiative Photo Collage Authoring},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459427},
doi = {10.1145/1459359.1459427},
abstract = {Creating an artifact that captures the story or memory from a large photo collection is a difficult task, because the tools available are either too difficult to learn, or oversimplified to the point that they lack flexibility. Individual techniques have been developed to automate parts of the selection-editing-composition cycle, but relatively little has been done to strike the right overall balance between the fully automatic and the fully manual. In this paper, we present miCollage, which attempts to piece together individual technologies to create a compelling collage authoring experience. The system consists of three main components. In the selection component, the system can make proactive suggestions about which photos to add to the collage as well as help the user to find similar or related photos. In the editing component, the system applies automatic cropping and enhancement to the images. In the layout component, the system suggests alternative layouts but is still able to accommodate manual changes, while satisfying various spatial constraints. The user interface connects the components seamlessly, allowing a best of both worlds between fully manual and fully automatic collage authoring.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {509–518},
numpages = {10},
keywords = {photo collections, constraint satisfaction, image triage, image editing, page layout, mosaics, user-centered design},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459428,
author = {Lanir, Joel and Booth, Kellogg S. and Tang, Anthony},
title = {MultiPresenter: A Presentation System for (Very) Large Display Surfaces},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459428},
doi = {10.1145/1459359.1459428},
abstract = {We introduce MultiPresenter, a novel presentation system designed to work on very large display spaces (multiple displays or physically large high-resolution displays). MultiPresenter allows presenters to organize and present pre-made and dynamic presentations that take advantage of a very large display space accessed from a personal laptop. Presenters can use the extra space to provide long-term persistency of information to the audience. Our design deliberately separates content generation (authoring) from the presentation of content. We focus on supporting presentation flow and a variety of presentation styles, ranging from automated, scripted sequences of pre-made slides to highly dynamic ad-hoc, and non-linear content. By providing smooth transition between these styles, presenters can easily alter the flow of content during a presentation to adapt to an audience or to change emphasis in response to emerging interests. We describe our goals, rationale and the design process, providing a detailed description of the current version of the system, and discuss our experience using it throughout a one-semester first year computer science course.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {519–528},
numpages = {10},
keywords = {high-resolution displays, multiple displays, human-centered design, presentations},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256780,
author = {Gatica-Perez, Daniel},
title = {Session Details: HCM Track H2: Experience},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256780},
doi = {10.1145/3256780},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459430,
author = {Danninger, Maria and Stiefelhagen, Rainer},
title = {A Context-Aware Virtual Secretary in a Smart Office Environment},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459430},
doi = {10.1145/1459359.1459430},
abstract = {A lot of the communication at the workplace - via the phone as well as face-to-face - occurs in inappropriate contexts, disturbing meetings and conversations, invading personal and corporate privacy, and more broadly breaking social norms. This is because both, callers and visitors in front of closed office doors, face the same problem: they can only guess the other person's current availability for a conversation.We present a context-aware Virtual Secretary designed to facilitate more socially appropriate communication at the workplace. This service aims towards understanding a person's activity in smart offices, and passes on important contextual information to callers and visitors in order to facilitate more informed human decisions about how and when to initiate contact.We have deployed this Virtual Secretary in the office of a senior researcher, mediating all his actual phone calls and in-person meetings for several weeks. With the Virtual Secretary active, the number of inappropriate workplace interruptions could be significantly reduced.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {529–538},
numpages = {10},
keywords = {smart space, interruptibility, field experiment, context-awareness, computer-mediated communication, availability, workplace, virtual secretary},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459431,
author = {Junuzovic, Sasa and Hegde, Rajesh and Zhang, Zhengyou and Chou, Phil A. and Liu, Zicheng and Zhang, Cha},
title = {Requirements and Recommendations for an Enhanced Meeting Viewing Experience},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459431},
doi = {10.1145/1459359.1459431},
abstract = {We have found that viewing recorded meetings using traditional meeting viewers whose interfaces consist of an automatic speaker and a fixed context view does not provide sufficient information and control to the users. In particular, a survey of users who watch meeting recordings on a regular basis revealed that it is also useful to provide (1) speaker-related information, including who the speaker is talking to, looking at, and being interrupted by, and (2) more control of the interface, including changing the relative sizes of the speaker and context views and navigating within the context view. We present a 3D interface prototype designed specifically to meet these requirements when viewing recorded meetings. We describe in detail the results of a user study comparing the effectiveness of the new and traditional style interfaces with respect to these requirements. Based on this study, we present a set of guidelines for future interfaces.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {539–548},
numpages = {10},
keywords = {recommendations, requirements, remote meeting viewer},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459432,
author = {Kammerl, Julius and Steinbach, Eckehard},
title = {Deadband-Based Offline-Coding of Haptic Media},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459432},
doi = {10.1145/1459359.1459432},
abstract = {In this work, a novel perceptual coding approach for offline compression of haptic media is presented. Our scheme exploits the properties of human haptic perception and hides coding artifacts introduced by lossy compression below the human perception thresholds. We combine the concept of Just Noticeable Differences with predictive coding in order to achieve high coding efficiency without impairing user perception. In our experiments, we apply the proposed lossy compression scheme to haptic data that has been recorded during a telemanipulation session in a virtual environment. Our user studies reveal that our approach leads to strong data reduction up to 100:1 while preserving a high-quality haptic experience during playback of the compressed haptic data streams.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {549–558},
numpages = {10},
keywords = {predictive coding, perceptual coding, multimedia, haptics, compression, telepresence, differential coding, deadband, psychophysics, teleaction},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256781,
author = {Kerne, Andruid},
title = {Session Details: Art Track 1: Adaptation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256781},
doi = {10.1145/3256781},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459434,
author = {Stockholm, Jack and Pasquier, Philippe},
title = {Eavesdropping: Audience Interaction in Networked Audio Performance},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459434},
doi = {10.1145/1459359.1459434},
abstract = {Eavesdropping is an internet-based, interactive audio system that explores network mediated, musical performance in shared public spaces. The project aims to develop an environment which increases audience interaction and connectedness in a localized, computer-controlled performance. The system is a client-server architecture made of three components: (1) an audio preparation interface, (2) an interactive performance interface, and (3) a machine learning-based conductor. An artificial conductor mixes an acoustic ecology based on mood data entered by participants while learning from their feedback. Technicalities and early evaluation are presented.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {559–568},
numpages = {10},
keywords = {artificial intelligence, computer music, acoustic ecology, net art, reinforcement learning, auditory display},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459435,
author = {Johnson, Kirsten},
title = {Lost Cause, an Interactive Film Project},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459435},
doi = {10.1145/1459359.1459435},
abstract = {One of the challenges in designing an interactive cinematic experience is to offer interactive choices, which does not distract the user from immersion into the story. The interactive film Lost Cause focuses on the life of the main character explored through the perspectives of three characters. The correlated design of interface, interactions and narrative structure in Lost Cause uses different techniques to support an immersive yet engaging interactive story experience. One technique is to build an interface which encourages smooth viewer oscillation between the content of the story and interactivity of the interface. Another technique is challenged-based immersion created through viewer's navigation to discover the story. A user study was conducted to analyze how well interactions in Lost Cause affect viewer understanding and immersion into the story. The results from this study suggest the overall design is versatile and provides various degrees of viewer immersion.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {569–578},
numpages = {10},
keywords = {interactivity, split-screen, interactive cinema, interface design, immersion, interactive narrative},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256782,
author = {Nack, Frank},
title = {Session Details: Art Track 2: Dancing With ...},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256782},
doi = {10.1145/3256782},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459437,
author = {Sheppard, Renata M. and Kamali, Mahsa and Rivas, Raoul and Tamai, Morihiko and Yang, Zhenyu and Wu, Wanmin and Nahrstedt, Klara},
title = {Advancing Interactive Collaborative Mediums through Tele-Immersive Dance (TED): A Symbiotic Creativity and Design Environment for Art and Computer Science},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459437},
doi = {10.1145/1459359.1459437},
abstract = {The Tele-immersive Dance Environment (TED) is a geographically distributed, real-time 3-D virtual room where multiple participants interact independent of physical distance. TED, a highly interactive collaborative environment, offers digital options with multiple viewpoints, enhancing the creative movement composition involved with dance choreography. A symbiotic relationship for creativity and design exists between dance artists and computer scientists as the tele-immersive environment is analyzed as a creativity and learning tool. We introduce the advancements of the interactive digital options, new interface developments, user study results, and the possibility of a computational model for human creativity through Laban Movement Analysis.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {579–588},
numpages = {10},
keywords = {laban movement analysis, tele-immersion, shared virtual space, choreography, collaboration, creativity, dance},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459438,
author = {Naef, Martin and Boyd, Cathie},
title = {Feasibility of the Living Canvas: Restricting Projection to a Performer on Stage},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459438},
doi = {10.1145/1459359.1459438},
abstract = {The Living Canvas initiative aims to use a performer on stage as a dynamic projection surface. Using machine vision in the near-infrared spectrum enables the system to follow and adapt to the performer, restricting projection to the silhouette. Ultimately, the system aims to create the illusion of a completely dynamic costume. This paper introduces the concept and presents an implementation and analysis of the performance-critical stages of the projection pipeline, proving the feasibility of the idea as well as analysing the limitations introduced by current digital projection technology.Bringing together the research from computer graphics and machine vision with the artistic vision and guidance from Cryptic, the initiative aims to create and explore a new expressive medium by taking projection systems on stage to a highly interactive level and providing a powerful new tool for live video artists.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {589–598},
numpages = {10},
keywords = {art &amp; design, performance, augmented reality, projection systems},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256783,
author = {Wakkary, Ron},
title = {Session Details: Art Track 3: A Space ...},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256783},
doi = {10.1145/3256783},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459440,
author = {Pohle, Tim and Knees, Peter and Widmer, Gerhard},
title = {Sound/Tracks: Real-Time Synaesthetic Sonification and Visualisation of Passing Landscapes},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459440},
doi = {10.1145/1459359.1459440},
abstract = {When travelling on a train, many people enjoy looking out of the window at the landscape passing by. We present sound/tracks, an application that translates the perceived movement of the scenery and other visual impressions, such as passing trains, into music. The continuously changing view outside the window is captured with a camera and translated into MIDI events that are replayed instantaneously. This allows for a reflection of the visual impression, adding a sound dimension to the visual experience and deepening the state of contemplation. The application is intended to be run on both mobile phones (with built-in camera) and on laptops (with a connected Web-cam). We propose and discuss different approaches to translating the video signal into an audio stream, present different application scenarios, and introduce a method to visualise the dynamics of complete train journeys by "re-transcribing" the captured video frames used to generate the music.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {599–608},
numpages = {10},
keywords = {mobile music generation, sonification, train journey},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459441,
author = {Kortbek, Karen Johanne and Gr\o{}nb\ae{}k, Kaj},
title = {Interactive Spatial Multimedia for Communication of Art in the Physical Museum Space},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459441},
doi = {10.1145/1459359.1459441},
abstract = {This paper discusses the application of three spatial multimedia techniques for communication of art in the physical museum space. In contrast to the widespread use of computers in cultural heritage and natural science museums, it is generally a challenge to introduce technology in art museums without disturbing the art works. This has usually been limited to individual audio guides. In our case we strive to achieve holistic and social experiences with seamless transitions between art experience and communication related to the artworks.To reach a holistic experience with minimal disturbance of the artworks we apply three spatial multimedia techniques where the only interaction device needed is the human body. The three techniques are: 1) spatially bounded audio; 2) floor-based multimedia; 3) multimedia interior. The paper describes the application of these techniques for communication of information in a Mariko Mori exhibition. The multimedia installations and their implementation are described. It is argued that the utilization of the spatial multimedia techniques support holistic and social art experience. The multimedia installations were in function for a three and a half month exhibition period and they were approved on beforehand by the artist to be in concordance with the artworks.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {609–618},
numpages = {10},
keywords = {communicating art, spatial multimedia, art museums, body as an interaction device, interaction design, directional audio, user experience},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256784,
author = {Ngo, Chong-Wah},
title = {Session Details: Content Track Short Papers Session 1: Content Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256784},
doi = {10.1145/3256784},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459443,
author = {Ji, Rongrong and Sun, Xiaoshui and Yao, Hongxun and Xu, Pengfei and Liu, Tianqiang and Liu, Xianming},
title = {Attention-Driven Action Retrieval with DTW-Based 3d Descriptor Matching},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459443},
doi = {10.1145/1459359.1459443},
abstract = {From visual perception viewpoint, actions in videos can capture high-level semantics for video content understanding and retrieval. However, action-level video retrieval meets great challenges, due to the interferences from global motions or concurrent actions, and the difficulties in robust action describing and matching. This paper presents a content-based action retrieval framework to enable effective search of near-duplicated actions in large-scale video database. Firstly, we present an attention shift model to distill and partition human-concerned saliency actions from global motions and concurrent actions. Secondly, to characterize each saliency action, we extract 3D-SIFT descriptor within its spatial-temporal region, which is robust against rotation, scale, and view point variances. Finally, action similarity is measured using Dynamic Time Warping (DTW) distance to offer tolerance for action duration variance and partial motion missing. Search efficiency in large-scale dataset is achieved by hierarchical descriptor indexing and approximate nearest-neighbor search. In validation, we present a prototype system VILAR to facilitate action search within "Friends" soap operas with excellent accuracy, efficiency, and human perception revealing ability.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {619–622},
numpages = {4},
keywords = {dynamic time warping, attention shift, 3d-sift, action retrieval, video content analysis},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459444,
author = {Liu, Ming and Chen, Shifeng and Liu, Jianzhuang},
title = {Precise Object Cutout from Images},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459444},
doi = {10.1145/1459359.1459444},
abstract = {In this paper we propose a novel approach to the problem of interactive foreground/background segmentation in images. With user provided strokes which indicate foreground and background seeds, we estimate two Gaussian mixture models, one for foreground and the other for background, and define two quantities to measure the initial probabilities of each pixel belonging to the foreground and the background respectively. An optimization function constructed based on the quantities and the boundary and coherent region information is proposed to solve the segmentation problem. By relaxing the hard binary segmentation to a soft labelling problem in the continuous domain, a closed form global optimal solution can be achieved, which directly results in the final binary segmentation output. Experimental results demonstrate the excellent performance of our algorithm.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {623–626},
numpages = {4},
keywords = {interactive image segmentation, label relaxation, optimization},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459445,
author = {Cheng, Xiangang and Hu, Yiqun and Chia, Liang-Tien},
title = {Image Near-Duplicate Retrieval Using Local Dependencies in Spatial-Scale Space},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459445},
doi = {10.1145/1459359.1459445},
abstract = {This paper presents an efficient and effective solution for retrieving Image Near-Duplicate (IND). Different from traditional methods, we analyze the local dependencies among region descriptors in a spatial-scale space. Such local dependencies in spatial-scale space(LDSS) encodes not only visual appearance but also the spatial and scale co-occurrence of them. The local dependencies are integrated over all spatial locations and multiple scales to form the image representation, which is invariant to spatial transformation and scale change. We evaluate our proposed LDSS method for IND retrieval using an existing benchmark as well as a new dataset extracted from the keyframes of TRECVID corpus. Compared to the state-of-the-art results, local dependencies in spatial-scale space(LDSS) approach has been shown to significantly improve the accuracy of IND retrieval.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {627–630},
numpages = {4},
keywords = {local dependencies, bag-of-words, co-occurrence, image near-duplicate},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459446,
author = {Tang, Jinhui and Li, Haojie and Qi, Guo-Jun and Chua, Tat-Seng},
title = {Integrated Graph-Based Semi-Supervised Multiple/Single Instance Learning Framework for Image Annotation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459446},
doi = {10.1145/1459359.1459446},
abstract = {Recently, many learning methods based on multiple-instance (local) or single-instance (global) representations of images have been proposed for image annotation. Their performances on image annotation, however, are mixed as for certain concepts the single-instance representations of images are more suitable, while for some other concepts the multiple-instance representations are better. Thus in this paper, we explore an unified learning framework that combines the multiple-instance and single-instance representations for image annotation. More specifically, we propose an integrated graph-based semi-supervised learning framework to utilize these two types of representations simultaneously, and explore an effective and computationally efficient strategy to convert the multiple-instance representation into a single-instance one. Experiments conducted on the Coral image dataset show the effectiveness and efficiency of the proposed integrated framework.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {631–634},
numpages = {4},
keywords = {image annotation, multiple/single instance learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459447,
author = {Sun, Yongqing and Shimada, Satoshi and Taniguchi, Yukinobu and Kojima, Akira},
title = {A Novel Region-Based Approach to Visual Concept Modeling Using Web Images},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459447},
doi = {10.1145/1459359.1459447},
abstract = {A novel region-based approach is proposed to model semantic concepts using web images. Web images are mined to obtain multiple visual patterns automatically that then are used to model a semantic concept. First, the salient region groups corresponding to the representative visual patterns of a concept are mined and selected as positive samples. Next, a representative visual pattern is built in each salient region group by using a BDA classifier. Finally all the visual patterns are aggregated to describe the concept by using a BDA ensemble approach. Because the proposed method models a semantic concept utilizing multiple visual patterns, it enhances the visual variability of a visual model when learning from diverse web images and improves the robustness of the visual model in handling segmentation-related uncertainties. Experiment results demonstrate our method performs well on generic images including not only "object" concepts, but also complex "scene" concepts.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {635–638},
numpages = {4},
keywords = {web image mining, visual concept modeling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459448,
author = {Jia, Yangqing and Wang, Jingdong and Zhang, Changshui and Hua, Xian-Sheng},
title = {Finding Image Exemplars Using Fast Sparse Affinity Propagation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459448},
doi = {10.1145/1459359.1459448},
abstract = {In this paper, we propose a novel approach to organize image search results obtained from state-of-the-art image search engines in order to improve user experience. We aim to discover exemplars from search results and simultaneously group the images. The exemplars are delivered to the user as a summary of search results instead of the large amount of unorganized images. This gives the user a brief overview of search results with a small amount of images, and helps the user to further find the images of interest. We adopt the idea of affinity propagation and design a fast sparse affinity propagation algorithm to find exemplars that best represent the image search results. Experiments on real-world data demonstrate the effectiveness of our method both visually and quantitatively.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {639–642},
numpages = {4},
keywords = {image exemplar, sparse affinity propagation, web search},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459449,
author = {H\"{o}rster, Eva and Lienhart, Rainer},
title = {Deep Networks for Image Retrieval on Large-Scale Databases},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459449},
doi = {10.1145/1459359.1459449},
abstract = {Currently there are hundreds of millions (high-quality) images in online image repositories such as Flickr. This makes is necessary to develop new algorithms that allow for searching and browsing in those large-scale databases. In this work we explore deep networks for deriving a low-dimensional image representation appropriate for image retrieval. A deep network consisting of multiple layers of features aims to capture higher order correlations between basic image features. We will evaluate our approach on a real world large-scale image database and compare it to image representations based on topic models. Our results show the suitability of the approach for very large databases.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {643–646},
numpages = {4},
keywords = {image retrieval, deep networks},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459450,
author = {Wang, Meng and Hua, Xian-Sheng},
title = {Study on the Combination of Video Concept Detectors},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459450},
doi = {10.1145/1459359.1459450},
abstract = {This paper studies the combination of video concept detectors with a labeled fusion set. We point out that the computational cost of the grid search for fusion weights increases exponentially with the number of detectors, and it is thus infeasible when dealing with a large number of detectors. To avoid the difficulty, we adopt incremental fusion approach, i.e., in each round two detectors are combined and hence only 1-dimensional grid search is needed. We propose a Bottom-Up Incremental Fusion (BUIF) method which keeps selecting the detectors with lowest performance for combination. We conduct experiments on TRECVID benchmark dataset for 39 concepts with 38 detection methods. Ten different fusion strategies are compared, and empirical results have demonstrated the superiority of the proposed incremental fusion approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {647–650},
numpages = {4},
keywords = {video concept detection, fusion},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459451,
author = {Wu, Po Tun and Yang, Yi Hsuan and Chen, Kuan Ting and Hsu, Winston H. and Li, Tien Hsu and Lee, Chun Jen},
title = {Keyword-Based Concept Search on Consumer Photos by Web-Based Kernel Function},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459451},
doi = {10.1145/1459359.1459451},
abstract = {In light of the strong demands for semantic search over large-scale consumer photos, which generally lack reliable user-provided annotations, we investigate the feasibility and challenges entailed by the new paradigm, concept search - retrieving visual objects by large-scale automatic concept detectors with keywords. We investigate the problem in three folds: (1) the effective concept mapping and selection methods over large-scale concept ontology; (2) the quality and feasibility of the pre-trained concept detectors applying on cross-domain consumer data (i.e., Flickr photos); (3) the search quality by fusing automatic concepts and user-annotated data (tags). Through experiments over large-scale benchmarks, TRECVID and Flickr550, we confirm the effectiveness of concept search in the proposed framework, where the semantic mapping by web-based kernel function over Google snippets significantly outperforms conventional WordNet-like methods both in accuracy and efficiency.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {651–654},
numpages = {4},
keywords = {retrieval, web-based kernel function, consumer photo, query-concept mapping, concept search},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459452,
author = {Yang, Yi and Zhuang, Yueting and Wang, Wenhua},
title = {Heterogeneous Multimedia Data Semantics Mining Using Content and Location Context},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459452},
doi = {10.1145/1459359.1459452},
abstract = {Because it is very common that the heterogeneous multimedia data of the same semantics always exist jointly in many domain and application specific databases, it is very helpful to consider the location information when analyzing multimedia data. In this paper we propose a method of integrating the content and location context for multimedia data mining to enable the cross-media retrieval, by which the query examples and the returned results can be of different modalities, e.g. to query audios by an example of image. We construct a graph model by combing the multimedia content and location information. The graph model is then refined according to different strategies. The semantic correlations among multimedia data are calculated by learning the high-order neighborhood structure of the graph and the Multimedia Correlation Space is constructed in which the cross-media retrieval can be performed. We also propose different methods of Relevance Feedback to improve the search results. Experiments demonstrate the promise of the proposed method.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {655–658},
numpages = {4},
keywords = {multimedia content analysis, multimedia location context, cross media retrieval, multimedia information retrieval},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459453,
author = {Tan, Hung-Khoon and Ngo, Chong-Wah and Wu, Xiao},
title = {Modeling Video Hyperlinks with Hypergraph for Web Video Reranking},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459453},
doi = {10.1145/1459359.1459453},
abstract = {In this paper, we investigate a novel approach of exploiting visual-duplicates for web video reranking using hypergraph. Current graph-based reranking approaches consider mainly the pair-wise linking of keyframes and ignore reliability issues that are inherent in such representation. We exploit higher order relation to overcome the issues of missing links in visual-duplicate keyframes and in addition identify the latent relationships among keyframes. Based on hypergraph, we consider two groups of video threads: visual near-duplicate threads and story threads, to hyperlink web videos and describe the higher order information existing in video content. To facilitate reranking using random walk algorithm, the hypergraph is converted to a star-like graph using star expansion algorithm. Experiments on a dataset of 12,790 web videos show that hypergraph reranking can improve web video retrieval up to 45% over the initial ranked result by the video sharing websites and 8.3% over the pair-wise based graph reranking in mean average precision (MAP).},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {659–662},
numpages = {4},
keywords = {web video reranking, higher order relation, hypergraph},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459454,
author = {Ouyang, Yi and Tang, Ming and Wang, Jinqiao and Lu, Hanqing and Ma, Songde},
title = {Boosting Relative Spaces for Categorizing Objects with Large Intra-Class Variation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459454},
doi = {10.1145/1459359.1459454},
abstract = {In this paper, a novel method for object categorization is proposed. We first analyze the phenomenon of large intra-class variation and attribute it to the "subcategory" problem. To reveal the local and distinct properties of the different subcategories, relative spaces are constructed. Then the weighted FLDs (Fisher Linear Discriminant) as weak learners trained in relative spaces are integrated with the boosting framework to form the final classifier. Experiments on 8 categories from Caltech database show the effectiveness of our algorithm.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {663–666},
numpages = {4},
keywords = {geometric blur, relative space, object categorization, adaboost},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459455,
author = {Varewyck, Matthias and Pauwels, Johan and Martens, Jean-Pierre},
title = {A Novel Chroma Representation of Polyphonic Music Based on Multiple Pitch Tracking Techniques},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459455},
doi = {10.1145/1459359.1459455},
abstract = {It is common practice to map the frequency content of music onto a chroma representation, but there exist many different schemes for constructing such a representation. In this paper, a new scheme is proposed. It comprises a detection of salient frequencies, a conversion of salient frequencies to notes, a psychophysically motivated weighting of harmonics in support of a note, a restriction of harmonic relations between different notes and a restriction of the deviations from a predefined pitch scale (e.g. the equally tempered western scale). A large-scale experimental evaluation has confirmed that the novel chroma representation more closely matches manual chord labels than the representations generated by six other tested schemes. Therefore, the new chroma representation is expected to improve applications such as song similarity matching and chord detection and labeling.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {667–670},
numpages = {4},
keywords = {music information retrieval, chroma extractor},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459456,
author = {Li, Zhu and Fu, Yun and Huang, Thomas and Yan, Shuicheng},
title = {Real-Time Human Action Recognition by Luminance Field Trajectory Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459456},
doi = {10.1145/1459359.1459456},
abstract = {The explosive growth of video content in recent years fueled by the technological leaps in computing and communication has created new challenges for video content analysis that can serve applications in video surveillance, video searching and mining. Human action detection and recognition is one of the important tasks in this effort. In this paper, we present a luminance field manifold trajectory analysis based solution for human activity recognition, without explicit object level information extraction and understanding. This approach is computationally efficient and can operate in real time. The recognition performance is also comparable with the state of art in comparable set ups.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {671–676},
numpages = {6},
keywords = {video analysis, video understanding, human action recognition},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459457,
author = {Xu, Min and Jin, Jesse S. and Luo, Suhuai and Duan, Lingyu},
title = {Hierarchical Movie Affective Content Analysis Based on Arousal and Valence Features},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459457},
doi = {10.1145/1459359.1459457},
abstract = {Emotional factors directly reflect audiences' attention, evaluation and memory. Affective contents analysis not only create an index for users to access their interested movie segments, but also provide feasible entry for video highlights. Most of the work focus on emotion type detection. Besides emotion type, emotion intensity is also a significant clue for users to find their interested content. For some film genres (Horror, Action, etc), the segments with high emotion intensity have the most possibilities to be video highlights. In this paper, we propose a hierarchical structure for emotion categories and analyze emotion intensity and emotion type by using arousal and valence related features hierarchically. Firstly, High, Medium and Low are detected as emotion intensity levels by using fuzzy c-mean clustering on arousal features. Fuzzy clustering provides a mathematical model to represent vagueness, which is close to human perception. After that, valence related features are used to detect emotion types (Anger, Sad, Fear, Happy and Neutral). Considering video is continuous time series data and the occurrence of a certain emotion is affected by recent emotional history, Hidden Markov Models (HMMs) are used to capture the context information. Experimental results shows the movie segments with high emotion intensity cover over 80% of the movie highlights in Horror and Action movies and the hierarchical method outperforms the one-step method on emotion type detection. Meanwhile, it is flexible for user to pick up their favorite affective content by choosing both emotion intensity levels and emotion types.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {677–680},
numpages = {4},
keywords = {fuzzy clustering, affective content, movie, emotion intensity, emotion type, hidden markov model},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459458,
author = {Gallagher, Andrew C. and Neustaedter, Carman G. and Cao, Liangliang and Luo, Jiebo and Chen, Tsuhan},
title = {Image Annotation Using Personal Calendars as Context},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459458},
doi = {10.1145/1459359.1459458},
abstract = {In this paper, we introduce the idea of using the context of a personal calendar for labeling photo collections. Calendar event annotations are matched to images based on image capture time, and a Na\"{\i}ve Bayes model considers features from the calendar events as well as from computer vision-based image analysis to determine if the image actually matches the calendar event. This approach has the benefit that it requires no extra annotation from the consumer, since most people already keep calendars. In our test collections, 36% of personal images could be tagged with a label from a personal calendar. Note that our preliminary results represent a lower bound on the performance that is possible because all the system components are expected to improve over time. As people migrate toward digital calendars, we can also expect more consistency in their calendar labels, which should improve the annotation accuracy.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {681–684},
numpages = {4},
keywords = {calendar, content-based image retrieval, image annotation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256785,
author = {Jiang, Shuqiang},
title = {Session Details: Content Track Short Papers Session 2: Content Analysis and Applications},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256785},
doi = {10.1145/3256785},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459460,
author = {Tang, Nick C. and Shih, Timothy K. and Liao, Hong-Yuan Mark and Tsai, Joseph C. and Zhong, Hsing-Ying},
title = {Motion Extrapolation for Video Story Planning},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459460},
doi = {10.1145/1459359.1459460},
abstract = {We create video scenes using existing videos. A panorama is generated from background video, with foreground objects removed by video inpainting technique. A video planning script is provided by the user on the panorama with accurate timing of actors. Actions of these actors are extensions of existing cyclic motions, such as walking, tracked and extrapolated using our motion analysis techniques. Although the types of video story generated are limited, however, it is possible to use the mechanism proposed to generate forgery videos. Interested readers should visit our tool demonstration and forgery videos at http://member.mine.tku.edu.tw/www/ACMMM08-VideoPlanning.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {685–688},
numpages = {4},
keywords = {video inpainting, special effect, tracking, layer segmentation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459461,
author = {Thang, Truong Cong and Kang, Jung Won and Yoo, Jeong-Ju and Kim, Jae-Gon},
title = {Multilayer Adaptation for MGS-Based SVC Bitstream},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459461},
doi = {10.1145/1459359.1459461},
abstract = {Medium grained scalability (MGS) of scalable video coding is expected to be of high interest due to the advantage of packet-based scalability. In this paper, we study multilayer adaptation for MGS-based bitstream. Our adaptation method not only considers which packets to be dropped, but also modifies layer dependency of NAL units to maintain bitstream conformance. Compared to conventional method of SVC, the proposed method can both expand the range of supported bitrates and improve the quality.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {689–692},
numpages = {4},
keywords = {video adaptation, scalable video coding, mgs},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459462,
author = {Garg, Neha P. and Favre, Sarah and Salamin, Hugues and Hakkani T\"{u}r, Dilek and Vinciarelli, Alessandro},
title = {Role Recognition for Meeting Participants: An Approach Based on Lexical Information and Social Network Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459462},
doi = {10.1145/1459359.1459462},
abstract = {This paper presents experiments on the automatic recognition of roles in meetings. The proposed approach combines two sources of information: the lexical choices made by people playing different roles on one hand, and the Social Networks describing the interactions between the meeting participants on the other hand. Both sources lead to role recognition results significantly higher than chance when used separately, but the best results are obtained with their combination. Preliminary experiments obtained over a corpus of 138 meeting recordings (over 45 hours of material) show that around 70% of the time is labeled correctly in terms of role.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {693–696},
numpages = {4},
keywords = {meeting recordings, role recognition, lexical analysis, social network analysis},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459463,
author = {Ding, Yi and Fan, Guoliang},
title = {Multi-Channel Segmental Hidden Markov Models for Sports Video Mining},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459463},
doi = {10.1145/1459359.1459463},
abstract = {We present a new multi-channel segmental hidden Markov model (MCSHMM) for sports video mining that is a unique probabilistic graphical model with two advantages. One is the integration of both hierarchical and parallel structures that offer more flexibility and capacity of capturing the interaction between multiple Markov chains. The other is the incorporation of the segmental HMM that represents a variable-length sequence of observations. Especially, we develop a maximum a posteriori (MAP) estimator to optimize model structures and model parameters simultaneously. The proposed MCSHMM is used for American football video analysis, where two semantics structures, play types and camera views, are involved. The experiment shows that the MCSHMM outperforms previous HMM-based approaches.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {697–700},
numpages = {4},
keywords = {sports video mining, entropic prior, hidden markov models},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459464,
author = {Sheng, Tao and Hua, Guogang and Guo, Hongxing and Zhou, Jingli and Chen, Chang Wen},
title = {Rate Allocation for Transform Domain Wyner-Ziv Video Coding without Feedback},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459464},
doi = {10.1145/1459359.1459464},
abstract = {In this paper, we propose a new rate allocation algorithm for transform domain Wyner-Ziv video coding (WZVC) without feedback. In contrast to conventional video coding, Wyner-Ziv video coding aims to design simple intra-frame encoding and complex inter-frame decoding based on the Slepian-Wolf and Wyner-Ziv distributed source coding theorems. To allocate proper number of bits to each frame, most existing Wyner-Ziv video coding solutions need a feedback channel (FC) at the decoder. However, in many video coding applications, the FC is not allowed. Moreover, the FC will introduce latency and an increase of decoder complexity because several iterative decoding operations may be needed to decode the data to achieve target video quality. The proposed algorithm predicts the number of bits for each Wyner-Ziv frame at the encoder as a function of the coding mode and the quantization parameters. Such predictions will not significantly increase the complexity at the encoder. However, the prediction will be able to properly select the best mode and quantization parameter for encoding each Wyner-Ziv frame. Experimental results show that the proposed algorithms is able to achieve good encoder rate allocation while still maintains consistent coding efficiency. Comparing to the WZVC coder with FC, this new WZVC coder without FC induces only a small loss in Rate-Distortion performance.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {701–704},
numpages = {4},
keywords = {rate allocation, feedback channel, wyner-ziv video coding},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459465,
author = {Yang, Heng and Wang, Qing and He, Zhoucan},
title = {Randomized Sub-Vectors Hashing for High-Dimensional Image Feature Matching},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459465},
doi = {10.1145/1459359.1459465},
abstract = {High-dimensional image feature matching is an important part of many image matching based problems in computer vision which are solved by local invariant features. In this paper, we propose a new indexing/searching method based on Randomized Sub-Vectors Hashing (called RSVH) for high-dimensional image feature matching. The essential of the proposed idea is that the feature vectors are considered similar (measured by Euclidean distance) when the L2 norms of their corresponding randomized sub-vectors are approximately same respectively. Experimental results have demonstrated that our algorithm can perform much better than the famous BBF (Best-Bin-First) and LSH (Locality Sensitive Hashing) algorithms in extensive image matching and image retrieval applications.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {705–708},
numpages = {4},
keywords = {nearest neighbor searching, randomized sub-vectors hashing, high-dimensional feature matching},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459466,
author = {Akdemir, Umut and Turaga, Pavan and Chellappa, Rama},
title = {An Ontology Based Approach for Activity Recognition from Video},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459466},
doi = {10.1145/1459359.1459466},
abstract = {Representation and recognition of human activities is an important problem for video surveillance and security applications. Considering the wide variety of settings in which surveillance systems are being deployed, it is necessary to create a common knowledge-base or ontology of human activities. Most current attempts at ontology design in computer vision for human activities have been empirical in nature. In this paper, we present a more systematic approach to address the problem of designing ontologies for visual activity recognition. We draw on general ontology design principles and adapt them to the specific domain of human activity ontologies. Then, we discuss qualitative evaluation principles and provide several examples from existing ontologies and how they can be improved upon. Finally, we demonstrate quantitatively in terms of recognition performance, the efficacy and validity of our approach for bank and airport tarmac surveillance domains.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {709–712},
numpages = {4},
keywords = {activity ontologies, visual surveillance},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459467,
author = {Liu, Feng and Wang, Jinjun and Zhu, Shenghuo and Gleicher, Michael and Gong, Yihong},
title = {Noisy Video Super-Resolution},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459467},
doi = {10.1145/1459359.1459467},
abstract = {Low-quality videos often not only have limited resolution, but also suffer from noise. Directly up-sampling a video without considering noise could deteriorate its visual quality due to magnifying noise. This paper addresses this problem with a unified framework that achieves simultaneous de-noising and super-resolution. This framework formulates noisy video super-resolution as an optimization problem, aiming to maximize the visual quality of the result. We consider a good quality result to be fidelity-preserving, detail-preserving and smooth. Accordingly, we propose measures for these qualities in the scenario of de-noising and super-resolution. The experiments on a variety of noisy videos demonstrate the effectiveness of the presented algorithm.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {713–716},
numpages = {4},
keywords = {video super-resolution},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459468,
author = {Liu, Chunxi and Jiang, Shuqiang and Huang, Qingming},
title = {Naming Faces in Broadcast News Video by Image Google},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459468},
doi = {10.1145/1459359.1459468},
abstract = {Naming faces is important for news videos browsing and indexing. Although some research efforts have been contributed to it, they only use the concurrent information between the face and name or employ some clues as features and use simple heuristic method or machine learning approach to finish the task. They use little extra knowledge about the names and faces. Different from previous work, in this paper we present a novel approach to name the faces by exploring extra knowledge obtained from image google. The behind assumption is that the faces of those important persons will turn out many times in the web images and could be retrieved from image google easily. Firstly, faces are detected in the video frames; and the name entities of candidate persons are extracted from the textual information by automatic speech recognition and close caption detection. Then, these candidate person names are used as queries to find the name related person images through image google. After that, the retrieved result is analyzed and some typical faces are selected through feature density estimation. Finally, the detected faces in the news video are matched with the faces selected from the result returned by image google to label each face. Experimental results on MSNBC news and CNN news demonstrate that the proposed approach is effective.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {717–720},
numpages = {4},
keywords = {naming faces, news video browsing and indexing, news video analysis},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459469,
author = {Geng, Xin and Smith-Miles, Kate and Zhou, Zhi-Hua},
title = {Facial Age Estimation by Nonlinear Aging Pattern Subspace},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459469},
doi = {10.1145/1459359.1459469},
abstract = {Human age estimation by face images is an interesting yet challenging research topic emerging in recent years. This paper extends our previous work on facial age estimation (a linear method named AGES). In order to match the nonlinear nature of the human aging progress, a new algorithm named KAGES is proposed based on a nonlinear subspace trained on the aging patterns, which are defined as sequences of individual face images sorted in time order. Both the training and test (age estimation) processes of KAGES rely on a probabilistic model of KPCA. In the experimental results, the performance of KAGES is not only better than all the compared algorithms, but also better than the human observers in age estimation. The results are sensitive to parameter choice however, and future research challenges are identified.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {721–724},
numpages = {4},
keywords = {age estimation, face image},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459470,
author = {Cao, Liangliang and Dikmen, Mert and Fu, Yun and Huang, Thomas S.},
title = {Gender Recognition from Body},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459470},
doi = {10.1145/1459359.1459470},
abstract = {This paper studies the problem of recognizing gender from full body images. This problem has not been addressed before, partly because of the variant nature of human bodies and clothing that can bring tough difficulties. However, gender recognition has high application potentials, e.g. security surveillance and customer statistics collection in restaurants, supermarkets, and even building entrances. In this paper, we build a system of recognizing gender from full body images, taken from frontal or back views. Our contributions are three-fold. First, to handle the variety of human body characteristics, we represent each image by a collection of patch features, which model different body parts and provide a set of clues for gender recognition. To combine the clues, we build an ensemble learning algorithm from those body parts to recognize gender from fixed view body images (frontal or back). Second, we relax the fixed view constraint and show the possibility to train a flexible classifier for mixed view images with the almost same accuracy as the fixed view case. At last, our approach is shown to be robust to small alignment errors, which is preferred in many applications.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {725–728},
numpages = {4},
keywords = {gender recognition, human body},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459471,
author = {Cui, Jingyu and Wen, Fang and Tang, Xiaoou},
title = {Real Time Google and Live Image Search Re-Ranking},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459471},
doi = {10.1145/1459359.1459471},
abstract = {Nowadays, web-scale image search engines (e.g. Google, Live Image Search) rely almost purely on surrounding text features. This leads to ambiguous and noisy results. We propose to use adaptive visual similarity to re-rank the text-based search results. A query image is first categorized into one of several predefined intention categories, and a specific similarity measure is used inside each category to combine image features for re-ranking based on the query image. Extensive experiments demonstrate that using this algorithm to filter output of Google and Live Image Search is a practical and effective way to dramatically improve the user experience. A real-time image search engine is developed for on-line image search with re-ranking: http://mmlab.ie.cuhk.edu.hk/intentsearch},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {729–732},
numpages = {4},
keywords = {image search, adaptive similarity, intention, visual},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459472,
author = {Chen, Francine and Adcock, John and Krishnagiri, Shruti},
title = {Audio Privacy: Reducing Speech Intelligibility While Preserving Environmental Sounds},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459472},
doi = {10.1145/1459359.1459472},
abstract = {Audio monitoring has many applications but also raises privacy concerns. In an attempt to help alleviate these concerns, we have developed a method for reducing the intelligibility of speech while preserving intonation and the ability to recognize most environmental sounds. The method is based on identifying vocalic regions and replacing the vocal tract transfer function of these regions with the transfer function from prerecorded vowels, where the identity of the replacement vowel is independent of the identity of the spoken syllable. The audio signal is then re-synthesized using the original pitch and energy, but with the modified vocal tract transfer function. We performed an intelligibility study which showed that environmental sounds remained recognizable but speech intelligibility can be dramatically reduced to a 7% word recognition rate.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {733–736},
numpages = {4},
keywords = {audio privacy, surveillance, remote awareness},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459473,
author = {Chen, Hong-Ming and Chang, Ming-Hsiu and Chang, Ping-Chieh and Tien, Ming-Chun and Hsu, Winston H. and Wu, Ja-Ling},
title = {SheepDog: Group and Tag Recommendation for Flickr Photos by Automatic Search-Based Learning},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459473},
doi = {10.1145/1459359.1459473},
abstract = {Online photo albums have been prevalent in recent years and have resulted in more and more applications developed to provide convenient functionalities for photo sharing. In this paper, we propose a system named SheepDog to automatically add photos into appropriate groups and recommend suitable tags for users on Flickr. We adopt concept detection to predict relevant concepts of a photo and probe into the issue about training data collection for concept classification. From the perspective of gathering training data by web searching, we introduce two mechanisms and investigate their performances of concept detection. Based on some existing information from Flickr, a ranking-based method is applied not only to obtain reliable training data, but also to provide reasonable group/tag recommendations for input photos. We evaluate this system with a rich set of photos and the results demonstrate the effectiveness of our work.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {737–740},
numpages = {4},
keywords = {recommendation system, flickr, concept detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459474,
author = {Ding, Haoyang and Liu, Jing and Lu, Hanqing},
title = {Hierarchical Clustering-Based Navigation of Image Search Results},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459474},
doi = {10.1145/1459359.1459474},
abstract = {Usually, the image search results contain multiple topics on semantic level and even semantically consistent images have diverse appearances on visual level. How to organize the results into semantically and visually consistent clusters becomes a necessary task to facilitate users' navigation. To attack this, HiCluster, an effective method to organize image search results is designed in this paper, which employs both textual and visual analysis. First, we extract some query-related key phrases to enumerate specific semantics of the given query and cluster them into some semantic clusters using K-lines-based clustering algorithm. Second, the resulting images corresponding to each key phrase are clustered with Bregman Bubble Clustering (BBC) algorithm, which partially groups images in the whole set while discarding some scattered noisy ones. At last, a novel user interface (UI) is designed to provide users with the diverse and helpful information based on the hierarchical clustering structure. Experiments on web images demonstrate the effectiveness and potential of the system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {741–744},
numpages = {4},
keywords = {user interface, search result clustering, image search},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459476,
author = {Feng, Wu-chi and Dang, Thanh and Kassebaum, John and Bauman, Tim},
title = {Supporting Region-of-Interest Cropping through Constrained Compression},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459476},
doi = {10.1145/1459359.1459476},
abstract = {The ability to create very high-resolution video is becoming relative easy to do today, either through a single high definition video camera or panoramic video stitched from multiple cameras. This means that supporting region-of-interest cropping will become more important in the future. In this paper, we propose a mechanism to support region-of-interest adaptation of stored video. The proposed approach creates a compression compliant stream (e.g., MPEG-2), while still allowing it to be cropped. Using data from a motion-picture industry video camera, we show that the proposed approach, while adding a small amount of overhead, can be used to efficiently crop large resolution video into a smaller stream.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {745–748},
numpages = {4},
keywords = {video adaptation, region-of-interest, video coding, streaming},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459477,
author = {Hong, Guangming and Rahmati, Ahmad and Wang, Ye and Zhong, Lin},
title = {SenseCoding: Accelerometer-Assisted Motion Estimation for Efficient Video Encoding},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459477},
doi = {10.1145/1459359.1459477},
abstract = {Accelerometers have appeared on many camcorders, cameras and mobile phones. We present algorithms that estimate camera movement from accelerometer readings and apply the estimation to significantly improve the compute-intense motion estimation in video encoding. We have implemented a working prototype that simultaneously captures video and three-axis acceleration data. The video is then compressed with a reference MPEG-2 encoder, modified to incorporate the accelerometer readings to assist motion estimation. Our experimental data shows a two to three times speed improvement for the entire encoding process, in comparison with full search.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {749–752},
numpages = {4},
keywords = {mpeg-2, motion estimation, accelerometer, video encoding},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459478,
author = {Krasic, Charles and L\'{e}gar\'{e}, Jean-S\'{e}bastien},
title = {Interactivity and Scalability Enhancements for Quality-Adaptive Streaming},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459478},
doi = {10.1145/1459359.1459478},
abstract = {In this paper we describe the design and implementation of our adaptive media streaming system and its support for fully interactive video navigation. The system builds upon and extends previous work on adaptive streaming, to encompass coordinated adaptation of network, processor, and storage resources. The adaptation methods we describe allow our application to provide robust and responsive streaming, that supports a wider set of video navigation modes unseen before in any previous streaming application.In addition to the technical contributions toward streaming, and to shed light on the motivation for our approach, this paper also outlines a prototype application we are building above our adaptive-streaming framework for distributed collaborative video authoring. The goal of this application is to assist (distributed) teams of amateur cinematographers in authoring video projects, and, through teamwork, to elevate the quality of user generated content.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {753–756},
numpages = {4},
keywords = {interactivity, storage, video streaming, quality-adaptation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459479,
author = {Zhang, Hao and Yeo, Chuohao and Ramchandran, Kannan},
title = {VSYNC: A Novel Video File Synchronization Protocol},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459479},
doi = {10.1145/1459359.1459479},
abstract = {VSYNC is a novel incremental video file synchronization system that efficiently synchronizes two video files at remote ends through a bi-directional communications link. Retransmission of a video file that has been modified only slightly, for the purpose of synchronization with a remote-end copy, is extremely expensive but avoidable. VSYNC is a bi-directional algorithm designed to automatically detect and transmit changes in the modified video file without the knowledge of what was changed. Another feature of VSYNC is that it allows synchronization to within some user defined distortion constraint. A hierarchical hashing scheme is designed to compare video chunks, converting the high-level content information to a low-level hash stream that is more amenable to the tools of coding theory. Our approach shows impressive gains in transmission rate-savings. In a typical example of two 12 sec video files with about 10% of the frames being edited, transmission savings of 44% to 87% can be obtained compared to directly sending the updated video files using H.264 and rsync [1].},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {757–760},
numpages = {4},
keywords = {vsync, zsync, rsync, video file synchronization, video hash},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459480,
author = {Benevenuto, Fabricio and Duarte, Fernando and Rodrigues, Tiago and Almeida, Virgilio A.F. and Almeida, Jussara M. and Ross, Keith W.},
title = {Understanding Video Interactions in Youtube},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459480},
doi = {10.1145/1459359.1459480},
abstract = {This paper seeks understanding the user behavior in a social network created essentially by video interactions. We present a characterization of a social network created by the video interactions among users on YouTube, a popular social networking video sharing system. Our results uncover typical user behavioral patterns as well as show evidences of anti-social behavior such as self-promotion and other types of content pollution.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {761–764},
numpages = {4},
keywords = {promotion, video response, social networks},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459481,
author = {Shao, Mingkai and Dumitrescu, Sorina and Wu, Xiaolin},
title = {Toward the Optimal Multirate Multicast for Lossy Packet Network},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459481},
doi = {10.1145/1459359.1459481},
abstract = {Multirate multicast is a powerful methodology of multimedia communication in heterogenous networks. With recent advances of network coding theory, many multirate multicast techniques using network coding have been proposed and they hold promises of improving information multicast efficiency over the traditional application-layer multicast protocols. However, all these network coding based multirate multicast solutions adopted a layered coding technique and involves computationally expensive optimization to decide the amount of information transmitted in each multicast layer. In this paper, we propose a novel multirate multicast framework for lossy networks, which uses both the uneven erasure protection (UEP) technique and linear network codes. The new multicast framework not only simplifies the construction of the multicast codes, but also guarantees the strict fairness among clients of different bandwidths.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {765–768},
numpages = {4},
keywords = {uep, mdc, multirate multicast, network coding},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459482,
author = {Lee, Wei Siong and Tan, Yih Han and Tham, Jo Yew and Goh, Kwong Huang and Wu, Dajun},
title = {LACING: An Improved Motion Estimation Framework for Scalable Video Coding},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459482},
doi = {10.1145/1459359.1459482},
abstract = {Temporal scalability in H.264/SVC video compression standard can be achieved with the hierarchical B-pictures (HB) structure. When performing motion estimation (ME) in the HB structure, the temporal distance between the current frame and reference frame(s) can be large (up to 32 frames apart). This limits the performance of fast search algorithm as larger search window is often necessary. Extensive experiments showed that popular fast suboptimal block ME algorithms are ineffective at tracking large motions across several frames. In this paper, we propose a new framework called Lacing which integrates well with any fast block ME techniques to significantly improve the motion prediction accuracy in quality of the motion-compensated frame and also result in smoother motion vector fields with lower entropy.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {769–772},
numpages = {4},
keywords = {scalable video coding, motion estimation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459483,
author = {Valenzise, Giuseppe and Naccari, Matteo and Tagliasacchi, Marco and Tubaro, Stefano},
title = {Reduced-Reference Estimation of Channel-Induced Video Distortion Using Distributed Source Coding},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459483},
doi = {10.1145/1459359.1459483},
abstract = {Channel-induced distortion estimation is an important aspect in the delivery of video contents over IP networks: the QoS requirements of both content providers and content users conflict with the intrinsic best-effort nature of packet-switched networks, which may introduce annoying artifacts in the received streams due to channel errors or jitter. In this paper we propose a Reduced-Reference video quality assessment method, based on objective quality metrics, which enables distortion estimation at the macroblock level. The content provider transmits a small feature vector for each frame, starting from random projections computed for each macroblock. In order to reduce the bit rate of the transmitted feature vector, we encode it using Distributed Source Coding (DSC) tools. The content user decodes the feature vector using the received sequence as side information. Additionally, the end-user may take advantage of some prior information about the support of the errors in the frame in such a way that the required bit length of the transmitted feature vector is further reduced. In our experiments, using 4 random projections, the use of DSC enables a bit saving of 70% w.r.t. scalar quantization and transmission of the original feature vector; when also the a priori error map is available at the decoder, the average length of the transmitted partial reference can be further reduced by another 5% of average.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {773–776},
numpages = {4},
keywords = {distortion estimation, quality assessment, dsc},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459484,
author = {Deshpande, Sachin G.},
title = {High Quality Video Streaming Using Content-Awareadaptive Frame Scheduling with Explicit Deadlineadjustment},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459484},
doi = {10.1145/1459359.1459484},
abstract = {We propose an application layer algorithm for content-aware adaptive video frame scheduling on bandwidth varying channels. The proposed approach includes an iterative algorithm which sequentially uses the rate constraint and delay constraint for video frames to arrive at a decision to drop/ send video frames. In addition to our base algorithm, we also propose an explicit deadline adjustment (EDA) algorithm. The main idea behind EDA relates to the sender adjusting/ postponing the deadline of an important frame which is estimated to miss its deadline by actively dropping one (or more) less important next frame(s) and sending the deadline adjusted/ postponed frame to be displayed in place of the next dropped frame(s). The rate-distortion (R-D) performance of our proposed approach and its comparison with prior art method is shown.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {777–780},
numpages = {4},
keywords = {adaptive streaming, rd optimal streaming, frame deadline, frame scheduling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459485,
author = {Shi, Shu and Nahrstedt, Klara and Campbell, Roy H.},
title = {View-Dependent Real-Time 3d Video Compression for Mobile Devices},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459485},
doi = {10.1145/1459359.1459485},
abstract = {3D video is an emerging technology that promises immersive experiences in a truly seamless environment. Currently, 3D video systems still require excessive bandwidth and computation power provided by gigabit switches and multi-core workstations machines. In order to extend the experience to mobile devices, we present a view-dependent compression methodology that shows great promise in making 3D video a reality on resource-constrained mobile devices. Using our technology, we are able to achieve a software-only rendering on a Nokia N800 PDA with only wireless network transmission. We believe that with the use of newer handhelds and improvements to our compression techniques, we will be able to deliver full-motion 3D video soon.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {781–784},
numpages = {4},
keywords = {3d video, viewpoint, compression},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459486,
author = {Xiao, Xin and Shi, Yuanchun and Gao, Yuan},
title = {On Optimal Scheduling for Layered Video Streaming in Heterogeneous Peer-to-Peer Networks},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459486},
doi = {10.1145/1459359.1459486},
abstract = {Layered video streaming in peer-to-peer networks has drawn great interests since not only it can accommodate a large number of users but also it handles heterogeneities of client networks. However, to our knowledge, there's still a lack of systematical study on the data scheduling (i.e. requesting and relaying data) for layered streaming, and previous works in this area just focus on maximizing the throughput and/or minimizing the packet delay. In this paper, firstly, according to the characteristics caused by layered coding, we propose the four objectives that should be achieved by data scheduling for layered streaming: high throughput, high layer delivery ratio, low useless packets ratio and low subscription jitter; then, we use a 3-stage scheduling approach to request missed blocks, where each stage has different scheduling objective but collaborate with each other. The min-cost network flow model, probability decision mechanism and multi-window remedy mechanism are employed in Free Stage, Decision Stage and Remedy Stage, respectively, to achieve the above four goals. Extensive experimental results indicate that our approach outperforms other schemes in both throughput and layer delivery ratio. Besides, the useless packets number and subscription jitters are kept low.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {785–788},
numpages = {4},
keywords = {heterogeneity, layered streaming, peer-to-peer, qos, data scheduling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459488,
author = {Xu, Jie and Ye, Getian and Herman, Gunawan and Zhang, Bang},
title = {An Efficient Approach to Detecting Pedestrians in Video},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459488},
doi = {10.1145/1459359.1459488},
abstract = {In this paper, we propose an efficient approach to moving pedestrian detection in video. This approach incorporates both motion and shape information and learns a codebook of shape context descriptors from a very small number of training samples. During the testing process, moving edgelets are firstly identified between adjacent frames using a local search method. Shape context descriptors for numerous sample points on identified edgelets are then produced and are matched against the instances of the learned codebook to generate initial hypotheses. The final hypotheses for pedestrians are obtained by pruning initial hypotheses. The proposed approach has the following advantages by comparison with the existing techniques: (1) lower computational cost, (2) lower false positive rate, and (3) fewer training samples. Experiments with a publicly available dataset confirm the performance of the proposed approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {789–792},
numpages = {4},
keywords = {moving edgelet, codebook, pedestrian detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459489,
author = {Tajika, Taichi and Yonezawa, Tomoko and Mitsunaga, Noriaki},
title = {Intuitive Page-Turning Interface of e-Books on Flexible e-Paper Based on User Studies},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459489},
doi = {10.1145/1459359.1459489},
abstract = {In this paper we propose an intuitive page-turning and browsing interface of e-books on a flexible e-paper based on user studies. Our user studies showed various types of page-turning actions such as flipping, grasping, and sliding by different situations or users. We categorized these actions into three categories: turn, flip through, and leaf through the page(s). Based on this categorized model, we have developed a conceptual design and prototype of an interface for an e-book reader, which enables intuitive page-turning interactions using a simple architecture in both hardware and software design. The prototype has a flexible plastic sheet with bend sensors, which is attached to a small LCD monitor to physically unite the visual display with a tangible control interface based on the natural page-turning actions as used in reading a real book. The prototype handles all three page-turning actions observed in the user studies by interpreting the bend degree of the sheet.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {793–796},
numpages = {4},
keywords = {page-turning interface, e-paper, e-book reader, bend sensor},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459490,
author = {Yamasaki, Toshihiko and Nishioka, Yoshifumi and Aizawa, Kiyoharu},
title = {Interactive Retrieval for Multi-Camera Surveillance Systems Featuring Spatio-Temporal Summarization},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459490},
doi = {10.1145/1459359.1459490},
abstract = {An interactive interface is presented for near-synchronized and distributed multi-camera surveillance systems. Human tracking using multiple cameras is conducted employing a particle filter in conjunction with linear least-square trajectory extrapolation and color histogram matching. Spatial and temporal statistical information such as histograms of the number of pedestrians on a timeline-basis, pedestrians' trajectories over a certain period, pedestrian flow, and so forth is efficiently summarized and visualized. In addition, a sketch-based retrieval interface is also developed. As a result, our system facilitates operators to easily extract and access to important scenes from a huge amount of surveillance data. The real-life experiments in the public street demonstrated the validity of our system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {797–800},
numpages = {4},
keywords = {multi-camera, surveillance, sketch-based retrieval},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459491,
author = {Zhang, Liang and Shi, Yuanchun and Fan, Mingming},
title = {UCam: Direct Manipulation Using Handheld Camera for 3d Gesture Interaction},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459491},
doi = {10.1145/1459359.1459491},
abstract = {This paper presents UCam a novel approach in 3D Gesture Interaction based on handheld camera movement. UCam reflects hand's movement and directly maps it to the movement of 3D object based on visual tracking of feature-like points on incoming frames. Only one button is needed to differentiate rotation and translation. The advantages of this technique lie in the popularity and low cost of handheld cameras, low requirement and no need of adjustment of background and easy to use for beginners. To evaluate UCam, it is compared with mouse in some 3D controlling tasks. The results show that UCam is more flexible and easier to use and master in most cases. Even for complicated tasks, UCam has comparable performance as mouse.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {801–804},
numpages = {4},
keywords = {direct manipulating, interaction, camera movement},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459492,
author = {Xu, Lei and Liu, Yingfei and Wang, Kongqiao and Wang, Hao},
title = {Automatic Text Discovering through Stroke-Based Segmentation and Text String Combination},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459492},
doi = {10.1145/1459359.1459492},
abstract = {In this paper we present a novel framework of automatic text discovering for content-based multimedia application. For single image, the stroke-based binarization and the coarse-to-fine text extraction will collaborate to generate a clean text image for recognition. For image sequence, multi-frame text enhancement is adopted to increase the text/background contrast, and the recognition results are finally refined by the text string combination algorithm to get more precise semantic information. Two prototype demos have been successfully developed on mobile phones. The experimental results on different platforms show the superior performance of the proposed method.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {805–808},
numpages = {4},
keywords = {text string combination, text discovering, stroke-based segmentation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459493,
author = {Jayagopi, Dinesh Babu and Hung, Hayley and Yeo, Chuohao and Gatica-Perez, Daniel},
title = {Predicting the Dominant Clique in Meetings through Fusion of Nonverbal Cues},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459493},
doi = {10.1145/1459359.1459493},
abstract = {This paper addresses the problem of automatically predicting the dominant clique (i.e., the set of K-dominant people) in face-to-face small group meetings recorded by multiple audio and video sensors. For this goal, we present a framework that integrates automatically extracted nonverbal cues and dominance prediction models. Easily computable audio and visual activity cues are automatically extracted from cameras and microphones. Such nonverbal cues, correlated to human display and perception of dominance, are well documented in the social psychology literature. The effectiveness of the cues were systematically investigated as single cues as well as in unimodal and multimodal combinations using unsupervised and supervised learning approaches for dominant clique estimation. Our framework was evaluated on a five-hour public corpus of teamwork meetings with third-party manual annotation of perceived dominance. Our best approaches can exactly predict the dominant clique with 80.8% accuracy in four-person meetings in which multiple human annotators agree on their judgments of perceived dominance.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {809–812},
numpages = {4},
keywords = {meetings, dominant clique, dominance modeling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459494,
author = {Li, Haojie and Tang, Jinhui and Li, Guangda and Chua, Tat-Seng},
title = {Word2Image: Towards Visual Interpreting of Words},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459494},
doi = {10.1145/1459359.1459494},
abstract = {Besides the traditional textual semantic description to convey the meanings of a certain concept or word, visual illustration is a complementary, yet important and more intuitive way to interpret the word. Thus the technique that converts word to image is desirable though it is very difficult. Since a word usually has different semantic aspects, we need several correct and semantic-rich images to represent the word. In this paper, we explore how to leverage the web image collections to fulfill such task and develop a novel multimedia application system, Word2Image. Various techniques, including the correlation analysis, semantic and visual clustering are adapted into our system to produce sets of high quality, precise, diverse and representative images to visually translate a given word. The objective and subjective evaluations show the feasibility and effectiveness of our approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {813–816},
numpages = {4},
keywords = {multimedia dictionary, diversity, representativeness, image dictionary},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459495,
author = {Chiu, Patrick and Fujii, Koichi and Liu, Qiong},
title = {Content Based Automatic Zooming: Viewing Documents on Small Displays},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459495},
doi = {10.1145/1459359.1459495},
abstract = {We present an automatic zooming technique that leverages content analysis for viewing a document page on a small display such as a mobile phone or PDA. The page can come from a scanned document (bitmap image) or an electronic document (text and graphics data plus metadata). The page with text and graphics is segmented into regions. For each region, a scale-distortion function is constructed based on image analysis of the signal distortion that occurs at different scales. During interactive viewing of the document, as the user navigates by moving the viewport around the page, the zoom factor is automatically adjusted by optimizing the scale-distortion functions of the regions visible in the viewport.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {817–820},
numpages = {4},
keywords = {small displays, document viewing, document image analysis, automatic zooming},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459496,
author = {Atkins, C. Brian},
title = {Blocked Recursive Image Composition},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459496},
doi = {10.1145/1459359.1459496},
abstract = {Presentations that feature arrangements of photos (e.g., collages, photobooks and slideshows) are a popular means of sharing and communication. In the prevalent framework for creating such presentations, layouts are supplied to the user in the form of designed templates. However, a template library may not support a desired number of photos, combination of aspect ratios or range of areas. To alleviate this issue, we present BRIC, a method for arranging practically any number of photos on a rectangular canvas. Primary constraints ensure respect of photo aspect ratios and specified gutter thickness; while secondary constraints encourage layouts in which photo areas correspond to desired relative area values. Photos are arranged based on a recursive partition of the page, with photo areas computed as the solution to a linear system implied by the partition. We present a detailed description of BRIC along with examples that demonstrate its versatility.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {821–824},
numpages = {4},
keywords = {collage, photo layout, album, composition, automatic layout},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459497,
author = {He, Fang and Yu, Nenghai and Rui, Xiaoguang},
title = {Multi-Progressive Model for Web Image Annotation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459497},
doi = {10.1145/1459359.1459497},
abstract = {Web image annotation is a very important method to effectively index and search images on the internet. Many web image annotation approaches utilized not only the visual information but also the texts emerged with web image. However, they failed to utilize the whole relations among annotations, which reflect the specific semantic content of images. In this paper we propose a novel Multi-Progressive Model (MPM) for web image annotation that leverages word correlations between available texts of web images and an automatic-built vocabulary. The proposed approach treat the available text of web images as initial annotations, and extend them by using a pre-defined lexicon to include more words which are potentially relevant to the target image. It then rank initial and extended annotations by taking advantage of whole words relations without bringing huge computation. The multi-progressive model can be viewed as a greedy optimization algorithm that approximately optimizes the joint annotation probability in a progressive way. Experimental results on web images demonstrate the effectiveness of the proposed model.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {825–828},
numpages = {4},
keywords = {multi-progressive model, web image annotation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459498,
author = {Chu, Wei-Ta and Lin, Chia-Hung},
title = {Automatic Selection of Representative Photo and Smart Thumbnailing Using Near-Duplicate Detection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459498},
doi = {10.1145/1459359.1459498},
abstract = {This paper presents two applications about representative photo selection and smart thumbnailing using the results of near-duplicate detection. For a given photo cluster, near-duplicate photo pairs are first determined, and the relationships between them are modeled by a graph. The most typical one is then automatically selected by examining the mutual relation between them. For smart thumbnailing, we determine the region-of-interest of the selected representative photo based on locally matched feature points, which is a view different from conventional saliency-based approaches. The experiments show satisfactory performance in representative selection and promising results in ROI determination.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {829–832},
numpages = {4},
keywords = {image selection, near-duplicate detection, region-of-interest},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459499,
author = {Greenhill, Stewart and Venkatesh, Svetha},
title = {Contextual Navigation in a Multimedia Journal},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459499},
doi = {10.1145/1459359.1459499},
abstract = {This work presents a framework for multimedia journaling, maintaining strong relationships between the document and embedded media. This enables media archives that are robust to changes in software environments, such as changes in web-sharing services, proprietary file formats and enables portability across operating system. We develop a journaling application using an existing multimedia framework, and show the power of the paradigm with specific case studies.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {833–836},
numpages = {4},
keywords = {audio, blog, journal, cluster, photo, multimedia browser, query, video, filter, personal media management},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459500,
author = {Chang, Chia-Hu and Hsieh, Kuei-Yi and Chung, Ming-Che and Wu, Ja-Ling},
title = {ViSA: Virtual Spotlighted Advertising},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459500},
doi = {10.1145/1459359.1459500},
abstract = {With the constraint of limited intrusiveness, maximizing the advertising efficiency in sports video has been known as a challenging problem. In this paper, we propose a virtual advertisement system with novel advertising strategy, called Virtual Spotlighted Advertising (ViSA), for broadcasting tennis videos. We take psychology, advertising theory, and computational aesthetics into account for improving the effectiveness of advertising. ViSA system automatically detects the candidate insertion points in both temporal and spatial domains and estimates the maximum effective region for message communication. Then, the harmonically re-colored advertisements with foveation model based non-uniform transparency, are projected on the court in tennis videos. The experiments and evaluation results showed the effectiveness of ViSA, for sports video advertising, in terms of recall and recognition.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {837–840},
numpages = {4},
keywords = {less intrusiveness, tennis video analysis, virtual advertisement, image advertising},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459501,
author = {Yoon, Sang Min and Graf, Holger},
title = {Eye Tracking Based Interaction with 3d Reconstructed Objects},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459501},
doi = {10.1145/1459359.1459501},
abstract = {This paper addresses a methodology on how users might interact with objects which have been reconstructed from uncalibrated multiple images. Tracking the user's eye movement for Human-Computer Interaction in an augmented reality environment provides a convenient, natural, and highbandwidth source for navigating and zooming in/out of the 3D reconstructed object. The calculated 3D eye position, which is detected by multiple cues from a stereo camera, is synchronized with the 3D position of 3D reconstructed object. Our proposed method of image based 3D modelling is based on voxel carving with photo consistency check. Experimental results show that our proposed interaction methodology using a new solution of 3D eye tracking can be successfully applied for useful tools in context aware applications and improve its usability.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {841–844},
numpages = {4},
keywords = {hci, 3d reconstruction, eye tracking},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459502,
author = {Marques Neto, Manoel Carvalho and Santos, Celso A.S.},
title = {An Event-Based Model for Interactive Live TV Shows},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459502},
doi = {10.1145/1459359.1459502},
abstract = {Over the years some models for multimedia spatio-temporal synchronization has been proposed. In most of these models, a known drawback is that they generally abstract the media content and consider only start and end events content presentation. Thus, it is not possible to specify synchronization relations involving "within" media content events. Highlights in a live TV show are examples of these events. In such cases, it may be expected that highlights "may occur", but it is not possible to determine "when". In order to solve this drawback, this article presents an approach for modeling live Interactive TV shows that support the definition of synchronization constraints related to events not known at authoring time. These events are structured in objects, using an object oriented paradigm, and broadcasted to TV receivers as way to assure the synchronization relations},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {845–848},
numpages = {4},
keywords = {models, temporal synchronization, hypermedia, multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459503,
author = {Farrahi, Katayoun and Gatica-Perez, Daniel},
title = {What Did You Do Today? Discovering Daily Routines from Large-Scale Mobile Data},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459503},
doi = {10.1145/1459359.1459503},
abstract = {We present a framework built from two Hierarchical Bayesian topic models to discover human location-driven routines from mobile phones. The framework uses location-driven bag representations of people's daily activities obtained from celltower connections. Using 68,000+ hours of real-life human data from the Reality Mining dataset, we successfully discover various types of routines. The first studied model, Latent Dirichlet Allocation (LDA), automatically discovers characteristic routines for all individuals in the study, including "going to work at 10am", "leaving work at night", or "staying home for the entire evening". In contrast, the second methodology with the Author Topic model (ATM) finds routines characteristic of a selected groups of users, such as "being at home in the mornings and evenings while being out in the afternoon", and ranks users by their probability of conforming to certain daily routines.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {849–852},
numpages = {4},
keywords = {routine discovery, mobile phone, topic models, reality mining, human activity modeling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459504,
author = {Iqbal, Razib and Shirmohammadi, Shervin},
title = {Online Adaptation for Video Sharing Applications},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459504},
doi = {10.1145/1459359.1459504},
abstract = {The main concept of Peer-to-Peer (P2P) streaming is that viewers will contribute their bandwidth to the overlay and act as a relay for the video streams. In this paper, we introduce how a peer may implement an adaptive streaming scheme to serve peers in a P2P application. The technical contribution of this paper is to present the effectiveness and feasibility of utilizing the available computing power of the participating peers to serve mobile and heterogeneous clients by adapting the video content on the fly. The benefit is that there is no need for a dedicated adaptation or streaming server deployed in the system for video streaming/sharing applications. We emphasize on structured metadata-based adaptation and streaming utilizing MPEG-21 gBSD. Here, we briefly illustrate our scheme and present some experimental evaluations supporting our design choices.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {853–856},
numpages = {4},
keywords = {peer-to-peer streaming, video adaptation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459505,
author = {Negoescu, Radu Andrei and Gatica-Perez, Daniel},
title = {Topickr: Flickr Groups and Users Reloaded},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459505},
doi = {10.1145/1459359.1459505},
abstract = {With the increased presence of digital imaging devices there also came an explosion in the amount of multimedia content available online. Users have transformed from passive consumers of media into content creators. Flickr.com is such an example of an online community, with over 2 billion photos (and more recently, videos as well), most of which are publicly available. The user interaction with the system also provides a plethora of metadata associated with this content, and in particular tags. One very important aspect in Flickr is the ability of users to organize in self-managed communities called groups. Although users and groups are conceptually different, in practice they can be represented in the same way: a bag-of-tags, which is amenable for probabilistic topic modeling. We present a topic-based approach to represent Flickr users and groups and demonstrate it with a web application, Topickr, that allows similarity based exploration of Flickr entities using their topic-based representation, learned in an unsupervised manner.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {857–860},
numpages = {4},
keywords = {flickr, topic models},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459506,
author = {Tan, Hung-Khoon and Wu, Xiao and Ngo, Chong-Wah and Zhao, Wan-Lei},
title = {Accelerating Near-Duplicate Video Matching by Combining Visual Similarity and Alignment Distortion},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459506},
doi = {10.1145/1459359.1459506},
abstract = {In this paper, we investigate a novel approach to accelerate the matching of two video clips by exploiting the temporal coherence property inherent in the keyframe sequence of a video. Motivated by the fact that keyframe correspondences between near-duplicate videos typically follow certain spatial arrangements, such property could be employed to guide the alignment of two keyframe sequences. We set the alignment problem as an integer quadratic programming problem, where the cost function takes into account both the visual similarity of the corresponding keyframes as well as the alignment distortion among the set of correspondences. The set of keyframe-pairs found by our algorithm provides our proposal on the list of candidate keyframe-pairs for near-duplicate detection using local interest points. This eliminates the need for exhaustive keyframe-pair comparisons, which significantly accelerates the matching speed. Experiments on a dataset of 12,790 web videos demonstrate that the proposed method maintains a similar near-duplicate video retrieval performance as the hierarchical method proposed in [12] but with a significantly reduced number of keyframe-pair comparisons.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {861–864},
numpages = {4},
keywords = {near-duplicate web video retrieval, keyframe alignment},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459507,
author = {Diakopoulos, Nicholas and Luther, Kurt and Essa, Irfan},
title = {Audio Puzzler: Piecing Together Time-Stamped Speech Transcripts with a Puzzle Game},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459507},
doi = {10.1145/1459359.1459507},
abstract = {We have developed an audio-based casual puzzle game which produces a time-stamped transcription of spoken audio as a by-product of play. Our evaluation of the game indicates that it is both fun and challenging. The transcripts generated using the game are more accurate than those produced using a standard automatic transcription system and the time-stamps of words are within several hundred milliseconds of ground truth.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {865–868},
numpages = {4},
keywords = {speech transcription, puzzle, human computation, game},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459508,
author = {Carter, Scott and Denoue, Laurent},
title = {PicNTell: A Camcorder Metaphor for Screen Recording},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459508},
doi = {10.1145/1459359.1459508},
abstract = {PicNTell is a new technique for generating compelling screencasts where users can quickly record desktop activities and generate videos that are embeddable on popular video sharing distributions such as YouTube®. While standard video editing and screen capture tools are useful for some editing tasks, they have two main drawbacks: (1) they require users to import and organize media in a separate interface, and (2) they do not support natural (or camcorder-like) screen recording, and instead usually require the user to define a specific region or window to record. In this paper we review current screen recording use, and present the PicNTell system, pilot studies, and a new six degree-of-freedom tracker we are developing in response to our findings.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {869–872},
numpages = {4},
keywords = {screencasting, multimedia recording and playback, natural input},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459510,
author = {Teixeira, Luis F. and Martins, Luis G. and Lagrange, Mathieu and Tzanetakis, George},
title = {MarsyasX: Multimedia Dataflow Processing with Implicit Patching},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459510},
doi = {10.1145/1459359.1459510},
abstract = {The design and implementation of multimedia signal processing systems is challenging especially when efficiency and real-time performance is desired. In many modern applications, software systems must be able to handle multiple flows of various types of multimedia data such as audio and video. Researchers frequently have to rely on a combination of different software tools for each modality to assemble proof-of-concept systems that are inefficient, brittle and hard to maintain. Marsyas is a software framework originally developed to address these issues in the domain of audio processing. In this paper we describe MarsyasX, a new open-source cross-modal analysis framework that aims at a broader score of applications. It follows a dataflow architecture where complex networks of processing objects can be assembled to form systems that can handle multiple and different types of multimedia flows with expressiveness and efficiency.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {873–876},
numpages = {4},
keywords = {multimedia processing framework, dataflow processing, open-source library},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459511,
author = {Villa, Robert and Gildea, Nicholas and Jose, Joemon M.},
title = {Collaborative Awareness in Multimedia Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459511},
doi = {10.1145/1459359.1459511},
abstract = {Awareness of another's activity is an important aspect of facilitating collaboration between users, enabling an "understanding of the activities of others" [1]. Many environments where multimedia search is required are collaborative in nature, such as when a group of artists and animators are engaged in the production of a multimedia product. In this paper we introduce a study which used a novel evaluation methodology, where pairs of users competed to find the most relevant shots for a topic, with the aim of evaluating the role of awareness within an environment conductive to its use. Results based on event log and questionnaire data are reported, and conclusions presented illustrating some of the issues and pitfalls of using awareness in a video search system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {877–880},
numpages = {4},
keywords = {video retrieval, multimedia retrieval, collaborative search},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459512,
author = {Srinivasan, S H. and Sawant, Neela},
title = {Finding Near-Duplicate Images on the Web Using Fingerprints},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459512},
doi = {10.1145/1459359.1459512},
abstract = {The traditional near-duplicate detection systems developed for digital photo management and copyright protection are not applicable for the de-duplication of large-scale web image corpus. In this paper, we present a fast, accurate and highly scalable image fingerprinting technique suited for near-duplicate detection at the web-scale. The image fingerprint is a compact 130 bit representation computed using Fourier-Mellin transform. Near-duplicate images are detected in O(1) time using fingerprint equality and is faster than fast approximate near-neighbor searches like LSH.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {881–884},
numpages = {4},
keywords = {fourier-mellin transform, near-duplicate image detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459513,
author = {Gro\ss{}e, Philipp W.L. and Holzapfel, Hartwig and Waibel, Alex},
title = {Confidence Based Multimodal Fusion for Person Identification},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459513},
doi = {10.1145/1459359.1459513},
abstract = {Person identification is of great interest for various kinds of applications and interactive systems. In our system we use face recognition and voice recognition from data recorded in an interactive dialogue system. In such a system, sequential images and sequential utterances can be used to improve recognition accuracy over single hypotheses. The presented approach uses confidence-based fusion for sequence hypotheses, for multimodal fusion, and to provide a reliability measure of the classification quality that can be used to decide when to trust and when to ignore classification results.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {885–888},
numpages = {4},
keywords = {confidence, open set person identification, multimodal fusion},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459514,
author = {Girgensohn, Andreas and Shipman, Frank and Wilcox, Lynn},
title = {Determining Activity Patterns in Retail Spaces through Video Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459514},
doi = {10.1145/1459359.1459514},
abstract = {Retail establishments want to know about traffic flow and patterns of activity in order to better arrange and staff their business. A large number of fixed video cameras are commonly installed at these locations. While they can be used to observe activity in the retail environment, assigning personnel to this is too time consuming to be valuable for retail analysis. We have developed video processing and visualization techniques that generate presentations appropriate for examining traffic flow and changes in activity at different times of the day. Taking the results of video tracking software as input, our system aggregates activity in different regions of the area being analyzed, determines the average speed of moving objects in the region, and segments time based on significant changes in the quantity and/or location of activity. Visualizations present the results as heat maps to show activity and object counts and average velocities overlaid on the map of the space.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {889–892},
numpages = {4},
keywords = {retail analysis, multiple security cameras, activity segmentation, video surveillance},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459515,
author = {Kawamoto, Shin-ichi and Yotsukura, Tatsuo and Morishima, Shigeo and Nakamura, Satoshi},
title = {Post-Recording Tool for Instant Casting Movie System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459515},
doi = {10.1145/1459359.1459515},
abstract = {This paper proposes a universal user-friendly post-recording tool for an Instant Casting Movie System (ICS) that enables anyone to be a movie star using his or her own voice and faces. A personal CG character is automatically generated by scanning one's face geometry and image in ICS. Voice is as essential to identify a person as face. However, a character's voice is only based on gender in ICS. We proposed a novel voice recording tool for participants of all ages in a short time. Post-recording tasks are very difficult because speakers should speak in synchronization with the mouth movements of the CG characters. Therefore this task is generally recorded by professional voice actors. Our proposed tool has the following four features: 1) various supporting information for synchronization with voice and mouth movement timing for users; 2) automatic post-processing of recorded voices for compositing mixed audio; 3) intuitively displays operation for people of all ages; and 4) handles multiple users in parallel for quick recording. We developed a prototype speech synchronization system using a post-recording tool and conducted subjective evaluation experiments of it. Over 60% of the subjects responded that the tool's interface can be operated easily.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {893–896},
numpages = {4},
keywords = {voice post-recording, instant casting movie system},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459516,
author = {Omojokun, Olufisayo and Genovese, Michael and Isbell, Charles},
title = {Impact of User Context on Song Selection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459516},
doi = {10.1145/1459359.1459516},
abstract = {The rise of digital music has led to a parallel rise in the need to manage music collections of several thousands of songs on a single device. Manual selection of songs for a music listening experience is a cumbersome task. In this paper, we present an initial exploration of the feasibility of using song signal properties and user context information to assist in automatic song selection. Users listened to music over the course of a month while their context and song selections were tracked. Initial results suggest the use of context information can improve automated song selection when patterns are learned for each individual.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {897–900},
numpages = {4},
keywords = {modeling, prediction, context, music, song selection, sensors},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459517,
author = {Choudary, Omar and Charvillat, Vincent and Grigoras, Romulus},
title = {Mobile Guide Applications Using Representative Visualizations},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459517},
doi = {10.1145/1459359.1459517},
abstract = {Recent developments of hardware capabilities on mobile devices have made Location Based Services(LBS) very popular. This triggered an increasing need of rich visual content in mobile guide applications.This paper presents two original approaches for using representative visualizations: artistic views which represent an arbitrary deformation made by an artist and perspective views(3D-like) obtained from 3D models. Both approaches are based on learning GPS-to-image relations. We show an efficient use of the thin-plate spline for registering GPS coordinates with images. We also show the implementation of our guiding system on two mobile platforms.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {901–904},
numpages = {4},
keywords = {lbs, visual representation, mobile guide, artistic views},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459518,
author = {Srinivasan, S H. and Kukreja, Mayank},
title = {Tagboards for Video Tagging},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459518},
doi = {10.1145/1459359.1459518},
abstract = {There is an explosion of media content, especially user generated content, on the Internet. Efficient search of media data still remains a distant goal. Tagging has been used recently to bridge the "semantic gap" for images. Tagging of video data poses several challenges. The temporal nature of video requires support from the media player for video tagging. Unlike images which can be tagged simultaneously with viewing, tagging of video is typically not done at first viewing: videos are best tagged after viewing them at least once in entirety. This is not a preferred solution since the entire video has to be played again for tagging. In this paper, we propose a compact representation called "tagboard" which reduces the cognitive overload associated with video tagging. Tagboard also facilitates tag propagation. The technique is based on image similarity and dimensionality reduction. The ease of tagging improves metadata creation there by improving social search.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {905–908},
numpages = {4},
keywords = {video tagging},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459519,
author = {Gao, Jiang},
title = {Hybrid Tracking and Visual Search},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459519},
doi = {10.1145/1459359.1459519},
abstract = {Robust local features such as SIFT and SURF have been applied to many interesting image matching applications. These features are by nature very computational intensive even for modern desktop PCs. We have developed a framework for efficient feature extraction and matching for still images on a mobile device. In this paper we extend the still-image framework to video sequences. It is inefficient to perform feature extraction and matching for each frame in the video sequence. By tracking the content of the frames, feature extraction and image matching need only be performed when there is new content. We show promising experimental results using this approach.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {909–912},
numpages = {4},
keywords = {robust local feature, tracking, image matching},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459520,
author = {Wang, Yi-Tang and Tien, Ming-Chun and Wu, Ja-Ling and Yang, Chih-Wei and Ma, Matthew Huei-Ming},
title = {Video-Based CPR Analysis System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459520},
doi = {10.1145/1459359.1459520},
abstract = {In this paper, we propose a system for automatically detecting cardiopulmonary resuscitation (CPR) events and analyzing CPR qualities in surveillance videos. The system is further applied to the training of healthcare providers in the emergency room. Instructors could more efficiently evaluate the CPR quality performed by a medical team with the aid of our system. We extract motion vectors in all image blocks, and take motion information as a clue to classify video sequences into CPR and non-CPR segments based on Support Vector Machine (SVM). We further analyze several indicators of CPR quality and attach CPR information to the video. In order not to infringe the privacy of the patient, we apply a mosaic mechanism to mask the skin color regions of the patient. Our system has been applied to several simulated CPR video sequences and the results show acceptable accuracy in CPR detection and CPR information measurement.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {913–916},
numpages = {4},
keywords = {surveillance video analysis, cpr quality, cpr detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459521,
author = {Kaji, Hideki and Arikawa, Masatoshi},
title = {Personal Location Based Services on Place-Enhanced Blog},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459521},
doi = {10.1145/1459359.1459521},
abstract = {This paper proposes personal location based services on blog to make users recall past memories and things to do by displaying their personal records like diary, schedule and to-do list depending on the positions of the users. A lot of Internet users are recording their personal experiences and knowledge as texts and other digital media on the network. These users' created content is called "User Generated Content" (UGC). One of the reasons that users record their experiences and knowledge is not only informing other users about author's opinions but also retrieving them as needed. Although users can retrieve these records easily through their personal computer, it is difficult to retrieve them at the right place because most UGC systems do not have features to deal with some place attributes, on the other hand current commercial LBS do not support to deal with personal records. Our proposed tool provides users with an environment to store personal records with related place attributes, and to retrieve these personal records at the right place. There are two applications on this tool, a place enhanced blog and a place reminder on a mobile phone. A place enhanced blog provide users with blog interfaces for inputting place information. The place reminder is a browser for spatial data on the place enhanced blog. Users can generate place information by writing personal records on their blog. Furthermore, using the place reminder, other users can retrieve all users' permitted personal records on the spots. Also, the place reminder can serve as a simple navigation system using both GPS and non-GPS navigation methods},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {917–920},
numpages = {4},
keywords = {mobile device, personal information, location based service, blog},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459522,
author = {Edwards, George T. and Liu, Leslie S. and Moulic, Randy and Shea, Dennis G.},
title = {Proxima: A Mobile Augmented-Image Search System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459522},
doi = {10.1145/1459359.1459522},
abstract = {Image-based search has become an increasingly active area of research. Despite the fact that many resources have been spent on creating new ideas and improving existing algorithms and designs, existing image-based search engines do not provide scalability or accuracy that is even remotely close to today's text-based search engines. The challenges in image-based search are due to a number of factors, including the computation intensity of existing algorithms and the difficulty of detecting and recognizing objects. In this paper, we present a system called Proxima, which leverages contextual information provided by a mobile device, such as time, location, and user data, to search for people in a stored image database who are "similar" to the person in the input image. Unlike other systems that automatically associate metadata with images, Proxima utilizes the image itself as part of the database query. We also describe a prototype that implements the Proxima algorithm to provide an image-based search service for social networking.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {921–924},
numpages = {4},
keywords = {image search, mobile system design, service oriented architecture},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459523,
author = {Bobier, Bruce A. and Wirth, Michael},
title = {Content-Based Image Retrieval Using Hierarchical Temporal Memory},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459523},
doi = {10.1145/1459359.1459523},
abstract = {Several querying interfaces for content-based image retrieval (CBIR) are reviewed and a new CBIR system is introduced that uses Hierarchical Temporal Memory for the automatic indexing of architectural images and provides a sketch-based and iconic index querying interface. Experimentation shows the system is robust for recognizing query images under varying amounts of noise, distortion, occlusion, blurring, and affine transformation.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {925–928},
numpages = {4},
keywords = {querying interfaces, content-based image retrieval, hierarchical temporal memory},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459524,
author = {Sano, Masanori and Yagi, Nobuyuki and Katayama, Norio and Satoh, Shin'ichi},
title = {Image-Based Quiz Generation from News Video Archives Based on Principal Object},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459524},
doi = {10.1145/1459359.1459524},
abstract = {This paper describes our attempt to generate quizzes automatically from news programs, as novel method for content creation. Of the many types of possible quizzes we focus on the format of a multiple-choice quiz with accompanying images. To begin, we worked on formulating the engineering problem of quiz generation - selection of images suitable for a quiz, selection of text that explains the image, and the generation of alternative choices that are similar but different by selecting from subtitles. All the approaches were designed based on principal object in the image and text. As the suitable image for the quiz, an image which has a clear subject - we defined it as spotlight-image - is introduced. As for generation of the similar but different choices, categorizing the subject into person or other can make the correspondence between images and sentence much clear, and then appropriate choices are generated. By introducing these techniques, the accuracy has risen (35.7% up) and the effectiveness of our proposed method were confirmed.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {929–932},
numpages = {4},
keywords = {concrete noun, similar but non identical, spotlight image, news analysis, quiz generation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459525,
author = {Saini, Mukesh K. and Singh, Vivek K. and Jain, Ramesh C. and Kankanhalli, Mohan S.},
title = {Multimodal Observation Systems},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459525},
doi = {10.1145/1459359.1459525},
abstract = {In recent years, we have seen a significant research interest in a number of multimodal sensing applications like surveillance, video ethnography, tele-presence, assisted living, life blogging etc. However, these applications are currently evolving as separate silos with no interconnection. Further, the individual application-centric architectures typically tend to focus on specific sensors, specific (hardwired) queries and deal with specific environments. We present a generic sensing architecture 'Observation System', which allows multiple users to undertake different applications through abstracted interaction with a common set of sensors. The observation system observes behavior of various objects in an environment and keeps a record of important events and activities in an eventbase. In this system, multifarious data collected from disparate sensors and other sources are correlated to understand and gain insights in the environment. The observation system has applications in many areas including but not limited to surveillance, traffic monitoring, ethnography, marketing, and healthcare. In this paper, we present the architecture and functionality of such a system and present details of activity detection using multiple sensor streams in a distributed sensing environment. We also present results of such an approach and potential extensions to the analysis of more complex activities and events.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {933–936},
numpages = {4},
keywords = {events, query, system, application, surveillance},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459526,
author = {Kanneh, Andrea and Sakr, Ziad},
title = {Biometric User Verification Using Haptics and Fuzzy Logic},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459526},
doi = {10.1145/1459359.1459526},
abstract = {Security is always of a great concern for both organizations and individuals. The concept of biometric measures has been used to identify individuals as long as we have existed and has evolved with time. Behavioral biometrics can be further enhanced with the use of haptic devices. This paper presents a system that uses a haptic device to capture behavioral biometric features. The system then uses a fuzzy logic controller to verify user identification. This study shows that even with a simple interface, effective security could be achieved. Possible applications include online (signature) recognition or access control to highly restricted areas and/or data within an organization.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {937–940},
numpages = {4},
keywords = {reachin, security, biometrics, fuzzy logic, measurement, haptics, design},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459527,
author = {Keval, Hina and Sasse, M. Angela},
title = {To Catch a Thief -- You Need at Least 8 Frames per Second: The Impact of Frame Rates on User Performance in a CCTV Detection Task},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459527},
doi = {10.1145/1459359.1459527},
abstract = {The new generation of digital CCTV systems can be tailored to serve a wide range of security requirements. However, many digital CCTV systems produce video which is insufficient in video quality to support specific security tasks, such as crime detection. We report a study investigating the impact of lowering frame rates on an observer's ability to distinguish between crime and no crime events from post-event recorded video. 80 participants viewed 32 video scenes at 1, 5, 8, and 12 frames per second (fps). The task required observers to determine if one of three possible events had occurred. Results showed that the number of correct detections, task confidence decreased significantly at 8 fps and lower. Our results provide CCTV practitioners with a minimum frame rate level (8 fps) for event detection, a task performed by CCTV users of varying skill and experience.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {941–944},
numpages = {4},
keywords = {cctv video, crime detection, task performance, frame rate, video quality, multimedia, security},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459529,
author = {Gilroy, Stephen W. and Cavazza, Marc and Chaignon, R\'{e}mi and M\"{a}kel\"{a}, Satu-Marja and Niranen, Markus and Andr\'{e}, Elisabeth and Vogt, Thurid and Urbain, J\'{e}r\^{o}me and Billinghurst, Mark and Seichter, Hartmut and Benayoun, Maurice},
title = {E-Tree: Emotionally Driven Augmented Reality Art},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459529},
doi = {10.1145/1459359.1459529},
abstract = {In this paper, we describe an Augmented Reality Art installation, which reacts to user behaviour using Multimodal analysis of affective signals. The installation features a virtual tree, whose growth is influenced by the perceived emotional response from spectators. The system implements a 'magic mirror' paradigm (using a large-screen display or projection system) and is based on the ARToolkit with extended representations for scene graphs. The system relies on a PAD dimensional model of affect to support the fusion of different affective modalities, while also supporting the representation of affective responses that relate to aesthetic impressions. The influence of affective input on the visual component is achieved by mapping affective data to an L-System governing virtual tree behaviour. We have performed an early evaluation of the system, both from the technical perspective and in terms of user experience. Post-hoc questionnaires were generally consistent with data from multimodal affective processing, and users rated the overall experience as positive and enjoyable, regardless of how proactive they were in their interaction with the installation.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {945–948},
numpages = {4},
keywords = {augmented reality, affective computing, multimodal interaction, interactive digital arts},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459530,
author = {Bizzocchi, Jim},
title = {Winterscape and Ambient Video: An Intermedia Border Zone},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459530},
doi = {10.1145/1459359.1459530},
abstract = {"Ambient Video" is an emergent media form that operates in an intermedia border zone, sharing the aesthetics of cinema, video, painting, and fine art photography. Winterscape is an ambient video work that incorporates these directions. Like any ambient work, Winterscape must reward, but never require, viewer attention. It must also retain its visual interest over repeated viewings. In order to meet this aesthetic challenge, Winterscape relies on three artistic strategies: strong visual form and composition, manipulation of cinematic time, and deep manipulation of image and transition. At the same time it vests control over interaction with the viewer.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {949–952},
numpages = {4},
keywords = {cinema, poetics, post-production, video art, ambient video, video, moving image, visual effects, experimental film},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459531,
author = {Kirton, Travis and Ogawa, Hideaki and Sommerer, Christa and Mignonneau, Laurent},
title = {PINS: A Prototype Model towards Thedefinition of Surface Games},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459531},
doi = {10.1145/1459359.1459531},
abstract = {This paper presents an in-progress work that seeks to define a field within tangible gaming: "Surface Games." Surface Games is a classification that seeks to extend the traditional notion of board games by being played on interactive / tangible surfaces. Surface Games open up interesting new possibilities for exploring tangible interaction with respect to play. In particular, Surface Games create exciting opportunities for challenging traditional paradigms of board games such as: pre-defined boards, pre-defined playing pieces, turn-based action, and the responsibility of players for game management and rule-checking. The goal of Surface Games is to explore new interaction paradigms that bridge physical and digital gaming environments in the context of computational surfaces.This paper describes four key paradigms for computationally extending board games: surface as referee and fluid interaction, relative environments, physicality and game balance, and tangible playware and surfaces. We explore these paradigms through a prototype game - PINS - that has been designed specifically for a multitouch-tangible surface.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {953–956},
numpages = {4},
keywords = {surface games, tangible playware, fluid interaction, relative environments, multitouch-tangible},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459532,
author = {Scheible, J\"{u}rgen and Ojala, Timo and Coulton, Paul},
title = {MobiToss: A Novel Gesture Based Interface for Creating and Sharing Mobile Multimedia Art on Large Public Displays},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459532},
doi = {10.1145/1459359.1459532},
abstract = {This paper presents MobiToss, a novel application for creating and sharing mobile multimedia art with an off-the-shelf mobile phone equipped with built-in accelerometer sensors allowing gesture control. The user first takes a photo or captures a video with the phone and then using a 'throwing' gesture transfers the clip onto a large public display for instant viewing and manipulation by tilting the phone in different directions. The system augments the user-created clip with other items such as music or brand labels and the encoded clip is automatically sent back to the phone as a personal artefact of the event. The clip is also uploaded to a dedicated community website for sharing the created multimedia art with others. MobiToss could be deployed e.g. in clubs, pubs and concerts as a participatory VJ-tool. In addition to the design the paper presents the results of a preliminary user evaluation, which highlight the novel art experience provided by MobiToss.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {957–960},
numpages = {4},
keywords = {experimental evaluation, interactive content, public display, gesture control},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459533,
author = {Jacquemin, Christian},
title = {Allegra: A New Instrument for Bringing Interactive Graphics to Life},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459533},
doi = {10.1145/1459359.1459533},
abstract = {Physical models and particles are very attractive for digital artists because they offer new ways of exploring representations of the living and animated world. We propose an environment for the design, the parameterization and the control in real-time of mass-spring systems. Control is based on live image analysis, whether video capture or real-time 3D image synthesis. Efficient and fluid rendering is ensured by parallel computing on the graphic processor. Several setups are presented to illustrate the wide variety of designs that can be implemented through this platform.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {961–964},
numpages = {4},
keywords = {mass-spring systems, particles, gpgpu, physical models},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459534,
author = {Birchfield, David and Mechtley, Brandon and Hatton, Sarah and Thornburg, Harvey},
title = {Mixed-Reality Learning in the Art Museum Context},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459534},
doi = {10.1145/1459359.1459534},
abstract = {We describe the realization of two interactive, mixed-reality installations arising from a partnership of K-12, university, and museum participants. Our goal was to apply emerging technologies to produce an innovative, hands-on arts learning experience within a conventional art museum. Suspended Animation, a Reflection on Calder is a mixed-reality installation created in response to a sculpture by Alexander Calder. Another Rack for Peto was created in response to a painting by John Frederick Peto. Both installations express formal aspects of the original artworks, and allow visitors to explore specific conceptual themes through their interactions. The project culminated in a six-month exhibition where the original artworks were presented alongside these new installations. We present data that the installations were well received by an audience of 25,000 visitors.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {965–968},
numpages = {4},
keywords = {mixed-reality, museum, art, interactivity, learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459535,
author = {Codognet, Philippe P.},
title = {The Palimpsest System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459535},
doi = {10.1145/1459359.1459535},
abstract = {We propose a method for turning a slideshow of photographs into a media art installation consisting of a digital video flux of intermingled images, a continuous, ever-changing stream of dynamically created pictures composed of parts of basic images taken from a database of digital photographs. We called our system Palimpsest, referring to the tradition of medieval copies of books, when a manuscript is erased and re-written with new text. The basic idea is to combine one photograph with another on a pixel-by-pixel basis and to apply Cellular Automata rules to further mix in real-time the images together. The Cellular Automaton, will, from a series of randomly chosen seed pixels, slowly merge an image with another. It is worth noticing that more than two images can overlap at the same time. The key point is that this transformation process is part of the artwork itself, bringing some mesmerizing aspect to the flux of images.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {969–972},
numpages = {4},
keywords = {digital art},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459536,
author = {Majchrzak, Miroslaw},
title = {Analytic Capacities of an Original Tonality Analysis Method, Based on the Example of Chopin's Preludes Op.28},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459536},
doi = {10.1145/1459359.1459536},
abstract = {My presentation concerns the application of an original tonality analysis method for the purpose of study of the tonal structure of F. Chopin's Preludes op. 28. The present paper aims at initially showing the method's possibilities on the example of pieces written in all the 24 keys, as may be used for analysis of the tonal structure in musical pieces.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {973–976},
numpages = {4},
keywords = {algorithm, music analysis, tonality},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459537,
author = {Plewe, Daniela Alina},
title = {Transactional Arts: Interaction as Transaction},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459537},
doi = {10.1145/1459359.1459537},
abstract = {Interactive media, especially the internet, are often situated in an economic context where interactions are actually transactions. We focus on artists, who use or apply economic principles and coin those works as "transactional arts"i". We will observe that many accomplished new media works actually have transactional features. In the field of transactional arts, marketplaces can become an art form, buying and selling are means of self-expression, mesh-ups may resemble online businesses and most important, incentives become artistic material.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {977–980},
numpages = {4},
keywords = {transactional art, interactive art, economics, transactional media},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256786,
author = {Basu, Anup},
title = {Session Details: Demo Session 1},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256786},
doi = {10.1145/3256786},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459539,
author = {Frank, Jakob and Lidy, Thomas and Hlavac, Peter and Rauber, Andreas},
title = {Map-Based Music Interfaces for Mobile Devices},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459539},
doi = {10.1145/1459359.1459539},
abstract = {The pervasion of digital music calls for novel techniques to search, retrieve and access music collections. Particularly mobile devices are, due to their limited display size and input capabilities, in a need of possibilities for intuitive and quick selection of music that go beyond mere browsing through song lists and directories. We propose a graphical user interface for mobile devices presenting a music map that organizes a music collection automatically by sound similarity through audio analysis. This map provides an overview over large audio collections and offers several interaction possibilities to give users a quick and direct access to their music. It allows instant creation of playlists based on music of a desired genre by pointing on clusters or drawing paths on the map. The application not only eases access to music, but also enables novel application scenarios for collaborative music experience. The software has been implemented for a range of mobile devices, such as PDAs and smartphones.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {981–982},
numpages = {2},
keywords = {music retrieval, visualization, mobile devices, pocketsom, music maps, audio content analysis, self-organizing map},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459540,
author = {Erol, Berna and Graham, Jamey and Ant\'{u}nez, Emilio and Hull, Jonathan J.},
title = {HOTPAPER Demonstration: Multimedia Interaction with Paper Using Mobile Phones},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459540},
doi = {10.1145/1459359.1459540},
abstract = {HotPaper enables users to electronically interact with paper documents by pointing their camera phones at a document. Electronic interaction is in the form of reading and/or writing images, audio and video clips, urls, notes, etc to the paper. HotPaper does not require any machine readable code, such as a barcode or watermark, to be present on the document. It utilizes our Brick Wall Coding document recognition algorithm [1] that uses a small document patch image to retrieve electronic versions of documents without performing OCR. The HotPaper demonstration runs on a Treo 700w 312 MHz mobile phone and performs document recognition at 4 frames per second using 176x144 video input.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {983–984},
numpages = {2},
keywords = {mobile imaging, mobile interaction, linking paper to electronic data, markerless linking},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459541,
author = {Zhang, Shiliang and Huang, Qingming and Tian, Qi and Jiang, Shuqiang and Gao, Wen},
title = {I.MTV: An Integrated System for Mtv Affective Analysis},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459541},
doi = {10.1145/1459359.1459541},
abstract = {In modern time, MTV has become an important favorite pastime to people because of its conciseness, convenience and the ability to bring both audio and visual experiences to audiences. It has become an significant task to develop new techniques for natural, user-friendly, and effective MTV access. In this demo, an integrated system (i.MTV) is constructed for MTV Affective Analysis, Visualization, Retrieval, and User Profile Analysis. We not only perform the effective MTV affective analysis, but also propose novel Affective Visualization techniques to make the abstract affective states intuitive and friendly to users. Based on the affective analysis and visualization, MTV affective retrieval and management are achieved. Furthermore, novel methods are proposed for user affective preferences analysis and MTV recommendation.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {985–986},
numpages = {2},
keywords = {affective visualization, dimensional affective model, affective content analysis, affinity propagation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459542,
author = {Yu, Yi and Downie, J. Stephen and Moerchen, Fabian and Chen, Lei and Joe, Kazuki and Oria, Vincent},
title = {COSIN: Content-Based Retrieval System for Cover Songs},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459542},
doi = {10.1145/1459359.1459542},
abstract = {We develop a content-based audio COver Song IdeNtification (COSIN) system to detect/group cover songs. The COSIN takes music audio content as input and performs similarity searching to locate variants of the input (i.e., cover versions). Identified cover songs are returned in the rank order according to their similarity to the input. The COSIN also incorporates a set of tools to evaluate retrieval performance so researchers can explore different retrieval schemes and parameters (e.g. recall, precision). The COSIN utilizes a suite of techniques to detect cover songs including: Pitch + Dynamic Programming (DP), Chroma + DP, and Semantic Feature Summarization (SFS) + Hash-Based Approximate Matching (HBAM). Demonstration system shows that COSIN is a very potential music content retrieval tool. Running some music retrieval schemes on COSIN platform, recent experiments with SFS + LSH Variants demonstrate a nicely balanced efficiency (search speed) v. performance (search accuracy) tradeoff.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {987–988},
numpages = {2},
keywords = {content-based audio retrieval, musical audio sequences summarization, hash-based indexing, cover songs},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459543,
author = {Chu, Shih-Wei and Yeh, Mei-Chen and Cheng, Kwang-Ting},
title = {A Real-Time, Embedded Face-Annotation System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459543},
doi = {10.1145/1459359.1459543},
abstract = {Face detection and recognition have numerous multimedia applications of broad interest, one of which is automatic face annotation. There exist many robust algorithms tackling these problems but most of these algorithms are computationally demanding and have only been implemented in PC- or server-based environments. In this demonstration we show a real-time face-annotation system on a commercial PDA development platform. We examine the challenges faced in the design and development of a practical system that can achieve detection and recognition in real-time using limited memory and computational resources which are common constraints for embedded applications.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {989–990},
numpages = {2},
keywords = {embedded system, face annotation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459544,
author = {Ballan, Lamberto and Bertini, Marco and Jain, Arjun},
title = {A System for Automatic Detection and Recognition of Advertising Trademarks in Sports Videos},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459544},
doi = {10.1145/1459359.1459544},
abstract = {In this technical demonstration we show the current version of our trademark detection and recognition system that has been developed in collaboration with a sport marketing firm1 with the aim of evaluating the visibility of advertising trademarks in broadcast sporting events. We propose a semi-automatic system for detecting and retrieving trademark appearances in sports videos. A human annotator supervises the results of the automatic annotation through an interface that shows the time and the position of the detected trademarks; due to this fact the aim of the system is to provide a good recall figure, so that the supervisor can safely skip the parts of the video that have been marked as not containing a trademark, thus speeding up his work.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {991–992},
numpages = {2},
keywords = {video retrieval, sports videos, trademark matching},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459545,
author = {H\"{u}rst, Wolfgang and Meier, Konrad and G\"{o}tz, Georg},
title = {Timeline-Based Video Browsing on Handheld Devices},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459545},
doi = {10.1145/1459359.1459545},
abstract = {Small screen sizes and limited interaction possibilities appear to be the main challenges when developing video browsing interfaces for mobile devices, such as smartphones and PDAs. In this demo, we present several pen-based designs for interactive video browsing on a PDA. Each implementation enables users to skim a video along the timeline at different speeds and granularity levels. Using the whole touch-sensitive screen as input area, different pen movements are mapped to different browsing behaviors, offering more functionality and creating a better user experience.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {993–994},
numpages = {2},
keywords = {mobile video, interfaces, pda, video, video browsing},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459546,
author = {Hefeeda, Mohamed and Hsu, Cheng-Hsin and Liu, Yi},
title = {Testbed and Experiments for Mobile TV (DVB-H) Networks},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459546},
doi = {10.1145/1459359.1459546},
abstract = {We present a complete, running, testbed for mobile TV networks that employ the Digital Video Broadcast - Handheld (DVB-H) open standard. DVB-H based networks have been deployed in several countries around the world and currently being pilot-tested in many others. Nevertheless, there exists no open-source testbed in the literature to enable researchers to analyze and optimize the performance of such networks; most testbeds are proprietary. Our testbed implements the complete stack of the DVB-H standard and it streams real videos to actual handheld devices. It integrates several off-the-shelf hardware components and devices with software components. Some of the software components are developed by us and others are leveraged (after bug fixes and modifications) from open-source projects. In addition, we present several experiments to: (i) evaluate and compare multiple energy-saving techniques recently proposed for mobile TV networks, and (ii) demonstrate a new method to reduce the channel switching delay.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {995–996},
numpages = {2},
keywords = {performance evaluation, broadcast networks},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459547,
author = {Cui, Jingyu and Wen, Fang and Tang, Xiaoou},
title = {IntentSearch: Interactive on-Line Image Search Re-Ranking},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459547},
doi = {10.1145/1459359.1459547},
abstract = {In this demo, we present IntentSearch, an interactive system for realtime web based image retrieval. IntentSearch works directly on top of Microsoft Live Image Search, and re-ranks its results according to user specified query image(s) and the automatically inferred user intention. Besides searching in the interface of Microsoft Live Image Search, we also design a more flexible interface to let users browse and play with all the images in the current search session, which makes web image search more efficient and interesting. Please visit http://mmlab.ie.cuhk.edu.hk/intentsearch for the experience.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {997–998},
numpages = {2},
keywords = {image search, visual, intention},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459548,
author = {Kitamura, Keigo and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
title = {Food Log by Analyzing Food Images},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459548},
doi = {10.1145/1459359.1459548},
abstract = {In this paper, a food-logging system that can distinguish food images from other images, analyze the food balance, and visualize the log is presented. The image processing is based on feature vectors consisting of color histograms, DCT coefficients, detected image patterns and so forth. Support Vector Machine (SVM) was used to detect food images and to analyze the food balance. Experimental results show that the food image extraction presents above 88% of accuracy and the food balance estimation is achieved with more than 73% of accuracy.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {999–1000},
numpages = {2},
keywords = {life-log, multimedia interfaces, food},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459549,
author = {Luo, Hangzai and Fan, Jianping and Keim, Daniel A.},
title = {Personalized News Video Recommendation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459549},
doi = {10.1145/1459359.1459549},
abstract = {In this paper, we have developed an interactive system to enable personalized news video recommendation. First, multi-modal information channels (audio, video and closed captions) are seamlessly integrated and synchronized to achieve more reliable news topic detection, and the contextual relationships between the news topics are extracted automatically. Second, topic network and hyperbolic visualization are seamlessly integrated to achieve interactive navigation and exploration of large-scale collections of news videos at the topic level, so that users can have a good global overview of large-scale collections of news videos at the first glance. In such interactive topic network navigation and exploration process, the users' personal background knowledge can be taken into consideration for obtaining the news topics of interest interactively, building up their mental models of news needs precisely and formulating their searches easily by selecting the visible news topics on the screen directly. Our system can further recommend the relevant web news, the new search directions, and the most relevant news videos according to their importance and representativeness scores.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1001–1002},
numpages = {2},
keywords = {exploratory search, video recommendation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459550,
author = {Yang, Yi Hsuan and Lin, Yu Ching and Cheng, Heng Tze and Chen, Homer H.},
title = {Mr. Emo: Music Retrieval in the Emotion Plane},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459550},
doi = {10.1145/1459359.1459550},
abstract = {This technical demo presents a novel emotion-based music retrieval platform, called Mr. Emo, for organizing and browsing music collections. Unlike conventional approaches which quantize emotions into classes, Mr. Emo defines emotions by two continuous variables arousal and valence and employs regression algorithms to predict them. Associated with arousal and valence values (AV values), each music sample becomes a point in the arousal-valence emotion plane, so a user can easily retrieve music samples of certain emotion(s) by specifying a point or a trajectory in the emotion plane. Being content centric and functionally powerful, such emotion-based retrieval complements traditional keyword- or artist-based retrieval. The demo shows the effectiveness and novelty of music retrieval in the emotion plane.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1003–1004},
numpages = {2},
keywords = {music information retrieval, emotion plane, emotion recognition},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459552,
author = {Lu, Huanhuan and Zhang, Bingjun and Wang, Ye and Leow, Wee Kheng},
title = {IDVT: An Interactive Digital Violin Tutoring System Based on Audio-Visual Fusion},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459552},
doi = {10.1145/1459359.1459552},
abstract = {iDVT (interactive Digital Violin Tutor) is a violin learning system exploiting physical and virtual resources and interactivity. It aims at providing the user with new effective learning experience. This demonstration paper briefly describes the structure of the system and the underlying audio-visual processing techniques employed in the system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1005–1006},
numpages = {2},
keywords = {fingering analysis, multimodal fusion, onset detection, hand tracking, music transcription},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459553,
author = {Ishikawa, Akio and Panahpour Tehrani, Mehrdad and Naito, Sei and Sakazawa, Shigeyuki and Koike, Atsushi},
title = {Free Viewpoint Video Generation for Walk-through Experience Using Image-Based Rendering},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459553},
doi = {10.1145/1459359.1459553},
abstract = {This paper presents a novel method to represent a real 3D world using IBR (Image-based Rendering) technology. The major achievement is realization of "walk-through" experience, in which audiences can see the scene of sport games as if they were the player, while the conventional IBR method cannot provide such an experience. The key idea is a newly introduced framework "locally divided ray space" to capture the ray inside the scene. Consequently, the proposed algorithm can generate a virtual image inside the scene using multiple real images captured from outside. In this paper, the outline of the proposed method as well as the possible demonstration at the conference venue is described.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1007–1008},
numpages = {2},
keywords = {free viewpoint video, ray space, local region, image-based rendering, walk-through},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459554,
author = {Wu, Tien-Lin and Wang, Hsuan-Kai and Ho, Chien-Chang and Lin, Yuan-Pin and Hu, Ting-Ting and Weng, Ming-Fang and Chan, Li-Wei and Yang, Chang-Hua and Yang, Yi-Hsuan and Hung, Yi-Ping and Chuang, Yung-Yu and Chen, Hsin-Hsi and Chen, Homer H. and Chen, Jyh-Horng and Jeng, Shyh-Kang},
title = {Interactive Content Presentation Based on Expressed Emotion and Physiological Feedback},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459554},
doi = {10.1145/1459359.1459554},
abstract = {In this technical demonstration, we showcase an interactive content presentation (ICP) system that integrates media-expressed-emotion-based composition, user-perceived preference feedback, and interactive digital art creation. ICP harmonizes the browsing of multimedia contents by presenting them in the form of music videos (photos, blog articles with accompanied music) based on their expressed emotion similarity. ICP facilitates content browsing by automatically and dynamically selecting the media to be played next in real time, responding to user's preference feedback measured from physiological signals. In addition, ICP enhances the enjoyments of content browsing by incorporating interactive digital art creation. ICP achieves these goals by properly integrating recent researches on media-expressed emotion classification,cross-media composition, and physiological signal processing.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1009–1010},
numpages = {2},
keywords = {media-expressed emotion, user-perceived preference feedback, cross-media composition, interactive digital art creation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459555,
author = {Kurtisi, Zefir and Gu, Xiaoyuan and Wolf, Lars},
title = {Video Demo: NMP with SDSL Access Network},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459555},
doi = {10.1145/1459359.1459555},
abstract = {Network-centric Music Performance (NMP) enables Internet-based multiparty music performance through cyberspace. Our goal is to support real-time distributed natural audio delivery over the network, using audio compression schemes with exceptional audio quality. Among many challenges, a tight delay bound between the production and perception of the audio is a dominant requirement. The demo video shows a recorded session of a live network-centric music performance demonstration involving a SDSL access network. It demonstrates that musicians can play together smoothly on a virtual stage provided by NMP. Beyond our previous work of realizing NMP in local and wide area research networks, this is the first time NMP has been enabled with a commercially available synchronous DSL access network. The positive ratings by the musicians confirm that NMP is ready for large-scale commercial deployment once such access technology is widely available and affordable to the end users.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1011–1012},
numpages = {2},
keywords = {audio delivery, digital audio, music, network entertainment},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459556,
author = {Ihaddadene, Nacim and Sharif, Md. Haidar and Djeraba, Chabane},
title = {Crowd Behaviour Monitoring},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459556},
doi = {10.1145/1459359.1459556},
abstract = {We present a tool that automatically detects abnormal situations in crowded scenes in real time. The followed approach analyzes the general motion aspect, instead of tracking subjects one by one, by detecting abnormal optical flow patterns of tracked KLT points. The number of tracked points is reduced by using a learned mask. We define a measure that describes the situation abnormality based on crowd density, direction variance and distribution, mean velocity and sometimes trajectory matching. To demonstrate the interest of this approach, we present the results on the detection of collapsing events in real videos of airport escalator exits.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1013–1014},
numpages = {2},
keywords = {crowd behavior, video surveillance, event detection},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459557,
author = {Liu, Qiong and Chiu, Patrick and Wilcox, Lynn},
title = {Document Finder},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459557},
doi = {10.1145/1459359.1459557},
abstract = {This demo introduces a tool for accessing an e-document by capturing one or more images of a real object or document hardcopy. This tool is useful when a file name or location of the file is unknown or unclear. It can save field workers and office workers from remembering/exploring numerous directories and file names. Frequently, it can convert tedious keyboard typing in a search box to a simple camera click. Additionally, when a remote collaborator cannot clearly see an object or a document hardcopy through remote collaboration cameras, this tool can be used to automatically retrieve and send the original e-document to a remote screen or printer.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1015–1016},
numpages = {2},
keywords = {image local features, document retrieval, camera enabled user interface},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459558,
author = {Friedland, Gerald and Vinyals, Oriol},
title = {Live Speaker Identification in Conversations},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459558},
doi = {10.1145/1459359.1459558},
abstract = {The following article describes our technical demonstration of an online speaker identification system for conversations. A laptop with an internal microphone is centrally placed in the table of a meeting room. The system is able to identify the current speaker independent of spoken text or language with a latency of about 1.5 seconds and an accuracy of about 85% (as evaluated against the NIST RT benchmark). A Java GUI shows the image of the current speaker along with a timeline containing past speakers. Speakers are added to the system's database using a one-minute training procedure.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1017–1018},
numpages = {2},
keywords = {online, diarization, speaker identification, conversations},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459559,
author = {Tekli, Joe and Chbeir, Richard and Yetongnon, Kokou},
title = {XS3: A System for Similarity Evaluation in Multimedia-Based Heterogeneous XML Repositories},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459559},
doi = {10.1145/1459359.1459559},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1019–1020},
numpages = {2},
keywords = {multimedia data and metadata, structural similarity, semi-structured data, semantic similarity, xml, tree edit distance},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459560,
author = {Wen, Fang and Chen, Shifeng and Tang, Xiaoou},
title = {EasyToon: Cartoon Personalization Using Face Photos},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459560},
doi = {10.1145/1459359.1459560},
abstract = {In this demo, we present a family photo album based cartoon personalization system, EasyToon. Using the family photo album as the candidate pool, a personalized cartoon image is obtained in two main steps. First, the best face candidate is selected from the album interactively. Then a personalized cartoon image is automatically synthesized by lending the selected face into the target cartoon image. By integrating state of the art computer vision and graphics technologies and effective UI design EasyToon can generate a personalized cartoon storyboard easily and quickly.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1021–1022},
numpages = {2},
keywords = {cartoon personalization, family photo album},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459561,
author = {Tsai, Sam S. and Chen, David and Singh, Jatinder Pal and Girod, Bernd},
title = {Rate-Efficient, Real-Time Cd Cover Recognition on a Camera-Phone},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459561},
doi = {10.1145/1459359.1459561},
abstract = {Automatic CD cover recognition has interesting applications for comparison shopping and music sampling. We demonstrate a real-time CD cover recognition using a cameraphone. By snapping a picture of a CD cover with her cameraphone, a user can conveniently retrieve information related to the CD. Robust image feature extraction is applied to overcome the image distortions in the query photo. To limit the amount of data transmitted over a wireless network, we compress the query image or features extracted from the query image. On the database side, fast and reliable image matching against a database of 10,000 CD covers is accomplished using a scalable vocabulary tree.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1023–1024},
numpages = {2},
keywords = {scale-invariant feature, scalable vocabulary tree, content-based image retrieval, mobile augmented reality},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459562,
author = {Wu, Qiong and Boulanger, Pierre and Bischof, Walter F.},
title = {Bi-Layer Video Segmentation with Foreground and Background Infrared Illumination},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459562},
doi = {10.1145/1459359.1459562},
abstract = {In this paper, we investigate two ways of employing infrared video with color video for automatic foreground-background video segmentation: foreground infrared (IR) illumination and background IR illumination. Foreground IR illumination gives an initial foreground template, which is combined with image segmentation to complete foreground segmentation. Two algorithms are explored, Graph Cut and Relaxation Labeling. The disadvantage of foreground IR illumination can be compensated by background illumination.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1025–1026},
numpages = {2},
keywords = {video segmentation, graph cut, relaxation labeling},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459563,
author = {Li, Lusong and Mei, Tao and Hua, Xian-Sheng and Li, Shipeng},
title = {ImageSense},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459563},
doi = {10.1145/1459359.1459563},
abstract = {This demonstration presents an innovative contextual advertising platform for online image service, called ImageSense. Unlike most current ad-networks which treat image advertising as general text advertising by displaying relevant ads based on the contents of the Web page, ImageSense aims to embed more contextually relevant ads at less intrusive positions within each suitable image. Given a Web page containing images, ImageSense is able to decompose the page into a set of semantic blocks, select the suitable images from these blocks for advertising, rank the ads according to the relevance derived from surrounding text and visual similarity, and insert the relevant ads into the nonintrusive areas within the selected images. ImageSense represents one of the first attempts towards contextual image advertising which enables both the publishers and advertisers deliver more effective ads carried through image contents.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1027–1028},
numpages = {2},
keywords = {web page segmentation, image advertising, image saliency},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256787,
author = {Sundaram, Hari and Aizawa, Kiyoharu},
title = {Session Details: Doctoral Symposium},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256787},
doi = {10.1145/3256787},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459565,
author = {Lablack, Adel},
title = {Head Pose Estimation for Visual Field Projection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459565},
doi = {10.1145/1459359.1459565},
abstract = {The aim of our work is to extract the location of interest inside a target scene. This is done by analyzing videos captured by cameras monitoring an area under surveillance where people are passing in front of a target scene. We apply a head pose estimation module to the people detected in the scene, which tells us if they are looking or not at the target scene. The projection of the visual field on the target scene which extracts the location of interest, requires a person's head pose and his/her distance from the target scene. This process allows an accurate explanation of the human activity and interest in front of an advertising poster, a shop window or a shelf for example.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1029–1030},
numpages = {2},
keywords = {target scene, visual field, video analysis, head pose},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459566,
author = {Kortbek, Karen Johanne},
title = {Interaction Design for Public Spaces},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459566},
doi = {10.1145/1459359.1459566},
abstract = {In this abstract I describe the doctorial research project "Interaction Design for Public Spaces". The objective of the project is to explore and design interaction contexts in culture related public spaces such as museums, experience centres and festivals. As a perspective on this domain, I will focus on the usage of the body as an interaction device. Furthermore, the project will involve a dramaturgic take on communication and design of interactive systems in the pursuit of new ways to stage the interactive contexts.The outcome of the project will be guidelines and conceptual frameworks which will help interaction designers when designing for bodily movement, and communicating and staging interactive content in public spaces.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1031–1034},
numpages = {4},
keywords = {drama, environments, communication, interaction design, social interaction, culture related public spaces, staging, body as interaction device, user experiences},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459567,
author = {Moebs, Sabine A.},
title = {A Learner, is a Learner, is a User, is a Customer: QoS-Based Experience-Aware Adaptation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459567},
doi = {10.1145/1459359.1459567},
abstract = {This paper describes an outline of the doctoral thesis work concerning adaptation policies towards quality of experience (QoE) in performance-aware adaptive multimedia e-learning systems. QoE is considered to be mainly affected by the psychological concept of flow and learning-related factors. In turn, for multimedia systems, these factors can be heavily influenced by quality of service (QoS). In an ideal world, QoS would not be an issue and content optimally tailored to a user's needs could always be perfectly delivered. Unfortunately, delivery conditions are not always ideal, and it may be infeasible to deliver certain multimedia content such as high quality video while maintaining an acceptable QoS. The goal of this research is to balance the constraints imposed by QoS restrictions with the requirements of flow and learning in order to produce the highest possible QoE for the learner using an adaptive multimedia system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1035–1038},
numpages = {4},
keywords = {flow, qoe, qos, learning},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459568,
author = {Vaishnavi, Ishan},
title = {A Scheduling Algorithm for Time Bounded Delivery of Packets on the Internet},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459568},
doi = {10.1145/1459359.1459568},
abstract = {This thesis aims to provide a better scheduling algorithm for Real-Time delivery of packets. A number of emerging applications such as VoIP, Tele-immersive environments, distributed media viewing and distributed gaming require real-time delivery of packets. Currently the scheduling algorithms used to decide the priority ordering over packets only consider the deadline as the parameter. This may not be optimal, since it ignores the network traffic over the routes that these packets may take. The idea of this thesis is to propose a method of calculating a probability measure of each packet meeting it's deadline at every intermediate node. This probability measure is based on the time left for deadline to expire, the number of nodes further in the packets route to reach it's destination and the traffic on this route. The algorithm assumes a certain level on time synchronisation over the network.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1039–1042},
numpages = {4},
keywords = {distributed systems, real time networks},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459569,
author = {Hsu, Cheng-Hsin and Hefeeda, Mohamed},
title = {Video Communication Systems with Heterogeneous Clients},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459569},
doi = {10.1145/1459359.1459569},
abstract = {Modern wireless mobile devices have evolved to small computers that can render multimedia content, while desktop/laptop computers have become more computationally powerful with faster Internet access. As these computing devices getting more popular, users demand for more and higher quality videos in many communication applications, where clients are heterogeneous in terms of network bandwidth and computing power. The goal of this thesis is to improve client perceived-quality of various video communication systems by adopting scalable video coding tools that enable efficient rate adaptation. We seek to understand scalable coding standards and design optimization and streaming algorithms to make the best possible use of them in practical systems. We consider practical problems of video communication systems in three different environments: Internet streaming systems, TV broadcast networks, and mobile video communication systems. We propose efficient algorithms to solve the considered problems. We evaluate the proposed algorithms using numerical methods and/or simulations. Most importantly, we design and implement testbeds to validate our algorithms. The expected results of applying our algorithms to video communication systems are better video quality and higher user satisfaction as well as better bandwidth utilization and lower processing overhead.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1043–1046},
numpages = {4},
keywords = {quality optimization, scalable video coding, video streaming},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459570,
author = {Cheng, Wei},
title = {Streaming of 3D Progressive Meshes},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459570},
doi = {10.1145/1459359.1459570},
abstract = {Streaming of progressive meshes enables users to view 3D meshes with increasing level of details, by sending a coarse version of a mesh initially, followed by a sequence of refinements to incrementally improve the quality. Our research concentrates on how to send refinements to quickly improve the quality. Two important factors are considered in choosing the sending order: the dependency among the data and the view point of the user. First, we develop an analytical model to investigate the effect of dependency when progressive meshes are transmitted over a lossy network. Second, we propose a receiver-driven protocol to streaming progressive meshes according to users' view-points in a scalable way. Third, to further improve the scalability, we discuss how to apply our receiver-driven protocol in a hybrid peer-to-peer streaming system.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1047–1050},
numpages = {4},
keywords = {view-dependent, progressive meshes, peer-to-peer, 3d streaming},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256788,
author = {Guan, Ling and Zhang, Zhengyou},
title = {Session Details: Brave New Topics},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256788},
doi = {10.1145/3256788},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459572,
author = {Li, Zhiwei and Zhang, Lei and Ma, Wei-Ying},
title = {Delivering Online Advertisements inside Images},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459572},
doi = {10.1145/1459359.1459572},
abstract = {We present in this paper a new channel to deliver online advertisements along with Web images and show a new business model to monetize billions of Web images. The idea is intuitively inspired by image displaying processes on the Web, which typically require people to wait a few seconds before they see full resolution images. This is due to large file sizes and limited network bandwidth. To utilize idle time and the display area, we propose an innovative method for non-intrusively embedding ads into images in a visually pleasant manner. To maintain a smooth user experience, we utilize the thumbnail of the full-resolution image because it is small and visually similar to the full-resolution image. At the client side, a rendering engine first enlarges and blurs the thumbnail, and then blends the pre-chosen ads information into the enlarged image. Based on this idea, we propose three typical scenarios that can adopt the proposed image-advertising mode. More importantly, we can encourage providers of images or other users to participate in our online image ads service by tagging or annotating images. We envision revenue sharing with the providers participating in our service, and we expect that a large number of users will actively submit, tag and annotate images using the system. We have implemented a prototype image ads system, and conducted a series of experiments and user studies to evaluate such a new advertisement channel. The experimental results and user studies show that the proposed online image ad delivery is a non-intrusive ads mode, and the proposed solution is practical. This work also opens multiple new research directions ranging from multimedia to web data mining},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1051–1060},
numpages = {10},
keywords = {online ads, targeting ads, image ads},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459573,
author = {Vinciarelli, Alessandro and Pantic, Maja and Bourlard, Herv\'{e} and Pentland, Alex},
title = {Social Signal Processing: State-of-the-Art and Future Perspectives of an Emerging Domain},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459573},
doi = {10.1145/1459359.1459573},
abstract = {The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for Social Signal Processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially-aware computing.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1061–1070},
numpages = {10},
keywords = {social signals, behaviour analysis, human centered computing},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459574,
author = {Luo, Jiebo and Yu, Jie and Joshi, Dhiraj and Hao, Wei},
title = {Event Recognition: Viewing the World with a Third Eye},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459574},
doi = {10.1145/1459359.1459574},
abstract = {Semantic event recognition based only on vision cues is a challenging problem. This problem is particularly acute when the application domain is unconstrained still images available on the Internet or in personal repositories. In recent years, it has been shown that metadata captured with pictures can provide valuable contextual cues complementary to the image content and can be used to improve classification performance. With the recent geotagging phenomenon, an important piece of metadata available with many geotagged pictures now on the World Wide Web is GPS information. In this study, we obtain satellite images corresponding to picture location data and investigate their novel use to recognize the picture-taking environment, as if through a third eye above the object. Additionally, we combine this inference with classical vision-based event detection methods and study the synergistic fusion of the two approaches. We employ both color- and structure-based visual vocabularies for characterizing ground and satellite images, respectively. Training of satellite image classifiers is done using a multiclass AdaBoost engine while the ground image classifiers are trained using SVMs. Modeling and prediction involve some of the most interesting semantic event-activity classes encountered in consumer pictures, including those that occur in residential areas, commercial areas, beaches, sports venues, and parks. The powerful fusion of the complementary views achieves significant performance improvement over the ground view baseline. With integrated GPS-capable cameras on the horizon, we believe that our line of research can revolutionize event recognition and media annotation in years to come.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1071–1080},
numpages = {10},
keywords = {event recognition, visual vocabulary, geotagged photographs},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256789,
author = {Steinmetz, Arnd},
title = {Session Details: Open Source},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256789},
doi = {10.1145/3256789},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459576,
author = {Lohse, Marco and Winter, Florian and Repplinger, Michael and Slusallek, Philipp},
title = {Network-Integrated Multimedia Middleware (NMM)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459576},
doi = {10.1145/1459359.1459576},
abstract = {Today's multimedia infrastructures adopt a centralized approach, where all multimedia processing takes place within a single system. The network is, at best, used for streaming data transmission. Since there is a strong trend towards networked systems, these traditional approaches are becoming obsolete. In contrast, the Network-Integrated Multimedia Middleware (NMM) offers a multimedia architecture, which considers the network as an integral part and enables the intelligent use of devices distributed across the network.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1081–1084},
numpages = {4},
keywords = {home networking, streaming, middleware, multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459577,
author = {Lux, Mathias and Chatzichristofis, Savvas A.},
title = {Lire: Lucene Image Retrieval: An Extensible Java CBIR Library},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459577},
doi = {10.1145/1459359.1459577},
abstract = {LIRe (Lucene Image Retrieval) is a light weight open source Java library for content based image retrieval. It provides common and state of the art global image features and offers means for indexing and retrieval. Due to the fact that it is based on a light weight embedded text search engine, it can be integrated easily in applications without relying on a database server.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1085–1088},
numpages = {4},
keywords = {image retrieval, image indexing, image features, image search},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459578,
author = {Allusse, Yannick and Horain, Patrick and Agarwal, Ankit and Saipriyadarshan, Cindula},
title = {GpuCV: An Opensource GPU-Accelerated Framework Forimage Processing and Computer Vision},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459578},
doi = {10.1145/1459359.1459578},
abstract = {This paper presents GpuCV, an open source multi-platform library for easily developing GPU-accelerated image processing and Computer Vision operators and applications. It is meant for computer vision scientist not familiar with GPU technologies. It is designed to be compatible with Intel's OpenCV library by offering GPU-accelerated operators that can be integrated into native OpenCV applications. The GpuCV framework transparently manages hardware capabilities, data synchronization, activation of low level GLSL and CUDA programs, on-the-fly benchmarking and switching to the most efficient implementation and finally offers a set of image processing operators with GPU acceleration available.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1089–1092},
numpages = {4},
keywords = {image processing, computer vision, glsl, nvidia cuda, gpgpu},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459579,
author = {Berger, Andreas and Pallara, Lorenzo and Schatz, Raimund},
title = {An Open Source Software Framework for DVB-* Transmission},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459579},
doi = {10.1145/1459359.1459579},
abstract = {This article gives a brief overview of our Open Source software framework for generating MPEG-2 Transport Streams carrying digital TV programs and related metadata. Due to the framework's modular architecture and its support for a large variety of broadcast standards, it enables the realization of a broad spectrum of prototype implementations and test setups in the context of Multimedia research and development. We describe three different example configurations for the generation of DVB-T and DVB-H streams that also support MHP applications for local interactivity.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1093–1096},
numpages = {4},
keywords = {realtime systems, linux, mpeg-2, digital video broadcasting, mhp, open source},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459580,
author = {San Pedro, Jose},
title = {Fobs: An Open Source Object-Oriented Library for Accessing Multimedia Content},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459580},
doi = {10.1145/1459359.1459580},
abstract = {The exceptionally large nature of multimedia content has motivated the creation of many different compression algorithms and encapsulation formats to make its transportation and storage feasible. Developers of multimedia applications have to deal repeatedly with the massive number of forms in which content is present, turning the single task of media access into an unnecessary challenge. The open source project FOBS provides a way to abstract developers from these difficulties, by offering an intuitive and powerful object oriented multimedia access API. FOBS has been conceived to be inherently platform independent and to be easily adaptable to multiple programming languages, making the addition of multimedia support possible in almost any application.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1097–1100},
numpages = {4},
keywords = {api, fobs, format, object oriented, multimedia access, codec},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256790,
author = {Haenselmann, Thomas},
title = {Session Details: Video Abstracts},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256790},
doi = {10.1145/3256790},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459582,
author = {Tang, Nick C. and Shih, Timothy K. and Zhong, Hsing-Ying and Tsai, Joseph C. and Tang, Chin-Yao},
title = {Video Falsifying for Special Effect Production},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459582},
doi = {10.1145/1459359.1459582},
abstract = {Video Falsifying is a technique for generating fake video by altering, combining, or creating new video contents. For instance, the outcome of a 100-meter race can be changed. Alternatively, the winner of a horse race can be created from combined videos. The technique is for special effect production in movie industry. We demonstrate how to use our video falsifying techniques in special effects production. Interested readers should look at our demonstration website at http://member.mine.tku.edu.tw/www/ACMMM08VideoDemo.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1101–1102},
numpages = {2},
keywords = {video inpainting, special effect, layer segmentation, tracking},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459583,
author = {Lampi, Fleming and Kopf, Stephan and Effelsberg, Wolfgang},
title = {Automatic Lecture Recording},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459583},
doi = {10.1145/1459359.1459583},
abstract = {We present our system for automatic lecture recording. In contrast to traditional lecture recording systems our approach aims to imitate a real camera team, consisting of multiple cameramen and a director, in order to make the recording more vivid. At first, we introduce the environment. Then, we present our approach, beginning with the director module, followed by the cameraman module and the sensor tools module, referring to the jobs of the respective human originals. The director is based on an Extended Finite State Machine, with transition conditions depending on input from sensor tools. The cameraman automatically controls iris, focus, etc., and also takes basic cinematographic rules into account. The video concludes with an example result of our virtual camera team of a real lecture. A video demo of our approach is available at [4].},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1103–1104},
numpages = {2},
keywords = {lecture transmission, automation, educational multimedia application, video production rules, camera-team, lecture recording},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459584,
author = {Sheppard, Renata and Kamali, Mahsa and Tamai, Morihiko and Rivas, Raoul and Yang, Zhenyu and Wu, Wanmin and Nahrstedt, Klara},
title = {Tele-Immersive Dance (TED): Evolution in Progress},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459584},
doi = {10.1145/1459359.1459584},
abstract = {We demonstrate the Tele-immersive Dance Environment (TED), a geographically distributed, real-time 3-D virtual room where multiple participants interact independent of physical distance. TED, a highly interactive collaborative environment, offers digital options with multiple viewpoints, enhancing the creative movement composition involved with dance choreography. We present the advancements of the interactive digital options, new interface developments, and user study results.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1105–1106},
numpages = {2},
keywords = {creativity, dance, tele-immersion, shared virtual space},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459585,
author = {Chiu, Patrick and Huang, Jeffrey and Back, Maribeth and Diakopoulos, Nicholas and Doherty, John and Polak, Wolf and Sun, Xiaohua},
title = {MTable: Browsing Photos and Videos on a Tabletop System},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459585},
doi = {10.1145/1459359.1459585},
abstract = {In this video demo, we present mTable, a multimedia tabletop system for browsing photo and video collections. We have developed a set of applications for visualizing and exploring photos, a board game for labeling photos, and a 3D cityscape metaphor for browsing videos. The system is suitable for use in a living room or office lounge, and can support multiple displays by visualizing the collections on the tabletop and showing full-size images and videos on another flat panel display in the room.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1107–1108},
numpages = {2},
keywords = {photo browsing, multimedia visualization, video browsing, photo labeling, tabletop display},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459586,
author = {Kimber, Don and Rieffel, Eleanor and Vaughan, Jim and Doherty, John},
title = {Virtual Physics Circus},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459586},
doi = {10.1145/1459359.1459586},
abstract = {This video shows the Virtual Physics Circus, a kind of playground for experimenting with simple physical models. The system makes it easy to create worlds with common physical objects such as swings, vehicles, ramps, and walls, and interactively play with those worlds. The system can be used as a creative art medium as well as to gain understanding and intuition about physical systems. The system can be controlled by a number of UI devices such as mouse, keyboard, joystick, and tags which are tracked in 6 degrees of freedom.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1109–1110},
numpages = {2},
keywords = {physics, design, education, art, virtual reality, mixed reality},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459587,
author = {Martinet, Anthony and Martinet, Jean and Ihaddadene, Nacim and Lew, Stanislas and Djeraba, Chabane},
title = {Analyzing Eye Fixations and Gaze Orientations on Films and Pictures},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459587},
doi = {10.1145/1459359.1459587},
abstract = {Eye movements are arguably the most natural and repetitive movement of a human being. The most mundane activity, such as watching television or reading a newspaper, involves this automatic activity which consists of shifting our gaze from one point to another. Identification of the components of eye movements (fixations and saccades) is an essential part in the analysis of visual behavior because these types of movements provide the basic elements used by further investigations of human vision. However, many of the algorithms that detect fixations present a number of problems. In this paper, we present the results of a new fixation identification technique that is based on clustering of eye positions, using projections and a projection aggregation applied to static pictures. We also present results of a new method that computes dispersion of eye fixations in videos considering a multi-user environment.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1111–1112},
numpages = {2},
keywords = {scan path, eye gaze, film, dispersion, eye fixation, user, picture},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256791,
author = {Boll, Suzanne and Syeda-Mahmood, Tanveer},
title = {Session Details: Panels},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256791},
doi = {10.1145/3256791},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459589,
author = {Kerne, Andruid and Wakkary, Ron and Nack, Frank and Steggell, Amanda and Jaimes, Alejandro and Candan, K. Sel\c{c}uk and Del Bimbo, Alberto and Jennings, Pamela and Dulic, Aleksandra},
title = {Connecting Artists and Scientists in Multimedia Research},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459589},
doi = {10.1145/1459359.1459589},
abstract = {Historically, the ACM Multimedia Conference is split into a "technical" program and an "arts" program. These programs sometimes seem completely separate from one another, victims of a "semantic gap" between disciplines. The goal of this panel is to create a space in which scientists learn from artists, and arts from science. We need to discover new connections between modalities of research. In order to create the most exciting and powerful future forms of interactive multimedia systems, the ones that will create the most beneficial broader impact on humanity, we need to foster new collaborations between artists and scientists. This panel seeks to bridge the great divide of language and communities that has fragmented us, creating a new space for developing connections between the arts and sciences of multimedia research, as embodied through the artists and scientists of ACM Multimedia. The goal is to make this conference a premier site for catalyzing emergent connections.Among the ancient Greeks, the techne, which included the sciences, were based in the arts. Our modes of knowledge production have since separated and grown alienated. "What weird stuff are those people doing?" To bridge the divide, we must understand and acknowledge differences, and use this as a basis for discovering common ground. In the sciences, knowledge is constructed empirically, through hypotheses and validations. Aesthetics and concepts play essential roles in how works of art are formulated to stimulate experiences. Where do these aims intersect and how might they inform each other?In this conference, one track develops pattern recognition methods for content analysis and retrieval. Another develops network and system techniques. Applications invoke these methods in usable systems. Human-centered multimedia serves as a bridge connecting human experiences with algorithmic methods. The interactive art program develops new concepts of how multimedia can function culturally, amidst society, through tangible demonstrations of these concepts.Juxtaposing differences in methodologies and epistemologies is a method for provoking thought and identifying connections, which can lead to the development of new knowledge [1]. The goal of the panel is to catalyze discussions that build a foundation of mutual understanding and respect, and from this foundation, to build new ideas and human relationships that can lead to fruitful collaborations in the cycles to come.The panel will begin by asking each participant to characterize their approach to research, and to consider connections between the arts and sciences. "How do you formulate research goals and objectives? What are the most significant methods that you use to carry them out?" Significant contrasts in the epistemologies that underlie different modalities of research will be exposed. Artists will be asked, "How can multimedia content analysis, processing, retrieval, networking, applications, and human-centered systems contribute to your art? How can your art contribute to multimedia research in content analysis, processing, retrieval, networking, applications, and human-centered systems?" Scientists will be asked, "How can conceptual and embodied components of interactive multimedia artworks, creativity support tools, and artbased media collections contribute to your research? How can your research contribute to interactive multimedia art?"These individual statements will be followed by discussion. Panel and audience members will be asked to synthesize perspectives across disciplines, to reflect on what they have learned from each other and how this can influence future research.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1113–1114},
numpages = {2},
keywords = {interdisciplinary, hybrid, collaboration, arts, sciences},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459590,
author = {Friedland, Gerald and H\"{u}rst, Wolfgang and Knipping, Lars},
title = {Multimedia Education: Can We Find Unity in Diversity?},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459590},
doi = {10.1145/1459359.1459590},
abstract = {The field of multimedia is composed of a variety of research areas. This diversity makes multimedia such a special and interesting research field. However, the different vocabularies, methods, and cultures of the involved communities also introduce barriers that make it difficult to teach the field as a unified subject. The panel invites experts to discuss if and how we can teach the diversity in multimedia as a single subject and how we as researchers and educators can help to foster this goal. The following text provides background information to the topic and introduces the organizer's hypotheses to be discussed at the panel.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1115–1116},
numpages = {2},
keywords = {education, multimedia education, multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256792,
author = {Nack, Frank},
title = {Session Details: Art Works/Science World},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256792},
doi = {10.1145/3256792},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459592,
author = {Knees, Peter and Pohle, Tim and Widmer, Gerhard},
title = {Sound/Tracks: Real-Time Synaesthetic Sonification of Train Journeys},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459592},
doi = {10.1145/1459359.1459592},
abstract = {Travelling on a train and looking out of the window at the moving scenery reveals a composition of "visual music" with its own tempo and rhythm, its own colours and harmonies. The project sound/tracks aims at capturing these visual impressions and translates them into a musical composition in real-time - producing an immediate and unique soundtrack to the train journey based on the passing landscape. To this end, the outside impressions are captured with a camera and translated into instantaneously played back piano music. The immediately added sound dimension allows for reflection of the visual impression and deepening of the state of contemplation. For the resulting compositions, the passing scenery can be considered the score. "Re-transcription" of this score to an image gives a panoramic overview over the complete journey and exhibits some interesting effects caused by the movement of the train, such as compression and stretching of passing objects. In addition to intensifying the experience of a train journey, sound/tracks permits to persistently capture and archive the fleeting impressions of journey and composition and allows for re-experiencing the trip both visually and acoustically at a later point.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1117–1118},
numpages = {2},
keywords = {mobile music generation, real-time sonification, train journey},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459593,
author = {Dulic, Aleksandra and Newby, Kenneth C. and Gotfrit, Martin},
title = {In a Thousand Drops...: Refracted Glances},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459593},
doi = {10.1145/1459359.1459593},
abstract = {In this paper, we describe an interactive audiovisual installation In a thousand drops... refracted glances. The work creates a dynamic cinematic environment encompassing multiple screens and audio channels that work to present fragments of the bodies of humans in hybrid relations to them, thereby creating a sense of the fragility of experience. A motion sensing system tracks and maps participants' movement flows and locations onto a set of generative video animations and musical processes.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1119–1120},
numpages = {2},
keywords = {chaosmos, multiplicity, media diffusion, identity, interaction, fragmented bodies, distributed image},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459594,
author = {Steggell, Amanda},
title = {The Emotion Organ},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459594},
doi = {10.1145/1459359.1459594},
abstract = {The Emotion Organ is a synaesthetic simulacrum machine for exploring the sensational interplay of feeling, seeing, hearing, smelling and motion. It is also a time machine - a restored, re-engineered parlor organ from 1895 that builds upon a trajectory of several centuries-worth of ideas about synaesthetic phenomena, combining both antiquated and emerging technology to achieve a media-archaeological synthesis.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1121–1122},
numpages = {2},
keywords = {sensational interplay, touch, emotion organ, audio, olfactory, synaesthetic simulacrum machine, visual, senses, motion},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459595,
author = {Cayley, John and Perring, Giles},
title = {Imposition},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459595},
doi = {10.1145/1459359.1459595},
abstract = {This paper provides a brief project statement for imposition, the networked performance of an evolving collaborative work engaged with ambient, time-based poetics and harmonically organized, language-driven sound.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1123–1124},
numpages = {2},
keywords = {electronic literature, new media, multimedia, writing digital media},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459596,
author = {Dunning, Alan and Woodrow, Paul and Hollenberg, Morley},
title = {Ghosts in the Machine},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459596},
doi = {10.1145/1459359.1459596},
abstract = {Electronic Voice Phenomenon recordings are made by recording controlled static. Occasionally, one hears sounds that are like human speech. For some the voices are simply subjective interpretations, for others, the voices are a possible means of communication with the dead.This paper briefly describes a recent work of the Einstein's Brain Project referencing the ideas in EVP to examine ways in which we transform worlds, and bodies in worlds, through pareidolia, apophenia and the gestalt effect. The work uses the strategies of EVP, voice and pattern recognition, and face tracking to generate voices, and images from apparently closed, silent and empty spaces and systems.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1125–1126},
numpages = {2},
keywords = {pattern recognition, face tracking, electronic voice phenomena, art and science, art},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459597,
author = {Choi, Yuri and Kwon, Soonil and Kim, Yong Ho},
title = {Le Salon de R\'{e}Currence},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459597},
doi = {10.1145/1459359.1459597},
abstract = {Hair salon is an ordinary space we casually visit when we want to change our hairstyles. Although it requires active interaction with hairdresser to get the style as desired, possible motions and gestures of customers are usually limited. In this work, we present a multimedia work that attempts to show the visualization of hairstyles in fantasy based on the customer's simple gestures including eye movement or foot tapping, and to create spatialized sound according to the eye-tracking information to offer multi-modal feedback. We present this as an approach to our daily lives using ordinary media.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1127–1128},
numpages = {2},
keywords = {eye-gaze tracking, mirror stage theory, ordinary media, hair salon, fluid simulation, visualization, 3d sound},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459598,
author = {Jennings, Pamela},
title = {Exploring Open Narrative Structures with Tangibles},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459598},
doi = {10.1145/1459359.1459598},
abstract = {The Constructed Narratives Construction Kit is a tangible game interface built on the 802.15.4 wireless protocol designed for supporting players social awareness of others who are also engaged in the collaborative building activity.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1129–1130},
numpages = {2},
keywords = {802.15.4 mesh networks, public space interfaces, tangible social interface, collaborative design, computer supported collaborative play},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459599,
author = {Shea, Geoffrey},
title = {Portage: Locative, Streetscape Art},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459599},
doi = {10.1145/1459359.1459599},
abstract = {Portage is an aesthetic, technical and social research project which has, as one of its outcomes, publicly installed interactive artworks, activated largely through mobile phones. We have created a number of interactions that facilitate both haptic and digitally mediated experiences. Users choose among a range of activities that are completed individually or in collaboration: playing musical instruments via mobile devices, viewing themselves as video surveillance subjects, or setting off a series of visual and aural feedback loops based on the presence of mobile devices. The experiences created go beyond cell phones; they include elements of sculpture, performance and other forms of site specificity or locativity.In this presentation at the ACM MM 08 Art Exhibit we are demonstrating three interactive objects which highlight many of our concerns and objectives: two musical sound sculptures and an electronic newspaper box.The sound sculptures function first and foremost as intriguing visual and aural objects. Created in collaboration with our software and hardware developers and installation artist Leigh-Ann Pahapill, the pieces are meant to respond to the specific streetscape environment where they were first installed.One sculpture is an electro-mechanical rhythm machine consisting of differently tuned cowbells attached to drum pedals and computer controlled motors. Each is mounted next to, and strikes, a steel bike lock stand commonly found or overlooked in the urban landscape. Visitors are initially attracted to the odd machines with their brightly coloured bells. A label on the stands instructs them to use their data-enabled cell phones to launch a rhythm sequencing program to take control of these 'drums.' A web-based visual interface on the phone displays a constantly looping eight-beat phrase with three rows of check-boxes: one for each drum. By simply checking off different combinations users create their own sequenced patterns which can loop continuously until modified by the next user.In other instantiations of the same piece, users with more sophisticated Bluetooth capable phones had the option to download the application and communicate with the controlling computer just using local Bluetooth networking. In either case, people without cell phones (or the inclination to use them) can simply play the drums by stomping on the drum pedals just as any rock drummer would.At the centre of these variations was our concern with the realities of cell phone usage in urban Canada. The cost associated with using a cell phone data plan for web browsing has remained too high for too long, curtailing more sophisticated usage and creative content development, especially compared to other countries. On the other hand, Bluetooth, which is generally a free service, is more complicated to use, requiring users to connect, download, authorize, install, find and launch potentially useful or interesting programs. And of course, the relatively low rate of cell phone penetration and usage in Canada is highlighted by the manual or haptic interface option.The second sound sculpture takes the form of a remotely played xylophone. While we are continuing to add instruments to our ensemble, we consciously chose to start with one that focuses on programmatic playing (the rhythm sequencer) and one that encourages live, melodic improvisation. The xylophone, which is designed to rest directly on the sidewalk and blend into the streetscape, consists of a range of tuned metal tines struck from below by computer actuated solenoids. Again, two interfaces were employed to explore the range of user interaction. In one, users dial a local number connecting to an Asterisk server. The twelve keys on the phone are then used to trigger the twelve notes in a scale. Any sequence of keys can be pressed and then the corresponding notes play in a loop four times. Users can sit back and listen to the pattern they just played or busy themselves keying in the next pattern.In another presentation users had the option to dial another number connecting to a VOIP server. Intended to explore a more direct, spontaneous form of musical creation, they were then encouraged to sing or whistle into the phone. Pitch detection software converted their sounds into a signal to play the corresponding notes on the xylophone. Some reticence was encountered by users when asked to sing into their phones on a crowded city street. Again, users could simply pick up mallets and play the xylophone directly.In another vein, we have created a newspaper vending box (also a familiar fixture on urban Canadian streets) with a screen depicting an electronic 'newspaper.' Our tabloid style paper features a large image with a headline emblazoned over it. Cell phone users with text messaging (the lowest common denominator of mobile data services) are encouraged to re-write the headline simply by sending a text message. Users with more capable MMS camera phones are further encouraged to change the image. The two most recent previous images continued to be displayed in smaller 'feature' boxes at the bottom of the screen/paper.In all of these installations we were handing over responsibility for the actual, specific content to the users. While we did create the general forms for the interactions, these, too, were in response to the existing context of the streetscape setting. Our goal of involving users with various or no levels of mobile technology was meant to interrogate the principals of technological and media consumerism and industrial practices. By presenting our work as physical streetscape artifacts we were able to engage viewers even before they decided to interact (or not) and created visual, aural and textual experiences that could be appreciated simply and directly.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1131–1132},
numpages = {2},
keywords = {instrument, interactive, art, locative, mobile, music},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459600,
author = {Kobayashi, Hiroki and Ueoka, Ryoko and Hirose, Michitaka},
title = {Wearable Forest-Feeling of Belonging to Nature},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459600},
doi = {10.1145/1459359.1459600},
abstract = {Wearable Forest is a clothing design that bio-acoustically interacts with distant wildlife in a remote forest through a remote-controlled speaker and microphone using a network. It expresses the bioacoustical beauty of nature in its unique aesthetic appeal for users and allows the users interact with a forest in real time through a network to acoustically experience a distant forest soundscape, thus merging man and nature without environmental destruction. This novel interactive sound system can create a sense of unity between users and a remote soundscape, enabling users to feel a sense of belonging to nature even in the midst of a city.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1133–1134},
numpages = {2},
keywords = {smart fashion, nature interface, hcbi (human computer biosphere interaction), soundscape visualization},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459601,
author = {Seo, Jinsil and Corness, Greg},
title = {Nite_aura: Audio-Visual Immersive Installation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459601},
doi = {10.1145/1459359.1459601},
abstract = { 'nite_aura' has been developed as both an artistic work and an instrument for research into immersive environments. 'nite_aura' is an audio-visual, interactive installation exploring physical motion within an alternative immersive environment. The enchantment of the night's sky has inspires flights of imagination and whispered wishes in humans of all ages. 'nite_aura' embodies these memories and experiences allowing for visitors to whisper to the air and play with fields of stars surrounding them in an interactive immersive art installation focusing on sensual and physical interaction. The work investigates the effect of the texture of space, light and sound in providing a comforting, relaxing, and meditative quality of immersion.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1135–1136},
numpages = {2},
keywords = {alternative immersive installation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459602,
author = {Merhi, Yucef},
title = {Super Atari Poetry},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459602},
doi = {10.1145/1459359.1459602},
abstract = {Super Atari Poetry is an interactive multiplayer poetry game-installation based on Atari 2600 consoles. It emphasizes the importance of cultural and historical values on game programming, showing the intrinsic relationship between natural language and machine language. Using the joysticks, players can create up to one thousand poems from verses that were previously written by the artist.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1137–1138},
numpages = {2},
keywords = {poetry, new media art, video game, social networks},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459603,
author = {Andreyev, Julie},
title = {FWDrift [Remix]},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459603},
doi = {10.1145/1459359.1459603},
abstract = {This paper describes the FWDrift [remix] performance, which uses the car as a platform for experimentation with local knowledge of the cityscape and the embodies experience of its geography.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1139–1140},
numpages = {2},
keywords = {fwdrift, performance, pubic, urban space, art installation},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256793,
author = {M\"{u}hlh\"{a}user, Max and Poncelon, Dulce},
title = {Session Details: Co-Located Workshop Overviews},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256793},
doi = {10.1145/3256793},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459605,
author = {Aghajan, Hamid and Prati, Andrea},
title = {ACM Multimedia 2008: 1st Workshop on Vision Networks for Behavior Analysis (VNBA 2008)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459605},
doi = {10.1145/1459359.1459605},
abstract = {The VNBA workshop marks a new era of the successful series of the Video Surveillance and Sensor Networks (VSSN) workshops, held until 2006 in conjunction with the ACM Multimedia conference. This new version of the workshop inherits from VSSN the experienced Technical Program Committee as well as the interests of its community, but shifts the focus to cover higher level topics and applications under the common framework of "behaviour analysis", hence aiming to adapt to the evolved directions of interest in the field, and reaching out to other research communities with overlapping interests.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1141–1142},
numpages = {2},
keywords = {sensor networks, behavior analysis, smart environments, computer vision, distributed camera systems, surveillance, human-computer interaction},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459606,
author = {Lugmayr, Artur and Stockleben, Bjoern and Risse, Thomas and Kaario, Juha and Laurila, Kari},
title = {ACM Multimedia 2008: 1st Workshop on Semantic Ambient Media Experiences (SAME2008) Namu Series},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459606},
doi = {10.1145/1459359.1459606},
abstract = {The Semantic Ambient Media Experiences (SAME) workshop series aims at the development of semantic ambient media as new form of media. SAME provides a forum for scientists, practitioners, artists, content producers, industry, and researchers to discuss results in the field of ambient media. The multidisciplinary workshop shall raise awareness and promote collaboration between the leaders in the field of ambient media. The SAME 2008 was the premiere of a series of workshops held in conjunction with ACM Multimedia 2008 in Vancouver, Canada. The result of the workshop shall be an Internet platform for people interested in the field of ambient media. The workshop aims at the creation of a think-tank of creative thinkers with interest in glimpsing the future of semantic ambient media. To join our future activities or our mailing list, please refer to our website on http://namu.cs.tut.fi/acmmm2008/same2008/.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1143–1144},
numpages = {2},
keywords = {art, ubiquitous computation, experience, ambient media, context awareness, pervasive computation, semantic, ambient content},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459607,
author = {Brooks, Kevin and Kelliher, Aisling and Nack, Frank},
title = {Second International Workshop on Story Representation, Mechanism and Context (SRMC 2008)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459607},
doi = {10.1145/1459359.1459607},
abstract = {Stories are one of the primary forms we use to organize our lived experiences into patterned narratives that aspire to communicate that which is memorable and valuable. SRMC 2008 provides a forum for multimedia researchers and practitioners to share their novel insights and understandings of the human and machine storytelling abilities necessary for the development of engaging, participatory and sustainable multimedia systems.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1145–1146},
numpages = {2},
keywords = {digital storytelling, narrative, virtual, games, social networks, reality},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459608,
author = {Doulamis, Anastasios and van Gool, Luc and Nixon, Mark and Varvarigou, Theodora and Doulamis, Nikolaos},
title = {First ACM International Workshop on Analysis and Retrieval of Events, Actions and Workflows in Video Streams},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459608},
doi = {10.1145/1459359.1459608},
abstract = {AREA 2008 is the first ACM international workshop on analysis and retrieval of events, actions and workflows in video streams. Such research is nowadays critical for many real-life applications, such as area supervision, semantic characterization and annotation of video streams, quality assurance, and security. This workshop consists of 16 high quality papers organized in four thematic sessions. More specifically, the first session is dedicated to new objects tracking algorithms under complex environments and to object labeling techniques. The second session deals with methods, tools and architectures for detecting high level semantics (events, actions, and workflows) in video sequences. The third session presents new algorithms for analyzing video sequences oriented to detecting humans' actions or implicitly annotating multimedia content. Finally, the fourth includes a special session of the recent advantages of the ongoing research projects in the field of multimedia analysis, cognitive video supervision, personalized video annotation, fast retrieval of multimedia content in compressed domain and scheduling tools for interactive multimedia services. We hope that these proceedings will serve as a valuable reference for analysis of events in video streams.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1147–1148},
numpages = {2},
keywords = {retrieval of actions, analysis of events, tracking, even-driven video analysis},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459609,
author = {Fotouhi, Farshad and Grosky, William I. and Stanchev, Peter},
title = {MS 2008: Second International Workshop on the Many Faces of Multimedia Semantics},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459609},
doi = {10.1145/1459359.1459609},
abstract = {The Second International Workshop on the Many Faces of Multimedia Semantics (MS'08) is the second such workshop held in conjunction with the ACM International Multimedia Conference. This workshop provides a forum for practitioners, artists, and scientists to exchange ideas on how to develop adaptive media retrieval techniques which satisfy individual user needs.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1149–1150},
numpages = {2},
keywords = {ontologies, emergence, pragmatics, metadata, retrieval, semantic web, adaptive, semantics},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459610,
author = {Jaimes, Alejandro and Nicklas, Daniela and Sebe, Nicu},
title = {3rd International Workshop on Human-Centered Computing (HCC '08)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459610},
doi = {10.1145/1459359.1459610},
abstract = {In this workshop summary we describe the motivation for continued discussion in Human-Centered Computing, giving an outline of the articles presented at the workshop, its expected outcomes, and future activities. We emphasize the reasoning behind a non-traditional format for the workshop, which builds on the previous workshops on "Human-Centered Multimedia" held in conjunction with ACM Multimedia 2007 and 2006.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1151–1152},
numpages = {2},
keywords = {human-computer interfaces, folk computing, multimodal interaction, multimedia, human-centered computing},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459611,
author = {Cipolla Ficarra, Francisco V.},
title = {1st Workshop Communicability Design and Evaluation in Cultural and Ecological Multimedia Systems: (Communicabilityms '08)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459611},
doi = {10.1145/1459359.1459611},
abstract = {The workshop Communicability Design and Evaluation in Cultural and Ecological Multimedia Systems (Communicability MS2008) is a premier scientific meeting for discussing the latest advances in the areas quality design, communication and semiotics, multimedia heuristics evaluation, cultural and natural heritage. To join our future activities, please refer to our website on http://www.alaipo.com or http://www.ainci.com},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1171–1172},
numpages = {2},
keywords = {design, tourism, ubiquitous web, usability, interface, quality, ecology, web 3.0, communicability, semiotics, evaluation, credibility, software engineering, human-computer interaction, cultural heritage},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/3256794,
author = {Zimmermann, Roger},
title = {Session Details: Tutorials},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3256794},
doi = {10.1145/3256794},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459613,
author = {Fazio, Nelly and Poncele\'{o}n, Dulce},
title = {Tutorial on Content Protection},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459613},
doi = {10.1145/1459359.1459613},
abstract = {Devising effective Content Protection mechanisms and building satisfactory Digital Rights Management systems have been top priorities for the Publishing and Entertainment Industries in recent years. In this tutorial, we focus on protection tools and standards for entertainment content. We analyze the challenges of content protection systems, ranging from legal aspects and business models to cryptographic techniques. We discuss current technology licensing practices, describe state-of-the-art broadcast encryption mechanisms and forensic approaches, and present content protection standards such as 4C and AACS.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1153–1154},
numpages = {2},
keywords = {broadcast encryption, traitor tracing, 4c, blu-ray disc, aacs},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459614,
author = {Yan, Rong and Hsu, Winston},
title = {Recent Developments in Content-Based and Concept-Based Image/Video Retrieval},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459614},
doi = {10.1145/1459359.1459614},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1155–1156},
numpages = {2},
keywords = {image/video retrieval, concept-based retrieval, content-based retrieval},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459615,
author = {Celma, \`{O}scar and Lamere, Paul},
title = {If You like the Beatles You Might like...: A Tutorial on Music Recommendation},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459615},
doi = {10.1145/1459359.1459615},
abstract = {As the world of online music grows, music recommendation systems become an increasingly important way for music listeners to discover new music. Commercial recommenders such as Last.fm and Pandora have enjoyed commercial and critical success. But how well do these systems really work? How good are the recommendations? How far into the Long Tail do these recommenders reach? In this tutorial we look at the current state-of-the-art in music recommendation. We examine current commercial and research systems, focusing on the advantages and the disadvantages of the various recommendation strategies. We look at some of the challenges in building music recommenders and we explore some of the ways that Multimedia Information Retrieval techniques can be used to improve future recommenders, including a multi-modal approach merging the different fields (audio, image, video and text).},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1157–1158},
numpages = {2},
keywords = {evaluation, social tagging, audio analysis, long tail, complex network analysis, music recommender system},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459616,
author = {Iglesias, Rosa and El Saddik, Abdulmotaleb},
title = {A Glimpse of Multimedia Ambient Intelligence},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459616},
doi = {10.1145/1459359.1459616},
abstract = {Ambient Intelligence (AmI) characterizes a new paradigm for novel interactions between a person and his/her everyday environment. This will impose major challenges on multimedia research to support human-centered information, communication, services and entertainment everyday and everywhere.This tutorial presents a brief overview of systems, technologies and applications that are part of Multimedia AmI. It is the purpose of this tutorial first to give an introduction to Multimedia AmI and Technology Roadmapping. It is then introduced the multimedia AmI services, components and access networks that are essential for the development of AmI environments. Next, fundamental technologies are presented such as wired/wireless, gateway, and middleware technologies. Finally, some applications together with the concepts of "Personal Administrator" and "Extended Home Environment" are introduced. This paper gives a brief overview of the areas covered in the tutorial.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1159–1160},
numpages = {2},
keywords = {ubiquitous computing, multimedia, human-computer interfaces, ambient intelligence (ami)},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459617,
author = {Pradhan, Gaurav N. and Prabhakaran, Balakrishnan},
title = {Storage, Retrieval, and Communication of Body Sensor Network Data},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459617},
doi = {10.1145/1459359.1459617},
abstract = {Recently, Body Sensor Networks (BSNs) are being deployed for monitoring and managing medical conditions as well as human performance in sports. These BSNs include various sensors such as accelerometers, gyroscopes, EMG (Electromyogram), EKG (Electro-cardiograms), and other sensors depending on the needs of the medical conditions. Data from these sensors are typically Time Series data and the data from multiple sensors form multiple, multidimensional time series data. Analyzing data from such multiple medical sensors pose several challenges: different sensors have different characteristics, different people generate different patterns through these sensors, and even for the same person the data can vary widely depending on time and environment.This tutorial describes the technologies that go behind BSNs - both in terms of the hardware infrastructure as well as the basic software. First, we outline the BSN hardware features and the related requirements. We then discuss the energy and communication choices for BSNs. Next, we discuss approaches for classification, data mining, visualization, and securing these data. We also show several demonstrations of body sensor networks as well as the software that aid in analyzing the data.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1161–1162},
numpages = {2},
keywords = {principal component analysis, body sensor networks, factor analysis, fuzzy clustering, multidimensional, association rules},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459618,
author = {Saddik, Abdulmotaleb El and Cha, Jongeun and Kahol, Kanav},
title = {Haptics Technologies: Theory and Applications from a Multimedia Perspective},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459618},
doi = {10.1145/1459359.1459618},
abstract = {The desire for natural intuitive modes of interactions with digital media has led to development of multimodal interfaces that aim to engage the users through a confluence of modalities such as audio, video etc. Haptic systems that enable touch based interactions with digital environments are a recent addition to multimodal systems and have widespread applications; there is a need to develop a sound design, development and evaluation strategy to leverage the availability of the haptic modality. This tutorial aims to provide an initial impetus in the direction of enabling multimedia researchers to conduct research in the area of haptic user interfaces. The tutorial will present the audience with an introduction to the field of haptics. The presented material will be made accessible to the multimedia community, relating material from the haptics domain to multimedia algorithms, systems etc. At the completion of the tutorial, the students will be able to analyze and apply algorithms and strategies for design, development, and evaluation of touch based user interfaces.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1163–1164},
numpages = {2},
keywords = {haptic user interfaces},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459619,
author = {Chakraborty, Samarjit and Wang, Ye},
title = {Multimedia Power Management on a Platter: From Audio to Video &amp; Games},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459619},
doi = {10.1145/1459359.1459619},
abstract = {Today, battery-life is a major design concern for all portable devices ranging from cell phones to PDAs and portable game consoles. The purpose of this tutorial will be to give an overview of power management techniques that are applicable to multimedia applications running on such battery-operated portable devices. In particular, we will discuss a host of techniques, some of which are applicable to audio processing applications, some to video processing, and the others to interactive 3D game applications. The tutorial will be helpful to students, researchers, application developers and engineers who have a background in traditional real-time multimedia applications and would like to get an overview of the important issues and solutions pertaining to using and developing power management techniques for the multimedia domain.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1165–1166},
numpages = {2},
keywords = {multimedia systems, portable devices, power management, video, games},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459620,
author = {Scheible, J\"{u}rgen},
title = {Mobile Phone Programming for Multimedia},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459620},
doi = {10.1145/1459359.1459620},
abstract = {This paper presents an overview of a compact hands-on tutorial, which introduces a novel way of creative mobile phone programming for multi-media that is easy to learn and fun to do. Pedagogically fine-tuned, it teaches the programming of a large set of mobile phone features including camera, accelerometer sensor for gesture based interfaces, sound, video, messaging, telephony, location, Bluetooth, graphics, Wi-fi, GPS and networking. Developing applications on the mobile platform was time consuming in the past and required a steep learning curve. Also, mobile platforms have often been closed or were too restricted. Mobile Python, also known as 'Python for S60' - in short PyS60 - offers a crucial turning point here. It provides a Python execution environment on the mobile phone. Nokia Research developed PyS60 as an Open Source Project [9] starting from 2004. It allows developing mobile applications even by novice programmers, artists and people from the creative communities. In a matter of days, people can build powerful applications based on their own ideas and contribute them to the mobile space.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1167–1168},
numpages = {2},
keywords = {tutorial, python for s60, mobile python},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

@inproceedings{10.1145/1459359.1459621,
author = {Sharda, Nalin},
title = {Workshop Process for Authoring Educational Multimedia Using Movement Oriented Design (MOD)},
year = {2008},
isbn = {9781605583037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1459359.1459621},
doi = {10.1145/1459359.1459621},
abstract = {The lack of systematic processes for authoring educational multimedia content is impeding the realization of its full potential. Catering to different learning styles is also a challenge in creating effective educational multimedia content. Storytelling has been recognized as an effective pedagogical technique to keep the learners engaged. Movement Oriented Design (MOD) paradigm provides a framework for systematically developing educational multimedia stories using good storytelling principles articulated by the masters of storytelling, such as Aristotle and Robert McKee. This paper introduces Movement Oriented Design principles, and imperatives for creating good educational multimedia stories. Next it presents the workshop process for developing educational multimedia narratives.},
booktitle = {Proceedings of the 16th ACM International Conference on Multimedia},
pages = {1169–1170},
numpages = {2},
keywords = {learning styles, content creation, story development, narrative model, movement oriented design (mod)},
location = {Vancouver, British Columbia, Canada},
series = {MM '08}
}

