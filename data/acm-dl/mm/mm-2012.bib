@inproceedings{10.1145/3246389,
author = {Babaguchi, Noboru},
title = {Session Details: Plenary Talk 1},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246389},
doi = {10.1145/3246389},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393349,
author = {Fujita, Masahiro},
title = {Future Direction of Digital Content: 20th Anniversary Keynote Talk},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393349},
doi = {10.1145/2393347.2393349},
abstract = {We review the history and trends of multimedia technologies, especially focusing on audio &amp; video products. Our lifestyles have been changing according to the developments of these technologies. We forecast the direction of multimedia technologies and our lifestyles in future. We can guess that the future direction of audio &amp; video is going to higher and higher fidelity. Display devices will be larger and flexible. In addition, wearable devices will be popular as consumer products. It will be achieved by printing manufacture technologies. These devices and high fidelity content will realize immersive super reality.While we will enjoy the traditional audio &amp; video content with the immersive super reality, we will use more social media by which realtime &amp; global human-human interaction will be realized. We will be able to utilize other people's knowledge and decisions in realtime using the interactive &amp; immersive super reality.In addition to the social networks with humans, sensor &amp; actuator networks will also be globally established. This will extend our perception and decision abilities. We will find new problems with the distributed sensor &amp; actuator networks, which we did not find with our ordinary perceptions. Using interactive &amp; immersive super reality, we will collaboratively solve personal daily problems as well as global problems.Thus, in future we will realize collaborative community with interactive &amp; immersive super reality. In the commu- nity we will enjoy entertainment content and will solve many problems collaboratively. This community itself will be the content in the future.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1–2},
numpages = {2},
keywords = {digital content, collaborative community, display devices, interactive &amp; immersive super reality, social media, printing manufacture},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246390,
author = {Nahrstedt, Klara and Slaney, Malcolm},
title = {Session Details: Panel 1: 20th Anniversary Panel},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246390},
doi = {10.1145/3246390},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393351,
author = {Nahrstedt, Klara and Slaney, Malcolm},
title = {Coulda, Woulda, Shoulda: 20 Years of Multimedia Opportunities},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393351},
doi = {10.1145/2393347.2393351},
abstract = {The ACM Special Interest Group on Multimedia (SIGMM) is celebrating the 20th anniversary of establishing its premier conference, the ACM International Conference on Multimedia (ACM Multimedia). The panel "Coulda, Woulda, Shoulda" is part of the celebration at the ACM Multimedia 2012. The panelists and the audience will discuss the 20 years of multimedia opportunities that our community has seen, took upon and pushed forward to advance the state of the art.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {3–4},
numpages = {2},
keywords = {multimedia processing, multimedia information systems, information interfaces and presentation, multimedia content analysis, mobile multimedia, personal multimedia computing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246391,
author = {Aizawa, Kiyoharu},
title = {Session Details: Plenary Talk 2},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246391},
doi = {10.1145/3246391},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393353,
author = {Kamitani, Yukiyasu},
title = {Decoding Visual Experience from the Human Brain},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393353},
doi = {10.1145/2393347.2393353},
abstract = {Brain activity can be seen as "codes" that encode mental states. Recent advances in human neuroimaging such as functional magnetic resonance imaging (fMRI) have revealed brain regions that encode specific behavior and cognition. Despite the wide-spread use of human neuroimaging, its potential to read out, or "decode", mental contents from brain activity has not been fully explored. In this talk, I present methods for decoding visual representations from fMRI activity patterns based on machine learning techniques. I show how early visual features represented in "subvoxel" neural structures could be decoded from ensemble fMRI responses. Decoding of stimulus features is extended to the method for neural mind-reading, which attempts to predict a person's subjective state using a decoder trained with unambiguous stimulus presentation. We then discuss a modular decoding approach, in which a wide variety of percepts can be decoded by combining the outputs of multiple decoder modules. On the basis of this approach, we were able to reconstruct arbitrary visual images using the decoder trained on fMRI responses to only several hundred random images. Finally, I discuss potential applications of neural decoding to brain-based communications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {5–6},
numpages = {2},
keywords = {neural decoding, mri, machine learning, vision},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246392,
author = {Xie, Lexing and Shamma, David A. and Snoek, Cees},
title = {Session Details: Panel 2: Panel Discussion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246392},
doi = {10.1145/3246392},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393355,
author = {Xie, Lexing and Shamma, David Ayman and Snoek, Cees},
title = {Content is Dead: Long-Live Content!},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393355},
doi = {10.1145/2393347.2393355},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {7–8},
numpages = {2},
keywords = {context, multimedia, social, content},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246393,
author = {Smith, John},
title = {Session Details: Best Paper Session},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246393},
doi = {10.1145/3246393},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393357,
author = {Liu, Heng and Mei, Tao and Luo, Jiebo and Li, Houqiang and Li, Shipeng},
title = {Finding Perfect Rendezvous on the Go: Accurate Mobile Visual Localization and Its Applications to Routing},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393357},
doi = {10.1145/2393347.2393357},
abstract = {While on the go, more and more people are using their phones to enjoy ubiquitous location-based services (LBS). One of the fundamental problems of LBS is localization. Researchers are now investigating ways to use a phone-captured image for localization as it contains more scene context information than the embedded sensors. In this paper, we present a novel approach to mobile visual localization that accurately senses geographic scene context according to the current image (typically associated with a rough GPS position). Unlike most existing visual localization methods, the proposed approach is capable of providing a complete set of more accurate parameters about the scene geo---including the actual locations of both the mobile user and perhaps more importantly the captured scene along with the viewing direction. Our approach takes advantage of advanced techniques for large-scale image retrieval and 3D model reconstruction from photos. Specifically, we first perform joint geo-visual clustering in the cloud to generate scene clusters, with each scene represented by a 3D model. The 3D scene models are then indexed using a visual vocabulary tree structure. The phone-captured image is used to retrieve the relevant scene models, then aligned with the models, and further registered to the real-world map. Our approach achieves an estimation accuracy of user location within 14 meters, viewing direction within 9 degrees, and scene location within 21 meters. Such a complete set of accurate geo-parameters can lead to various LBS applications for routing that cannot be achieved with most existing methods. In particular, we showcase three novel applications: 1) accurate self-localization, 2) collaborative localization for rendezvous routing, and 3) routing for photographing. The evaluations through user studies indicate these applications are effective for facilitating the perfect rendezvous for mobile users.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {9–18},
numpages = {10},
keywords = {location-based services, geo-tagging, scene reconstruction, mobile visual localization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393358,
author = {Sang, Jitao and Xu, Changsheng},
title = {Right Buddy Makes the Difference: An Early Exploration of Social Relation Analysis in Multimedia Applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393358},
doi = {10.1145/2393347.2393358},
abstract = {Social media is becoming popular these days, where user necessarily interacts with each other to form social networks. Influence network, as one special case of social network, has been recognized as significantly impacting social activities and user decisions. We emphasize in this paper that the inter-user influence is essentially topic-sensitive, as for different tasks users tend to trust different influencers and be influenced most by them. While existing research focuses on global influence modeling and applies to text-based networks, this work investigates the problem of topic-sensitive influence modeling in the multimedia domain.We propose a multi-modal probabilistic model, considering both users' textual annotation and uploaded visual image. This model is capable of simultaneously extracting user topic distributions and topic-sensitive influence strengths. By identifying the topic-sensitive influencer, we are able to conduct applications like collective search and collaborative recommendation. A risk minimization-based general framework for personalized image search is further presented, where the image search task is transferred to measure the distance of image and personalized query language models. The framework considers the noisy tag issue and enables easy incorporation of social influence. We have conducted experiments on a large-scale Flickr dataset. Qualitative as well as quantitative evaluation results have validated the effectiveness of the topic-sensitive influencer mining model, and demonstrated the advantage of incorporating topic-sensitive influence in personalized image search and topic-based image recommendation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {19–28},
numpages = {10},
keywords = {multimedia application, social relation analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393359,
author = {Wang, Zhi and Sun, Lifeng and Chen, Xiangwen and Zhu, Wenwu and Liu, Jiangchuan and Chen, Minghua and Yang, Shiqiang},
title = {Propagation-Based Social-Aware Replication for Social Video Contents},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393359},
doi = {10.1145/2393347.2393359},
abstract = {Online social network has reshaped the way how video contents are generated, distributed and consumed on today's Internet. Given the massive number of videos generated and shared in online social networks, it has been popular for users to directly access video contents in their preferred social network services. It is intriguing to study the service provision of social video contents for global users with satisfactory quality-of-experience. In this paper, we conduct large-scale measurement of a real-world online social network system to study the propagation of the social video contents. We have summarized important characteristics from the video propagation patterns, including social locality, geographical locality and temporal locality. Motivated by the measurement insights, we propose a propagation-based social-aware replication framework using a hybrid edge-cloud and peer-assisted architecture, namely PSAR, to serve the social video contents. Our replication strategies in PSAR are based on the design of three propagation-based replication indices, including a geographic influence index and a content propagation index to guide how the edge-cloud servers backup the videos, and a social influence index to guide how peers cache the videos for their friends. By incorporating these replication indices into our system design, PSAR has significantly improved the replication performance and the video service quality. Our trace-driven experiments further demonstrate the effectiveness and superiority of PSAR, which improves the local download ratio in the edge-cloud replication by 30%, and the local cache hit ratio in the peer-assisted replication by 40%, against traditional approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {29–38},
numpages = {10},
keywords = {social network, hybrid edge-cloud and P2P, video replication},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393360,
author = {Lin, Shih-Yao and Shie, Chuen-Kai and Chen, Shen-Chi and Hung, Yi-Ping},
title = {Action Recognition for Human-Marionette Interaction},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393360},
doi = {10.1145/2393347.2393360},
abstract = {In this paper, we propose a human-marionette interaction system based on a human action recognition approach for applications to interactive artistic puppetry and a mimicking-marionette game. We developed an intelligent marionette called "i-marionette" that is controlled by a sophisticated control device to achieve various human actions. Moreover, we utilized an action recognition approach to enable the i-marionette to learn and recognize complex dance movements. The idea of artistic puppetry is to present a conflict scenario between two different cultural worlds: the performer is active and represents the culture of modern technology based in the real world. In contrast, the i-marionette represents traditional culture and is passive and based in a virtual world. The active performer guides the passive i-marionette to form a space-time connection between the real world and the virtual world. The i-marionette mimics the performer's action, while the performer also mimics the i-marionette's action. The performance represents an artistic conception in which humans invent technology and the i-marionette is manipulated by human control. However, in this interactive circle, the human is implicitly affected by the i-marionette. In our mimicking-marionette game, a player mimics the i-marionette's action. Subsequently, our human action recognition system measures the action similarity between the player and the i-marionette, and our system provides a similarity score.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {39–48},
numpages = {10},
keywords = {interactive game, interactive art, human action recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246394,
author = {Huet, Benoit},
title = {Session Details: Full Paper Session 1: Content-Based Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246394},
doi = {10.1145/3246394},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393362,
author = {Yang, Yang and Yang, Linjun and Wu, Gangshan and Li, Shipeng},
title = {A Bag-of-Objects Retrieval Model for Web Image Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393362},
doi = {10.1145/2393347.2393362},
abstract = {Image search reranking has been an active research topic in recent years to boost the performance of the existing web image search engine which is mostly based on textual metadata of images. Various approaches have been proposed to rerank images for general queries and argue that, they may not necessarily be optimal for queries in specific domain, e.g., object queries, since the reranking algorithms are operated on whole images, instead of the relevant parts of images. In this paper, we propose a novel bag-of-objects retrieval model for image search reranking of object queries. Firstly, we employ a common object discovery algorithm to discover query-relevant objects from the search results returned by text-based image search engine. Then, the query and its result images are represented as a language model on the query relevant object vocabulary, based on which the ranking function can be derived. As the common object discovery is unreliable and may introduce noises, we propose to incorporate the attributes of the discovered objects, e.g., size, position, etc., into the ranking function through a linear model, and the weights on the object attributes can be learned. The experiments on two subsets of Web Queries dataset comprising object queries demonstrate that our approach can significantly outperform the existing reranking methods on object queries.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {49–58},
numpages = {10},
keywords = {bag-of-objects model, image search reranking, supervised reranking, retrieval model},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393363,
author = {Nie, Liqiang and Yan, Shuicheng and Wang, Meng and Hong, Richang and Chua, Tat-Seng},
title = {Harvesting Visual Concepts for Image Search with Complex Queries},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393363},
doi = {10.1145/2393347.2393363},
abstract = {The use of image reranking to boost retrieval performance has been found to be successful for simple queries. It is, however, less effective for complex queries due to the widened semantic gap. This paper presents a scheme to enhance web image reranking for complex queries by fully exploring the information from simple visual concepts. Given a complex query, our scheme first detects the noun-phrase based visual concepts and crawls their top ranked images from popular image search engines. Next, it constructs a heterogeneous probabilistic network to model the relatedness between the complex query and each of its crawled images. The network seamlessly integrates three layers of relationships, i.e., the semantic-level, cross-modality level as well as visual-level. These mutually reinforced layers are established among the complex query and its involved visual concepts, by harnessing the contents of images and their associated textual cues. Based on the derived relevance scores, a new ranking list is generated. Extensive evaluations on a real-world dataset demonstrate that our model is able to characterize the complex queries well and achieve promising performance as compared to the state-of-the-art methods. Based on the proposed scheme, we introduce two applications: photo-based question answering and textual news visualization. Comprehensive experiments well validate the proposed scheme.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {59–68},
numpages = {10},
keywords = {complex query, image search, news visualization, photo-based qa},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393364,
author = {Shi, Miaojing and Sun, Xinghai and Tao, Dacheng and Xu, Chao},
title = {Exploiting Visual Word Co-Occurrence for Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393364},
doi = {10.1145/2393347.2393364},
abstract = {Bag-of-visual-words (BOVW) based image representation has received intense attention in recent years and has improved content based image retrieval (CBIR) significantly. BOVW does not consider the spatial correlation between visual words in natural images and thus biases the generated visual words towards noise when the corresponding visual features are not stable. In this paper, we construct a visual word co-occurrence table by exploring visual word co-occurrence extracted from small affine-invariant regions in a large collection of natural images. Based on this visual word co-occurrence table, we first present a novel high-order predictor to accelerate the generation of neighboring visual words. A co-occurrence matrix is introduced to refine the similarity measure for image ranking. Like the inverse document frequency (idf), it down-weights the contribution of the words that are less discriminative because of frequent co-occurrence. We conduct experiments on Oxford and Paris Building datasets, in which the ImageNet dataset is used to implement a large scale evaluation. Thorough experimental results suggest that our method outperforms the state-of-the-art, especially when the vocabulary size is comparatively small. In addition, our method is not much more costly than the BOVW model.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {69–78},
numpages = {10},
keywords = {bovw, co-occurrence matrix, high-order predictor},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393365,
author = {Zhang, Hanwang and Zha, Zheng-Jun and Yan, Shuicheng and Bian, Jingwen and Chua, Tat-Seng},
title = {Attribute Feedback},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393365},
doi = {10.1145/2393347.2393365},
abstract = {This work presents a new interactive Content Based Image Retrieval (CBIR) scheme, termed Attribute Feedback (AF). Unlike traditional relevance feedback purely founded on low-level visual features, the Attribute Feedback system shapes users' information needs more precisely and quickly by collecting feedbacks on intermediate level semantic attributes. At each interactive iteration, AF first determines the most informative binary attributes for feedbacks, preferring the attributes that frequently (rarely) appear in current search results but are unlikely (likely) to be users' interest. The binary attribute feedbacks are then augmented by a new type of attributes, "affinity attributes", each of which is off-line learnt to describe the distance between user's envisioned image(s) and a retrieved image with respect to the corresponding affinity attribute. Based on the feedbacks on binary and affinity attributes, the images in corpus are further re-ranked towards better fitting the users' information needs. Extensive experiments on two real-world image datasets well demonstrate the superiority of the proposed scheme over other state-of-the-art relevance feedback based CBIR solutions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {79–88},
numpages = {10},
keywords = {attribute feedback, relevance feedback, image search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246395,
author = {Friedland, Gerald},
title = {Session Details: Full Paper Session 2: Audio and Music},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246395},
doi = {10.1145/3246395},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393367,
author = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Wang, Hsin-Min and Jeng, Shyh-Kang},
title = {The Acoustic Emotion Gaussians Model for Emotion-Based Music Annotation and Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393367},
doi = {10.1145/2393347.2393367},
abstract = {One of the most exciting but challenging endeavors in music research is to develop a computational model that comprehends the affective content of music signals and organizes a music collection according to emotion. In this paper, we propose a novel acoustic emotion Gaussians (AEG) model that defines a proper generative process of emotion perception in music. As a generative model, AEG permits easy and straightforward interpretations of the model learning processes. To bridge the acoustic feature space and music emotion space, a set of latent feature classes, which are learned from data, is introduced to perform the end-to-end semantic mappings between the two spaces. Based on the space of latent feature classes, the AEG model is applicable to both automatic music emotion annotation and emotion-based music retrieval. To gain insights into the AEG model, we also provide illustrations of the model learning process. A comprehensive performance study is conducted to demonstrate the superior accuracy of AEG over its predecessors, using two emotion annotated music corpora MER60 and MTurk. Our results show that the AEG model outperforms the state-of-the-art methods in automatic music emotion annotation. Moreover, for the first time a quantitative evaluation of emotion-based music retrieval is reported.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {89–98},
numpages = {10},
keywords = {gaussian mixture model, computational emotion model, music retrieval, em algorithm, automatic music emotion recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393368,
author = {Wang, Xinxi and Rosenblum, David and Wang, Ye},
title = {Context-Aware Mobile Music Recommendation for Daily Activities},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393368},
doi = {10.1145/2393347.2393368},
abstract = {Existing music recommendation systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we present in this paper a novel approach to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. We present a probabilistic model to integrate contextual information with music content analysis to offer music recommendation for daily activities, and we present a prototype implementation of the model. Finally, we present evaluation results demonstrating good accuracy and usability of the model and prototype.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {99–108},
numpages = {10},
keywords = {music recommendation, mobile computing, sensors, context awareness, activity classification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393369,
author = {Liu, Zimu and Feng, Yuan and Li, Baochun},
title = {MusicScore: Mobile Music Composition for Practice and Fun},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393369},
doi = {10.1145/2393347.2393369},
abstract = {In this paper, we present our design of MusicScore, a professional grade application on the iOS platform for music composition and live performance tracking, used by composers and amateurs alike. As its foundation, we have designed and implemented a high-quality music engraver, capable of real-time interactive rendering on mobile devices, as well as an intuitive user interface based on multi-touch, both built from scratch using Objective-C and Cocoa Touch. To make MusicScore appealing to the general population for their practice and fun, we have introduced a unique auditory capability to MusicScore, so that it can "listen" to and analyze live instrument performance in real time. In order to compensate for the imperfect audio sensing system on mobile devices, we have proposed a collaborative sensing solution to better capture music signals in real time. To maximize the accuracy of live progress tracking and performance evaluation using a mobile device, we have designed a collection of note detection and tempo-based note matching algorithms, using a combination of microphone and accelerometer sensors. Based on our real-world implementation of MusicScore, extensive evaluation results show that MusicScore can achieve acceptably low error ratios, even for music pieces performed by highly inexperienced players.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {109–118},
numpages = {10},
keywords = {music composition, signal processing, mobile application},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393370,
author = {Chen, Chien-nan and Chu, Cing-yu and Yeh, Su-ling and Chu, Hao-hua and Huang, Polly},
title = {Modeling the Qoe of Rate Changes in SKYPE/SILK VoIP Calls},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393370},
doi = {10.1145/2393347.2393370},
abstract = {The effective end-to-end transport of delay-sensitive voice data has long been a problem in multimedia networking. One of the major issues is determining the sending rate of real-time VoIP streams such that the user experience is maximized per unit network resource consumed. A particularly interesting complication that remains to be addressed is that the available bandwidth is often dynamic. Thus, it is unclear whether a marginal increase warrants better user experience. If a user naively tunes the sending rate to the optimum at any given opportunity, the user experience could fluctuate.To investigate the effects of magnitude and frequency of rate changes on user experience, we recruited 127 human participants to systematically score emulated Skype calls with different combinations of rate changes, including varying magnitude and frequency of rate changes. Results show that 1) the rate change frequency affects the user experience on a logarithmic scale, echoing Weber-Fechner's Law [1], 2) the effect of rate change magnitude depends on how users perceive the quality difference, and 3) this study derives a closed-form model of user perception for rate changes for Skype calls.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {119–128},
numpages = {10},
keywords = {VoIP, rate adaptation, user perception, psychophysics, QoE},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246396,
author = {Hauptmann, Alexander G.},
title = {Session Details: Full Paper Session 3: Video Applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246396},
doi = {10.1145/3246396},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393372,
author = {Lissermann, Roman and Olberding, Simon and Petry, Benjamin and M\"{u}hlh\"{a}user, Max and Steimle, J\"{u}rgen},
title = {PaperVideo: Interacting with Videos on Multiple Paper-like Displays},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393372},
doi = {10.1145/2393347.2393372},
abstract = {Sifting and sense-making of video collections are important tasks in many professions. In contrast to sense-making of paper documents, where physical structuring of many documents has proven to be key to effective work, interaction with video is still restricted to the traditional "one video at a time" paradigm. This paper investigates how interaction with video can benefit from paper-like displays that allow for working with multiple videos simultaneously in physical space. We present a corresponding approach and system called PaperVideo, including novel interaction concepts for both video and audio. These include spatial techniques for temporal navigation, arranging, grouping and linking of videos, as well as for managing video contents and simultaneous audio playback on multiple displays. An evaluation with users provides insights into how paper-based navigation with videos improves active video work.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {129–138},
numpages = {10},
keywords = {electronic paper, thinfilm display, tangible user interface, pile, video, flexible display, multiple displays},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393373,
author = {Saini, Mukesh Kumar and Gadde, Raghudeep and Yan, Shuicheng and Ooi, Wei Tsang},
title = {MoViMash: Online Mobile Video Mashup},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393373},
doi = {10.1145/2393347.2393373},
abstract = {With the proliferation of mobile video cameras, it is becoming easier for users to capture videos of live performances and socially share them with friends and public. As an attendee of such live performances typically has limited mobility, each video camera is able to capture only from a range of restricted viewing angles and distance, producing a rather monotonous video clip. At such performances, however, multiple video clips can be captured by different users, likely from different angles and distances. These videos can be combined to produce a more interesting and representative mashup of the live performances for broadcasting and sharing. The earlier works select video shots merely based on the quality of currently available videos. In real video editing process, however, recent selection history plays an important role in choosing future shots. In this work, we present MoViMash, a framework for automatic online video mashup that makes smooth shot transitions to cover the performance from diverse perspectives. Shot transition and shot length distributions are learned from professionally edited videos. Further, we introduce view quality assessment in the framework to filter out shaky, occluded, and tilted videos. To the best of our knowledge, this is the first attempt to incorporate history-based diversity measurement, state-based video editing rules, and view quality in automated video mashup generations. Experimental results have been provided to demonstrate the effectiveness of MoViMash framework.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {139–148},
numpages = {10},
keywords = {mobile video, virtual director, video mashup},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393374,
author = {Zhang, Zhebin and Zhou, Chen and Xin, Bo and Wang, Yizhou and Gao, Wen},
title = {An Interactive System of Stereoscopic Video Conversion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393374},
doi = {10.1145/2393347.2393374},
abstract = {With the recent booming of 3DTV industry, more and more stereoscopic videos are demanded by the market. This paper presents a system of converting conventional monocular videos to stereoscopic ones. In this system, an input video is firstly segmented into shots to reduce operations on similar frames. Then, automatic depth estimation and interactive image segmentation are integrated to obtain depth maps and foreground/background segments on selected key frames. Within each video shot, such results are propagated from key frames to non-key frames. Combined with a depth-to-disparity conversion method, the system synthesizes the counterpart (either left or right) view for stereoscopic display by warping the original frame according to disparity maps. For evaluation, we use human labeled depth map as the reference and compute both the mean opinion score (MOS) and Peak signal-to-noise ratio (PSNR) to valuate the converted video quality. Experiment results demonstrate that the proposed conversion system and methods achieves encouraging performance.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {149–158},
numpages = {10},
keywords = {3dtv, foreground segmentation, 2d-to-3d conversion, depth estimation, depth to disparity},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393375,
author = {Kegel, Ian and Cesar, Pablo and Jansen, Jack and Bulterman, Dick C.A. and Stevens, Tim and Kort, Joke and F\"{a}rber, Nikolaus},
title = {Enabling 'togetherness' in High-Quality Domestic Video},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393375},
doi = {10.1145/2393347.2393375},
abstract = {Low-cost video conferencing systems have provided an existence proof for the value of video communication in a home setting. At the same time, current systems have a number of fundamental limitations that inhibit more general social interactions among multiple groups of participants. In our work, we describe the development, implementation and evaluation of a domestic video conferencing system that is geared to providing true 'togetherness' among conference participants. We show that such interactions require sophisticated support for high-quality audiovisual presentation, and processing support for person identification and localisation. In this paper, we describe user requirements for effective interpersonal interaction. We then report on a system that implements these requirements. We conclude with a systems and user evaluation of this work. We present results that show that participants in a video conference can be made feel as 'together' as collocated players of a board game.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {159–168},
numpages = {10},
keywords = {interaction, togetherness, social communication, video conferencing, visual composition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246397,
author = {Ngo, Chong-Wah},
title = {Session Details: Full Paper Session 4: Large Scale Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246397},
doi = {10.1145/3246397},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393377,
author = {Zhou, Wengang and Lu, Yijuan and Li, Houqiang and Tian, Qi},
title = {Scalar Quantization for Large Scale Image Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393377},
doi = {10.1145/2393347.2393377},
abstract = {Bag-of-Words (BoW) model based on SIFT has been widely used in large scale image retrieval applications. Feature quantization plays a crucial role in BoW model, which generates visual words from the high dimensional SIFT features, so as to adapt to the inverted file structure for indexing. Traditional feature quantization approaches suffer several problems: 1) high computational cost---visual words generation (codebook construction) is time consuming especially with large amount of features; 2) limited reliability---different collections of images may produce totally different codebooks and quantization error is hard to be controlled; 3) update inefficiency--once the codebook is constructed, it is not easy to be updated. In this paper, a novel feature quantization algorithm, scalar quantization, is proposed. With scalar quantization, a SIFT feature is quantized to a descriptive and discriminative bit-vector, of which the first tens of bits are taken out as code word. Our quantizer is independent of collections of images. In addition, the result of scalar quantization naturally lends itself to adapt to the classic inverted file structure for image indexing. Moreover, the quantization error can be flexibly reduced and controlled by efficiently enumerating nearest neighbors of code words.The performance of scalar quantization has been evaluated in partial-duplicate Web image search on a database of one million images. Experiments reveal that the proposed scalar quantization achieves a relatively 42% improvement in mean average precision over the baseline (hierarchical visual vocabulary tree approach), and also outperforms the state-of-the-art Hamming Embedding approach and soft assignment method.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {169–178},
numpages = {10},
keywords = {large-scale image retrieval, sift, scalar quantization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393378,
author = {Wang, Jingdong and Li, Shipeng},
title = {Query-Driven Iterated Neighborhood Graph Search for Large Scale Indexing},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393378},
doi = {10.1145/2393347.2393378},
abstract = {In this paper, we address the approximate nearest neighbor (ANN) search problem over large scale visual descriptors. We investigate a simple but very effective approach, neighborhood graph search, which constructs a neighborhood graph to index the data points and conducts a local search, expanding neighborhoods with a best-first manner, for ANN search. Our empirical analysis shows that neighborhood expansion is very efficient, with O(1) cost, for a new NN candidate location, and has high chances to locate true NNs and hence it usually performs well. However, it often gets sub-optimal solutions since local search only checks the neighborhood of the current solution, or conducts exhaustive and continuous neighborhood expansions to find better solutions, which deteriorates the query efficiency.In this paper, we propose a query-driven iterated neighborhood graph search approach to improve the performance. We follow the iterated local search (ILS) strategy, widely-used in combinatorial optimization, to find a solution beyond a local optimum. We handle the key challenge in making neighborhood graph search adapt to ILS, Perturbation, which generates a new pivot to restart a local search. To this end, we present a criterion to check if the local search over a neighborhood graph arrives at the local solution. Moreover, we exploit the query and search history to design the perturbation scheme, resulting in a more effective search. The major benefit is avoiding unnecessary neighborhood expansions and hence more efficiently finding true NNs. Experimental results on large scale SIFT matching, similar image search, and shape retrieval with non-metric distance measures, show that our approach performs much better than previous state-of-the-art ANN search approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {179–188},
numpages = {10},
keywords = {local neighborhood graph search, ann search, iterated, query-driven},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393379,
author = {Tolias, Giorgos and Kalantidis, Yannis and Avrithis, Yannis},
title = {SymCity: Feature Selection by Symmetry for Large Scale Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393379},
doi = {10.1145/2393347.2393379},
abstract = {Many problems, including feature selection, vocabulary learning, location and landmark recognition, structure from motion and 3d reconstruction, rely on a learning process that involves wide-baseline matching on multiple views of the same object or scene. In practical large scale image retrieval applications however, most images depict unique views where this idea does not apply. We exploit self-similarities, symmetries and repeating patterns to select features within a single image. We achieve the same performance compared to the full feature set with only a small fraction of its index size on a dataset of unique views of buildings or urban scenes, in the presence of one million distractors of similar nature. Our best solution is linear in the number of correspondences, with practical running times of just a few milliseconds.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {189–198},
numpages = {10},
keywords = {symmetry detection, indexing, feature selection, self-similarity, image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393380,
author = {Liu, Zhen and Li, Houqiang and Zhou, Wengang and Tian, Qi},
title = {Embedding Spatial Context Information into Inverted Filefor Large-Scale Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393380},
doi = {10.1145/2393347.2393380},
abstract = {One most popular approach for large-scale content-based image retrieval is based on the Bag-of-Visual-Words model. Since the spatial context among local features is very important for visual content identification, many approaches index local features' geometric clues, such as location, scale and orientation for post-verification. To obtain consistent accuracy performance, the amount of top ranked images that post-verification approach needs to process is proportional to the image database size. When the database is very large, the verified images will be too many to be processed in real-time response. To address this issue, in this paper, we explore two approaches to embed spatial context information into the inverted file. The first one is to build a spatial relationship dictionary embedded with spatial context among local features, which we call one-one spatial relationship method. The second one is to generate a spatial context binary signature for each feature, which we call one-multiple spatial relationship method. Then we build an inverted file with spatial information between local features. The geometric verification is implicitly achieved while traversing the inverted file. Experimental results on benchmark Holidays dataset demonstrate the efficiency of the proposed algorithm.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {199–208},
numpages = {10},
keywords = {sift, large-scale image retrieval, spatial relationship dictionary, one-multiple relationship, spatial context binary signature, feature tuple, spatial context, one-one relationship},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246398,
author = {Sebe, Nicu},
title = {Session Details: Full Paper Session 5: Person and Face Analysis},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246398},
doi = {10.1145/3246398},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393382,
author = {Dibeklio\u{g}lu, Hamdi and Gevers, Theo and Salah, Albert Ali and Valenti, Roberto},
title = {A Smile Can Reveal Your Age: Enabling Facial Dynamics in Age Estimation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393382},
doi = {10.1145/2393347.2393382},
abstract = {Estimation of a person's age from the facial image has many applications, ranging from biometrics and access control to cosmetics and entertainment. Many image-based methods have been proposed for this problem. In this paper, we propose a method for the use of dynamic features in age estimation, and show that 1) the temporal dynamics of facial features can be used to improve image-based age estimation; 2) considered alone, static image-based features are more accurate than dynamic features. We have collected and annotated an extensive database of face videos from 400 subjects with an age range between 8 and 76, which allows us to extensively analyze the relevant aspects of the problem. The proposed system, which fuses facial appearance and expression dynamics, performs with a mean absolute error of 4.81 (4.87) years. This represents a significant improvement of accuracy in comparison to the sole use of appearance-based features.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {209–218},
numpages = {10},
keywords = {facial dynamics, age estimation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393383,
author = {Bu, Jiajun and Xu, Bin and Wu, Chenxia and Chen, Chun and Zhu, Jianke and Cai, Deng and He, Xiaofei},
title = {Unsupervised Face-Name Association via Commute Distance},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393383},
doi = {10.1145/2393347.2393383},
abstract = {Recently, the task of unsupervised face-name association has received a considerable interests in multimedia and information retrieval communities. It is quite different with the generic facial image annotation problem because of its unsupervised and ambiguous assignment properties. Specifically, the task of face-name association should obey the following three constraints: (1) a face can only be assigned to a name appearing in its associated caption or to null; (2) a name can be assigned to at most one face; and (3) a face can be assigned to at most one name. Many conventional methods have been proposed to tackle this task while suffering from some common problems, eg, many of them are computational expensive and hard to make the null assignment decision. In this paper, we design a novel framework named face-name association via commute distance (FACD), which judges face-name and face-null assignments under a unified framework via commute distance (CD) algorithm. Then, to further speed up the on-line processing, we propose a novel anchor-based commute distance (ACD) algorithm whose main idea is using the anchor point representation structure to accelerate the eigen-decomposition of the adjacency matrix of a graph. Systematic experiment results on a large scale and real world image-caption database with a total of 194,046 detected faces and 244,725 names show that our proposed approach outperforms many state-of-the-art methods in performance. Our framework is appropriate for a large scale and real-time system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {219–228},
numpages = {10},
keywords = {commute distance, unsupervised, face-name association},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393384,
author = {Lu, Xin and Suryanarayan, Poonam and Adams, Reginald B. and Li, Jia and Newman, Michelle G. and Wang, James Z.},
title = {On Shape and the Computability of Emotions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393384},
doi = {10.1145/2393347.2393384},
abstract = {We investigated how shape features in natural images influence emotions aroused in human beings. Shapes and their characteristics such as roundness, angularity, simplicity, and complexity have been postulated to affect the emotional responses of human beings in the field of visual arts and psychology. However, no prior research has modeled the dimensionality of emotions aroused by roundness and angularity. Our contributions include an in depth statistical analysis to understand the relationship between shapes and emotions. Through experimental results on the International Affective Picture System (IAPS) dataset we provide evidence for the significance of roundness-angularity and simplicity-complexity on predicting emotional content in images. We combine our shape features with other state-of-the-art features to show a gain in prediction and classification accuracy. We model emotions from a dimensional perspective in order to predict valence and arousal ratings which have advantages over modeling the traditional discrete emotional categories. Finally, we distinguish images with strong emotional content from emotionally neutral images with high accuracy.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {229–238},
numpages = {10},
keywords = {shape features, psychology, human emotion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393385,
author = {Nguyen, Tam V. and Liu, Si and Ni, Bingbing and Tan, Jun and Rui, Yong and Yan, Shuicheng},
title = {Sense Beauty via Face, Dressing, and/or Voice},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393385},
doi = {10.1145/2393347.2393385},
abstract = {Discovering the secret of beauty has been the pursuit of artists and philosophers for centuries. Nowadays, the computational model for beauty estimation has been actively explored in computer science community, yet with the focus mainly on facial features. In this work, we perform a comprehensive study of female attractiveness conveyed by single/multiple modalities of cues, i.e., face, dressing and/or voice, and aim to uncover how different modalities individually and collectively affect the human sense of beauty. To this end, we collect the first Multi-Modality Beauty (M2B) dataset in the world for female attractiveness study, which is thoroughly annotated with attractiveness levels converted from manual k-wise ratings and semantic attributes of different modalities. A novel Dual-supervised Feature-Attribute-Task (DFAT) network is proposed to jointly learn the beauty estimation models of single/multiple modalities as well as the attribute estimation models. The DFAT network differentiates itself by its supervision in both attribute and task layers. Several interesting beauty-sense observations over single/multiple modalities are reported, and the extensive experimental evaluations on the collected M2B dataset well demonstrate the effectiveness of the proposed DFAT network for female attractiveness estimation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {239–248},
numpages = {10},
keywords = {dressing, attributes, dual-supervised feature-attribute-task network, {face, voice} attractiveness},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246399,
author = {Venkatasubrahanian, Nalini},
title = {Session Details: Full Paper Session 6: Video Distribution},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246399},
doi = {10.1145/3246399},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393387,
author = {Shen, Haiying and Li, Ze and Wang, Hailang and Li, Jin},
title = {Leveraging Social Network Concepts for Efficient Peer-to-Peer Live Streaming Systems},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393387},
doi = {10.1145/2393347.2393387},
abstract = {In current peer-to-peer (P2P) live streaming systems, nodes in a channel form a P2P overlay for video sharing. To watch a new channel, a node depends on the centralized server to join in the overlay of the channel. The increase in the number of channels in today's live streaming applications triggers users' desire of watching multiple channels successively or simultaneously. However, the support of such watching modes in current applications is no better than joining in different channel overlays successively or simultaneously, which if widely used, poses heavy burden on the centralized server. In order to achieve higher efficiency and scalability, we propose a Social network-Aided efficient liVe strEaming system (SAVE). SAVE regards users' channel switching or multi-channel watching as interactions between channels. By collecting the information of channel interactions and nodes' interests and watching times, SAVE forms nodes in multiple channels with frequent interactions into an overlay, constructs bridges between overlays of channels with less frequent interactions, and enables nodes to identify friends sharing similar interests and watching times. Thus, a node can connect to a new channel while staying in its current overlay, using bridges or relying on its friends, reducing the need to contact the centralized server. Extensive experimental results from the PeerSim simulator and PlanetLab verify that SAVE outperforms other popular protocols in system efficiency and server load reduction.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {249–258},
numpages = {10},
keywords = {P2P live streaming, P2P networks, social networks},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393388,
author = {Feng, Yuan and Li, Baochun and Li, Bo},
title = {Jetway: Minimizing Costs on Inter-Datacenter Video Traffic},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393388},
doi = {10.1145/2393347.2393388},
abstract = {It is typical for video streaming service providers (such as NetFlix) to rely on services from cloud providers (such as Amazon), in order to build a scalable video streaming platform with high availability. The trend is largely driven by the fact that cloud providers deploy a number of datacenters inter-connected by high-capacity links, spanning different geographical regions. Video traffic across datacenters, such as video replication and transit server-to-customer video serving, constitutes a large portion of a cloud provider's inter-datacenter traffic. Charged by ISPs, such inter-datacenter video traffic incurs substantial operational costs to a cloud provider. In this paper, we argue that costs incurred by such inter-datacenter video traffic can be reduced or even minimized by carefully choosing paths, and by assigning flow rates on each inter-datacenter link along every path. We present Jetway, a new set of algorithms designed to minimize cloud providers' operational costs on inter-datacenter video traffic, by optimally routing video flows in an online fashion. Algorithms in Jetway are designed by following a methodical approach based on an in-depth theoretical analysis. As a highlight of this paper, we have built a real-world system framework to implement and deploy Jetway in the Amazon EC2 datacenters. With both simulations and real-world experiments using our implementation, we show that Jetway effectively helps transmitting videos across datacenters with reduced costs to cloud providers and satisfactory real-world performance.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {259–268},
numpages = {10},
keywords = {flow optimization, inter-datacenter traffic},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393389,
author = {Changuel, Nesrine and Sayadi, Bessem and Kieffer, Michel},
title = {Control of Distributed Servers for Quality-Fair Delivery of Multiple Video Streams},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393389},
doi = {10.1145/2393347.2393389},
abstract = {This paper proposes a quality-fair video delivery system able to transmit several encoded video streams to mobile users sharing some wireless resource. Video quality fairness, as well as similar delivery delay is targeted among streams. The proposed control system is implemented within some aggregator located near the bottleneck of the network.This is done by allocating the transmission rate among streams based on the quality of the already encoded and buffered packets in the aggregator. Encoding rate targets are evaluated by the aggregator and fed back to each remote video server, or directly evaluated by each server in a distributed way. Each encoding rate target is adjusted for each stream independently based on the corresponding buffering delay in the aggregator.The transmission and encoding rate control problems are addressed with a control-theoretic perspective. The system is described with a multi-input multi-output model and several Proportional Integral (PI) controllers are used to adjust the video quality as well as the buffering delay. The study of the system equilibrium and stability provides guidelines for choosing the parameters of the PI controllers. Experimental results show that better quality fairness is obtained compared to classical transmission rate fair streaming solutions while keeping similar buffering delays.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {269–278},
numpages = {10},
keywords = {distributed systems, video, control design},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393390,
author = {Li, Xin and Dong, Mian and Ma, Zhan and Fernandes, Felix C.A.},
title = {GreenTube: Power Optimization for Mobile Videostreaming via Dynamic Cache Management},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393390},
doi = {10.1145/2393347.2393390},
abstract = {Mobile video streaming has become one of the most popular applications in the trend of smartphone booming and the prevalence of 3G/4G networks, i.e., HSPA, HSPA+, and LTE. However, the prohibitively high power consumption by 3G/4G radios in smartphones reduces battery life significantly and thus severely hurts user experience. To tackle this challenge, we designed GreenTube, a system that optimizes power consumption for mobile video streaming by judiciously scheduling downloading activities to minimize unnecessary active periods of 3G/4G radio. GreenTube achieves this by dynamically managing the downloading cache based on user viewing history and network condition. We implemented GreenTube on Android-based smartphones. Experimental results show that GreenTube achieves large power reductions of more than 70% (on the 3G/4G radio) and 40% (for the whole system). We believe GreenTube is a desirable upgrade to the Android system, especially in the light of increasing LTE popularity.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {279–288},
numpages = {10},
keywords = {mobile video streaming, power optimization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246400,
author = {Tian, Qi},
title = {Session Details: Full Paper Session 7: Visual Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246400},
doi = {10.1145/3246400},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393392,
author = {Worring, Marcel and Engl, Andreas and Smeria, Camelia},
title = {A Multimedia Analytics Framework for Browsing Image Collections in Digital Forensics},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393392},
doi = {10.1145/2393347.2393392},
abstract = {Searching through large collections of images to find patterns of use or to find sets of relevant items is difficult, especially when the information to consider is not only the content of the images itself, but also the associated metadata. Multimedia analytics is a new approach to such problems. We consider the case of forensic experts facing image collections of growing size during digital forensic investigations. We answer the forensic challenge by developing specialised novel interactive visualisations which employ content-based image clusters in both the analysis as well as in all visualizations. Their synergy makes the task of manually browsing these collections more effective and efficient. Evaluation of such multimedia analytics is a notoriously hard problem as there are so many factors influencing the result. As a controlled evaluation, we developed a user simulation framework to create image collections with time and directory information as metadata. We apply it in a number of scenarios to illustrate its use. The simulation tool is available to other researchers via our website.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {289–298},
numpages = {10},
keywords = {visual analytics, information visualization, image search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393393,
author = {Cao, Liangliang and Li, Zhenguo and Mu, Yadong and Chang, Shih-Fu},
title = {Submodular Video Hashing: A Unified Framework towards Video Pooling and Indexing},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393393},
doi = {10.1145/2393347.2393393},
abstract = {This paper develops a novel framework for efficient large-scale video retrieval. We aim to find video according to higher level similarities, which is beyond the scope of traditional near duplicate search. Following the popular hashing technique we employ compact binary codes to facilitate nearest neighbor search. Unlike the previous methods which capitalize on only one type of hash code for retrieval, this paper combines heterogeneous hash codes to effectively describe the diverse and multi-scale visual contents in videos. Our method integrates feature pooling and hashing in a single framework. In the pooling stage, we cast video frames into a set of pre-specified components, which capture a variety of semantics of video contents. In the hashing stage, we represent each video component as a compact hash code, and combine multiple hash codes into hash tables for effective search. To speed up the retrieval while retaining most informative codes, we propose a graph-based influence maximization method to bridge the pooling and hashing stages. We show that the influence maximization problem is submodular, which allows a greedy optimization method to achieve a nearly optimal solution. Our method works very efficiently, retrieving thousands of video clips from TRECVID dataset in about 0.001 second. For a larger scale synthetic dataset with 1M samples, it uses less than 1 second in response to 100 queries. Our method is extensively evaluated in both unsupervised and supervised scenarios, and the results on TRECVID Multimedia Event Detection and Columbia Consumer Video datasets demonstrate the success of our proposed technique.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {299–308},
numpages = {10},
keywords = {submodular, video hashing, indexing, multiple feature hashing, feature pooling},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393394,
author = {Castanon, Gregory D. and Caron, Andre Louis and Saligrama, Venkatesh and Jodoin, Pierre-marc},
title = {Exploratory Search of Long Surveillance Videos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393394},
doi = {10.1145/2393347.2393394},
abstract = {We present a fast and flexible content-based retrieval method for surveillance video. Designing a video search robust to uncertain activity duration, high variability in object shapes and scene content is challenging. We propose a two-step approach to video search. First, local features are inserted into an inverted index using locality-sensitive hashing (LSH). Second, we utilize a novel dynamic programming (DP) approach to robustify against temporal distortion, limited obscuration and imperfect queries. DP exploits causality to assemble the local features stored in the index into a video segment which matches the query video. Pre-processing of archival video is performed in real-time, and retrieval speed scales as a function of the number of matches rather than video length. We derive bounds on the rate of false positives, demonstrate the effectiveness of the approach for counting, motion pattern recognition and abandoned object applications using seven challenging video datasets and compare with existing work.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {309–318},
numpages = {10},
keywords = {surveillance, dynamic programming, exploratory search, video search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393395,
author = {Kofler, Christoph and Yang, Linjun and Larson, Martha and Mei, Tao and Hanjalic, Alan and Li, Shipeng},
title = {When Video Search Goes Wrong: Predicting Query Failure Using Search Engine Logs and Visual Search Results},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393395},
doi = {10.1145/2393347.2393395},
abstract = {The recent increase in the volume and variety of video content available online presents growing challenges for video search. Users face increased difficulty in formulating effective queries and search engines must deploy highly effective algorithms to provide relevant results. Although lately much effort has been invested in optimizing video search engine results, relatively little attention has been given to predicting for which queries results optimization is most useful, i.e., predicting which queries will fail. Being able to predict when a video search query would fail is likely to make the video search result optimization more efficient and effective, improve the search experience for the user by providing support in the query formulation process and in this way boost the development of video search engines in general. While insight about a query's performance in general could be obtained using the well-known concept of query performance prediction (QPP), we propose a novel approach for predicting a failure of a video search query in the specific context of a search session. Our 'context-aware query failure' prediction approach uses a combination of 'user indicators' and 'engine indicators' to predict whether a particular query is likely to fail in the context of a particular search session. User indicators are derived from the search log and capture the patterns of query (re)formulation behavior and the click-through data of a user during a typical video search session. Engine indicators are derived from the video search results list and capture the visual variance of search results that would be offered to the user for the given query. We validate our approach experimentally on a test set containing 1+ million video search queries and show its effectiveness compared to a set of conventional QPP baselines. Our approach achieves a 13% relative improvement over the baseline.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {319–328},
numpages = {10},
keywords = {video search, query failure, query performance prediction, visual relatedness, transaction log analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246401,
author = {Nack, Frank},
title = {Session Details: Full Paper Session 8: Human-Centric Media},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246401},
doi = {10.1145/3246401},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393397,
author = {Srivastava, Ruchir and Feng, Jiashi and Roy, Sujoy and Yan, Shuicheng and Sim, Terence},
title = {Don't Ask Me What i'm like, Just Watch and Listen},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393397},
doi = {10.1145/2393347.2393397},
abstract = {Traditional (based on psychology) approaches for personality assessment of an individual require him/her to fill up a questionnaire. This paper presents a novel way of utilizing multimodal cues to automatically fill up the questionnaire. The contributions of this work are three-fold. (1) Novel psychology-based audio/visual/lexical features are proposed and shown to be effective in predicting answers to a personality questionnaire, Big-Five Inventory-10 (BFI- 10). (2) Extracted features are used to learn linear and kernel versions of a novel regression model, 'SLoT', to automatically predict BFI-10 answers. The model is based on Sparse and Low-rank Transformation (SLoT). (3) Predicted answers are used to compute personality scores using standard BFI-10 scoring scheme. We evaluated our approach on a dataset of 3907 clips (for 50 characters from movies of diverse genres) manually labeled with BFI-10 answers and personality scores as ground-truth. Experiments indicate that the proposed 'SLoT' model effectively automates the answering process by emulating human understanding. We also conclude that predicting personality scores through predicting answers first is better than directly predicting scores based on audio/visual features (as studied in state-of-the art methods).},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {329–338},
numpages = {10},
keywords = {emotion recognition, personality assessment, multimodal features, movie analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393398,
author = {Poulsen, Esben Skouboe and Andersen, Hans J\o{}rgen and Jensen, Ole B. and Gade, Rikke and Thyrrestrup, Tobias and Moeslund, Thomas B.},
title = {Controlling Urban Lighting by Human Motion Patterns Results from a Full Scale Experiment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393398},
doi = {10.1145/2393347.2393398},
abstract = {This paper presents a full-scale experiment investigating the use of human motion intensities as input for interactive illumination of a town square in the city of Aalborg in Denmark. As illuminators sixteen 3.5 meter high RGB LED lamps were used. The activity on the square was monitored by three thermal cameras and analysed by computer vision software from which motion intensity maps and peoples trajectories were estimated and used as input to control the interactive illumination. The paper introduces a 2-layered interactive light strategy addressing ambient and effect illumination criteria totally four light scenarios were designed and tested. The result shows that in general people immersed in the street lighting did not notice that the light changed according to their presence or actions, but people watching from the edge of the square noticed the interaction between the illumination and the immersed persons. The experiment also demonstrated that interactive can give significant power savings. In the current experiment there was a difference of 92% between the most and less energy consuming light scenario.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {339–348},
numpages = {10},
keywords = {urban lighting, responsive light design, urban light design, interactive illumination},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393399,
author = {Yanulevskaya, Victoria and Uijlings, Jasper and Bruni, Elia and Sartori, Andreza and Zamboni, Elisa and Bacci, Francesca and Melcher, David and Sebe, Nicu},
title = {In the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393399},
doi = {10.1145/2393347.2393399},
abstract = {Most artworks are explicitly created to evoke a strong emotional response. During the centuries there were several art movements which employed different techniques to achieve emotional expressions conveyed by artworks. Yet people were always consistently able to read the emotional messages even from the most abstract paintings. Can a machine learn what makes an artwork emotional? In this work, we consider a set of 500 abstract paintings from Museum of Modern and Contemporary Art of Trento and Rovereto (MART), where each painting was scored as carrying a positive or negative response on a Likert scale of 1-7. We employ a state-of-the-art recognition system to learn which statistical patterns are associated with positive and negative emotions. Additionally, we dissect the classification machinery to determine which parts of an image evokes what emotions. This opens new opportunities to research why a specific painting is perceived as emotional. We also demonstrate how quantification of evidence for positive and negative emotions can be used to predict the way in which people observe paintings.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {349–358},
numpages = {10},
keywords = {emotion recognition, visual art, abstract paintings, eye tracking},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393400,
author = {Xu, Qianqian and Huang, Qingming and Yao, Yuan},
title = {Online Crowdsourcing Subjective Image Quality Assessment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393400},
doi = {10.1145/2393347.2393400},
abstract = {Recently, HodgeRank on random graphs has been proposed as an effective framework for multimedia quality assessment problem based on paired comparison method. With the random design on large graphs, it is particularly suitable for large scale crowdsourcing experiments on Internet. However, to make it more practical toward this purpose, it is necessary to develop online algorithms to deal with sequential or streaming data. In this paper, we propose an online rating scheme based on HodgeRank on random graphs, to assess image quality when assessors and image pairs enter the system in a sequential way in a crowdsourceable scenario. The scheme is shown in both theory and experiments to be effective by exhibiting similar performance to batch learning under the Erd\"{o}s-R\'{e}nyi random graph model for sampling. It enables us to derive global rating and monitor intrinsic inconsistency in the real time. We demonstrate the effectiveness of the proposed framework on LIVE and IVC databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {359–368},
numpages = {10},
keywords = {topology evolution, paired comparison, hodgerank, triangular curl, online, crowdsourcing, random graphs, subjective image quality assessment, persistent homology},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246402,
author = {Shen, Heng Tao},
title = {Session Details: Full Paper Session 9: Presentation and Organization},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246402},
doi = {10.1145/3246402},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393402,
author = {Gupta, Raj Kumar and Chia, Alex Yong-Sang and Rajan, Deepu and Ng, Ee Sin and Zhiyong, Huang},
title = {Image Colorization Using Similar Images},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393402},
doi = {10.1145/2393347.2393402},
abstract = {We present a new example-based method to colorize a gray image. As input, the user needs only to supply a reference color image which is semantically similar to the target image. We extract features from these images at the resolution of superpixels, and exploit these features to guide the colorization process. Our use of a superpixel representation speeds up the colorization process. More importantly, it also empowers the colorizations to exhibit a much higher extent of spatial consistency in the colorization as compared to that using independent pixels. We adopt a fast cascade feature matching scheme to automatically find correspondences between superpixels of the reference and target images. Each correspondence is assigned a confidence based on the feature matching costs computed at different steps in the cascade, and high confidence correspondences are used to assign an initial set of chromatic values to the target superpixels. To further enforce the spatial coherence of these initial color assignments, we develop an image space voting framework which draws evidence from neighboring superpixels to identify and to correct invalid color assignments. Experimental results and user study on a broad range of images demonstrate that our method with a fixed set of parameters yields better colorization results as compared to existing methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {369–378},
numpages = {10},
keywords = {cascade feature matching, automatic colorization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393403,
author = {Kuhna, Mikko and Kivel\"{a}, Ida-Maria and Oittinen, Pirkko},
title = {Semi-Automated Magazine Layout Using Content-Based Image Features},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393403},
doi = {10.1145/2393347.2393403},
abstract = {We present a system for automating magazine layout process and data on its performance in a user evaluation test. The purpose of the study is to find the feasibility of the key system variables on the end-user. This semi-automatic system is based on content-based image feature algorithms to automate the layout process. The image related automation includes image cropping, overlaying text on top of images, image color palette creation, and image alignment. The algorithms rely on principles of photography and are used here together in the context of graphic design. For example the rule-of-space and leading line concepts can be extended to layout contexts for improved alignment of text and images. The computation is based on automatic analysis, such as face detection, color saliency, and textureness. We used the automation to create a functional prototype, an iPad magazine, using an open HTML5 eBook framework, which relies on CSS3 media queries for the layout adaptation. User experiments (40 participants) were conducted where the system was compared against two commercial iPad magazine systems in terms of overall usability with emphasis on visual aspects. Questionnaires and free commenting was used in a task-based usage scenario. The experiments were video recorded and the user comments were transcribed and coded into attributes. Usability results mixed with qualitative observation are reported. Results show that usability, read- ability and visuality are important to the users. Our system was considered more usable than the other two systems with some of the defining aspects being simplicity in terms of usability, subjective readability and visual clarity.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {379–388},
numpages = {10},
keywords = {user evaluation, layout adaptation, responsive design, image feature, page layout},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393404,
author = {Chandra, Surendar and Biehl, Jacob T. and Boreczky, John and Carter, Scott and Rowe, Lawrence A.},
title = {Understanding Screen Contents for Building a High Performance, Real Time Screen Sharing System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393404},
doi = {10.1145/2393347.2393404},
abstract = {Faithful sharing of screen contents is an important collaboration feature. Prior systems were designed to operate over constrained networks. They performed poorly even without such bottlenecks. To build a high performance screen sharing system, we empirically analyzed screen contents for a variety of scenarios. We showed that screen updates were sporadic with long periods of inactivity. When active, screens were updated at far higher rates than was supported by earlier systems. The mismatch was pronounced for interactive scenarios. Even during active screen updates, the number of updated pixels were frequently small. We showed that crucial information can be lost if individual updates were merged. When the available system resources could not support high capture rates, we showed ways in which updates can be effectively collapsed. We showed that Zlib lossless compression performed poorly for screen updates. By analyzing the screen pixels, we developed a practical transformation that significantly improved compression rates. Our system captured 240 updates per second while only using 4.6 Mbps for interactive scenarios. Still, while playing movies in fullscreen mode, our approach could not achieve higher capture rates than prior systems; the CPU remains the bottleneck. A system that incorporates our findings is deployed within the lab.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {389–398},
numpages = {10},
keywords = {displaycast, screencast, screen capture, screen sharing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393405,
author = {Bhojan, Anand and Chong, Lee Kee and Chang, Ee-Chien and Chan, Mun Choon and Akkihebbal, Ananda L. and Ooi, Wei Tsang},
title = {El-Pincel: A Painter Cloud Service for Greener Web Pages},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393405},
doi = {10.1145/2393347.2393405},
abstract = {Due to their thin size, vivid colors, high contrast and power efficiency, OLED (Organic Light-Emitting Diode) display and its variants such as AMOLED (Active Matrix OLED) displays are increasingly replacing traditional LCD (Liquid Crystal Display) screens in smart phones. However, the power efficiency of OLED screens greatly depends on the luminance and colors of the displayed contents on the screen. Web browsing is one of the most widely used applications in mobile devices. In this paper, we present our cloud service, which intelligently re-paints the web pages in real-time with power efficient colors and HVS (Human Visual System) based tone mapping techniques, without adversely affecting the identity (brand color) of the web pages as well as the user's browsing experience. El-pincel helps to save up to 60% of OLED energy with color combinations that ensure good legibility and pleasing affective response to human eyes.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {399–408},
numpages = {10},
keywords = {color transformation, OLED display, browsing, low power, color harmony, cloud service, tone mapping},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246403,
author = {Nakamura, Yuichi},
title = {Session Details: Full Paper Session 10: Haptics},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246403},
doi = {10.1145/3246403},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393407,
author = {Chaudhari, Rahul and \c{C}izmeci, Burak and Kuchenbecker, Katherine J. and Choi, Seungmoon and Steinbach, Eckehard},
title = {Low Bitrate Source-Filter Model Based Compression of Vibrotactile Texture Signals in Haptic Teleoperation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393407},
doi = {10.1145/2393347.2393407},
abstract = {Vibrotactile signals convey the touch-based characteristics of object surfaces felt through a tool. They particularly enhance the quality of human-machine interactions by providing realistic haptic perception of textures. In this paper, inspired by the similarities observed between vibrotactile texture signals and speech signals, we present a novel vibrotactile texture codec for bilateral teleoperation, based on well-known speech coding techniques. The proposed low bitrate, high quality codec preserves not only the spectral signature vital to the general feel of the texture, but also important temporal features of the texture signal. We report a compression ratio of 8:1 (12.5 %) with a constant output bitrate of 4 kbps, and we validate the perceptual transparency of the codec via rigorous subjective tests and analyses.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {409–418},
numpages = {10},
keywords = {teleoperation, haptic textures, data compression},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393408,
author = {McDaniel, Troy L. and Goldberg, Morris and Bala, Shantanu and Fakhri, Bijan and Panchanathan, Sethuraman},
title = {Vibrotactile Feedback of Motor Performance Errors for Enhancing Motor Learning},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393408},
doi = {10.1145/2393347.2393408},
abstract = {Feedback related to motor performance is integral to improving the control, timing and coordination of movements. However, motor learning traditionally occurs within a group setting, limiting the quality of instruction and feedback. Even during one-on-one instruction, there are impediments to feedback such as physical separation between trainer and trainee, common in many sports such as snowboarding and swimming. We propose an inexpensive solution for real-time vibrotactile positioning and speed feedback that can complement traditional motor learning, and is compatible with existing vibrotactile motor instructions. We present a psychophysical study that examined participants' initial reactions to feedback stimuli pertaining to position and speed adjustments. Results support the proposed design in terms of both usability and naturalness, and provide insight into participants' conceptualization of feedback signals and feedback for rotational movements.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {419–428},
numpages = {10},
keywords = {vibrotactile feedback, motor learning, instructions},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393409,
author = {Zhou, Yinsheng and Sim, Khe Chai and Tan, Patsy and Wang, Ye},
title = {MOGAT: Mobile Games with Auditory Training for Children with Cochlear Implants},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393409},
doi = {10.1145/2393347.2393409},
abstract = {Cochlear implants have improved the lives of tens of thousands of the hearing impaired by providing sufficient auditory perception for speech, but these devices are far from satisfactory for music perception. Many cochlear implant recipients, especially pre-lingually deafened children, have difficulty recognizing and producing specific pitches. To improve musical auditory habilitation for children post cochlear implantation, we developed MOGAT: MObile Games with Auditory Training. The system includes three musical games built with off-the-shelf mobile devices to train their pitch perception and intonation skills respectively, and a cloud-based web service which allows music therapists to monitor and design individual training for children. The design of the games and web service was informed by a pilot survey (N=60 children). To ensure widespread use with low-cost mobile devices, we minimized the computation load while retaining highly accurate audio analysis. A 6-week user study (N=15 children) showed that the music habilitation with MOGAT was intuitive, enjoyable and motivating. It has improved most children's pitch discrimination and production, and several children's improvement was statistically significant (p&lt;0.05).},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {429–438},
numpages = {10},
keywords = {auditory habilitation, music, game, mobile, children, cochlear implant},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246404,
author = {Scherp, Ansgar},
title = {Session Details: Full Paper Session 11: Event Recognition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246404},
doi = {10.1145/3246404},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393411,
author = {Weng, Ming-Fang and Lin, Yen-Yu and Tang, Nick C. and Liao, Hong-Yuan Mark},
title = {Visual Knowledge Transfer among Multiple Cameras for People Counting with Occlusion Handling},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393411},
doi = {10.1145/2393347.2393411},
abstract = {We present a framework to count the number of people in an environment where multiple cameras with different angles of view are available. We consider the visual cues captured by each camera as a knowledge source, and carry out cross-camera knowledge transfer to alleviate the difficulties of people counting, such as partial occlusions, low-quality images, clutter backgrounds, and so on. Specifically, this work distinguishes itself with the following contributions. First, we overcome the variations of multiple heterogeneous cameras with different perspective settings by matching the same groups of pedestrians taken by these cameras, and present an algorithm for accomplishing cross-camera correspondence. Second, the proposed counting model is composed of a pair of collaborative regressors. While one regressor measures people counts by the features extracted from intra-camera visual evidences, the other recovers the yielded residual by taking the conflicts among inter-camera predictions into account. The two regressors are elegantly coupled, and jointly lead to an accurate counting system. Additionally, we provide a set of manually annotated pedestrian labels on the PETS 2010 videos for performance evaluation. Our approach is comprehensively tested in various settings and compared with competitive baselines. The significant improvement in performance manifests the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {439–448},
numpages = {10},
keywords = {people counting, correspondence estimation, transfer learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393412,
author = {Jiang, Lu and Hauptmann, Alexander G. and Xiang, Guang},
title = {Leveraging High-Level and Low-Level Features for Multimedia Event Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393412},
doi = {10.1145/2393347.2393412},
abstract = {This paper addresses the challenge of Multimedia Event Detection by proposing a novel method for high-level and low-level features fusion based on collective classification. Generally, the method consists of three steps: training a classifier from low-level features; encoding high-level features into graphs; and diffusing the scores on the established graph to obtain the final prediction. The final prediction is derived from multiple graphs each of which corresponds to a high-level feature. The paper investigates two graph construction methods using logarithmic and exponential loss functions, respectively and two collective classification algorithms, i.e. Gibbs sampling and Markov random walk. The theoretical analysis demonstrates that the proposed method converges and is computationally scalable and the empirical analysis on TRECVID 2011 Multimedia Event Detection dataset validates its outstanding performance compared to state-of-the-art methods, with an added benefit of interpretability.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {449–458},
numpages = {10},
keywords = {collective classification, multi-modal integration, feature fusion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393413,
author = {Miller, Chreston and Quek, Francis},
title = {Interactive Data-Driven Discovery of Temporal Behavior Models from Events in Media Streams},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393413},
doi = {10.1145/2393347.2393413},
abstract = {This paper investigates a technique for the discovery of temporal behavior models within multimedia event data. Advancements in both technology and the marketplace present us the opportunity for research in analysis of situated human behavior using video and other sensor data (media streams). By situated analysis, we mean the study of behavior in time as opposed to looking at behavior in the form of aggregated data divorced from how they occur in context. Human and social scientists seek to model behavior captured in media, and these data may be represented in a multi-dimensional event data space derived from media streams. The knowledge of these scientists (experts) is a valuable resource which can be leveraged to search this space. We propose a solution that incorporates the expert in an iteratively, interactive data-driven discovery process to evolve a desired behavior model. We test our solution's accuracy on a multimodal meeting corpus with a progressive three tiered approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {459–468},
numpages = {10},
keywords = {structure model learning, human-machine cooperation, model evolution, event data, temporal behavior models},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393414,
author = {Ma, Zhigang and Yang, Yi and Cai, Yang and Sebe, Nicu and Hauptmann, Alexander G.},
title = {Knowledge Adaptation for Ad Hoc Multimedia Event Detection with Few Exemplars},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393414},
doi = {10.1145/2393347.2393414},
abstract = {Multimedia event detection (MED) has a significant impact on many applications. Though video concept annotation has received much research effort, video event detection remains largely unaddressed. Current research mainly focuses on sports and news event detection or abnormality detection in surveillance videos. Our research on this topic is capable of detecting more complicated and generic events. Moreover, the curse of reality, i.e., precisely labeled multimedia content is scarce, necessitates the study on how to attain respectable detection performance using only limited positive examples. Research addressing these two aforementioned issues is still in its infancy. In light of this, we explore Ad Hoc MED, which aims to detect complicated and generic events by using few positive examples. To the best of our knowledge, our work makes the first attempt on this topic. As the information from these few positive examples is limited, we propose to infer knowledge from other multimedia resources to facilitate event detection. Experiments are performed on real-world multimedia archives consisting of several challenging events. The results show that our approach outperforms several other detection algorithms. Most notably, our algorithm outperforms SVM by 43% and 14% comparatively in Average Precision when using Gaussian and Χ2 kernel respectively.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {469–478},
numpages = {10},
keywords = {structural adaptive regression (sar), multimedia event detection (med), knowledge adaptation, ad hoc med},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246405,
author = {Xie, Lexing},
title = {Session Details: Full Paper Session 12: Semantic Tagging},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246405},
doi = {10.1145/3246405},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393416,
author = {Qi, Zhongang and Yang, Ming and Zhang, Zhongfei (Mark) and Zhang, Zhengyou},
title = {Multi-View Learning from Imperfect Tagging},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393416},
doi = {10.1145/2393347.2393416},
abstract = {In many real-world applications, tagging is imperfect: incomplete, inconsistent, and error-prone. Solutions to this problem will generate societal and technical impacts. In this paper, we investigate this arguably new problem: learning from imperfect tagging. We propose a general and effective learning scheme called the Multi-view Imperfect Tagging Learning (MITL) to this problem. The main idea of MITL lies in extracting the information of the imperfectly tagged training dataset from multiple views to differentiate the data points in the role of classification. Further, a novel discriminative classification method is proposed under the framework of MITL, which explicitly makes use of the given multiple labels simultaneously as an additional feature to deliver a more effective classification performance than the existing literature where one label is considered at a time as the classification target while the rest of the given labels are completely ignored at the same time. The proposed methods can not only complete the incomplete tagging but also denoise the noisy tagging through an inductive learning. We apply the general solution to the problem with a more specific context - imperfect image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods on solving the problem of learning from imperfect tagging in cross-media.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {479–488},
numpages = {10},
keywords = {multi-label space, correction and prediction, imperfect tagging, image annotation completion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393417,
author = {Lindner, Albrecht and Shaji, Appu and Bonnier, Nicolas and S\"{u}sstrunk, Sabine},
title = {Joint Statistical Analysis of Images and Keywords with Applications in Semantic Image Enhancement},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393417},
doi = {10.1145/2393347.2393417},
abstract = {With the advent of social image-sharing communities, millions of images with associated semantic tags are now available online for free and allow us to exploit this abundant data in new ways. We present a fast non-parametric statistical framework designed to analyze a large data corpus of images and semantic tag pairs and find correspondences between image characteristics and semantic concepts. We learn the relevance of different image characteristics for thousands of keywords from one million annotated images. We demonstrate the framework's effectiveness with three different examples of semantic image enhancement: we adapt the gray-level tone-mapping, emphasize semantically relevant colors, and perform a defocus magnification for an image based on its semantic context. The performance of our algorithms is validated with psychophysical experiments.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {489–498},
numpages = {10},
keywords = {semantic image processing, image understanding, statistical analysis, large-scale experimentation, crowd sourcing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393418,
author = {Lu, Zhiwu and Peng, Yuxin},
title = {Image Annotation by Semantic Sparse Recoding of Visual Content},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393418},
doi = {10.1145/2393347.2393418},
abstract = {This paper presents a new semantic sparse recoding method to generate more descriptive and robust representation of visual content for image annotation. Although the visual bag-of-words (BOW) representation has been reported to achieve promising results in image annotation, its visual codebook is completely learnt from low-level visual features using quantization techniques and thus the so-called semantic gap remains unbridgeable. To handle such challenging issue, we utilize both the annotations of training images and the predicted annotations of test images to improve the original visual BOW representation. This is further formulated as a sparse coding problem so that the noise issue induced by the inaccurate quantization of visual features can also be handled to some extent. By developing an efficient sparse coding algorithm, we successfully generate a new visual BOW representation for image annotation. Since such sparse coding has actually incorporated the high-level semantic information into the original visual codebook, we thus consider it as semantic sparse recoding of the visual content. Although the predicted annotations of test images are also used as inputs by the traditional image annotation refinement, we focus on the visual BOW representation refinement for image annotation in this paper. The experimental results on two benchmark datasets show the superior performance of our semantic sparse recoding method in image annotation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {499–508},
numpages = {10},
keywords = {semantic gap, semantic sparse recoding, visual bow representation, image annotation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393419,
author = {Wu, Fei and Yuan, Ying and Rui, Yong and Yan, Shuicheng and Zhuang, Yueting},
title = {Annotating Web Images Using NOVA: NOn-ConVex Group SpArsity},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393419},
doi = {10.1145/2393347.2393419},
abstract = {As image feature vector is large, selecting the right features plays a fundamental role in Web image annotation. Most existing approaches are either based on individual feature selection, which leads to local optima, or using a convex penalty, which leads to inconsistency. To address these difficulties, in this paper we propose a new sparsity-based approach NOVA (NOn-conVex group spArsity). To the best of our knowledge, NOVA is the first to introduce non-convex penalty for group selection in high-dimensional heterogeneous features space. Because it is a group-sparsity approach, it approximately reaches global optima. Because it uses non-convex penalty, it achieves the consistency. We demonstrate the superior performance of NOVA via three means. First, we present theoretical proof that NOVA is consistent, satisfying un-biasness, sparsity and continuity. Second, we show NOVA converges to the true underlying model by using a ground-truth-available generative-model simulation. Third, we report extensive experimental results on three diverse and widely-used data sets Kodak, MSRA-MM 2.0, and NUS-WIDE. We also compare NOVA against the state-of-the-art approaches, and report superior experimental results.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {509–518},
numpages = {10},
keywords = {group feature selection, non-convex group sparsity, consistent, image annotation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246406,
author = {Yan, Shuicheng},
title = {Session Details: Full Paper Session 13: Image Analysis},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246406},
doi = {10.1145/3246406},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393421,
author = {Sun, Zhenbang and Wang, Changhu and Zhang, Liqing and Zhang, Lei},
title = {Query-Adaptive Shape Topic Mining for Hand-Drawn Sketch Recognition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393421},
doi = {10.1145/2393347.2393421},
abstract = {In this work, we study the problem of hand-drawn sketch recognition. Due to large intra-class variations presented in hand-drawn sketches, most of existing work was limited to a particular domain or limited pre-defined classes. Different from existing work, we target at developing a general sketch recognition system, to recognize any semantically meaningful object that a child can recognize. To increase the recognition coverage, a web-scale clipart image collection is leveraged as the knowledge base of the recognition system. To alleviate the problems of intra-class shape variation and inter-class shape ambiguity in this unconstrained situation, a query-adaptive shape topic model is proposed to mine object topics and shape topics related to the sketch, in which, multiple layers of information such as sketch, object, shape, image, and semantic labels are modeled in a generative process. Besides sketch recognition, the proposed topic model can also be used for related applications such as sketch tagging, image tagging, and sketch-based image search. Extensive experiments on different applications show the effectiveness of the proposed topic model and the recognition system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {519–528},
numpages = {10},
keywords = {query-adaptive shape topic model, sketch recognition, sketch-based image search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393422,
author = {Han, Yahong and Wu, Fei and Lu, Xinyan and Tian, Qi and Zhuang, Yueting and Luo, Jiebo},
title = {Correlated Attribute Transfer with Multi-Task Graph-Guided Fusion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393422},
doi = {10.1145/2393347.2393422},
abstract = {Due to the describable or human-nameable nature of visual attributes, the attribute-based methods have been receiving much attentions in recent years in many applications. The advantages of the utilization of visual attributes are that they can be composed to create descriptions at various levels of specificity or they can be learned once and then applied to recognize new objects or categories. Therefore, attribute prediction becomes an essential problem to boost image understanding. This paper proposes an approach for correlated attribute transfer from a well-defined source image set to an uncontrolled target image set for attribute prediction. We call it correlated attribute transfer with multi-task graph-guided fusion (CAT-MtG2F). The novelty of CAT-MtG2F is to encourage highly correlated attributes to share a common set of relevant low-level features and transfer the learned common structure from the source image set to the target image set. The experiments show that the proposed CAT-MtG2F achieves better performance in attribute prediction.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {529–538},
numpages = {10},
keywords = {attribute prediction, feature selection, transfer learning, multi-task graph-guided fusion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393423,
author = {Xie, Lingxi and Tian, Qi and Zhang, Bo},
title = {Spatial Pooling of Heterogeneous Features for Image Applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393423},
doi = {10.1145/2393347.2393423},
abstract = {The Bag-of-Features (BoF) model has played an important role for image representation in many multimedia applications. It has been extensively applied to many tasks including image classification, image retrieval, scene understanding, and so on. Despite the advantages of this model such as simplicity, efficiency and generality, there are also notable drawbacks for this model, including poor power of semantic expression of local descriptors, and lack of robust structures upon single visual words. To overcome these problems, various techniques have been proposed, such as multiple descriptors, spatial context modeling and interest region detection. Though they have been proven to improve the BoF model to some extent, there still lacks a coherent scheme to integrate each individual module.To address the problems above, we propose a novel framework with spatial pooling of heterogeneous features. Our framework differs from the traditional Bag-of-Features model on three aspects. First, we propose a new scheme for combining texture and edge based local features together at the descriptor extraction level. Next, we build geometric visual phrases to model spatial context upon heterogeneous features for mid-level representation of images. Finally, based on a smoothed edgemap, a simple and effective spatial weighting scheme is performed on our mid-level image representation. We test our integrated framework on several benchmark datasets for image classification and retrieval applications. The extensive results show the superior performance of our algorithm over state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {539–548},
numpages = {10},
keywords = {bof framework, complementary descriptors, spatial weighting, geometric visual phrases, image representation, multimedia applications, phrase pooling},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393424,
author = {Ushiku, Yoshitaka and Harada, Tatsuya and Kuniyoshi, Yasuo},
title = {Efficient Image Annotation for Automatic Sentence Generation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393424},
doi = {10.1145/2393347.2393424},
abstract = {Sentence generation from images is an ultimate goal of image recognition. In this paper, we attack a novel problem, the "multi-keyphrase problem", to address this goal. We hypothesize that image contents can be described with multi-keyphrases, and that a natural sentence can be generated by connecting multi-keyphrases with an experimental grammar model. Existing methods require semantic knowledge such as labels of an object, action, or scene. Using these methods, we must strive to prepare a highly organized dataset. Therefore, we propose a novel online learning method for multi-keyphrase estimation. The proposed framework, although simple and scalable, can generate sentences from images with no semantic knowledge. Moreover, the proposed method for multi-keyphrase estimation is applicable to image annotation, and it achieves state-of-the-art performance. Our experiment using only images and texts demonstrates that the proposed framework is useful for sentence generation from images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {549–558},
numpages = {10},
keywords = {multi-stack decoding, passive-aggressive, online learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246407,
author = {Mei, Tao},
title = {Session Details: Full Paper Session 14: Mobile Systems},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246407},
doi = {10.1145/3246407},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393426,
author = {Tseng, Yu-Chuan and Huang, Yi-Ching and Wu, Kuan-Ying and Chin, Chi-Ping},
title = {Dinner of Luci\'{e}Rnaga: An Interactive Play with IPhone App in Theater},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393426},
doi = {10.1145/2393347.2393426},
abstract = {Interactive digital art in the field of performance is emerging as an increasingly important form of artistic expression in Taiwan. Dinner of Luci\'{e}rnaga is an interdisciplinary project produced by more than ten talented members which include the director, dancer, choreographer, artist, interactive designer, sound designer, iPhone app engineer, image processing designer, stage designer and light designer. The goal of this project is to create new modes of interactive participation between the performers and audience through the use of an innovative iPhone application that links dancer to audience and audience to dancer. The application not only plays a key role in connecting the audience and dancer, but also uses an interesting sound generation application that enhances the spectators' experience. It creates and shares special interactive experiences. Dinner of Luci\'{e}rnaga is a story about the relationship of light and human in the digital era. It is a stunning performance in visuals and interactive process with focus on new interface that are put into use in authentic environments for validation by audience. In this paper, we will discuss our artistic motivation, the development of the digitally interactive performances, and our process for creating the digitally interactive performance entitled Dinner of Luci\'{e}rnaga.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {559–568},
numpages = {10},
keywords = {digital performance, image processing, mobile art, iphone app, interaction},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393427,
author = {Yang, Xin and Cheng, Kwang-Ting (Tim)},
title = {Accelerating SURF Detector on Mobile Devices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393427},
doi = {10.1145/2393347.2393427},
abstract = {Running a SURF (Speeded Up Robust Features) detector on mobile devices remains too slow to support emerging applications such as mobile augmented reality. Porting it without adapting the algorithm to account for mobile platform limitations could result in significant runtime degradation. In this paper, we identify two mismatches between the SURF algorithm and the mobile hardware that cause substantial slow-down of the point detection process: 1) mismatch between the data access pattern and the small cache size, and 2) mismatch between the huge amount of branches and high pipeline hazard penalty. To address the mismatches, we propose two techniques: tiled SURF and gradient moment based orientation assignment. Tiled SURF improves data locality and greatly reduces memory traffic. A method for determining the optimal tile sizes, named content-aware tiling, is designed to minimize runtime and maximize detection accuracy. To avoid the penalties caused by pipeline hazards, we replace the original orientation operator with branching-free gradient moment computations. The proposed techniques are tested on three mobile platforms. Comparing to the original SURF, the accelerated SURF achieves a 6x~8x speedup without sacrificing recognition accuracy. Meanwhile, it achieves 59%~80% reductions in the runtime ratio of the detector running on mobile platforms compared with on x86-based PCs.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {569–578},
numpages = {10},
keywords = {acceleration, surf, pipeline hazards penalty, branch prediction, data access patterns, point detection, cache miss, mobile phones},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393428,
author = {Dai, Lican and Yue, Huanjing and Sun, Xiaoyan and Wu, Feng},
title = {IMShare: Instantly Sharing Your Mobile Landmark Images by Search-Based Reconstruction},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393428},
doi = {10.1145/2393347.2393428},
abstract = {Instantly sharing captured landmark images is becoming fashionable, much like when you write a blog or chat with friends by mobile phone. However, real-time transmission of high-resolution images poses a significant challenge to contemporary mobile networks. Either long delays in transmission or largely reduced image resolution can lead to bad user experience. In this paper, we propose a novel mobile-cloud scheme IMShare to enable instant sharing of high-resolution images. On the mobile side, high-resolution images are described by their thumbnails and SIFT (Scale-Invariant Feature Transform) descriptors. After compression, data sent by mobile phones can be reduced to an average of 2.6 kilobytes (KB) per mega pixel. On the cloud side, high-resolution images are reproduced from a large-scale image database by retrieving partial duplicate images by SIFT descriptors and stitching corresponding image patches together under the guidance of the thumbnails. IMShare is the first scheme to demonstrate that not only visually pleasant images can be reconstructed using this mobile-cloud method but also the reconstruction can be done in seconds using parallel computing. Our user study of a half million images in a database shows that the proposed IMShare significantly outperforms the current method on subjective quality.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {579–588},
numpages = {10},
keywords = {image reconstruction, image sharing, sift descriptor, partial duplicate image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393429,
author = {Liu, Jiajun and Huang, Zi and Chen, Lei and Shen, Heng Tao and Yan, Zhixian},
title = {Discovering Areas of Interest with Geo-Tagged Images and Check-Ins},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393429},
doi = {10.1145/2393347.2393429},
abstract = {Geo-tagged image is an ideal source for the discovery of popular travel places. However, the aspects of popular venues for daily-life purposes like dining and shopping are often missing in the mined locations from geo-tagged images. Fortunately check-in websites provide us a unique opportunity of analyzing people's preferences in their daily lives to complement the knowledge mined from geo-tagged images. This paper presents a novel approach for the discovery of Areas of Interest (AoI). By analyzing both geo-tagged images and check-ins, the approach exploits travelers' flavors as well as the preferences of daily-life activities of local residents to find AoI in a city. The proposed approach consists of two major steps. Firstly, we devise a density-based clustering method to discover AoI, mainly based on the image densities but also reinforced by the secondary densities from the images' neighboring venues. Then we propose a novel joint authority analysis framework to rank AoI. The framework simultaneously considers both the location-location transitions, and the user-location relations. An interactive presentation interface for visualizing AoI is also presented. The approach is tested with very large datasets for Shanghai city. They consist of 49,460 geo-tagged images from Panoramio.com, and 1,361,547 check-ins from the check-in website Qieke.com. By evaluating the ranking accuracy and quality of AoI, we demonstrate great improvements of our method over compared methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {589–598},
numpages = {10},
keywords = {check-in, area of interest, geo-tagged photo},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246408,
author = {Hsu, Winston},
title = {Session Details: Full Paper Session 15: Image Content Analysis},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246408},
doi = {10.1145/3246408},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393431,
author = {Letessier, Pierre and Buisson, Olivier and Joly, Alexis},
title = {Scalable Mining of Small Visual Objects},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393431},
doi = {10.1145/2393347.2393431},
abstract = {This paper presents a scalable method for automatically discovering frequent visual objects in large multimedia collections even if their size is very small. It first formally revisits the problem of mining or discovering such objects, and then generalizes two kinds of existing methods for probing candidate object seeds: weighted adaptive sampling and hashing-based methods. The idea is that the collision frequencies obtained with hashing-based methods can actually be converted into a prior probability density function given as input to a weighted adaptive sampling algorithm. This allows for an evaluation of any hashing scheme effectiveness in a more generalized way, and a comparison with other priors, e.g. guided by visual saliency concerns. We then introduce a new hashing strategy, working first at the visual level, and then at the geometric level. This strategy allows us to integrate weak geometric constraints into the hashing phase itself and not only neighborhood constraints as in previous works. Experiments conducted on a new dataset introduced in this paper will show that using this new hashing-based prior allows a drastic reduction of the number of tentative probes required to discover small objects instantiated several times in a large dataset.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {599–608},
numpages = {10},
keywords = {RMMH, LSH, scalable, small objects, discovery, computer vision, weak geometry, mining, hashing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393432,
author = {Zhang, Wei and Pang, Lei and Ngo, Chong-Wah},
title = {Snap-and-Ask: Answering Multimodal Question by Naming Visual Instance},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393432},
doi = {10.1145/2393347.2393432},
abstract = {In real-life, it is easier to provide a visual cue when asking a question about a possibly unfamiliar topic, for example, asking the question, "Where was this crop circle found?". Providing an image of the instance is far more convenient than texting a verbose description of the visual properties, especially when the name of the query instance is not known. Nevertheless, having to identify the visual instance before processing the question and eventually returning the answer makes multimodal question-answering technically challenging. This paper addresses the problem of visual-to-text naming through the paradigm of answering-by-search in a two-stage computational framework, which is composed out of instance search (IS) and similar question ranking (QR). In IS, names of the instances are inferred from similar visual examples searched through a million-scale image dataset. For recalling instances of non-planar and non-rigid shapes, spatial configurations that emphasize topology consistency while allowing for local variations in matches have been incorporated. In QR, the candidate names of the instance are statistically identified from search results and directly utilized to retrieve similar questions from community-contributed QA (cQA) archives. By parsing questions into syntactic trees, a fuzzy matching between the inquirer's question and cQA questions is performed to locate answers and recommend related questions to the inquirer. The proposed framework is evaluated on a wide range of visual instances (e.g., fashion, art, food, pet, logo, and landmark) over various QA categories (e.g., factoid, definition, how-to, and opinion).},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {609–618},
numpages = {10},
keywords = {multimedia question answering, similar question search, visual instance search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393433,
author = {Liu, Si and Feng, Jiashi and Song, Zheng and Zhang, Tianzhu and Lu, Hanqing and Xu, Changsheng and Yan, Shuicheng},
title = {Hi, Magic Closet, Tell Me What to Wear!},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393433},
doi = {10.1145/2393347.2393433},
abstract = {In this paper, we aim at a practical system, magic closet, for automatic occasion-oriented clothing recommendation. Given a user-input occasion, e.g., wedding, shopping or dating, magic closet intelligently suggests the most suitable clothing from the user's own clothing photo album, or automatically pairs the user-specified reference clothing (upper-body or lower-body) with the most suitable one from online shops.Two key criteria are explicitly considered for the magic closet system. One criterion is to wear properly, e.g., compared to suit pants, it is more decent to wear a cocktail dress for a banquet occasion. The other criterion is to wear aesthetically, e.g., a red T-shirt matches better white pants than green pants. To narrow the semantic gap between the low-level features of clothing and the high-level occasion categories, we adopt middle-level clothing attributes (e.g., clothing category, color, pattern) as a bridge. More specifically, the clothing attributes are treated as latent variables in our proposed latent Support Vector Machine (SVM) based recommendation model. The wearing properly criterion is described in the model through a feature-occasion potential and an attribute-occasion potential, while the wearing aesthetically criterion is expressed by an attribute-attribute potential. To learn a generalize-well model and comprehensively evaluate it, we collect a large clothing What-to-Wear (WoW) dataset, and thoroughly annotate the whole dataset with 7 multi-value clothing attributes and 10 occasion categories via Amazon Mechanic Turk. Extensive experiments on the WoW dataset demonstrate the effectiveness of the magic closet system for both occasion-oriented clothing recommendation and pairing.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {619–628},
numpages = {10},
keywords = {latent svm, clothing pairing, clothing recommendation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393434,
author = {Lu, Chun-Shien and Hsu, Chao-Yung},
title = {Constraint-Optimized Keypoint Inhibition/Insertion Attack: Security Threat to Scale-Space Image Feature Extraction},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393434},
doi = {10.1145/2393347.2393434},
abstract = {Scale-space image feature extraction (SSIFE) has been widely adopted in broad areas due to its powerful resilience to attacks. However, the security threat to SSIFE-based applications, which will be addressed in this paper, is relatively unexplored.The security threat to SSIFT (called ST-SSIFE), composed of a constrained-optimization keypoint inhibition attack (KIHA) and a keypoint insertion attack (KISA), is specifically designed in this paper for scale-space feature extraction methods, such as SIFT and SURF.In ST-SSIFE, KIHA aims at making a fool of feature extraction protocols in that the detection rules are purposely violated so as to suppress the existence of a local maximum around a local region.We show that KIHA can be accomplished quickly via Lagrange multiplier but the resultant new keypoint generation (NKG) problem can be solved via Karush Kuhn Tucker (KKT) conditions.In order to leverage among keypoint removal with minimum distortion, suppression of NKG, and complexity, we further present a hybrid scheme of integrating Lagrange multiplier and KKT conditions.On the other hand, KISA is designed via an efficient coarse-to-fine descriptor matching strategy to yield fake feature points so as to create false positives.Experiments, conducted on keypoint removal rate evaluation and an image copy detection method operating on a web-scale image database as a case study, demonstrate the feasibility of our method.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {629–638},
numpages = {10},
keywords = {security, feature extraction, optimization, scale-space, attack, sift},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246409,
author = {Wen, Zhen},
title = {Session Details: Full Paper Session 16: Social Media},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246409},
doi = {10.1145/3246409},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393436,
author = {Wei, Xiao-Yong and Yang, Zhen-Qun},
title = {Mining In-Class Social Networks for Large-Scale Pedagogical Analysis},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393436},
doi = {10.1145/2393347.2393436},
abstract = {Modeling the in-class student social networks is a highly desired goal in educational literature. However, due to the difficulty to collect social data, most of the conventional studies can only be conducted in a qualitative way on a small-scale of dataset obtained through questionnaires or interviews. We propose to solve the problems of data collection, social network construction and analysis with multimedia technology, in the way that we can automatically recognize the positions and identities of the students in classroom and construct the in-class social networks accordingly. With the social networks and the statistics on a large-scale dataset, we have demonstrated that the pedagogical analysis for investigating the co-learning patterns among the students can be conducted in a quantitative way, which provides the statistical clues about why prior studies reach conflicting conclusions on the relation between the students' positions in social networks and their academic performances. The experimental results have validated the effectiveness of the proposed approaches in both technical and pedagogical senses.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {639–648},
numpages = {10},
keywords = {pedagogical analysis, in-class social networks},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393437,
author = {Roy, Suman Deb and Mei, Tao and Zeng, Wenjun and Li, Shipeng},
title = {SocialTransfer: Cross-Domain Transfer Learning from Social Streams for Media Applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393437},
doi = {10.1145/2393347.2393437},
abstract = {The usage and applications of social media have become pervasive. This has enabled an innovative paradigm to solve multimedia problems (e.g., recommendation and popularity prediction), which are otherwise hard to address purely by traditional approaches. In this paper, we investigate how to build a mutual connection among the disparate social media on the Internet, using which cross-domain media recommendation can be realized. We accomplish this goal through SocialTransfer---a novel cross-domain real-time transfer learning framework. While existing transfer learning methods do not address how to utilize the real time social streams, our proposed SocialTransfer is able to effectively learn from social streams to help multimedia applications, assuming an intermediate topic space can be built across domains. It is characterized by two key components: 1) a topic space learned in real time from social streams via Online Streaming Latent Dirichlet Allocation (OSLDA), and 2) a real-time cross-domain graph spectra analysis based transfer learning method that seamlessly incorporates learned topic models from social streams into the transfer learning framework. We present as use cases of emph{SocialTransfer} two video recommendation applications that otherwise can hardly be achieved by conventional media analysis techniques: 1) socialized query suggestion for video search, and 2) socialized video recommendation that features socially trending topical videos. We conduct experiments on a real-world large-scale dataset, including 10.2 million tweets and 5.7 million YouTube videos and show that emph{SocialTransfer} outperforms traditional learners significantly, and plays a natural and interoperable connection across video and social domains, leading to a wide variety of cross-domain applications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {649–658},
numpages = {10},
keywords = {cross-domain media retrieval, recommendation, transfer learning, social media},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393438,
author = {Liu, Dong and Ye, Guangnan and Chen, Ching-Ting and Yan, Shuicheng and Chang, Shih-Fu},
title = {Hybrid Social Media Network},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393438},
doi = {10.1145/2393347.2393438},
abstract = {Analysis and recommendation of multimedia information can be greatly improved if we know the interactions between the content, user, and concept, which can be easily observed from the social media networks. However, there are many heterogeneous entities and relations in such networks, making it difficult to fully represent and exploit the diverse array of information. In this paper, we develop a hybrid social media network, through which the heterogeneous entities and relations are seamlessly integrated and a joint inference procedure across the heterogeneous entities and relations can be developed. The network can be used to generate personalized information recommendation in response to specific targets of interests, e.g., personalized multimedia albums, target advertisement and friend/topic recommendation. In the proposed network, each node denotes an entity and the multiple edges between nodes characterize the diverse relations between the entities (e.g., friends, similar contents, related concepts, favorites, tags, etc). Given a query from a user indicating his/her information needs, a propagation over the hybrid social media network is employed to infer the utility scores of all the entities in the network while learning the edge selection function to activate only a sparse subset of relevant edges, such that the query information can be best propagated along the activated paths. Driven by the intuition that much redundancy exists among the diverse relations, we have developed a robust optimization framework based on several sparsity principles. We show significant performance gains of the proposed method over the state of the art in multimedia retrieval and recommendation using data crawled from social media sites. To the best of our knowledge, this is the first model supporting not only aggregation but also judicious selection of heterogeneous relations in the social media networks.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {659–668},
numpages = {10},
keywords = {edge selective information propagation, hybrid social media network, multimedia retrieval and recommendation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2393439,
author = {Chen, Yan-Ying and Hsu, Winston H. and Liao, Hong-Yuan Mark},
title = {Discovering Informative Social Subgraphs and Predicting Pairwise Relationships from Group Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2393439},
doi = {10.1145/2393347.2393439},
abstract = {An increasing number of users are contributing the sheer amount of group photos (e.g., for family, classmates, colleagues, etc.) on social media for the purpose of photo sharing and social communication. There arise strong needs for automatically understanding the group types (e.g., family vs. classmates) for recommendation services (e.g., recommending a family-friendly restaurant) and even predicting the pairwise relationships (e.g., mother-child) between the people in the photo for mining implicit social connections. Interestingly, we observe that the group photos are composed of atomic subgroups corresponding to certain social relationships. For this work, we propose a novel framework to (1) connect faces of different attributes and positions as a face graph and (2) discover informative subgraphs to represent social subgroups in group photos. A group photo can be further represented by a bag-of-face-subgraphs (BoFG) -- the occurring frequency of social subgroups, which is informative to categorize specific group types or events. We demonstrate the effectiveness of BoFG in recognizing family photos and achieve 30.5% relative improvement over the state-of-the-art low-level features. Moreover, we propose to predict the pairwise relationships (e.g., husband-wife) in a face graph by the co-occurrence information (e.g., co-occurring with a child) in the mined subgraphs. The experiments demonstrate that the informative social subgroups significantly outperform prior work (36% relatively) which considers merely facial attributes for determining pairwise relationships.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {669–678},
numpages = {10},
keywords = {social relationship, face subgraph mining, face graph},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246410,
author = {Del Bimbo, Alberto},
title = {Session Details: Poster Session I},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246410},
doi = {10.1145/3246410},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396290,
author = {Fang, Yuming and Lin, Weisi and Chen, Zhenzhong and Tsai, Chia-Ming and Lin, Chia-Wen},
title = {Video Saliency Detection in the Compressed Domain},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396290},
doi = {10.1145/2393347.2396290},
abstract = {Saliency detection is widely used to extract the regions of interest in images. Many saliency detection models have been proposed for videos in the uncompressed domain. However, videos are always stored in the compressed domain such as MPEG2, H.264, MPEG4 Visual, etc. In this study, we propose a video saliency detection model based on feature contrast in the compressed domain. Four features of luminance, color, texture and motion are extracted from DCT coefficients and motion vectors in the video bitstream. The static saliency map of video frames is calculated based on the luminance, color and texture features, while the motion saliency map for video frames is computed by motion feature. The final saliency map for video frames is obtained through combining the static saliency map and motion saliency map. Experimental results show good performance of the proposed video saliency detection model in the compressed domain.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {697–700},
numpages = {4},
keywords = {video saliency detection, compressed domain},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396291,
author = {Zhang, Chi and Wang, Weiqiang},
title = {A Robust and Efficient Shot Boundary Detection Approach Based on Fisher Criterion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396291},
doi = {10.1145/2393347.2396291},
abstract = {In this paper, we present a robust and efficient approach which is capable to simultaneously detect various shot boundaries in a unified way. The proposed approach first detects general shot boundaries based on the idea of Fisher criterion, and then classifies them into two categories, cut and gradual transition (GT), by an SVM classifier. Further computation is performed for the GT shot boundaries to expand rough boundary locations between two frames into the transition interval consisting of all the transitional frames. Finally, a postprocessing module is employed to merge overlapped transitions. The evaluation experiments show that the proposed approach has the impressive performance in both efficiency and accuracy, and outperforms the best results of all the participants of TRECVID 2006.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {701–704},
numpages = {4},
keywords = {shot boundary detection, fisher criterion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396292,
author = {Wenger, Stephan and Magnor, Marcus},
title = {A Genetic Algorithm for Audio Retargeting},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396292},
doi = {10.1145/2393347.2396292},
abstract = {We present an audio retargeting technique to create custom soundtracks for movies and games from existing audio material (typically music) by automatically rearranging audio segments. Constraints can be specified to make the length of the audio fit the length of a movie scene, or to align parts of a piece of music with particular events. Existing approaches typically create soundtracks with many unnecessary and often disruptive transitions. We extend a recent analysis and resynthesis method with a novel genetic algorithm for finding an optimal succession of audio segments that minimizes the number of audible transitions and repetitions as well as the deviation from user-specified constraints. Compared to prior work, our experiments with audio examples from different musical genres show a significant improvement with respect to the optimization criteria, and the resulting soundtracks contain few, if any, noticeable transitions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {705–708},
numpages = {4},
keywords = {audio retargeting, audio synthesis, example-based synthesis, sound textures},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396293,
author = {Noh, Hyeonwoo and Han, Bohyung},
title = {Seam Carving with Forward Gradient Difference Maps},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396293},
doi = {10.1145/2393347.2396293},
abstract = {We propose a new energy function for seam carving based on forward gradient differences to preserve regular structures in images. The energy function measures the curvature inconsistency between the pixels that become adjacent after seam removal, and involves the difference of gradient orientation and magnitude of the pixels. Our objective is to minimize the differences induced by the removed seam, and the optimization is performed by dynamic programming based on multiple cumulative energy maps, each of which corresponds to the seam pattern associated with a pixel. The proposed technique preserves straight lines and regular shapes better than the original and improved seam carving, and can be easily combined with other types of energy functions within the seam carving framework. We evaluated the performance of our algorithm by comparing with the original and improved seam carving algorithms using public data.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {709–712},
numpages = {4},
keywords = {seam carving, forward gradient difference maps, image retargetting},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396294,
author = {Chen, Chongyu and Cai, Jianfei and Lin, Weisi and Shi, Guangming},
title = {Surveillance Video Coding via Low-Rank and Sparse Decomposition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396294},
doi = {10.1145/2393347.2396294},
abstract = {Surveillance videos are usually with a static or gradually changed background. The state-of-the-art block-based codec, H.264/AVC, is not sufficiently efficient for encoding surveillance videos since it cannot exploit the strong background temporal redundancy in a global manner. In this paper, motivated by the recent advance on low-rank and sparse decomposition (LRSD), we propose to apply it for the compression of surveillance videos. In particular, the LRSD is employed to decompose a surveillance video into the low-rank component, representing the background, and the sparse component, representing the moving objects. Then, we design different coding methods for the two different components. We represent the frames of the background by very few independent frames based on their linear dependency, which dramatically removes the temporal redundancy. Experimental results show that, for the compression of surveillance videos, the proposed scheme can significantly outperform H.264/AVC, up to 3 dB PSNR gain, especially at relatively low bit rates.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {713–716},
numpages = {4},
keywords = {low-rank and sparse decomposition, cur decomposition, surveillance video compression},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396295,
author = {Yen, Jui-Yu and Chen, Bo-Hao and Huang, Shih-Chia},
title = {Enhanced Extraction of Moving Objects in Variable Bit-Rate Video Streams},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396295},
doi = {10.1145/2393347.2396295},
abstract = {Motion detection plays an important role in the video surveillance system. Video communications over wireless networks can easily suffer from network congestion or unstable bandwidth, especially for embedded application. A rate control scheme produces various bit-rate video streams to match the available network bandwidth. However, effective detection of moving objects in various bit-rate video streams is a very difficult problem. This paper proposes an advanced approach based on the counter-propagation network through artificial neural networks to achieve effective moving object detection in various bit-rate video streams. We compare our method with other state-of-the-art methods. To demonstrate the performance of our proposed method in regard to object extraction, we analyze qualitative and quantitative comparisons in real-world limited bandwidth networks over a wide range of natural video sequences. The overall results show that our proposed method substantially outperforms other state-of-the-art methods by Similarity and F1 accuracy rates of 73.84% and 84.94%, respectively.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {717–720},
numpages = {4},
keywords = {video surveillance, variable bit-rate, neural network, motion detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396296,
author = {Li, Bing and Xiong, Weihua and Hu, Weiming and Ding, Xinmiao},
title = {Context-Aware Affective Images Classification Based on Bilayer Sparse Representation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396296},
doi = {10.1145/2393347.2396296},
abstract = {In image understanding, the automatic recognition of emotion in an image is becoming important from an applicative viewpoint. Considering the fact that the emotion evoked by an image is not only from its global appearance but also interplays among local regions, we propose a novel context-aware classification model based on bilayer sparse representation (BSR) that simultaneously takes the local context and global-local context into account. The BSR model contains two layers: global sparse representation (GSR) and local sparse representation (LSR). The GSR is to define global similarities between a test image and all training images; while the LSR is to define similarities of local regions' appearances and their co-occurrence between a test image and all training images. The experiments on two data sets demonstrate that our method is effective on affective images classification.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {721–724},
numpages = {4},
keywords = {context-aware, affective image classification, bilayer sparse representation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396297,
author = {Zhou, Xiuzhuang and Lu, Jiwen and Hu, Junlin and Shang, Yuanyuan},
title = {Gabor-Based Gradient Orientation Pyramid for Kinship Verification under Uncontrolled Environments},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396297},
doi = {10.1145/2393347.2396297},
abstract = {This paper presents a Gabor-based Gradient Orientation Pyramid (GGOP) feature representation method for kinship verification from facial images. First, we perform Gabor wavelet on each face image to obtain a set of Gabor magnitude (GM) feature images from different scales and orientations. Then, we extract the Gradient Orientation Pyramid (GOP) feature of each GM feature image and perform multiple feature fusion for kinship verification. When combined with the discriminative support vector machine (SVM) classifier, GGOP demonstrates the best performance in our experiments, in comparison with several state-of-the-art face feature descriptors. Experimental results are presented to show the efficacy of our proposed approach. Moreover, the performance of our proposed method is also comparable to that of human observers.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {725–728},
numpages = {4},
keywords = {ace analysis, kinship verification, image descriptors},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396298,
author = {Lin, Tao and Lin, Liang and Wang, Qing},
title = {Robust Stroke-Based Video Animation via Layered Motion and Correspondence},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396298},
doi = {10.1145/2393347.2396298},
abstract = {This paper investigates a novel approach to reduce artifacts and visual flickering in generating painterly animations from real video clips. In the traditional painterly animation methods, the brush strokes are propagated over video frames by calculating optical flows, and the visual impression of animations are severely affected by incorrect correspondences. In our method, we combine motion segmentation and occlusion handing to establish accurate dense feature correspondences, which is shown to robust propagate brush strokes against complex motions and occlusions. Moreover, a beforehand rendering strategy is presented to alleviate stroke flickering. In the experiments, we generate a number of animations in cartoon and oil painting style. The quantitative evaluations of brush stabilization is presented as well.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {729–732},
numpages = {4},
keywords = {layered motion, video stylization, non-photorealistic rendering},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396299,
author = {Sun, Wenxiu and Au, Oscar C. and Xu, Lingfeng and Li, Yujun and Hu, Wei and Yu, Zhiding},
title = {Texture Optimization for Seamless View Synthesis through Energy Minimization},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396299},
doi = {10.1145/2393347.2396299},
abstract = {In this paper, we present a view synthesis method named Visto which aims to generate seamless novel views from a monocular view input. We formulate the problem as joint optimization of inter-view texture similarity and geometry preservation, which significantly differs from traditional view synthesis framework. In this way, the image characteristics of virtual view are inherently inherited from the reference view without introducing any image prior or texture modeling technique. The energy function is minimized using Gauss-Seidel-like approach, and the quality of the virtual view is refined iteratively. The proposed approach also tolerates small depth map errors. Further more, the algorithm is parallel friendly. The simulation results outperform several existing state-of-the-art monocular view synthesis systems.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {733–736},
numpages = {4},
keywords = {gauss-seidel-like, inter-view similarity, geometry preservation, texture optimization, seamless view synthesis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396300,
author = {Xu, Xin-Shun and Jiang, Yuan and Xue, Xiangyang and Zhou, Zhi-Hua},
title = {Semi-Supervised Multi-Instance Multi-Label Learning for Video Annotation Task},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396300},
doi = {10.1145/2393347.2396300},
abstract = {Traditional approaches for automatic video annotation usually represent one video clip with a flat feature vector, neglecting the fact that video data contain natural structures. It is also noteworthy that a video clip is often relevant to multiple concepts. Indeed, the video annotation task is inherently a Multi-Instance Multi-Label learning (MIML) problem. Considering that manually annotating videos is labor-intensive and time-consuming, this paper proposes a semi-supervised MIML approach, SSMIML, which is able to exploit abundant unannotated videos to help improve the annotation performance. This approach takes label correlations into account, and enforces similar instances to share similar multi-labels. Evaluation on TREVID 2005 show that the proposed approach outperforms several state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {737–740},
numpages = {4},
keywords = {semi-supervised learning, multi-instance multi-label learning, video annotation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396301,
author = {Zhou, Youjie and Luo, Jiebo},
title = {Geo-Location Inference on News Articles via Multimodal PLSA},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396301},
doi = {10.1145/2393347.2396301},
abstract = {The fast evolution and adoption of social media creates an increasingly need for location based services. Location inference on news or events becomes an essential problem. This paper addresses the problem by extracting location involved topics (geo-topic) using both text content and visual content. This paper proposes a geo-topic extraction framework for geo-location inference, including location name entity recognition, location related image association and a multimodal location dependent pLSA geo-topic model. Experiments have shown that our fused model improves the f-score in geo-location inference by 10% compared with single modality based models.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {741–744},
numpages = {4},
keywords = {feature fusion, geo-location inference, multimodal pLSA},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396302,
author = {Mao, Zhendong and Zhang, Yongdong and Gao, Ke and Zhang, DongMing},
title = {A Method for Detecting Salient Regions Using Integrated Features},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396302},
doi = {10.1145/2393347.2396302},
abstract = {We develop a novel algorithm for detecting salient regions. By analyzing the advantages and disadvantages of the existing methods, five principles for designing salient region detection algorithms are summarized. Based on these principles, we propose a novel method that generates saliency map with highlighted salient regions by utilizing two different features, namely visual saliency value and spatial weight. The visual saliency value is determined based on local contrast differences and low-level feature frequencies. The spatial weight is computed by analyzing the size and location of salient regions. Experimental results show that the proposed algorithm outperforms 7 state-of-the-art methods on the public image set.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {745–748},
numpages = {4},
keywords = {integrated features, salient region detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396303,
author = {Cai, Xinyuan and Wang, Chunheng and Xiao, Baihua and Chen, Xue and Zhou, Ji},
title = {Deep Nonlinear Metric Learning with Independent Subspace Analysis for Face Verification},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396303},
doi = {10.1145/2393347.2396303},
abstract = {Face verification is the task of determining by analyzing face images, whether a person is who he/she claims to be. It is a very challenge problem, due to large variations in lighting, background, expression, hairstyle and occlusion. The crucial problem is to compute the similarity of two face vectors. Metric learning has provides a viable solution to this problem. Until now, many metric learning algorithms have been proposed, but they are usually limited to learning a linear transformation (i.e. finding a global Mahalanobis metric). In this brief, we propose a nonlinear metric learning method, which learns an explicit mapping from the original space to an optimal subspace, using deep Independent Subspace Analysis network. Compared to kernel methods, which can also learn nonlinear transformations, our method is a deep and local learning architecture, and therefore exhibits more powerful ability to learn the nature of highly variable dataset. We evaluate our method on the LFW benchmark, and results show very comparable performance to the state-of-art methods (achieving 92.28% accuracy), while maintaining simplicity and good generalization ability.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {749–752},
numpages = {4},
keywords = {deep learning architecture, face verification, independent subspace analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396304,
author = {Su, Zhuo and Deng, Daiguo and Yang, Xue and Luo, Xiaonan},
title = {Color Transfer Based on Multiscale Gradient-Aware Decomposition and Color Distribution Mapping},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396304},
doi = {10.1145/2393347.2396304},
abstract = {Automatic global color transfer is a challenging problem in image editing. In this paper, we propose a novel color transfer method, which is based on the gradient-aware decomposition and the color distribution mapping. Firstly, a gradient-aware decomposition model is established to separate the target image into the base and detail layers. Then, the colors of each separated base layer are enforced to match those of a given reference image by Pitie's multi-dimensional probability density function transfer method. After that, all mapped base layers are combined with corresponding boosted detail layers to produce the final output. The experiments demonstrate that our method can achieve a visual satisfied result without post-processing gradient correction.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {753–756},
numpages = {4},
keywords = {gradient-aware decompostion, color transfer, distribution},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396305,
author = {Yang, Yi-Hsuan},
title = {On Sparse and Low-Rank Matrix Decomposition for Singing Voice Separation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396305},
doi = {10.1145/2393347.2396305},
abstract = {Over recent years there has been a growing interest in finding ways to transform signals/matrices into sparse or low-rank representations, i.e., representations which are sparse in support or of low redundancy. Such decompositions are proving to be particularly powerful for a variety of signal processing and compression problems. In this paper, we investigate the application of this technique to the challenging task of singing voice/accompaniment separation for popular music. The vocal part is modeled as a sparse signal, whereas the instrumental part is considered to be low-rank. In addition, to better account for the particular properties of music, two new algorithms are proposed to improve the decomposition, including the incorporation of harmonicity priors and a back-end drum removal procedure. Evaluations on the MIR-1K benchmark dataset show that the proposed algorithms outperform the state-of-the-art by 0.01-2.41 db.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {757–760},
numpages = {4},
keywords = {low-rank and sparse decomposition, singing voice separation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396306,
author = {Sun, Xiaoshuai and Yao, Hongxun},
title = {Memorable Basis: Towards Human-Centralized Sparse Representation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396306},
doi = {10.1145/2393347.2396306},
abstract = {Previous studies of sparse representation in multimedia research focus on developing reliable and efficient dictionary learning algorithms. Despite the sparse prior, how to integrate other related perceptual factors of human being into dictionary learning process was seldom studied. In this paper, we investigate the influence of image memorability for human-centralized sparse representation. Based on the results of a photo memory game, we are able to quantitatively characterize an image's memorability which allows us to train sparse bases from the most memorable images instead of randomly selected natural images. We believed that such kind of basis is more consistent with neural networks in human brain and hence can better predict where human looks. To test our hypothesis, we choose human eye-fixation prediction problem for quantitative evaluation. The experimental results demonstrate the superior performance of our Memorable Basis compared to traditional sparse basis trained from unselected images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {761–764},
numpages = {4},
keywords = {visual attention, image memorability, eye fixation prediction, human centralized sparse representation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396307,
author = {Phan, Trung Quy and Shivakumara, Palaiahnakote and Tan, Chew Lim},
title = {Detecting Text in the Real World},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396307},
doi = {10.1145/2393347.2396307},
abstract = {The problem of text detection in natural scene images is challenging because of the unconstrained sizes, colors, backgrounds and alignments of the characters. This paper proposes novel symmetry features for this task. Within a text line, the intra-character symmetry captures the correspondence between the inner contour and the outer contour of a character while the inter-character symmetry helps to extract information from the gap region between two consecutive characters. A formulation based on Gradient Vector Flow is used to detect both types of symmetry points. These points are then grouped into text lines using the consistency in sizes, colors, and stroke and gap thickness. Therefore, unlike most existing methods which use only character features, our method exploits both the text features and the gap features to improve the detection result. Experimentally, our method compares well to the state-of-the-art on public datasets for natural scenes and street-level images, an emerging category of image data. The proposed technique can be used in a wide range of multimedia applications such as content-based image/video retrieval, mobile visual search and sign translation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {765–768},
numpages = {4},
keywords = {scene text detection, natural scene text, texture analysis, gradient vector flow, symmetry detection, street view images},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396308,
author = {Han, Zhen and Jiang, Junjun and Hu, Ruimin and Lu, Tao and Huang, Kebin},
title = {Face Image Super-Resolution via Nearest Feature Line},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396308},
doi = {10.1145/2393347.2396308},
abstract = {In this paper, we propose a manifold learning based algorithm using 'Nearest Feature Line - NFL' to hallucinate high-resolution face image. According to the fact that existing NFL can effectively characterize the geometrical proportions to the face samples, we propose using NFL metric to define the neighborhood relations between face samples. Our algorithm can solve the problem that traditional method cannot effectively reveal the similar local geometry between high-resolution and low-resolution face manifolds under the condition that the training sample size is small. Moreover, in order to enhance the representation capacity of available face samples and reduce the computational complexity, we select neighborhood samples for each input LR image. Experimental results demonstrate that our algorithm can generates clearer local feature details, and the PSNR is 1.4 dB higher than that of the best manifold learning based method reported so far.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {769–772},
numpages = {4},
keywords = {manifold learning, nearest feature line, super-resolution},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396309,
author = {Fu, Zhenyong and Lu, Hongtao and Ip, Horace H.S. and Lu, Zhiwu},
title = {Modalities Consensus for Multi-Modal Constraint Propagation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396309},
doi = {10.1145/2393347.2396309},
abstract = {This paper presents a novel modalities consensus framework for multi-modal pairwise constraint propagation (MCP). We first combine multiple single-modal constraint propagation (SCP) problems together, and then explicitly introduce a new modalities consensus regularizer to force the propagation results on different modalities to be consistent with each other. With a separable consensus regularizer, the proposed approach can be effectively solved using an alternating optimization way. More importantly, based on our modalities consensus framework, two single-modal constraint propagation algorithms can be directly reformulated as two well-defined multi-modal solutions. Experimental results on constrained clustering tasks have shown that the proposed framework can achieve significant improvements with respect to the state of the arts.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {773–776},
numpages = {4},
keywords = {pairwise constraint propagation, multi-modal analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396310,
author = {Luo, Ping and Wang, Xiaogang and Lin, Liang and Tang, Xiaoou},
title = {Joint Semantic Segmentation by Searching for Compatible-Competitive References},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396310},
doi = {10.1145/2393347.2396310},
abstract = {This paper presents a framework for semantically segmenting a target image without tags by searching for references in an image database, where all the images are unsegmented but annotated with tags. We jointly segment the target image and its references by optimizing both semantic consistencies within individual images and correspondences between the target image and each of its references. In our framework, we first retrieve two types of references with a semantic-driven scheme: i) the compatible references which share similar global appearance with the target image; and ii) the competitive references which have distinct appearance to the target image but similar tags with one of the compatible references. The two types of references have complementary information for assisting the segmentation of the target image. Then we construct a novel graphical representation, in which the vertices are superpixels extracted from the target image and its references. The segmentation problem is posed as labeling all the vertices with the semantic tags obtained from the references. The method is able to label images without the pixel-level annotation and classifier training, and it outperforms the state-of-the-arts approaches on the MSRC-21 database.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {777–780},
numpages = {4},
keywords = {label propagation, image search, semantic segmentation, scene understanding},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396311,
author = {Chen, Tianlong and Liu, Chunxi and Huang, Qingming},
title = {An Effective Multi-Clue Fusion Approach for Web Video Topic Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396311},
doi = {10.1145/2393347.2396311},
abstract = {The efficient organization and navigation of web videos in the topic level could enhance the user experience and boost the user's understanding about the happened events. Due to the potential application prospects, topic detection attracts increasing research interests in the last decade. On one hand, the user concerned real world hot topic always leads to a massive discussion in the video sharing sites, such as YouTube, Youku, etc. On the other hand, the search volume of the topic related keywords are growing explosively in the search engine such as Google, Yahoo, etc. These keywords are the queries formulated by the users to search their concerned topics. They reflect the users' intention and could be used as a clue to find the hot topics. In this paper, different from the traditional topic detection methods, which mainly rely on data clustering, we propose a novel multi-clue fusion approach for web video topic detection. In our approach, firstly by utilizing the video related tag information, a maximum average score and a burstiness degree are proposed to extract the dense-bursty tag groups. Secondly, the near-duplicate keyframes (NDK) are extracted from the videos and fused with the extracted tag groups. After that, the hot search keywords from the search engine are used as guidance for topic detection. Finally, these clues are combined together to detect the topics hidden in the web video data. Experiment is conducted on the YouTube video data and the results demonstrate that the proposed method is effective.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {781–784},
numpages = {4},
keywords = {topic detection, multi-clues fusion, tag group},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396312,
author = {Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
title = {3D Fingertip and Palm Tracking in Depth Image Sequences},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396312},
doi = {10.1145/2393347.2396312},
abstract = {We present a vision-based approach for robust 3D fingertip and palm tracking on depth images using a single Kinect sensor. First the hand is segmented in the depth images by applying depth and morphological constraints. The palm is located by performing distance transform to the hand contour and tracked with a Kalman filter. The fingertips are detected by combining three depth-based features and tracked with a particle filter over successive frames. Quantitative results on synthetic depth sequences show the proposed scheme can track the fingertips quite accurately. Besides, its capabilities are further demonstrated through a real-life human-computer interaction application.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {785–788},
numpages = {4},
keywords = {geodesic distance, human-computer interaction, kinect sensor, fingertip tracking},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396313,
author = {Mohammadi, Gelareh and Origlia, Antonio and Filippone, Maurizio and Vinciarelli, Alessandro},
title = {From Speech to Personality: Mapping Voice Quality and Intonation into Personality Differences},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396313},
doi = {10.1145/2393347.2396313},
abstract = {From a cognitive point of view, personality perception corresponds to capturing individual differences and can be thought of as positioning the people around us in an ideal personality space. The more similar the personality of two individuals, the closer their position in the space. This work shows that the mutual position of two individuals in the personality space can be inferred from prosodic features. The experiments, based on ordinal regression techniques, have been performed over a corpus of 640 speech samples comprising 322 individuals assessed in terms of personality traits by 11 human judges, which is the largest database of this type in the literature. The results show that the mutual position of two individuals can be predicted with up to 80% accuracy.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {789–792},
numpages = {4},
keywords = {big five personality model, nonverbal vocal behavior, social signal processing, personality assessment},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396314,
author = {Kim, Samuel and Filippone, Maurizio and Valente, Fabio and Vinciarelli, Alessandro},
title = {Predicting the Conflict Level in Television Political Debates: An Approach Based on Crowdsourcing, Nonverbal Communication and Gaussian Processes},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396314},
doi = {10.1145/2393347.2396314},
abstract = {One of the most recent trends in multimedia indexing is to represent data in terms of the social and psychological phenomena that users perceive. In such a perspective this article proposes an approach for the automatic detection of conflict level in television political debates. The proposed approach includes the use of crowdsourcing techniques for modeling the perception of data consumers, the extraction of (language independent) nonverbal behavioral cues and the application of regression techniques based on Gaussian Processes. The experiments have been performed over 1430 clips of 30 seconds extracted from 45 political debates (roughly 12 hours of material). The results show that a correlation up to 0.8 can be achieved between the actual and predicted conflict level.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {793–796},
numpages = {4},
keywords = {nonverbal vocal behavior, social signal processing, conflict, multimedia indexing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396315,
author = {Bu, Pengyang and Wang, Nan and Ai, Haizhou},
title = {Using Structural Patches Tiling to Guide Human Head-Shoulder Segmentation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396315},
doi = {10.1145/2393347.2396315},
abstract = {In this paper, we propose a novel and effective structural patches tiling procedure which is able to generate high quality probabilistic masks to guide semantic segmentation.In this structural patches tiling procedure, we first apply a local patch structure classifier trained by random forest to the input image in a sliding window manner, and then construct an MRF iteratively optimized to assemble a high quality probabilistic mask from responses collected from the previous stage.Our main contributions are twofold: A patch-based classification procedure which is fast and capable of capturing rich local structures compared with pixel-based ones; a flexible Markovian sliding window merging algorithm which integrates context information into traditional sliding window method. Without loss of generality, we use head-shoulder segmentation to illustrate this procedure's power. Experiments on daily photos and comparisons with previous work demonstrate that we are able to achieve state-of-the-art head-shoulder segmentation results thanks to this structural patches tiling procedure.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {797–800},
numpages = {4},
keywords = {head-shoulder segmentation, markovian sliding window, patch-based classification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396316,
author = {Zhang, Bao and Zhao, Handong and Cao, Xiaochun},
title = {Video Object Segmentation with Shortest Path},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396316},
doi = {10.1145/2393347.2396316},
abstract = {Unsupervised video object segmentation is to automatically segment the foreground object in the video without any prior knowledge. This paper proposes an object-level method to segment foreground object, while existing methods are normally based on low level information. We firstly find all the object-like regions. Then based on the corresponding map between the successive frames, the video segmentation problem is converted to graph model one. Rather than adopting TRW-S which might result in a local optimal solution, a shortest path algorithm is explored to get a globally optimum solution. Compared with the state-of-the-art object-level method, our method not only guarantees the continuity of segmentation result but also works well even under the big disturbance of fast motion object in the background. The experimental results on two open datasets (SegTrack and Berkeley Motion Segmentation Dataset) and video sequences captured by ourselves demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {801–804},
numpages = {4},
keywords = {video object segmentation, shortest path solution},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396317,
author = {Chen, Ding-Jie and Chen, Hwann-Tzong and Chang, Long-Wen},
title = {Video Object Cosegmentation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396317},
doi = {10.1145/2393347.2396317},
abstract = {We introduce and address the problem of video object cosegmentation, which concerns the task of segmenting the common object in a pair of video sequences. We present a new algorithm that works on super-voxels in videos to solve this task. The algorithm computes i the intra-video relative motion derived from dense optical flow and ii) the inter-video co-features based on Gaussian mixture models. The experimental results show that, by integrating the intra-video and inter-video information, our algorithm is able to obtain better results of segmenting video objects.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {805–808},
numpages = {4},
keywords = {GMM, co-segmentation, video, motion, graph cut},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396318,
author = {Chen, Zhineng and Ngo, Chong-Wah and Cao, Juan and Zhang, Wei},
title = {Community as a Connector: Associating Faces with Celebrity Names in Web Videos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396318},
doi = {10.1145/2393347.2396318},
abstract = {Associating celebrity faces appearing in videos with their names is of increasingly importance with the popularity of both celebrity videos and related queries. However, the problem is not yet seriously studied in Web video domain. This paper proposes a Community connected Celebrity Name-Face Association approach (C-CNFA), where the community is regarded as an intermediate connector to facilitate the association. Specifically, with the names and faces extracted from Web videos, C-CNFA decomposes the association task into a three-step framework: community discovering, community matching and celebrity face tagging. To achieve the goal of efficient name-face association under this umbrella, algorithms such as the constrained density-based clustering and exemplar based voting are developed by leveraging different pieces of visual and contextual cues. The evaluation on 0.4 million faces and 144 celebrities shows the effectiveness of the proposed C-CNFA approach. Moreover, using the obtained associations, encouraging results are reported in celebrity video ranking.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {809–812},
numpages = {4},
keywords = {community analysis, name-face association, celebrity videos},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396319,
author = {Petridis, Stavros and Bilakhia, Sanjay and Pantic, Maja},
title = {Comparison of Prediction-Based Fusion and Feature-Level Fusion across Different Learning Models},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396319},
doi = {10.1145/2393347.2396319},
abstract = {There is evidence in neuroscience indicating that prediction of spatial and temporal patterns in the brain plays a key role in perception. This has given rise to prediction-based fusion as a method of combining information from audio and visual modalities. Models are trained on a per-class basis, to learn the mapping from one feature-space to another. When presented with unseen data, each model predicts the respective feature-sets using its learnt mapping, and the prediction error is combined within each class. The model which best describes the audiovisual relationship (by having the lowest combined prediction error) provides its label to the input data. Previous studies have only used neural networks to evaluate this method of combining modalities - this paper extends this to other learning methods, including Long Short-Term Memory recurrent neural networks (LSTMs), Support Vector Machines (SVMs), Relevance Vector Machines (RVMs), and Gaussian Processes (GPs). Our results on cross-database experiments on nonlinguistic vocalisation recognition show that feature-prediction significantly outperforms feature-fusion for neural networks, LSTMs, and GPs, while performance on SVMs and RVMs is more ambiguous and neither model gains an absolute advantage over the other.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {813–816},
numpages = {4},
keywords = {nonlinguistic information processing, prediction-based classification/fusion, audiovisual fusion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396320,
author = {Rzeszutek, Richard and Phan, Raymond and Androutsos, Dimitrios},
title = {Depth Estimation for Semi-Automatic 2D to 3D Conversion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396320},
doi = {10.1145/2393347.2396320},
abstract = {The conversion of monoscopic footage into stereoscopic or multiview content is a difficult and time consuming task. A number of semi-automatic methods have been developed to speed up the process and provide some control to the user. However these methods require that the user provide detailed labels indicating the relative depth of objects in the scene. In this paper we present a method to automatically estimate depth in such a way that it is amenable to semi-automatic conversion. The method is designed to simplify the depth labelling task so that the user does not have to provide as many depth labels.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {817–820},
numpages = {4},
keywords = {computer vision, motion estimation, 2D-to-3D conversion, depth estimation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396321,
author = {Yao, Ting and Ngo, Chong-Wah and Zhu, Shiai},
title = {Predicting Domain Adaptivity: Redo or Recycle?},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396321},
doi = {10.1145/2393347.2396321},
abstract = {Over the years, the academic researchers have contributed various visual concept classifiers. Nevertheless, given a new dataset, most researchers still prefer to develop large number of classifiers from scratch despite expensive labeling efforts and limited computing resources. A valid question is why not multimedia community ``embrace the green" and recycle off-the-shelf classifiers for new dataset. The difficulty originates from the domain gap that there are many different factors that govern the development of a classifier and eventually drive its performance to emphasize certain aspects of dataset. Reapplying a classifier to an unseen dataset may end up GIGO (garbage in, garbage out) and the performance could be much worse than re-developing a new classifier with very few training examples. In this paper, we explore different parameters, including shift of data distribution, visual and context diversities, that may hinder or otherwise encourage the recycling of old classifiers for new dataset. Particularly, we give empirical insights of when to recycle available resources, and when to redo from scratch by completely forgetting the past and train a brand new classifier. Based on these analysis, we further propose an approach for predicting the negative transfer of a concept classifier to a different domain given the observed parameters. Experimental results show that the prediction accuracy of over 75% can be achieved when transferring concept classifiers learnt from LSCOM (news video domain), ImageNet (Web image domain) and Flickr-SF (weakly tagged Web image domain), respectively, to TRECVID 2011 dataset (Web video domain).},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {821–824},
numpages = {4},
keywords = {domain adaptation, cross-domain concept learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396322,
author = {Jiang, Xi and Zhang, Tuo and Hu, Xintao and Lu, Lie and Han, Junwei and Guo, Lei and Liu, Tianming},
title = {Music/Speech Classification Using High-Level Features Derived from Fmri Brain Imaging},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396322},
doi = {10.1145/2393347.2396322},
abstract = {With the availability of large amount of audio tracks through a variety of sources and distribution channels, automatic music/speech classification becomes an indispensable tool in social audio websites and online audio communities. However, the accuracy of current acoustic-based low-level feature classification methods is still rather far from satisfaction. The discrepancy between the limited descriptive power of low-level features and the richness of high-level semantics perceived by the human brain has become the 'bottleneck' problem in audio signal analysis. In this paper, functional magnetic resonance imaging (fMRI) which monitors the human brain's response under the natural stimulus of music/speech listening is used as high-level features in the brain imaging space (BIS). We developed a computational framework to model the relationships between BIS features and low-level features in the training dataset with fMRI scans, predict BIS features of testing dataset without fMRI scans, and use the predicted BIS features for music/speech classification in the application stage. Experimental results demonstrated the significantly improved performance of music/speech classification via predicted BIS features than that via the original low-level features.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {825–828},
numpages = {4},
keywords = {music/speech classification, brain imaging space, functional magnetic resonance imaging, semantic gap},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396323,
author = {Liu, Jen-Yu and Yeh, Chin-Chia and Yang, Yi-Hsuan and Teng, Yuan-Ching},
title = {Bilingual Analysis of Song Lyrics and Audio Words},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396323},
doi = {10.1145/2393347.2396323},
abstract = {Thanks to the development of music audio analysis, state-of-the-art techniques can now detect musical attributes such as timbre, rhythm, and pitch with certain level of reliability and effectiveness. An emerging body of research has begun to model the high-level perceptual properties of music listening, including the mood and the preferable listening context of a music piece. Towards this goal, we propose a novel text-like feature representation that encodes the rich and time-varying information of music using a composite of features extracted from the song lyrics and audio signals. In particular, we investigate dictionary learning algorithms to optimize the generation of local feature descriptors and also probabilistic topic models to group semantically relevant text and audio words. This text-like representation leads to significant improvement in automatic mood classification over conventional audio features.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {829–832},
numpages = {4},
keywords = {topic model, MLLDA, LDA, sparse coding, audioword},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396324,
author = {Tang, Ye and Yang, Yu-Bin and Gao, Yang},
title = {Self-Paced Dictionary Learning for Image Classification},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396324},
doi = {10.1145/2393347.2396324},
abstract = {Image classification is an important research task in multimedia content analysis and processing. Learning a compact dictionary easying to derive sparse representation is one of the focused issues in the state-of-the-art image classification framework. Most existing dictionary learning approaches assign equal importance to all training samples, which in fact have different complexity in terms of sparse representation. Meanwhile, the contextual information "hidden" in different samples is ignored as well. In this paper, we propose a self-paced dictionary learning algorithm in order to accommodate the "hidden" information of the samples into the learning procedure, which uses the easy samples to train the dictionary first, and then iteratively introduces more complex samples in the remaining training procedure until the entire training data are all easy samples. The algorithm adaptively chooses the easy samples in each iteration, while the learned dictionary in the previous iteration is in turn used as a basis for the current iteration. This strategy implicitly takes advantage of the contextual relationships among training samples. The number of the chosen samples in each iteration is determined by an adaptive threshold function proposed in this paper. Experimental results on benchmark datasets, including Caltech-101 and 15-Scene, show that our algorithm leads to better dictionary representation and classification performance than the baseline methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {833–836},
numpages = {4},
keywords = {self-paced, image classification, dictionary learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396325,
author = {Wu, Xixuan and Qiao, Yu and Wang, Xiaogang and Tang, Xiaoou},
title = {Cross Matching of Music and Image},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396325},
doi = {10.1145/2393347.2396325},
abstract = {Human perception of music and image are highly correlated. Both of them can inspire human sensation like emotion and power. This paper investigates how to model the relationship between music and image using 47,888 music-image pairs extracted from music videos. We have two basic observations for this relationship: 1) music space exhibits simpler cluster structure than image space, and 2) the relationship between the two spaces is complex and nonlinear. Based on these observations, we develop Multiple Ranking Canonical Correlation Analysis (MR-CCA) to learn such relationship. MR-CCA clusters the music-image pairs according to their music parts, and then conducts Ranking CCA (R-CCA) for each cluster. Compared with classical CCA, R-CCA takes account of the pairwise ranking information available in our dataset. MR-CCA improves performance and significantly reduce computational cost. Experiment results show that R-CCA outperforms CCA, and MR-CCA has the best performance with a consistency score of 84.52% with human labeling. The proposed method can be generalized to model cross media relationship and has potential applications in video generation, background music recommendation, and joint retrieval of music and image.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {837–840},
numpages = {4},
keywords = {ranking canonical correlation analysis, combination of clusters, music-image similarity},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396326,
author = {Peters, Nils and Lei, Howard and Friedland, Gerald},
title = {Name That Room: Room Identification Using Acoustic Features in a Recording},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396326},
doi = {10.1145/2393347.2396326},
abstract = {This paper presents a system for identifying the room in an audio or video recording through the analysis of acoustical properties. The room identification system was tested using a corpus of 13440 reverberant audio samples. With no common content between the training and testing data, an accuracy of 61% for musical signals and 85% for speech signals was achieved. This approach could be applied in a variety of scenarios where knowledge about the acoustical environment is desired, such as location estimation, music recommendation, or emergency response systems.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {841–844},
numpages = {4},
keywords = {media content analysis and processing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396327,
author = {Wong, Lai-Kuan and Wong, Kok-Lim},
title = {Enhancing Visual Dominance by Semantics-Preserving Image Recomposition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396327},
doi = {10.1145/2393347.2396327},
abstract = {We present a semi-automatic photographic recomposition approach that employs a semantics-preserving warp of the input image to enhance the visual dominance of the main subjects. Our method uses the tearable image warping method to shift the subjects against the background (and vice versa), so that their visual dominance is improved, and yet preserve the desired spatial semantics between the subjects and the background. The recomposition is guided by a measure of the resulting visual dominance of the main subjects. Our user experiment shows the effectiveness of the approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {845–848},
numpages = {4},
keywords = {visual dominance, tearable image warping, image semantics, computational aesthetics, photographic recomposition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396328,
author = {Xiao, Jie and Zhou, Wengang and Li, Xia and Wang, Meng and Tian, Qi},
title = {Image Tag Re-Ranking by Coupled Probability Transition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396328},
doi = {10.1145/2393347.2396328},
abstract = {The large amount of user-tagged images on social networks is helpful to facilitate image management and image search. However, many tags are weakly relevant or irrelevant to the visual content, resulting in unsatisfactory performance in tag related applications. In this paper, we propose a coupled probability transition algorithm to estimate the text-visual group relevance from the observed data and then leverage it to predict tag relevance for a new query image. The visual group for a given tag is a cluster of images that are visually similar and share the same tag. The tag-visual group relevance is uncovered by exploiting the mutual reinforcement in visual space and semantic space alternatively. Experiments on NUS-WIDE dataset show the validity and superiority of the proposed approach over existing methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {849–852},
numpages = {4},
keywords = {image tag re-ranking, coupled probability transition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396329,
author = {Li, Zechao and Liu, Jing and Jiang, Yu and Tang, Jinhui and Lu, Hanqing},
title = {Low Rank Metric Learning for Social Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396329},
doi = {10.1145/2393347.2396329},
abstract = {With the popularity of social media applications, large amounts of social images associated with rich context are available, which is helpful for many applications. In this paper, we propose a Low Rank distance Metric Learning (LRML) algorithm by discovering knowledge from these rich contextual data, to boost the performance of CBIR. Different from traditional approaches that often use the must-links and cannot-links between images, the proposed method exploits information from the visual and textual domains. We assume that the visual similarity estimated by the learned metric is expected to be consistent with the semantic similarity in the textual domain. Since tags are usually noisy, misspelling or meaningless, we also leverage the preservation of visual structure to prevent overfitting those noisy tags. On the other hand, the metric is straightforward constrained to be low rank. We formulate it as a convex optimization problem with nuclear norm minimization and propose an effective optimization algorithm based on proximal gradient method. With the learned metric for image retrieval, some experimental evaluations on a real-world dataset demonstrate the outperformance of our approach over other related work.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {853–856},
numpages = {4},
keywords = {social images retrieval, metric learning, low rank},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396330,
author = {Jia, Jia and Wu, Sen and Wang, Xiaohui and Hu, Peiyun and Cai, Lianhong and Tang, Jie},
title = {Can We Understand van Gogh's Mood? Learning to Infer Affects from Images in Social Networks},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396330},
doi = {10.1145/2393347.2396330},
abstract = {Can we understand van Gogh's mood from his artworks? For many years, people have tried to capture van Gogh's affects from his artworks so as to understand the essential meaning behind the images and catch on why van Gogh created these works. In this paper, we study the problem of inferring affects from images in social networks. In particular, we aim to answer: What are the fundamental features that reflect the affects of the authors in images? How the social network information can be leveraged to help detect these affects? We propose a semi-supervised framework to formulate the problem into a factor graph model. Experiments on 20,000 random-download Flickr images show that our method can achieve a precision of 49% with a recall of 24% on inferring authors'affects into 16 categories. Finally, we demonstrate the effectiveness of the proposed method on automatically understanding van Gogh's Mood from his artworks, and inferring the trend of public affects around special event.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {857–860},
numpages = {4},
keywords = {emotion, color features, image, affects, factor graph model},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396331,
author = {Liu, Yang and Liu, Jing and Li, Zechao and Niu, Biao and Lu, Hanqing},
title = {Social Tag Alignment with Image Regions by Sparse Reconstructions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396331},
doi = {10.1145/2393347.2396331},
abstract = {How to align social tags with image regions without additional human intervention is a challenging but a valuable task since it can provide more detailed image semantic information and improve the accuracy of image retrieval. To this end, we propose a novel tag-to-region method with two phases of sparse reconstructions by exploring the large-scale user contributed resources. Given an image with social tags, we first explore the tagging information of large-scale social images to sparsely reconstruct the label vector of the given image, and then use the reconstructing weights as the semantic relevance to the image. With the top $T$ semantically relevant images, we further employ a group sparse coding algorithm to reconstruct each region of the given image, in which the regions from the social images with a common label are deemed as a label group. The group sparsity works on the assumption that one image region corresponds to tags as few as possible. Finally, the region-level tags can be predicted based on the reconstruction error in the corresponding label groups. Extensive experiments on MSRC and SAIAPR TC-12 datasets demonstrate the encouraging performance of our method in comparison with other baselines.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {861–864},
numpages = {4},
keywords = {sparse coding, image region-tag alignment},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396332,
author = {Wang, Yanxiang and Sundaram, Hari and Xie, Lexing},
title = {Social Event Detection with Interaction Graph Modeling},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396332},
doi = {10.1145/2393347.2396332},
abstract = {This paper focuses on detecting social, physical-world events from photos posted on social media sites. The problem is important: cheap media capture devices have significantly increased the number of photos shared on these sites. The main contribution of this paper is to incorporate online social interaction features in the detection of physical events. We believe that online social interaction reflect important signals among the participants on the "social affinity" of two photos, thereby helping event detection. We compute social affinity via a random-walk on a social interaction graph to determine similarity between two photos on the graph. We train a support vector machine classifier to combine the social affinity between photos and photo-centric metadata including time, location, tags and description. Incremental clustering is then used to group photos to event clusters. We have very good results on two large scale real-world datasets: Upcoming and MediaEval. We show an improvement between 0.06-0.10 in F1 on these datasets.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {865–868},
numpages = {4},
keywords = {social media, similarity metric, event detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246411,
author = {Cao, Liangliang},
title = {Session Details: Poster Session 2},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246411},
doi = {10.1145/3246411},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396334,
author = {Bian, Jingwen and Zha, Zheng-Jun and Zhang, Hanwang and Tian, Qi and Chua, Tat-Seng},
title = {Visual Query Attributes Suggestion},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396334},
doi = {10.1145/2393347.2396334},
abstract = {Query suggestion is an effective solution to help users deliver their search intent. While many query suggestion approaches have been proposed for test-based image retrieval with query-by-keywords, query suggestion for content-based image retrieval (CBIR) with query-by-example (QBE) has been seldom studied. QBE usually suffers from the "intention gap" problem, especially when the user fails to get an appropriate query image to express his search intention precisely. In this paper, we propose a novel query suggestion scheme named Visual Query Attributes Suggestion (VQAS) for image search with QBE. Given a query image, informative attributes are suggested to the user as complements to the query. These attributes reflect the visual properties and key components of the query. By selecting some suggested attributes, the user can provide more precise search intent which is not captured by the query image. The evaluation results on two real-world image datasets show the effectiveness of VQAS in terms of retrieval performance and the quality of query suggestions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {869–872},
numpages = {4},
keywords = {query suggestion, image search, attribute},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396335,
author = {Cai, Junjie and Zha, Zheng-Jun and Zhou, Wengang and Tian, Qi},
title = {Attribute-Assisted Reranking for Web Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396335},
doi = {10.1145/2393347.2396335},
abstract = {Image search reranking is an effective approach to refine the text-based image search result. Most existing reranking approaches are based on low-level visual features. In this paper, we propose to exploit semantic attributes for image search reranking. Based on the classifiers for all the pre-defined attributes, each image is represented by a attribute feature consisting of the responses from these classifiers. A hypergraph is then used to model the relationship between images by integrating low-level visual features and attribute features. Hypergraph ranking is performed to order the images. Its basic principle is that visually similar images should have similar ranking scores. We conduct experiments on 300 queries in MSRA-MM V2.0 dataset. The experimental results demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {873–876},
numpages = {4},
keywords = {image search reranking, attribute},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396336,
author = {Li, Xia and Zhou, Wengang and Tang, Jinhui and Tian, Qi},
title = {Query Expansion Enhancement by Fast Binary Matching},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396336},
doi = {10.1145/2393347.2396336},
abstract = {Query expansion has been successfully employed to improve the performance of image retrieval system. It usually expands the original query based on the information from top ranked images. However, it may fail when some of the top ranked images are false positive or contain noisy features. To minimize the amount of irrelevant local features introduced, we propose to enhance query expansion by fast binary matching. More specifically, the noisy points on a candidate image are filtered out by local verification with their mapped locations on the query image. We further rank the expansion results by three different measurements based on local patch similarity in the image space. Experiments on partial-duplicate Web image search with a database of one million images show that the proposed approach achieves promising improvement in mean Average Precision (mAP) over the state-of-the-art query expansion approaches, and remains efficient in search time.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {877–880},
numpages = {4},
keywords = {query expansion, image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396337,
author = {Liu, Xianglong and He, Junfeng and Liu, Di and Lang, Bo},
title = {Compact Kernel Hashing with Multiple Features},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396337},
doi = {10.1145/2393347.2396337},
abstract = {Hashing methods, which generate binary codes to preserve certain similarity, recently have become attractive in many applications like large scale visual search. However, most of state-of-the-art hashing methods only utilize single feature type, while combining multiple features has been proved very helpful in image search. In this paper we propose a novel hashing approach that utilizes the information conveyed by different features. The multiple feature hashing can be formulated as a similarity preserving problem with optimal linearly-combined multiple kernels. Such formulation is not only compatible with general types of data and diverse types of similarities indicated by different visual features, but also helpful to achieve fast training and search. We present an efficient alternating optimization to learn the hashing functions and the optimal kernel combination. Experimental results on two well-known benchmarks CIFAR-10 and NUS-WIDE show that the proposed method can achieve 11% and 34% performance gains over state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {881–884},
numpages = {4},
keywords = {compact hashing, multiple feature, multiple kernel},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396338,
author = {Tu, Weiwen and Pan, Rong and Wang, Jingdong},
title = {Similar Image Search with a Tiny Bag-of-Delegates Representation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396338},
doi = {10.1145/2393347.2396338},
abstract = {Similar image search over a large image database has been attracting a lot of attention recently. The widely-used solution is to use a set of codes, which we call bag-of-delegates, to represent each image, and to build inverted indices to organize the image database. The search can be conducted through the inverted indices, which is the same to the way of using texts to index images for search and has been shown to be efficient and effective.In this paper, we propose a tiny bag-of-delegates representation that uses a small amount of delegates with a high search performance guaranteed. The main advantage is that less storageis required to save the inverted indices while having a high search accuracy. We propose an adaptive forward selection scheme to sequentially learn more and more inverted indices that are constructed based on subspace partition, e.g. using spatial partition trees. Experimental results demonstrate that our approach can require a smaller number of delegates while achieving the same accuracy and taking similar time.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {885–888},
numpages = {4},
keywords = {adaptive forward selection, spatial partition trees, similar image search, bag-of-delegates},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396339,
author = {Cao, Chen and Chen, Shifeng and Li, Yuhong and Liu, Jianzhuang},
title = {Online Non-Feedback Image Re-Ranking via Dominant Data Selection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396339},
doi = {10.1145/2393347.2396339},
abstract = {Image re-ranking aims at improving the precision of keyword-based image retrieval, mainly by introducing visual features to re-rank. Many existing approaches require offline training for every keyword, which are unsuitable for online image search. Other real-time approaches demand user interaction, which are inappropriate for large-scale image collection. To improve the accuracy of web image retrieval in a practicable manner, we propose a novel re-ranking algorithm to explore the cluster information of the image set. First, we build spectral graph on images that retrieved bysearch engine, and remove isolated nodes as noisy images. Then, we select positive samples from the most dominant cluster in initial top-ranked images, and the samples are used for semi-supervised learning and ranking. Our algorithm is online and non-feedback. Experiments on two public databases demonstrate that our algorithm outperforms the state-of-the-art approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {889–892},
numpages = {4},
keywords = {spectral ranking, image re-ranking},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396340,
author = {Zhao, Kun and Liu, Wei and Liu, Jianzhuang},
title = {Optimal Semi-Supervised Metric Learning for Image Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396340},
doi = {10.1145/2393347.2396340},
abstract = {In a typical content-based image retrieval (CBIR) system, images are represented as vectors and similarities between images are measured by a specified distance metric. However, the traditional Euclidean distance cannot always deliver satisfactory performance, so an effective metric sensible to the input data is desired. Tremendous recent works on metric learning have exhibited promising performance, but most of them suffer from limited label information and expensive training costs. In this paper, we propose two novel metric learning approaches, Optimal Semi-Supervised Metric Learning and its kernelized version. In the proposed approaches, we incorporate information from both labeled and unlabeled data to design a convex and computationally tractable learning framework which results in a globally optimal solution to the target metric of much lower rank than the original data dimension. Experiments on several image benchmarks demonstrate that our approaches lead to consistently better distance metrics than the state-of-the-arts in terms of accuracy for image retrieval.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {893–896},
numpages = {4},
keywords = {semi-supervised learning, metric learning, image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396341,
author = {Wen, Yue and Gao, Yue and Hong, Richang and Luan, Huanbo and Liu, Qiong and Shen, Jialie and Ji, Rongrong},
title = {View-Based 3D Object Retrieval by Bipartite Graph Matching},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396341},
doi = {10.1145/2393347.2396341},
abstract = {Bipartite graph matching has been investigated in multiple view matching for 3D object retrieval. However, existing methods employ one-to-one vertex matching scheme while more than two views may share close semantic meanings in practice. In this work, we propose a bipartite graph matching method to measure the distance between two objects based on multiple views. In the proposed method, representative views are first selected by using view clustering for each object, and the corresponding weights are given based on the cluster results. A bipartite graph is constructed by using the two groups of representative views from two compared objects. To calculate the similarity between two objects, the bipartite graph is first partitioned to several subsets, and the views in the same sub-set are with high possibility to be with similar semantic meanings. The distances between two objects within individual subsets are then assembled through the graph to obtain the final similarity. Experimental results and comparison with the state-of-the-art methods demonstrate the effectiveness of the proposed algorithm.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {897–900},
numpages = {4},
keywords = {3D object retrieval, graph matching, bipartite graph},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396342,
author = {Ohbuchi, Ryutarou and Kurita, Yukinori},
title = {Local Geometry Adaptive Manifold Re-Ranking for Shape-Based 3D Object Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396342},
doi = {10.1145/2393347.2396342},
abstract = {This paper proposes an improvement to Manifold Ranking algorithm used for search results ranking in the context of shape-based 3D model retrieval. Manifold Ranking algorithm by Zhou et al estimates, given a set of high-dimensional feature vectors, a lower-dimensional manifold on which the features lie. It then computes diffusion-based distances from a feature vector (or feature vectors) to the other feature vectors on the manifold. When applied to content-based retrieval, overall retrieval accuracy is significantly better than a "simple" fixed distance metric. However, in a small neighborhood of query, retrieval ranks obtained by a "simple" distance metric (e.g., L1-norm) performs better than those obtained by Manifold Ranking. Proposed re-ranking algorithm tries to combine ranking results due to both simple distance metric and Manifold Ranking in an automatic query expansion framework for better ranking results. Experimental evaluation has shown that the proposed method is effective in improving retrieval accuracy.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {901–904},
numpages = {4},
keywords = {manifold ranking, distance metric learning, content-based retrieval, automatic query expansion, 3d model retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396343,
author = {Ye, Junfeng and Chen, Jia and Chen, Zejia and Zhu, Yihe and Bao, Shenghua and Su, Zhong and Yu, Yong},
title = {DLMSearch: Diversified Landmark Search by Photo},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396343},
doi = {10.1145/2393347.2396343},
abstract = {This paper focuses on the problem of searching for diversified landmarks with photos. More particularly, we propose a system called DLMSearch which handles image query, searches for diversified landmarks and provides representative visual summaries. DLMSearch allows a user to upload a query photo and searches for landmarks with high relevance and diversity in real time. Then DLMSearch presents a delicate photo summary for each returned landmark, considering both visual representativeness and diversity. Quantative evaluations on a web-scale landmark photo collection demonstrate the effectiveness of the DLMSearch system. Experimental results verify the merits of the proposed system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {905–908},
numpages = {4},
keywords = {content-based image retrieval, landmark search, visual summary, diversity},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396344,
author = {Fu, Hao and Qiu, Guoping},
title = {Fast Semantic Image Retrieval Based on Random Forest},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396344},
doi = {10.1145/2393347.2396344},
abstract = {This paper introduces random forest as a computational and data structure paradigm for fusing low-level visual features and high-level semantic concepts for image retrieval. We use visual features to split the tree nodes and use the image labels to supervise the splitting to make images located at the same tree node share similar semantic concepts as well as visual similarities. We exploit such a random forest and define the semantic neighbor set (SNS) of a given image as the union of all images in the leaf nodes that this image falls onto. From SNS we further define the semantic similarity measure (SSM) between two images as the number of trees in which they share the same leaf nodes within a SNS. With SNS and SSM, example-based image retrieval becomes that of first finding the SNS of the querying image and then ranking the images according to the SSMs between the querying image and images in its SNS. We also show that the new technique can be adapted for keyword-based semantic image retrieval. The inherent efficient tree data structure leads to fast solutions. We will present experimental results to show the effectiveness of this new semantic image retrieval technique.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {909–912},
numpages = {4},
keywords = {random forest, semantic image retrieval, semantic nearest neighbor},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396345,
author = {Tseng, Kai-Yu and Lin, Yen-Liang and Chen, Yu-Hsiu and Hsu, Winston H.},
title = {Sketch-Based Image Retrieval on Mobile Devices Using Compact Hash Bits},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396345},
doi = {10.1145/2393347.2396345},
abstract = {The advent of touch panels in mobile devices has provided a good platform for mobile sketch search. However, most of the previous sketch image retrieval systems usually adopt an inverted index structure on large-scale image database, which is formidable to be operated in the limited memory of mobile devices. In this paper, we propose a novel approach to address these challenges. First, we effectively utilize distance transform (DT) features to bridge the gap between query sketches and natural images. Then these high-dimensional DT features are further projected to more compact binary hash bits. The experimental results show that our method achieves very competitive retrieval performance with MindFinder approach [3] but only requires much less memory storage (e.g., our method only requires 3% of total memory storage of MindFinder in 2.1 million images). Due to its low consumption of memory, the whole system can independently operate on the mobile devices.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {913–916},
numpages = {4},
keywords = {sketch, hash},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396346,
author = {Feng, Songhe and Lang, Congyan and Li, Bing},
title = {Towards Relevance and Saliency Ranking of Image Tags},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396346},
doi = {10.1145/2393347.2396346},
abstract = {Social image tag ranking has emerged as an important research topic recently due to its potential application on web image search. This paper presents an adaptive all-season tag ranking algorithm which can handle the images with and without distinct object(s) using different tag ranking strategies. Firstly, based on saliency map derived from the visual attention model, a linear SVM is trained to pre-classify an image as attentive or non-attentive category by using the gray histogram descriptor on the corresponding saliency map. Then, an image with distinct object is processed by an attention-driven tag saliency ranking algorithm emphasizing distinct object. On the other hand, an image without distinct object is processed by the tag relevance ranking algorithm via the sparse representation based neighbor-voting strategy. Such adaptive ranking strategy can be regarded as taking full advantage of existing tag ranking paradigms. Experiments conducted on well-known image data sets demonstrate the effectiveness and efficiency of the proposed framework.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {917–920},
numpages = {4},
keywords = {visual attention model, adaptive tag ranking, multiple-instance learning, sparse representation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396347,
author = {Pan, Yi-Feng and Sun, Jian and Chen, Siyuan and He, Yuan and Xia, Yingju and Sun, Jun and Naoi, Satoshi},
title = {Mobile-Based Advertisement Information Retrieval from Images and Websites},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396347},
doi = {10.1145/2393347.2396347},
abstract = {In the real world, there are a huge amount of advertisement (ad) boards to make customers have a visual awareness of the products or services easily. However, information appearing in the ad boards is so limited that customers always want to know more ad details in a convenient way. In this paper, we present an mobile-based prototype system to automatically extract web ad information from images and websites. After capturing ad images by smartphones and sending them to a remote server, ad image text is recognized by OCR engine, from where ad phrases and keywords are extracted and combined together as queries. Ad web page candidates are then obtained by specific search engines and clustered to remove noises. OCR results are further used to estimate valid ad topic web pages which are pushed back to end users for searching more detailed ad information. Based on the experiments on a real-world ad image dataset collected by ourselves, true ad topic web pages can be found from top-one and top-ten returned pages in about 51.85% and 83.33% query images respectively, which illustrates the effectiveness of the proposed system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {921–924},
numpages = {4},
keywords = {advertisement information retrieval, mobile visual search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396348,
author = {Ahlstr\"{o}m, David and Hudelist, Marco A. and Schoeffmann, Klaus and Schaefer, Gerald},
title = {A User Study on Image Browsing on Touchscreens},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396348},
doi = {10.1145/2393347.2396348},
abstract = {Default image browsing interfaces on touch-based mobile devices provide limited support for image search tasks. To facilitate fast and convenient searches we propose an alternative interface that takes advantage of 3D graphics and arranges images on a rotatable globe according to color similarity. In a user study we compare the new design to the iPad's image browser. Results collected from 24 participants show that for color-sorted image collections the globe can reduce search time by 23% without causing more errors and that it is perceived as being fun to use and preferred over the standard browsing interface by 70% of the participants.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {925–928},
numpages = {4},
keywords = {image browsing and search, mobile devices, small screens},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396349,
author = {Xiao, Yanhui and Zhu, Zhenfeng and Wei, Shikui and Zhao, Yao},
title = {Discriminative ICA Model with Reconstruction Constraint for Image Classification},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396349},
doi = {10.1145/2393347.2396349},
abstract = {Independent Component Analysis (ICA) is an effective unsupervised tool to learn statistically independent representations. However, ICA is not only sensitive to whitening but also difficult to learn an over-complete basis set. Consequently, ICA with soft Reconstruction cost(RICA) was presented to learn sparse representations with over-complete basis even on unwhitened data. Nevertheless, this model may not be an optimal discriminative model for classification tasks, because it failed to consider the association between the training sample and its class. In this paper, we propose a supervised Discriminative ICA model with Reconstruction constraint for image classification, named DRICA. DRICA brings in class information to learn the over-complete basis by incorporating inhomogeneous representation cost constraint into the RICA framework. This constraint leads to partition the set of basis vectors into several subsets corresponding to the sample classes, where each subset could sparsely model data samples from the same class but not others. Therefore, the proposed ICA model can learn an over-complete basis and an optimal multi-class classifier jointly. Some experiments carried out on several standard image databases validate the effectiveness of DRICA for image classification.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {929–932},
numpages = {4},
keywords = {image classification, independent component analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396350,
author = {Zhang, Jiemi and Wu, Chenxia and Cai, Deng},
title = {Search Web Images Using Objects, Backgrounds and Conditions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396350},
doi = {10.1145/2393347.2396350},
abstract = {As the volumes of web images have grown rapidly in the last decade, Content-Based Image Retrieval (CBIR) has attracted substantial interests as an effective tool to manage the images. Most existing CBIR systems focus on the object in the image, while ignoring the conditions (day/night, sunny/rain, etc) and the backgrounds, both of which are very helpful to meet the user's information need. To overcome this shortcoming, in this paper, we present a novel CBIR system depending on a novel query formulation considering three aspects: Object, Background and Condition. Specifically, we design a user-friendly interface to help the user formulate a query. The interface can allow the user to give the percentage, relative position and size of each object in the background. Moreover, a corresponding effective ranking method is proposed to return the desirable search results. Experimental results demonstrate that our proposed system improves the searching performance and the user experience compared with the existing searching systems.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {933–936},
numpages = {4},
keywords = {user experience, content based image retrieval, query formulation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396351,
author = {Hsu, Huan-Cheng and Huang, Chun-Rong and Lu, Chun-Shien},
title = {Sparsity Cue in Image Copy Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396351},
doi = {10.1145/2393347.2396351},
abstract = {Image copy detection is an art of searching duplicates from a target database. Computationally efficient and robust detection is still a challenging issue. Inspired by the recent study of sparsity in the context of compressed sensing, we propose a sparse representation-based image copy detection method exploiting sparsity as the cue for searching duplicates. We find that although sparse representation can describe an image in a compact manner, the inherent discriminable features, as far as we know, are not entirely explored. In this paper, we study the discrimination ability inherent in sparsity via online dictionary learning and compact feature descriptor representation. Experimental results show that our method, compared with state-of-the-art, is computationally efficient and attains better or comparable detection performance measured in terms of precision and recall rates.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {937–940},
numpages = {4},
keywords = {spare representation, attack, sparsity, copy detection, online dictionary learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396352,
author = {Wang, Yandan and Belkhatir, Mohammed and Tahayna, Bashar},
title = {Near-Duplicate Video Retrieval Based on Clustering by Multiple Sequence Alignment},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396352},
doi = {10.1145/2393347.2396352},
abstract = {In Near-Duplicate Video Retrieval (NDVR), recent works have focused on bettering index structures and matching schemes not only to improve retrieval accuracy but also to enforce scalability in an effort to keep up with the ever-growing size of video collections. In this paper, we propose a framework for the retrieval of Near-Duplicate Videos (NDVs) based on a pre-processing step of clustering inspired by Multiple Sequence Alignment (MSA) of DNA sequences. In our technique, we represent videos as alphabetical genomes and MSA is employed to automatically cluster a video collection. NDVR is then conducted on these formed clusters instead of the original video collection. Experimentally, we show that our clustering-based approach, while being significantly faster than state-of-the-art techniques that are not based on a pre-processing clustering step, i.e. the n-gram and Dynamic Time Warping (DTW) techniques, yields equivalent results in a precision/recall evaluation framework.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {941–944},
numpages = {4},
keywords = {multiple sequence alignment, clustering, near-duplicate video retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396353,
author = {Liu, Cong and Ling, Hefei and Zou, Fuhao and Yan, Lingyu},
title = {Neighborhood Preserving Hashing for Fast Similarity Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396353},
doi = {10.1145/2393347.2396353},
abstract = {Fast similarity search methods are increasingly critical for many large-scale learning tasks, particularly in the communities of machine learning and data mining. Recently, data-aware hashing method is regarded as a promising approach for similarity search which maps high-dimensional feature vectors into efficient and compact hash codes while preserving the corresponding neighborhood structure. Although some recent hashing methods based on eigenvalue decomposition perform well, they suffer from semantic loss. In this paper, we concentrate on this issue and propose a novel neighborhood preserving hashing approach which adopts a brand-new method to combine non-negative matrix factorization and locality linear embedding without introducing any additional parameter. The combination of these two classical techniques ensures that we obtain a parts-based representation which not only fulfill the psychological and physiological requirements of human perception but also conserve the intrinsic neighborhood structure of the original data. Experiments are conducted to demonstrate that the proposed approach is superior to some state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {945–948},
numpages = {4},
keywords = {non-negative matrix factorization, fast similarity search, locally linear embedding, semantic loss},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396354,
author = {Kiani Galoogahi, Hamed and Sim, Terence},
title = {Face Photo Retrieval by Sketch Example},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396354},
doi = {10.1145/2393347.2396354},
abstract = {Face photo-sketch matching has received great attention in recent years due to its vital role in law enforcement. The major challenge of matching face photo and sketch is difference of visual characteristics between face photo and sketch which is referred as modality gap. Earlier approaches have reduced the modality gap by synthesizing face photos and sketches in a same modality (photo or sketch). However, the effectiveness of these approaches is highly affected by synthesis results. That means a poor synthesis might degrade the performance of matching. Therefore, recent works have focused to directly match face photo and sketch of different modalities. However, the features used by these approaches are not robust against modality gap. In this paper, a modality-invariant face descriptor called Gabor Shape is proposed to retrieve face photos based on a probe sketch. Experiments on CUFS and CUFSF datasets show that the new descriptor outperforms the state-of-the-art approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {949–952},
numpages = {4},
keywords = {radon transform, gabor filters, photo retrieval, face sketch},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396355,
author = {Xia, Junhai and Gao, Ke and Zhang, Dongming and Mao, Zhendong},
title = {Geometric Context-Preserving Progressive Transmission in Mobile Visual Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396355},
doi = {10.1145/2393347.2396355},
abstract = {Progressive transmission is very effective to reduce retrieval latency in mobile visual search. However, the acceleration effects of existing progressive transmission strategies are often limited because of the neglect of geometric information in the query image. This paper proposes an effective and efficient geometric context-preserving progressive transmission method, which is suitable for mobile visual search. Here a query image is divided into blocks and local features in the same block are used as query units rather than a single feature. Since clustered features with geometric information are more discriminative, only a few of them could support correct matching with high precision. Thus our method significantly decreases the number of features needed for transmission, and dramatically reduces the retrieval latency. Experiments on Stanford dataset for mobile visual search show that, with comparable precision, we uses 43% less retrieval time than existing progressive transmission method. Moreover, we establish and release a large-scale image dataset called MVSBench which is more difficult and suitable for mobile visual search. It contains 75500 images and considers many variations like view change, blur, scale, illumination and rotation. MVSBench is another major contribution of this paper, and our method also outperforms other strategies on this dataset.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {953–956},
numpages = {4},
keywords = {mobile visual search, progressive transmission, geometric context},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396356,
author = {Gao, Haidong and Tang, Siliang and Zhang, Yin and Jiang, Dapeng and Wu, Fei and Zhuang, Yueting},
title = {Supervised Cross-Collection Topic Modeling},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396356},
doi = {10.1145/2393347.2396356},
abstract = {Nowadays, vast amounts of multimedia data can be obtained across different collections (or domains). Therefore, it poses significant challenges for the utilization of those cross-collection data, for examples, the summarization of similarities and differences of data across different domains (e.g., CNN and NYT), as well as finding visually similar images across different visual domains (e.g., photos, paintings and hand-drawn sketches). In this paper, a supervised cross-collection Latent Dirichlet Allocation (scLDA) approach is proposed to utilize the data across different collections. As a natural extension of traditional Latent Dirichlet Allocation (LDA), scLDA not only takes the structural priors of different collections into consideration, but also exploits the category information. The strength of this work lies in integrating topic modeling, cross-domain learning and supervised learning together. We conduct scLDA for comparative text mining as well as classification of news articles and images from different collections. The results suggest that our proposed scLDA can generate meaningful collection-specific topics and achieves better retrieval accuracy than other related topic models.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {957–960},
numpages = {4},
keywords = {latent dirichlet allocation, cross-collection classification, topic modeling},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396357,
author = {Zhai, Xiaohua and Peng, Yuxin and Xiao, Jianguo},
title = {PDSS: Patch-Descriptor-Similarity Space for Effective Face Verification},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396357},
doi = {10.1145/2393347.2396357},
abstract = {In this paper, we propose the Patch-Descriptor-Similarity Space (PDSS) for unconstrained face verification, which is challenging due to image variations in pose, lighting, facial expression, and occlusion. Our proposed PDSS considers jointly patch, descriptor and similarity measure, which are ignored by the existing work. PDSS is extremely effective for face verification because each axis of PDSS will boost each other and could maximize the effect of every axis. Each point in PDSS reflects a distinct partial-matching between two facial images, which could be robust to variations in the facial images. Moreover, by selecting the discriminating point subset from PDSS, we could describe accurately the characteristic similarities and differences between two facial images, and further decide whether they represent the same person. In PDSS, each axis can describe effectively the distinct features of the faces: each patch (the first axis) reflects a distinct trait of a face; the descriptor (the second axis) is used to describe such face trait; and the similarity between two features can be measured by a certain kind of similarity measure (the third axis). The experiment adopts the extensively-used Labeled Face in the Wild (LFW) unconstrained face recognition dataset (13K faces), and our proposed PDSS approach achieves the best result, compared with the state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {961–964},
numpages = {4},
keywords = {context-based partial matching, unconstrained, patch-descriptor-similarity space, face verification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396358,
author = {Revaud, Jerome and Douze, Matthijs and Schmid, Cordelia},
title = {Correlation-Based Burstiness for Logo Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396358},
doi = {10.1145/2393347.2396358},
abstract = {Detecting logos in photos is challenging. A reason is that logos locally resemble patterns frequently seen in random images. We propose to learn a statistical model for the distribution of incorrect detections output by an image matching algorithm. It results in a novel scoring criterion in which the weight of correlated keypoint matches is reduced, penalizing irrelevant logo detections. In experiments on two very different logo retrieval benchmarks, our approach largely improves over the standard matching criterion as well as other state-of-the-art approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {965–968},
numpages = {4},
keywords = {burstiness, correlation, image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396359,
author = {Wu, Chun-Che and Kuo, Yin-Hsi and Hsu, Winston},
title = {Large-Scale Simultaneous Multi-Object Recognition and Localization via Bottom up Search-Based Approach},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396359},
doi = {10.1145/2393347.2396359},
abstract = {Recently, smart phones not only perform the basic communication function but also become the first choice in information collection. For instance, when smartphone users want to obtain relevant information about the products on the shelf, all they have to do is take a snapshot and send it back to the server. In order to save time and effort for the users, it is important to retrieve information as many as possible from one shot. Thus, multiple object recognition and localization over large-scale object classes (database) is the first bottleneck to break through. To tackle this issue, we propose a bottom up search-based approach, which localizes the grid-based search candidates in Markov Random Field (MRF). The proposed approach enables simultaneously recognizing and localizing multiple objects; therefore, it reduces response time and ensures the accuracy as well. Experimental results show that the proposed method can have 40% relative improvement over the state-of-the-art bag-of-words model. We also demonstrate the proposed method in two datasets and show that our method can have good improvement in running time (5 times faster), and also competitive accuracy for multi-object recognition and localization.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {969–972},
numpages = {4},
keywords = {algorithm, multi-object recognition, markov random field},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396360,
author = {Zhou, Rong and Chen, Liuli and Zhang, Liqing},
title = {Sketch-Based Image Retrieval on a Large Scale Database},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396360},
doi = {10.1145/2393347.2396360},
abstract = {The paper presents a simple and effective sketch-based algorithm for large scale image retrieval. One of the main challenges in image retrieval is to localize a region in an image which would be matched with the query image in contour. To tackle this problem, we use the human perception mechanism to identify two types of regions in one image: the first type of region (the main region) is defined by a weighted center of image features, suggesting that we could retrieve objects in images regardless of their sizes and positions. The second type of region, called region of interests (ROI), is to find the most salient part of an image, and is helpful to retrieve images with objects similar to the query in a complicated scene. So using the two types of regions as candidate regions for feature extraction, our algorithm could increase the retrieval rate dramatically. Besides, to accelerate the retrieval speed, we first extract orientation features and then organize them in a hierarchal way to generate global-to-local features. Based on this characteristic, a hierarchical database index structure could be built which makes it possible to retrieve images on a very large scale image database online. Finally a real-time image retrieval system on 4.5 million database is developed to verify the proposed algorithm. The experiment results show excellent retrieval performance of the proposed algorithm and comparisons with other algorithms are also given.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {973–976},
numpages = {4},
keywords = {contour, sketch, saliency, hierarchical retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396361,
author = {Borth, Damian and Ulges, Adrian and Breuel, Thomas M.},
title = {Dynamic Vocabularies for Web-Based Concept Detection by Trend Discovery},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396361},
doi = {10.1145/2393347.2396361},
abstract = {We present a novel approach towards automatic vocabulary selection for video concept detection. Our key idea is to expand concept vocabularies with trending topics that we mine automatically on other media like Wikipedia or Twitter. We evaluate several strategies for extending concept detection to auto-detect these topics in new videos, either by linking them to a static concept vocabulary, by a visual learning of trends on the fly, or by an expansion of the vocabulary.Our study on 6,800 YouTube clips and the top 23 target trends (covering a timespan of 6 months) demonstrates that a direct visual classification of trends (by a "live" learning on trend videos) outperforms an inference from static vocabularies. However, further improvements can be achieved by a combination of both approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {977–980},
numpages = {4},
keywords = {concept detection, vocabulary, social media, trends},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396362,
author = {Wang, Meng and Ishwar, Prakash and Konrad, Janusz and Gazen, Cenk and Saboo, Rohit},
title = {Coherent Image Selection Using a Fast Approximation to the Generalized Traveling Salesman Problem},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396362},
doi = {10.1145/2393347.2396362},
abstract = {Searching for images on-line using keywords returns results that are often difficult to interpret. This becomes even more complicated if one attempts to compare image search output for several keywords with a common theme. We focus on the latter problem and propose a method to efficiently compare sets of images in order to find representative images, one from each set, that are coherent in certain sense. However, the search for an optimal set of representative images is very complex even for as few as 10 sets of 20 images each since all possible combinations of 10 images need to be considered. Therefore, we formulate our problem as the Generalized Traveling Salesman Problem (GTSP) and propose an efficient approximation algorithm to solve it. Our approximate GTSP algorithm is faster than other well-known approximations and is also more likely to reach the exact solution for large-scale inputs. We present a number of experimental results using the proposed algorithm and conclude that it can be a useful, almost real-time tool for on-line search.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {981–984},
numpages = {4},
keywords = {image retrieval, visualization, general traveling salesman problem (gtsp), image ranking},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396363,
author = {Tolchinsky, Pancho and Chiarandini, Luca and Jaimes, Alejandro},
title = {PRiSMA: Searching Images in Parallel},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396363},
doi = {10.1145/2393347.2396363},
abstract = {PRiSMA is an image search application for tablet and desktop devices intended to facilitate and promote the searching of images in parallel. With an intuitive user interface, users can branch their queries into multiple horizontal sliding strips to simultaneously explore different perspectives of large image collections (e.g., colors, geographical location or topic). Strips can be easily created, tailored, merged, and removed, allowing users to effectively perform multiple queries and manage the results in a dynamic and orderly fashion. With PRiSMA we aim to explore the potential and limitations of parallel image search from a user perspective.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {985–988},
numpages = {4},
keywords = {image browsing, parallel search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396364,
author = {Wu, Yue and Lu, Shiyang and Mei, Tao and Zhang, Jian and Li, Shipeng},
title = {Local Visual Words Coding for Low Bit Rate Mobile Visual Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396364},
doi = {10.1145/2393347.2396364},
abstract = {Mobile visual search has attracted extensive attention for its huge potential for numerous applications. Research on this topic has been focused on two schemes: sending query images, and sending compact descriptors extracted on mobile phones. The first scheme requires about 30-40KB data to transmit, while the second can reduce the bit rate by 10 times. In this paper, we propose a third scheme for extremely low bit rate mobile visual search, which sends compressed visual words consisting of vocabulary tree histogram and descriptor orientations rather than descriptors. This scheme can further reduce the bit rate with few extra computational costs on the client. Specifically, we store a vocabulary tree and extract visual descriptors on the mobile client. A light-weight pre-retrieval is performed to obtain the visited leaf nodes in the vocabulary tree. The orientation of each local descriptor and the tree histogram are then encoded to be transmitted to server. Our new scheme transmits less than 1KB data, which reduces the bit rate in the second scheme by 3 times, and obtains about 30% improvement in terms of search accuracy over the traditional Bag-of-Words baseline. The time cost is only 1.5 secs on the client and 240 msecs on the server.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {989–992},
numpages = {4},
keywords = {visual word, vocabulary tree, compression, mobile visual search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396365,
author = {Huitl, Robert and Schroth, Georg and Hilsenbeck, Sebastian and Schweiger, Florian and Steinbach, Eckehard},
title = {Virtual Reference View Generation for CBIR-Based Visual Pose Estimation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396365},
doi = {10.1145/2393347.2396365},
abstract = {Determining the pose of a mobile device based on visual information is a promising approach to solve the indoor localization problem. We present an approach that transforms localized images along a mapping trajectory into virtual viewpoints that cover a set of densely sampled camera positions and orientations in a confined environment. The viewpoints are represented by their respective bag-of-features vectors and image retrieval techniques are applied to determine the most likely pose of query images at very low computational complexity. As virtual image locations and orientations are decoupled from actual image locations, the system is able to work with sparse reference imagery and copes well with perspective distortion. Experiments confirm that pose retrieval performance is significantly improved.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {993–996},
numpages = {4},
keywords = {visual localization, synthetic view, virtual view, image retrieval, pose estimation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396366,
author = {Huang, Yen-Ta and Chen, Kuan-Ting and Hsieh, Liang-Chi and Hsu, Winston and Su, Ya-Fan},
title = {Detecting the Directions of Viewing Landmarks for Recommendation by Large-Scale User-Contributed Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396366},
doi = {10.1145/2393347.2396366},
abstract = {In this work, we try to estimate the viewing directions of photos to browse landmarks with different views. To detect the viewing directions of the photos, we propose to leverage tags and geo-information associated with user-contributed photos to predict the locations of the landmarks. By utilizing the tags along with these photos, we demonstrate that the deviation between the estimated locations and the landmarks is about 30 meters on average. The detected locations of the landmarks can be used to further estimate the viewing directions in which the photos were taken. In the experiments, we show that the average error to the real viewing directions is less than 14 degrees. The estimated viewing directions enable illustrating landmarks from different views and recommending users the popular views for taking photos on the landmarks.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {997–1000},
numpages = {4},
keywords = {photo aesthetics, viewing direction, landmark, location},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396367,
author = {Yap, Kim-Hui and Li, Zhen and Zhang, Da-Jiang and Ng, Zhan-Ke},
title = {Efficient Mobile Landmark Recognition Based on Saliency-Aware Scalable Vocabulary Tree},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396367},
doi = {10.1145/2393347.2396367},
abstract = {In recent years, the Scalable Vocabulary Tree (SVT) has been shown to be effective in image recognition. However, in mobile landmark image recognition where the foreground is the landmark to be recognized while the background is cluttered, the current SVT framework ignores different local importance of image, hence restricting its performance. In this paper, we propose a new landmark recognition framework that can incorporate saliency information to improve the recognition performance relative to the baseline SVT method. Specifically, the saliency information is incorporated in three phases: image descriptor calculation, vocabulary tree generation, and image representation. We constructed a city-scale landmark dataset in Singapore, and the experimental results show that the proposed mobile landmark recognition by incorporating saliency information outperforms the baseline SVT recognition by about 9%.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1001–1004},
numpages = {4},
keywords = {scalable vocabulary tree, saliency map, mobile landmark recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396368,
author = {Kharitonova, Yekaterina and Tung, Qiyam and Danehy, Alexander and Efrat, Alon and Barnard, Kobus},
title = {Client-Side Backprojection of Presentation Slides into Educational Video},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396368},
doi = {10.1145/2393347.2396368},
abstract = {A significant part of many videos of lectures is presentation slides that occupy much of the field of view. Further, for a student studying the lecture, having the slides sharply displayed is especially important, compared with the speaker, background, and audience. However, even if the original capture supports it, the bandwidth required for real time viewing is substantive, especially in the context of mobile devices. Here we propose reconstructing the video on the client side by backprojecting high resolution slide images into the video stream with the slide area blacked out. The high resolution slide deck can be sent once, and inserted into the video on the client side based on the transformation (a homography) computed in advance. We further introduce the idea that needed homography transformations can be approximated using affine transformations, which allows it to be done using built-in capabilities of HTML 5. We find that it is possible to significantly reduce bandwidth by compressing the modified video, while improving the slide area quality, but leaving the non-slide area roughly the same.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1005–1008},
numpages = {4},
keywords = {homography, presentation slides, lecture video, backprojection, affine transformation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396369,
author = {Bernardo, Marco V. and Pinheiro, Ant\'{o}nio M. G. and Pereira, Manuela and Fiadeiro, Paulo Torr\~{a}o},
title = {A Study on the User Perception to Color Variations},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396369},
doi = {10.1145/2393347.2396369},
abstract = {A study on the perceived quality of images displayed with color changes is presented. Under the D65 standard illuminant colors are changed in the CIE 1976 (L*a*b*) color space, with the application of a predefined chromatic error ΔE*ab. The colors were initially divided into clusters with the K-Means algorithm. Each color cluster is shifted by the predefined chromatic error with a random direction in a*b* chromatic coordinates. Applying the ΔE*ab errors of 3, 6, 9, 12 and 15 units to the five hyperspectral images a set of modified images was collected. Those images were shown to individuals, that were asked to rank those images quality based on their naturalness. The Medium Opinion Scores was computed and allowed to test and quantify the sensibility to color changes.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1009–1012},
numpages = {4},
keywords = {color, assessment, quality, mean opinion score},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396370,
author = {Pieters, Bart and Hollemeersch, Charles and De Cock, Jan and De Neve, Wesley and Lambert, Peter and Van de Walle, Rik},
title = {Parallel Deblocking Filtering in H.264/AVC Using Multiple CPUs and GPUs},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396370},
doi = {10.1145/2393347.2396370},
abstract = {Deblocking filtering in the H.264/AVC standard is a computationally complex process because of the filter's high content adaptivity. Furthermore, the deblocking filter introduces a significant number of data dependencies, making parallel processing not obvious. Our previous works analyzed the dependencies of the filter and proposed a massively-parallel implementation, specifically tailored for execution on a single GPU. In this paper, we extend this work by proposing a parallel processing scheme for accelerating deblocking filtering using multiple CPU cores or GPUs. This scheme allows for standard-compliant filtering, regardless of slice configuration. Results show that our multi-GPU implementation using our proposed scheme achieves faster-than real-time deblocking at over 3794 frames per second for 1080p video pictures by using three GPUs. A multi-core CPU implementation using 8 CPU cores allows 1080p deblocking filtering of up to 695 frames per second.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1013–1016},
numpages = {4},
keywords = {parallel decoding, multi core, h.264/avc deblocking, gpu},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396371,
author = {Hosseini, Mohammad and Fedorova, Alexandra and Peters, Joseph and Shirmohammadi, Shervin},
title = {Energy-Aware Adaptations in Mobile 3d Graphics},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396371},
doi = {10.1145/2393347.2396371},
abstract = {Smartphone devices are becoming the de facto personal computing platform, rivaling the desktop, as the number of smartphone users is projected to reach 1.1 billion by 2013. Unlike the desktop, smartphones have a constrained energy budget, which is further challenged by increasingly sophisticated applications. Amongst the most popular applications on smartphone devices are games and virtual environments that rely on 3D graphics. Due to the computational intensity of geometry and rasterization, as well as the perpetually illuminated display, these applications are extremely power-hungry. To prolong the battery life of devices running these applications, we propose two new energy-aware adaptation schemes that can be employed in 3D graphics applications: lighting limitation and textural transformation. Our results show that we can conserve between 20% and 33% of energy with acceptable sacrifices to a user's visual experience.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1017–1020},
numpages = {4},
keywords = {energy-efficient rendering, mobile 3d graphics, graphics adaptation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396372,
author = {Nguyen, Viet Anh and Vu, Tien Dung and Yang, Hongsheng and Lu, Jiangbo and Do, Minh N.},
title = {ITEM: Immersive Telepresence for Entertainment and Meetings with Commodity Setup},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396372},
doi = {10.1145/2393347.2396372},
abstract = {This paper presents an Immersive Telepresence system for Entertainment and Meetings (ITEM). The system aims to provide a radically new video communication experience by seamlessly merging participants into the same virtual space to allow a natural interaction among them and shared collaborative contents. With the goal to make a scalable, flexible system for various business solutions as well as easily accessible by massive consumers, we address the challenges in the whole pipeline of media processing, communication, and displaying in our design and realization of such a system. Extensive experiments show the developed system runs reliably and comfortably in real time with a minimal setup requirement (e.g., a webcam, a laptop/desktop connected to the public Internet) for tele-immersive video communication. With such a really minimal deployment requirement, we present a variety of interesting applications and user experiences created by ITEM.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1021–1024},
numpages = {4},
keywords = {tele-immersive system, video conferencing, video object cutout},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396373,
author = {Shen, Zhijie and Zimmermann, Roger},
title = {Reducing Cross-Group Traffic with a Cooperative Streaming Architecture},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396373},
doi = {10.1145/2393347.2396373},
abstract = {Cooperative approaches, such as P2P networks, have demonstrated their effectiveness in video delivery. However, with underlay structure considered, it is still possible to further improve traffic efficiency. In this paper, we discuss the problem of localizing the traffic traversal across peer groups, which are partitioned according to underlay characteristics. We first provide three concrete examples to demonstrate this common challenge, which we theoretically formulate afterwards. Finally, we propose a ring overlay approach, which performs excellently to solve the problem, while tolerating peer dynamics and supporting peer heterogeneity.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1025–1028},
numpages = {4},
keywords = {cooperative networks, peer-to-peer, traffic locality, wireless},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396374,
author = {Ji, Wen and Li, Zhu and Chen, Yiqiang},
title = {QoE-Based Opportunistic Transmission for Video Broadcasting in Heterogeneous Circumstance},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396374},
doi = {10.1145/2393347.2396374},
abstract = {This paper presents an opportunistic transmission scheme for layered video broadcasting to multiple heterogeneous devices. In contrast to conventional wireless video broadcasting system, the main ideas proposed here include: (i) exploit the quality of heterogeneous user experience (QoE) metric under wireless broadcasting scenario, with consideration of various channel state, device capability, video content urgency and the number of demanding users. (ii) formulate reliable multiple video streams broadcasting to heterogeneous devices as an aggregate maximum utility achieving problem. (iii) use opportunistic scheduling to select suitable users in each transmission interval so as to improve the broadcasting utility. (iv) use parallel-pipe structure to transmit the layered video with Fountain coding protection, which provide reliable and low-latency transmission in heterogeneous circumstance. Numerical experiments demonstrate that the proposed scheme outperforms conventional methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1029–1032},
numpages = {4},
keywords = {broadcasting, heterogeneous, opportunistic, video},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396375,
author = {Calagari, Kiana and Pakravan, Mohammad Reza and Shirmohammadi, Shervin},
title = {ROI-Based Protection Scheme for High Definition Interactive Video Applications},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396375},
doi = {10.1145/2393347.2396375},
abstract = {In this work, first, a Region of Interest (ROI)-based Unequal Loss Protection (ULP) scheme with no delay is proposed for High Definition (HD) interactive video applications such as video calls. The proposed scheme uses Data Partitioning and Flexible Macro block Ordering (FMO) tools of H.264/AVC and reduces error propagation. The obtained data demonstrates that this scheme achieves better ROI quality than others in high Packet Loss Rates (PLR). Second, it is also shown that the efficient scheme for protecting ROI-based interactive video applications in wide PLR ranges is switching both the Forward Error Correction (FEC) distribution pattern and the codec configuration between various scenarios depending on network conditions. This is shown by using both PSNR and SSIM metrics for quality measurement.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1033–1036},
numpages = {4},
keywords = {error propagation, fmo, data partitioning, fec, hd video, h.264/avc, roi, ulp, interactive video, slicing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396376,
author = {Wirth, Thomas and S\'{a}nchez, Yago and Holfeld, Bernd and Schierl, Thomas},
title = {Advanced Downlink LTE Radio Resource Management for HTTP-Streaming},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396376},
doi = {10.1145/2393347.2396376},
abstract = {Video traffic contributes to the majority of data packets transported over cellular wireless. Future broadband wireless access networks based on 3GPP's Long Term Evolution offer mechanisms for optimized transmission with high data rates and low delay. However, especially when packets are transmitted in the LTE downlink and if services are run over-the-top (OTT), optimization of radio resources in a multi-user environment for video services becomes infeasible. The current market trend is moving to OTT solutions, also for video transmission, where an emerging standard based on HTTP streaming - DASH - is expected to have a huge success in the upcoming years. The solution presented in this paper consists of a novel technique, which combines LTE features with knowledge on DASH sessions for optimization of the wireless resources. The combined optimization yields an improved transmission of videos over cellular wireless systems which are based on LTE and LTE-Advanced.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1037–1040},
numpages = {4},
keywords = {radio resource management, cross-layer design, http-streaming, lte/lte-a, dash},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246412,
author = {Panchanathan, Sethuraman},
title = {Session Details: Poster Session 3},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246412},
doi = {10.1145/3246412},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396378,
author = {Xu, Mengdi and Ni, Bingbing and Dong, Jian and Huang, Zhongyang and Wang, Meng and Yan, Shuicheng},
title = {Touch Saliency},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396378},
doi = {10.1145/2393347.2396378},
abstract = {In this work, we propose a new concept of touch saliency, and attempt to answer the question of whether the underlying image saliency map may be implicitly derived from the accumulative touch behaviors (or more specifically speaking, zoom-in and panning manipulations) when many users browse the image on smart mobile devices with multi-touch display of small size. The touch saliency maps are collected for the images of the recently released NUSEF dataset, and the preliminary comparison study demonstrates: 1) the touch saliency map is highly correlated with human eye fixation map for the same stimuli, yet compared to the latter, the touch data collection is much more flexible and requires no cooperation from users; and 2) the touch saliency is also well predictable by popular saliency detection algorithms. This study opens a new research direction of multimedia analysis by harnessing human touch information on increasingly popular multi-touch smart mobile devices.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1041–1044},
numpages = {4},
keywords = {touch saliency, visual saliency, fixations},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396379,
author = {Yang, Yang and Yang, Yi and Huang, Zi and Liu, Jiajun and Ma, Zhigang},
title = {Robust Cross-Media Transfer for Visual Event Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396379},
doi = {10.1145/2393347.2396379},
abstract = {In this paper, we present a novel approach, named Robust Cross-Media Transfer  (RCMT), for visual event detection in social multimedia environments. Different from most existing methods, the proposed method can directly take different types of noisy social multimedia data as input and conduct robust event detection. More specifically, we build a robust model by employing an l2,1-norm regression model featuring noise tolerance, and also manage to integrate different types of social multimedia data by minimizing the distribution difference among them. Experimental results on real-life Flickr image dataset and YouTube video dataset demonstrate the effectiveness of our proposal, compared to state-of-the-art algorithms.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1045–1048},
numpages = {4},
keywords = {cross-media, visual event detection, transfer learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396380,
author = {Yu, Gang and Yuan, Junsong and Liu, Zicheng},
title = {Predicting Human Activities Using Spatio-Temporal Structure of Interest Points},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396380},
doi = {10.1145/2393347.2396380},
abstract = {Early recognition and prediction of human activities are of great importance in video surveillance, e.g., by recognizing a criminal activity at its beginning stage, it is possible to avoid unfortunate outcomes. We address early activity recognition by developing a Spatial-Temporal Implicit Shape Model (STISM), which characterizes the space-time structure of the sparse local features extracted from a video. The early recognition of human activities is accomplished by pattern matching through STISM. To enable efficient and robust matching, we propose a new random forest structure, called multi-class balanced random forest, which makes a good trade-off between the balance of the trees and the discriminative abilities. The prediction is done simultaneously for multiple classes, which saves both the memory and computational cost. The experiments show that our algorithm significantly outperforms the state of the arts for the human activity prediction problem.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1049–1052},
numpages = {4},
keywords = {random forest, action prediction, hough voting},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396381,
author = {Lin, Yan-Ching and Hu, Min-Chun and Cheng, Wen-Huang and Hsieh, Yung-Huan and Chen, Hong-Ming},
title = {Human Action Recognition and Retrieval Using Sole Depth Information},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396381},
doi = {10.1145/2393347.2396381},
abstract = {Observing the widespread use of Kinect-like depth cameras, in this work, we investigate into the problem of using sole depth data for human action recognition and retrieval in videos. We proposed the use of simple depth descriptors without learning optimization to achieve promising performances as compatible to those of the leading methods based on color images and videos, and can be effectively applied for real-time applications. Because of the infrared nature of depth cameras, the proposed approach will be especially useful under poor lighting conditions, e.g. the surveillance environments without sufficient lighting. Meanwhile, we proposed a large Depth-included Human Action video dataset, namely DHA, which contains 357 videos of performed human actions belonging to 17 categories. To the best of our knowledge, the DHA is one of the largest depth-included video datasets of human actions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1053–1056},
numpages = {4},
keywords = {human action recognition, human action video retrieval, depth information},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396382,
author = {Yang, Xiaodong and Zhang, Chenyang and Tian, YingLi},
title = {Recognizing Actions Using Depth Motion Maps-Based Histograms of Oriented Gradients},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396382},
doi = {10.1145/2393347.2396382},
abstract = {In this paper, we propose an effective method to recognize human actions from sequences of depth maps, which provide additional body shape and motion information for action recognition. In our approach, we project depth maps onto three orthogonal planes and accumulate global activities through entire video sequences to generate the Depth Motion Maps (DMM). Histograms of Oriented Gradients (HOG) are then computed from DMM as the representation of an action video. The recognition results on Microsoft Research (MSR) Action3D dataset show that our approach significantly outperforms the state-of-the-art methods, although our representation is much more compact. In addition, we investigate how many frames are required in our framework to recognize actions on the MSR Action3D dataset. We observe that a short sub-sequence of 30-35 frames is sufficient to achieve comparable results to that operating on entire video sequences.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1057–1060},
numpages = {4},
keywords = {rgbd camera, feature representation, depth maps, 3d action recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396383,
author = {Lu, Jiwen and Hu, Junlin and Zhou, Xiuzhuang and Shang, Yuanyuan},
title = {Activity-Based Person Identification Using Sparse Coding and Discriminative Metric Learning},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396383},
doi = {10.1145/2393347.2396383},
abstract = {This paper presents a new activity-based person identification method using sparse coding and discriminative metric learning. Different from gait recognition where human walking activity is only utilized for person identification, we aim to recognize people from different activities such as running, jumping, skipping, and so on. For each activity video clip, we extract the binary human body mask using background substraction. Then, we cluster these body masks into a number of clusters by sparse coding with mean pooling to extract features for each video clip. Subsequently, we learn a discriminative distance metric under which intraclass (activities performed by the same person) variations are minimized and the interclass (activities performed by different persons) are maximized, simultaneously, such that more discriminative information can be exploited for recognition. Experimental results on a publicly available database are presented to show the efficacy of our proposed method.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1061–1064},
numpages = {4},
keywords = {metric learning, person identification, sparse coding},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396384,
author = {Althoff, Tim and Song, Hyun Oh and Darrell, Trevor},
title = {Detection Bank: An Object Detection Based Video Representation for Multimedia Event Recognition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396384},
doi = {10.1145/2393347.2396384},
abstract = {While low-level image features have proven to be effective representations for visual recognition tasks such as object recognition and scene classification, they are inadequate to capture complex semantic meaning required to solve high-level visual tasks such as multimedia event detection and recognition. Recognition or retrieval of events and activities can be improved if specific discriminative objects are detected in a video sequence. In this paper, we propose an image representation, called Detection Bank, based on the detection images from a large number of windowed object detectors where an image is represented by different statistics derived from these detections. This representation is extended to video by aggregating the key frame level image representations through mean and max pooling. We empirically show that it captures complementary information to state-of-the-art representations such as Spatial Pyramid Matching and Object Bank. These descriptors combined with our Detection Bank representation significantly outperforms any of the representations alone on TRECVID MED 2011 data.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1065–1068},
numpages = {4},
keywords = {object detection, multimedia event recognition, trecvid, representation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396385,
author = {Chu, Hang and Lin, Weiyao and Wu, Jianxin and Zhou, Xingtong and Chen, Yuanzhe and Li, Hongxiang},
title = {A New Heat-Map-Based Algorithm for Human Group Activity Recognition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396385},
doi = {10.1145/2393347.2396385},
abstract = {In this paper, a new heat-map-based (HMB) algorithm is proposed for human group activity recognition. The proposed algorithm first models people trajectories as series of "heat sources" and then applies a thermal diffusion process to create a heat map (HM) for representing the group activities. Based on this heat map, a new surface-fitting (SF) method is also proposed for recognizing human group activities. Our proposed HM feature can efficiently keep the temporal motion information of the group activities while the proposed SF method can effectively catch the characteristics of the heat map for activity recognition. Experimental results demonstrate the effectiveness of our proposed algorithm.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1069–1072},
numpages = {4},
keywords = {heat map, human group activity recognition, surface fitting},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396386,
author = {Yu, Qian and Liu, Jingen and Cheng, Hui and Divakaran, Ajay and Sawhney, Harpreet},
title = {Multimedia Event Recounting with Concept Based Representation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396386},
doi = {10.1145/2393347.2396386},
abstract = {Multimedia event detection has drawn a lot of attention in recent years. Given a recognized event, in this paper, we conduct a pilot study of the multimedia event recounting problem, which answers the question why this video is recognized as this event, i.e. what evidences this decision is made on. In order to provide a semantic recounting of the multimedia event, we adopt a concept-based event representation for learning a discriminative event model. Then, we present a recounting approach that exactly recovers the contribution of semantic evidence to the event classification decision. This approach can be applied on any additive discriminative classifiers. The promising result is shown on the MED11 dataset that contains 15 events in thousands of YouTube like videos.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1073–1076},
numpages = {4},
keywords = {multimedia event representation, textual descriptions of video content, multimedia event recounting},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396387,
author = {Tian, Gang and Guan, Genliang and Wang, Zhiyong and Feng, Dagan},
title = {What is Happening: Annotating Images with Verbs},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396387},
doi = {10.1145/2393347.2396387},
abstract = {Image annotation has been widely investigated to discover the semantics of an image. However, most of the existing algorithms focus on noun tags (e.g. concepts and objects). Since an image is a snapshot of the real world event, annotating images with verbs will enable richer understanding of an image. In this paper, we propose a data-driven approach to verb oriented image annotation. At first, we obtain verb candidates by generating search queries for a given image with initial noun tags and establishing a sentence corpus from those queries. We utilize visualness to filter tags which are not visually presentable (e.g. pain) and differentiate tags into two categories (i.e. scene based and object based) to impose linguistic rules in verb extraction. Then we further re-rank the candidate verbs with the tag context discovered from the images which are both semantically and visually similar to the given image in the MIRFlickr dataset. Our experimental results from user study demonstrate that our proposed approach is promising.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1077–1080},
numpages = {4},
keywords = {semantic similarity, image annotation, verb tag, re-ranking, visualness, visual similarity},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396388,
author = {Wu, Zhe and Li, Xiong and Zhao, Xu and Liu, Yuncai},
title = {Hybrid Generative-Discriminative Recognition of Human Action in 3D Joint Space},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396388},
doi = {10.1145/2393347.2396388},
abstract = {We propose a novel human action recognition method based on the generative feature mapping over 3D human body joint sequences. The proposed method relies on Hidden Markov Model (HMM), but differs from the previous methods in the way of incorporating HMM and discriminative classifier, aiming to capture more discriminative information. Firstly, we use HMMs to model the joint sequences of human body. Then the Posterior Divergence is used to build feature mappings from the trained HMMs. The derived feature mappings map a variable-length joint sequence to a fixed-dimension feature vector which will be delivered to SVM for classification. We evaluate the proposed method and related methods on a large number of 3D joint sequences. The experimental results show its competitive performance, in comparison with other state-of-the-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1081–1084},
numpages = {4},
keywords = {action recognition, posterior divergence},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396389,
author = {Zhang, Cong and Yang, Xiaokang and Zhu, Jun and Lin, Weiyao},
title = {Parsing Collective Behaviors by Hierarchical Model with Varying Structure},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396389},
doi = {10.1145/2393347.2396389},
abstract = {Collective behaviors are usually composed of several groups. Considering the interactions among groups, this paper presents a novel framework to parse collective behaviors for video surveillance applications. We first propose a latent hierarchical model (LHM) with varying structure to represent the behavior with multiple groups. Furthermore, we also propose a multi-layer-based (MLB) inference method, where a sample-based heuristic search (SHS) is introduced to infer the group affiliation. And latent SVM is adopted to learn our model. With the proposed LHM, not only are the collective behaviors detected effectively, but also the group affiliation in the collective behaviors is figured out. Experiment results demonstrate the effectiveness of the proposed framework.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1085–1088},
numpages = {4},
keywords = {sampling-based heuristic search, multi-layer-based inference method, collective behavior, latent hierarchical model},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396390,
author = {Connaghan, Damien and O'Connor, Noel},
title = {Toward next Generation Coaching Tools for Court Based Racquet Sports},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396390},
doi = {10.1145/2393347.2396390},
abstract = {Even with today's advances in automatic indexing of multimedia content, existing coaching tools for court sports lack the ability to automatically index a competitive match into key events. This paper proposes an automatic event indexing and event retrieval system for tennis, which can be used to coach from beginners upwards. Event indexing is possible using either visual or inertial sensing, with the latter potentially providing system portability. To achieve maximum performance in event indexing, multi-sensor data integration is implemented, where data from both sensors is merged to automatically index key tennis events. A complete event retrieval system is also presented to allow coaches to build advanced queries which existing sports coaching solutions cannot facilitate without an inordinate amount of manual indexing.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1089–1092},
numpages = {4},
keywords = {visual sensing, event indexing, inertial sensing, sports coaching},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396391,
author = {O'Hare, Neil and Aiello, Luca Maria and Jaimes, Alejandro},
title = {Predicting Participants in Public Events Using Stock Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396391},
doi = {10.1145/2393347.2396391},
abstract = {Pictures taken by journalists for distribution and for inclusion in stock photo collections are often enriched with metadata. One key aspect of such photos is that they focus largely on events and feature celebrities and other public figures. They may provide interesting insights into how such public figures are related to each other in terms of the events they attend, and in their social proximity in terms of how often they are photographed together. In this paper, we study a corpus of approximately 9 million stock photographs taken over a 10 year period and, using their metadata, we extract a social network from co-appearance of public figures in events depicted in the photographs. We exploit this latent social information and combine it with the rich image metadata to explore the possibility of predicting attendees at future events, showing promising performance for this task.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1093–1096},
numpages = {4},
keywords = {photo collections, public events, public figures social network},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396392,
author = {Li, Weihai and Yan, Yupeng and Yu, Nenghai},
title = {Breaking Row-Column Shuffle Based Image Cipher},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396392},
doi = {10.1145/2393347.2396392},
abstract = {In this paper, a redundancy based cipher-only attack is proposed to break row-column shuffle based image encryption algorithms, which are considered to be safe under cipher-only attack before although it is well known that they are fragile under known-plaintext attack. This attack is carried out on the shuffle operations itself by analyzing the redundancy remained in cipher image, and doesn't care how the shuffle tables are generated. So, no matter how the shuffling tables are generated, this attack is valid. Experimental results show high quality deciphered images from one single cipher image, and that demonstrate the validity of our attack method. This attack method is also a potential threat to shuffle-scramble combined encryptions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1097–1100},
numpages = {4},
keywords = {row-column shuffle, image encryption, cryptanalysis, redundancy},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396393,
author = {Li, Wei and Zhu, Bilei and Wang, Zhurong},
title = {On the Music Content Authentication},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396393},
doi = {10.1145/2393347.2396393},
abstract = {Digital audio has been ubiquitous over the past decade. Since it can be easily modified by editing tools, there has been a strong need to protect its content for secure multimedia applications. Existing audio authentication algorithms are mainly focused on either human speech or general audio with music as part of the test data, while special research on music authentication has been somewhat neglected. In this article, we propose a novel algorithm to protect the integrity and authenticity of music signals. Its main contributions include: (1) Music is segmented into beat-based frames, which not only endows the authentication units with more semantic meaning but also perfectly resolves the challenging synchronization problem; (2) Robust hashes are generated from Chroma-based mid-level audio feature which can appropriately characterize the music content, and integrated with an encryption procedure to ensure the security against malicious block-wise vector quantization attack; (3) Fuzzy logic is adopted to make the authentication decision in light of three measures defined on bit errors, coinciding with the inherent blurred nature of authentication. Experiments exhibit good discriminative ability between admissible and malicious operations.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1101–1104},
numpages = {4},
keywords = {fuzzy classification, tamper classification, authenticity degree, music content authentication, beat-based segmentation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396394,
author = {Mohanty, Manoranjan and Atrey, Pradeep and Ooi, Wei Tsang},
title = {Secure Cloud-Based Medical Data Visualization},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396394},
doi = {10.1145/2393347.2396394},
abstract = {Outsourcing the tasks of medical data visualization to cloud centers presents new security challenges. In this paper, we propose a framework for cloud-based remote medical data visualization that protects the security of data at the cloud centers. To achieve this, we integrate the cryptographic secret sharing with pre-classification volume ray-casting and propose a secure volume ray-casting pipeline that hides the color-coded information of the secret medical data during rendering at the data centers. Results and analysis show the utility of the proposed framework.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1105–1108},
numpages = {4},
keywords = {secret sharing, cloud computing, ray casting, 3d medical data visualization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396395,
author = {Zhou, Ke and Liu, Jin and Tian, Hui and Li, Chunhua},
title = {State-Based Steganography in Low Bit Rate Speech},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396395},
doi = {10.1145/2393347.2396395},
abstract = {Common least significant bit (LSB) relevant steganography methods in speech frames often base on bits evaluation by certain speech quality evaluation criterion and together with some coding and embedding strategies to enhance imperceptibility and efficiency. However, some embedding capabilities and security strategy are neglected. This paper proposes a state-based steganography method which fully investigates speech frame features in order to expand embedding capabilities and enhance steganography security. In the proposed method, embedding capabilities are measured by available numbers of states relative to current frame parameters rather than information bits. And secret information embedding procedure is performed by the chosen states mapped operations. This is a basic fine-grained steganography solution which is useful for other algorithms based on it. The experimental results have demonstrated that statebased method outperforms traditional LSB substitution in overall performance and introduces limited latency, which meets the realtime requirement in covert communications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1109–1112},
numpages = {4},
keywords = {state-based, low bit rate speech, steganography, least significant bit},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396396,
author = {Yin, Jing and Fang, Yanmei},
title = {Markov-Based Image Forensics for Photographic Copying from Printed Picture},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396396},
doi = {10.1145/2393347.2396396},
abstract = {Nowadays, photographic-copying technique is very popular along with the rapid development of the image-capturing device, especially digital camera. As a result, the recaptured images, i.e., images taken from real-scene images displayed on various medium, e.g., LCD screen, are used in illegal cases now and then. In this paper, by comparing the recaptured images with their corresponding real-scene images, we find the recapturing procedure changes the statistics of the images. Then the Markov process based features extracted from the Discrete Cosine Transform(DCT) coefficients array are proposed to characterize this changes. During experimentation, a large and typical image dataset, which consisted of 3994 real-scene images and 3994 recaptured images that are taken from printed pictures with diversified image contents and camera models, is build and used for training and testing the classifier Support Vector Machine(SVM). Experimental results show that the proposed forensics scheme performs very well and outperforms the state-of-art methods.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1113–1116},
numpages = {4},
keywords = {photographic copying, digital image forensics, svm, markov process},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396397,
author = {Ye, Conghuan and Ling, Hefei and Zou, Fuhao and Liu, Cong},
title = {Secure Content Sharing for Social Network Using Fingerprinting and Encryption in the TSH Transform Domain},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396397},
doi = {10.1145/2393347.2396397},
abstract = {In this paper, a JFE (joint fingerprinting and encryption) scheme based on the Tree-Structured Harr (TSH) transform is proposed with the purpose of protecting media distribution in social network environment. The motivation is to map hierarchical community structure of social network into tree structure of TSH transform for fingerprinting and encryption. Firstly, the fingerprint code is produced using social network analysis. Secondly, the content is decomposed by the TSH transform. Thirdly, the content is fingerprinted and encrypted in the TSH transform domain. At last, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first JFE method in the TSH transform domain using social network analysis. The use of fingerprinting along with encryption can provide a double-layer of protection to media sharing in social network environment. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1117–1120},
numpages = {4},
keywords = {fingerprinting, tsh, media sharing, collusion attack, encryption, social network},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396398,
author = {Cristani, Marco and Roffo, Giorgio and Segalin, Cristina and Bazzani, Loris and Vinciarelli, Alessandro and Murino, Vittorio},
title = {Conversationally-Inspired Stylometric Features for Authorship Attribution in Instant Messaging},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396398},
doi = {10.1145/2393347.2396398},
abstract = {Authorship attribution (AA) aims at recognizing automatically the author of a given text sample. Traditionally applied to literary texts, AA faces now the new challenge of recognizing the identity of people involved in chat conversations. These share many aspects with spoken conversations, but AA approaches did not take it into account so far. Hence, this paper tries to fill the gap and proposes two novelties that improve the effectiveness of traditional AA approaches for this type of data: the first is to adopt features inspired by Conversation Analysis (in particular for turn-taking), the second is to extract the features from individual turns rather than from entire conversations. The experiments have been performed over a corpus of dyadic chat conversations (77 individuals in total). The performance in identifying the persons involved in each exchange, measured in terms of area under the Cumulative Match Characteristic curve, is 89.5%.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1121–1124},
numpages = {4},
keywords = {chat analysis, re-identification, authorship attribution},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396399,
author = {Sankoh, Hiroshi and Sugano, Masaru and Naito, Sei},
title = {Dynamic Camera Calibration Method for Free-Viewpoint Experience in Sport Videos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396399},
doi = {10.1145/2393347.2396399},
abstract = {In this paper, we propose a dynamic camera calibration and object extraction method for sport videos captured with a moving pan-tilt-zoom camera. Such technology realizes an immersive free-viewpoint experience whereby audiences can see real sport scenes from any viewpoint. Camera calibration and object extraction are two of the most important processes for rendering free-viewpoint video, since a 3-dimensional model of each object needs to be reconstructed in every frame based on objects' textures and camera parameters. Most conventional rendering methods only use static cameras whose camera parameters and background models change little. However, since the cameras have to be set apart widely enough to capture the entire scene, the resolution of each object becomes low and is not sufficient for rendering high-quality free-viewpoint video. In order to obtain the texture of an object in high resolution from a moving pan-tilt-zoom camera, our proposed method estimates camera parameters by identifying reliable corresponding feature points between video frames, and also extracts the precise textures of objects using estimated camera parameters. Experimental results revealed that the proposed method successfully estimated precise camera parameters compared to the conventional method. Furthermore, by applying our proposed approach, the free-viewpoint video was rendered without visual defects.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1125–1128},
numpages = {4},
keywords = {free-viewpoint video, object extraction, homography matrix, camera parameter, graph cut, calibration},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396400,
author = {Ruhl, Kai and Hell, Benjamin and Klose, Felix and Lipski, Christian and Petersen, S\"{o}ren and Magnor, Marcus},
title = {Improving Dense Image Correspondence Estimation with Interactive User Guidance},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396400},
doi = {10.1145/2393347.2396400},
abstract = {High quality dense image correspondence estimation between two images is an essential pre-requisite for view interpolation in visual media production. Due to the ill-posed nature of the problem, automated estimation approaches are prone to erroneous correspondences and subsequent quality degradation, e.g. in the presence of ambiguous movements that require human scene understanding to resolve. Where visually convincing results are essential, artifacts resulting from estimation errors must be repaired by hand with image editing tools. In this paper, we propose a new workflow alternative by fixing the correspondences instead of fixing the interpolated images. We combine realtime interactive correspondence display, multi-level user guidance and algorithmic subpixel precision to counteract failure cases of automated estimation algorithms. Our results show that already few interactions improve the visual quality considerably.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1129–1132},
numpages = {4},
keywords = {user input, dense image correspondence estimation, view interpolation, interactive, optical flow},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396401,
author = {Chen, Bisheng and Wang, Jingdong and Huang, Qinghua and Mei, Tao},
title = {Personalized Video Recommendation through Tripartite Graph Propagation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396401},
doi = {10.1145/2393347.2396401},
abstract = {The rapid growth of the number of videos on the Internet provides enormous potential for users to find content of interest to them. Video search, such as Google, Youtube, Bing, is a popular way to help users to find desired videos. However, it is still very challenging to discover new video contents for users. In this paper, we address the problem of providing personalized video suggestions for users. Rather than only exploring the user-video graph that is formulated using the click-through information, we also investigate other two useful graphs, the user-query graph indicating if a user ever issues a query, and the query-video graph indicating if a video appears in the search result of a query. The two graphs act as a bridge to connect users and videos, and have a large potential to improve the recommendation as the queries issued by a user essentially imply his interest. As a result, we reach a tripartite graph over (user, video, query). We develop an iterative propagation scheme over the tripartite graph to compute the preference information of each user. Experimental results on a dataset of 2,893 users, 23,630 queries and 55,114 videos collected during Feb. 1-28, 2011 demonstrate that the proposed method outperforms existing state-of-the-art approaches, co-views and random walks on the user-video bipartite graph.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1133–1136},
numpages = {4},
keywords = {personalized video recommendation, tripartite graph},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396402,
author = {Hidayati, Shintami C. and Cheng, Wen-Huang and Hua, Kai-Lung},
title = {Clothing Genre Classification by Exploiting the Style Elements},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396402},
doi = {10.1145/2393347.2396402},
abstract = {This paper presents a novel approach to automatically classify the upperwear genre from a full-body input image with no restrictions of model poses, image backgrounds, and image resolutions. Five style elements, that are crucial for clothing recognition, are identified based on the clothing design theory. The corresponding features of each of these style elements are also designed. We illustrate the effectiveness of our approach by showing that the proposed algorithm achieved overall precision of 92.04%, recall of 92.45%, and F score of 92.25% with 1,077 clothing images crawled from popular online stores.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1137–1140},
numpages = {4},
keywords = {style element, clothing genre, classification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396403,
author = {You, Chuang-Wen and Hsieh, Yung-Huan and Cheng, Wen-Huang},
title = {AttachedShock: Facilitating Moving Targets Acquisition on Augmented Reality Devices Using Goal-Crossing Actions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396403},
doi = {10.1145/2393347.2396403},
abstract = {The prevalence of augmented reality devices in our daily lives offers increasing opportunities for users to navigate the real world. However, as users move, on-screen targets move unpredictably, and eventually disappear from the screen in mobile navigation scenarios. The changing target movement pattern creates difficulty for users in selecting the targets on time before targets escape from the screen. This study proposes a novel target selecting technique, AttachedShock, for easing target selection tasks on augmented reality devices by crossing a naturally expanding wave pattern that is attached to targets. We evaluated the effectiveness of the proposed technique by conducting comparative studies on measuring the performance of four techniques under various mobile navigation scenarios. The results indicate that the proposed technique assists users in selecting moving targets to improve the error rate substantially, by a minimum of 61.75%, and incurs acceptable distractions to users, compared to other techniques.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1141–1144},
numpages = {4},
keywords = {moving target selection, target acquisition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396404,
author = {Yang, Xin and Liao, Chunyuan and Liu, Qiong},
title = {MixPad: Augmenting Interactive Paper with Mice &amp; Keyboards for Cross-Media and Fine-Grained Interaction with Documents},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396404},
doi = {10.1145/2393347.2396404},
abstract = {Existing interactive paper systems suffer from the disparate input devices for paper and computers. The finger-pen-only input on paper causes frequent devices switching (e.g. pen vs. mouse) during cross-media interactions, and may have issues of occlusion and precision. We propose MixPad, a novel interactive paper system, which allows users to exploit mice and keyboards to digitally manipulate fine-grained document content on paper, such as copying an arbitrary image region to a computer and clicking on a word for web search. With the combined input channels, MixPad enables richer digital functions on paper and facilitates bimanual operations cross different media. A preliminary user study shows positive feedback on this interaction technique.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1145–1148},
numpages = {4},
keywords = {fine-grained, mouse, document, keyboard, cross-media, interactive paper, bimanual},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396405,
author = {Mai, Long and Le, Hoang and Niu, Yuzhen and Lai, Yu-Chi and Liu, Feng},
title = {Detecting Rule of Simplicity from Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396405},
doi = {10.1145/2393347.2396405},
abstract = {Simplicity refers to one of the most important photography composition rules. Simplicity states that simplifying the image background can draw viewers' attention to the subject of interest in a photograph and help them better comprehend and appreciate it. Understanding whether a photo respects photography rules or not facilitates photo quality assessment. In this paper, we present a method to automatically detect whether a photo is composed according to the rule of simplicity. We design features according to the definition, implementation and effect of the rule. First, we make use of saliency analysis to infer the subject of interest in a photo and measure its compactness. Second, we segment an image into background and foreground and measure the homogeneity within the background as another feature. Third, when looking at an image created with the rule of simplicity, different viewers tend to agree on what the subject of interest is in this photo. We accordingly measure the consistency among various saliency detection results as a feature. We experiment with these features in a range of machine learning methods. Our experiments show that our methods, together with these features, provide an encouraging result in detecting the rule of simplicity in a photo.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1149–1152},
numpages = {4},
keywords = {photo quality assessment, photography rules detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396406,
author = {Yeh, Mei-Chen and Li, Po-Yi},
title = {An Approach to Automatic Creation of Cinemagraphs},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396406},
doi = {10.1145/2393347.2396406},
abstract = {A cinemagraph is a new type of medium that infuses a static image with the dynamics of one particular region. It is in many ways intermediate between a photograph and a video, and has a number of attractive potential applications, such as the creation of dynamic scenes for games and interactive environments. However, creating cinemagraphs is time consuming and requires certain level of proficiency on photo editing techniques. In this paper, we present a fully automatic approach that creates cinemagraphs from video sequences. Specifically, we view cinemagraph construction as a constrained optimization problem that seeks a sub-volume in video with the maximum cumulative flow fields. The problem can be efficiently solved by a branch and-bound search scheme. A user survey is conducted to understand user preferences and demonstrate the performance of the proposed approach. The findings of this study should provide information for various design choices for an easy and versatile authoring tool for cinemagraphs.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1153–1156},
numpages = {4},
keywords = {motion analysis, cinemagraph, user study},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396407,
author = {Yasunaga, Takuya and Nakazawa, Atsushi and Takemura, Haruo},
title = {Human-Computer Dance Interaction with Realtime Accelerometer Control},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396407},
doi = {10.1145/2393347.2396407},
abstract = {Motion-capture-based character animations are widely used in computer graphics and interactive games.In this paper, we show a novel approach to create dancing character animations that react to input music and an accelerometer manipulated by a user. Since the sensor reads express intensities of users' body movements, the system can synthesize character motions whose intensities are synchronized to those of users. Our system consists of analysis phase and synthesis phase. In the analysis phase, the musical beat and segments are detected from input sound, and motion rhythm and intensities are found from motion capture data. With the results of this analysis, we generate a motion graph that can generate character motions matched to the musical rhythm. In synthesis phase, the system receives the output data from an accelerometer and traverses the motion graph according to the matching result between the sensor data and the motion intensity. As the result, our system adds an interactive component to live dancing performed by virtual characters.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1157–1160},
numpages = {4},
keywords = {character animation, motion capture, music analysis, interactive system, accelerometer control},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396408,
author = {Komai, Yuto and Yang, Nan and Takiguchi, Tetsuya and Ariki, Yasuo},
title = {Robust AAM-Based Audio-Visual Speech Recognition against Face Direction Changes},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396408},
doi = {10.1145/2393347.2396408},
abstract = {As one of the techniques for robust speech recognition under noisy environments, audio-visual speech recognition (AVSR) using lip dynamic scene information together with audio information is attracting attention, and the research has advanced in recent years. However, in visual speech recognition (VSR), when a face turns sideways, the shape of the lip as viewed from the camera changes and the recognition accuracy degrades significantly. Therefore, many of the conventional VSR methods are limited to situations in which the face is viewed from the front. This paper proposes a VSR method to convert faces viewed from various directions into faces that are viewed from the front using Active Appearance Models (AAM). In the experiment, even when the face direction changes about 30 degrees relative to a frontal view, the recognition accuracy improved significantly.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1161–1164},
numpages = {4},
keywords = {audio-visual, speech recognition, face direction},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396409,
author = {Ochi, Daisuke and Kimata, Hideaki and Noto, Hajime and Kojima, Akira},
title = {A Study on Making Camera Trajectory from Panorama Watching Manipulation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396409},
doi = {10.1145/2393347.2396409},
abstract = {We propose a new interactive panorama video delivery system that enables users not only to select favorite parts of an event site, but also to watch repeatedly with stable and the user-selected camera trajectory. In this paper, we present a system that makes a favorable and stable camera trajectory as a scenario from user's manipulations of a tablet device in a format that can be easily shared with others. We also evaluate the validity of the camera trajectory results obtained our stabilization technique.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1165–1168},
numpages = {4},
keywords = {panorama video, tablet manipulation, video stabilization, camera trajectory},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396410,
author = {Takenaka, Kazuhito and Bando, Takashi and Nagasaka, Shogo and Taniguchi, Tadahiro},
title = {Drive Video Summarization Based on Double Articulation Structure of Driving Behavior},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396410},
doi = {10.1145/2393347.2396410},
abstract = {This paper provides a novel summarization method for drive videos using driving behavior, such as driver maneuvers and vehicle reaction, recorded simultaneously alongside video. We segmented the driving behavior into chunks via an unsupervised manner and summarized the drive videos using the chunks, i.e., the switching points of the chunks were emphasized and the middle of the chunks were compressed. As the result of subjective evaluation, we found that the chunks were more consistent with human-recognized driving context than the image-based method and that the summarized video was more suitable for reviewing entire driving scenes, i.e., our method achieved an efficient summarization.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1169–1172},
numpages = {4},
keywords = {drive video summarization, double articulation structure},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396411,
author = {Khosla, Rajiv and Chu, Mei-Tai and Kachouie, Reza and Yamada, Keiji and Yoshihiro, Fujita and Yamaguchi, Tomoharu},
title = {Interactive Multimodal Social Robot for Improving Quality of Care of Elderly in Australian Nursing Homes},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396411},
doi = {10.1145/2393347.2396411},
abstract = {This paper describes the design of multimodal robotic system, embodiment of multimodal interaction (voice, gestures, emotion, touch panel and dance) in assistive social robot (Matilda) for modeling group based and one-to-one interactions with technology adverse elderly in nursing homes. It describes the human-centered evaluation of Matilda based on quality, naturalness, user satisfaction and predictive accuracy based on first ever field trials in Australia. The multimodal social robots have facilitated breaking the technology barriers with the elderly leading to several aged care facilities and community centers showing interest in future trials.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1173–1176},
numpages = {4},
keywords = {old people, multimodal interaction, dementia, assitive social tobot},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396412,
author = {Xu, Lei and Fang, Yikai and Wang, Kongqiao and Li, Jiangwei},
title = {Plug&amp;Touch: A Mobile Interaction Solution for Large Display via Vision-Based Hand Gesture Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396412},
doi = {10.1145/2393347.2396412},
abstract = {Smart phones and external large displays are a superior combination for content sharing and multiple person interaction. However, the potential of such a combination is not fully explored due to the lack of effective and portable interaction solutions. To solve this problem, we present a novel mobile solution called Plug&amp;Touch, which is very easy to be set up and enables mobile users to directly and naturally operate on the large display surface. Plug&amp;Touch utilizes vision-based methods to detect the user's finger tip and then control user interface. The Local Binary Pattern asymmetric texture difference is proposed to extract the foreground region, and the coarse-to-fine intersection process is adopted to precisely localize the finger tip. Plug&amp;Touch has been implemented on the Symbian3 platform with two defined gestures - click and scroll. Experimental results and user evaluation has shown the practicability of Plug&amp;Touch.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1177–1180},
numpages = {4},
keywords = {free hand gesture, mobile phone, asymmetric texture difference, large display},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396413,
author = {Chen, Yingju and Lee, Jeongkyu},
title = {Ulcer Detection in Wireless Capsule Endoscopy Video},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396413},
doi = {10.1145/2393347.2396413},
abstract = {Wireless Capsule Endoscopy (WCE) is a painless and noninvasive technique that allows physicians to visualize the entire small bowel. Before WCE was introduced, examining the entire small bowel was impossible without surgical procedure. Although WCE is a technology breakthrough, the manual video diagnosis session is time consuming and is prone to human cognition errors. Therefore, gastroenterologists urge computer vision researchers to develop computer-aided diagnosis systems to assist the review session. In this paper, we would like to present an ulcer detection scheme that identifies ulcers in WCE video. A saliency map is created by means of channel mixer to highlight suspicious regions. Once the suspicious regions are identified, we extract textural features along the contour and classify them using a trained classifier for ulcer validation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1181–1184},
numpages = {4},
keywords = {wireless capsule endoscopy, ulcer detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396414,
author = {Grace, Lindsay D.},
title = {Critical Gameplay: Designing Games to Critique Convention},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396414},
doi = {10.1145/2393347.2396414},
abstract = {This paper discusses the critical gameplay design project and its newest evolution. After being exhibited in several international venues in 2009-2011, the project has evolved to employ a more pointed critique of game culture and how people play. The paper describes, Big Huggin', an affection game, Match, a game of procedural racism and Healer, and an unshooter about the Nanjing Massacre. These games continue to explore the notion of values in design.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1185–1188},
numpages = {4},
keywords = {game process workflow, game art, rapid game development, critical gameplay, game design, procedural rhetoric, critical design},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396415,
author = {Xu, Mingliang and Wu, Yunpeng and Ye, Yangdong},
title = {Smooth and Efficient Crowd Transformation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396415},
doi = {10.1145/2393347.2396415},
abstract = {Crowd transformation has been an important research field due to its diverse range of applications that include film production, computer games, robotics and performance training. We propose a novel approach with continuous space and continuous time for smooth crowd transformation. Most algorithms simulating the transformation of crowds focus on the trajectories of individual participants. As a contrast, the approach we proposed here focuses on the balanced assignment to maintain the collective behavior of the full crowd. We use a bitmap-based recognition of the starting and final formations and quantify the performance of the transformation with the mutual information. We demonstrate that our method is computationally efficient for several group sizes and formations.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1189–1192},
numpages = {4},
keywords = {social force model, mutual information, intelligent virtual agent, crowd simulation, crowd formation control, collective behavior},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396416,
author = {Myojin, Seiko and Sato, Arata and Shimada, Nobutaka},
title = {Augmented Reality Card Game Based on User-Specific Information Control},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396416},
doi = {10.1145/2393347.2396416},
abstract = {In this paper, we describe new way to use augmented reality (AR) for entertainment. AR is a technology that overlays virtual objects on real objects in real world as a natural and intuitive interface. We think AR also can provide multiple users with different sight in real world and control information displayed for each user. We use these AR features for entertainment to present another enjoyment for users. As an example of our proposal, we implemented a card game system and evaluated it to examine usefulness of our concept for entertainment.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1193–1196},
numpages = {4},
keywords = {augmented reality, game design, card game, user-specific information control, entertainment computing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396417,
author = {D'Amico, Gianpaolo and Del Bimbo, Alberto and Ferracani, Andrea and Landucci, Lea and Pezzatini, Daniele},
title = {Indoor and Outdoor Profiling of Users in Multimedia Installations},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396417},
doi = {10.1145/2393347.2396417},
abstract = {We present a work-in-progress interactive exhibit for the mu- seum of Onna (L'Aquila, Italy). The Onna Onlus and the citizens of Onna have the ambitious project to rebuild the town affected by the earthquake of April 2009 and to create a museum in memory of Onna. In addition to the tradi- tional fruition tools for museums, we have been asked for an interactive system capable to communicate the past events and the efforts invested. We are working on a multi-modal system composed of an indoor environment in which visitors can interact with a natural interface system and an outdoor module based on a cross-platform mobile application. A pro- filing method is also exploited in order to extract a profile of interest of each visitor and then use it to suggest in-depth personalized and geo-located information about the disaster and the local history via multimedia contents.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1197–1200},
numpages = {4},
keywords = {real-time location, interactive museums, RFID, multimedia installations, interaction design, location awareness, user interfaces, natural interaction, mobile application},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396418,
author = {Han, Yoon Chung and Han, Byeong-jun},
title = {Digiti Sonus: An Interactive Fingerprint Sonification},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396418},
doi = {10.1145/2393347.2396418},
abstract = {Fingerprints are one of the most unique visual patterns on human body. It represents both innate and acquired identities of an individual. In this paper, we focus on relationship between fingerprint patterns and human identities by transforming image into audio. Digiti Sonus, an interactive fingerprint sonification installation, contains a novel idea to facilitate and enhance an interactive auditory meaning by transforming user-intended fingerprint expression into audio spectrogram. In order to enable personalized sonification, the installation employed dynamic filter generation based on minutiae extraction using core-invariant scanning method and image skeletonization.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1201–1204},
numpages = {4},
keywords = {fingerprint sonification, heterogeneous data interpretation, fingerprint image processing, interactive multimedia art},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396419,
author = {McVeigh-Schultz, Joshua and Stein, Jen and Watson, Jeff and Fisher, Scott},
title = {Extending the Lifelog to Non-Human Subjects: Ambient Storytelling for Human-Object Relationships},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396419},
doi = {10.1145/2393347.2396419},
abstract = {In this paper, we describe an approach to lifelogging that positions everyday objects, vehicles, and built environments as worthy of their own lifelog systems. We use this approach to anchor what we call ambient storytelling, and we argue that this methodology opens up new opportunities to design for rich and enduring relationships between humans and non-humans. We will explore this research approach through a series of examples, including: (1) a building that learns about its occupants and reveals its lifelog through playful solicitation and reciprocation, (2) story-objects that reveal backstory to their users, and (3) an automotive-sensor system and lifelog platform that facilitates context-specific playful interaction between a driver and their car. In this last example, we were interested in how a vehicle-based lifelog could augment drivers' existing propensities to project character onto their cars. In each of these examples, we reposition the concept of the lifelog to consider the "lives" of objects and the relationship between humans and non-humans as a worthwhile area of research.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1205–1208},
numpages = {4},
keywords = {human-object relationship, interactive architecture, architecture, ambient storytelling, environmental lifelogging, vehicular lifelog, story objects, lifelog},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246413,
author = {Jaimes, Alejandro and Chua, Tat-seng},
title = {Session Details: Brave New Ideas Program},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246413},
doi = {10.1145/3246413},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396421,
author = {Singh, Vivek K. and Gao, Mingyan and Jain, Ramesh},
title = {Situation Recognition: An Evolving Problem for Heterogeneous Dynamic Big Multimedia Data},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396421},
doi = {10.1145/2393347.2396421},
abstract = {With the growth in social media, internet of things, and planetary-scale sensing there is an unprecedented need to assimilate spatio-temporally distributed multimedia streams into actionable information. Consequently the concepts like objects, scenes, and events, need to be extended to recognize situations (e.g. epidemics, traffic jams, seasons, flash mobs). This paper motivates and computationally grounds the problem of situation recognition. It describes a systematic approach for combining multimodal real-time big data into actionable situations. Specifically it presents a generic approach for modeling and recognizing situations. A set of generic building blocks and guidelines help the domain experts model their situations of interest. The created models can be tested, refined, and deployed into practice using a developed system (EventShop). Results of applying this approach to create multiple situation-aware applications by combining heterogeneous streams (e.g. Twitter, Google Insights, Satellite imagery, Census) are presented.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1209–1218},
numpages = {10},
keywords = {situation awareness, situation detection, events, situation, social networks, modeling, sensor networks},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396422,
author = {Bruni, Elia and Uijlings, Jasper and Baroni, Marco and Sebe, Nicu},
title = {Distributional Semantics with Eyes: Using Image Analysis to Improve Computational Representations of Word Meaning},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396422},
doi = {10.1145/2393347.2396422},
abstract = {The current trend in image analysis and multimedia is to use information extracted from text and text processing techniques to help vision-related tasks, such as automated image annotation and generating semantically rich descriptions of images. In this work, we claim that image analysis techniques can "return the favor" to the text processing community and be successfully used for a general-purpose representation of word meaning. We provide evidence that simple low-level visual features can enrich the semantic representation of word meaning with information that cannot be extracted from text alone, leading to improvement in the core task of estimating degrees of semantic relatedness between words, as well as providing a new, perceptually-enhanced angle on word semantics. Additionally, we show how distinguishing between a concept and its context in images can improve the quality of the word meaning representations extracted from images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1219–1228},
numpages = {10},
keywords = {visual words, semantics, language, object recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396423,
author = {Wang, Xin-Jing and Xu, Zheng and Zhang, Lei and Liu, Ce and Rui, Yong},
title = {Towards Indexing Representative Images on the Web},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396423},
doi = {10.1145/2393347.2396423},
abstract = {Even after 20 years of research on real-world image retrieval, there is still a big gap between what search engines can provide and what users expect to see. To bridge this gap, we present an image knowledge base, ImageKB, a graph representation of structured entities, categories, and representative images, as a new basis for practical image indexing and search. ImageKB is automatically constructed via a both bottom-up and top-down, scalable approach that efficiently matches 2 billion web images onto an ontology with millions of nodes. Our approach consists of identifying duplicate image clusters from billions of images, obtaining a candidate set of entities and their images, discovering definitive texts to represent an image and identifying representative images for an entity. To date, ImageKB contains 235.3M representative images corresponding to 0.52M entities, much larger than the state-of-the-art alternative ImageNet that contains 14.2M images for 0.02M synsets. Compared to existing image databases, ImageKB reflects the distributions of both images on the web and users' interests, contains rich semantic descriptions for images and entities, and can be widely used for both text to image search and image to text understanding.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1229–1238},
numpages = {10},
keywords = {image understanding, large-scale text to image translation, image knowledge base},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396424,
author = {Hanjalic, Alan and Kofler, Christoph and Larson, Martha},
title = {Intent and Its Discontents: The User at the Wheel of the Online Video Search Engine},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396424},
doi = {10.1145/2393347.2396424},
abstract = {We embrace the position of the user in the driver's seat of the video search engine by proposing a principled framework for multimedia retrieval that moves beyond what users are searching for also to encompass why they search. This 'why' is understood as the reason, purpose or immediate goal behind a user information need, which we identify as the underlying user 'intent'. Breaking information needs down into a topical dimension representing 'what' and an intent dimension representing 'why' will allow online video search engines to provide users with more satisfying search results. Until now, research on intent has remained small scale, limited by the lack of a systematic method for arriving at possible dimensions of user intent that provide productive areas of inquiry for multimedia research. We demonstrate how mining information from user descriptions of video information needs on the social Web makes it possible to identify useful intent categories for online video search and carry out validation experiments showing that these categories display enough invariance to be successfully modeled by a video search engine. In a final experiment, we demonstrate the potential for these categories to improve video retrieval with a large user study confirming that users associate salient differences within topically homogenous video search engine results lists with these intent categories. This reveals the potential to refine video results list using user intent.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1239–1248},
numpages = {10},
keywords = {user intent, search, multimedia information retrieval, multimedia indexing, retrieval algorithms},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246414,
author = {Kato, Hirokazu},
title = {Session Details: Technical Demo Session 1},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246414},
doi = {10.1145/3246414},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396426,
author = {Lin, Yuan and Lin, Qian and Tang, Feng and Wang, Shengjin},
title = {Face Replacement with Large-Pose Differences},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396426},
doi = {10.1145/2393347.2396426},
abstract = {In this paper, we present a novel face replacement system exchanging faces with large-pose differences. Traditional 2D image based face replacement can only replace faces with similar pose and appearance. This significantly limits the application of face replacement. In this paper, we propose to build a 3D head model from a single frontal face photo. The automatically constructed 3D head can be rendered under arbitrary poses and illuminations. This makes it possible to do swapping for faces with large pose variations. In the demo, the user captures a frontal face image using a capture device such as a webcam or a smartphone, and then the algorithm can automatically build the 3D model using feature detection, face alignment and reconstruction. This 3D model is used to swap to any other target face photo the user selects. While our system is automatic, we also provide interactive tools for the user to adjust the feature detection to enhance the results.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1249–1250},
numpages = {2},
keywords = {3D head reconstruction, multi-resolution spline, color transfer, face replacement},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396427,
author = {Tang, Feng and Tang, Hao and Tretter, Daniel R. and Lin, Qian},
title = {TouchPaper: Making Print Interactive},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396427},
doi = {10.1145/2393347.2396427},
abstract = {Traditional printed materials such as photobooks and collages are static in that the information conveyed is unchanging and limited to what was printed. In this paper, we describe an online service for augmenting prints with rich media by using image recognition. A print can have multiple hotspots with each one linking to different content on the web (e.g. Facebook, Youtube video, animations, etc.). When a print is viewed through a mobile device, it is automatically recognized and the interactive hot regions are highlighted on the screen. The user can click regions of interest to link to relevant content on the web. This technology significantly enhances the potential for personalization and interaction and ultimately the end user experience and the value of the prints. As an example application, we leverage our Facebook auto-photobook application to automatically create interactive links for each photo in a page. The user can easily view additional content such as photo comments for each photo through the augmentation mechanism.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1251–1252},
numpages = {2},
keywords = {interactive photobook, touchpaper, image recognition, augmented reality},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396428,
author = {Yang, Hongsheng and Sun, Huanliang and Lu, Jiangbo},
title = {Quicktoon: A Real-Time Video Stylization and Sharing System on General Processors},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396428},
doi = {10.1145/2393347.2396428},
abstract = {We present a video stylization and sharing system named QuickToon, which supports generating a variety of useful and pleasing visual effects and allows easy use in video chat or sharing via social networking services (SNS). Based on our highly efficient edge-preserving smoothing filter, non-photo-realistic video rendering effects can be generated on the fly such as skin beautification, cartoon-like rendition, object outline, pencil sketch, and color stroke. Without requiring any GPUs that would otherwise seriously limits portability, the QuickToon system runs comfortably in real time for VGA- or HD-sized videos on general processors such as CPUs. The system can take in video frames either from a live webcam feed or any photos/videos from the local store, while the transformed imagery can be used in a live Skype video call, or saved locally, or uploaded and shared to SNS with one click. We demonstrate with concrete examples the functionality of this system, and underline its utility in video communication and photo/video sharing.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1253–1254},
numpages = {2},
keywords = {video communication, photo sharing, video stylization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396429,
author = {Sun, Zhenbang and Wang, Changhu and Zhang, Liqing and Zhang, Lei},
title = {Sketch2Tag: Automatic Hand-Drawn Sketch Recognition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396429},
doi = {10.1145/2393347.2396429},
abstract = {In this work, we introduce the Sketch2Tag system for hand-drawn sketch recognition. Due to large variations presented in hand-drawn sketches, most of existing work was limited to a particular domain or limited predefined classes. Different from existing work, Sketch2Tag is a general sketch recognition system, towards recognizing any semantically meaningful object that a child can recognize. This system enables a user to draw a sketch on the query panel, and then provides real-time recognition results. To increase the recognition coverage, a web-scale clipart image collection is leveraged as the knowledge base of the recognition system. Better understanding a user's drawing will be of great value to a variety of applications, such as, improving the sketch-based image search by combining the recognition results as textual queries.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1255–1256},
numpages = {2},
keywords = {sketch-based image search, sketch recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396430,
author = {Qi, Xianbiao and Xiao, Rong and Zhang, Lei and Li, Chun-Guang and Guo, Jun},
title = {A Rapid Flower/Leaf Recognition System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396430},
doi = {10.1145/2393347.2396430},
abstract = {In this work, we introduce a rapid and accurate flower/leaf recognition system. The system could process one query in less than 0.35s with users' simple interaction. Meanwhile, high accuracy and recall is achieved. Furthermore, low computational resource and memory cost are required by the system. Now, the system is demonstrated on 172 categories of flowers, the largest flower dataset until now, and 220 categories of leaves.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1257–1258},
numpages = {2},
keywords = {leaf recognition, flower recognition, plant recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396431,
author = {Yeh, Mei-Chen and Li, Po-Yi},
title = {A Tool for Automatic Cinemagraphs},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396431},
doi = {10.1145/2393347.2396431},
abstract = {A cinemagraph is a new type of medium that infuses a static image with the dynamics of one or a few particular regions. It is in many ways intermediate between a photograph and a video, and provides a simple, yet expressive way to mix static and dynamic elements from a video clip. However, the process of creating cinemagraphs is usually tedious for end users and requires serious photo editing skills. In this demonstration we show a tool that creates cinemagraphs in a fully automatic manner. The technique should enable new features such as "intelligent cinemagraph mode" for digital cameras that provides an alternative method to capture the moment.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1259–1260},
numpages = {2},
keywords = {photography, cinemagraph},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396432,
author = {Lin, Yan-Ching and Hu, Min-Chun and Cheng, Wen-Huang and Hsieh, Yung-Huan and Chen, Hong-Ming},
title = {Actions Speak Louder than Words: Searching Human Action Video Based on Body Movement},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396432},
doi = {10.1145/2393347.2396432},
abstract = {Human action video search is a frequent demand in multimedia applications, and conventional video search schemes based on keywords usually fail to correctly find relevant videos due to noisy video tags. Observing the widespread use of Kinect-like depth cameras, we propose to search human action videos by directly performing the target action with body movements. Human actions are captured by Kinect and the recorded depth information is utilized to measure the similarity between the query action and each human action video in the database. We use representative depth descriptors without learning optimization to achieve real-time and promising performance as compatible as those of the leading methods based on color images and videos. Meanwhile, a large Depth-included Human Action video dataset, namely DHA, is collected to prove the effectiveness of the proposed video search system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1261–1262},
numpages = {2},
keywords = {human action recognition, depth information, human action video retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396433,
author = {Chen, Chi-Wen and Hu, Min-Chun and Cheng, Wen-Huang and Chang, Che-Han and Lai, Jui-Hsin and Wu, Ja-Ling},
title = {Action Tutor: Real-Time Exemplar-Based Sequential Movement Assessment with Kinect Sensor},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396433},
doi = {10.1145/2393347.2396433},
abstract = {With the aid of depth camera, such as Microsoft Kinect, the difficulty of vision-based posture estimation is greatly decreased, and human action analysis has achieved a wide range of applications. However, there is still much to do to develop effective movement assessment technique, which bridges the results of human posture estimation and the understanding of human action performance. In this work, we propose an action tutor system which enables the user to interactively retrieve the learning exemplar of the target action movement and to immediately acquire motion instructions while learning it in front of the Kinect. In the retrieval stage, non-linear time warping algorithms are designed to retrieve video segments similar to the query movement roughly performed by the user. In the learning stage, the user learns according to the selected video exemplar, and the motion assessment including both static and dynamic differences is presented to the user in a more effective and organized way, helping him/her to perform the action movement correctly.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1263–1264},
numpages = {2},
keywords = {movement assessment, human action video retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396434,
author = {Shafiei, Arash and Ngo, Quang Minh Khiem and Guntur, Ravindra and Saini, Mukesh Kumar and Pang, Cong and Ooi, Wei Tsang},
title = {Jiku Live: A Live Zoomable Video Streaming System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396434},
doi = {10.1145/2393347.2396434},
abstract = {We present Jiku Live, a client-server system that supports zoom and pan operations in live video streaming from network cameras. The client is an Android mobile application that plays back live video from a selected camera and supports multi-touch zoom and pan interaction. The server acquires video streams from network cameras and transcodes the video feeds into one-second video segments at multiple resolutions. The transcoded video supports random access into any region-of-interest (RoI) within the video. Upon receiving zoom or pan requests, the server transmits the RoIs from the corresponding video segments to the client.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1265–1266},
numpages = {2},
keywords = {live video streaming, zoomable video},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396435,
author = {Doman, Keisuke and Kuai, Cheng Ying and Takahashi, Tomokazu and Ide, Ichiro and Murase, Hiroshi},
title = {Smart VideoCooKing: A Multimedia Cooking Recipe Browsing Application on Portable Devices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396435},
doi = {10.1145/2393347.2396435},
abstract = {This demo presents "Smart VideoCooKing" which is a multimedia cooking recipe browsing application on portable Android devices. A multimedia cooking recipe is a cooking recipe where each cooking operation is associated with a corresponding video clip describing it, aimed to facilitate the understanding of cooking operations. In combination with third-party applications, "Smart VideoCooKing" provides useful functions such as playing cooking video clips describing cooking operations quickly, searching information of ingredients easily, and reading aloud cooking directions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1267–1268},
numpages = {2},
keywords = {multimedia cooking recipe, cooking support},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396436,
author = {Kimber, Don and Shingu, Jun and Vaughan, Jim and Arendash, David and Lee, David and Back, Maribeth},
title = {Through the Looking Glass: Mirror Worlds for Augmented Awareness &amp; Capability},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396436},
doi = {10.1145/2393347.2396436},
abstract = {We demonstrate a system for supporting mirror worlds - 3D virtual models of physical spaces that reflect the structure and activities of those spaces to help support context awareness and tasks such as planning and recollection of events. Through views on web pages, portable devices, or on `magic window' displays in the physical space, remote people may `look in' to the space, while people within the space are provided information that would not otherwise be obvious. For example, by looking at a mirror display, people can learn how long others have been present, or where they have been. People in one part of a building can get a sense of activities in the rest of the building. The system can be used to bridge across sites and help provide different parts of an organization with a shared awareness of each other's activities. We demonstrate viewers for several mirror worlds we have created, including for the ACM conference venue in Nara.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1269–1270},
numpages = {2},
keywords = {virtual tours, augmented reality, 3D applications},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396437,
author = {Vliegendhart, Raynor and Larson, Martha and Hanjalic, Alan},
title = {LikeLines: Collecting Timecode-Level Feedback for Web Videos through User Interactions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396437},
doi = {10.1145/2393347.2396437},
abstract = {Conventional online video players do not make the inner structure of the video apparent, making it hard to jump straight to the interesting parts. Our LikeLines system provides its users with a navigable heat map of interesting regions for the videos they are watching. Its novelty lies in its combination of content analysis and both explicit and implicit user interactions. The system can be readily used and deployed to collect large amounts of interaction data needed for in-depth research on timecode-level feedback.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1271–1272},
numpages = {2},
keywords = {explicit feedback, web video player, viewer interactions, implicit feedback},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396438,
author = {Thomee, Bart and Rae, Adam},
title = {Exploring and Browsing Photos through Characteristic Geographic Tag Regions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396438},
doi = {10.1145/2393347.2396438},
abstract = {We present a system that supports zoomable browsing and exploration of photos taken across the globe. Our system is based on a novel algorithm that automatically uncovers the colloquial boundaries of regions that are characteristic for individual tags used in a large collection of geo-referenced photos. We first model the data using scale-space theory, which allows us to represent it simultaneously across different scales as a family of increasingly smoothed density distributions, after which we derive the region boundaries by applying image analysis techniques to the scale-space representation of each tag. The interface visualizes the shape and size of the resulting boundaries for each tag along the dimensions of space and time across multiple scales, giving the user the ability to explore the world as patchwork of dynamic characterizing geographic tag regions and to browse through their associated photos.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1273–1274},
numpages = {2},
keywords = {visualization, tagged images, scale-space theory, flickr, geo-referenced photos, exploration, browsing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396439,
author = {Jiang, Yuning and Yuan, Junsong and Meng, Jingjing},
title = {Rapid Object Search Engine for Contextual Advertisement},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396439},
doi = {10.1145/2393347.2396439},
abstract = {Visual object search, with the goal to find and locate the target object in large image or video collections, is of great interest for many applications and hence has received intensive attentions in recent years. In this demo, we present a spatial context-aware large-scale visual object search system, which is robust to cluttered backgrounds and can well handle scale variations of the objects. Different from the traditional image retrieval systems only matching individual points or fixed-scale spatial contexts, the proposed system considers spatial contexts of varying sizes and shapes, in the form of randomized spatial partition (RSP), and hence provides more accurate search results. Moreover, compared to the computational expensive RANSAC algorithm used in the state-of-the-art retrieval systems, the RSP framework lends our system to easy parallelization and significant speedup for object localization. Consequently, our system works accurately and efficiently. In addition, an Android application has been developed for mobile tasks, by which the user can take a photo of the object he/she wants and then search the same products and their selling information.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1275–1276},
numpages = {2},
keywords = {randomized spatial partition, spatial context, visual object search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396440,
author = {Marutani, Takafumi and Mase, Kenji and Fujii, Toshiaki and Kawamoto, Tetsuya},
title = {Multi-View Video Contents Viewing System by Synchronized Multi-View Streaming Architecture},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396440},
doi = {10.1145/2393347.2396440},
abstract = {We developed a novel networked video viewing system for multi-view video contents with video streaming technology. This work's contribution is that our developed system confirmed the validity of simultaneous multiple video streaming architecture that incorporates real-time channel switching and a target-oriented viewing interface. In addition, we newly introduced an inter-channel bandwidth management scheme to achieve cost-effective multi-view streaming.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1277–1278},
numpages = {2},
keywords = {video streaming, multi-view video, multi-view interface, free-viewpoint TV},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396441,
author = {Kaufmann, Bonifaz and Hitz, Martin},
title = {X-Large Virtual Workspaces for Projector Phones through Peephole Interaction},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396441},
doi = {10.1145/2393347.2396441},
abstract = {In peephole interaction a window to a virtual workspace is moved in space to reveal additional content. It is a promising interaction technique for mobile projector phones to display large workspaces which contain more information than can be appropriately displayed on a small smartphone screen. In this paper we describe a projector phone prototype that implements peephole pointing without instrumenting the environment or using any additional hardware besides a smartphone and a handheld projector. This device allows for the first time to perform peephole interaction in the wild. Moreover, we demonstrate some applications we have built to exploit and investigate the full potential of peephole interaction with projector phones.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1279–1280},
numpages = {2},
keywords = {peephole pointing, brainstorming, handheld projector, projector phone, drawing, peephole interaction},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396442,
author = {Kaiser, Rene and Weiss, Wolfgang and Borsum, Malte and Kochale, Axel and Masetti, Marco and Zampichelli, Valentina},
title = {Demo: Virtual Director for Live Event Broadcast},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396442},
doi = {10.1145/2393347.2396442},
abstract = {We demonstrate the Production Scripting Engine (PSE), a Virtual Director software automatically selecting, framing and cutting camera shots as part of an interactive live broadcast production system. It reasons for multiple viewers with different devices and preferences in parallel within real-time constraints. The PSE takes individual parameters into account and executes a set of pragmatic and cinematographic principles to decide which of the available content is shown. The decision making process is distributed and takes a rule-based approach with exchangable production logic and domain models. The demo uses recorded content from both a high-resolution 180 degree panoramic camera and broadcast cameras. Extracted metadata from both automatic and manual annotation are indexed in an efficient knowledge base, which streams it to the PSE to simulate a live broadcast. We use a specific renderer that allows to view spherical content. Viewers may experience the output on 2 different devices and tweak preferences to adapt the Virtual Director's behavior.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1281–1282},
numpages = {2},
keywords = {live broadcast, virtual director, decision making},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396443,
author = {Okura, Fumio and Kanbara, Masayuki and Yokoya, Naokazu},
title = {Fly-through Heijo Palace Site: Historical Tourism System Using Augmented Telepresence},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396443},
doi = {10.1145/2393347.2396443},
abstract = {We have developed an augmented telepresence system which enables virtual tourism beyond time and space. Augmented telepresence provides a user with both the view of a remote location and related information using augmented reality techniques. This study deals with the geometric and photometric registration problems to generate movie-quality augmented omnidirectional videos automatically. The user can look around the scene from the sky above Heijo palace Site which is an ancient capital in Nara, Japan in the technical demonstration.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1283–1284},
numpages = {2},
keywords = {augmented telepresence, aerial omnidirectional image, unmanned airship},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396444,
author = {Collins, Kevin and O'Connor, Noel E. and Muntean, Gabriel Miro},
title = {Mobile Multimedia Presentation in Self-Forming Mobile Device Groups: Ad-Hoc Networks in Practice},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396444},
doi = {10.1145/2393347.2396444},
abstract = {This demo exhibits a new application of mobile ad-hoc networks, where, a group of mobile devices are connected to allow synchronized presentation of multimedia content. The demo is in the form of an interactive tour. Participants have a mobile device and the tour is led by a guide, who takes the group on an informative tour of a locale. The tour is augmented with the presentation of multimedia content on the devices, highlighting points of interest. Content presentation is controlled by the guide and is synchronized using the ad-hoc network. This is an edutainment application but the underlying technology could be applied elsewhere including educational settings and in entertainment.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1285–1286},
numpages = {2},
keywords = {ad-hoc networks, multimedia, groups, synchronization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396445,
author = {Okunaka, Takeshi and Tonomura, Yoshinobu},
title = {Eyeke: What You Hear is What You See},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396445},
doi = {10.1145/2393347.2396445},
abstract = {This demonstration shows an interactive visual-to-auditory scene sensing system called Eyeke (Eye Mike), which converts visual features of what it sees into sound output aiming at helping us to interact with our surroundings. Eyeke works robustly by restricting colors in its image processing and easy-and-effective calibration. Demonstrations of Eyeke as a music instrument and a camera-based scene sonar are shown.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1287–1288},
numpages = {2},
keywords = {music note, auditory features, sound, sensing, color, visual features, conversion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396446,
author = {Vonikakis, Vassilios and Winkler, Stefan},
title = {System for Creating Slideshows Based on People and Their Emotions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396446},
doi = {10.1145/2393347.2396446},
abstract = {We demonstrate a system for the automatic creation of slideshows from photo collections, based on a user-specified group of people, their emotions and other image similarity criteria such as color, timeline and scene characteristics. The main objective of the system is to form meaningful image sequences for slideshows, with minimal user input. The intuitive interface allows the user to provide a personalized definition of image similarity, resulting in different image sequences, and making the system highly user-adaptable and flexible.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1289–1290},
numpages = {2},
keywords = {automatic slideshow, emotion estimation, people identification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396447,
author = {Zhang, Richong and Guo, Xiaohui and Sun, Hailong and Huai, Jinpeng and Liu, Xudong},
title = {GTravel: A Global Social Travel System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396447},
doi = {10.1145/2393347.2396447},
abstract = {This paper presents an global social travel system to assist tourists in their itinerary planning, tour navigation, and travel knowledge sharing. In particular, firstly, we propose an efficient and flexible itinerary planning algorithm for organizing itineraries. Secondly, we design an intelligent tour path planning and navigation system by mining patterns from trajectories and geo-photos shared by other tourists. Thirdly, we provide a framework to monitor the status and events that may affect the predefined itineraries. Finally, the social media module enhances the opportunities of experience (itinerary, trajectory, and travelogue) sharing and helps travelers make better decisions. Concrete demonstrations of our system are also provided in this paper to show the flexibility and applicability of our system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1291–1292},
numpages = {2},
keywords = {travel package recommendation, trajectory mining, path navigation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246415,
author = {Tian, Qi},
title = {Session Details: Technical Demo Session 2},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246415},
doi = {10.1145/3246415},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396449,
author = {Horiuchi, Toshiharu and Sankoh, Hiroshi and Kato, Tsuneo and Naito, Sei},
title = {Interactive Music Video Application for Smartphones Based on Free-Viewpoint Video and Audio Rendering},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396449},
doi = {10.1145/2393347.2396449},
abstract = {This paper presents a novel interactive music video application for smartphones based on free-viewpoint video technology in conjunction with three-dimensional positional audio technology. A user can enjoy a music video from a moving viewpoint that the user can manipulate by the touch screen, with the positional audio through the headphone. A user can even manipulate the positions of the performers on the stage as well as the viewpoint. The application, consisting of our audio rendering engine for multiple AAC ADTS files and our video rendering engine for multiple H.264 ES files, runs on a smartphone in stand-alone mode. The application has been released as official content from a music label for Android and iOS.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1293–1294},
numpages = {2},
keywords = {tablet, head-related transfer function, smartphone, free-viewpoint video, three-dimensional positional audio},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396450,
author = {Liu, Fan and Tang, Jinhui and Zhao, Ruizhen and Tang, Zhenmin},
title = {Abnormal Behavior Recognition System for ATM Monitoring by RGB-D Camera},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396450},
doi = {10.1145/2393347.2396450},
abstract = {In this demo, we present an effective real-time system for ATM intelligent monitoring by using Kinect of Microsoft. With Kinect, we can easily detect people in ATM room and get their position information. By analyzing position information and video content, the system detects abnormal behaviors such as face-hiding, peeping and wandering, while records the time of these abnormal videos. Therefore, it not only prevents crimes but also helps to find suspects quickly after crimes have happened. The experimental results show that the system has the advantages of robustness, and provides a new mean for preventing financial crimes.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1295–1296},
numpages = {2},
keywords = {kinect, abnormal behavior, peep recognition, ATM, face-hiding},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396451,
author = {Fujisawa, Makoto and Amano, Toshiyuki and Taketomi, Takafumi and Yamamoto, Goshiro and Uranishi, Yuki and Miyazaki, Jun},
title = {Interactive Photomosaic System Using GPU},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396451},
doi = {10.1145/2393347.2396451},
abstract = {A photomosaic is a type of decorative art made up from various other photographs. We present a method for quickly generating photomosaics and propose an interactive recursive photomosaic system. Users can operate the system by using a large display with a touch input function, which allows them to alter the appearance of the image dynamically.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1297–1298},
numpages = {2},
keywords = {GPU, photomosaic},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396452,
author = {Bloess, Mark and Kim, Heung-Nam and El Saddik, Abdulmotaleb},
title = {PhacePhinder: Harnessing Social Networks to Build Social Face Databases for Mobile Devices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396452},
doi = {10.1145/2393347.2396452},
abstract = {This demo presents a client-server application which collects images and personal information from social networks to build face recognition databases. The client runs on a mobile phone allowing any face captured by a mobile phone's camera to be identified. Using the personal information collected we give back the most meaningful social connection between the user and the recognized individual.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1299–1300},
numpages = {2},
keywords = {social network analysis, content recommendation, people tags, facebook, face recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396453,
author = {Xie, Chengli and Wang, Jinqiao and Zhang, Yifan and Lu, Hanqing},
title = {Real-Time Multiple Object Instances Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396453},
doi = {10.1145/2393347.2396453},
abstract = {In this paper, we present a novel, real-time multiple object instance detection system via template matching and pairwise classification. Instance detection aims to find and locate exactly the same object instances as specified. Our system is composed of two heterogeneous stages. The first stage adopts instance-specific detection to generate candidates. And the second stage makes use of a pairwise-based classifier across instance categories to test and verify these candidates with respect to templates. Experiments show the superiority of our approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1301–1302},
numpages = {2},
keywords = {multiple instances, pairwise classification, instance detection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396454,
author = {Wu, Di and Zhu, Fan and Shao, Ling and Zhang, Hui},
title = {One Shot Learning Gesture Recognition with Kinect Sensor},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396454},
doi = {10.1145/2393347.2396454},
abstract = {Gestures are both natural and intuitive for Human-Computer-Interaction (HCI) and the one-shot learning scenario is one of the real world situations in terms of gesture recognition problems. In this demo, we present a hand gesture recognition system using the Kinect sensor, which addresses the problem of one-shot learning gesture recognition with a user-defined training and testing system. Such a system can behave like a remote control where the user can allocate a specific function using a prefered gesture by performing it only once. To adopt the gesture recognition framework, the system first automatically segments an action sequence into atomic tokens, and then adopts the Extended-Motion-History-Image (Extended-MHI) for motion feature representation. We evaluate the performance of our system quantitatively in Chalearn Gesture Challenge, and apply it to a virtual one shot learning gesture recognition system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1303–1304},
numpages = {2},
keywords = {human-computer-interaction, hand gesture recognition, RGBD camera, one shot learning},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396455,
author = {Plant, William and Schaefer, Gerald},
title = {Interactive Exploration of Large Remote Image Databases},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396455},
doi = {10.1145/2393347.2396455},
abstract = {Mapping-based visualisations of image databases are well suited to users wanting to survey the overall content of a collection. Given the large amount of image data contained within such visualisations, however, this approach has yet to be applied to large image databases stored remotely. In this technical demonstration, we showcase our Web-Based Images Browser (WBIB). Our novel system makes use of image pyramids so that users can interactively explore mapping-based visualisations of large remote image databases.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1305–1306},
numpages = {2},
keywords = {visualisation, image databases, image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396456,
author = {Ochi, Daisuke and Kimata, Hideaki and Noto, Hajime and Kojima, Akira},
title = {Scenario-Driven Interactive Panorama Video Delivery: Promptly Watch and Share Enjoyable Parts of an Event},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396456},
doi = {10.1145/2393347.2396456},
abstract = {We propose a scenario-driven interactive panorama video delivery system that allows users to repeatedly watch the enjoyable parts of an event and share them with others. It provides functions for interactively watching panorama video parts, as well as for scenario making (with user-selected camera trajectory), and delivery that allows users to share their panorama watching experiences. In this technical demos, we demonstrate the system that can make a favorable and stable camera trajectories as a scenario from intuitive user manipulations of a tablet device in a format that can be easily shared with others.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1307–1308},
numpages = {2},
keywords = {camera trajectory, panorama video, video stabilization, tablet manipulation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396457,
author = {Zhou, Yinsheng and Monserrat, Toni-Jan Keith Palma and Wang, Ye},
title = {MOGAT: A Cloud-Based Mobile Game System with Auditory Training for Children with Cochlear Implants},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396457},
doi = {10.1145/2393347.2396457},
abstract = {Musical auditory habilitation is an essential process in adapting cochlear implant recipients to the musical hearing context provided by cochlear implants. However, due to the cost and time limitation, it is impossible for hearing healthcare professionals to provide intensive and extensive musical auditory habilitation for every cochlear implant recipient. In order to provide an efficient and cost-effective musical auditory training for children with cochlear implants, we designed and developed MObile Games with Auditory Training (MOGAT) on off-the-shelf mobile devices. MOGAT includes three intuitive and interesting mobile games for training pitch perception and production, and a cloud-based web service for music therapists to support and evaluate individual habilitation. We demonstrate MOGAT for enhancing musical habilitation for children with cochlear implants.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1309–1310},
numpages = {2},
keywords = {music, children, mobile, auditory habilitation, game, cochlear implant},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396458,
author = {Li, Zhonghua and Wang, Ye},
title = {A Domain-Specific Music Search Engine for Gait Training},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396458},
doi = {10.1145/2393347.2396458},
abstract = {This paper demonstrates a domain-specific music retrieval system to help music therapists find appropriate music for Parkinson's disease patients in their gait training. Different from existing music search engines, this system incorporates multiple music dimensions (i.e., tempo, cultural style, and beat strength) required in gait training, and facilitates the searching process by allowing music retrieval directly on these dimensions. To support music search by tempo, a user-perception based method is also proposed to improve state-of-the-art tempo estimation algorithms. We conducted a user study and evaluated the system efficacy in searching music using different types of queries based on these music dimensions. Experimental results demonstrate the effectiveness and usability of our system in therapeutic gait training.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1311–1312},
numpages = {2},
keywords = {domain-specific, gait training, music retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396459,
author = {Wang, Xinxi and Wang, Ye and Rosenblum, David},
title = {A Daily, Activity-Aware, Mobile Music Recommender System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396459},
doi = {10.1145/2393347.2396459},
abstract = {Existing music recommender systems rely on collaborative filtering or content-based technologies to satisfy users' long-term music playing needs. Given the popularity of mobile music devices with rich sensing and wireless communication capabilities, we demonstrate in this demo a novel system to employ contextual information collected with mobile devices for satisfying users' short-term music playing needs. In our system, contextual information is integrated with music content analysis to offer recommendation for daily activities.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1313–1314},
numpages = {2},
keywords = {sensors, activity classification, context awareness, music recommendation, mobile computing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396460,
author = {Yamada, Takayuki and Gohshi, Seiichi and Echizen, Isao},
title = {Use of Invisible Noise Signals to Prevent Privacy Invasion through Face Recognition from Camera Images},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396460},
doi = {10.1145/2393347.2396460},
abstract = {A method is proposed for preventing privacy invasion through unintentional capture of facial images. Prevention methods such as covering the face and painting particular patterns on the face are effective but hinder face-to-face communication. The proposed method overcomes this problem through the use of a device worn on the face that transmits near-infrared signals that are picked up by camera image sensors, which makes the face in captured images undetectable. The device is similar in appearance to a pair of eyeglasses, and the signals cannot be seen by the human eye, so face-to-face communication is not hindered. Testing of a prototype "privacy visor" showed that it can effectively prevent privacy invasion via face detection by corrupting the facial images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1315–1316},
numpages = {2},
keywords = {face recognition, unintentional facial capture, privacy invasion, infrared LED},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396461,
author = {Zhang, Ying and Zimmermann, Roger},
title = {DVS: A Dynamic Multi-Video Summarization System of Sensor-Rich Videos in Geo-Space},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396461},
doi = {10.1145/2393347.2396461},
abstract = {It is now very easy to produce user generated videos (UGV) due to progress in camera and recording technologies on mobile devices, such as smartphones. Additionally, the ubiquitous, built-in sensors in digital devices can greatly enrich these videos with sensor descriptions, for example geo-spatial properties. A repository of such sensor-rich videos is a valuable source of information for prospective tourists when they plan to visit a city and would like to get a preview of its main attractions. Inspired by this, we have built an interactive geo-video search system. On a given map, a user specifies a start point and a destination and the system dynamically retrieves a video summarization along the path between the two points. Moreover, the user can interactively update the query during the video playback by dragging the route on the map. The main features of our technique are, first, that it is fully automatic and leverages sensor meta-data information which is acquired in conjunction with videos. Second, the system dynamically adapts to query updates in real-time. Third, a concise but comprehensive summarization from multiple user generated videos is presented for any queried route.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1317–1318},
numpages = {2},
keywords = {video summarization, sensor data mining, geo-tagging},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396462,
author = {Wang, Guanfeng and Seo, Beomjoo and Zimmermann, Roger},
title = {Motch: An Automatic Motion Type Characterization System for Sensor-Rich Videos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396462},
doi = {10.1145/2393347.2396462},
abstract = {Camera motion information facilitates higher-level semantic description inference in many video applications, e.g., video retrieval. However, an efficient and accurate methodology for annotating videos with camera motion information is still an elusive goal. In our recent work we have investigated the fusion of captured video with a continuous stream of sensor meta-data. For these so-called sensor-rich videos we present a system, called Motch, which precisely partitions a video document into subshots, automatically characterizes the camera motions and provides video subshot browsing based on an interactive, map-based interface. Moreover, the system computes and presents motion type statistics for each video in real time and renders different subshots distinctively on the map synchronously with the video playback.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1319–1320},
numpages = {2},
keywords = {motion characterization, location sensors, mobile video, web services},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396463,
author = {Kitahara, Tetsuro and Kimura, Syohei and Suzuki, Yuu and Suzuki, Tomofumi},
title = {Hummi-Com: Humming-Based Music Composition System},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396463},
doi = {10.1145/2393347.2396463},
abstract = {In this paper, we propose a composition-by-humming system, called Hummi-Com, that automatically corrects musically inappropriate notes. Although various systems with a composition-by-humming function have been developed, it is difficult in practice for non-musicians to compose musically appropriate melodies with these systems due to the target user's insufficient skill at controlling pitch. In this paper, we propose note correction rules for avoiding such musically inappropriate outputs. This rule set is designed based on the idea of searching for a reasonable trade-off between removing dissonant nonchord tones and retaining musically acceptable nonchord tones.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1321–1322},
numpages = {2},
keywords = {music composition support, melody correction, composition-by-humming},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246416,
author = {Kato, Hirokazu},
title = {Session Details: Technical Demo Session 3},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246416},
doi = {10.1145/3246416},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396465,
author = {Lu, Shiyang and Mei, Tao and Wang, Jingdong and Zhang, Jian and Wang, Zhiyong and Feng, David Dagan and Sun, Jian-Tao and Li, Shipeng},
title = {Browse-to-Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396465},
doi = {10.1145/2393347.2396465},
abstract = {This demonstration presents a novel interactive online shopping application based on visual search technologies. When users want to buy something on a shopping site, they usually have the requirement of looking for related information from other web sites. Therefore users need to switch between the web page being browsed and other websites that provide search results. The proposed application enables users to naturally search products of interest when they browse a web page, and make their even causal purchase intent easily satisfied. The interactive shopping experience is characterized by: 1) in session---it allows users to specify the purchase intent in the browsing session, instead of leaving the current page and navigating to other websites; 2) in context---the browsed web page provides implicit context information which helps infer user purchase preferences; 3) in focus---users easily specify their search interest using gesture on touch devices and do not need to formulate queries in search box; 4) natural-gesture inputs and visual-based search provides users a natural shopping experience. The system is evaluated against a data set consisting of several millions commercial product images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1323–1324},
numpages = {2},
keywords = {user interaction, online shopping, visual search, gesture},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396466,
author = {Wang, Jing and Wang, Jingdong and Hua, Xian-Sheng and Li, Shipeng},
title = {Scalable Similar Image Search by Joint Indices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396466},
doi = {10.1145/2393347.2396466},
abstract = {Text-based image search is able to return desired images for simple queries, but has limited capabilities in finding images with additional visual requirements. As a result, an image is usually used to help describe the appearance requirements. In this demonstration, we show a similar image search system that can support the joint textual and visual query. We present an efficient and effective indexing algorithm, neighborhood graph index, which is suitable for millions of images, and use it to organize joint inverted indices to search over billions of images.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1325–1326},
numpages = {2},
keywords = {joint inverted index, neighborhood graph index, similar image search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396467,
author = {Wang, Peng and Zhang, Dongqing and Wang, Jingdong and Wu, Zhong and Hua, Xian-Sheng and Li, Shipeng},
title = {Color Filter for Image Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396467},
doi = {10.1145/2393347.2396467},
abstract = {Image search relying on surrounding texts can return reliably relevant images to some extent. Most recent efforts are focusing on utilizing visual contents to help users find images with specific visual requirements. In this demonstration, we show a color filter scheme for image search, which enables users to find images containing objects or scenes with their interested color. Color is one of the most crucial cues in describing visual contents and has been frequently used in various applications. The key components in developing our demo include salient object detection and perceptual color naming. The color filter in Microsoft Bing image search is developed using these techniques.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1327–1328},
numpages = {2},
keywords = {image search, color filter},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396468,
author = {Ercolessi, Philippe and Bredin, Herv\'{e} and S\'{e}nac, Christine},
title = {StoViz: Story Visualization of TV Series},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396468},
doi = {10.1145/2393347.2396468},
abstract = {Recent TV series tend to have more and more complex plot. They follow the lives of numerous characters and are made of multiple intertwined stories. In this paper, we introduce StoViz, a web-based interface allowing a fast overview of this kind of episode structure, based on our plot de-interlacing system. StoViz has two main goals. First, it provides the user with a useful overview of the episode by displaying each story separately and a short abstract extracted from them. Then, it allows an efficient visual comparison of the output of any automatic plot de-interlacing algorithm with the manual annotation in terms of stories and is therefore very helpful for evaluation purposes. StoViz is available online at http://stoviz.niderb.fr.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1329–1330},
numpages = {2},
keywords = {plot de-interlacing, non-linear browsing, visualization, evaluation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396469,
author = {Nguyen, Tam V. and Li, Lusong and Tan, Jun and Yan, Shuicheng},
title = {3DME: 3D Media Express from RGB-D Images},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396469},
doi = {10.1145/2393347.2396469},
abstract = {Considering the continuously increasing availability and accessibility of 3D media and the depth camera such as Kinect, we demonstrate an innovative 3D media system called 3DME. The objective of this demo is three-fold. First, the demo exhibits the creation of 3D images from RGB-D images. Second, 3DME allows a user to insert impressive effects to the produced 3D content. Last but not least, our demo is one of the first attempts towards advertising for 3D content which enables both the advertisers and content providers deliver more effective ads carried through 3D media.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1331–1332},
numpages = {2},
keywords = {3D media, depth information, 3D advertising},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396470,
author = {Liu, Si and Nguyen, Tam V. and Feng, Jiashi and Wang, Meng and Yan, Shuicheng},
title = {Hi, Magic Closet, Tell Me What to Wear!},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396470},
doi = {10.1145/2393347.2396470},
abstract = {In this demo, we present a practical system, magic closet, for automatic occasion-oriented clothing pairing. Given a user-input occasion, e.g., wedding or shopping, the magic closet intelligently and automatically pairs the user-specified reference clothing (upper-body or lower-body) with the most suitable one from online shops. Two key criteria are explicitly considered for the magic closet system. One criterion is to wear properly, e.g., compared to suit pants, it is more decent to wear a cocktail dress for a banquet occasion. The other criterion is to wear aesthetically, e.g., a red T-shirt matches better white pants than green pants. To narrow the semantic gap between the low-level visual features and the high-level occasion categories, we propose to adopt middle-level clothing attributes (e.g., clothing category, color, pattern) as a bridge. More specifically, the clothing attributes are treated as latent variables in our proposed latent Support Vector Machine (SVM) based recommendation model. The wearing properly criterion is described through a feature-occasion potential and an attribute-occasion potential, while the wearing aesthetically criterion is expressed by an attribute-attribute potential.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1333–1334},
numpages = {2},
keywords = {occasion oriented clothing pairing, latent SVM},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396471,
author = {Liu, Si and Song, Zheng and Wang, Meng and Xu, Changsheng and Lu, Hanqing and Yan, Shuicheng},
title = {Street-to-Shop: Cross-Scenario Clothing Retrieval via Parts Alignment and Auxiliary Set},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396471},
doi = {10.1145/2393347.2396471},
abstract = {We address a cross-scenario clothing retrieval problem- given a daily human photo captured in general environment, e.g., on street, finding similar clothing in online shops, where the photos are captured more professionally and with clean background. There are large discrepancies between daily photo scenario and online shopping scenario. We first propose to alleviate the human pose discrepancy by locating 30 human parts detected by a well trained human detector. Then, founded on part features, we propose a two-step calculation to obtain more reliable one-to-many similarities between the query daily photo and online shopping photos: 1) the within-scenario one-to-many similarities between a query daily photo and an extra auxiliary set are derived by direct sparse reconstruction; 2) by a cross-scenario many-to-many similarity transfer matrix inferred offline from the auxiliary set and the online shopping set, the reliable cross-scenario one-to-many similarities between the query daily photo and all online shopping photos are obtained.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1335–1336},
numpages = {2},
keywords = {latent SVM, clothing recommendation, clothing pairing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396472,
author = {Ye, Junfeng and Chen, Jia and Chen, Zejia and Zhu, Yihe and Bao, Shenghua and Su, Zhong and Yu, Yong},
title = {Searching for Diversified Landmarks by Photo},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396472},
doi = {10.1145/2393347.2396472},
abstract = {This demo focuses on the problem of searching for diversified landmarks with photos as input. More particularly, we propose a system called DLMSearch that allows a user to upload a photo as a query and searches for a diverse set of relevant landmarks in real time. It also presents a photo summary for each retrieved landmark, considering both visual representativeness and diversity. Our online demo is available at http://lm.apexlab.org/landmark/demo.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1337–1338},
numpages = {2},
keywords = {visual summary, landmark search, diversity, content-based image retrieval},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396473,
author = {Zhang, Hanwang and Zha, Zheng-Jun and Bian, Jingwen and Gao, Yue and Luan, Huanbo and Chua, Tat-Seng},
title = {Attribute Feedback},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396473},
doi = {10.1145/2393347.2396473},
abstract = {This demonstration presents a new interactive Content Based Image Retrieval (CBIR) system, termed Attribute Feedback (AF). Unlike traditional relevance feedback purely founded on low-level features, AF system shapes user's search intents more precisely and quickly by collecting feedbacks on intermediate-level semantic attribute. At each interaction iteration, the AF system first determines the most informative binary attributes for feedbacks and then augments the binary attribute feedbacks by a new type of attributes, "affinity attributes", each of which is learnt offline to describe the distance/similarity between user's envisioned image(s) and a retrieved image with respect to the corresponding affinity attribute. Based on the feedbacks on binary and affinity attributes, the images in corpus are further re-ranked towards better fitting user's search intents. The experimental results on two real-world image datasets have demonstrated the superiority of the AF system over other state-of-the-art relevance feedback based CBIR approaches.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1339–1340},
numpages = {2},
keywords = {relevance feedback, attribute feedback, image search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396474,
author = {Tankoyeu, Ivan and St\"{o}ttinger, Julian and Paniagua, Javier and Giunchiglia, Fausto},
title = {Personal Photo Indexing},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396474},
doi = {10.1145/2393347.2396474},
abstract = {Sorting one's own private photo collection is a time consuming and tedious task. We demonstrate our event-centered approach to perform this task fully automatically. In the course of the demonstration, we either use our own photo collections, or invite the conference visitors to bring their own cameras and photos. We will sort the photos into a semantically meaningful hierarchy for the users within a couple of minutes. Events as a media aggregator allow a user to manage and annotate a photo collection in more convenient and natural to the human being way. Based on the recognized user behavior the application is able to reveal the nature of an event and build its hierarchy with a event/sub-event relationship. One important prerequisite of our approach is a precise GPS based spatial annotation of the photos. To accommodate for devices without GPS chips or temporary low GPS perception, we propose an approach to enrich the collection with automatically estimated GPS data by semantically interpolating possible routes of the user. We are positive that we can provide a well received service for the conference visitors, especially since the conference venue will trigger a lot of memorable photos. Large scale experimental validation showed that the approach is able to recreate a user's desired hierarchy with an F-measure of about 0.8.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1341–1342},
numpages = {2},
keywords = {photo context analysis, event-based indexing, personal photo collection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396475,
author = {Zhou, Rong and Chen, Liuli and Zhang, Liqing},
title = {Guess What You Draw: Interactive Contour-Based Image Retrieval on a Million-Scale Database},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396475},
doi = {10.1145/2393347.2396475},
abstract = {We propose a real-time image retrieval system which allows users to search target images whose objects are similar to the query in contour, regardless of their sizes and positions appearing in the images. Even in a complicated scene, as long as the object's contour is most salient in the target image, the system is still able to capture it and lists the image in the retrieval results. Therefore, the system has better retrieval rate than existing systems and algorithms. One typical application of the proposed system is to help the computer understand what does the user draw or upload. It is based on the statistical distributions of tags of retrieved images, and the proposed system feeds back some candidate tags related to the query image. Such tags could be used for further retrieval to refine the result list. In addition, the system provides a friendly interactive interface with multiple queries. These queries are from different combinations of tags, a hand-drawn sketch and a natural image, and could help users search images flexibly and conveniently. The system runs on a database of 1.3 million images and could achieve a real-time retrieval speed. The results in the demonstration show excellent retrieval performance of the proposed system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1343–1344},
numpages = {2},
keywords = {image retrieval, million-scale, contour, sketch},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396476,
author = {Zhang, Wei and Pang, Lei and Ngo, Chong-Wah},
title = {FashionAsk: Pushing Community Answers to Your Fingertips},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396476},
doi = {10.1145/2393347.2396476},
abstract = {We demonstrate a multimedia-based question-answering system, named FashionAsk, by allowing users to ask questions referring to pictures snapped by mobile devices. Specifically, instead of asking verbose questions to depict visual instances, direct pictures are provided as part of questions. To answer these multi-modal questions, FashionAsk performs a large-scale instance search to infer the names of instances, and then matches with similar questions from community-contributed QA websites as answers. The demonstration is conducted on a million-scale dataset of Web images and QA pairs in the domain of fashion products. Asking a multimedia question through FashionAsk can take as short as five seconds to retrieve the candidate answer as well as suggested questions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1345–1346},
numpages = {2},
keywords = {instance naming, question matching, multimedia question answering},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396477,
author = {Jiang, Yu-Gang and Dai, Qi and Zheng, Yingbin and Xue, Xiangyang and Liu, Jie and Wang, Dong},
title = {A Fast Video Event Recognition System and Its Application to Video Search},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396477},
doi = {10.1145/2393347.2396477},
abstract = {Techniques for recognizing complex events in diverse Internet videos are important in many applications. State-of-the-art video event recognition approaches normally involve modules that demand extensive computation, which prevents their application to large scale problems. In this demonstration, we present a fast video event recognition system, which requires just a few seconds to process a general YouTube video with a few minutes of duration. The development of this system is grounded on several important findings from a large set of empirical studies, where we systematically evaluated many technical options for each critical module of a present-day video event recognition framework. Pooling the insights gained from this study leads to a speeded-up event recognition system that is 220-times faster than a decent baseline while still has a high degree of recognition accuracy. We also demonstrate the technical feasibility of using event recognition results as the sole clue for video search, where the similarity of videos is determined based on the consistency of the event recognition confidence scores. We showcase this capability using an Internet video dataset containing about 10 thousands of YouTube videos. Very promising results were observed.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1347–1348},
numpages = {2},
keywords = {internet videos, fast video event recognition, speed efficiency, video search},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396478,
author = {Bertini, Marco and Del Bimbo, Alberto and Ferracani, Andrea and Pezzatini, Daniele},
title = {Social and Automatic Annotation of Videos for Semantic Profiling and Content Discovery},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396478},
doi = {10.1145/2393347.2396478},
abstract = {This demo presents a system based on social relationships, social knowledge and automatic video and textual content analysis for the discovery of videos in social networks. The system, developed as a web application, allows users to annotate, manually and automatically, and comment video frames and scenes enriching their content with tags, references to Facebook users and pages and Wikipedia resources. These annotations are used to semantically model the profile of each user extracting and expanding his interests and folksonomy, as well as resources of interest in his social graph. The automatically generated profile page is used to suggest to users new resources, Facebook friends and videos whose content is related to their interests and allows profile curation. A screencast showing an example of these functionalities is publicly available at: http://vimeo.com/miccunifi/facetube},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1349–1350},
numpages = {2},
keywords = {social video retrieval, internet videos, social video tagging},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246417,
author = {Worring, Marcel and Jing, Yushi},
title = {Session Details: Multimedia Grand Challenge},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246417},
doi = {10.1145/3246417},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396480,
author = {Joly, Alexis and Champ, Julien and Letessier, Pierre and Herv\'{e}, Nicolas and Buisson, Olivier and Viaud, Marie-Luce},
title = {Visual-Based Transmedia Events Detection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396480},
doi = {10.1145/2393347.2396480},
abstract = {This paper presents a visual-based media event detection system based on the automatic discovery of the most circulated images across the main news media (news websites, press agencies, TV news and newspapers). Its main originality is to rely on the transmedia contextual information to denoise the raw visual detections and consequently focus on the most salient transmedia events.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1351–1352},
numpages = {2},
keywords = {transmedia, event detection, mining},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396481,
author = {Chen, Hsuan-Sheng and Tsai, Wen-Jiin},
title = {Technicolor Challenge: An Event Classification Framework by Probabilistic Context Modeling of Multimodal Features},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396481},
doi = {10.1145/2393347.2396481},
abstract = {Semantic high-level event recognition of videos is one of most interesting issues for multimedia searching and indexing. Since high-level events are usually domain-specific, a generic framework which can adapt itself to new domains without or with a few modifications is needed. To this end, this paper presents a generic framework for video event classification using temporal context of interval-based multimodal features. In the framework, a co-occurrence symbol transformation method is proposed to explore full temporal relations among multiple modalities in probabilistic HMM event classification. The results of our experiments on baseball video event classification demonstrate the superiority of the proposed approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1353–1354},
numpages = {2},
keywords = {co-occurrence symbol, multimodal feature, semantics, interval-based representation, probabilistic modeling, temporal relation, HMM, multivariate temporal data classification, event classification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396482,
author = {Bansal, Romil and Kumaran, Radhika and Mahajan, Diwakar and Khurdiya, Arpit and Dey, Lipika and Ghosh, Hiranmay},
title = {TWIPIX: A Web Magazine Curated from Social Media},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396482},
doi = {10.1145/2393347.2396482},
abstract = {This paper describes a method to identify events being vigorously discussed on social media and to present them in the form of a web-based daily magazine. Tweet texts and hyperlinked information sources, such as images and news articles, are analyzed to discover the events. The events are selected for presentation using an "interestingness factor", which combines several facets of the discussions surrounding the events. The events are correlated based on their content similarity.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1355–1356},
numpages = {2},
keywords = {event, Olympics, web magazine, tweet},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396483,
author = {Bao, Bing-Kun and Min, Weiqing and Sang, Jitao and Xu, Changsheng},
title = {Multimedia News Digger on Emerging Topics from Social Streams},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396483},
doi = {10.1145/2393347.2396483},
abstract = {With the overwhelming information from social media networks and news portals, it is crucial to provide users a complete package of visual and textual information with popular interests automatically. To this concern, we present a news detection and pushing system, called Me-Digger (Multimedia News Digger), which not only effectively detects emerging topics from social streams but also provides the corresponding information in multiple modalities. Me-digger is the first systematic effort to leverage three sources of data, that is, Twitter, Flickr and Google news, to output with vivid visual and textual contents on emerging topics. Enabled by a novel general-structured high-order co-clustering approach, it has a more accurate detection of emerging topics compared to the existing methods on micro-blog social streams.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1357–1358},
numpages = {2},
keywords = {emerging topic detection, co-clustering, cross media},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396484,
author = {Wang, Zhiyu and Cui, Peng and Xie, Lexing and Chen, Hao and Zhu, Wenwu and Yang, Shiqiang},
title = {Analyzing Social Media via Event Facets},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396484},
doi = {10.1145/2393347.2396484},
abstract = {Microblog is a prominent information platform for sharing experiences, discussing current events, and exchanging ideas. Many events are first reported in social media, and increasing amounts of rich-media content are associated with the posts, making them more credible and attractive. We design a rich-media analysis system to address the important challenge of sensing and exploring events from social media in real-time. The system includes a novel bilateral correspondence topic model to extract representative content and meaningful facets about events over time. It also includes a digital magazine that anchors user interactions with event facets. We demonstrate several examples from more than 4 million rich media microblogs, showing the effectiveness of key content extraction and natrual interactions with facets.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1359–1360},
numpages = {2},
keywords = {multi-modal microblog, BC-LDA, automatic layout},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396485,
author = {Chan, Yin-Tzu and Hsu, Hao-Chen and Li, Po-Yi and Yeh, Mei-Chen},
title = {Automatic Cinemagraphs for Ranking Beautiful Scenes},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396485},
doi = {10.1145/2393347.2396485},
abstract = {This work addresses the NHK challenge that aims at automatic recognition of beautiful scenes in broadcast programs. We propose a method that can be used to automatically extract beautiful scenes from videos and rank the scenes in terms of beauty. In particular, we introduce cinemagraphs as an alternative manner for presenting beautiful scenes, and a notion of beauty based on the presence of interesting motions. The method is fully automatic, requires no training phase, and produces beautiful scene cinemagraphs as by-products.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1361–1362},
numpages = {2},
keywords = {beauty, cinemagraph, aesthetic quality assessment},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396486,
author = {Redi, Miriam and Merialdo, Bernard},
title = {Where is the Beauty? Retrieving Appealing VideoScenes by Learning Flickr-Based Graded Judgments},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396486},
doi = {10.1145/2393347.2396486},
abstract = {In this paper we describe a system that automatically extracts appealing scenes from a set of broadcasting videos. Unlike traditional computational aesthetic models that try to predict the hardly measurable degree of "beauty", we chose to build a system that retrieves "interesting" scenes. We create a training database of Flickr images annotated with their corresponding Flickr "interestingness" degree. We then extract existing and novel aesthetic/semantic features from the training set. Based on such features, we build a graded-relevance "interestingness" model and we rank the test shots according to their predicted "interestingness".},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1363–1364},
numpages = {2},
keywords = {image aesthetics, interestingness, semantic indexing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396487,
author = {Li, Bing and Feng, Songhe and Xiong, Weihua and Hu, Weiming},
title = {Scaring or Pleasing: Exploit Emotional Impact of an Image},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396487},
doi = {10.1145/2393347.2396487},
abstract = {Automatic image emotion analysis has emerged as a hot topic due to its potential application on high-level image understanding. Considering the fact that the emotion evoked by an image is not only from its global appearance but also interplays among local regions, we propose a novel affective image classification system based on bilayer sparse representation (BSR). The BSR model contains two layers: The global sparse representation (GSR) is to define global similarities between a test image and all the training images; and the local sparse representation (LSR) is to define similarities of local regions' appearances and their co-occurrence between a test image and all the training images. The experiments on real data sets demonstrate that our system is effective on image emotion recognition.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1365–1366},
numpages = {2},
keywords = {affective image classification, bilayer sparse representation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396488,
author = {Lux, Mathias and Taschwer, Mario and Marques, Oge},
title = {Classification of Photos Based on Good Feelings: ACM MM 2012 Multimedia Grand Challenge Submission},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396488},
doi = {10.1145/2393347.2396488},
abstract = {Many photo-taking scenarios have emotional triggers such as happiness, joy, or in general a "good feeling". In this paper we postulate that if a photo was taken to preserve a good feeling, it will most likely trigger a good feeling when re-visiting the photo later on. We present an approach to classify photos and decide automatically whether they have been taken to preserve a good feeling or not. We employ an annotated data set to create a model and apply this model to enhance classical image search. The result is a novel search approach, demonstrated by a proof-of-concept application capable of performing keyword-based search and filtering out the results that are considered unlikely to convey a good feeling to the viewer.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1367–1368},
numpages = {2},
keywords = {affection, image classification, image search, user intentions},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396489,
author = {Wang, Xiaohui and Jia, Jia and Hu, Peiyun and Wu, Sen and Tang, Jie and Cai, Lianhong},
title = {Understanding the Emotional Impact of Images},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396489},
doi = {10.1145/2393347.2396489},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1369–1370},
numpages = {2},
keywords = {social networks, emotional impact, prediction model},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396490,
author = {Vonikakis, Vassilios and Winkler, Stefan},
title = {Emotion-Based Sequence of Family Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396490},
doi = {10.1145/2393347.2396490},
abstract = {This paper presents a method for the automatic creation of slideshows from family photo collections based on the emotions of a given group of people. The user specifies the desired person(s) to be included in the slideshow. A natural image sequence is formed based on people's emotions and several other, user-defined image similarity attributes, in order to form meaningful slideshow transitions. This process makes use of a new image dissimilarity function, which can integrate various attribute combinations and preferences, making the system highly user-adaptable and flexible.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1371–1372},
numpages = {2},
keywords = {automatic slideshow, people identification, emotion estimation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396491,
author = {Liu, Zhenbao and Tang, Sicong and Qin, Hongliang and Bu, Shuhui},
title = {Evaluating User's Energy Consumption Using Kinect Based Skeleton Tracking},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396491},
doi = {10.1145/2393347.2396491},
abstract = {By utilizing the dataset provided by 3DLife/Huawei Challenge of ACM Multimedia, we propose a refreshing application that automatically evaluates player's energy consumption in gaming scenarios by a model with tracked skeleton, which may help users to know their exercise effects and even diet or reduce their weights. We develop a program to compute the energy consumption in real time by analyzing data captured from Microsoft Kinect, and also give a cue in the dynamic interaction. We model 3D human skeleton by joining different body parts with 15 nodes, and decompose player action into rigid body motions of these parts. Amount of energy consumed in the action is calculated as the sum of powers required to overcome gravity of each part. Experimental results show that instantaneous and total energy consumption of different dancers can be stably calculated. The hardware system is based on low-price Kinect, and easily accepted by users. The proposed application also provides a quantitative approach which help users to control their dining and exercise intensity.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1373–1374},
numpages = {2},
keywords = {energy consumption, skeleton tracking, kinect},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396492,
author = {Liutkus, Antoine and Dremeau, Ang\'{e}lique and Alexiadis, Dimitrios and Essid, Slim and Daras, Petros},
title = {Analysis of Dance Movements Using Gaussian Processes: Extended Abstract},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396492},
doi = {10.1145/2393347.2396492},
abstract = {This work addresses the Huawei/3DLife Grand Challenge, presenting a novel method for the analysis of dance movements. The approach focuses on the decomposition of the dance movements into elementary motions. Placing this problem into a probabilistic framework, we propose to exploit Gaussian processes to accurately model the different components of the decomposition. The preliminary results, presented in this paper, are very promising. In particular, two applications are considered, illustrating the relevance of the proposed approach, namely the correction of tracking errors and the smoothing of some movements of the teacher to help toward the dance learning.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1375–1376},
numpages = {2},
keywords = {dance analysis, gaussian process, 3Dlife, interactive environments, grand challenge},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396493,
author = {Yu, Yi and Shen, Zhijie and Zimmermann, Roger},
title = {Automatic Music Soundtrack Generation for Outdoor Videos from Contextual Sensor Information},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396493},
doi = {10.1145/2393347.2396493},
abstract = {We present a system to automatically generate soundtracks for user-generated outdoor videos (UGV) based on concurrently captured contextual sensor information with mobile apps for the ACM Multimedia 2012 Google challenge: Automatic Music Video Generation. Our method addresses the use case of making "a video much more attractive for sharing by adding a matching soundtrack to it." Our system correlates viewable scene information from sensors with geographic contextual tags from OpenStreetMap. The co-occurance of geo-tags and mood tags are investigated from a set of categories of the web site Foursquare.com and a mapping from geo-tags to mood tags is obtained. Finally, a music retrieval component returns music based on matching mood tags. The experimental results show that our system can successfully create soundtracks that are related to the mood and situation of UGVs and therefore enhance the enjoyment of viewers. Our system sends only sensor data to a cloud service and is therefore bandwidth efficient since video data does not need to be transmitted for analysis.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1377–1378},
numpages = {2},
keywords = {music mood, music soundtrack generation, geographic tagging, location sensors, location-mood categories, mobile videos},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396494,
author = {Wang, Ju-Chiang and Yang, Yi-Hsuan and Jhuo, I-Hong and Lin, Yen-Yu and Wang, Hsin-Min},
title = {The Acousticvisual Emotion Guassians Model for Automatic Generation of Music Video},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396494},
doi = {10.1145/2393347.2396494},
abstract = {This paper presents a novel content-based system that utilizes the perceived emotion of multimedia content as a bridge to connect music and video. Specifically, we propose a novel machine learning framework, called Acousticvisual Emotion Guassians (AVEG), to jointly learn the tripartite relationship among music, video, and emotion from an emotion-annotated corpus of music videos. For a music piece (or a video sequence), the AVEG model is applied to predict its emotion distribution in a stochastic emotion space from the corresponding low-level acoustic (resp. visual) features. Finally, music and video are matched by measuring the similarity between the two corresponding emotion distributions, based on a distance measure such as KL divergence.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1379–1380},
numpages = {2},
keywords = {cross-modal media retrieval, emotion recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396495,
author = {Wu, Xixuan and Xu, Bing and Qiao, Yu and Tang, Xiaoou},
title = {Automatic Music Video Generation: Cross Matching of Music and Image},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396495},
doi = {10.1145/2393347.2396495},
abstract = {Music and image are two most popular media on the Internet. Human perception of music and image are highly correlated. Music video is one of such products, in which music and image are complement to each other. In this paper, we present a system which can automatically generate music video for a given song. The challenge of such system comes from how to select relative images and align them with the song. This paper deals with this challenge by leveraging lyrics (if exists) and the semantic similarity between music and image. We retrieve related image in internet with lyrics keyword as query and use a learning based method to estimate a semantic score between an image and a music segment. Finally we construct a music video after quality filtering and refinement. Our system also allows users to upload their images and re-pick recommended images to personalize the music video.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1381–1382},
numpages = {2},
keywords = {music video generation, music-image similarity estimation, multiple ranking canonical correlation analysis},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396496,
author = {Liem, Cynthia C. S. and Bazzica, Alessio and Hanjalic, Alan},
title = {MuseSync: Standing on the Shoulders of Hollywood},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396496},
doi = {10.1145/2393347.2396496},
abstract = {In this extended abstract, we present a novel story-driven approach to soundtrack retrieval for user-generated videos. Cinematic knowledge on cross-modal associations is exploited through folksonomic story text retrieval from collaborative online metadata resources. Subsequently, audiovisual synchronization is applied based on high-level features described by users. The approach is demonstrated in the MuseSync prototype system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1383–1384},
numpages = {2},
keywords = {popular media, multimodal techniques, audiovisual synchronization, web video, folksonomy, cross-modal connotation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246418,
author = {Ngo, Chong-Wah and Yanai, Keiji},
title = {Session Details: Doctoral Symposium Session 1: Best Paper Session},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246418},
doi = {10.1145/3246418},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396498,
author = {Chen, Yan-Ying},
title = {People Search and Activity Mining in Large-Scale Community-Contributed Photos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396498},
doi = {10.1145/2393347.2396498},
abstract = {A growing number of users are contributing a huge amount of photos containing people (e.g., family, classmates, colleagues, etc.) to social media for the purpose of photo sharing and social communication. There arises a strong need for automatically analyzing the people shown in large-scale photos because these visual data comprise abundant consumer activities which greatly benefit demographic analysis and enhance marketing research. In this work, we aim at learning facial attributes (gender, race, age, etc.) by these publicly available photos and exploiting the detected facial attributes for locating designated persons, profiling user preferences and predicting social group types. In addition, community-contributed data possess rich contexts such as tags, geo-locations and time stamps, which strongly correlate with user intentions and preferences. The knowledge would be informative to actively refine the recognition models and promising towards improvement of photo management, personalized recommendation and social networking. Most importantly, this framework effectively relieves costly annotation efforts and ensures scalability for large-scale media.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1385–1388},
numpages = {4},
keywords = {personalized recommendation, image retrieval, social subgraphs, facial attributes},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396499,
author = {Zhang, John R.},
title = {Upper Body Gestures in Lecture Videos: Indexing and Correlating to Pedagogical Significance},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396499},
doi = {10.1145/2393347.2396499},
abstract = {The growth of digitally recorded educational lectures has led to a problem of information overload. Semantic video browsers present one solution whereby content-based features are used to highlight points of interest. We focus on the domain of single-instructor lecture videos. We hypothesize that arm and upper body gestures made by the instructor can yield significant pedagogic information regarding the content being discussed such as importance and difficulty. Furthermore, these gestures may be classified, automatically detected and correlated to pedagogic significance (e.g., highlighting a subtopic which may be a focal point of a lecture). This information may be used as cues for a semantic video browser. We propose a fully automatic system which, given a lecture video as input, will segment the video into gestures and then identify each gesture according to a refined taxonomy. These gestures will then be correlated to a vocabulary of significance. We also plan to extract other features of gestures such as speed and size and examine their correlation to pedagogic significance. We propose to develop body part recognition and temporal segmentation techniques to aid natural gesture recognition. Finally, we plan to test and verify the efficacy of this hypothesis and system on a corpus of lecture videos by integrating the points of pedagogic significance as indicated by the gestural information into a semantic video browser and performing user studies. The user studies will measure the accuracy of the correlation as well as the usefulness of the integrated browser.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1389–1392},
numpages = {4},
keywords = {pedagogy, body model, gesture, lecture, pose},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396500,
author = {Yonetani, Ryo},
title = {Modeling Video Viewing Behaviors for Viewer State Estimation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396500},
doi = {10.1145/2393347.2396500},
abstract = {Human gaze behaviors when watching videos reflect their cognitive states as well as characteristics of the video scenes being watched. Our goal is to establish a method to estimate the viewer states from his/her eye movements toward general videos, such as TV news and commercials. The proposed method is based on a novel model of video viewing behaviors, which takes into account structural and statistical relationships between video dynamics, gaze dynamics and viewer states. This model realizes statistical learning of gaze information while considering dynamic characteristics of video scenes to achieve viewer-state estimation. In this paper, we present an overview of the viewer-state estimation method based on the model of video-viewing behaviors, including several past work done by the author's team.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1393–1396},
numpages = {4},
keywords = {saliency dynamics, gaze behavior, viewer state estimation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396501,
author = {Zhao, Zhen Wei},
title = {Challenges in Supporting Non-Linear and Non-Continuous Media Access in P2P Systems},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396501},
doi = {10.1145/2393347.2396501},
abstract = {With the proliferation of interactive media, non-linear and non-continuous media accesses have become a norm. Examples of such interactive media include video-on-demand, massive multiplayer virtual environments, massively multiplayer online game, zoomable video, Google Earth, and free-viewpoint video. To enable scalable delivery of these media contents to potentially millions of users, P2P architecture is often adopted. Non-linear and non-continuous user access pattern, however, pose new challenges on P2P interactive media streaming systems. These challenges include the need of more sophisticated prefetching scheme, increasing server load, high demand for short content discovery time, and difficulty of bandwidth allocation. We first try to understand the effect of non-linear and non-continuous media access on the streaming system performance, in particular, the server load, through analytical modeling. Then a novel user access pattern driven content discovery mechanism is proposed to address the content discovery challenge during non-continuous access. Works aiming at the rest of challenges are ongoing.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1397–1400},
numpages = {4},
keywords = {non-linear access pattern, user interaction, peer-to-peer, non-continuous access pattern, streaming},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246419,
author = {Ngo, Chong-Wah and Yanai, Keiji},
title = {Session Details: Doctoral Symposium 2: Oral Paper Session},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246419},
doi = {10.1145/3246419},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396503,
author = {Yousefi, Shahrouz},
title = {3D Photo Browsing for Future Mobile Devices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396503},
doi = {10.1145/2393347.2396503},
abstract = {By introducing the interactive 3D photo/video browsing and exploration system, we propose novel approaches for handling the limitations of the current 2D mobile technology from two aspects: interaction design and visualization. Our contributions feature an effective interaction that happens in the 3D space behind the mobile device's camera. 3D motion analysis of the user's gesture captured by the device's camera is performed to facilitate the interaction between users and multimedia collections in various applications. This approach will solve a wide range of problems with the current input facilities such as miniature keyboards, tiny joysticks and 2D touch screens. The suggested interactive technology enables users to control, manipulate, organize, and re-arrange their photo/video collections in 3D space using bare-hand, marker-less gesture. Moreover, with the proposed techniques we aim to visualize the 2D photo collection, in 3D, on normal 2D displays. This process is automatically done by retrieving the 3D structure from single images, finding the stereo/multiple views of a scene or using the geo-tagged meta-data from huge photo collections. By using the design and implementation of the contributions of this work, we aim to achieve the following goals: Solving the limitations of the current 2D interaction facilities by 3D gestural interaction; Increasing the usability of the multimedia applications on mobile devices; Enhancing the quality of user experience with the digital collections.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1401–1404},
numpages = {4},
keywords = {photo browsing, 3D visualization, motion analysis, 3D gestural interaction, quality of experience},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396504,
author = {Walber, Tina},
title = {Making Use of Eye Tracking Information in Image Collection Creation and Region Annotation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396504},
doi = {10.1145/2393347.2396504},
abstract = {The goal of this work is to implicitly gain information about images from human eye movements and to use this information to improve the handling of images. Users' points of gaze are measured with an eye tracker while they are viewing or tagging images. By means of the gaze data, image selections will be created automatically and tags will be assigned to specific image regions. So far, we have shown that one can assign given tags to regions in a manually segmented image.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1405–1408},
numpages = {4},
keywords = {segmentation, region labeling, image collection, eye tracking},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396505,
author = {Lin, Yen-Liang},
title = {Investigating 3D Model and Part Information for Improving Content-Based and Attribute-Based Object Retrieval},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396505},
doi = {10.1145/2393347.2396505},
abstract = {Content-based and attribute-based image/object retrieval are the key techniques for managing and analyzing the exponentially growing media collections. However, due to large variations in viewing angle/position, illumination, occlusion, and background, traditional object retrieval is extremely challenging. This article discusses an ongoing research in investigating the value of 3D models and informative parts for the object retrieval system. To show the feasibility of the proposed method, we apply it to the problem of multi-view vehicle retrieval. Preliminary results on vehicle retrieval demonstrate that our approach significantly outperforms the prior content-based image retrieval (CBIR) methods. Motivated by the proposed method, we are now extending the framework to attribute-based retrieval system over other domains (e.g., people) with 3D information augmented by Kinect devices.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1409–1412},
numpages = {4},
keywords = {vehicle retrieval, 3D vehicle model fitting, part rectification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396506,
author = {Roodaki, Hoda},
title = {An Adaptive Framework for Scalable Multi-View Video Coding for the H.264/AVC Standard},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396506},
doi = {10.1145/2393347.2396506},
abstract = {Multi-view 3D video is currently attracting growing attention in several applications such as the 3DTV, free-view point video and entertainment industry where it can be used to provide multi-perspective viewing and 3D scene experiences. In multi-view 3D video, several 3D video sequences should be captured simultaneously from the same scene but through different viewing angles. One of the major challenges in this field is how to transmit the large amount of data of a multi-view 3D video sequence over error prone channels to heterogeneous devices with different bandwidth, resolution, and processing power, while maintaining a high visual quality. Scalable Multi-view 3D Video Coding (SMVC) is one of the methods to address this challenge. But there are many difficulties in SMVC that makes it impractical in most 3D video applications. In this work, we propose an adaptive framework to use SMVC in various 3D video applications effectively. The current prototype shows enhanced capability in handling the existing 3D video applications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1413–1416},
numpages = {4},
keywords = {scalable multi-view 3D video quality assessment, rate control, SMVC data assignment, scalable modalities},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396507,
author = {Shen, Yun-Chung},
title = {Distributed Video Coding with Improved Side Information Refinement and Parallelized Architecture Design},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396507},
doi = {10.1145/2393347.2396507},
abstract = {Despite gaining certain improvements in coding performances recently, the effectiveness (in terms of rate-distortion) and the efficiency (in terms of decoding speed) of the existing Wyner-Ziv (WZ) codecs are far behind that of the state-of-the-art prediction-based video coding standards. In other words, for decades, the poor RD performance and the high coding delay hinder WZ codecs from being applied to practical applications. To respond to the above challenges, an effective and efficient WZ video coding architecture, referred to herein as "Improved Side Information and Parallelized Design" (ISIPaD) for distributed video coding (DVC), is proposed and realized to enhance the RD performance and the execution speed at the same time. This work details the proposed coding scheme and shows its RD superiority over the related works. With much enhanced performances in both rate distortion (up to 3.6dB in RD measures) and execution time (60.97 times faster in speed), as compared with those of the state-of-the-art WZ video codecs, our work demonstrates a great potential for DVC being applied to real applications in the near future.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1417–1420},
numpages = {4},
keywords = {distributed video coding, Wyner-Ziv video coding, side information refinement, block mode selection},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246420,
author = {Ngo, Chong-Wah and Yanai, Keiji},
title = {Session Details: Doctoral Symposium 3: Poster Session},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246420},
doi = {10.1145/3246420},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396509,
author = {Sang, Jitao},
title = {Collective Search and Recommendation in Social Media},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396509},
doi = {10.1145/2393347.2396509},
abstract = {This PhD thesis proposal is focused on proposing solutions to the problem of collective search and recommendation in social media. User and data are two fundamental elements under social media environment. To cope with the semantic gap between social media data and semantic meaning, and the complexity of user intent and requirements, we propose to conduct research on three stages: (1) multimedia content analysis; (2) user understanding and (3)collective search and recommendation. We address the large-scale, multi-modal and heterogeneous characteristics of social media analysis by developing methodology from factor analysis, generative topic model and collaborative filtering. Progresses and advances along the three research lines have been presented, with future directions and open discussions concluded in the end.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1421–1424},
numpages = {4},
keywords = {social media, collective search, collective recommendation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396510,
author = {Lindner, Albrecht},
title = {Semantic Awareness for Automatic Image Interpretation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396510},
doi = {10.1145/2393347.2396510},
abstract = {This thesis aims at developing methods to make digital devices more automatic and intuitive while focusing on image-related applications. We learn associations between image characteristics and keywords with a statistical framework based on large databases of annotated images. Such associations are widely exploitable and we demonstrate two main applications: semantic image enhancement and automatic color naming. Both applications show convincing results and suggest that the framework can be extended to other areas.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1425–1428},
numpages = {4},
keywords = {statistics, large scale, crowd source, semantics, data mining, image processing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396511,
author = {Ballas, Nicolas and Delezoide, Bertrand and Pr\^{e}teux, Fran\c{c}oise},
title = {Trajectory Signature for Action Recognition in Video},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396511},
doi = {10.1145/2393347.2396511},
abstract = {Bag-of-Words representation based on trajectory local features and taking into account the spatio-temporal context through static segmentation grids is currently the leading paradigm to perform action annotation.While providing a coarse localization of low-level features, those approaches tend to be limited by the grid rigidity. In this work we propose two contributions on trajectory based signatures. First, we extend a local trajectory feature to characterize the acceleration in videos, leading to invariance to camera constant motion. We also introduce two new adaptive segmentation grids, namely Adaptive Grid (AG) and Deformable Adaptive Grid (DAG). AG is learnt from videos data, to fit a given dataset and overcome static grid rigidity. DAG is also learnt from video data. Moreover, it can be adapted to a specific video through a deformation operation. Our adaptive grids are then exploited by a Bag-of-Words model at the aggregation step for action recognition. Our proposal is evaluated on 4 publicly available datasets.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1429–1432},
numpages = {4},
keywords = {tunnel, bag-of-words, spatio-temporal context, trajectory, action recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396512,
author = {Miller, Chreston},
title = {Interactive Data-Driven Search and Discovery of Temporal Behavior Patterns from Media Streams},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396512},
doi = {10.1145/2393347.2396512},
abstract = {The presented thesis work addresses how social scientists may derive patterns of human behavior captured in media streams. Currently, media streams are being segmented into sequences of events describing the actions captured in the streams, such as the interactions among humans. This segmentation creates a challenging data space to search characterized by non-numerical, temporal, descriptive data, e.g., Person A walks up to Person B at time T. We present an approach that allows one to interactively search and discover temporal behavior patterns within such a data space.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1433–1436},
numpages = {4},
keywords = {structural model learning, model evolution, temporal behavior models, human-machine cooperation, event data},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396513,
author = {Okura, Fumio},
title = {Spacetime Freeview Generation Using Image-Based Rendering, Relighting, and Augmented Telepresence},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396513},
doi = {10.1145/2393347.2396513},
abstract = {This paper proposes an freeview generation technique providing the users to change their viewpoints beyond time and space. The study consists of three technical elements: image-based rendering, relighting, and augmented telepresence. Before now, we have developed two systems relating this study: an augmented telepresence system and a full spherical HDR aerial imaging system.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1437–1440},
numpages = {4},
keywords = {freeview generation, image-based rendering, relighting, augmented telepresence, aerial omnidirectional image},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396514,
author = {Adzic, Velibor},
title = {What You See is What You Should Get},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396514},
doi = {10.1145/2393347.2396514},
abstract = {This paper summarizes our work on optimization of video content compression in the context of the end user. I will briefly explain motivation and novelty of the work that presents the bulk of my dissertation research. Preliminary experiments with promising results are encouraging further steps that should lead to completion of a model for video compression that uses most of available bits for the video content that is actually seen. By this we mean the content that user attends to. Everything else is coded with much less bits, leading to significant savings compared to state-of-the-art coding techniques. Our work should be regarded as extension and not replacement of the hybrid coding paradigm.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1441–1444},
numpages = {4},
keywords = {cognition, perception, video compression},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396515,
author = {Lin, Yu-Hsun},
title = {3D Multimedia Signal Processing},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396515},
doi = {10.1145/2393347.2396515},
abstract = {3D multimedia attracts people by the vivid and the fascinating depth perception. The movie industry is in the Renaissance phase with the aid of 3D multimedia since the 3D experience will be amplified in the movie theater. Although we are living in a 3D world, the development of 3D multimedia just started in recent years. Therefore, we want to propose a 3D multimedia signal processing framework to deal with the various challenges coming from this newly appeared media. We have addressed the research issues regarding content creation, data compression and content protection with which the traditional 2D multimedia based approaches will not performed efficiently and properly. Furthermore, we are studying the core issue of 3D multimedia which is the quality assessment of stereopsis. We expect the research of 3D multimedia signal processing can further popularize the 3D contents and brings new thoughts of multimedia research to enable new paradigms of 3D multimedia applications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1445–1448},
numpages = {4},
keywords = {3D multimedia, human visual system, signal processing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246421,
author = {Sano, Masanori},
title = {Session Details: Open Source Software Competition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246421},
doi = {10.1145/3246421},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396517,
author = {Anjos, Andr\'{e} and El-Shafey, Laurent and Wallace, Roy and G\"{u}nther, Manuel and McCool, Christopher and Marcel, S\'{e}bastien},
title = {Bob: A Free Signal Processing and Machine Learning Toolbox for Researchers},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396517},
doi = {10.1145/2393347.2396517},
abstract = {Bob is a free signal processing and machine learning toolbox originally developed by the Biometrics group at Idiap Research Institute, Switzerland. The toolbox is designed to meet the needs of researchers by reducing development time and efficiently processing data. Firstly, Bob provides a researcher-friendly Python environment for rapid development. Secondly, efficient processing of large amounts of multimedia data is provided by fast C++ implementations of identified bottlenecks. The Python environment is integrated seamlessly with the C++ library, which ensures the library is easy to use and extensible. Thirdly, Bob supports reproducible research through its integrated experimental protocols for several databases. Finally, a strong emphasis is placed on code clarity, documentation, and thorough unit testing. Bob is thus an attractive resource for researchers due to this unique combination of ease of use, efficiency, extensibility and transparency. Bob is an open-source library and an ongoing community effort.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1449–1452},
numpages = {4},
keywords = {machine learning, biometrics, signal processing, pattern recognition, computer vision, open source},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396518,
author = {Chandra, Surendar and Rowe, Lawrence A.},
title = {DisplayCast: A High Performance Screen Sharing System for Intranets},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396518},
doi = {10.1145/2393347.2396518},
abstract = {DisplayCast is a many to many screen sharing system that is targeted towards Intranet scenarios. The capture software runs on all computers whose screens need to be shared. It uses an application agnostic screen capture mechanism that creates a sequence of pixmap images of the screen updates. It transforms these pixmaps to vastly improve the lossless Zlib compression performance. These algorithms were developed after an extensive analysis of typical screen contents. DisplayCast shares the processor and network resources required for screen capture, compression and transmission with host applications whose output needs to be shared. It balances the need for high performance screen capture with reducing its resource interference with user applications. DisplayCast uses Zeroconf for naming and asynchronous location. It provides support for Cisco WiFi and Bluetooth based localization. It also includes a HTTP/REST based controller for remote session initiation and control. DisplayCast supports screen capture and playback in computers running Windows 7 and Mac OS X operating systems. Remote screens can be archived into a H.264 encoded movie on a Mac. They can also be played back in real time on Apple iPhones and iPads. The software is released under a New BSD license.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1453–1456},
numpages = {4},
keywords = {screencast, screen sharing, screen capture, displaycast},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396519,
author = {Holub, Petr and Matela, Ji\v{r}\'{\i} and Pulec, Martin and \v{S}rom, Martin},
title = {UltraGrid: Low-Latency High-Quality Video Transmissions on Commodity Hardware},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396519},
doi = {10.1145/2393347.2396519},
abstract = {This paper gives a brief overview of UltraGrid---a high-quality video transmission software. Employing a commodity PC and Mac hardware, UltraGrid allows for transmission of video in resolutions ranging through HD up to 2160p/4K with end-to-end latency as low as 75ms. The high-quality is achieved either by transmitting uncompressed video streams or streams with low compression ratio. UltraGrid was originally developed as a research tool to demonstrate capabilities of 10Gbps networks and to study multi-point data distribution in high-speed network environments. Through years it has evolved into an open-source project intended to support applications having quality demands beyond the competence of standard videoconferencing or streaming tools. Nowadays, the main application areas are collaborative environments, medical, cinematography and broadcasting applications, as well as various educational activities.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1457–1460},
numpages = {4},
keywords = {video network transmission, collaborative systems, GPU parallel processing, videoconferncing, video processing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396520,
author = {Pang, Lei and Zhang, Wei and Tan, Hung-Khoon and Ngo, Chong-Wah},
title = {Video Hyperlinking: Libraries and Tools for Threading and Visualizing Large Video Collection},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396520},
doi = {10.1145/2393347.2396520},
abstract = {While HTML documents could be effortlessly hyperlinked by markup tags, creation of the hyperlinks for multimedia objects is by no means easy due to the involvement of various visual processing units and intensive computational overhead. This paper introduces an open source, named VIREO-VH, which provides end-to-end support for creating hyperlinks to thread and visualize collections of videos. The software components include video pre-processing, bag-of-words based inverted file indexing for scalable near-duplicate keyframe search, localization of partial near-duplicate segments, and galaxy visualization of video collection. The open source has been internally used by VIREO research team since 2007, and was evolved over years based on experiences through developing various multimedia applications.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1461–1464},
numpages = {4},
keywords = {partial near-duplicates, video hyperlinking, large-scale video browsing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396521,
author = {Pedersoli, Fabrizio and Adami, Nicola and Benini, Sergio and Leonardi, Riccardo},
title = {XKin -: EXtendable Hand Pose and Gesture Recognition Library for Kinect},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396521},
doi = {10.1145/2393347.2396521},
abstract = {In this work we provide an open-source framework for Kinect enabling more natural and intuitive hand-gesture communication between human and computer devices. The software package is endowed with useful tools for training the system to work with user-defined postures and gestures. The XKin project is fully implemented in C and freely available at https://github.com/fpeder/XKin under FreeBSD License. Our goal is to encourage contributions from other researchers and developers in building an open and effective system for empowering a natural modality for human-machine interaction.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1465–1468},
numpages = {4},
keywords = {kinect, gesture recognition, computer vision},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396522,
author = {Waltl, Markus and Rainer, Benjamin and Timmerer, Christian and Hellwagner, Hermann},
title = {A Toolset for the Authoring, Simulation, and Rendering of Sensory Experiences},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396522},
doi = {10.1145/2393347.2396522},
abstract = {This paper describes a toolset for the authoring, simulating, and rendering of multimedia content annotated with Sensory Effect Metadata (SEM) descriptions as specified in Part 3 of the MPEG V standard. This part of MPEG-V standardizes the description of sensory effects (e.g., light, wind) in order to be rendered on sensory devices (e.g., fans, vibration chairs) aiming at generating a sensory experience stimulating possibly all human senses. Our implementation comprises a toolset to author sensory effects associated with multimedia content and the simulation thereof. Furthermore, it includes a library, a standalone player, and a Web browser plug-in which enables the playback and rendering of sensory effects on off-the-shelf rendering devices and in various contexts. All software modules are available under the GNU General Public License (GPL) v3 and the GNU Lesser General Public License (LGPL) v3 respectively.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1469–1472},
numpages = {4},
keywords = {MPEG-V, sensory experience, web browser plug-in, media player, sensory effects, simulator, annotation tool},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246422,
author = {Mei, Tao},
title = {Session Details: Video Program},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246422},
doi = {10.1145/3246422},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396524,
author = {Guthier, Benjamin and Kopf, Stephan and Effelsberg, Wolfgang},
title = {A Real-Time System for Capturing HDR Videos},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396524},
doi = {10.1145/2393347.2396524},
abstract = {We present a system that is capable of capturing, processing and displaying High Dynamic Range (HDR) videos at 23 frames per second. For each frame in the video, a sequence of regular images is captured in quick succession at optimally chosen shutter speeds. Intermediate camera motion is compensated and the images are combined into an HDR frame which is tone mapped for display. Our system makes use of novel algorithms that are fast enough for processing the frames in real-time. The most time-consuming operations are parallelized and executed on a graphics card for a speed-up of 15 to 1 over the sequential version.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1473–1476},
numpages = {4},
keywords = {processing, display, high dynamic range},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396525,
author = {Lo, Raymond Chun Hing and Mann, Steve and Huang, Jason and Rampersad, Valmiki and Ai, Tao},
title = {High Dynamic Range (HDR) Video Image Processing for Digital Glass},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396525},
doi = {10.1145/2393347.2396525},
abstract = {We present highly parallelizable and computationally efficient High Dynamic Range (HDR) image compositing, reconstruction, and spatotonal mapping algorithms for processing HDR video. We implemented our algorithms in the EyeTap Digital Glass electric seeing aid, for use in everyday life. We also tested the algorithms in extreme dynamic range situations, such as, electric arc welding. Our system runs in real-time, and requires no user intervention, and no fine-tuning of parameters after a one-time calibration, even under a wide variety of very difficult lighting conditions (e.g. electric arc welding, including detailed inspection of the arc, weld puddle, and shielding gas in TIG welding). Our approach can render video at 1920x1080 pixel resolution at interactive frame rates that vary from 24 to 60 frames per second with GPU acceleration. We also implemented our system on FPGAs (Field Programmable Gate Arrays) for being miniaturized and built into eyeglass frames.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1477–1480},
numpages = {4},
keywords = {field programmable gate array, XDR, graphics processing unit, extreme HDR, spatial-tonal mapping, extreme high dynamic range, HDR video},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396526,
author = {Raghuraman, Suraj and Venkatraman, Karthik and Wang, Zhanyu and Wu, Jian and Clements, Jacob and Lotfian, Reza and Prabhakaran, Balakrishnan and Guo, Xiaohu and Jafari, Roozbeh and Nahrstedt, Klara},
title = {Immersive Multiplayer Tennis with Microsoft Kinect and Body Sensor Networks},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396526},
doi = {10.1145/2393347.2396526},
abstract = {We present an immersive gaming demonstration using the minimum amount of wearable sensors. The game demonstrated is two-player tennis. We combine a virtual environment with real 3D representations of physical objects like the players and the tennis racquet (if available). The main objective of the game is to provide as real an experience of tennis as possible, while also being as less intrusive as possible. The game is played across a network, and this opens the possibility of two remote players playing a game together on a single virtual tennis pitch. The Microsoft Kinect sensors are used to obtain a 3D point cloud and a skeletal map representation of the player. This 3D point cloud is mapped on to the virtual tennis pitch. We also use a wireless wearable Attitude and Heading Reference System (AHRS) mote, which is strapped onto the wrist of the players. This mote gives us precise information about the movement (swing, rotation etc.) of the playing arm. This information along with the skeletal map is used to implement the physics of the game. Using this game we demonstrate our solutions for simultaneous data acquisition, 3D point-cloud mapping in a virtual space, use of the Kinect and AHRS sensors to calibrate real and virtual objects and for interaction of virtual objects with a 3D point cloud.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1481–1484},
numpages = {4},
keywords = {tele-immersion, wireless sensor networks, gaming, virtual reality},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246423,
author = {Moriyama, Tomoe and Ando, Hideyuki},
title = {Session Details: Multimedia Art Exhibition},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246423},
doi = {10.1145/3246423},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396528,
author = {Sekikawa, Munehisa and Tsuji, Akinori and Kimoto, Keiko and Aihara, Ikkyu and Ito, Daisuke and Ueta, Tetsushi and Aihara, Kazuyuki and Kawakami, Hiroshi},
title = {Optically Coupled Oscillators (OCOs) -- LED Fireflies},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396528},
doi = {10.1145/2393347.2396528},
abstract = {We design oscillators controlled by optical input, which we call "LED fireflies," and study their synchronization phenomena. The most distinctive feature of LED fireflies is their collective behavior. The LED fireflies produce a huge variety of synchronous patterns. We demonstrate an optical art work that behaves like living entities by making use of their property.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1485–1486},
numpages = {2},
keywords = {bifurcation, oscillator, synchronization},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396529,
author = {Mima, Yoshiaki and Kimura, Ken-ichi and Yanagi, Hidekatsu},
title = {ThinkingGarden},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396529},
doi = {10.1145/2393347.2396529},
abstract = {The concept of the interactive installation ThinkingGarden is described, in this paper. ThinkingGarden is a system to make experiences that people usually have in Japanese garden. Those experiences are provided as the cycles of awareness, thinking, and understanding.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1487–1488},
numpages = {2},
keywords = {chance, garden, creativity, texture},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396530,
author = {Ando, Hideyuki and Watanabe, Junji and Sato, Masahiko},
title = {Empathetic Heartbeat},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396530},
doi = {10.1145/2393347.2396530},
abstract = {Empathy is important for our society and individuals, since it is what facilitates our interactions and connections to the other people around us. Our experience-based installation "empathetic heartbeat" aims to have the participant remind the existence of his/her heart in the internal body and recognize that our bodies are medium for feeling empathy with others. Here we describe the concept of the installation and participant's experience.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1489–1490},
numpages = {2},
keywords = {vibration, empathy, heartbeat, emotion},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396531,
author = {Ogawa, Hideaki and Ogawa, Emiko and Naveau, Manuela and Lindinger, Christopher and Haring, Roland and Gardiner, Matthew and Mara, Martina and H\"{o}rtner, Horst},
title = {Participatory Art Cards &amp; Archive System for Public Exhibition: A Case Study through Ars Wild Card},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396531},
doi = {10.1145/2393347.2396531},
abstract = {This paper describes the general idea of Ars Wild Card: a mobile application for visitors to create their own art cards, to guide interactive art exhibitions. The art cards created by visitors are collected and shared on the Internet. The Ars Wild Card system generates an archive through visitors' participatory process in the exhibition. This research aims to search for important factors to create a communication bridge between people and public art exhibition. We present our findings as a case study from our practical applications of Ars Wild Card.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1491–1492},
numpages = {2},
keywords = {mobile guidance, museum, creative catalyst, digital archive},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396532,
author = {Tsai, Shih-Ting and Chen, Ming-Hsiu Mia and Chang, Chi-Cheng and Kuo, Yu-Hung and Lawry, Miranda and Ting, Yi-Shu},
title = {The Silent Power: Applications of Research in Medical x-Ray Combining with Photography and Digital Graphic Design},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396532},
doi = {10.1145/2393347.2396532},
abstract = {X-ray is a kind of secret power, it is usually used on the medicine, and it can be perspective human body and some items which are not metals. In the graphic art and design filed, digital photography is the common method for creating image, go through this method and the after effect editing, so many alluring images are presented for the audience; however, these images which are created by digital photography only can present the externals. The internal world is always as a secret region, we maybe know what it is, but we couldn't comprehend how it is like, thus, x-ray is the best way to understand what the internals is. In this research, medical contrast is used with lily flower, let the flower absorb the contrast for 12 hours, and the contrast will winding through the whole flower, then exposure the flower by x-ray, therefore, because of wavelength of radiation and contrast, we can see the internal of stem and leaves. The second method is used on human body. One of the authors is model in this part, and takes photo with same view and angle by digital camera and medical x-ray; consequently, the audience could see the internal and external view of models' body at the same time. On the basis of this research project, the intended outcome is going to bring authors some new vision, and after go through the whole project, authors wish some new imaginative ideas would be created in the testing procedure, therefore we could obtain new thoughts and experience from this project, and the x-ray combining design could let us know this silent power could be so different and it can be used not just in medicine, and delivering more ideas and methods to the other designers; artists and researchers who are interested on the same topic in design.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1493–1494},
numpages = {2},
keywords = {x-ray, design},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396533,
author = {Luo, He-Lin and Hung, Yi-Ping},
title = {Interactive Art "Maelstrom&amp;Vortex": The Body's Speed -- a Race between Digital and Analog Speeds},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396533},
doi = {10.1145/2393347.2396533},
abstract = {With an explosion of accessible information, the digital era has been integrated into our daily lives. At any time, people can operate handheld computers (cell phones) to connect to the online world. The thrill of speed is slowly replacing the analog speed of the human body. Even as the human body attempts to catch up with the speed of a computer, it tries to flee from it at the same time. This type of hope returns us to an original and natural sense of ambivalence, so that, in post-digital times, people will attempt to find the divine light of the analog era. The work, "Maelstrom&amp;Vortex", utilizes interactive art to try to explain analog and digital speeds. Through the interaction, people can experience differences in speed between physical sensations and computer data as it re-combines them with a sense of space. In the work, the analog movement of mechanical motors, the digital capture of cameras, and digital computation of computers are combined, and then intervened with the analog-like human body. This allows participants to discover the relationship amongst analog, digital, and the body.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1495–1496},
numpages = {2},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396534,
author = {Shirose, Makoto and Hirose, Masahito and Oku, Kentato and Koide, Masato and Hirai, Naoya},
title = {Passage+: Mobile Content Platform of an Augmented Reality and Virtual Objects},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396534},
doi = {10.1145/2393347.2396534},
abstract = {This paper describes Passage+, a multimedia information presentation system for smartphones. The Passage+ gives one's path planning and route choice more diversities by displaying CG virtual objects, movies and images on the smartphone display. This work is achieved by using an Augmented Reality based on image recognition and GPS. Passage+ is an attempt to promote new discoveries by presenting alternative paths and entertaining routes. We adapted Passage+ to ACMMM's navigation system at Nara park, and collaborated with museums and cultural facilities nearby during the conference.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1497–1498},
numpages = {2},
keywords = {mobile augmented reality, exhibition, navigation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396535,
author = {Maruya, Kazushi and Uetsuki, Miki and Ando, Hideyuki and Watanabe, Junji},
title = {"Yu Bi Yomu": Interactive Reading of Dynamic Text},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396535},
doi = {10.1145/2393347.2396535},
abstract = {In this paper, we propose novel software to read text documents with tablet PCs and describe its installation plan. The software, "Yu bi Yomu," was developed as an iOS application, supposing to work on the Apple iPad. In that software, the contrast of displayed texts is changed dynamically in response to users' trailing behavior. In the installation, we will project the displays of tablet PCs operating "Yu bi Yomu" onto large screens attached to the wall or floor. The installation using this novel software provides new reading experiences.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1499–1500},
numpages = {2},
keywords = {tablet-PC, interactive user interface, dynamic text},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396536,
author = {Burbano Valdes, Andres E. and Bazo, Danny and DiCicco, \c{S}\"{o}len K. and Forbes, Angus},
title = {The New Dunites},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396536},
doi = {10.1145/2393347.2396536},
abstract = {The New Dunites is an interdisciplinary media arts research project that investigates the archeological site where the set for Cecile B. DeMille's The Ten Commandments was buried in 1923 [1]. In particular, this multi-phase endeavor involved the gathering of geophysical and archeological data, the historical study of the dawn of cinema in California, and a series of novel interactive multimedia installations that explored new avenues in the representation of scientific and cultural data.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1501–1502},
numpages = {2},
keywords = {media arts, new media, data visualization, geophysics, media archaeology, archaeology, cinema history},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246424,
author = {Luo, Jiebo and Venkatesh, Svetha},
title = {Session Details: Workshops},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246424},
doi = {10.1145/3246424},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396538,
author = {Cesar, Pablo and Shamma, David A. and Williams, Doug and Snoek, Cees G.M.},
title = {International Workshop on Socially-Aware Multimedia (SAM'12)},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396538},
doi = {10.1145/2393347.2396538},
abstract = {Multimedia social communication is filtering into everyday use. Videoconferencing is appearing in the living room and beyond, television is becoming smart and social, and media sharing applications are transforming the way we converse and recall events. The confluence of computer-mediated interaction, social networking, and multimedia content are radically reshaping social communications, bringing new challenges and opportunities. This workshop provides an opportunity to explore socially-aware multimedia, in which the social dimension of mediated interactions between people are considered as important as the characteristics of the media content. Even though this social dimension is implicitly addressed in some current solutions, further research is needed to better understand what makes multimedia socially-aware. In other words, social interactivity needs to become a first class citizen of multimedia research.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1503–1504},
numpages = {2},
keywords = {information retrieval, social media, multimedia, social communication, micro-blogging, communication, social interaction},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396539,
author = {Chen, Kuan-Ta and Chu, Wei-Ta and Larson, Martha and Ooi, Wei Tsang},
title = {ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396539},
doi = {10.1145/2393347.2396539},
abstract = {Crowdsourcing for multimedia involves exploiting both human intelligence and the combination of a large number of individual human contributions (i.e., the 'wisdom of the crowd') to develop techniques, systems and data sets that advance the state of the art. The ACM Multimedia 2012 Workshop on Crowdsourcing for Multimedia (CrowdMM 2012) provides a forum presenting crowdsourcing techniques for multimedia, as well as innovative ideas exemplifying how multimedia research can benefit from crowdsourcing. Through presented papers, invited talks and a panel, the workshop will promote interactive discussion on the scope and research potentials of crowdsourcing. The goal is to provide information to the multimedia research community on the principles of crowdsourcing and to inspire researchers to address the limitations of current studies by innovative use of human computation and collective intelligence. The workshop views crowdsourcing in the broad sense: it encompasses both unsolicited human contributions, e.g., tags assigned by users to images, and also solicited contributions, e.g., annotations gathered by making use of crowdsourcing platforms that micro-outsource tasks to a large pool of human workers.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1505–1506},
numpages = {2},
keywords = {human computing, multimedia system, crowdsourcing},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396540,
author = {Spampinato, Concetto and Mezaris, Vasileios and van Ossenbruggen, Jacco},
title = {Multimedia Analysis for Ecological Data},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396540},
doi = {10.1145/2393347.2396540},
abstract = {The ACM International Workshop on Multimedia Analysis for Ecological Data (MAED'12) is held as part of ACM Multimedia 2012. MAED'12 is concerned with the processing, interpretation, and visualization of ecology-related multimedia content with the aim to support biologists in their investigations for analyzing and monitoring natural environments, with particular attention to living organisms and pollution effects.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1507–1508},
numpages = {2},
keywords = {pollution monitoring, ecological multimedia data retrieval, multimedia content analysis, animal and plant identification},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396541,
author = {Liem, Cynthia C. S. and M\"{u}eller, Meinard and Tjoa, Steven K. and Tzanetakis, George},
title = {2nd International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM)},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396541},
doi = {10.1145/2393347.2396541},
abstract = {The International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies (MIRUM) at ACM Multimedia was proposed in order to gather experts from the Music and Multimedia Information Retrieval communities, as well as other neighboring fields, and to provide a high-profile platform for presenting current work on Music Information Retrieval with a strong focus on user-centered and multimodal approaches. Following a successful first edition at ACM Multimedia 2011, a second edition of MIRUM was held at ACM Multimedia 2012, which is the focus of this overview. After a description of the rationale and focus areas of the workshop, the accepted submissions and other program elements are summarized.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1509–1510},
numpages = {2},
keywords = {music information retrieval, multimodal music processing, user-centered design, cross-domain methodology transfer},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396542,
author = {Cao, Liangliang and Friedland, Gerald and Larson, Martha},
title = {GeoMM'12: ACM International Workshop on Geotagging and Its Applications in Multimedia},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396542},
doi = {10.1145/2393347.2396542},
abstract = {Geotagging is the process of adding geographical identification metadata to various media files such as photos, videos, websites, messages, and tweets. It is not limited to GPS sensor data but an extension of current multimedia files with a wide variety of location-specific information. The GeoMM'12 workshop presents research on recent research on geotagging within the context of multimedia analysis. This workshop aims to not only provide more cutting edge algorithms, but also motivate novel applications in this promising field.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1511–1512},
numpages = {2},
keywords = {geolocation, multimedia, geotagging},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396543,
author = {Friedland, Gerald and Ellis, Daniel PW and Metze, Florian},
title = {AMVA'12: ACM International Workshop on Audio and Multimedia Methods for Large-Scale Video Analysis},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396543},
doi = {10.1145/2393347.2396543},
abstract = {Media sharing sites on the Internet and the one-click upload capability of smartphones have led to a deluge of online multimedia content. Everyday, thousands of videos are uploaded into the web creating an ever-growing demand for methods to make them easier to retrieve, search, and index. While visual information is a very important part of a video, acoustic information often complements it. This is especially true for the analysis of consumer-produced, "unconstrained" videos from social media networks, such as YouTube uploads or Flickr content. The goal of the 1st ACM International Workshop on Audio and Multimedia Methods for Large-Scale Video Analysis (AMVA) is to bring together researchers and practitioners in this newly emerging field, and to foster discussion on future directions of the topic by providing a forum for focused exchanges on new ideas, developments, and results. The aim is to build a strong community and a venue that at some point can become its own conference.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1513–1514},
numpages = {2},
keywords = {video analysis, audio, big data, multimodal, multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396544,
author = {Hossain, M. Shamim and El Saddik, Abdulmotaleb},
title = {ACM International Workshop on Cloud-Based Multimedia Applications and Services for e-Health(CBMAS-EH 2012)},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396544},
doi = {10.1145/2393347.2396544},
abstract = {Cloud-Based Multimedia services and technologies are emerging as an innovative means of accessing and delivering e-health resources and services in the years to come. The research in multimedia cloud for E-health is still in its infancy, and several technical issues remain open. Prior to its general use and adoption, careful consideration and evaluation are required. This article provides a summary and overview of the First International ACM workshop on Cloud-Based Multimedia Applications and Services for E-Health.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1515–1516},
numpages = {2},
keywords = {e-health, media cloud computing and communication},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396545,
author = {Sano, Mutsuo and Ide, Ichiro and Yamakata, Yoko},
title = {Overview of the ACM Multimedia 2012 Workshop on Multimedia for Cooking and Eating Activities (CEA'12)},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396545},
doi = {10.1145/2393347.2396545},
abstract = {This overview introduces the aim of the CEA'12 workshop and the list of papers presented in the workshop.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1517–1518},
numpages = {2},
keywords = {eating, cooking, workshop, overview},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396546,
author = {Fonseca, David and Zabulis, Xenophon and Ramzan, Naeem and Kurti, Arianit and Pileggi, Salvatore Flavio and Ko, Heedong},
title = {1st International ACM Workshop on User Experience in E-Learning and Augmented Technologies in Education},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396546},
doi = {10.1145/2393347.2396546},
abstract = {UXeLATE2012 is the 1st International ACM Workshop on User Experience in e-Learning and Augmented Technologies in Education in conjunction with the ACM International Multimedia Conference (MM'12) at Nara, Japan. The workshop has a half day program, with a selection of six papers, and one keynote talk of a recognized expert in the field of usability, mobile technology and education.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1519–1520},
numpages = {2},
keywords = {e-learning, technologies in education, augmented reality, human-computer interaction, user experience},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396547,
author = {Oomen, Johan and Aroyo, Lora and Marchand-Maillet, St\'{e}phane and Douglass, Jeremy},
title = {Personalized Access to Cultural Heritage: Multimedia by the Crowd, for the Crowd},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396547},
doi = {10.1145/2393347.2396547},
abstract = {The primary goal of this workshop is to gather researchers and practitioners from different fields, e.g., multimedia retrieval, user interaction, arts and heritage curation, interface design and user modeling, in order to showcase novel applications and discuss opportunities that grow from the connections between users and multimedia systems in the cultural heritage domain.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1521–1522},
numpages = {2},
keywords = {multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396548,
author = {Shao, Ling and Shan, Caifeng and Etoh, Minoru},
title = {The Second ACM International Workshop on Interactive Multimedia on Mobile and Portable Devices},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396548},
doi = {10.1145/2393347.2396548},
abstract = {When mobile and portable devices become ubiquitous for people's daily life, how to design multimedia user interfaces of these products that enable natural, intuitive and fun interaction is one of the main challenges the multimedia community is facing. Following several successful events, the 2nd ACM International workshop on Interactive Multimedia on Mobile and Portable Devices (IMMPD'12) aims to bring together researchers from both academia and industry in domains including computer vision, audio and speech processing, machine learning, pattern recognition, communications, human-computer interaction, and media technology to share and discuss recent advances in interactive multimedia.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1523–1524},
numpages = {2},
keywords = {multimedia, pattern recognition, consumer electronics, human-computer interaction, computer vision},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/3246425,
author = {Boll, Susanne and Xu, Changsheng},
title = {Session Details: Tutorials},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3246425},
doi = {10.1145/3246425},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396550,
author = {Schaefer, Gerald},
title = {Interacting with Image Collections: Visualisation and Browsing of Image Repositories},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396550},
doi = {10.1145/2393347.2396550},
abstract = {In this tutorial paper, we look at a variety of techniques and methods for effective and intuitive image database visualisation and browsing. While interaction with traditional image retrieval systems can lead to a confusing and frustrating user experience, image browsing systems attempt to provide the user with an intuitive interface to manage potentially large image databases.We discuss how image databases are visualised in the three main approaches of mapping-based, clustering-based and graph-based image database navigation systems, and how intuitive browsing operations are supported.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1527–1528},
numpages = {2},
keywords = {image database visualisation, image database browsing, image databases},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396551,
author = {Friedland, Gerald},
title = {Privacy Concerns in Multimedia and Their Solutions},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396551},
doi = {10.1145/2393347.2396551},
abstract = {This article summarizes the corresponding 3-hour tutorial at ACM Multimedia 2012.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1529–1530},
numpages = {2},
keywords = {multimedia content analysis, privacy, societal concerns},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396552,
author = {Gunes, Hatice and Schuller, Bj\"{o}rn},
title = {Dimensional and Continuous Analysis of Emotions for Multimedia Applications: A Tutorial Overview},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396552},
doi = {10.1145/2393347.2396552},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1531–1532},
numpages = {2},
keywords = {tutorial, dimensional emotion prediction, sentiment analysis, affective computing, continuous analysis, music mood classification, audio/visual emotion recognition},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396553,
author = {Timmerer, Christian and Griwodz, Carsten},
title = {Dynamic Adaptive Streaming over HTTP: From Content Creation to Consumption},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396553},
doi = {10.1145/2393347.2396553},
abstract = {In this tutorial we present dynamic adaptive streaming over HTTP ranging from content creation to consumption. It particular, it provides an overview of the recently ratified MPEG-DASH standard, how to create content to be delivered using DASH, its consumption, and the evaluation thereof with respect to competing industry solutions. The tutorial can be roughly clustered into three parts. In part I we will provide an introduction to DASH, part II covers content creation, delivery, and consumption, and, finally, part III deals with the evaluation of existing (open source) MPEG-DASH implementations compared to state-of-art deployed industry solutions.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1533–1534},
numpages = {2},
keywords = {dash, dynamic adaptive http streaming, streaming, adaptation, MPEG},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396554,
author = {Shen, Jialie and Wang, Meng and Yan, Shuicheng and Cui, Peng},
title = {Multimedia Recommendation},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396554},
doi = {10.1145/2393347.2396554},
abstract = {Due to the rapid growth of online multimedia information, the problem of information overload has become more and more serious in recent decades. To address this problem, various multimedia recommendation technologies have been developed by different research communities (e.g., multimedia systems, information retrieval, and machine learning). Meanwhile, many commercial web systems (e.g., Flick, Youtube, and Last.fm) have successfully applied recommendation techniques to provide users personalized multimedia content and services in a convenient and flexible way. This tutorial focuses on exploring the state-of-the-art in multimedia recommendation. We also discuss the experience gained from developing existing systems and review key challenges associated with large-scale multimedia recommendation.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1535–1536},
numpages = {2},
keywords = {multimedia, recommendation},
location = {Nara, Japan},
series = {MM '12}
}

@inproceedings{10.1145/2393347.2396555,
author = {Jaimes, Alejandro},
title = {A Human-Centered Perspective on Multimedia Data Science: Tutorial Overview},
year = {2012},
isbn = {9781450310895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393347.2396555},
doi = {10.1145/2393347.2396555},
abstract = {This tutorial focuses on the analysis of user behavior in multimedia through large-scale data analysis. This includes discovering and leveraging search and navigation patterns, understanding how elements of interaction impact behavior, and how we can use controlled experiments in combination with user studies and other techniques to gain insights into human behavior with a particular emphasis on multimedia, particularly in the context of social media.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimedia},
pages = {1537–1538},
numpages = {2},
keywords = {multimedia, data science},
location = {Nara, Japan},
series = {MM '12}
}

