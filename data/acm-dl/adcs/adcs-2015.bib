@inproceedings{10.1145/2838931.2838942,
author = {Crane, Matt and Trotman, Andrew},
title = {Collision Resolution in Hash Tables for Vocabulary Accumulation During Parallel Indexing},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838942},
doi = {10.1145/2838931.2838942},
abstract = {During indexing the vocabulary of a collection needs to be built. The structure used for this needs to account for the skew distribution of terms. Parallel indexing allows for a large reduction in number of times the global vocabulary needs to be examined, however, this also raises a new set of challenges. In this paper we examine the structures used to resolve collisions in a hash table during parallel indexing, and find that the best structure is different from those suggested previously.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {1},
numpages = {4},
keywords = {Parallel, Indexing, Collision resolution},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838943,
author = {Crane, Matt and Trotman, Andrew and Eyers, David},
title = {Improving Throughput of a Pipeline Model Indexer},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838943},
doi = {10.1145/2838931.2838943},
abstract = {There are many competing models for the indexing process of an information retrieval system, one of which is a pipeline based model. Information retrieval is also an inherently parallel process, indexing one document is independent of another document. A pipeline model allows for easy experimentation on the parallelism within an indexer. In this paper we investigate areas within a pipeline where indexing throughput can be increased, as well as exploiting the inherent parallelism of indexing.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {2},
numpages = {4},
keywords = {Parallelism, Indexing, Buffering},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838934,
author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},
title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838934},
doi = {10.1145/2838931.2838934},
abstract = {This paper presents a benchmark dataset, CQADupStack, for use in community question-answering (cQA) research. It contains threads from twelve StackExchange subforums, annotated with duplicate question information. We provide pre-defined training and test splits, both for retrieval and classification experiments, to ensure maximum comparability between different studies using the set. Furthermore, it comes with a script to manipulate the data in various ways. We give an analysis of the data in the set, and report benchmark results on a duplicate question retrieval task using well established retrieval models.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {3},
numpages = {8},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838933,
author = {Mackenzie, Joel and Choudhury, Farhana M. and Culpepper, J. Shane},
title = {Efficient Location-Aware Web Search},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838933},
doi = {10.1145/2838931.2838933},
abstract = {Mobile search is quickly becoming the most common mode of search on the internet. This shift is driving changes in user behaviour, and search engine behaviour. Just over half of all search queries from mobile devices have local intent, making location-aware search an increasingly important problem. In this work, we compare the efficiency and effectiveness of two general types of geographical search queries, range queries and k nearest neighbor queries, for common web search tasks. We test state-of-the-art spatial-textual indexing and search algorithms for both query types on two large datasets. Finally, we present a rank-safe dynamic pruning algorithm that is simple to implement and use with current inverted indexing techniques. Our algorithm is more efficient than the tightly coupled best-in-breed hybrid indexing algorithms that are commonly used for top-k spatial textual queries, and more likely to find relevant documents than techniques derived from range queries.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {4},
numpages = {8},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838938,
author = {Moffat, Alistair and Bailey, Peter and Scholer, Falk and Thomas, Paul},
title = {INST: An Adaptive Metric for Information Retrieval Evaluation},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838938},
doi = {10.1145/2838931.2838938},
abstract = {A large number of metrics have been proposed to measure the effectiveness of information retrieval systems. Here we provide a detailed explanation of one recent proposal, INST, articulate the various properties that it embodies, and describe a number of pragmatic issues that need to be taken in to account when writing an implementation. The result is a specification for a program inst_eval for use in TREC-style IR experimentation.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {5},
numpages = {4},
keywords = {relevance measures, test collections, User behavior},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838937,
author = {Park, Laurence A. F. and Stone, Glenn},
title = {The Effect of Assessor Coverage and Assessor Accuracy on Rank Aggregation Precision},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838937},
doi = {10.1145/2838931.2838937},
abstract = {Rank aggregation is the process of aggregating multiple rankings provided by multiple assessors, of a given set of items, into a single ranking. Each assessor, whether it be human or computer based, is a resource that we use to obtain the multiple rankings. The accuracy of the aggregated ranking depends on the accuracy of the assessor ranking and the assessor coverage of the items. Our question is, given limited assessment resources, should each assessor rank many items to obtain item coverage, spending little time on each item, or should each assessor rank only a few items, but spend more time on each item to obtain a high accuracy ranking? In this article, we take a first step towards answering this question, by developing a model, based on simulation, showing the effect of the number of items assigned to an assessor and the accuracy of the assessment on the precision of the aggregated ranking. We find that when using Binomial allocation of items to assessors, increasing the assessor accuracy provides a greater increase in aggregated rank accuracy.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {6},
numpages = {4},
keywords = {Rank aggregation, assessor accuracy, assessor coverage},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838935,
author = {Phung, Viet and De Vine, Lance},
title = {A Study on the Use of Word Embeddings and PageRank for Vietnamese Text Summarization},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838935},
doi = {10.1145/2838931.2838935},
abstract = {Automatic text summarization is the process of automatically reducing the length of documents without losing the primary ideas. Due to the flood of digital text-based information, there is a great demand for summarization systems. In this paper, we investigate a number of word-embedding based approaches for sentence representation which are combined with the PageRank algorithm to select sentences for summary construction. We compare these new methods with a range of other current approaches to summarization. While the same summarization approaches can generally be applied across different languages, we target Vietnamese because of the relative lack of previous work in this space and also because it provides a good example of a language which generally requires word segmentation. Our experiments find that a word-embedding and graph based approach is an effective strategy for Vietnamese summarization and that word segmentation is not necessary for achieving good summarization results.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {7},
numpages = {8},
keywords = {word embeddings, Vietnamese, graph-based model, single-document summarization, PageRank},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838941,
author = {Thomas, Paul and Omari, Rollin and Rowlands, Tom},
title = {Towards Searching Amongst Tables},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838941},
doi = {10.1145/2838931.2838941},
abstract = {An increasing number of data sets are being published online, in institutional or government repositories as well as by individual researchers, journalists, and others. These data are often represented as tables of various kinds: however, repositories have poor search over and inside tables. It is difficult for a user to tell from a repository's portal whether a useful dataset is available, and this problem is only likely to get worse.We describe this problem, and demonstrate that the na\"{\i}ve approach of full-text search is not appropriate. We describe an alternative, based on inferring types of data and indexing columns as a unit, and demonstrate some improvements in early success especially when long captions are not available.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {8},
numpages = {4},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838932,
author = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
title = {How Well Sentence Embeddings Capture Meaning},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838932},
doi = {10.1145/2838931.2838932},
abstract = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications.In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {9},
numpages = {8},
keywords = {word embeddings, sentence embeddings, semantic consistency evaluation, Semantic vector space representations},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838939,
author = {Yasukawa, Michiko and Culpepper, J. Shane and Scholer, Falk},
title = {Data Fusion for Japanese Term and Character N-Gram Search},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838939},
doi = {10.1145/2838931.2838939},
abstract = {Term segmentation plays a vital role in building effective information retrieval systems. In particular, languages such as Japanese and Chinese require a morphological analyzer or a word segmenter to identify potential terms. The alternative approach to indexing a segmented collection is n-gram search, where every n-length sequence of symbols is indexed. Both approaches have strengths and weaknesses when applied to non-English collections. In this study, we explore data fusion techniques to answer the following question: if there are multiple ranked lists of documents from both word and n-gram indexes, can we improve overall effectiveness by combining them? We consider three empirical methods for combining search results using eight different search indexes and twenty-one different search models with and without automatic query expansion. Our approach is language independent; however, we focus on Japanese test collections -- NTCIR IR4QA -- as our testbed for the current experiments. Our experimental results demonstrate that the combination of the two different segmentation approaches has the potential to significantly outperform the best word-segmented search methods.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {10},
numpages = {4},
keywords = {morphological analysis, term segmentation, n-gram search},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838940,
author = {Zhou, Liyuan and Hawking, David and Thomas, Paul},
title = {Text Segmentation and Chinese Site Search},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838940},
doi = {10.1145/2838931.2838940},
abstract = {Automatic segmentation and overlapping bigrams are the most common methods for overcoming the lack of explicit word boundaries in Chinese text. Past studies have compared their effectiveness, but findings have been equivocal and site search has been little studied. We compare representatives of the two approaches using a 465,000 page crawl and test queries applicable to the university context. 503 pairs of result sets were judged by 56 Chinese students.Although there are differences on certain queries, we find no overall advantage to either method. To understand the merits of each approach, we analyze cases where they performed differently. Our analysis enumerates situations which favour segmentation, and those which favour bigrams. We observe that further improvements in segmentation accuracy will not improve retrieval effectiveness.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {11},
numpages = {4},
keywords = {site search, segmentation, Chinese IR},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

@inproceedings{10.1145/2838931.2838936,
author = {Zuccon, Guido and Koopman, Bevan and Bruza, Peter and Azzopardi, Leif},
title = {Integrating and Evaluating Neural Word Embeddings in Information Retrieval},
year = {2015},
isbn = {9781450340403},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2838931.2838936},
doi = {10.1145/2838931.2838936},
abstract = {Recent advances in neural language models have contributed new methods for learning distributed vector representations of words (also called word embeddings). Two such methods are the continuous bag-of-words model and the skipgram model. These methods have been shown to produce embeddings that capture higher order relationships between words that are highly effective in natural language processing tasks involving the use of word similarity and word analogy. Despite these promising results, there has been little analysis of the use of these word embeddings for retrieval.Motivated by these observations, in this paper, we set out to determine how these word embeddings can be used within a retrieval model and what the benefit might be. To this aim, we use neural word embeddings within the well known translation language model for information retrieval. This language model captures implicit semantic relations between the words in queries and those in relevant documents, thus producing more accurate estimations of document relevance.The word embeddings used to estimate neural language models produce translations that differ from previous translation language model approaches; differences that deliver improvements in retrieval effectiveness. The models are robust to choices made in building word embeddings and, even more so, our results show that embeddings do not even need to be produced from the same corpus being used for retrieval.},
booktitle = {Proceedings of the 20th Australasian Document Computing Symposium},
articleno = {12},
numpages = {8},
location = {Parramatta, NSW, Australia},
series = {ADCS '15}
}

