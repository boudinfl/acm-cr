@inproceedings{10.1145/1517303.1517305,
author = {Lucas, Keny T.},
title = {Parallel Algorithm for Sorting on OTIS-Ring Multicomputer},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517305},
doi = {10.1145/1517303.1517305},
abstract = {The Optical Transpose Interconnection System (OTIS) based interconnection network has already gained popularity among researchers for achieving high performance computation. It has become one of the efficient models for optoelectronic parallel computers. Sorting is one of the fundamental problems and is commonly used in many scientific and engineering applications. We are motivated to develop sorting algorithm due to its popularity. In this paper, we propose a parallel algorithm for sorting N (= n2) data elements on an OTIS-Ring optoelectronic parallel computer containing N (= n2) processors. To analyze the time complexity of our proposed algorithm, we count the data movements on electronic links (electronic move) and that on optical links (OTIS move) separately. Our proposed parallel algorithm for sorting requires 0.5n(n2+3n+1) electronic moves + n(2n+0.5) OTIS moves. Thus, the ratio of number of electronic moves to OTIS move of our proposed algorithm is O(n)},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {1},
numpages = {5},
keywords = {interconnection network, parallel algorithm, time complexity, sorting, OTIS},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517306,
author = {Marwah, Manish and Sharma, Ratnesh and Shih, Rocky and Patel, Chandrakant and Bhatia, Vaibhav and Mekanapurath, Mohandas and Velumani, Rajkumar and Velayudhan, Sankaragopal},
title = {Data Analysis, Visualization and Knowledge Discovery in Sustainable Data Centers},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517306},
doi = {10.1145/1517303.1517306},
abstract = {A significant amount of energy consumption is now attributed to data centers due to their ever increasing numbers, size and power densities. Thus, there are efforts focused at making a data center more sustainable by reducing its energy consumption and carbon footprint. This requires an end-to-end management approach with requirements derived from service level agreements (SLAs) and a flexible infrastructure that can be closely monitored and finely controlled. The infrastructure can then be manipulated to satisfy the requirements while optimizing for sustainability metrics and total cost of operations. In this paper, we explore the role of data analysis, visualization and knowledge discovery techniques in improving the sustainability of a data center. We present use cases from a large, sensor-rich, state-of-the-art data center on the application of these techniques to the three main sub-systems of a data center, namely, power, cooling and compute. Furthermore, we provide recommendations for where these techniques can be used within these sub-systems for improving sustainability metrics of a data center.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {2},
numpages = {8},
keywords = {compute, visualization, data analysis, knowledge discovery, cooling, data centers, sustainability, power},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517307,
author = {Nataraj, R V and Selvan, S},
title = {A Framework for Mining Top-k Frequent Closed Itemsets Using Order Preserving Generators},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517307},
doi = {10.1145/1517303.1517307},
abstract = {In this paper, we propose OP-TKC (Order Preserving Top K Closed itemsets) algorithm for mining top-k frequent closed itemsets. Our methodology visits the closed itemsets lattice in breadth first manner and generates all the top-k closed itemsets without generating all the closed itemsets of a given dataset i.e. in the search space, only closed itemsets that belongs to top-k are expanded and all other closed itemsets are pruned off. Our algorithm computes all the top-k closed itemsets with O(D+ k) space complexity, where D is the dataset. Experiments involving publicly available datasets show that our algorithm takes less memory and running time than TFP algorithm.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {3},
numpages = {6},
keywords = {algorithms, data mining, mining methods, closed itemsets},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517308,
author = {Kumar, Manish and Moriah, Shane and Krishnamoorthy, Srikumar},
title = {Performance Evaluation of Similarity Join for Real Time Information Integration},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517308},
doi = {10.1145/1517303.1517308},
abstract = {Approximate join processing serves a key role in many application areas such as data cleansing, data integration, text mining, and bio-informatics. There has been much research interest in approximate join processing based on the concept of an edit distance metric. Approximate join processing algorithms generally use a variety of qgram based filtering techniques to improve the scalability of the system. The primary approach taken in the literature involves the exploitation of methods inside a particular database language. However, this is impractical in the case of heterogeneous data spread across multiple databases. A popular alternative approach involves the direct comparison of all permutations of two string pairings. However, such algorithms don't scale well for very large databases, even after applying qgram filters. Here we design a novel, stand-alone filtering technique, essentially a modification of the HashJoin algorithm, to improve the scalability of similarity join processing algorithms. We implement the algorithm and conduct a number of experiments to study the performance of the system. The presented algorithm is also integrated with a real-life data federation solution called Infosys Gradient. The paper presents the performance results on a real-life test bed.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {4},
numpages = {7},
keywords = {approximate join processing, edit distance, similarity join},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517309,
author = {Giri, Nupur and Mundle, Siddharth and Ray, Arpita and Bodhe, Shrikant},
title = {Multi Agent System Based Service Architectures for Service Level Agreement in Cellular Networks},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517309},
doi = {10.1145/1517303.1517309},
abstract = {To provide Service Level Agreement (SLA) in wireless cellular network the network provider needs to do resource management through call admission and channel allocation strategies. Also if the SLA is unsatisfactory it has to react rapidly to reinstate the promised quality of service (QoS), provide alternatives or do congestion control.Our Paper proposes two Multi Agent System based, Centralized and Distributed, Service Architectures for SLA management. We extend the SHUFFLE model by introducing Network Provider Cell Agent (NPCA) which works in collaboration with Network Resource Provider Agent. Dynamic Call Admission is implemented using NPCA - NPCA interaction to accommodate real time traffic. The Agent are implemented using open source Java Agent DEevelopment Framework (JADE). The results of Multi agent and Non agent based connection level parameters are compared to understand the overhead characteristics of the systems.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {5},
numpages = {6},
keywords = {JADE, call admission control, service level agreement, multi agent system},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517310,
author = {Prashanth, C. R. and Ganavi, S. P. and Mahalakshmi, T. D. and Raja, K. B. and Venugopal, K. R. and Patnaik, L. M.},
title = {Iris Feature Extraction Using Directional Filter Bank for Personal Identification},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517310},
doi = {10.1145/1517303.1517310},
abstract = {Iris Recognition is a biometric tool, which has great emphasis in both research and practical applications. In this paper an Iris Recognition System using Directional Filter Bank (IRSDFB) is proposed. The normalized Iris is fragmented into three regions. The most distinctive features of the region nearer to the pupil are encoded to form the feature vector using Directional Filter Bank, instead of considering the entire Iris including the occluded portion. Therefore the Iris images captured with lesser cooperation can also be verified successfully. The decidability index of IRSDFB model is more compared to the existing algorithm using Gabor filter bank.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {6},
numpages = {8},
keywords = {biometrics, iris fragmentation, directional filter bank, iris recognition, decidability index},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517311,
author = {Ramachandra, A C and Pavithra, K and Yashasvini, K and Raja, K B and Venugopal, K R and Patnaik, L M},
title = {Offline Signature Authentication Using Cross-Validated Graph Matching},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517311},
doi = {10.1145/1517303.1517311},
abstract = {The biometric system is used to identify a person depending on his physiological or behavioral characteristics. Signature verification is a commonly accepted biometric method and is widely used for banking transactions. In this paper, we propose Offline Signature Authentication using Cross-validated Graph Matching (OSACGM) algorithm. The signatures are pre-processed in which signature extraction method is used to obtain high resolution for smaller normalization box. The similarity measure between two signatures in the database is determined by (i) constructing a bipartite graph G, (ii) obtaining complete matching in G and (iii) finding minimum Euclidean distance by Hungarian method. An optimum decision threshold value is determined using Cross-validation technique to select reference signatures. The test feature is extracted from the given test signature by pre-processing. Then the test feature is compared with the threshold value to authenticate the test signature. Compared to the existing algorithm, our algorithm gives better Equal Error Rate (EER) for skilled and random forgeries.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {7},
numpages = {8},
keywords = {equal error rate, cross-validation, biometrics, complete matching, offline signature verification, bipartite graph},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517312,
author = {Karjikar, Faisal and Roy, Suman and Padmanabhuni, Srinivas},
title = {Intelligent Business Knowledge Management Using Topic Maps},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517312},
doi = {10.1145/1517303.1517312},
abstract = {We present a method, complete with case study, for capturing and managing the information stored in business transactions based on the widely used UBL. Where conventional wisdom would use a RDBMS-based approach, we use Topic Maps as the knowledge representation format and show how by effectively leveraging its features we can efficiently navigate, discover and retrieve content in ways not possible with the conventional approach.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {8},
numpages = {8},
keywords = {querying, translator, business intelligence, case study, business documents, UBL, knowledge representation, topic maps, information retrieval},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517313,
author = {Jindal, M. K. and Sharma, R. K. and Lehal, G. S.},
title = {Segmentation of Touching Characters in Upper Zone in Printed Gurmukhi Script},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517313},
doi = {10.1145/1517303.1517313},
abstract = {A new technique for segmenting touching characters in upper zone of printed Gurmukhi script has been presented in this paper. The technique is based on the structural properties of the Gurmukhi script characters. Concavity and convexity of the characters has been studied and using top profile projections, the touching characters in upper zone have been segmented. Recognition rate of 91% has been achieved for segmenting the touching characters in upper zone.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {9},
numpages = {6},
keywords = {Gurmukhi script, projection profiles, touching characters, upper zone},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517314,
author = {Agarwal, Manisha and Jailia, Manisha},
title = {An Interactive Method for Generalized Association Rule Mining Using FP-Tree},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517314},
doi = {10.1145/1517303.1517314},
abstract = {Generalized association rule mining plays a very important role in Knowledge discovery in Databases (KDD). Generalized association rule mining is an extension of traditional association rule mining to discover more informative rules. In this paper, we describe a formal method for the problem of mining generalized association rules. In proposed method, The subset-superset and the parent-child relationships among generalized itemsets are introduced to present the different views of generalized itemsets, i.e. concept hierarchy.There are two phases in our proposed work; phases are "Level Defragmentation" and "Branch Defragmentation". Input of our algorithm is a conceptual hierarchy and a FP-tree. Using our proposed approaches, one can transform a lower level FP-tree to a Higher level FP-tree. Through higher level FP-tree we generate generalized association rule.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {10},
numpages = {6},
keywords = {association rule mining, FP-tree, data mining},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517316,
author = {Nagaraju, A and Ramachandram, S},
title = {Rough Set Based Ad-Hoc on Demand Distance Vector Routing Algorithm for MANETs},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517316},
doi = {10.1145/1517303.1517316},
abstract = {The existing reactive routing algorithms in Mobile Ad-hoc Networks (MANETs) generally send route request packet to all the neighbor nodes when it needs to find the destination. This greatly reduces the performance of network and consumes much of the bandwidth. The existing routing algorithms such as Ad-hoc On-demand Routing Algorithm (AODV) and Dynamic Source Routing (DSR) are suitable for networks with frequent changing topology. In this paper we propose a technique to reduce broadcasting by finding equivalence classes based on rough set concept among the neighbors of a source node. This protocol tries to establish a long term valid path from source node to destination node. This means the protocol is of more use full in several applications such as multimedia and video conferences.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {11},
numpages = {5},
keywords = {route request, equivalence class, AODV, rough set},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517317,
author = {Reddy D, Kiran Kumar and Maralla, Karthiek and G, Raj Kumar and Thirumaran, M},
title = {A Greedy Approach with Criteria Factors for QoS Based Web Service Discovery},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517317},
doi = {10.1145/1517303.1517317},
abstract = {Normally, the web service discovery mechanisms in web-sites have a trade-off between being Quality-driven or transaction cost-driven As goals of our paper, we decide upon several factors including the above-mentioned Quality and Transaction cost factors along with the parameters under each factor and use a priority-based greedy approach to involve minimum number of factors to achieve maximum efficiency in the web service discovery. We also present a model with QoS parameters which in a great way help in rating the web services with the end user's perspective on the basis of three levels of views, namely the business level view, Service level view and System level view. In this paper, we've broadly discussed around twenty eight parameters categorically falling under all the above-mentioned factors. To reduce the complexity of involving all the parameters we follow the mechanism of Greedy approach to evaluate the parameters and estimate the criteria rank for each and every parameter. The core principle of the Greedy approach, in general, is to involve minimum number of input data to achieve the maximum efficiency in the final optimized result.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {12},
numpages = {5},
keywords = {criteria rank, greedy approach, web service discovery, QWS parameters, QoS model},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517318,
author = {Sivadasan, Praveen and SojanLal, P and Sivadasan, Naveen},
title = {JDATATRANS for Array Obfuscation in Java Source Codes to Defeat Reverse Engineering from Decompiled Codes},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517318},
doi = {10.1145/1517303.1517318},
abstract = {Software obfuscation or obscuring a software is an approach to defeat the practice of reverse engineering a software for using its functionality illegally in the development of another software. Java applications are more amenable to reverse engineering and re-engineering attacks through methods such as decompilation because Java class files store the program in a semi complied form called 'byte' codes. The existing obfuscation systems obfuscate the Java class files. Obfuscated source code produce obfuscated byte codes and hence two level obfuscation (source code and byte code level) of the program makes it more resilient to reverse engineering attacks. But source code obfuscation is much more difficult due to richer set of programming constructs and the scope of the different variables used in the program and only very little progress has been made on this front. Hence programmers resort to adhoc manual ways of obscuring their program which makes it difficult for its maintenance and usability. To address this issue partially, we developed a user friendly tool JDATATRANS to obfuscate Java source code by obscuring the array usages. Using various array restructuring techniques such as 'array splitting', 'array folding' and 'array flattening', in addition to constant hiding, our system obfuscate the input Java source code and produce an obfuscated Java source code that is functionally equivalent to the input program. We also performed a number of experiments to measure the potency, resilience and cost incurred by our tool.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {13},
numpages = {4},
keywords = {restructured arrays, source code obfuscation, reverse engineering},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517319,
author = {Nagaraju, A and Ramachandram, S},
title = {Adaptive Partial Dominating Set Algorithm for Mobile Ad-Hoc Networks},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517319},
doi = {10.1145/1517303.1517319},
abstract = {In Mobile Ad-hoc Networks (MANETs) each node has a transmission radius and is able to send message to all of its neighbors that are located with in its radius. In reactive based routing algorithms such as Ad-hoc on demand Routing Algorithm (AODV) and Dynamic Source Routing Algorithm (DSR) a node broadcasts control information (Route Request, Route Reply, and Route Error) to all its neighbor nodes. In this paper we modify the existing Partial Dominant Pruning algorithm to find adaptive dominating set by reducing the coverage area of a node which desires to send control information. It does not require any communication over head to reduce the coverage area.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {14},
numpages = {4},
keywords = {AODV, route request, control information},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517320,
author = {Singh, Dayashankar and Dutta, Maitreyee and Singh, Sarvpal H.},
title = {Neural Network Based Handwritten Hindi Character Recognition System},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517320},
doi = {10.1145/1517303.1517320},
abstract = {Neural Networks are being used for character recognition from last many years but most of the works were reported to English character recognition. Character recognition is one of the applications of pattern recognition, which has enormous scientific and practical interest. Many scientific efforts have been dedicated to pattern recognition problems and much attention has been paid to develop recognition system that must be able to recognize a character. The main driving force behind neural network research is the desire to create a machine that works similar to the manner our own brain works. Neural networks have been used in a variety of different areas to solve a wide range of problems. A very little work has been reported for Handwritten Hindi Character recognition. In this paper, we have implemented Gradient feature extraction technique, which provides more than 94% recognition accuracy. We have acquired 1000 samples of handwritten Hindi characters by initializing the mouse in graphics mode. The 500 samples have been used for training the network (Train Data) and remaining 500 samples have been used for testing the network (Test Data). The system has been trained using several different forms of handwritings provided by both male and female participants of different age groups. Finally, this rigorous training results an automatic HCR system using MLP network. The error backpropagation algorithm has been used to train the MLP network. A comparative analysis was performed by implementing both global input and Gradient feature input. We have concluded that gradient feature extraction technique provides better recognition accuracy with reduced training time.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {15},
numpages = {4},
keywords = {pattern recognition, neural networks, hindi character recognition (HCR), multilayer perceptron (MLP), recognition accuracy and training time, feature extraction techniques},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517321,
author = {Karthi, R. and Arumugam, S. and RameshKumar, K.},
title = {A Novel Discrete Particle Swarm Clustering Algorithm for Data Clustering},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517321},
doi = {10.1145/1517303.1517321},
abstract = {In this paper, a novel Discrete Particle Swarm Clustering algorithm (DPSC) for data clustering has been proposed. The particle positions and velocities are defined in a discrete form and an efficient approach is developed to move the particles for constructing new clustering solutions. DPSC algorithm has been applied to solve the data clustering problems by considering two performance metrics, such as TRace Within criteria (TRW) and Variance Ratio Criteria (VRC). The result obtained by the proposed algorithm has been compared with the published results of Combinatorial Particle Swarm Optimization (CPSO) algorithm and Genetic Algorithm (GA). The performance analysis demonstrates the effectiveness of the proposed algorithm in solving the partitional data clustering problems.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {16},
numpages = {4},
keywords = {TRace Within criterion, particle swarm optimization, variance ratio criterion, data clustering},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517322,
author = {Ramachandran, Vivek Anandan and Krishnamurthi, Ilango},
title = {<i>NLION</i>: <i>N</i>Atural <i>L</i>Anguage <i>I</i>Nterface for Querying <i>ON</i>Tologies},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517322},
doi = {10.1145/1517303.1517322},
abstract = {Use of semantic web models for representing expert knowledge about a domain as an ontology is becoming increasingly common. However, since these ontologies contain detailed domain information in a formal specification, it cannot be easily understood by a casual user. Providing a natural language interface to ontologies will help such users to retrieve the required information and will further enable use of these ontologies in sophisticated applications. This paper presents NLION, a Natural Language Interface for querying ONtologies, which accepts a natural language query and outputs a SPARQL query. For achieving this, we use semantic relation tagging to recognize the meaning of the user query with respect to the target ontology. NLION effectiveness was analyzed and it was found to be 61.60%.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {17},
numpages = {4},
keywords = {semantic relation and SPARQL, ontology, natural language interface},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517323,
author = {D'Mello, Demian Antony and Ananthanarayana, V. S.},
title = {A Tree Structure for Web Service Compositions},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517323},
doi = {10.1145/1517303.1517323},
abstract = {The composition of Web services is gaining a considerable momentum as an approach to the effective integration of distributed, heterogeneous and autonomous applications to build more sophisticated and value added services. Algorithms are needed to select the best services for the individual tasks of the composition. QoS is an important criterion to select the best service for the tasks of composition. The objective of selection algorithm is to maximize the QoS of the composition based on the composite service provider's (CSP) QoS requirements. In this paper, we define the various business offers (service offers) of Web services. We classify the CSP's requirements defined on the QoS and service offers based on its structure. We propose a tree structure to represent the CSP's requirements defined on the multiple QoS properties and service offers involving AND and OR operators with varied preferences.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {18},
numpages = {4},
keywords = {service selection, QoS, service offer, composition},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517324,
author = {Hudli, Anand V. and Shivaradhya, Balasubrahmanya and Hudli, Raghu V.},
title = {Level-4 SaaS Applications for Healthcare Industry},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517324},
doi = {10.1145/1517303.1517324},
abstract = {Software applications and solutions as Software as a Service (SaaS) has been gaining momentum all over the world in recent times. In the SaaS context, applications are hosted by the vendor using the vendor's IT infrastructure. Most SaaS applications are web-based. What separates SaaS application from multi-user web-based applications are SaaS is not just a web-front to a back-end application as in a bank's application. SaaS is not just a multi-user application, but multi-tenant application. Each customer organization is a tenant in a SaaS application. Multiple customer organizations may use the same application and each organization may have multiple users.Four levels of SaaS application maturity have been defined in the literature. A Level 4 SaaS application supports multiple tenants with a multiple but identical instances of the application and a metadata configuration of the application and offers superior ability to scale and handle large volumes of traffic and transactions.In this paper we present a SaaS application, addressing all issues involved in a Level 4 SaaS application. We have chosen the healthcare industry as a business context, as it has multiple players. Multi-tenant is a natural phenomenon on such a context.We have recognized that while SaaS may offer a business solution to a customer, it still needs to integrate with other business applications and workflows. We provide integration case studies in a SaaS Context. We also discuss security issues in a SaaS context. We present a multi-key secure encryption model to secure user data and a fully user defined Role Based Access Control model.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {19},
numpages = {4},
keywords = {collaborative computing platforms, multi-tenancy SaaS, SaaS integration},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517325,
author = {Admuthe, L. S. and Apte, S. D.},
title = {Optimization of Spinning Process Using Hybrid Approach Involving ANN, GA and Linear Programming},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517325},
doi = {10.1145/1517303.1517325},
abstract = {The spinning process is one of the important production processes in the textile industry. Yarn is created using Cotton Fibre on a rotor or ring spinning machine. The quality of resulting yarn is very important in determining their application possibilities. An important aspect of production process is selection of raw material, quality of resulting yarn and cost. The yarn should have Optimal Product Characteristics with minimum cost.The proposed paper aims to study profitability and predictability by developing computational model. ANN is used for prediction of yarn properties from fibre properties. These fibre properties are optimized using Genetic Algorithm(GA). GA also considers available stock into account. Linear Programming is used to decide proportionality of fibres in cotton bled and cost optimization. The result shows that the accuracy of proposed integrated approach is higher than Industrial Standards},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {20},
numpages = {4},
keywords = {genetic algorithm, optimization, back-propogation algorithm},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517326,
author = {Chudasma, Nrupen and Chaudhary, Sanjay},
title = {Service Composition Using Service Selection with WS-Agreement},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517326},
doi = {10.1145/1517303.1517326},
abstract = {The business requirements are dynamic in nature. Identification of suitable business partners who can satisfy a set of guarantees "on demand" is a critical process. Selection of business partners is influenced by several parameters including maximizing profit, minimizing cost, reliability, credit history, etc. A business process can be decomposed into several tasks, possibly expressed as Web services implementing desired business functionalities. Each service offers a set of guarantees, each defining a Service Level Objective(SLO). Many service providers may provide the similar business functionality in the form of service with different SLOs. Thus, it is necessary to select services offered by service partner(s) such that overall performance of the process is improved. By specifying requirements of a service in the form of multiple service level objectives (SLOs), selection of a service provider can be achieved based on the best or optimal matching of SLOs of service consumer and providers. A set of services, SLOs, and participating actors constitute a Service Level Agreement (SLA). In this work, WS-Agreement specification is used to specify the SLA. The service partner must comply with the multiple criteria and preferences of a requester. Multi Criteria Decision Making (MCDM) method is also required in this process.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {21},
numpages = {5},
keywords = {web services, ontology, MCDM, service level objective (SLO), WS-agreement, service-oriented computing (SOC), service composition, service level agreement (SLA)},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517327,
author = {Goyal, Navneet and Sharma, Yashvardhan},
title = {New Binning Strategy for Bitmap Indices on High Cardinality Attributes},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517327},
doi = {10.1145/1517303.1517327},
abstract = {Bitmap indices are the preferred indexing structures for read only &amp; high dimensional data in data warehouses and scientific databases. High cardinality attributes pose a new challenge in terms of having space efficient bitmap indices. Binning is a common technique for reducing space requirements of bitmap indices. It is found that binning has an adverse affect on the query performance. A new efficient binning strategy is proposed for bitmap indices for high cardinality attributes. Exact bins are created based on query distribution. Exact bins are allowed to overlap. This gives a considerable performance advantage over the conventional non-overlapping bins at the expense of marginal increase in space overheads. Overlapping bins minimize the number of candidate-checks that need to be performed for a given set of queries. Algorithms are also presented for performing candidate checks more efficiently. Experimental results are presented in support of the new binning strategy.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {22},
numpages = {5},
keywords = {bitmap index, candidate checks, equi-width binning, binning},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517328,
author = {Bala, Pradip Kumar},
title = {A Technique for Mining Negative Association Rules},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517328},
doi = {10.1145/1517303.1517328},
abstract = {In this paper, a methodology for mining and exploring all forms of negative association rules has been suggested. As the existing softwares do not provide any module to mine negative association rules directly and efficiently, the present research may be useful for developing softwares for mining negative association rules.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {23},
numpages = {4},
keywords = {data mining software, data mining, lift, negative association rule, association rule},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517329,
author = {Das, Tirthankar and Roush, Ellard T. and Nandana, Pramod},
title = {Quantum Leap Cluster Upgrade},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517329},
doi = {10.1145/1517303.1517329},
abstract = {This paper describes upgrading a running high availability cluster while applications are still running. The whole software stack can be upgraded including operating system, cluster software, volume manager software, application software etc with almost zero downtime. This is done by splitting the cluster in two partitions and upgrading one at a time. This method is different from other split mode upgrade technology by providing the ability to complete all software initialization on the partition running the new software while the older partition hosts service. We call this method Quantum Leap because the cluster leaps from one version of the software to the other.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {24},
numpages = {4},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517330,
author = {Sureka, Ashish and Mirajkar, Pranav Prabhakar and Varma, Kishore Indukuri},
title = {A Rapid Application Development Framework for Rule-Based Named-Entity Extraction},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517330},
doi = {10.1145/1517303.1517330},
abstract = {Named Entity Recognition and Classification (NERC) consist of identifying and labeling specific pieces of information like proper names from free-form textual data. There are primarily three approaches to named-entity extraction: hand-crafted rule based, machine-learning based and hybrid. Rule-based approaches consist of defining heuristics in the form of regular expressions or linguistic pattern and making use of dictionaries and lexicons for extracting named-entities. Rule-based approaches have proven to be quite successful but one of their limitations is that it requires a domain expert to manually define and encode the rules. The process of hand-engineering rules is a time consuming and tedious process. It also requires a domain expert, cannot be easily ported to other domains and languages and becomes hard to maintain. Machine learning based approaches tries to overcome these limitations by automatically learning rules or inducing a model rather than defining the rules by a human expert. In this work, we present our research on overcoming the limitations of rule-based approach by building a rapid application development framework that can expedite the process of rule-building and making it easy to maintain and apply it to other domains. We describe a framework that can enable a business user to easily define and maintain rules and lexicons.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {25},
numpages = {4},
keywords = {natural language processing, named-entity extraction, software architecture, rule-based systems, rapid application development framework},
location = {Bangalore, India},
series = {COMPUTE '09}
}

@inproceedings{10.1145/1517303.1517331,
author = {Manjula Devi, T. H. and Shenoy, Pooja P. and Saigali, Swathi and Mathew, Harsha and Raja, K. B. and Venugopal, K. R. and Patnaik, L. M.},
title = {Extracting Hidden Image Using Histogram, DFT and SVM},
year = {2009},
isbn = {9781605584768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1517303.1517331},
doi = {10.1145/1517303.1517331},
abstract = {In covert communication, Information hiding is rapidly gaining momentum. There are many sophisticated techniques being developed in steganography. There is a need of universal method to detect hidden image. We have proposed a Universal method to detect hidden message using Histogram, Discrete Fourier Transform and SVM (UDHDS). When compared to cover image stego image has irregular statistical characteristics. one class SVM is trained by these statistical features which are generated Using Histogram and DFT to discriminate the cover and stego image. The number of statistical features is less in UDHDS Algorithm when compared to the existing algorithm and found to be more efficient.},
booktitle = {Proceedings of the 2nd Bangalore Annual Compute Conference},
articleno = {26},
numpages = {4},
keywords = {histogram, SVM, steganalysis, DFT},
location = {Bangalore, India},
series = {COMPUTE '09}
}

