@inproceedings{10.1145/2998476.2998487,
author = {Bafna, Prafulla and Shirwaikar, Shailaja and Pramod, Dhanya},
title = {Semantic Clustering Driven Approaches to Recommender Systems},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998487},
doi = {10.1145/2998476.2998487},
abstract = {Recommender Systems (RS) have increasingly evolved from novelties used by few E-commerce sites to an essential component of business tools handling the world of E-commerce. Recommender Systems have been widely used for product recommendations such as books and movies as well as, it is also gaining ground in service recommendations such as hotels, restaurants and travel attractions. Collaborative filtering based on reviews and ratings is usually applied that uses Clustering technique. The primary step of converting textual reviews into a Feature Matrix (FM) can be greatly refined by using semantic similarity between terms. In this paper Wordnet based Synset grouping approach is presented that not only reduces dimensions in FM but also generates Feature vectors (FV) for each cluster with significantly improved cluster quality. The paper presents a three step approach of validating the reviews, grouping of reviews and review based recommendations using Feature vector. Real datasets extracted from travel sites are used for experiments.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {1–9},
numpages = {9},
keywords = {Recommender system, synsets, Feature vector},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998480,
author = {Rani, Asma and Goyal, Navneet and Gadia, Shashi K.},
title = {Efficient Multi-Depth Querying on Provenance of Relational Queries Using Graph Database},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998480},
doi = {10.1145/2998476.2998480},
abstract = {Data Provenance is the history associated with that data. It constitutes the origin, creation, processing, and archiving of data. In today's Internet era, it has gained significant importance for database analytics. Most of the provenance models store provenance information in relational databases for further querying and analysis. Although, querying of provenance in Relational Databases is very efficient for small data sets, it becomes inefficient as the provenance data grows and traversal depth of provenance query increases. This is mainly due to increase in number of join operations to search the entire provenance data. Graph Databases provide an alternative to RDBMSs for storing and analyzing provenance data as it can scale to billions of nodes and at the same time traverse thousands of relationships efficiently. In this paper, we propose efficient multi-depth querying of provenance data using graph databases. The proposed solution allows efficient querying of provenance of current as well as historical queries. A comparison between relational and graph databases is presented for varying provenance data size and traversal depths. Graph databases are found to scale well with increasing depth of provenance queries, whereas in relational databases the querying time increases exponentially.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {11–20},
numpages = {10},
keywords = {TPC-H, Relational Database, improved DPHQ, ZILD, Provenance Querying, Data Provenance, DPHQ, Query Inversion, Neo4j, Graph Database},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998477,
author = {Takale, Sheetal A. and Kulkarni, Prakash J.},
title = {Concept Preserving Visual Summarization of Social Image Search Results},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998477},
doi = {10.1145/2998476.2998477},
abstract = {Existing tag based social media search engines present search results as a ranked list of images. But, they fail to identify visual, textual and geographical concepts present in query results. In this paper, we present an approach for automatic generation of visual, textual and geographical concept preserving summary of social image search results. For user specified query, search results are collected from popular content-sharing websites such as Flickr. Aim of the algorithm is, to generate representative but diverse summary having a set of images, information about locations-of-interest (LOI) associated with the query, and a set of tags, describing the context of images. The proposed scheme exploits multiple modalities in order to understand context and content of geotagged social images. We formulate the problem as a graph clustering problem, where nodes are images and edge weight is computed as geo-graphical distance, tag-based similarity between images and visual similarity between images. In order to reduce the computational overhead, we implement late fusion of three different edge weight parameters. An innovative Graph based clustering algorithm using Haversine distance formula is proposed for geo-clustering of images. Performance evaluation is based on intrinsic and extrinsic methods. We also present an evaluation protocol having no human intervention for evaluating coverage of geographical spread of images in the final result and cluster coherence. Through empirical study, we demonstrate the effectiveness of our algorithm against state-of-the-art image search result summarization methods.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {21–29},
numpages = {9},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998483,
author = {Kattepur, Ajay and Nambiar, Manoj},
title = {Service Demand Modeling and Prediction with Single-User Performance Tests},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998483},
doi = {10.1145/2998476.2998483},
abstract = {Performance load tests of online transaction processing (OLTP) applications are expensive in terms of manpower, time and costs. Alternative performance modeling and prediction tools are required to generate accurate outputs with minimal input sample points. Service Demands (time needed to serve 1 request at queuing stations) are typically needed as inputs by most performance models. However, as service demands vary as a function of workload, models that input singular service demands produce erroneous predictions. The alternative, which is to collect service demands at varying workloads, require time and resource intensive load tests to estimate multiple sample points -- this defeats the purpose of performance modeling for industrial use. In this paper, we propose a service demand model as a function of concurrency that can be estimated with a single-user performance test. Further, we analyze multiple CPU performance metrics (cache hits/misses, branch prediction, context switches and so on) using Principal Component Analysis (PCA) to extract a regression function of service demand with increasing workloads. We use the service demand models as input to performance prediction algorithms such as Mean Value Analysis (MVA), to accurately predict throughput at varying workloads. This service demand prediction model uses CPU hardware counters, which is used in conjunction with a modified version of MVA with single-user service demand inputs. The predicted throughput values are within 9% deviation with measurements procured for a variety of application/hardware configurations. Such a service demand model is a step towards reducing reliance on conventional load testing for performance assurance.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {31–41},
numpages = {11},
keywords = {CPU Performance Counters, Service Demand Modeling, Principal Component Analysis, Performance Prediction, Mean Value Analysis},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998478,
author = {Desai, Ankit and Chaudhary, Sanjay},
title = {Distributed Decision Tree},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998478},
doi = {10.1145/2998476.2998478},
abstract = {Decision Tree is a tree-structured plan of a set of attributes to test in order to predict the output. MapReduce and Spark is a programming model used for processing data on a distributed file system. In this paper, MapReduce and Spark implementation of Decision Tree is named as Distributed Decision Tree (DDT) and Spark Tree (ST) respectively. Decision Tree (DT), Ensemble of Trees (BT), DDT and ST are compared over accuracy, size of tree and number of leaves of tree(s) generated. DDT and ST is empirically evaluated over 10 selected datasets. Using DDT, size of tree is reduced by 71% and 82% as compared to DT and BT respectively. In case of ST size of tree is reduced by 48% and 67% as compared to DT and BT. Number of leaves is reduced by 70% and 81% with respect to DT and BT, respectively using DDT. Whereas, it is reduced by 45% and 65% with respect to DT and BT in case of ST. We evaluated DDT and ST using Yahoo! Webscope dataset. Our evaluation shows improvement in accuracy as well as reduction in size of tree and number of leaves. Hence, DDT and ST outperformed DT and BT with respect to size of tree and number of leaves with adequate classification accuracy.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {43–50},
numpages = {8},
keywords = {Spark, Classification, Decision Tree, Distributed Decistion Tree, Hadoop Map-Reduce},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998479,
author = {Padiya, Trupti and Kanwar, Jai Jai and Bhise, Minal},
title = {Workload Aware Hybrid Partitioning},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998479},
doi = {10.1145/2998476.2998479},
abstract = {Real life databases exhibit highly skewed access patterns. These skewed access patterns can be exploited to partition the data considering the query workload. The presented work proposes Workload Aware Hybrid Partitioning (WAHP). WAHP identifies clusters of attributes which are queried together. It identifies workload aware clusters for the actual query workload using a hybrid combination of horizontal and vertical partitioning. The paper demonstrates WAHP experiment using TPC-C benchmark, where 9% of the actual TPC-C data in workload aware clusters, is able to answer 73% of hottest query-workload with an average execution time gain of 37% against original database.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {51–58},
numpages = {8},
keywords = {Query Workload, Workload Aware Hybrid Partitioning, Disk Resident Database, Data Partitioning, Query Execution},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998482,
author = {Jain, Prerit and Bendapudi, Haripriya and Rao, Shrisha},
title = {EEQuest: An Event Extraction and Query System},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998482},
doi = {10.1145/2998476.2998482},
abstract = {We present EEQuest, an application that extracts events from text using natural language processing (NLP) and supervised machine-learning techniques, and provides a system to query events extracted from a text corpus.We provide a use case for the application wherein we extract business-related events from news articles. The extracted events are then categorized based on the business organization/company that they are related to. Finally, the events are added to a knowledge base using which a query system is built. The system can be used to display events related to a particular organization or a group of organizations. Although we are using the system to extract business-related events, the event extraction mechanism can be used in a more general sense with any available textual data, to extract any kind of events that have a structure that can answer the question: Who did what, when and where?},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {59–66},
numpages = {8},
keywords = {supervised learning, artificial intelligence, information extraction, event extraction, natural language processing},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998485,
author = {Bansal, Rishab and Ravindar, Archana},
title = {Role of Reduced Inputs in Flag Mining},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998485},
doi = {10.1145/2998476.2998485},
abstract = {Typically compilers provide a wide choice of optimization flags that can be used to improve the application performance. The process of searching for the best flag combination for a given application is referred to Flag Mining. Brute force ways of flag mining are time consuming as it requires a number of runs with different combinations of flags. Flag mining techniques that are based on machine learning rely on a database consisting of measurements of application run-times obtained with a large number of combinations of binaries compiled with different flags. This work quantifies the impact of using reduced inputs in flag mining. Reduced inputs are much smaller inputs than real representative inputs and cause the application to run for less than 10 percent of original execution time. Some examples of reduced inputs are the train input used in SPEC benchmarks, MinneSPEC inputs. Using reduced inputs instead of full inputs would reduce time/space overhead of flag mining significantly when used in brute force or machine learning based methods. However inorder to use reduced inputs for flag mining, the behavior of the application compiled with a set of flags, when presented with reduced inputs should give similar benefits on full representative inputs. This can happen only if reduced inputs are an accurate representatives of ref inputs in the context of application performance. Our experiments show that reduced inputs correlate to full representative inputs for 5 out of 7 SPEC CPU2006 benchmarks on all 11 flag combinations considered with the GCC compiler and are found to reduce the experimentation time of flag mining by up to 82%. We also outline the necessary conditions that need to be satisfied by reduced inputs to qualify for use in flag mining.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {67–76},
numpages = {10},
keywords = {Compiler optimizations, SPEC, Performance, reduced inputs, benchmarks, representative inputs, Flag mining},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998481,
author = {Pandey, Saurabh K. and Zaveri, Mukesh A.},
title = {Hierarchical Tree-Based Optimized Communication for Real Time Event Driven Internet of Things},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998481},
doi = {10.1145/2998476.2998481},
abstract = {Internet of Things envisions to support real time application processing and operations in a heterogeneous environment. It acts as an umbrella framework where smart objects perform communication, computation and coordination activities. To detect, gather and report such activities occurring in a distributed environment, an event driven system is needed. These activities are affected by energy consumption and average delay in the network. In this context, an optimized communication hierarchy is needed for effective reporting of events. Moreover, among many approaches proposed to optimize the energy consumption, reducing the number of transmissions and effective routing strategy are considered as an accomplishment. In this view, this paper proposes a tree-based optimized communication method for event driven Internet of Things. It focuses on balancing the workload of the devices in the IoT environment. The proposed algorithm is evaluated for efficacy on IoT based platform.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {77–83},
numpages = {7},
keywords = {Response Time, Internet of Things, Event Detection},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998488,
author = {Agrawal, Amit and Jain, Vaibhav and Sheikh, Mohsin},
title = {Quantitative Estimation of Cost Drivers for Intermediate COCOMO towards Traditional and Cloud Based Software Development},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998488},
doi = {10.1145/2998476.2998488},
abstract = {Software project estimation is the process of analyzing the resource requirements for the given time duration of product development. Cost estimation models are used for calculating the associated amount required for developing the stakeholder's requirement within the defined time boundaries. Among several models available for the cost estimation of software projects, COCOMO is one of the well-known models which serve the field most. Resources applied for the given time will generate the rough estimates, but for more accurate values, various factors are analyzed. These factors are termed as cost drivers. Software estimation using COCOMO is performed by selecting values of cost drivers on a predefined scale. This approach solely depends on experience of a software analyst. However, there is a lack of a systematic approach available for the selection of values of these cost drivers. Our work suggests the quantification of cost drivers for intermediate COCOMO. Quantification will implicitly fetch the values from the system and its environment which reduces the manual selection of ranges of scaling factors. Hence the systems cost will be generated directly without analyst and selector logic. Finally, if the selection of correct scaling is performed, then the calculation of cost will definitely get improved. An experimental analysis is performed between the above suggested model and the Intermediate COCOMO. The results show that the "COCOMOUP" is performing well under the known conditions and in uncertain requirements conditions, the system is getting better predictions.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {85–95},
numpages = {11},
keywords = {Quantification, Cost Estimation, Intermediate COCOMO, Cloud Based Software Cost Estimation, Cost Drivers, KLOC (Kilo Line of Code)},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998486,
author = {Thakare, Sanjay B. and Kiwelekar, Arvind W.},
title = {SkipLPA: An Efficient Label Propagation Algorithm for Community Detection in Sparse Network},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998486},
doi = {10.1145/2998476.2998486},
abstract = {The propagation phase of label propagation algorithm is a computationally intensive process and overall performance of algorithm depends on it. This phase determines the label of all the nodes by processing nodes recursively in the network. Rather processing all the nodes if it is possible to skip certain nodes from the propagation phase, then the process will speed-up.We propose an efficient algorithm SkipLPA based on label propagation algorithm for the discovering community structure in the sparse network. The initialization phase is split into two sub-phases. First sub-phase: only certain nodes are initialized with unique labels. Second sub-phase: remaining nodes will get initial labels from connected nodes and excluded from the propagation phase. The algorithm is tested not only on benchmark networks but also on the real world networks, and efficiently recovers community structure. The performance of this algorithm improves drastically without compromising the quality of community detected, as well as the number of iterations are reduced by skipping certain nodes from the propagation phase.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {97–106},
numpages = {10},
keywords = {Social Network Analysis, Label Propagation, Community Detection},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998484,
author = {Ranjan, Aditya Kaushal and Kumar, Binay},
title = {Three Steps Secure Login: A Systematic Approach},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998484},
doi = {10.1145/2998476.2998484},
abstract = {Generally, user authentication performs through user's Id and password. In this process, user id remains visible and password remains secret. But through shoulder surfing and other attacks, the password can also be traced due to exact password characters are typed or marked by users during login. To counter this vulnerability of tracing password, we propose a novel login method that does not reveal the user-id/password even if keylogging traces the typed keyboard's characters. We also do a security analysis to show that proposed mechanism is able to withstand a number of attacks and also mitigates some of the attacks. We also do a usability survey to show its feasibility among real-time users without compromising any security features.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {107–114},
numpages = {8},
keywords = {keylogging, shoulder surfing, SQL injection, authentication, web-login, usability},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998489,
author = {Behera, Ranjan Kumar and Naik, Debadatta and Sahoo, Bibhudatta and Rath, Santanu Ku.},
title = {Centrality Approach for Community Detection in Large Scale Network},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998489},
doi = {10.1145/2998476.2998489},
abstract = {Identifying communities in social network plays an important role in predicting behavior of the complex network. Real world systems in social network can be modeled as a graph structure, where nodes represents the social entities and edges represents the relationships among the entities. Usually nodes inside a community are having similar kinds of properties and most of them are influence by one or more central nodes in the network. Hence centrality principle can be adapted for efficiently discovery of communities. In this paper, an attempt has been made for community detection using central nodes of the network. Discovering central nodes in large scale network is a challenging task due to its huge complex structure. Central nodes have been been identified using map reduce paradigm in order to carry out the computation in distributed manner. The process of discovering communities is then carried out using the identified central nodes. Experimental evaluation shows that the proposed method for community detection provides better performance in term of both accuracy and time complexity.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {115–124},
numpages = {10},
keywords = {Map-Reduce, Centrality Analysis, Modularity, Community Detection},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998497,
author = {Chandrakar, Omprakash and Saini, Jatinderkumar R.},
title = {Development of Indian Weighted Diabetic Risk Score (IWDRS) Using Machine Learning Techniques for Type-2 Diabetes},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998497},
doi = {10.1145/2998476.2998497},
abstract = {Undetected pre-diabetes and late diagnosis is a major problem in East Asian countries. Diabetes screening tools such as Diabetes Risk Score (DRS) can effectively help in detecting and preventing the disease among pre-diabetes persons. Several Risk Scores for Type -2 Diabetes have been proposed and being used. In current research, researchers have observed certain issues in the available DRS and advocate the need to address the same. In this study researchers propose a novel Indian Weighted Diabetic Risk Score (IWDRS). Machine Learning Techniques such as distance based clustering with Euclidean distance, k-means algorithm and discretization is used to derive weighted risk score for diabetes risk factors like age, BMI, waist circumference, personal history, family history, diet, physical activity, stress and life quality. Result analysis shows that the proposed approach is better than existing approach in scientific literature.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {125–128},
numpages = {4},
keywords = {Data Mining, Clustering, Type -2 Diabetes, Discretization, Weighted Diabetic Risk Score},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998490,
author = {Sahu, Tirath Prasad and Nagwani, Naresh Kumar and Verma, Shrish},
title = {Topical Authoritative Answerer Identification on Q&amp;A Posts Using Supervised Learning in CQA Sites},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998490},
doi = {10.1145/2998476.2998490},
abstract = {Community Question Answering (CQA) site is an online platform for hosting information in question-answer form by collaborative users worldwide. There are basically two types of user in this CQA sites: Asker -- who post their query as questions and Answerer -- who provide the answers to these questions. The semi-structured and growing size of contents in CQA sites is posing several challenges. As there is no restriction in posting the number of answers to a question, so the common challenge is to identify the authoritative answerers of a question in order to evaluate the answer quality for selecting the best answer. In this paper, we use latent dirichlet allocation (LDA) the statistical topic modelling on textual data and statistical computing on metadata to identify the features that would reflect the topical authoritative of answerer. Then these features are represented as vector for each answerer of the dataset under investigation for learning the classifier model. The various baseline classifier model are used to identify the topical authoritative answerer on Q&amp;A posts of two real dataset extracted from StackOverflow and AskUbuntu. The correctness and effectiveness of classifier models are evaluated using various parameters like accuracy, precision, recall, and kappa statistic. The experimental result shows that Random Forest classifier outperforms over each evaluation parameter than other classification algorithms.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {129–132},
numpages = {4},
keywords = {Feature Identification, Classification, Topic Modeling, Statistical Computing, Authority Users, CQA},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998491,
author = {Shukla, Anmol and Aggarwal, Dhruv and Keskar, Ravindra B.},
title = {A Methodology to Detect and Track Breaking News on Twitter},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998491},
doi = {10.1145/2998476.2998491},
abstract = {Twitter is an interesting platform for the dissemination of news. The real-time nature and brevity of the tweets are conducive to sharing of information related to important events as they unfold. But, one of the greatest challenges is to find the tweets that we can characterize as news in the ocean of tweets. In this paper, we propose a novel method for detecting and tracking breaking news from Twitter in real-time. We filter the stream of incoming tweets to remove junk tweets using a text classification algorithm. We also compare the performance of different supervised text classification algorithms for this task. We then cluster similar tweets, so that, tweets in the same cluster relate to the same real-life event and can be termed as a breaking news. Finally, we rank the news using a dynamic scoring system which also allows us to track the news over a period of time.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {133–136},
numpages = {4},
keywords = {Twitter, Online Clustering, News Detection and Tracking, Classification of Tweets},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998492,
author = {Gopi, Pooja and Ramalingam, Mohanasundari and Arumugam, Chamundeswari},
title = {Search Based Test Data Generation: A Multi Objective Approach Using MOPSO Evolutionary Algorithm},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998492},
doi = {10.1145/2998476.2998492},
abstract = {Search based test data generation plays an important role in software testing. Several search based evolutionary algorithms are used to find the optimal test data. Among these algorithms, a meta-heuristic algorithm called Particle Swarm Optimization (PSO) algorithm is adopted for finding the optimal test data for the given Software Under Test (SUT) due to its simplicity and fast convergence. The success of PSO as a single objective optimizer in the literature has motivated to solve multi objective optimization problems. Hence, Multi Objective Particle Swarm Optimization (MOPSO) is adopted for solving more than one objective. This research work consider two objectives which attempts to maximize the branch coverage and reduce the test suite size. A benchmark program is used for the experimental analysis using MOPSO algorithm. The experimental analysis was performed using MOTestGen tool to extract the results. The extracted results portraits the convergence and coverage performance in producing the optimal test data as the population size increases.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {137–140},
numpages = {4},
keywords = {multi objective, branch coverage, test suite, fitness function, MOPSO},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998496,
author = {Patidar, Chandra Prakash and Sharma, Meena},
title = {An Automated Approach for Cross-Browser Inconsistency (XBI) Detection},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998496},
doi = {10.1145/2998476.2998496},
abstract = {Due to the spread of the internet and the ever increasing number of web applications, the issue of compatibility across browsers has become very important. This compatibility issue is also referred as Cross Browser Inconsistency (XBI) wherein same website looks or behaves differently in different web browsers. In this paper our aim is to address this issue of compatibility and propose an automated approach of detecting XBIs. Cross Browser Inconsistencies can either be in the content, structure or behavior of the webpage. In order to get a grasp of the above mentioned types of inconsistencies, we surveyed some random websites and analyzed them in different browsers. We also studied the basic working of browser, in order to establish its connection with the occurrences of XBIs. Each browser has its own rendering mechanisms, which sometimes differs from standards. Hence, the execution of these websites is different in different browsers. Finally we have proposed an automated approach for XBI detection.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {141–145},
numpages = {5},
keywords = {Crawling, XBI Detection, Document Object Model (DOM), Cross Browser Inconsistency},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998494,
author = {Bose, Joy and Singhai, Amit and Patankar, Anish Anil and Kumar, Ankit},
title = {Attention Sensitive Web Browsing},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998494},
doi = {10.1145/2998476.2998494},
abstract = {The attention level is an important indicator of the user's level of interest while viewing any content. The web browser is one of the most popular means to access information, and the usage of browsers in mobile devices is increasing. In this paper we analyze the use of attention as an input for web browsers. Attention can be measured easily in real time using cheap commercially available wearable EEG sensors, such as NeuroSky's MindWave. We use the measured level of attention in the following ways: as an input mechanism for navigating through the controls on the web browser such as buttons, menus and hyperlinks, to correlate the attention with the section of the webpage being browsed and make the web browser responsive to the user's attention level in real time, and as an input that is fed back to the web server enabling the web content developer to make attention sensitive websites. For each of these, we provide the implementation details and some results obtained. We also provide some pointers how the input attention level event obtained from the EEG sensors can be standardized in the W3C specification.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {147–152},
numpages = {6},
keywords = {EEG, Web Browser, attention, Responsive Web Browsing, Electroencephalography, Human machine interfaces, Brain Computer Interfacing, W3C specification},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998498,
author = {Agarwal, Bhoomika and Ravikumar, Abhiram and Saha, Snehanshu},
title = {A Novel Approach to Big Data Veracity Using Crowdsourcing Techniques and Bayesian Predictors},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998498},
doi = {10.1145/2998476.2998498},
abstract = {In today's world data is being generated at a tremendous pace and there have to be enough measures in place to verify the nature of big data. Analysis performed on 'dirty' data may lead to erroneous insights and thereby shaping decisions poorly. The aspect of big data that deals with its correctness is known as big data veracity. Trusting the data acquired goes a long way in implementing decisions from an automated decision-making system and veracity helps to validate the data acquired. In this paper, we present our solution to the big data veracity problem using crowdsourcing techniques. Our solution involves the use of sentiment analysis, which deals with identifying the sentiment expressed in a piece of text. As a proof of concept, we have developed an app that requires users to tag tweets as per the sentiment it evokes in them. Each tweet would therefore get ratified by hundreds of our participants and the sentiment associated to the tweet gets tagged. The tagged emotion was then evaluated against the verified emotion as compared to a verified data set. This analysis was then plotted on a ROC curve and also evaluated against verified data using a Bayesian predictor trained with a trinomial function. As can be seen, an accuracy of 81% was obtained as displayed by the ROC curve and 89% through the Bayesian predictor. Also, a MAP analysis of the Bayesian predictor yields neutral sentiment as the most probable hypothesis. By doing this, we have proven that crowdsourcing of sentiment analysis is a viable solution to the problem of big data veracity and therefore an aid in making better decisions.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {153–160},
numpages = {8},
keywords = {Crowdsourcing, Machine Learning, Sentiment Analysis, Tweet Mining, Bayesian Predictor, Big Data},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998495,
author = {Joshi, Shweta and Sonawane, Kavita V.},
title = {Selection of Image Blocks Using Genetic Algorithm and Effective Embedding with DCT for Steganography},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998495},
doi = {10.1145/2998476.2998495},
abstract = {In today's digital world that we live in, security of information is crucial in various communication applications that are widely developed. Steganography is one of the highly secure information hiding techniques. It provides invisible communication and hides the existence of information. This paper focuses on 'before embedding technique' of hiding in image steganography by trying to find suitable places in cover image to embed the secret image. Genetic algorithm (GA) is applied to identify appropriate places in cover image where embedding of secret image will cause minimum distortion. After obtaining these places, embedding is performed using transform domain technique Discrete Cosine Transform (DCT). The secret image is first normalized and then embedded in the lower energy DCT blocks of the selected cover image regions. The experimental results show that the stego images obtained from the proposed method have less visual distortion with satisfactory values in parameters like MSE, PSNR and Correlation used for performance evaluation.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {161–166},
numpages = {6},
keywords = {Steganography, Genetic Algorithm, secure communication, Stego Image, DCT},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

@inproceedings{10.1145/2998476.2998493,
author = {Jhummarwala, Abdul and Alkathiri, Mazin and Karamta, Miren and Potdar, M. B.},
title = {Comparative Evaluation of Various Indexing Techniques of Geospatial Vector Data for Processing in Distributed Computing Environment},
year = {2016},
isbn = {9781450348089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998476.2998493},
doi = {10.1145/2998476.2998493},
abstract = {The explosion of ever increasing geospatial data is today met with the challenge of maintaining it in spatial databases and utilization of traditional methods of spatial data processing. The sheer volume and complexity of spatial databases makes them an ideal candidate for use with parallel and distributed processing architectures. There is a lot of enthusiasm toward using MapReduce paradigm and distributed computing for processing of large volumes of vector data. As spatial data cannot be indexed using traditional B-tree structures used by R/DBMS, several libraries such as JSI (Java Spatial Index), libspatialindex and SpatiaLite depend upon advanced data structures such as R/R*-tree, Quad-tree and their variants for spatial indexing. These indexing mechanisms have also been natively incorporated in frameworks such as Spatial Hadoop, Hadoop GIS SATO and GeoSpark. Additionally, most widely used open source RDBMS such as MySQL, Postgres and SQLite incorporate spatial indexing using extensions/add-ons. In this paper, we benchmark and compare the performance of various spatial indexing mechanisms in addition to evaluating the performance of distributed frameworks for planet sized datasets. We conclude by highlighting the characteristics of spatial tools and frameworks for better selection and implementation of R-tree indexing in a big geo-spatial processing system.},
booktitle = {Proceedings of the 9th Annual ACM India Conference},
pages = {167–172},
numpages = {6},
keywords = {Apache Hadoop, MapReduce, spatial indexing, R-tree, Geospatial processing},
location = {Gandhinagar, India},
series = {COMPUTE '16}
}

