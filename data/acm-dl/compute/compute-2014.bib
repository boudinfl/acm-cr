@inproceedings{10.1145/2675744.2675746,
author = {Singh, Satyendr and Siddiqui, Tanveer J. and Sharma, Sunil K.},
title = {Na\"{\i}ve Bayes Classifier for Hindi Word Sense Disambiguation},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675746},
doi = {10.1145/2675744.2675746},
abstract = {This paper investigates Na\"{\i}ve Bayes (NB) classifier for Hindi Word Sense Disambiguation (WSD) utilizing eleven features. The features used in the experiment includes local context, collocations, unordered list of words, nouns and vibhaktis. Evaluation is done on a manually created sense annotated Hindi corpus consisting of 60 polysemous Hindi nouns. A precision of 77.52% was observed, using unordered list of words in feature vector. We obtained maximum precision of 86.11% by utilizing nouns in feature vector after applying morphology. The experimental results demonstrates that by adding more rich features, WSD accuracy of NB classifier can be significantly improved over unordered list of words. We obtained precision of 56.49%, by utilizing vibhaktis in the feature vector.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {1},
numpages = {8},
keywords = {na\"{\i}ve bayes classifier, Hindi word sense disambiguation, supervised hindi WSD, bayesian classification},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675748,
author = {M, Chandrajit and R, Girisha and T, Vasudev},
title = {Motion Segmentation from Surveillance Videos Using T-Test Statistics},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675748},
doi = {10.1145/2675744.2675748},
abstract = {Motion segmentation is an important task in video surveillance and in many high level vision applications. In this paper, an adaptive method using statistics in temporal framework to segment moving objects from surveillance video sequences captured in dynamic environment is proposed. The proposed method first preprocesses the input frames of video using Gaussian filter for noise reduction. Motion segmentation is done by employing statistical T-test on neighborhood RGB color intensity values of each pixel in two successive temporal frames. Several experiments along with comparison with existing method have been carried out on the IEEE PETS (2009 and 2013) and IEEE Change Detection (2014) datasets which include thermal, normal, PTZ, aerial and night vision sensor videos to demonstrate the efficacy of the proposed methods in dynamic environment and results obtained are encouraging.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {2},
numpages = {10},
keywords = {T-test, motion segmentation, spatio-temporal, video surveillance},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675749,
author = {Indarapu, Sivaramakrishna Bharadwaj and Maramreddy, Manoj and Kothapalli, Kishore},
title = {Architecture- and Workload- Aware Heterogeneous Algorithms for Sparse Matrix Vector Multiplication},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675749},
doi = {10.1145/2675744.2675749},
abstract = {Multiplying a sparse matrix with a vector, denoted spmv, is a fundamental operation in linear algebra with several applications. Hence, efficient and scalable implementation of spmv has been a topic of immense research. Recent efforts are aimed at implementations on GPUs, multicore architectures, and such emerging computational platforms. Owing to the highly irregular nature of spmv, it is observed that GPUs and CPUs can offer comparable performance.In this paper, we propose three heterogeneous algorithms for spmv that simultaneously utilize both the CPU and the GPU. This is shown to lead to better resource utilization apart from performance gains. Our experiments of the work division schemes on standard datasets indicate that it is not in general possible to choose the most appropriate scheme given a matrix. We therefore consider a class of sparse matrices that exhibit a scale-free nature and identify a scheme that works well for such matrices. Finally, we use simple and effective mechanisms to determine the appropriate amount of work to be alloted to the CPU and the GPU.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {3},
numpages = {9},
keywords = {heterogeneous algorithms, workload-aware, spmv, scale-free matrices},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675750,
author = {Yadav, Vivek and Ramanathan, Chandrashekar},
title = {Automated Layout Preservation in Cross Language Translation of Document: An Integrated Approach and Implementation},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675750},
doi = {10.1145/2675744.2675750},
abstract = {Layout refers to format and placement of content in a document. Cross language document translation is a well known problem and is addressed widely. Such translations usually remain unsuccessful in preserving original format of document because of typographical differences across the script of different languages. To make translated document look aesthetically identical to the original document, preservation of layout is essential. In this paper, we propose an integrated approach to solve various problems that arise during the process of translation pertaining to the layout of document like content flow, table of content, maintaining relative position and aesthetics of content.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {4},
numpages = {8},
keywords = {automated layout, cross language translation, format preservation},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675751,
author = {Sarkar, Amit},
title = {Pipette: A Virtual Content Transporter},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675751},
doi = {10.1145/2675744.2675751},
abstract = {This paper presents the design and evaluation of Pipette, a new framework that provides a stylus-based interactive and animated user interface (UI) for digital content transfer. The pipette detects stylus gestures when performed on touch-based devices and uses the gesture to trigger content copy and paste operation Along with gesture, it uses the context or scenario in which user is present to differentiate between copy or paste action. Pipette creates a visual illusion of actual content transfer into the stylus by showing suction animation to represent copy action. We propose the use of mass-spring model for animating content deformation. A mass propagation suction algorithm is presented to simulate absorption of content into the stylus. Pipette also proposes showing a pouring animation to represent paste action. We analyze the utility of pipette via a mobile application and benchmark with existing state of the art systems. Results show the effectiveness of pipette compared to presently known techniques for copy and paste operation.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {5},
numpages = {10},
keywords = {copy-paste, mobile interaction, stylus, suction animation, drop animation},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675752,
author = {Vaikuntam, Aparna and Perumal, Vinodh Kumar},
title = {Evaluation of Contemporary Graph Databases},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675752},
doi = {10.1145/2675744.2675752},
abstract = {Graph databases attempt to make the modelling and processing of highly interconnected data, easier and more efficient by representing a system as a graph-like structure of nodes and edges. The fundamental premise of Graph Databases, unlike relational, is explicit and distinct definition of relationships and direct, non-index based access of related nodes from a given node. Growth of graph databases happened in large part with a need for complex processing of interconnected documents in the WWW and the surge in social networking. What was initially proprietary research has now transformed into a plethora of commercial and open source products, increasingly being adopted outside the internet services industry. This paper attempts to evaluate several such contemporary Graph Databases from a subjective feature-based and empirical performance-based perspective.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {6},
numpages = {10},
keywords = {evaluation, graph databases, performance based, empirical, feature based, open source, survey, subjective, databases},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675753,
author = {Mohandas, Anju and S, Sabitha},
title = {Privacy Preserving Content Disclosure for Enabling Sharing of Electronic Health Records in Cloud Computing},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675753},
doi = {10.1145/2675744.2675753},
abstract = {Cloud computing has been gaining acceptance in various scenarios ranging from storing user's personal data to managing PHRs(Personal Health Records) as it facilitates data sharing. The major threat that hinders its wide deployment even today is the fear of losing privacy of the data stored on the cloud. Thus privacy preserving data sharing becomes an important concern for cloud users. The conventional encryption schemes cannot be directly applied in this scenario. A promising approach is to use ABE(Attribute Based Encryption) for the same. CP-ABE, a variant of ABE, can provide fine grained access control but one draw back of CP-ABE is that it focuses only on who accesses the data. To address this concern, we combine CP-ABE with Anonymization to enforce fine grained privacy preserved access control and data sharing among a group of cloud users. Although, this paper discusses on sharing EHRs, the concepts can be generalized and applied to other scenarios as well.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {7},
numpages = {7},
keywords = {CP-ABE, EHR, anonymization, privacy, access control},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675754,
author = {Bidyarthy, Ajay Shankar and Raman, Rajiv and Thomas, Dilys},
title = {A Toolbox for Fast and Approximate Solutions for Large Linear and Semidefinite Programs},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675754},
doi = {10.1145/2675744.2675754},
abstract = {The multiplicative weights update is an algorithmic design technique that, over the years, has been discovered independently by several groups of researchers in different contexts. The earliest known use of this technique is by Brown and vonNeumann for the computation of equilibrium strategies in zero-sum games. These ideas and variants thereof have found applications in solving games, linear and semidefinite programming, computational geometry, machine learning, auctions and mechanism design, as well as data privacy.In this paper, we focus on the use of this technique for solving zero-sum games, and for solving linear and semidefinite programs. In particular, we present empirical results on the efficacy of this method to solve linear and semidefinite programs approximately.Our results suggest that this could be the preferred method in scenarios where one is interested in obtaining reasonably good results very quickly, as well as in scenarios where the problem sizes are very large. Another advantage of this technique is that it is amenable to a parallel or distributed implementation unlike interior-point or simplex methods, and therefore could form a primitive in cluster computing, or in sensor networks.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {8},
numpages = {8},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675755,
author = {Singh, Gagandeep and Shukla, Manish and Bhimalapuram, Prabhakar and Kothapalli, Kishore},
title = {Implementation of Kirchhoff-Helmholtz Transform on GPU for Use in Digital in-Line Holographic Microscopy},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675755},
doi = {10.1145/2675744.2675755},
abstract = {GPUs with their massive Single Instruction Multiple Data (SIMD) capability have become attractive for scientific computation. The Kirchhoff-Helmholtz Transform (KHT) is a two dimensional integral commonly used in numerical reconstruction of the object from its experimentally measured diffraction pattern (called hologram). We explore the evaluation of KHT using GPU at various levels of optimisation. These optimisations are of two types: (a) algorithmic: exploiting the symmetries inherent in KHT, and (b) programmatic: optimizations that are specific to the GPU architecture like the lookup tables, scheduling of read/writes. From the numerical experiments, we report the speedup for each level of optimisation.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {9},
numpages = {9},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675756,
author = {Pawar, Sandip and Chaudhuri, Parag},
title = {A Study on Parallelizing Grid-Based Fluid Simulations on the CPU},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675756},
doi = {10.1145/2675744.2675756},
abstract = {We present a study based on our initial efforts to parallelize a grid-based fluid solver. The standard Eulerian grid-based fluid solver discretizes the Navier Stokes equation on a regular grid and computes the fluid velocity and pressure in each grid cell. We use OpenMP to parallelize this computation and study the behaviour of our implementation on various machines with different number of CPU cores. In this study, we do not use the GPU to accelerate our simulation. We concentrate on understanding the behaviour of the simulation on the CPU instead.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {10},
numpages = {5},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675757,
author = {Azeem, Mohammed Abdul and Sharfuddin, Mohammed and Ragunathan, T.},
title = {Support-Based Replication Algorithm for Cloud Storage Systems},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675757},
doi = {10.1145/2675744.2675757},
abstract = {Replication has been used in cloud storage systems as a way to increase data availability. Existing replication strategies proposed for cloud storage systems allow users to fix the replication factor to replicate data. The problem with this approach is that either the user under estimates or over estimates the replication factor due to which the performance of the cloud storage system is affected. In this paper, we have proposed support-based replication algorithm which decides the replication factor based on the popularity of the file and access frequency of the blocks contained in the file. We have also proposed efficient way of placing the replicas in the cloud storage systems in order to improve the performance. Our analysis on storage and average read access time indicates that the proposed replication algorithm requires less storage space and provides better average read access time in comparison with Hadoop distributed file system.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {11},
numpages = {9},
keywords = {access frequency, replica factor, cloud storage system, replication strategies},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675758,
author = {Baskaran, Sriram and Apte, Manoj},
title = {RAPIDCharts: A Framework for Charting Dynamic Data Using XML},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675758},
doi = {10.1145/2675744.2675758},
abstract = {Visualization of the raw data into meaningful information is necessary for the proper understanding of the concepts at hand. Many real life scenarios require visualization of data as a statistical chart. Creation of such charts with dynamic data brings in challenges like impact at different levels in the code, redundancy of the code and restarts of servers every time a new build is released. In this paper, we have studied the issues with the conventional way of development and proposed a framework for charts by giving specifications in a separate eXtensible Markup Language (XML) file called Single File Specification System (SFS) and automating the process of creation of the charts and tables. We present RAPIDCharts, a framework that will reduce the impact of change or addition of new charts, redundancy of code and server restarts using an external file where the specifications are mentioned.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {12},
numpages = {10},
keywords = {charting framework, UI framework, user interface, XML framework, automatic UI generation, UI designing},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675759,
author = {Kushwaha, Nidhi and Vyas, O. P.},
title = {SemMovieRec: Extraction of Semantic Features of DBpedia for Recommender System},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675759},
doi = {10.1145/2675744.2675759},
abstract = {Recommendation System has been developed with the growth of Word Wide Web. Recently, Web3.0 or Semantic Web has changed the traditional way of its related approaches, by leveraging knowledge of Linked Open data Cloud which consist of domain specific and cross domain interconnected datasets. It fabricates thousands of RDF triples and millions of links (external/internal) to connect this open source data. As per our literature survey we have found that the Recommender System based on Linked Open data Cloud does not deal with this Knowledge Base in an efficient manner because of the problem of data sparsity and inconsistency, which results due to automatic generation of Resource Description Format data from unstructured documents that leads to garbage data have no sense in recommending. This paper aims to explore a hybrid recommender which can be used as a rating predictor as well as movie recommender of RDF datasets. Also, we present a new model for Recommender System that not only utilizes DBpedia Knowledge Base but also remove the former problems in Recommender System by using a preprocessing technique for sparsity removal. To prove the correctness and accuracy of our model we have implemented and tested it over other previous methodologies. In order to make our algorithm efficient, we also used different data structure for storing and processing.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {13},
numpages = {9},
keywords = {linked open data, hybrid, dbpedia, recommendation, recalll, precsion},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675760,
author = {Rao, Varshanth R. and Kumar K M, Anil},
title = {Predictive Node Expiration Based Energy-Aware Source Routing (PNEB ESR) Protocol for Wireless Sensor Networks},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675760},
doi = {10.1145/2675744.2675760},
abstract = {Wireless sensor networks essentially consist of an amalgam of autonomous sensor nodes deployed in an ad-hoc manner to collect information about the surroundings. These sensor nodes usually dispense the collected information to data sink nodes through information routing techniques. Predictive Node Expiration Based Energy -- aware Source Routing (PNEB ESR) protocol is an energy aware routing protocol which attempts to optimize the overall energy efficiency of the sensor network and ensures the sensed information reaches a data sink through a reliable path. In this protocol, the Energy Depletion Rate of a node is calculated at predetermined energy levels and is used to predict when a node will outlive its usefulness. This is used in the prediction of route expirations which will provide timely insights to the nodes on when to seek for alternative routes to data sinks. Simulations show that the PNEB ESR protocol is successful in making reliable route selections and theoretically minimizes the number of control packets flowing in the network. This leads to considerable reduction in processing and communication costs and hence minimizes overall energy consumption.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {14},
numpages = {6},
keywords = {variable power transmission, wireless sensor networks, energy aware routing protocols, sensitivity threshold, energy depletion rate},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675761,
author = {Batra, Payal Khurana and Kant, Krishna},
title = {Stable Cluster Head Selection in LEACH Protocol: A Cross-Layer Approach},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675761},
doi = {10.1145/2675744.2675761},
abstract = {Optimal cluster head count plays a key role in determining the efficiency of a cluster based routing protocol. If the cluster head count will be more than the optimal value, aggregation advantage cannot be taken fully. If clusters are less, then there will be more aggregation load on the cluster head and communication load on the cluster members as they have to communicate with cluster head over long distances. LEACH (Low Energy Clustering Adaptive Hierarchy) protocol, a popular clustering protocol, suffers from the cluster head variability problem. In this paper, we propose an approach which uses cross layer information to overcome this problem. NS-2 simulations show that improved LEACH increased the network lifetime i.e FND (First Node Death) time by 22%, HNA (Half Nodes Alive) by 24% and LND (Last Node Death) time by 12% respectively.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {15},
numpages = {6},
keywords = {clustering, energy efficiency, cluster head, cross layer},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675762,
author = {Nautial, Ankit and Sristy, Nagesh Bhattu and Somayajulu, D. V. L. N.},
title = {Finding Acronym Expansion Using Semi-Markov Conditional Random Fields},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675762},
doi = {10.1145/2675744.2675762},
abstract = {Acronyms are heavily used Out of Vocabulary terms in sms, search-queries, social media postings. The performance of text mining algorithms such as Part of Speech Tagging(POS), Named Entity Recognition, Chunking often suffer when they are applied over the noisy text. Text normalization systems are developed to normalize the noisy text. Acronym mapping and expansion has become an important component of the text normalization process. Since manually collecting acronyms and their corresponding expansions from the documents is difficult, automatically building such a dictionary using supervised learning is the need of the hour. In this work, we focus on the acronym search problem: Given acronyms as queries, finding their corresponding expansions in a document.Recent works formulate the given problem as a token-level sequence labelling task and employ Hidden Markov Model, or Conditional Random Fields, to tackle the problem. However, these models do not utilize the segment level information inherent in the expansion. Hence we propose a Semi-Markov Conditional Random Field based approach for the given problem, that gives us power to write more effective features that work on a group of neighbouring tokens together than the features working on individual tokens. We design and implement Semi-Markov Conditional Random Fields to identify the correct acronym expansions for data extracted from Wikipedia and compare the performance with the Conditional Random fields. The experimental results show that Semi-CRF based approach for the given task performs better than the CRF based approach.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {16},
numpages = {6},
keywords = {sequence labelling, expansion finding, text mining},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675763,
author = {Goyal, Poonam and Kumari, Sonal and Kumar, Dhruv and Balasubramaniam, Sundar and Goyal, Navneet},
title = {Parallelizing OPTICS for Multicore Systems},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675763},
doi = {10.1145/2675744.2675763},
abstract = {Parallelizing algorithms to leverage multiple cores in a processor or multiple nodes in a cluster setup is the only way forward to handle ever-increasing volumes of data. OPTICS is a well-known density based clustering algorithm to identify arbitrary shaped clusters. Since, hierarchical cluster ordering of OPTICS is sensitive to the order in which data is processed, typically a priority queue is used to maintain the order. This sequential access order makes it difficult to parallelize OPTICS. Moreover, the execution time of OPTICS increases with increase in density of data. We propose a parallel version of OPTICS for shared memory multi-core systems using a master-slave pattern for parallelization. The master runs concurrently with the slaves and distributes data to the slaves. Each slave performs neighborhood queries for a subset of data. Our approach ensures that cluster ordering matches with that of the classical OPTICS. Our solution runs in a mostly data parallel mode yielding scalable performance. We also argue that our approach is well suited for dense datasets in particular.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {17},
numpages = {6},
keywords = {multicore, density-based clustering, parallel computing, shared memory systems, optics},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675764,
author = {Yadav, Sonal and Gaur, M. S. and Laxmi, V. and Bhargava, Megha},
title = {Dynamic Fault Injection Model for On-Chip 2D Mesh Network},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675764},
doi = {10.1145/2675744.2675764},
abstract = {In current scenario, On-Chip Network advances to fulfill higher bandwidth demand rises by chip multiprocessor (CMP). Each nanometer technology of transistor is giving high performance but becoming more fault prone. Hardware faults are injected inside the network to determine its behavior by using some offline fault injection mechanism. This model address permanent and transient fault injection mechanism either at run time or compile time. The paper proposes a fault injection model of hardware faults using software based fault injection technique. Fault injection model is implemented by extending Nirgam Simulator. The effectiveness of injection model can be seen by analyzing results of latency and throughput in different scenario.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {18},
numpages = {6},
keywords = {latency, faults, throughput, hardware faults, on-chip networks, fault injection},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675765,
author = {Chaudhari, Narendra and Mahajan, Anjali},
title = {Parallelization of RNA Secondary Structure Prediction Algorithm on Multicore},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675765},
doi = {10.1145/2675744.2675765},
abstract = {RNA secondary structure prediction is invaluable in creating new drugs and understanding genetic diseases, in gene expression and regulation. RNA molecules have proven their clinically consequential deep inroads into the biological functions. Needless to mention about the role of even small RNA's with numerous nucleotides which does the function of gene splicing, editing, and regulation. There is accelerating increase of data set sizes originating from powerful high-throughput measuring devices in Bioinformatics, A major limiting factor in achieving good performance on modern architectures is memory latency and bandwidth. The cost of computation is very less than the cost of moving data from main memory. This disparity between communication and computation costs has compelled the programmers to make changes in the ways algorithms are designed.A promising solution to speed up sequential programs, which are difficult to parallelise otherwise is through Speculative parallelisation.This paper proposes a new software-only speculative parallelization scheme for implementing RNA Secondary Structure Prediction algorithm in parallel. The scheme is developed after a systematic evaluation of the design options available. It is also shown to be efficient, robust and to outperform previously proposed schemes used for parallel implementation of RNA Secondary Structure Prediction.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {19},
numpages = {6},
keywords = {RNA secondary structure prediction, speculative parallelization},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675766,
author = {Raju, Bksp Kumar and Geethakumari, G.},
title = {A Novel Approach for Incident Response in Cloud Using Forensics},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675766},
doi = {10.1145/2675744.2675766},
abstract = {Cloud Computing is a technology which can take distributed and high performance computing to the next level. The concept of distributed virtualization enables cloud service providers to ensure better service availability along with optimal utilization of resources. Users, on the other hand get benefited in terms of cost reduction, less maintenance overheads, rapid provision and release of resources with minimal service provider interaction. However, lack of transparency and security procedures in place is reducing the maximal usage of cloud services. These issues can be better addressed by facilitating digital forensics in cloud which involves lot of challenges. The main concern is lack of trust in cloud. Any security mechanism may get violated at some point of time and this can lead to the reputation loss for the cloud service provider (CSP). The CSP then has to rebuild the trust by effective incident management. One of the challenges in incident handling (IH) is detecting the incidents in a timely manner and identifying the intruder. In this paper, we are proposing a framework for the same.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {20},
numpages = {6},
keywords = {incident response, cloud forensics, traditional forensics, incident, cloud computing},
location = {Nagpur, India},
series = {COMPUTE '14}
}

@inproceedings{10.1145/2675744.2675767,
author = {Sarkar, Santonu and Mitra, Sayantan},
title = {Execution Profile Driven Speedup Estimation for Porting Sequential Code to GPU},
year = {2014},
isbn = {9781605588148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675744.2675767},
doi = {10.1145/2675744.2675767},
abstract = {Parallelization of an existing sequential application to achieve a good speed-up on a data-parallel infrastructure is quite difficult and time consuming effort. One of the important steps towards this is to assess whether the existing application in its current form can be parallelized to get the desired speedup. In this paper, we propose a method of analyzing an existing sequential source code that contains data-parallel loops, and give a reasonably accurate prediction of the extent of speedup possible from this algorithm. The proposed method performs static and dynamic analysis of the sequential source code to determine the time required by various portions of the code, including the data-parallel portions. Subsequently, it uses a set of novel invariants to calculate various bottlenecks that exists if the program is to be transferred to a GPGPU platform and predicts the extent of parallelization necessary by the GPU in order to achieve the desired end-to-end speedup. Our approach does not require creation of GPU code skeletons of the data parallel portions in the sequential code, thereby reducing the performance prediction effort. We observed a reasonably accurate speedup prediction when we tested our approach on multiple well-known Rodinia benchmark applications, a popular matrix multiplication program and a fast Walsh transform program.},
booktitle = {Proceedings of the 7th ACM India Computing Conference},
articleno = {21},
numpages = {6},
keywords = {speedup estimation, SIMT, warp, GPU, data transfer, code analysis, instrumentation},
location = {Nagpur, India},
series = {COMPUTE '14}
}

