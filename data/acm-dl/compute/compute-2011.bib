@inproceedings{10.1145/1980422.1980423,
author = {Dehuri, S. and Mishra, B. S. P. and Roy, R. and Cho, S.-B.},
title = {A Serial and Parallel Genetic Based Learning Algorithm for Bayesian Classifier to Predict Metabolic Syndrome},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980423},
doi = {10.1145/1980422.1980423},
abstract = {This paper presents a serial and parallel genetic based learnable bayesian classifier for designing a prognostic model for metabolic syndrome. The objective of the classifier is to address the fundamental problem of finding the optimal weight in the learnable bayesian classifier, by serial GA, and minimize the response time by parallel GA. The algorithms exhibit an improved capability to eliminate spurious features from the large dataset and aid the researchers in identifying those features that are solely responsible for high prediction accuracy. The effectiveness of the classifier are demonstrated using metabolic syndrome dataset obtained from Yonchon County of Korea.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {1},
numpages = {6},
keywords = {genetic algorithm, learnable naive bayesian classifier, metabolic syndrome diseases, parallel genetic algorithm},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980424,
author = {Laha, Arijit},
title = {An Agent-Based Architecture for a Knowledge-Work Support System},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980424},
doi = {10.1145/1980422.1980424},
abstract = {Enhancement of technology-based system support for knowledge workers is an issue of great importance. The "Knowledge work Support System (KwSS)" framework analyzes this issue from a holistic perspective. KwSS proposes a set of design principles for building a comprehensive IT-based support system, which enhances the capability of a human agent for performing a set of complex and interrelated knowledge-works relevant to one or more target task-types within a domain of professional activities. In this paper, we propose a high-level, software-agent based architecture for realizing a KwSS system that incorporates these design principles. Here we focus on developing a number of crucial enabling components of the architecture, including (1) an Activity Theory-based novel modeling technique for knowledge-intensive activities; (2) a graph theoretic formalism for representing these models in a knowledge base in conjunction with relevant entity taxonomies/ontologies; and (3) an algorithm for reasoning, using the knowledge base, about various aspects of possible supports for activities at performance-time.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {2},
numpages = {8},
keywords = {agent-based architecture, reasoning, activity modeling, activity theory, assistive system, knowledge representation, knowledge-work},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980425,
author = {Mehta, Hemant and Kanungo, Priyesh and Chandwani, Manohar},
title = {EcoGrid: A Dynamically Configurable Object Oriented Simulation Environment for Economy-Based Grid Scheduling Algorithms},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980425},
doi = {10.1145/1980422.1980425},
abstract = {In this paper, an economy-based Grid simulator named as EcoGrid is presented. EcoGrid is developed as a test-bed for Grid scheduling algorithms that are based on dynamic load balancing. EcoGrid is dynamically configurable. It optimizes the cost of execution of a process and maximizes the profit of a service provider system. As compared to other commonly used Grid simulation tools like GridSim and SimGrid, this simulator provides more enhanced features like configurable, scalable and extensible. This object oriented simulator also supports execution of Java applications. EcoGrid supports simulation of both the economy based and non-economy based scheduling algorithm.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {3},
numpages = {8},
keywords = {economy-based scheduling, simulation environment, grid computing, load generator, grid economy},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980426,
author = {Devi, S. M. Renuka and Bhagvati, Chakravarthy},
title = {A Novel Method for Image Retrieval Using Relevance Feedback and Unsupervised Clustering},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980426},
doi = {10.1145/1980422.1980426},
abstract = {The standard approach to content-based image retrieval is currently concerned with bridging the semantic gap or the gap between the results produced by the use of low-level features and the human end-user expectations based on high-level semantics. In this paper, we suggest that there are advantages to bridging the gap in two stages by proposing an intermediate level. We show that unsupervised clustering of low-level image features provides a suitable basis for an intermediate level representation and define a CBIR system using such an approach. The main advantages of using an intermediate level are (a) it is not necessary for all positive responses to a user query be categorized into a single class; (b) it is possible to overcome the small-sample problem with too few positive examples; and, (c) to improve performance without greatly increased computational cost. Experimental results on Wang's database (1000 images) and Corel Photo gallery (10,800 images) show that the intermediate level analysis leads to better results.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {4},
numpages = {6},
keywords = {expectation maximization, semantic images, minimum description length, Gaussian mixture model, CBIR},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980427,
author = {Devi, S. M. Renuka and Bhagvati, Chakravarthy},
title = {Connected Component in Feature Space to Capture High Level Semantics in CBIR},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980427},
doi = {10.1145/1980422.1980427},
abstract = {An important problem in Content Based Image Retrieval (CBIR) systems is the gap between the human high-level semantics and the low-level machine features. In this paper, we develop a novel approach based on the intuition that a query along with the responses from the user during a relevance feedback session provides sufficient cues for learning multiple high-level concepts associated with the query image. For example, a single query image showing a yellow rose contains several high-level semantics such as yellow roses, any rose flower, any yellow coloured flower, a flower, a flower in front-view, etc. Unlike the past approaches that modelled positive responses from the user as a single class with a unimodal probability distribution function, we show that it is more appropriate to group them into multiple connected components in the feature space. It is demonstrated that these components capture and differentiate between the various semantics of an image. We also show that these components may be computed automatically by using a Gaussian Mixture Model. Results on several images illustrate the potential of these connected components to capture the multiple semantics of an image.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {5},
numpages = {6},
keywords = {connected component, semantic images, CBIR, Gaussian mixture model},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980428,
author = {Hyser, Chris and Gmach, Daniel and Ml, Umesh and Chen, Yuan and Suryanarayana, Vijay},
title = {Improving Server Power Management in Research and Development Data Centers},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980428},
doi = {10.1145/1980422.1980428},
abstract = {Research data centers are often composed of thousands of diverse computer systems used for ongoing research, development, software regression and hardware compatibility testing. The usage patterns of many of these systems result in periodic non-use and extended periods of idleness. Users routinely fail to ensure that idle machines are powered down prior to overnight or extended absence periods. The annual amount of wasted energy in the HP Bangalore development data center is estimated at 14400 MWh resulting in over 8600 tons of CO2 emissions per year. In this paper, we propose Idle Machine Power Savings (IMPS), which seeks to address potential power cost savings and minimize environmental impact. IMPS consists of a low overhead, highly scalable data acquisition framework enabling the development of algorithms (an artificial neural network is used in the initial prototype) for automatic "extended idle" notifications and optional automatic shutdown of unused computers in data centers. This paper describes our approach, the framework, a prototype implementation and provides preliminary results. The results show an enormous potential for energy savings that translate directly into financial savings and lowered greenhouse gas emissions.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {6},
numpages = {6},
keywords = {sustainable computing, data mining, machine learning algorithms, power management, sustainability},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980429,
author = {Girisha, R. and Murali, S.},
title = {Tracking Humans Using Novel Optical Flow Algorithm for Surveillance Videos},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980429},
doi = {10.1145/1980422.1980429},
abstract = {We propose an approach to track moving objects (humans) using optical flow in surveillance videos in this paper. We combine object segmentation output with optical flow algorithm while tracking object. That is, the proposed algorithm uses the object segmentation results while calculating optical flow and optical flow is only calculated in silhouette regions of motion using Two Way ANOVA. We track silhouettes (possible human torso), since these are more robust to variations in lighting conditions. The experimental results have demonstrated that our approach achieved good performance and the operating speed is relatively lower than some of the other standard optical flow techniques. We test our approach on several video surveillance sequences, both in indoor and outdoor.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {7},
numpages = {8},
keywords = {optical flow, two way ANOVA, video surveillance, object segmentation, object tracking},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980430,
author = {Dawn, Suma and Saxena, Vikas and Sharma, Bhudev},
title = {DEM Registration Using Watershed Algorithm and Chain Coding},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980430},
doi = {10.1145/1980422.1980430},
abstract = {DEM (Digital Elevation Model) is the model that gives elevation of each point of the earth surface in discrete form in a 3-D space. Image registration, implies registration of multi-temporal, multi-modal, multi-resolution, images of the same area. Registration of DEMs is now a days gaining a lot of popularity among the research community. DEM registration, in this case, is the registration of multi-temporal DEMs. Popular techniques for feature extraction and matching include wavelet approach, robust SIFT, or are based "super-points". This paper presents the DEM registration scheme based on watershed transformation, followed by two post-processing steps of clustering and morphological operations which is applied on both the DEMs -- candidate, as well as, reference DEM. Chain coding based matching is concluding step of the complete process. The system is semi-automatic i. e. expert input is considered before the final registration. Experimental results give good outcomes as shown from the error matrix analysis of RMSE, and PSNR. The system may be extended by using fuzzy classification and context-sensitive learning.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {8},
numpages = {5},
keywords = {morphological operation, watershed transformation, chain-coding, k-means clustering},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980431,
author = {Hegde, Chetana and Phanindra, J. and Shenoy, P. Deepa and Venugopal, K. R. and Patnaik, L. M.},
title = {Human Authentication Using Finger Knuckle Print},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980431},
doi = {10.1145/1980422.1980431},
abstract = {Automated security is one of the major concerns of modern times. Secure and reliable authentication systems are in great demand. A biometric trait like Finger Knuckle Print (FKP) of a person is unique and secure. In this paper, we propose a human authentication system based on FKP image of a person. Depending on the security level required by an organization that implements the proposed system, we provide two modes of security viz. basic mode and advanced mode. The Radon Transform is applied on pre-processed FKP image and Eigen values are computed. For basic mode, we compute the correlation coefficient between the set of Eigen values stored in the database and that of input image to authenticate a person. For advanced level of security, we identify the peak points in Radon graph. The successive distances between those points are calculated and are stored in a vector. Now, the elements in distance vector stored in database and that of input image are compared. Such a match is considered to be success if the difference between two such elements is lesser than the threshold value. Now, the probability of success is computed. To authenticate a person in advanced mode, we use the correlation coefficient between Eigen values and the probability. For real time implementation, suitable GUI can be developed. The basic mode of security system is found to have FAR as 6.79% and FRR as 0.0517%. The advanced system has the FAR of about 1.55% and FRR as 1.02%.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {9},
numpages = {8},
keywords = {probability, FRR, FAR, correlation coefficient, eigen values, radon transform},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980432,
author = {Hudli, Shrinidhi and Hudli, Shrihari and Hudli, Raghu and Subramanian, Yashonath and Mohan, T. S.},
title = {GPGPU-Based Parallel Computation: Application to Molecular Dynamics Problems},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980432},
doi = {10.1145/1980422.1980432},
abstract = {Parallel and distributed computing techniques have been applied to solve problems that have long running times. Molecular dynamics (MD) is used extensively in physics, chemistry, material sciences and biology to study statistical mechanical properties of systems. MD simulators are known to have extremely long running times, especially as the number of particles being simulated increase. Researchers have used PVM and MPI techniques to speed up MD computations and have achieved 5X to 10X speed up using clusters of computers. Recently NVIDIA and other Graphics Processing Unit (GPU) manufacturers have made available GPU cards for general purpose computing besides rendering graphics. We present a novel idea of solving the MD simulation problem in parallel using General Purpose Graphics Processing Unit (GPGPU) cards. MD problems are particularly amenable to data parallelization. Exploiting this property of the problem and the parallel constructs of GPGPU libraries, we present a parallel MD algorithm, which has a peak speedup of 200X for large problem sizes. Our work can be generalized to parallelize the now popular MapReduce programming paradigm.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {10},
numpages = {8},
keywords = {parallel programming, molecular dynamics, CUDA},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980433,
author = {Narayanan, Krishnaprasad and Bose, Sumit Kumar and Rao, Shrisha},
title = {Towards 'integrated' Monitoring and Management of DataCenters Using Complex Event Processing Techniques},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980433},
doi = {10.1145/1980422.1980433},
abstract = {Diagnosing cause of system failure in data centers that house large interconnected complex computer systems is a herculean task. This is because different monitoring tools for network, storage, server, facilities and application provide useful information regarding the health of the communication systems, the storage arrays, the physical machines, the environmental factors and the applications within a data center respectively in only a piece-meal manner. The existing tools fail to provide a comprehensive view of the complete set of operations within a data-center. In the absence of integrated monitoring and management tools, a data center administrator has to manually shuffle through and analyze data from various logs generated by the disparate monitoring tools on occurrence of a fault for identifying the root cause. In this paper we propose an approach for integrated data center health monitoring and management framework on top of the existing monitoring tools. The integrated framework leverages complex event processing techniques to process massive streams of events from these tools in (near) real time and enables automatic reuse of the existing monitoring tools in a non-intrusive manner.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {11},
numpages = {5},
keywords = {fault diagnosis, integrated monitoring, complex event processing, data center},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980434,
author = {Mani, Sudha and Rao, Shrisha},
title = {Operating Cost Aware Scheduling Model for Distributed Servers Based on Global Power Pricing Policies},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980434},
doi = {10.1145/1980422.1980434},
abstract = {Reducing the power consumption and operational cost of IT servers is of great concern today. With the growth of the Internet and online services, the number of data centers is increasing day by day. Servers for many cloud applications and other large providers are spread globally. Energy costs across the globe vary dynamically. Servers operate at varied energy costs based on their location and time of use. The load of a server varies based on its geographical location and the time of operation. This paper focuses on exploiting the dynamic nature of electrical power pricing, so that a cost saving is obtained by geo-location of requests to servers operating at lower costs at particular times. There exist patterns of load that are similar for different types of servers. Scheduling decisions are made considering both loads and operating costs of the servers into account, i. e., requests are scheduled to run on servers operating at low cost that also have low expected load. In order to meet the business requirements of an application, scheduling decisions for requests that have stringent SLA considerations or high server affinity, are made by assigning high priority for these requests. Geo-location of requests is done for low priority requests.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {12},
numpages = {8},
keywords = {operating cost, scheduling, energy cost, distributed servers},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980435,
author = {Heredero, Genoveva Galarza and Bandyopadhyay, Subhadip and Laha, Arijit},
title = {Extraction of Contextual Information from Medical Case Research Report Using WordNet},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980435},
doi = {10.1145/1980422.1980435},
abstract = {Relevant information within a document are usually embedded within a few sentences or passages (units). If any semantic tagging can be associated at the unit level within a document, the understanding of the information will be deeper and quicker saving a lot of effort and time of the user. In this paper we propose a simple approach of sentence tagging using the relational semantic network among lexical units as presented in WordNet. The approach is to propose a domain specific sub-taxonomy of key concepts following WordNet structure and associate a meaning with each of the sentences contextually. This approach also identifies those words from the text that can provide important semantic information in a tag assignation task. The occurrence of keywords will determinate a series of patterns that can be converted into rules for deciding the tagging and also information extraction as a useful application.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {13},
numpages = {8},
keywords = {tagging, information retrieval, WordNet, information extraction, context, biomedical domain},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980436,
author = {Manjunath, S. and Guru, D. S. and Suraj, M. G. and Harish, B. S.},
title = {A Non Parametric Shot Boundary Detection: An Eigen Gap Based Approach},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980436},
doi = {10.1145/1980422.1980436},
abstract = {Shot boundary detection is one of the challenging and crucial task in designing video archival and retrieval system. Even though a good number of works on shot boundary detection can be found in the literature, still there is a need of developing novel shot boundary detection algorithms which can work in a real time environment. In this paper we present a novel, simple, non parametric shot boundary detection approach where eigen gap analysis is exploited to preserve the variations among the video frames. To detect different types of shot boundaries the concept of small eigen values on a curve obtained by plotting the variations of eigen gap across the video frames is used. To corroborate the efficacy of the proposed method an experimentation on a large set of video frames having different types of transitions is carried out. For experimentation, news, lecture and entertainment videos collected from the World Wide Web is used. Also a qualitative comparative analysis of the proposed method with other existing similar type of techniques has been provided in the paper.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {14},
numpages = {7},
keywords = {error rate, shot boundary detection, dominant point detection, hit rate, eigen gap, eigen value},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980437,
author = {Akhani, Janki and Chuadhary, Sanjay and Somani, Gaurav},
title = {Negotiation for Resource Allocation in IaaS Cloud},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980437},
doi = {10.1145/1980422.1980437},
abstract = {Infrastructure As A Service (IaaS) cloud providers manage a large set of computing resources. Resources are provided to cloud consumers on demand in the form of virtual machines. Haizea, a resource lease manager, supports advance reservation, immediate, best effort and deadline sensitive resource allocation policies. Haizea uses resource leases as resource allocation abstraction and provides these leases by allocating virtual machines. Haizea does not support negotiation for any allocation policy. This work proposes decision making algorithms and extends the current advance reservation algorithms in Haizea to provide negotiation based allocation. Experimental results show that the proposed algorithms maximize resource utilization and acceptance of requests in comparison with existing algorithms in Haizea.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {15},
numpages = {7},
keywords = {scheduling and virtualization, cloud, allocation},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980438,
author = {Kumar, K. M. Anil and Suresha},
title = {Detection of Web Users' Opinion from Multimodal Opinion Elements},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980438},
doi = {10.1145/1980422.1980438},
abstract = {In this paper we present an approach to identify opinion of web users from opinionated texts and to classify web users opinion into positive or negative. Today web users express their opinion using multimodal opinion elements such as opinion phrases, emoticons and short words. These form of multimodal opinion expressions are very popular and are used by a large number of web users to document their opinion. In this paper we use semantic based approach to find users opinion from multimodal opinion elements like phrases, emoticons and short words. Our approach detects these multimodal opinion elements and uses them to obtain semantic orientation scores. These scores are later used to identify users opinion from opinionated texts. Our approach is effective and provides better results compared to other approaches on different data sets comprising of opinion.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {16},
numpages = {4},
keywords = {sentiment analysis, opinion mining, affective computing},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980439,
author = {Kumar, Sushanta and Reddy, P. Krishna and Reddy, V. Balakista and Singh, Aditya},
title = {Similarity Analysis of Legal Judgments},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980439},
doi = {10.1145/1980422.1980439},
abstract = {In this paper, we have made an effort to propose approaches to find similar legal judgements by extending the popular techniques used in information retrieval and search engines. Legal judgements are complex in nature and refer other judgements. We have analyzed all-term, legal-term, co-citation and bibliographic coupling-based similarity methods to find similar judgements. The experimental results show that the legal-term cosine similarity method performs better than all-term cosine similarity method. Also, the results show that bibliographic coupling similarity method improves the performance over co-citation approach.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {17},
numpages = {4},
keywords = {legal judgments, case citation, information retrieval, case retrieval},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980440,
author = {Pandey, Gaurav and Rao, Vikram Nagaraja and Srivastava, Ashish Kumar and Banerjee, Udayan and Narasimhan, Eswaran},
title = {Current Cloud Scenario Review and Cost Optimization by Efficient Resource Provisioning},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980440},
doi = {10.1145/1980422.1980440},
abstract = {Cloud computing promises to deliver cost saving through the "pay as you use" paradigm. The focus is on adding computing resources when needed and releasing them when the need is serviced. Since cloud computing relies on providing computing power through multiple interconnected computers, there is a paradigm shift from one large machine to a combination of multiple smaller machine instances. In this paper, we review the current cloud computing scenario and provide a set of recommendations that can be used for designing custom applications suited for cloud deployment. We also present a comparative study on the change in cost incurred while using different combinations of machine instances for running an application on cloud; and derive the case for optimal cost.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {18},
numpages = {4},
keywords = {cloud computing, pricing, resource provisioning, application design},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980441,
author = {Tiwari, Harshit and Vajpayee, Alok and Singh, Ajit},
title = {A Bandwidth Aware Weight Based DSR Protocol for Mobile Ad Hoc Networks},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980441},
doi = {10.1145/1980422.1980441},
abstract = {Ad hoc network is a collection of nodes which do not require centralized administration and pre existing infrastructure. Each node can work like host as well as router. Energy efficient routing is the prime concern in mobile ad hoc networks. Various protocols and algorithms have been proposed for routing in ad hoc networks. In this paper we propose a bandwidth aware weight based DSR (BAWB-DSR) protocol which is an improvement of existing DSR and WBDSR protocol.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {19},
numpages = {4},
keywords = {energy efficient routing, mobile ad hoc network, stability, bandwidth, energy},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980442,
author = {Chitra, K. and Padmavathi, G.},
title = {FAutoREDwithRED: An Algorithm to Reduce Queue Oscillation in Internet Routers},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980442},
doi = {10.1145/1980422.1980442},
abstract = {Routers in Internet face the problem of congestion due to the increased use of real-time application in Internet. The use of real-time applications results in bursty traffic leading to congestion. Congested routers causes many problems such as large delay, unsteady queue size, unfairness among flows, low link utilization and high packet drop. Active Queue Management is a solution to the problem of congestion in the Internet routers. AQM solves these problems of congestion based on congestion metrics like queue length and input rate. In this paper Queue-based AQM is considered with flow information to improve the queue oscillation and fairness of the existing Queue-based algorithms. This proposed scheme aims to provide good performance under unresponsive load and offers a good QOS to all users.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {20},
numpages = {4},
keywords = {queue oscillation, congestion, misbehaving flows, fairness, drop probability},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980443,
author = {Kannan, Rajkumar and Bielikova, Maria and Andres, Frederic and Balasundaram, S. R.},
title = {Understanding Honest Feedbacks and Opinions in Academic Environments},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980443},
doi = {10.1145/1980422.1980443},
abstract = {Student evaluation is an important component of any higher education. Online student feedbacks and opinions have become increasingly popular means of gathering reviews and judging the quality of various services offered by an institution. This paper focuses on studying student behaviour while reporting their feedbacks. Particularly, we investigate the reliability of quantitative features through numerical ratings that students offer, by estimating the linguistic evidence from the free text that accompany the feature space. Our hypotheses is that higher the evidence of a feature in the free text, higher the quantitative numerical ratings. This study has been validated from students' feedbacks and opinions of our institution as well.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {21},
numpages = {4},
keywords = {feedback and opinion mining, online academic feedbacks, quality metrics},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980444,
author = {Sharma, Mohak and Reddy, P. Krishna},
title = {Using Lower-Bound Similarity to Enhance the Performance of Recommender Systems},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980444},
doi = {10.1145/1980422.1980444},
abstract = {Recommender systems employ the popular K-nearest neighbor collaborative filtering (K-CF) methodology and its variations for recommending the products. In K-CF approach, recommendation for a given user is computed based on the ratings of K-nearest neighbors. In K-CF approach, it can be noted that, the system identifies K neighbors for each user irrespective of the number of products he/she has rated. As a result, the user who have rated few products may get the less-similar neighbors and the user who have rated more products may miss the genuine neighbors. In the literature, the notion of lower-bound similarity has been proposed to improve the clustering performance in which the clusters are extracted by fixing the similarity threshold. In this paper, we have extended the notion of lower-bound similarity to recommender systems to improve the performance of K-CF approach. In the proposed approach, instead of fixing K for finding the neighborhood, the similarity threshold value is fixed to extract the neighbors for each user. As a result, each user gets appropriate number of neighbors based on the number of products rated by him/her in a dynamic manner. The experimental results, on MovieLens dataset, show that the proposed lower bound similarity CF approach improves the performance of recommender systems over K-CF approach.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {22},
numpages = {4},
keywords = {collaborative filtering, recommender systems, k-nearest neighbor, lower-bound similarity},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980445,
author = {Rathore, Abhishek and Bohara, Atul and Prashil, R. Gupta and Prashanth, T. S. Lakshmi and Srivastava, Praveen Ranjan},
title = {Application of Genetic Algorithm and Tabu Search in Software Testing},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980445},
doi = {10.1145/1980422.1980445},
abstract = {This paper presents a technique for automatic test-data generation in software testing. The proposed approach is based on genetic and tabu search algorithms. It combines the strength of two metaheuristic techniques and produces efficient results. The conventional approach for test-data generation using genetic algorithm is modified by applying a tabu search heuristic in mutation step. It also incorporates backtracking process to move search away from local optima. The experimental results show that the algorithm is effective in providing test data and its performance is better than simple genetic algorithm.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {23},
numpages = {4},
keywords = {structural testing, test-data generation, genetic algorithm, software testing, tabu search, control dependence graph},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980446,
author = {Chowattanakul, Wivorn and Rai, Harikrishna G. N. and Krishna, P. Radha},
title = {An Efficient Shape Based Feature for Retrieval of Healthcare Literatures Using CBIR Technique},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980446},
doi = {10.1145/1980422.1980446},
abstract = {Recent advances in healthcare such as Evidence Based Medicine (EBM) and Clinical Decision Support Systems (CDSS) requires practitioners to frequently access archived historical healthcare literatures and images. As the majority of healthcare literatures contain images such as medical images, clip arts, waveforms, flow charts and block diagrams, in this paper we present the use of Content Based Image Retrieval (CBIR) for efficient healthcare literature search and retrieval. We introduce a novel shape based feature called Fourier Edge Orientation Autocorrelogram (FEOAC) for search and retrieval of healthcare literatures. Scale and translation invariant Edge Orientation Autocorrelogram (EOAC) feature is made rotation invariant by applying Fourier transform. This Fourier based shape feature also reduces the feature set dimension enabling faster retrieval of document images in large databases. Experimental results show that FEOAC outperforms EOAC for search and retrieval of healthcare document images, with improved precision and recall rates.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {24},
numpages = {4},
keywords = {feature extraction, edge orientation autocorrelogram, Fourier transform, shape feature, CBIR},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980447,
author = {Elias, Susan and Gokul, Vanaja and Krithivasan, Kamala and Gheorghe, Marian and Zhang, Gexiang},
title = {Real Time Cross Layer Design Using Particle Swarm Optimization},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980447},
doi = {10.1145/1980422.1980447},
abstract = {Cross Layer Optimization (CLO) strategies are currently being incorporated in network operating system for efficient utilization of resources to enable effective information management. In wireless adhoc networks real time optimizations need to be performed and hence CLO strategies that have faster response time are required. In this paper we propose a Cross Layer Optimization strategy that uses a variant of the Particle Swarm Optimization (PSO) for real time cross layer design of the network. The variant of the PSO used in this research work uses digital pheromones for improved performance. The proposed PSO-CLO strategy can be used for delay sensitive, bandwidth intensive and loss-tolerant wireless multimedia transmissions that have an ever demanding need for better Quality of Service. Our experimental results show that the proposed PSO-CLO strategy has significantly faster response time in comparison with the classical CLO solutions.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {25},
numpages = {4},
keywords = {quality of service, digital pheromones, particle swarm optimization, cross layer optimization},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980448,
author = {Venkateshprasanna, H. M. and Gandhi, Rujuswami D. and Mahesh, Kavi and Suresh, J. K.},
title = {Enterprise Search through Automatic Synthesis of Tag Clouds},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980448},
doi = {10.1145/1980422.1980448},
abstract = {In a large organization, finding the right expert who can provide knowledge needed in a particular work context is critical to achieving higher levels of quality and productivity especially in the present scenario of a rapidly evolving technology and business environment where knowledge plays a vital role in value creation. Previous approaches to the problem of reliably finding experts in an area of knowledge focus on mapping employees to their knowledge based primarily on voluntary data inputs by users and minimally supplemented by data derived from enterprise systems. However, the generation of employee profiles through inputs is usually not based on a controlled vocabulary and therefore results in poor precision and recall in expertise search in addition to posing difficulties in integration with content search. In this paper, a novel approach that addresses some of these limitations is described wherein tag clouds for members are automatically generated from enterprise data. The paper also outlines the use of such tag clouds in an integrated search system that aims to provide an optimal set of experts as well as relevant content corresponding to a knowledge need.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {26},
numpages = {4},
keywords = {knowledge search, professional networking, expertise locator, knowledge management, tag cloud, enterprise search},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980449,
author = {Girisha, R. and Murali, S.},
title = {Novel Micro Aerial Vehicle Video Segmentation Algorithm},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980449},
doi = {10.1145/1980422.1980449},
abstract = {A camera mounted on a MICro Aerial Vehicle (MICAV) provides an excellent means to monitor large areas of a scene. In this paper, we present a novel approach for detecting motion regions in video sequence observed by a moving MICAV. Foreground object segmentation is done in four levels. In the first level, we use Pearson correlation coefficient (rab) to segment foreground from background. In the second level, we apply, self shadow removal algorithm based on Difference in Mean (Z) method. And in third and fourth stage, any remaining background spurious noise is removed concurrently, first calculating global histogram and then using local histogram based methods using HSI color space. The experiment is conducted on MICAV video and result shows the effectiveness of the proposed technique.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {27},
numpages = {4},
keywords = {MICAV, histogram, difference in mean, HIS, Pearson correlation},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980450,
author = {Chaturvedi, Setu K. and Swami, D. K. and Singh, Gulab},
title = {Dirichlet Distribution with Centroid Model (DDCM) Based Summarization Technique for Web Document Classification},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980450},
doi = {10.1145/1980422.1980450},
abstract = {Web document summarization deals with computing a summary for a set of related articles such that they give the user a general view about the events. One of the summarization objectives is that the sentences should cover the different events in the documents with the information covered in as few sentences as possible. Dirichlet Distribution Model can break down these documents into different sentence or events. However to reduce the common information content the sentences of the summary need to be orthogonal to each other since orthogonal vectors have the lowest possible similarity and correlation between them. Centroid Value Decomposition is used to get the orthogonal representations of vectors and representing sentences as vectors, we can get the sentences that are orthogonal in our proposed DDCM. Thus using DDM we get the different sentence in the document and using Centroid Model we find the words that best represent these sentences. The goal of this paper is to find minimum number of highly qualitative features by generating best summarization for web document classification. We conducted experiments with various Centroid based numbers of summarization approaches and obtain effective classification results. Experimental results show that our proposed DDCM summarization based classification approach achieved more accurate and improved result as compared to full text based classification.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {28},
numpages = {4},
keywords = {retrieve verification, course, class, DDM, centroid, sum, classification, frequency, web, relevant, document, student, similarity, average, summarization, normalized sum},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980451,
author = {Chandra, E. and Ajitha, P.},
title = {PCA for Heterogeneous Data Sets in a Distributed Data Mining},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980451},
doi = {10.1145/1980422.1980451},
abstract = {Principal Component Analysis(PCA) is bastion for distributed data analysis. Scalability of the data is limited when centralized data mining is taken into account. Unsupervised classification like clustering and supervised classification like other techniques needs dimensionality reduction as a major part. PCA serves as a base for reducing the dimensionality of data and communication bandwidth. Considering all these issues data mining may be pricier in comparing the distributed aspects. This paper deals with the algorithmic aspects of both Homogeneous and Heterogeneous databases in distributed data mining. Tediousness arose when the heterogeneous data bases are need to be integrated where there is chances of error handling is high. This paper proposes a new algorithm to deal with heterogeneous data and error components is also taken as a part of the algorithm.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {29},
numpages = {4},
keywords = {heterogeneous databases, distributed data mining, PCA, homogeneous databases},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980452,
author = {Patil, Nagamma and Toshniwal, Durga and Garg, Kumkum},
title = {Species Identification Based on Approximate Matching},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980452},
doi = {10.1145/1980422.1980452},
abstract = {Genomic data mining and knowledge extraction is an important problem in bioinformatics. Existing methods for species identification are based on n-grams. In this paper, we propose a novel approach for identification of species. Given a database of genomic sequences, our proposed work includes extraction of all candidate/subsequences that satisfy: length grater or equal to given minimum length, given number of mismatches and support grater or equal to user threshold. These patterns are used as features for classifier. Classification of genome sequences has been done by using data mining techniques namely, Naive Bayes, support vector machine and nearest neighbor. Individual classifier accuracies are reported. We also show the effect of sampling size on the classification accuracy and it was observed that classification accuracy increases with sampling size. Genome data of two species namely E. coli and Yeast are used to verify proposed method.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {30},
numpages = {4},
keywords = {data mining, soft computing, bioinformatics, approximate pattern matching, genome data, exact matching},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980453,
author = {Banday, M. Tariq and Shah, Nisar A.},
title = {Challenges of CAPTCHA in the Accessibility of Indian Regional Websites},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980453},
doi = {10.1145/1980422.1980453},
abstract = {This paper reviews Indian Government guidelines regarding the inclusion of regional content in government websites, compares them to World Wide Web Consortium's (W3C) guidelines for multilingual version and use of CAPTCHA tests and analyzes some Indian Government websites in light of these guidelines. It also analyzes some other non-Indian government websites in terms of multilingual content and use of CAPTCHA tests. It further reports results obtained through several user studies and experiments regarding the use of English and regional CAPTCHA tests on non-English pages of multilingual websites in terms of accuracy and response time.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {31},
numpages = {4},
keywords = {human interactive proof, web accessibility, HIP, CAPTCHA, Indian languages, regional websites},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980454,
author = {Guru, D. S. and Mallikarjuna, P. B. and Manjunath, S.},
title = {Segmentation and Classification of Tobacco Seedling Diseases},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980454},
doi = {10.1145/1980422.1980454},
abstract = {In this paper, we present a novel algorithm for extracting lesion area and application of neural network to classify seedling diseases such as anthracnose and frog-eye spots on tobacco leaves. The lesion areas with anthracnose and frog-eye spots on a leaf of tobacco seedlings are segmented by contrast stretching transformation with an adjustable parameter and morphological operations. First order statistical texture features are extracted from lesion area to detect and diagnose the disease type. These texture features are then used for classification purpose. A Probabilistic Neural Network (PNN) is employed to classify anthracnose and frog-eye spots present on tobacco seedling leaves. In order to corroborate the efficacy of the proposed model we have conducted an experimentation on a dataset of 800 extracted areas of tobacco seedling leaves which are captured in an uncontrolled lighting conditions. The methodology presented herein effectively detected and classified the tobacco seedlings lesions upto an accuracy of 88.5933%. Further the recommended features are compared with Gray Level Co-occurrence Matrix (GLCM) based features to bring out their superiorities.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {32},
numpages = {5},
keywords = {probabilistic neural network, first order statistical texture features, tobacco seedling diseases, lesion area extraction},
location = {Bangalore, India},
series = {COMPUTE '11}
}

@inproceedings{10.1145/1980422.1980455,
author = {Mishra, Anirvana and Singh, Gaurav and Bahl, Akshay},
title = {A Novel Approach to Keyphrase Extraction Using Augmented Transition Networks and Statistical Tools},
year = {2011},
isbn = {9781450307505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1980422.1980455},
doi = {10.1145/1980422.1980455},
abstract = {We present a novel approach to extract keyphrases based on Augmented Transition Networks (abbreviated as ATNs) followed by statistical methods from any given article, notes on a particular subject, or any other document source. The use of ATNs has completely ruled out the need of background corpora in identifying the potential keywords and keyphrases. Moreover, the use of ATNs has greatly reduced the search space for the statistical methods. We have devised two new methods namely, relaxed statistical analysis and stringent statistical analysis to identify the separability of phrases into sub phrases. In this paper, the two tier process is discussed in detail and illustrated with examples. We have also discussed the applications of this process briefly.},
booktitle = {Proceedings of the Fourth Annual ACM Bangalore Conference},
articleno = {33},
numpages = {4},
keywords = {statistical analysis, keyphrase mining, word collocation, augmented transition networks, text mining, keyphrase extraction},
location = {Bangalore, India},
series = {COMPUTE '11}
}

