@inproceedings{10.1145/2522548.2522598,
author = {Ganesan, Rajeshwari and Murarka, Yogesh and Sarkar, Santonu and Frey, Kristoffer},
title = {Empirical Study of Performance Benefits of Hardware Assisted Virtualization},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522598},
doi = {10.1145/2522548.2522598},
abstract = {An application's performance can suffer from significant computational overheads when it is moved from a native to a virtualized environment. Adoption of virtualization without understanding such overheads in detail can dramatically impact the overall performance of hosted applications. The rapid adoption of virtualization has fueled the development of new hardware technologies, which promise to optimize the performance and scalability of processor and network I/O virtualization. However, no comprehensive empirical study of the effectiveness of these hardware assistance technologies is publicly available. In this paper we focus on x86 architectures and study empirically the performance improvements introduced by Intel's VT and PCI-SIG's SR-IOV on a Xen-based hypervisor. Using a range of benchmark programs, we compare benchmark scores and resource utilization between native and virtual environments for two different testbeds, one with hardware assistance and one without. The results indicate that hardware assistance indeed eliminates most overheads, especially those relating to network I/O, but non-negligible CPU overheads still remain. Also, there is no hardware technology with specifically deals with disk I/O virtualization, and significant overheads do arise in workloads requiring intensive disk usage.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {1},
numpages = {8},
keywords = {SR-IOV, performance overhead, benchmarking, virtualization, hardware assistance},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522599,
author = {Garg, Richa and Veerubhotla, Ravi Sankar and Saxena, Ashutosh},
title = {<i>AtDRM</i>: A DRM Architecture with Rights Transfer and Revocation Capability},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522599},
doi = {10.1145/2522548.2522599},
abstract = {This work presents a new Digital Rights Management (DRM) architecture to address rights transfer and revocation aspects of protected content using a threshold Attribute-Based Encryption scheme. We considered two scenarios for rights transfer, here. In first case, a legitimate user needs to consume the protected content on multiple devices, while in second case the user wants to transfer the rights on the content to another user. These scenarios are significant in today's context with the proliferation of smart gadgets and shared cloud services. The architecture is flexible, offers individual or group level access to the protected content and does not require any pre-binding of the devices for consumption. The design inherits this feature from ABE scheme itself and therefore, it is possible to enforce controlled access for individuals as well as groups. Simultaneously, the architecture presents an efficient revocation scheme that eliminates the need of issuing new keys to users or re-encrypting the existing content, either for rights transfer or access revocation. Further, no specific revocation list is to be maintained for revoked users.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {2},
numpages = {6},
keywords = {digital rights management, revocation, rights transfer, group access controls, digital content security, attribute-based encryption},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522600,
author = {Agnihotram, Gopichand and Koduvely, Hari Manassery},
title = {A Comparative Study of Monte Carlo Methods to Compute Rare Event Probabilities of Failure in Reliability Models},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522600},
doi = {10.1145/2522548.2522600},
abstract = {This paper presents a comparative study of the performance of different Monte Carlo Simulation methods in the computation of rare event probabilities in Reliability Theory. We evaluate the performance of 4 well known Markov Chain Monte Carlo methods (MCMC), namely Metropolis-Hasting (MH), Hamiltonian or Hybrid Monte Carlo (HYBRID), Delayed Rejection and Adaptive Metropolis (DRAM), and Differential Evolution Adaptive Metropolis (DREAM), for computing the Probability of Failure using the Reliability Theory framework. We also compared the results of simulations with an approximate analytical method called First Order Reliability Method (FORM). The study shows that while both HYBRID and DREAM produce more accurate results, contrary to intuition, HYBRID method was very slow in performance.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {3},
numpages = {6},
keywords = {limit state equation, probability of failure, Markov chain Monte Carlo methods (MCMC), differential evaluation and adaptive metropolis (DREAM), Hamiltonian or hybrid Monte Carlo (HYBRID) method, metropolis-hasting (MH) method, delayed rejection and adaptive metropolis (DRAM), reliability models},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522748,
author = {Abadhan, Sabyasachi and De, Sohini and De, Suddhasil},
title = {Uncoupling of Mobile Cloud Computing Architecture Using Tuple Space: Modeling and Reasoning},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522748},
doi = {10.1145/2522548.2522748},
abstract = {Mobile Cloud Computing architecture provides the services of cloud computing to the mobile applications executing in users' mobile/portable devices. By facilitating these applications with the cloud services, it helps to overcome the inherent limitations of mobile/portable devices that are faced by their mobile applications. However, the cloud services in existing architecture become tightly coupled with the mobile applications while delivering the service, which is highly undesirable in the dynamic and unreliable mobile cloud computing paradigm. In this paper, a new mobile cloud computing architecture is proposed, where the mobile applications remain uncoupled from the leased cloud services during the service delivery. In this architecture, the tuple space model is used for uncoupling these interactions. The proposed approach of uncoupling improves the flexibility and efficiency of mobile cloud computing. This paper also suggests an approach for formalizing and reasoning of the proposed architecture, in order to properly validate its flexibility and efficiency while uncoupling the service access and delivery to the mobile applications. The formalization is carried out using Mobile UNITY.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {4},
numpages = {6},
keywords = {uncoupling, mobile UNITY, formalization, tuple space, cloud computing, mobile cloud computing},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522602,
author = {Saxena, Ashutosh and Jain, Ina and Gorantla, M. Choudary},
title = {An Integrated Framework for Enhancing Privacy in Online Social Networks},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522602},
doi = {10.1145/2522548.2522602},
abstract = {In recent times online social networks (OSN) have become one of the most widely used Internet applications with a very large number of users. In an OSN, the users exchange information via text, pictures and videos with their peers/friends for their personal or business interests. Since the information that flows across an OSN is generally of personal in nature, it is important that the users get assurance regarding their information is kept private to a level that they desire. In this paper, we analyze the potential problems with centralized OSNs and Distributed OSNs and then suggest an integrated framework which combines the advantages of these two frameworks and at the same time eliminates their drawbacks. The proposal is to have a framework with the familiar interface and applications that of the centralized OSNs and access control over the personal data like that of Distributed OSN.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {5},
numpages = {6},
keywords = {distributed online social network, access control, privacy, attribute based encryption},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522603,
author = {Datta, Subhajit and Sarkar, Santonu and Sajeev, A. S. M. and Kumar, Nishant},
title = {How Many Researchers Does It Take to Make Impact? Mining Software Engineering Publication Data for Collaboration Insights},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522603},
doi = {10.1145/2522548.2522603},
abstract = {In the three and half decades since the inception of organized research publication in software engineering, the discipline has gained a significant maturity. This journey to maturity has been guided by the synergy of ideas, individuals and interactions. In this journey software engineering has evolved into an increasingly empirical discipline. Empirical sciences involve significant collaboration, leading to large teams working on research problems. In this paper we analyze a corpus of 19,000+ papers, written by 21,000+ authors from 16 publication venues between 1975 to 2010, to understand what is the ideal team size that has produced maximum impact in software engineering research, and whether researchers in software engineering have maintained the same co-authorship relations over long periods of time as a means of achieving research impact.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {6},
numpages = {8},
keywords = {annova, T test, benchmarking, virtualization, software engineering research, topic analysis, DBLP, collaboration},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522604,
author = {Kurian, Devasia and Chelliah, Pethuru Raj},
title = {Upgradation of Business Applications with Autonomic Computing},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522604},
doi = {10.1145/2522548.2522604},
abstract = {Autonomic computing has come a long way since its inception a decade ago and has been positioned as a venerable and value-adding technology for producing and sustaining self-managing, real-time, and resilient systems for the future. A series of concerted efforts by multiple IT companies and academic research laboratories across the world have brought in a number of advancements in this discipline with vigorous study and research. A variety of proven and potential mathematical and computational concepts have been selected and synchronized to arrive at noteworthy improvements in the autonomic systems design, development, deployment, and delivery methods. Having understood the unique value-proposition and the significant achievements in the autonomic computing space, business executives and IT experts are consciously embracing the autonomic idea, which is very generic to be easily embedded in any kind of business and IT systems. However, the penetration of this technology into both IT and business applications has not been as originally envisaged by its creators due to various reasons.The business environment is still filled and saturated with large-scale packaged and monolithic applications. If the autonomic capabilities are innately squeezed into business and IT applications, then there can be major differentiators in how those applications function in seamlessly and spontaneously automating business operations. Both, existing as well as emerging applications can be targeted to become autonomic in their operations, outputs, and outlooks. In this paper, we have described how the leading enterprise packages (ERP, CRM, SCM, and so on.) can be enabled to be adaptive, highly available, secure, and scalable in their actions and reactions. The well-known enterprise applications such as CRM, Online Retail, and Marketing with focus on self-optimization characteristics are described here. A detailed analysis of a Discount Manager in an online retail scenario is also explained. The simulation results obtained clearly show how embedded autonomic capability is very close to human thinking and decision-making ability.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {7},
numpages = {7},
keywords = {CRM, ERP, autonomic computing, service oriented architecture, e-commerce, multi-agent architecture},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522605,
author = {Kumar, Sathiya Prabhu and Chiky, Raja and Lefebvre, Sylvain and Soudan, Eric Gressier},
title = {LibRe: A Consistency Protocol for Modern Storage Systems},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522605},
doi = {10.1145/2522548.2522605},
abstract = {The dramatic increase of data stored, processed and reused in all sectors leads to the evolution of modern storage systems known as NoSQL databases such as Amazon Dynamo, Cassandra, Big-Table, PNUTS, HBase. In order to boost the availability and performance of the system, these storage systems follow eventual consistency and don't offer tight consistency by default. Paxos is commonly used in this context to ensure tight consistency on demand. But it adds extra costs on messages management, mostly complexity and size. In addition, Paxos shrinks the space for other research areas such as cache memory optimization and load-balancing.This paper gives an opportunity to propose and discuss the challenges of a new consistency protocol for modern storage systems entitled 'LibRe'. LibRe follows the Eventual Consistency model. In addition, it logs operations executed on each node in the distributed system. This additional information is used by the load balancer and ensures that requests are not forwarded to a node where the data needed to serve the request are stale. Since Eventual Consistency already offers better Availability and Partition tolerance, the aspiration of associating LibRe with eventual consistency is to work out a better consistency management service providing also availability and partition tolerance. The simulation results for consistency and latency in LibRe are compared among traditional Pessimistic Consistency, Eventual Consistency, and Paxos. The overall results are discussed and new opportunities for research works are provided.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {8},
numpages = {9},
keywords = {eventual consistency, storage systems, CAP, consistency},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522606,
author = {Kumar, Naveen and Mathuria, Anish and Das, Manik Lal and Matsuura, Kanta},
title = {Improving Security and Efficiency of Time-Bound Access to Outsourced Data},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522606},
doi = {10.1145/2522548.2522606},
abstract = {In time-bound access control, access to the system resources by authorized users is limited to specific time periods. In 2012, Vimercati, Foresti, Jajodia and Livraga proposed a scheme for time-bound access control to outsourced data in cloud using hierarchical key derivation structure. We show that their scheme has a security flaw. A user, after access right revocation, can still have access to the resources associated to his revoked subscription interval. There is a scheme by Wang, Li, Owens and Bhargava for efficient revocation in data outsourcing scenario. The main advantage of their scheme is that it does not require data block re-encryption and updates when any user's access right changes. It has a disadvantage that any change in access right of a single user, requires the data owner to recompute and distribute access certificates to all the users who requires further data access. In order to mitigate the security flaw of Vimercati et al. scheme, we present a solution based on the data access mechanism proposed by Wang et al. such that any user's access right revocation will be independent of other user's data access. Our solution removes their drawback without sacrificing other desirable properties of the original scheme.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {9},
numpages = {8},
keywords = {key management, data outsourcing, access control},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2522607,
author = {Justin, Jithin and Beegom, A. S. Ajeena},
title = {Nearest Neighbour Based Social Recommendation Using Heat Diffusion},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2522607},
doi = {10.1145/2522548.2522607},
abstract = {Growth of the internet has lead to information overload. Recommender systems filter this vast amount of information and outputs useful information. They are traditionally based on users rating of items. But those systems were not useful for coldstart users i.e. users who have made very few purchases. So social recommenders began to evolve which makes use of social networks to improve recommendations. In this paper we propose a nearest neighbour based top N recommendation technique using social network. We model the data sources as graph and use heat diffusion process for generating recommendations. The experimental evaluation on the epinions dataset shows that our approach outperforms the approach that combines user based collaborative filtering approach and trust based approach.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {10},
numpages = {7},
keywords = {heat diffusion, recommendation, top N, trust, collaborative filtering},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523128,
author = {Parekh, Deep and Chaudhary, Sanjay},
title = {Durable Data Storage in Distributed Non Persistent Caching Environment},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523128},
doi = {10.1145/2522548.2523128},
abstract = {To achieve scalability is one of the major issues in dynamic web applications. One of the ways is to use proper caching mechanism. Memcached is a high performance <key, value=""> distributed cache. High frequently used data and costly data (cost is with respect to time to decompress) is stored in memcached server node. Data is not persistent in memcached server node. In distributed non persistence caching environment, durability of high frequently used data and costly data should be high in order to achieve high availability. Frequency of data being accessed, time to decompress the data and number of write operations in distributed server nodes are the key characteristics that needs to be considered in order to decide the durability value of the data. MemcacheS server node acts as a central entity. It creates replication of data in distributed memcached server nodes based on value of durability. In data read operation, memcacheS server node provides information to client node about memcached server nodes which have data. After failure of any memcached server node, value of stored data could be retrieved from other memcached server node with durability of data more than 1. The results indicate that, availability of data is increased, as it can be fetched from other memcached server nodes too. Durable data storage provides high availability of the data in non persistence caching environment.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {11},
numpages = {7},
keywords = {availability, software as a service, durability, cloud computing},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}</key,>

@inproceedings{10.1145/2522548.2523129,
author = {Kumar, Saurabh and Kulkarni, Mayank},
title = {Graph Based Techniques for User Personalization of News Streams},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523129},
doi = {10.1145/2522548.2523129},
abstract = {In this paper, we address the problem of user personalization and recommendation of news streams. This involves 'learning' from past user behaviour, such as the articles she read or did not read and accurately predicting new articles which she would be most likely to read. Our contribution in this paper is the development of a new algorithm for news personalization using an adaptation of the classical nearest neighbour algorithm coupled with a knowledge graph which we create. This algorithm provides a powerful tool for user behaviour analysis as we demonstrate in subsequent sections. Using implicit user data like the articles that were read as well as the articles that weren't along with their position and distance in the graph, we rank new articles on the basis of the predicted interest of the user in the content of that article.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {12},
numpages = {7},
keywords = {user personalization, user recommendation},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523130,
author = {Roy, Suman and Sawant, Kiran Prakash and Charvin, Olivier Maurice},
title = {Generating Topic Maps from XML/XSD Documents},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523130},
doi = {10.1145/2522548.2523130},
abstract = {Over the years, XML has become a prominent standard for data exchange on the Web. Although XML can support syntactic inter-operability, problems arise when data sources represented as XML documents are needed to be integrated. The reason is that XML does not support sharing of concepts. This issues can be overcome by having suitable ontology representations of XML documents. The Semantic Web languages can play an important role here as they support ontology representation. Topic Map is a Semantic Web formalism (an ISO standard) for knowledge representation and interchange with an emphasis on navigation and retrieval of information. In this work we propose a framework for automatically generating Topic Map ontology from XML. We make use of XQuery and Tolog to validate our method. We illustrate our technique on examples. We also provide performance measures of our approach on some standard data sets.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {13},
numpages = {8},
keywords = {XML, tolog, XQuery, topic maps, semantic technologies, translation, structured-unstructured information integration},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523131,
author = {Asadullah, Allahbaksh M. and Jain, Nikita and Kapoor, Kanika and Falih, Hajar},
title = {A Data-Centric Heuristic for Hadoop Provisioning in the Cloud},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523131},
doi = {10.1145/2522548.2523131},
abstract = {Research agencies and organizations work on algorithms and techniques to reduce Operational and Capital Expenditure. They move to Cloud to transform the Capital Expenditure (Capex) to Operational Expenditure (Opex). They use cloud to crunch large amount of commercial and social data. This paper proposes a heuristic approach to reduce the operational cost of virtual machines (VMs) running Hadoop. The heuristic is simple and effective, it scales the number of Hadoop nodes based on the type and size of the job submitted. We validate our heuristic with Hadoop word-count example on different data samples. Our implementation is independent of the cloud provider. Hence, the heuristic is applicable to both private and public cloud.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {14},
numpages = {6},
keywords = {distributed systems, heuristic, Hadoop, provision, software design, map reduce, cloud, operational cost},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523132,
author = {Patel, Hasmukh and Jinwala, Devesh C.},
title = {Modeling and Analysis of Internet Key Exchange Protocolv2 and a Proposal for Its Variant},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523132},
doi = {10.1145/2522548.2523132},
abstract = {With the growing concerns on the information security and the capabilities of an attacker, Denial of Service (DoS) attacks have become one of the most serious and potent form of network security attacks. The attackers target the protocols to exhaust resources by either disabling the servers or disrupting the protocol exchange. In fact, almost all the communication protocols including TCP, HIP, etc. are vulnerable to resource exhaustion attacks. One of the ways of dealing with the DoS attacks is to carry out a cost analysis of the impact of the DoS attacks and ensure that the impact of the same is minimized to the extent possible. We illustrate the applicability of the same to analyze the Internet Key Exchange (IKE) version2 protocol. IKEv2 is one of the significant protocol of IPSec suite. It establishes the security association between source and destination before IPSec connection establishes. In this paper, we analyze IKEv2 protocol to show that IKE v2 is susceptible to DoS attacks.To the best of our knowledge, although IKE v1 has been analyzed for susceptibility to the DoS attacks, IKE v2 has not yet been. We also propose a new variant of IKE v2 protocol using model given by Stebila and et al. to ensure mitigation of DoS attacks in IKEv2.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {15},
numpages = {6},
keywords = {DoS attacks, Meadows' cost-based framework, internet key exchange protocol version2},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523133,
author = {Narang, Pratik and Reddy, Jagan Mohan and Hota, Chittaranjan},
title = {Feature Selection for Detection of Peer-to-Peer Botnet Traffic},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523133},
doi = {10.1145/2522548.2523133},
abstract = {The use of anomaly-based classification of intrusions has increased significantly for Intrusion Detection Systems. Large number of training data samples and a good 'feature set' are two primary requirements to build effective classification models with machine learning algorithms. Since the amount of data available for malicious traffic will often be small compared to the available traces of benign traffic, extraction of 'good' features which enable detection of malicious traffic is a challenging area of work.This research work presents preliminary results of comparison of performance of three different feature selection algorithms - Correlation based feature selection, Consistency based subset evaluation and Principal component analysis-on three different Machine learning techniques- namely Decision trees, Na\"{\i}ve Bayes classifier, and Bayesian Network classifier. These algorithms are evaluated for the detection of Peer-to-Peer (P2P) based botnet traffic.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {16},
numpages = {9},
keywords = {classification, feature selection, machine learning, peer-to-peer, botnet},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523134,
author = {Sharma, Divya and Alam, Akz and Dasgupta, Parthasarathi and Saha, Debashis},
title = {A Ranking Algorithm for Online Social Network Search},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523134},
doi = {10.1145/2522548.2523134},
abstract = {Online Social Networks have become the new arena for people to stay in touch, pursue their interests and collaborate. In October 2012, Facebook reported a whopping 1 billion users which is testimony of the fact how online social networks have proliferated and made inroads into the real world. Some of the obvious advantages of social networks are (1) 24X7 availability allowing users to stay in virtual touch at any time of the day, (2) get in touch with people who have similar interests and collaborate with them and (3) the ability to be able to search for users to add to one's friend circle. The third advantage is the focus of this paper. With the increasing number of users on online social networks, it is important that when a user searches for another user, appropriate results are returned. This paper identifies three criteria -- proximity, similarity and interaction, which can be used to rank search results so that more appropriate results are presented to the searching user. Also, this algorithm allows the search ranking to be customized according to the nature of the online social network in question.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {17},
numpages = {7},
keywords = {online social network, algorithm, search ranking},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523135,
author = {Nirmala, S. Jaya and Maulik, Shah and Bhanu, S. Mary Saira},
title = {SLA Achievement by Negotiation in a Cloud},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523135},
doi = {10.1145/2522548.2523135},
abstract = {The performance of a computing cloud depends on its agile response to the user needs and in the quality of the service provided. SLA assurance will increase user's confidence in the system by guaranteeing the cloud resources needed to users at certain point in the future. Negotiation between the cloud provider and consumer is necessary to achieve SLA. Haizea is an open source lease manager which can act as a scheduler for OpenNebula. It uses resource leases as resource allocation abstraction and executes these leases by allocating virtual machines. Haizea does not support negotiation currently. This work implements negotiation process using Haizea to provide SLA based lease admission. The experiment results show that the proposed method increases system utilization and maximizes the number of leases accepted as compared to Haizea's default policies.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {18},
numpages = {4},
keywords = {cloud, advance reservation, SLA, leases, Haizea, OpenNebula},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523136,
author = {Manoharan, Prabukumar and Ray, Bimal Kumar},
title = {An Alternate Line Drawing Algorithm on Hexagonal Grid},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523136},
doi = {10.1145/2522548.2523136},
abstract = {In this paper an algorithm to scan convert a line using method of deviation on hexagonal grid is proposed. The method of deviation computes pixel nearest to analog line using only integer arithmetic. The proposed algorithm is compared favorably with the latest line drawing algorithm on a hexagonal grid. The main advantage of the proposed approach is that the same may be applied to an arbitrary curve. Owing to this approach we may visualize design ideas through animation and photorealistic renderings, and simulate how a design will perform in the real world in CAD tool with less aliasing artifact.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {19},
numpages = {7},
keywords = {line drawing, method of deviation, hexagonal grid, CAD},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523137,
author = {Bhatia, Munish and Kiran, D. C. and Misra, J. P. and Gurunarayanan, S.},
title = {Fine Grain Thread Scheduling on Multicore Processors: Cores with Multiple Functional Units},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523137},
doi = {10.1145/2522548.2523137},
abstract = {The proposed work discusses a global scheduling technique for multicore processors with specific focus on processor cores having multiple functional units. The design philosophy of the multicore architecture is to accommodate more cores with more execution capabilities on a chip by reducing other complex and redundant circuits. Due to the simplicity of hardware on the chip of multicore processor, the onus of detecting and exploiting the instruction level parallelism (ILP) in the program lies on the complier. Following work proposes a scheduling technique which is used to schedule the instructions onto multiple cores on chip each having multiple functional units. The goal is achieved by dissecting each basic block of the program's control flow graph (CFG) into sub-divisions called sub-blocks. These sub-blocks are then analyzed for the break-up of instructions on the basis of instruction type (Integer or Floating Point) and then they are scheduled onto different cores while trying to get a balanced trade-off between communication costs amongst the cores. The scheduler provides enough or approximately equal number of integer and floating point instructions to each core which may be executed in parallel on the core's multiple functional units (integer unit and floating point units), thus taking advantage of the core's architecture.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {20},
numpages = {6},
keywords = {multicore, static single assignment (SSA), instruction level parallelism, control flow graph},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523138,
author = {Sinha, Tanmay and Srikanth, Vrns and Sain, Mangal and Lee, Hoon Jae},
title = {Trends and Research Directions for Privacy Preserving Approaches on the Cloud},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523138},
doi = {10.1145/2522548.2523138},
abstract = {With advancements in hardware and software capabilities, cloud computing has evolved into a widely utilized paradigm for pay per use and robust computation. An unparalleled repository of user sensitive data that resides on the cloud poses severe threat to the privacy of individuals. Users authenticate, store and perform computations on their data using cloud services. From the cloud's perspective, it gathers additional user data via ubiquitous devices, mines this information to offer personalized services like recommendations and disseminates the results. However, the interactions between the cloud and user at each stage of this pipeline development is limited by privacy concerns. In recent years, much work has been done on designing privacy preserving approaches for improving cloud security and the trust network. A wide array of data mining, cryptography and information hiding techniques have been applied to cater to different aspects of providing risk free work environment in the cloud. Given the lack of management of this information, a systematic investigation is required to structurally organize the topics studied. This paper aims to clearly portray the stringent and urgent need for applying privacy preserving approaches to the cloud and highlight the relevant work that has been done along these lines. The key objective is to identify important areas of user-cloud interaction and demonstrate a survey on the state of the art algorithms that have led to improved cloud privacy in these areas The focus is on exploring criteria for the impact of such approaches on user cloud interaction. An understanding of the research issues associated with these sensitive areas of cloud computing may enable us to better leverage benefits of the cloud and reflect on future possibilities of exploration.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {21},
numpages = {12},
keywords = {personalization, privacy preservation, cloud computing, data mining, storage},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523139,
author = {Srivastava, Praveen Ranjan and Kapoor, Rahul and Gupta, Rajat},
title = {Study of the Effect of a Rule Based Classifier Modeled Anti-Corruption Body in a Neural Network Based Environment},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523139},
doi = {10.1145/2522548.2523139},
abstract = {A rigorous debate is going on the rules on which the Lokpal (an anti-corruption body) should work. In this paper rules have been defined on which an anti-corruption body will take decisions on a particular case. Three agents have been used -- citizen, bureaucrat and whistle blower to model the environment using neural network. External factors are defined which would affect the working of the Lokpal. The Lokpal body is modeled using rule-based classifier. The decisions of the above Lokpal model on corruption cases are studied. The effects of change in the external factors on the rate of corruption are discussed.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {22},
numpages = {9},
keywords = {judiciary, training, budget, artificial neural network, whistleblower, hierarchy, bureaucracy, rule-based classifier, corruption},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523140,
author = {Bhargava, Rohan and Tripathy, B. K. and Tripathy, Anurag and Dhull, Rajkamal and Verma, Ekta and Swarnalatha, P.},
title = {Rough Intuitionistic Fuzzy C-Means Algorithm and a Comparative Analysis},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523140},
doi = {10.1145/2522548.2523140},
abstract = {Data clustering algorithms are used in many fields like anonymisation of databases, image processing, analysis of satellite images and medical data analysis. There are several C-Means clustering algorithms in the literature. Besides the hard C-Means, there are uncertainty based C-Means algorithms like the Fuzzy C-Means algorithm and its variants, the Rough C-Means algorithm, the Intuitionistic Fuzzy C- Means algorithm and the hybrid C-Means algorithms (Rough Fuzzy C-Means algorithm). In this paper we propose a new hybrid clustering algorithm called Rough Intuitionistic Fuzzy C-Means and evaluate its performance in comparison to the other algorithms mentioned above. We have applied these algorithms on numerical as well as image data of two different types and used some benchmarking indexes for the evaluation of their performance. The results show that the proposed algorithm outperforms the existing algorithms in almost all cases.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {23},
numpages = {11},
keywords = {rough C-means, intuitionistic fuzzy C-means, fuzzy C-means, data clustering, rough intuitionistic fuzzy C-means, rough fuzzy C-means, C-means},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

@inproceedings{10.1145/2522548.2523168,
author = {Geetha, Mary A. and Acharjya, D. P. and Iyengar, N. Ch. S. N.},
title = {Algebraic Properties and Measures of Uncertainty in Rough Set on Two Universal Sets Based on Multi-Granulation},
year = {2013},
isbn = {9781450325455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522548.2523168},
doi = {10.1145/2522548.2523168},
abstract = {The fundamental concept of crisp set has been extended in many directions in recent past. The notion of rough set by Pawlak being noteworthy among them. A rough set captures indiscernibility of elements in a set. In the view of granular computing, rough set model is researched by single granulation. It has been extended to multi-granular rough set model in which the set approximations are defined by using multiple equivalence relations on the universe simultaneously. But, in many real life scenarios, an information system establishes the relation with different universes. This gave the extension of multi-granulation rough set on single universal set to multi-granulation rough set on two universal sets. In this paper, we define some algebraic properties and measures of uncertainty of multi-granulation rough set for two universal sets U and V. We study the algebraic properties that are interesting in the theory of multi-granular rough sets. This helps in describing and solving real life problems more accurately.},
booktitle = {Proceedings of the 6th ACM India Computing Convention},
articleno = {24},
numpages = {8},
keywords = {roughness, topological characterization, rough set, multi-granulation, measures of completeness, binary relation, knowledge representation, solitary set},
location = {Vellore, Tamil Nadu, India},
series = {Compute '13}
}

