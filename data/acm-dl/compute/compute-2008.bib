@inproceedings{10.1145/1341771.1341773,
author = {Agarwal, Vikas and Chafle, Girish and Mittal, Sumit and Srivastava, Biplav},
title = {Understanding Approaches for Web Service Composition and Execution},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341773},
doi = {10.1145/1341771.1341773},
abstract = {Web services have received much interest due to their potential in facilitating seamless business-to-business or enterprise application integration. Of particular interest is the Web Service Composition and Execution (WSCE) process - the creation of a workflow that realizes the functionality of a new service and its subsequent deployment and execution on a runtime environment. A significant number of solutions have been proposed in the literature for composition and execution of web services. However, in order to choose a suitable technique for an application scenario, one needs to systematically analyze the strengths and weaknesses of each of these solutions. To this end, we present an analysis that includes formalization of the WSCE process, a classification of existing solutions into four distinct categories (approaches), and an in-depth evaluation of these approaches. Our evaluation is based on multiple metrics that we deem critical for a WSCE system, e.g. composition effort, composition control, and ability to handle failures. We also present an application of this analysis to three different scenarios.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {1},
numpages = {8},
keywords = {planning, QoS, web services composition and execution},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341774,
author = {Sarkar, Santonu and Sindhgatta, Renuka and Pooloth, Krishnakumar},
title = {A Collaborative Platform for Application Knowledge Management in Software Maintenance Projects},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341774},
doi = {10.1145/1341771.1341774},
abstract = {In the era of global outsourcing, maintenance and enhancement activities are performed in distributed locations. In most cases, the domain expertise is not available which increases the complexity to manifold. A critical success factor in such a scenario is to have a collaborative platform for managing and sharing the domain specific knowledge across distributed locations. In our ongoing research we have developed a human assisted collaborative knowledge sharing tool called CollabDev. The aim of this tool is to analyze applications in multiple languages and render various structural, architectural, and functional insights to the people involved in maintenance. The novelty of this platform lies in integrating different elements of application knowledge by linking them to source code and allowing multiple developers to collaborate on-line by using annotations for the knowledge elements. The platform also provides diagnostic information on architecture of source code.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {2},
numpages = {7},
keywords = {code search, collaboration, code analysis, distributed maintenance},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341775,
author = {Ganesh, Arul and Gopinath, K},
title = {SPKI/SDSI Certificate Chain Discovery with Generic Constraints},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341775},
doi = {10.1145/1341771.1341775},
abstract = {The SPKI/SDSI is a security infrastructure whose principal goal is to facilitate the building of secure, scalable, distributed computing systems. Given a set of SPKI/SDSI certificates, the decision on granting access to a resource by a user is taken by using a certificate chain discovery process. SPKI/SDSI infrastructure allows validity specification. The validity specification is a time period during which a certificate is valid. This validity specification, as defined in the specification RFC-2693, allows for limited constraints on the certificate. But the specification also allows for more powerful constraints specification. In this paper we demonstrate how weak Monadic Second Order (WS1S) logic can be used for specification of general validity constraint, with specific example provided for time constraints which is represented as interval on an abstract domain, and manipulated as WS1S formula. We also show this logic can be combined with Weighted Pushdown System (WPDS) to formally answer most of authorization questions based on the given validity period.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {3},
numpages = {8},
keywords = {policy, SPKI/SDSI, certificate chain, WPDS, security, validity specification},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341776,
author = {Ponnalagu, K. and Narendra, N. C.},
title = {Deriving Service Variants from Business Process Specifications},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341776},
doi = {10.1145/1341771.1341776},
abstract = {Software service organizations typically tend to develop custom solutions upon entry to a project engagement. This is not a scalable proposition. Today, driven by the need for enhancing profitability, reuse of existing assets across customer engagements is a more viable strategy. This is leading to the transformation of software services organizations from a labor-based to an asset-based model. In the enterprise application development domain, business processes are used to model the dynamic behavior of enterprise applications. Hence one highly prevalent asset-based approach in this domain is to build a scalable portfolio of services using a service-oriented architecture (SOA) approach. This provides a mechanism for representing business processes as a set of business-aligned, loosely-coupled services that can be iteratively composed and re-composed to create loosely coupled composite applications that mirror and support business processes.A key research problem that arises here is how to instantiate a stated business process specification as a combination of variants of existing services in the portfolio. We show via an illustrative example that this problem is non-trivial and scalable, and it depends on the semantics of the business process specification, especially with respect to the inter-service data and control flow dependencies that the instantiated business process specification has to satisfy. To that end, we also present our Variation-Oriented Service Design (VOSD) algorithm for automatically deriving service-level variants from the stated business process specifications. We also show that our algorithm is scalable, since its asymptotic complexity is linear in the number of service variants. We also illustrate our algorithm via the example.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {4},
numpages = {9},
keywords = {service-oriented architecture, reuse, business process},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341777,
author = {Sethuraman, Arun and Yalla, Krishna Kumar and Sarin, Ankur and Gorthi, Ravi Prakash},
title = {Agents Assisted Software Project Management},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341777},
doi = {10.1145/1341771.1341777},
abstract = {The market opportunities for the field of software engineering have been on the rise for the last three decades. This field of engineering is relatively new and the management of projects in this field, though experiencing greater degrees of assistance from tools, still requires improvements. We present here details of our experiments on how the multi-agent technology can be used to improve the productivity and quality of software project management (SPM) process. We have picked up the quality review management sub-process of SPM and described its modeling using GAIA and its implementation using the JADE framework. We also present a case study and the productivity and quality improvements observed.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {5},
numpages = {8},
keywords = {productivity and quality improvements, software project management, multi agent systems, agent oriented development methodologies, project management},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341778,
author = {Pasala, Anjaneyulu and Lew Yaw Fung, Yannick L. H. and Akladios, Fady and Gorthi, Ravi Prakash},
title = {An Approach to Select Regression Tests to Validate .NET Applications upon Deployment of Components Upgrades},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341778},
doi = {10.1145/1341771.1341778},
abstract = {Current practice of executing entire system test suite to validate applications upon deployment of upgrades is both expensive and time consuming. The existing automatic regression tests selection techniques that recommend smaller regression test suites are dependent on availability of either source code or version change information. Therefore, in this paper, we propose a regression test strategy based on capturing and analyzing the dynamic behavior of the application. The approach recommends a smaller test suite to validate software applications upon deployment of upgrades of all types of components supplied in binaries. Methods based on dynamic analysis are more efficient as they analyze the application by executing the application according to its intended use. Based on the proposed approach, we have developed a prototype tool called InARTS that determines the impact of upgrades on .NET applications and suggests a reduced set of regression tests. Results of the case study performed on a practical application using InARTS are encouraging.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {6},
numpages = {8},
keywords = {change impact analysis, software maintenance, regression test selection, components upgrades},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341779,
author = {Laliwala, Zakir and Chaudhary, Sanjay},
title = {Service Grouping and Group Notification in Grid Business Process},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341779},
doi = {10.1145/1341771.1341779},
abstract = {Enterprises are becoming more collaborative, distributed, and heterogeneous with the advancement of Information and Communication Technology (ICT). As a result, business processes require integration with distributed heterogeneous services. Business process requires one or more services to fulfill the functionalities. Business processes are event-driven and change state frequently during the life cycle. Business process are running in parallel and interacting with multiple services, partners, and customers as per the requirements and policy. There is a need to aggregate information from multiple resources or services according to policy, to provide better query, search, and group notification. In this paper, we have proposed event-driven service-oriented grid architecture to achieve event-driven grouping and notifications for stateful services. We have proposed grid business process by evaluating different specifications related to grid and web services to capture the requirements of a dynamic business process. WS-ResourceFramework and WS-Notifications specifications are evaluated to build stateful services and to support state, transaction, and notification. WS-ServiceGroup specification is used to achieve aggregation of WS-Resources and services as well as group notification.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {7},
numpages = {8},
keywords = {grid business process, notification, web services, event-driven SOA, SOA, service grouping},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341780,
author = {Sharma, Ratnesh K. and Shih, Rocky and Bash, Cullen and Patel, Chandrakant and Varghese, Philip and Mekanapurath, Mohandas and Velayudhan, Sankaragopal and Kumar, Manu},
title = {On Building next Generation Data Centers: Energy Flow in the Information Technology Stack},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341780},
doi = {10.1145/1341771.1341780},
abstract = {The demand for data center solutions with lower total cost of ownership and lower complexity of management is driving the creation of next generation datacenters The information technology industry is in the midst of a transformation to lower the cost of operation through consolidation and better utilization of critical data center resources. Successful consolidation necessitates increasing utilization of capital intensive "always-on" data center infrastructure, and reducing the recurring cost of power. A need exists, therefore for an end to end methodology that can be used to design and manage dense data centers and determine the cost of operating a data center.The chip core to the cooling tower model must capture the power levels and thermo-fluids behavior of chips, systems, aggregation of systems in racks, rows of racks, room flow distribution, air conditioning equipment, hydronics, vapor compression systems, pumps and heat exchangers. Earlier work has outlined the foundation for creation of a "smart" data center through use of flexible cooling resources and a distributed sensing and control system that can provision the cooling resources based on the need. This paper shows a common platform which serves as an evaluation and basis for policy based control engine for such a "smart" data center with much broader reach -- from chip core to the cooling tower. In this paper, we propose a data center solution, which has three components: Cooling, Power and Compute. These three components collectively improve efficiency and manageability of the data center by supporting greater compaction, flexible building blocks that can be dynamically configured, dynamic optimization, better monitoring and visualization, and policy-based control. Coefficient of performance (COP) of the ensemble is defined that represents an overall measure of the efficiency of performance of energy flow during the operation of a data center.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {8},
numpages = {7},
keywords = {data center management, energy efficiency, smart cooling, sustainability, smart data center},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341781,
author = {Abhilash, R. and Das, Sukhendu},
title = {Video Cut and Paste for 3D Composition},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341781},
doi = {10.1145/1341771.1341781},
abstract = {Given two video sequences of different scenes, the problem of seamlessly transferring a 3D moving object from one sequence to the other is a complex task. In this paper, we present a method to extract the alpha matte of a moving 3D object from a source video, and then correctly augment the object into another target video. Our framework builds upon techniques in natural image and video matting, composition, and image-based rendering. Natural image matting is usually composed of: foreground and background color estimating and alpha estimating. Our proposed technique uses local and global color information to estimate the accurate alpha values and extends this approach to extract a moving object from a video sequence. From a video sequence consisting of a single moving object over a stationary background, we combine motion statistics, color and contrast cues to extract a foreground (moving) object efficiently with no interaction from the user. During the process of compositing the moving object (from source video) on the target video, our approach composes the object accurately and ensures that the implanted moving object does not collide with other objects (static or moving) already present. An intuitive user interface (UI) tool is designed and implemented to provide flexible control and facilitate 3D composition for the user.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {9},
numpages = {8},
keywords = {user interface, alpha matting, video-based rendering, 3D composition},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341782,
author = {Bastioni, Manuel and Re, Simone and Misra, Shakti},
title = {Ideas and Methods for Modeling 3D Human Figures: The Principal Algorithms Used by MakeHuman and Their Implementation in a New Approach to Parametric Modeling},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341782},
doi = {10.1145/1341771.1341782},
abstract = {This paper briefly presents the basic algorithms used by MakeHuman and their application in a new context from the point of view of the relationship between artist and computer. The problem we posed is the development of a valid tool specifically designed for the modeling of virtual humans, with a simple yet complete pose system including the simulation of muscular movement. It is a new interfacing idea, different from the common graphic board with thousands of parameters, much easier to use, fast and intuitive.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {10},
numpages = {6},
keywords = {3D human, 3d model, ACM proceedings, 3d modelling},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341783,
author = {Singh, Sanasam Ranbir and Murthy, Hema A. and Gonsalves, Timothy A.},
title = {Effect of Word Density on Measuring Words Association},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341783},
doi = {10.1145/1341771.1341783},
abstract = {The study of mining the associated words is not new. Because of its wide ranges of applications, it is still an important issue in Information Retrieval. The existing estimators such as joint probability, words association norm do not consider the density of the words present in each window. In this paper, we incorporate the word density and propose estimator based on word density to measure the association between the words. From various experimental results based on the human judgments and precision collected from search engines, we find that the precision of the estimators could be improved by incorporating word density. For all ranges of the size of the windows, our estimator outperforms all other estimators. We also observe that all these estimators (both existing and proposed one) perform relatively better when the windows contain around five sentences. We also show by using Spearman rank-order correlation coefficient that our estimator returns better quality of the ranking of the associated terms.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {11},
numpages = {8},
keywords = {words association norm, joint probability, words association, word density},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341784,
author = {Nikam, Shankar Bhausaheb and Agarwal, Suneeta},
title = {Level 2 Features and Wavelet Analysis Based Hybrid Fingerprint Matcher},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341784},
doi = {10.1145/1341771.1341784},
abstract = {In this work, we present a hybrid fingerprint verification system based on Level 2 features i.e. minutiae and multiresolution analysis of fingerprint images. Systems based only on minutiae features do not perform well for poor quality images. In practice, we often encounter extremely dry, wet fingerprint images with cuts, warts, etc. Due to such fingerprints, minutiae based systems show poor performance for real time authentication applications with large number of identities. To alleviate the problem of poor quality fingerprints, and to improve overall performance of the system, we have proposed hybrid fingerprint verification based on both minutiae features and wavelet statistical features. Final matching score is calculated by fusing two matching scores of minutiae based method and wavelet based algorithm. Proposed system is tested on DB1 (set A) database of FVC 2004. The experimental results have shown that, proposed approach is more efficient and suitable than conventional minutiae based methods for real time authentication systems with large size databases.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {12},
numpages = {7},
keywords = {wavelet, fingerprint, authentication, minutiae, biometrics},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341785,
author = {Aggarwal, Naveen and Prakash, Nupur and Sofat, Sanjeev},
title = {Detecting Camera Movements &amp; Production Effects in Digital Videos},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341785},
doi = {10.1145/1341771.1341785},
abstract = {Numerous algorithms have been proposed in the last decade, for detecting low level features and camera motion in video sequences. Along with different camera movements such as pan, tilt, and zoom, there may be some jitter movements due to hand shaking. Differentiating these jitter movements from actual camera motions is a very challenging task and is the main cause of false detections in most of the processes. Comparison of the work of different research groups working in this area can be seen from TRECVID 2005 &amp; 2006 results. Low Precision and Recall of different methods is a clear evident that no method is suitable for all types of video to detect different camera movements. Further many methods are either computation intensive or too much false detection reduces their efficiency. A method based on region matching using area correlation is proposed to differentiate the different camera movements. Different regions are classified based on their statistical similarity in each frame of video. For each region in two consecutive frames, area correlation function is calculated. This area correlation energy function is used to detect the similarity among the regions. Weighted sum of correlation function in all the mismatched regions and location of these are used to classify the camera movements in video into different types such as pan, tilt, zoom etc. This method is tested on different videos and reasonable performance is achieved. It can also be used as pre-classifier in prior to video transition detection methods to reduce their false detections.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {13},
numpages = {5},
keywords = {production effects, shot detection, video analysis, camera movements, correlation, scene analysis},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341786,
author = {Shanthi, I. Elizabeth and Izaaz, Y. and Nadarajan, R.},
title = {On the SD-Tree Construction for Optimal Signature Operations},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341786},
doi = {10.1145/1341771.1341786},
abstract = {Signature file based access methods initially applied on text have now been used to handle set-oriented queries in Object-Oriented Data Bases (OODB). All the proposed methods use either efficient search method or tree based intermediate data structure to filter data objects matching the query. Use of search techniques retrieves the objects by sequentially comparing the positions of 1s in it. Such methods take longer retrieval time. On the other hand tree based structures traverse multiple paths making comparison process tedious. In this paper we describe a new indexing technique for OODB using the dynamic balancing of B+ tree called SD (Signature Declustering) tree. In this work the positions of 1s in the signatures are distributed over a set of leaf nodes. Using this for a given query signature all the matching signatures can be retrieved cumulatively in a single node. Also for signature insertion and query searching an optimal search path is calculated so that the entire process is speeded up. Experiments have been conducted to analyze the time and space overhead of the SD-tree by varying the signature length and the distribution of signature weight. The study clearly indicates the advantage of fast retrieval time in a way quite different from the other methods suggested in the past.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {14},
numpages = {8},
keywords = {signature file, indexing, information retrieval, OODB},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341787,
author = {Biswas, Arindam and Khara, Suman and Bhowmick, Partha and Bhattacharya, Bhargab B.},
title = {Extraction of Regions of Interest from Face Images Using Cellular Analysis},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341787},
doi = {10.1145/1341771.1341787},
abstract = {A novel algorithm for extracting the regions of interest (ROI) from face images is presented in this paper. The novelty of the algorithm comes from its multi-resolution cellular analysis coupled with an adaptive thresholding technique incorporating a unique idea of exponential averaging. The complexity of the cellular ROIs reported by the algorithm from the frontal face view as input, is further controllable by the chosen cell size, which is its added advantage. Apart from the actual ROIs representing the eye pair, nostrils, and the mouth area, some regions of non-interest may also creep in while extracting the set of cellular regions from the face image, which are discarded by a simple geometric analysis using a containment tree. The containment tree, which is newly introduced in this paper, captures the underlying relationship of the cellular regions, which, when analyzed, returns the face ROIs in an elegant representation. Since the entire algorithm works purely in the integer domain with primitive operations (comparison, right shift, and addition) only, it runs very fast for both gray-scale and color images. Some experimental results on different facial images demonstrate its speed, robustness, and efficiency.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {15},
numpages = {8},
keywords = {face segmentation, region of interest, biometrics, containment tree, adaptive thresholding},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341788,
author = {Kalpana, B. and Nadarajan, R. and Babu, J. Senthil},
title = {A Galois Lattice Framework to Handle Updates in the Mining of Closed Itemsets in Dynamic Databases},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341788},
doi = {10.1145/1341771.1341788},
abstract = {Incrementality is a major challenge in the mining of dynamic databases. In such databases, the maintenance of association rules can be directly mapped into the problem of maintaining closed frequent itemsets. A number of incremental strategies have been proposed earlier with several limitations. A serious limitation is the need to examine the entire family of closed itemsets, whenever there are insertions or deletions in the database. The proposed strategy relies on an efficient and selective update of the closed itemsets using an indexed trie structure. The framework emphasizes on certain fundamental and structural properties of Galois Lattice theory to overcome the limitations of the earlier approaches. Apart from facilitating a selective update, the indexed structure removes the necessity of working with a wholly memory resident trie.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {16},
numpages = {6},
keywords = {association rules, incremental mining, Galois Lattices, closed frequent itemsets},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341789,
author = {Krishnamoorthy, Srikumar and Saple, Avdhoot Kishore and Achutharao, Prahalad Haldhoderi},
title = {An Integrated Query Optimization System for Data Grids},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341789},
doi = {10.1145/1341771.1341789},
abstract = {The disparate and geographically distributed data sources in an enterprise can be integrated using distributed computing technologies such as data grids. The real challenge involved in such data integration efforts is in the design and development of the distributed query processing engine that lie beneath such integrated systems. In the current literature, distributed query processing and optimization is carried out in three distinct phases namely, (1) creation of single node plan, (2) generation of parallel plan, and (3) optimal site selection for plan execution. As considering the three phases in isolation leads to sub-optimal plans, the paper proposes a new distributed query optimization model that integrates all the three phases of the query optimization. This paper also presents different heuristic approaches for solving the proposed integrated distributed query processing problem. Furthermore, the presented system is integrated with a data grid solution and several real-time experiments are conducted to demonstrate its usefulness.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {17},
numpages = {8},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341790,
author = {Sendhilkumar, S. and Geetha, T. V.},
title = {Personalized Ontology for Web Search Personalization},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341790},
doi = {10.1145/1341771.1341790},
abstract = {The highest problem faced by the users of web search today is the quality and the amount of the results they get back. The results often frustrate and consume precious time of the users. Most of the existing search engines perform searches that are keyword-based. They get the search query for the user, search for the presence of search words in the web documents, rank them and display the results to the user. The semantics of the user query and also the intention of the user are not considered. Hence to improve searching in the WWW, a new personalized search index called User Conceptual Index (UCI) has been designed, which provides a conceptual relation between the search keywords and the pages, which matches the user's information need. This paper focuses towards the development of automatically identified user profile called as a personalized ontology and page ontology for the improvement of an existing personalized web search system based on the UCI.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {18},
numpages = {7},
keywords = {personalized ontology, user profile, personalization, architecture, web search},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341791,
author = {Rajendran, T. and Sreenaath, K. V.},
title = {Secure Anonymous Routing in Ad Hoc Networks},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341791},
doi = {10.1145/1341771.1341791},
abstract = {Communication in Ad Hoc networks relies on the routing functionality of the intermediate nodes. Secure routing and preventing traffic analysis are important criterion for secure anonymous communication in Ad Hoc networks. By analyzing the traffic in ad hoc networks, the location and identity of the nodes can be found thereby losing anonymity. A number of techniques are available for anonymous routing. Existing techniques are vulnerable to packet type analysis attacks thus do not provide complete anonymity and security. Also they involve more cryptographic overhead. We propose a secure anonymous communication system for Ad Hoc networks involving less cryptographic operations and also addressing various drawbacks in existing techniques providing complete anonymity.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {19},
numpages = {7},
keywords = {ad-hoc network, anonymous routing},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341792,
author = {Raghavan, Sriram and B, Ravindran},
title = {Successive Refinement Algorithms for Distributed Area Coverage Using Mobile Robots},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341792},
doi = {10.1145/1341771.1341792},
abstract = {Distributed area coverage using multiple mobile robots poses several significant challenges. Minimizing the number of revisits and coordinating coverage across these robots are some of them. In this paper, we introduce a parameter called overlap_ratio to quantitatively define the overlap in coverage. Further, we design algorithms based on communication between the robots as the only coordinating tool. We demonstrate the utility of this parameter in conjunction with Pseudonet Coordination Architecture to develop area coverage algorithms through successive refinement. We analyze systematically how these algorithms perform on unknown terrains.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {20},
numpages = {9},
keywords = {overlap_ratio, mobile robots, pseudonet coordination architecture, area coverage task},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341794,
author = {Sureka, Ashish and De, Sudripto and Varma, Kishore},
title = {A Generic Software Architecture of a Text Processing System for Analyzing Product Warranty Claims Data},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341794},
doi = {10.1145/1341771.1341794},
abstract = {Product manufactures for goods such as automobiles, computers, and consumer electronics spend a huge amount of money towards warranty costs. Large and mid-size product manufacturers are looking for Information Technology based solutions to reduce the overall cost towards warranty and enable compliance to government regulations. In this paper, we present an application of text analytics applied on textual data recorded in warranty claim forms for the purpose of efficient defect discovery. We propose a generic architecture of a text analytics based system for gaining intelligence from the textual data stored in warranty claim forms. We present a concrete example in automotive sector.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {21},
numpages = {4},
keywords = {software architecture, warranty data analysis, text tagging and annotation, text analytics},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341795,
author = {Kharat, Satish and Mishra, Rajeev and Das, Ranadip and Vishwanathan, Srikanth},
title = {Migration of Software Partition in UNIX System},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341795},
doi = {10.1145/1341771.1341795},
abstract = {Software partitioning is a technology to partition a machine running a single instance of operating system image into multiple virtual machines called partitions. Each partition emulates an independent machine running a single instance of the operating system on dedicated hardware. All partitions are isolated form each other by the operating system. Software partitioning is very useful in server consolidation. A single powerful machine can be used to host many different servers each using a single software partition. This increases hardware resource utilization, gives flexibility to the administrator and can reduce system administration costs. The advantages offered by software partitioning are greatly enhanced with the capability to checkpoint a running software partition and restart it on a different machine. It helps in load balancing over hardware resources, load balancing over time and fault tolerance.Workload Partition [WPAR] is IBM's implementation that provides software partitioning capability on the AIX operating system. It is possible to do the live migration of the WPARs in and across AIX systems. The live migration is achieved by the checkpoint/restart mechanism. It is possible to checkpoint and restart WPARs running most existing AIX applications without any modification to the applications. Also the checkpoint and restart process is transparent to the application running inside the WPAR (Partition).This paper discusses the issues faced in implementing software partition checkpoint and restart in the AIX operating system. These issues will be typical to any standard UNIX operating system. To successfully checkpoint and restart a software partition, it is necessary not only to checkpoint all the user processes in the partition but also to checkpoint global data pertaining the partition itself and data shared between processes of the Partition like IPC data, Streams, timers, file handles, memory mapped regions, shared memory, System services, Virtual devices etc. The WPAR implementation handles both; the checkpoint of individual processes as well as checkpoint of partition wide data.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {22},
numpages = {4},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341796,
author = {Laha, Arijit},
title = {RAP: A Conceptual Business Intelligence Framework},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341796},
doi = {10.1145/1341771.1341796},
abstract = {Over last few years Data Warehouse (DW) has emerged as the core of the Business Intelligence (BI) infrastructure for many forward-looking organizations. It is often complemented by Business Performance Management (BPM) and workflow management tools for operationalizing of strategic and tactical decisions and tracking progress of activities across the organization. However, till now there is not much support available to the decision-makers to leverage "organizational experience" i.e., the past events, corresponding responses made as well as their respective impacts, to guide him/her in the decision-making process. In this paper a conceptual BI framework, the Reference-Activity-Projection (RAP) is proposed. RAP is designed to support the decision-making process through a systematic access mechanism to organizational experience. RAP uses along with standard BI tools and techniques, the emerging technologies such as complex event processing (CEP), unstructured/semi-structured data mining, social network analysis etc.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {23},
numpages = {4},
keywords = {social network analysis, event processing, data mining, organizational experience, data warehousing, business intelligence},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341797,
author = {Borkotoky, Chandragupta and Galgate, Swapil and Nimbekar, S. B.},
title = {Human Computer Interaction: Harnessing P300 Potential Brain Waves for Authentication of Individuals},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341797},
doi = {10.1145/1341771.1341797},
abstract = {We present a novel idea for user authentication that we call brain-thoughts. The goal of a brain-thought system would be to extract as much useful data as possible from a user's brain signals upon "transmitting" a thought. In this paper we are proposing the idea of recording the users' thoughts in the form of the P300 Positive Voltage Spikes. Provided that these brain signals can be recorded and processed in an accurate, repeatable and reliable way using software such as Matlab, Simulink, BCI2000 etc, a brain-thought system might provide a quasi two-factor, changeable, authentication method resilient to shoulder-surfing and hacking techniques. In order to reinforce the security, we are proposing a concept of integrating our P300 system with another voluntarily generated brain waves pattern which is created when the subject closes his eyelids. In this paper, we shall also discuss the motivation and potential of brain-thought authentication, the status quo of BCI technology, and outline the design of what we believe to be a currently feasible brain-thought security system along with the implementation of Signal Processing using Matlab. We also briefly mention the need for general exploration and open debate regarding ethical considerations for such technologies.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {24},
numpages = {4},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341798,
author = {A, Manoj Prasad and Selvaraj, Muthuselvam and Madhvanath, Sriganesh},
title = {Peer-to-Peer Ink Messaging across Heterogeneous Devices and Platforms},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341798},
doi = {10.1145/1341771.1341798},
abstract = {Pen and touch interfaces for personal and shared devices are becoming increasingly relevant today, in the context of mobility and ease of use. A key capability enabled by pen-interfaces is that of messaging using handwritten, as opposed to text messages. Not only are ink messages easier to enter than text messages (especially when a full keyboard is not present), they allow the incorporation of other elements such as drawings and doodles into instant messaging. However since ink formats are typically platform-specific and proprietary, messaging across different platforms such as Tablet PCs and Linux-based PDAs poses an interoperability problem. In this paper, we show how Ink Markup Language (InkML), an open draft standard from W3C, can be used to address this problem. In particular, we propose an Ink messaging protocol, and a system architecture for implementing the protocol operations. We have implemented this protocol as an extension to the Extensible Messaging and Presence Protocol (XMPP), an open IETF standard.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {25},
numpages = {5},
keywords = {ink messaging, digital ink markup language, XMPP},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341799,
author = {Harish, M. and Anandavelu, N. and Anbalagan, N. and Mahalakshmi, G. S. and Geetha, T. V.},
title = {Result Evaluation Strategies for Peer Selection in P2P},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341799},
doi = {10.1145/1341771.1341799},
abstract = {P2P networks provide a highly reliable way of sharing resources. For the peers to behave honestly trust needs to be incorporated. The trust framework should incorporate self-experience and reputation to calculate trustworthiness of a peer, in order to enable assessing the peers based on the services provided by them. Within the trust framework, various strategies for assessing the peers based on the performance shall be analysed which may result in selection of peers for providing certain type of services. This paper proposes Game Tree strategy, Tit for Tat strategy, Self Trust strategy, Dynamic strategy and Auditing strategy for selecting peers for doing job in trust-driven P2P networks.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {26},
numpages = {4},
keywords = {result evaluation strategies, game tree, trust framework, peer-to-peer},
location = {Bangalore, India},
series = {COMPUTE '08}
}

@inproceedings{10.1145/1341771.1341800,
author = {Sabharwal, Ritu and Guijarro, Julio},
title = {Avalanche: Managing Deployments for Enterprise Scale Grids},
year = {2008},
isbn = {9781595939500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1341771.1341800},
doi = {10.1145/1341771.1341800},
abstract = {Managing deployment of grid infrastructure software is a major challenge in setting up large and heterogeneous Grids. This is one of the key issues preventing wide spread adoption of Grid Computing in enterprises. This paper introduces 'Avalanche', an open source distributed deployment and change management system. Avalanche deals with the complete lifecycle of software deployments including installation, configuration, upgrades and un-installation in a secure manner. Other features include job scheduler support, failure detection and recovery, monitoring, automated provisioning, etc. Current version of Avalanche leverages upon open source technologies like HP-Labs SmartFrog, XMPP and Quartz Job Scheduler. Avalanche exposes these functionalities through a secure web based user-friendly interface. The initial results we received on experimental setups are encouraging.},
booktitle = {Proceedings of the 1st Bangalore Annual Compute Conference},
articleno = {27},
numpages = {4},
keywords = {SmartFrog, WSRF, deployment engine, globus toolkit, grid computing, quartz scheduler, XMPP},
location = {Bangalore, India},
series = {COMPUTE '08}
}

