@inproceedings{10.1145/3252950,
author = {Bhattacharya, Arnab},
title = {Session Details: Research Track Full Papers: Session 1},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3252950},
doi = {10.1145/3252950},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
numpages = {1},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140119,
author = {Mandal, Arpan and Chaki, Raktim and Saha, Sarbajit and Ghosh, Kripabandhu and Pal, Arindam and Ghosh, Saptarshi},
title = {Measuring Similarity among Legal Court Case Documents},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140119},
doi = {10.1145/3140107.3140119},
abstract = {Computing the similarity between two legal documents is an important challenge in the Legal Information Retrieval domain. Efficient calculation of this similarity has useful applications in various tasks such as identifying relevant prior cases for a given case document. Prior works have proposed network-based and text-based methods for measuring similarity between legal documents. However, there are certain limitations in the prior methods. Network-based measures are not always meaningfully applicable since legal citation networks are usually very sparse. On the other hand, only primitive text-based similarity measures, such as TF-IDF based approaches, have been tried till date. In this work, we focus on improving text-based methodologies for computing the similarity between two legal documents. In addition to TF-IDF based measures, we use advanced similarity measures (such as topic modeling) and neural network models (such as word embeddings and document embeddings). We perform extensive experiments on a large dataset of Indian Supreme Court cases, and compare among various methodologies for measuring the textual similarity of legal documents. Our experiments show that embedding based approaches perform better than other approaches. We also demonstrate that the proposed embedding-based methodologies significantly outperforms a baseline hybrid methodology involving both network-based and text-based similarity.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {1–9},
numpages = {9},
keywords = {Court Cases, Doc2vec, Topic Modeling, Word Embeddings, Legal Document Similarity, Legal Information Retrieval, Word2vec},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140111,
author = {Revanuru, Karthik and Turlapaty, Kaushik and Rao, Shrisha},
title = {Neural Machine Translation of Indian Languages},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140111},
doi = {10.1145/3140107.3140111},
abstract = {Neural Machine Translation (NMT) is a new technique for machine translation that has led to remarkable improvements compared to rule-based and statistical machine translation (SMT) techniques, by overcoming many of the weaknesses in the conventional techniques. We study and apply NMT techniques to create a system with multiple models which we then apply for six Indian language pairs. We compare the performances of our NMT models with our system using automatic evaluation metrics such as UNK Count, METEOR, F-Measure, and BLEU. We find that NMT techniques are very effective for machine translations of Indian language pairs. We then demonstrate that we can achieve good accuracy even using a shallow network; on comparing the performance of Google Translate on our test dataset, our best model outperformed Google Translate by a margin of 17 BLEU points on Urdu-Hindi, 29 BLEU points on Punjabi-Hindi, and 30 BLEU points on Gujarati-Hindi translations.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {11–20},
numpages = {10},
keywords = {India, Hindi, GPU, Computational Linguistics, Neural Networks, Machine Translation},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140118,
author = {Verma, Ishan and Singh, Lokendra},
title = {Multi-Structured Data Analytics Using Interactive Visualization to Aid Business Decision Making},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140118},
doi = {10.1145/3140107.3140118},
abstract = {Intelligent decision making highly relies on the ability to combine data from multiple sources from both within and outside the organization and generate appropriate insights. Multi-structured data analytics holds special significance for organizations, where different types of data are produced by different sources and applications, ad assimilating information from this diverse collection poses huge challenges. Data collection can contain information in the form of numbers, aggregates, time-series, reports, proposals, logs, communication records etc. Insight generation in such a scenario requires novel methodologies to judiciously link information from multiple sources and reason with them. This paper focuses on developing methodologies for intelligent integration of multi-structured data. The emphasis is to generate multiple superimposed data visualizations using intelligent methods of linking information amongst data from unstructured and structured sources and also to facilitate interactive data explorations through drill down facilities for interesting pieces of information.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {21–29},
numpages = {9},
keywords = {data integration, Multi-structured data analytics, integrated data},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140117,
author = {Sahay, Atul and Biswas, Pradipta},
title = {Webcam Based Eye Gaze Tracking Using a Landmark Detector},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140117},
doi = {10.1145/3140107.3140117},
abstract = {This paper proposes a real-time algorithm to detect users' gaze point in a video sequence from a standard web camera. We have shown that landmarks constructed for both eyes can reliably estimate the eyelid opening, which in turn can be used to tell where the user is staring at that particular moment. Further, the knowledge of the eye opening can be combined with the iris displacement from the reference point to predict the user's gaze point. We have reported a user study involving 8 users and we can track one of nine positions on screen within a radius of 11° of visual angle.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {31–37},
numpages = {7},
keywords = {iris localization, Viola-Jones object detector, Eye gaze tracking, landmark detector},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140114,
author = {Abulaish, Muhammad and Jahiruddin},
title = {A Novel Weighted Distance Measure for Multi-Attributed Graph},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140114},
doi = {10.1145/3140107.3140114},
abstract = {Due to exponential growth of complex data, graph structure has become increasingly important to model various entities and their interactions, with many interesting applications including, bioinformatics, social network analysis, etc. Depending on the complexity of the data, the underlying graph model can be a simple directed/undirected and/or weighted/un-weighted graph to a complex graph (aka multi-attributed graph) where vertices and edges are labelled with multi-dimensional vectors. In this paper, we present a novel weighted distance measure based on weighted Euclidean norm which is defined as a function of both vertex and edge attributes, and it can be used for various graph analysis tasks including classification and cluster analysis. The proposed distance measure has flexibility to increase/decrease the weightage of edge labels while calculating the distance between vertex-pairs. We have also proposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV files containing the list of vertex vectors and edge vectors, and calculates the distance between each vertex-pair using the proposed weighted distance measure. Finally, we have proposed a multi-attributed similarity graph generation algorithm, MAGSim, which reads the output of MAGDist algorithm and generates a similarity graph that can be analysed using classification and clustering algorithms. The significance and accuracy of the proposed distance measure and algorithms is evaluated on Iris and Twitter data sets, and it is found that the similarity graph generated by our proposed method yields better clustering results than the existing similarity graph generation methods.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {39–47},
numpages = {9},
keywords = {Similarity measure, Multi-attributed graph, Data mining, Clustering, Weighted distance measure},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3252951,
author = {Shukla, Rajesh},
title = {Session Details: Research Track Full Papers: Session 2},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3252951},
doi = {10.1145/3252951},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
numpages = {1},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140108,
author = {Dey, Subhomoy and Kashyap, Kishore},
title = {VAANI: A Voice-Based Authentication System for Linux Using Dynamic Threshold and Positive Selection},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140108},
doi = {10.1145/3140107.3140108},
abstract = {The following paper entails the design and development of a voice-based authentication system for Linux. The proposed system is text-dependent in nature. The system will verify a user based on his particular voice input and will give him access to a PC running on Linux. The system has two main modules -- the speaker verification module and the login module. The speaker verification module is responsible for verifying a valid user. The speaker verification module is inspired by the principles of immune system and the entire verification task is being modeled into an immune engineering framework. The speaker-recognition system is implemented on Python programming language, thus providing a free software approach for development. The login module deals with logging a verified user to a PC, which is implemented using Pluggable Authentication Modules (PAM). The developed system is deployed in less noise surroundings over a personal computer with in-built microphone.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {49–59},
numpages = {11},
keywords = {Delta features, Pluggable Authentication Module, Linux, Python, Dynamic Threshold, Dynamic Time Warping Speaker Recognition, Mel Frequency Cepstral Coefficients, Voice-based Authentication, Immune System},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140110,
author = {Mitra, Shubhadip and Dutta, Partha and Bhattacharya, Arnab},
title = {Optimal Algorithms for Min-Closed, Max-Closed and Arc Consistency over Connected Row Convex Constraints},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140110},
doi = {10.1145/3140107.3140110},
abstract = {A key research interest in the area of Constraint Satisfaction Problems (CSP) is to identify tractable classes of constraints and develop efficient algorithms for solving them. In this paper, we propose an optimal algorithm for solving r-ary min-closed and max-closed constraints. Assuming r = O(1), our algorithm has an optimal running time of O(ct) where c and t are the number of constraints and the maximum size of any constraint, respectively. This significantly improves the existing pairwise consistency based algorithm that takes O(c2t2) time. Moreover, for (binary) connected row convex (CRC) constraints, we design an optimal algorithm for arc consistency that runs in O(cd) time where d is the largest size of any domain. This again improves upon the existing O(cd2) algorithms. This, in turn, leads to a faster algorithm for solving CRC constraints. We also show how our solutions can be applied to determine problems in large distributed IT systems. The experimental evaluation shows that the proposed algorithms are several orders of magnitudes faster than the state-of-the-art algorithms.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {61–72},
numpages = {12},
keywords = {Min-Closed Constraints, Max-Closed Constraints, Constraint Satisfaction Problem, CSP, Connected Row-Convex Constraints},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140121,
author = {Sharma, Swati and Kaushal, Rishabh},
title = {Energy Conserving Secure VM Allocation in Untrusted Cloud Computing Environment},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140121},
doi = {10.1145/3140107.3140121},
abstract = {Cloud computing is the latest buzz in most of the IT organizations which are witnessing a a trend of migration from traditional computing to cloud computing, thereby reducing their infrastructure cost and improving efficiency and performance. Cloud computing provides services through virtualization layer, which helps to execute more than one operating systems and applications on a single machine. Being a crucial part of cloud computing, virtualization layer faces major security threats, most challenging being an insider threat wherein attacker can either compromise existing virtual machines (VMs) or create rogue VMs. The objective of this work is to propose virtual machine (VM) allocation algorithm which operates in an untrusted cloud computing environment with non-trustworthy VMs. Our approach is based on the notion of trust. Lack of trust is modeled by either introducing faults or monitoring SLAs per host on which VMs are hosted. Detailed experiments considering varying cloud infrastructure and varying workloads are conducted using CloudSim. Results show that proposed algorithm works well in untrusted environment while at the same time is energy efficient and reduces the computational costs by decreasing the number of migrations and SLA violations.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {73–81},
numpages = {9},
keywords = {Cloud Computing, Green Computing, Security},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140112,
author = {Ganapathi, Iyyakutti Iyappan and Prakash, Surya},
title = {3D Ear Based Human Recognition Using Gauss Map Clustering},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140112},
doi = {10.1145/3140107.3140112},
abstract = {This paper addresses the problem of human recognition using 3D ear biometrics. Existing feature extraction and description techniques in the literature for 3D shape recognition works well with the different class of shapes, however, not for profoundly comparable objects like human 3D ears. This work proposes an effective method utilizing Gauss mapping for feature keypoints detection and shape context to describe the detected keypoints. The proposed technique is as follows. A triangle for every point p is computed using two other points of the k-nearest neighbors within a sphere of radius r. A normal is computed for the obtained triangle and is mapped to a unit sphere. This mapping of normals is done for every conceivable triangle of point p. It is observed that mapped normals form a different number of clusters depending upon the type of surface point p belongs to. A point is considered as a keypoint if its projected normals form more than two clusters. Further, we project all the detected keypoints onto a plane and use them in the computation of feature descriptor vectors. Descriptor vector of a keypoint is computed by keeping it at the center and defining its shape context considering all other keypoints as its neighbors. To match a probe ear image with a gallery image for recognition, we compute correspondence for all the feature keypoints of the probe image to the feature keypoints of the gallery image. Final matching is performed by aligning the gallery image with the probe image and considering the registration error as the matching score. The experimental analysis conducted on University of Notre Dame (UND)-Collection J2 has achieved a verification accuracy of 98.20% with an equal error rate (EER) of 1.84%.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {83–89},
numpages = {7},
keywords = {Biometrics, person verification/ identification, Gauss Mapping, Ear recognition, 3D Ear, Feature Descriptor, Local Feature keypoints},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140113,
author = {Ali, Syed Sadaf and Prakash, Surya},
title = {Fingerprint Shell Construction with Prominent Minutiae Points},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140113},
doi = {10.1145/3140107.3140113},
abstract = {Fingerprint based authentication is one of the most extensively used authentication technique. Biometric systems relying on fingerprint usually directly uses minutiae points information of a fingerprint and store it as a user template. There are many recent works which show that original fingerprint of a user can be generated from the data of minutiae points. In case of traditional authentication systems based on password there is a liberty to change the password, however biometric data of a user cannot be changed as it is permanently associated with the human body. If any information related to biometric features of a user is stolen or compromised, then in that case we cannot change the compromised information. Therefore, it is essential to make sure that the biometric data is secure. Our motive is to generate a biometric template that will fulfill the necessities of performance, security, revocability and diversity. Moujahdi et al. proposed a technique called Fingerprint shell as a secure representation of fingerprint data using a user key. In this technique, a spiral curve is generated as a secured user template by using the distances between singular point and minutiae points. In this paper, we have proposed a technique in which we have included the quality of minutiae points for the construction of spiral curve. We have used a pair of unique user keys and utilized the information provided by minutiae points to generate a non-invertible user template. In case of compromising of user template by adversary, user has the liberty to generate new template by using different user keys, the new template and the compromised one are non-linkable. We tested our technique on FVC2002 DB1, FVC2002 DB2 and IIT Kanpur fingerprint databases using FVC protocol. Experimental results obtained are encouraging and demonstrate the viability of our technique.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {91–98},
numpages = {8},
keywords = {Diversity, Revocability, Template security, Biometric template, Authentication},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140125,
author = {Ghosh, Kripabandhu and Bhattacharya, Arnab},
title = {Stopword Removal: Why Bother? A Case Study on Verbose Queries},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140125},
doi = {10.1145/3140107.3140125},
abstract = {Stopword removal has traditionally been an integral step in information retrieval pre-processing. In this paper, we question the utility of this step in retrieving relevant documents for verbose queries on standard datasets. We show that stopword removal does not lead to noticeable difference in retrieval performance as opposed to not removing them. We observe this phenomenon in 7 FIRE test collections for 4 Indian languages, Bangla, Hindi, Gujarati and Marathi, as well as for European languages such as Czech (CLEF 2007) and Hungarian (CLEF 2005 to 2007). Since these languages are inflective, the stopword lists are not significant. More interestingly, for languages such as English (TREC678 Ad Hoc) and French (CLEF 2005 to 2007), stopword removal leads to a statistically significant drop in performance. This is due to using a generic stopword list that does not suit in many document retrieval tasks.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {99–102},
numpages = {4},
keywords = {Information retrieval evaluation, Stopword removal},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140132,
author = {Phadte, Akshata and Wagh, Ramrao},
title = {Word Level Language Identification System for Konkani-English Code-Mixed Social Media Text (CMST)},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140132},
doi = {10.1145/3140107.3140132},
abstract = {In this paper, we present an pure logic study on problem of word- level language identification for Konkani- English Code-Mixed Social Media Text (CMST). we describe a new dataset which contains of more than thousands posts from Facebook posts that exhibit code mixing between Konkani-English. To the best of our knowledge, our work is the first attempt at the creation of a linguistic resource for this language pair which will be made public and developed a language identification System for Konkani-English language pair. Using this Konkani-English tagged dataset we have carried out experiment on language detection at word level. We have used Different ways to solve language detection task, unsupervised dictionary-based detection technique, supervised Language identification of word level using sequence labelling using Conditional Random Fields based models, SVM, Random Forest.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {103–107},
numpages = {5},
keywords = {Konkani-English Code Mixed social media text, social media text tagging},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140122,
author = {Ravi, Vadlamani and Tejasviram, Vadali and Sharma, Anurag and Khansama, Rashmi Ranjan},
title = {Prediction Intervals via Support Vector-Quantile Regression Random Forest Hybrid},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140122},
doi = {10.1145/3140107.3140122},
abstract = {This paper presents a new method of determining prediction intervals via the hybrid of support vector machine and quantile regression random forest introduced elsewhere. Its effectiveness is tested on 5 benchmark regression problems. Fromthe experiments, we infer that the difference in performance of the prediction intervals from the proposed method and those from quantile regression and quantile regression random forest is statistically significant as shown by the Wilcoxon test at 5% level of significance. This is an important achievement of the paper.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {109–113},
numpages = {5},
keywords = {QR-QRRF, Support vector-quantile regression random forest hybrid, Wilcoxon test, Prediction intervals},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140123,
author = {Karkare, Amey and Agarwal, Nimisha},
title = {ParseIT: A Question-Answer Based Tool to Learn Parsing Techniques},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140123},
doi = {10.1145/3140107.3140123},
abstract = {Parsing (also called syntax analysis) techniques cover a substantial portion of any undergraduate Compiler Design course. We present ParseIT, a tool to help students understand the parsing techniques through question-answering. ParseIT automates the generation of tutorial questions based on the Context Free Grammar provided by the student and generates feedback for the student solutions. The tool generates multiple-choice questions (MCQs) and fill in the blank type questions, and evaluates students' attempts. It provides hints for incorrect attempts, again in terms of MCQs. The hints questions are generated for any correct choice that is missed or any incorrect choice that is selected. Another interesting form of hint generated is an input string that helps the students identify incorrectly filled cells of a parsing table.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {115–120},
numpages = {6},
keywords = {Compilers, E-Learning, Education, Intelligent Tutoring},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140126,
author = {Jaiswal, Sameer Ranjan and Sharma, Divyansh},
title = {Predicting Success of Bollywood Movies Using Machine Learning Techniques},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140126},
doi = {10.1145/3140107.3140126},
abstract = {An enormous amount of Bollywood movies are released every year making Bollywood one of the largest film industry in the world. In this study, we apply machine learning tools to create a model which can predict whether a Bollywood movie will be successful or not, before it is released. We have collected data from multiple sources like Cinemalytics, BoxOfficeIndia, YouTube and Wogma. We have designed factors like music score which is a unique element to Bollywood movies and greatly increases the accuracy of prediction. We label the prediction in two classes, Hit and Flop. After evaluating multiple classifiers, we have used Bagging algorithm to create the model.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {121–124},
numpages = {4},
keywords = {Machine Learning, Movie Success, YouTube Statistics, Bagging, Music Score, Bollywood, Feature Engineering},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140129,
author = {Kumar, B. Shravan and Ravi, Vadlamani},
title = {LDA Based Feature Selection for Document Clustering},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140129},
doi = {10.1145/3140107.3140129},
abstract = {In this paper, we propose a novel model for text document clustering. Usually, Text documents consist of a vast number of features. We selected important features through four methods term variance, document frequency, Latent Dirichlet Allocation, and Significance methods. We demonstrated the effectiveness of proposed model on 20NG and WebKB datasets which are publicly available. We evaluated the model with F-Score value. Results indicate that LDA performed best in capturing the discriminate features.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {125–130},
numpages = {6},
keywords = {Latent Dirichilet Allocation, Unsupervised feature selection, Text clustering, F-measure},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140131,
author = {Kashyap, Kanchan Lata and Bajpai, Manish Kumar and Khanna, Pritee},
title = {Breast Tissue Density Classification in Mammograms Based on Supervised Machine Learning Technique},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140131},
doi = {10.1145/3140107.3140131},
abstract = {Breast tissue density is one of the symptoms for breast cancer detection. Fully automatic breast tissue density classification is presented in this work. Present work consists of four steps which include breast region extraction and enhancement of mammograms, segmentation, feature extraction, and breast tissue density classification. Enhancement of mammogram is done by applying fractional order differential based filter. Segmentation of breast tissue segmentation has been done by using clustering based fast fuzzy c-means technique. Further, texture based local binary pattern (LBP) and dominant rotated local binary pattern (DRLBP) features have been computed from the extracted breast tissues to characterize its texture property. Support vector machine with linear kernel functions are used to classify the breast tissue density. Proposed algorithm is validated on the publicly available 322 mammograms of Mini-Mammographic Image Analysis Society (MIAS).},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {131–135},
numpages = {5},
keywords = {Breast tissue density, Fractional order, Support vector machine},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140128,
author = {Kashi, Rajanikanth N. and D'Souza, Meenakshi and Kishore, Koyalkar Raman},
title = {Incorporating Formal Methods and Measures Obtained through Analysis, Simulation Testing for Dependable Self-Adaptive Software in Avionics Systems},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140128},
doi = {10.1145/3140107.3140128},
abstract = {An Area 1rich with challenges is that of self-adaptive avionics software, with considerable thrust in both the American and European airspace modernization programs. Verifying functional requirements for such a system, ultimately leading to certification poses a unique set of problems, since these systems are required to be dependable. Also inherent is the subject of eliciting measures of adaptability which help evaluate the system in the context of non-functional requirements qualified by self-properties. We illustrate our approach for such a verification and evaluation exercise by proposing a combination of formal methods verification techniques and simulation based testing. The test bed is a representative self-adaptive software of a small UAS (Unmanned Aircraft System) avionics modeled as a multiagent BDI (Belief Desire Intention) system with evolutionary and reactive behaviours, illustrating important aspects of verification.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {137–142},
numpages = {6},
keywords = {Multi-agent Systems, Q-Learning, BDI, Self-Adaptive Software, Measures of Self-properties, Formal Methods, Model Checking},
location = {Bhopal, India},
series = {Compute '17}
}

@inproceedings{10.1145/3140107.3140130,
author = {Shivanagowda, G. M. and Goudar, R. H. and Kulkarni, U. P.},
title = {CRETAL: A Personalized Learning Environment in Conventional Setup},
year = {2017},
isbn = {9781450353236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3140107.3140130},
doi = {10.1145/3140107.3140130},
abstract = {The increased variety of learning resources, like e-books with modern collaborative tools, video lectures of different teachers across the world, lively discussion boards etc. have substantially affected learning styles of students. Having accepted such forms of learning materials, conventional academic setups are in desperate need to make learning activities efficient, effective and meaningful. In this paper a diverse resources hosting system CRETAL (Compiler of Resources in Engineering &amp;Technology to Aid Learning) with rich set of unique annotation tools are described to address the challenges of the modern students to query, access, extract, connect, process and share the important concepts from the different form of the learning materials created and adapted by the teacher. In addition to the system description, we also present the usability study along with the impact on students learning and teacher's decision when CRETAL was adapted in few undergraduate course using Educational Data mining and learning analytics.},
booktitle = {Proceedings of the 10th Annual ACM India Compute Conference},
pages = {143–148},
numpages = {6},
keywords = {Learning data, Personalized Learning, collaborative learning, video learning resources and annotation system, Recommendation System},
location = {Bhopal, India},
series = {Compute '17}
}

