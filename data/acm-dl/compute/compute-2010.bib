@inproceedings{10.1145/1754288.1754289,
author = {Kiran, R. Uday and Reddy, P. Krishna},
title = {Mining Periodic-Frequent Patterns with Maximum Items' Support Constraints},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754289},
doi = {10.1145/1754288.1754289},
abstract = {The single minimum support (minsup) based frequent pattern mining approaches like Apriori and FP-growth suffer from "rare item problem" while extracting frequent patterns. That is, at high minsup, frequent patterns consisting of rare items will be missed, and at low minsup, number of frequent patterns explode. In the literature, efforts have been made to extract rare frequent patterns under "multiple minimum support framework". In this framework, "rare frequent patterns" can be extracted by specifying minsup of the pattern using two models: minimum constraint model and maximum constraint model. In the literature, an approach has been proposed to extract only those frequent patterns which occur periodically. The basic model of periodic-frequent patterns is based on single minsup constraint. It was observed that the periodic-frequent pattern mining approach also suffers from the "rare item problem". An effort has been made to extract rare periodic-frequent patterns using minimum constraint model. In this paper, we have proposed a pattern-growth approach to extract rare periodic-frequent patterns by specifying minsup under maximum constraint model. Experiment results show that the proposed approach is efficient.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {1},
numpages = {8},
keywords = {minimum constraint model, multiple minimum supports, periodic-frequent patterns, maximum constraint model},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754290,
author = {Laha, Arijit},
title = {On the Issues of Building Information Warehouses},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754290},
doi = {10.1145/1754288.1754290},
abstract = {While performing knowledge-intensive tasks of professional nature, the knowledge workers need to access and process large volume of information. Apart from the quantity, they also require that the information received is of high quality in terms of authenticity and details. This, in turn, requires that the information delivered should also include argumentative support, exhibiting the reasoning process behind their development. In conventional document-centric practices for information management, such details are difficult to capture, represent/archive as well as retrieve/deliver. To achieve such capability we need to re-think some core issues of information management from the above requirements perspective. In this paper we develop a framework for comprehensive representation of information in archive, capturing informational contents along with their context. We shall call it the "Information Warehouse (IW)" framework of information archival. The IW is a significant yet technologically realizable conceptual advancement in supporting efficiently some interesting classes of applications, including Knowledge Work Support Systems (KWSS) outlined here, which can be very useful to the knowledge workers.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {2},
numpages = {8},
keywords = {contents and contexts, archival framework, knowledge-work support systems, information management},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754291,
author = {Breh, Rahul and Kumar, Vivek},
title = {Making Mathematical Problem Solving Exploratory and Social-Synergizing Computer Algebra Systems with Semantic and Web-2.0 Technology},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754291},
doi = {10.1145/1754288.1754291},
abstract = {Present computer algebra systems (CAS) are useful for sophisticated users, disregard the available information on the web and do not provide a social context for solving problems. In this paper we describe how semantic and web 2.0 technology can be used to make CAS as a tool to explore and learn mathematics in addition to giving mathematical problem solving a social context. We focus on design that supports social context -- actionable information [1] - for mathematical problem solving. We believe that this social context or intelligence mirrors more natural problem solving environment than the existing ones. Thus, the scope of CAS is broadened - from beginners, explorers and learners to experts. This attempt to gather intelligent information and present, from across the web, repository of already solved problems and user community reinforces the exploration and comprehension of the problem. We call this Social Computer Algebra System (SCAS). We use Sage [2] (Software for Algebra and Geometry Experimentation) as a specific case study, as Sage has browser-based interface, making it easy to put it within the semantic and web 2.0 framework.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {3},
numpages = {7},
keywords = {computer algebra systems, Sage, semantic web technology, web 2.0},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754292,
author = {Sen, Jaydip and Ukil, Arijit},
title = {A QoS-Aware End-to-End Connectivity Management Algorithm for Mobile Applications},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754292},
doi = {10.1145/1754288.1754292},
abstract = {In a dynamic heterogeneous environment, such as pervasive and ubiquitous computing, context-aware adaptation is a key concept to meet the varying requirements of different users. Context-awareness is the most promising way to manage the user information and to provide the means of communication at the right time in the right way. Connectivity and quality of service (QoS) of applications are two most important considerations that should be taken into account for designing a context-aware system. This paper presents connectivity from the view point of context awareness, identifies various relevant raw connectivity contexts, and discusses how high-level context information can be abstracted from the raw context information. It also presents a QoS-and context-aware algorithm for supporting mobile applications in a heterogeneous network environment. The unified approach towards connectivity information and QoS-awareness makes the algorithm more practical than most of the currently existing algorithms which consider connectivity and QoS separately. Simulation results show that the use of context information helps to improve the delivered application QoS.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {4},
numpages = {9},
keywords = {quality of service (QoS), context awareness, connectivity management, policy, heterogeneous network},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754293,
author = {Patil, Sangameshwar and Aphale, Guruprasad and Mehrotra, Ankit and Vin, Harrick and Kelkar, Rahul},
title = {Run-Time Dependency Tracking in Data Centers},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754293},
doi = {10.1145/1754288.1754293},
abstract = {Dependency among various machines, applications or different components of an application is an important issue for planning, operating and managing performance of data-centers and IT-infrastructure of an enterprise. Though this information is very critical, real-life experience with many data center administrators and users shows that it is often not available and it is gathered manually when the need arises. In this paper we present a method and tool for tracking the inter-dependence among IT infrastructure components in a data center using a combination of non intrusive and intrusive agent-based monitoring approach. If legal or operational constraints prevent use of the intrusive agent module, the tool still offers a lot of utility with its non-intrusive monitoring module and has zero impact on production environment. A useful feature of the tool is its ability to quantify the strength of dependency as per user-defined criteria. For web and database servers, the tool can carry out workload profiling on per-client, per-database, per-table basis without adding any overload on the servers. This significantly increases utility of the tool for additional business objectives like capacity planning and performance management. Efficacy of the solution has been proved in real-life case study carried out with large retail chain customer.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {5},
numpages = {6},
keywords = {data center, non-intrusive monitoring, dependency tracking and analysis},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754294,
author = {Pushpa, S. and Easwarakumar, K. S. and Elias, Susan and Maamar, Zakaria},
title = {Referral Based Expertise Search System in a Time Evolving Social Network},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754294},
doi = {10.1145/1754288.1754294},
abstract = {To solve some difficult problems that requires procedural knowledge, people often seek the advice of experts who have got competence in that problem domain. This paper focuses on locating and determining an expert in a particular knowledge domain. In most cases, social network of a user is explored through referrals to locate human experts. Past work in searching for experts through referrals focused primarily on static social network. However, static social network fail to accurately represent the set of experts, as in a knowledge domain as time evolves experts continuously keep changing. This paper focuses on the problem of finding experts through referrals in a time evolving co-author social network. Authors and co-authors of research publication for instance are domain experts. In this paper, we propose a solution where the network is expanded incrementally and the information on domain experts is suitably modified. This will avoid periodic global expertise recomputation and would help to effectively retrieve relevant information on domain experts. A novel data structure is also introduced in our study to effectively track the change in expertise of an author with time.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {6},
numpages = {8},
keywords = {referrals, distributed, time evolving social network, expertise search, trustworthy},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754295,
author = {Indukuri, Kishore Varma and Krishna, P. Radha},
title = {Mining E-Contract Documents to Classify Clauses},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754295},
doi = {10.1145/1754288.1754295},
abstract = {E-contracts begin as legal documents and end up as processes that help organizations abide by legal rules while fulfilling contract terms. As contracts are complex, their deployment is predominantly established and fulfilled with significant human involvement. One of the key difficulties with any kind of contract processing is the legal ambiguity, which makes it difficult to address any violation of the contract terms. Thus, there is a need to track clauses for the contract activities under execution and violation of clauses. This necessitates deriving clause patterns from e-contract documents and map to their respective activities for further monitoring and fulfillment of e-contracts during their enactment. In this paper, we present a classification approach to extract clause patterns from e-contract documents. This is a challenging task as activities and clauses are mostly derived from both legal and business process driven contract knowledge.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {7},
numpages = {5},
keywords = {text analytics, e-contracts, data mining},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754296,
author = {Jaya, A. and Uma, G. V.},
title = {An Intelligent System for Semi-Automatic Story Generation for Kids Using Ontology},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754296},
doi = {10.1145/1754288.1754296},
abstract = {Story grabs the attention of all sorts of people like young, old, children etc. Everyone is always having a special interest in either reading or listening stories. Readers may have different taste and interests and also they have their own choice of stories. Some may be fond of mystic ones or science fiction, or mythology, or romantic stories but in common people love to read stories. Creative people may land their footsteps in the art of writing stories with their own creativity and imagination. They will produce interesting themes for the stories with their innovative thoughts. A writer has the responsibility to make the readers visualize the story in their mind in order to make the story as vivid and lively. The art of writing attractive stories requires a lot of creativity and intelligence to lead the story in a right way. This Automatic story generation system provides an environment for the user to construct or rewrite the story as per their desire, through user interaction. The most attractive feature of this system is that it allows the user to select the characters, objects and location for the story in which they are constructed. Ontology helps to provide the attributes of the characters, objects and locations to the generated story. As a result, ontology preserves the meaning of system generated story in an interesting way. Ontology is a formal explicit specification of shared conceptualization. This intelligent automatic story generation system helps for dynamic construction of stories using ontology in a neat fashion.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {8},
numpages = {6},
keywords = {characters, conception of themes, semantic ordering, story generation, interaction, ontology},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754297,
author = {Sureka, Ashish and Indukuri, Kishore Varma},
title = {Linguistic Analysis of Bug Report Titles with Respect to the Dimension of Bug Importance},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754297},
doi = {10.1145/1754288.1754297},
abstract = {We perform linguistic analysis of bug-report titles obtained from the publicly available Bugzilla defect tracking tool for the open-source Firefox browser (Mozilla project) and present the results of our analysis. Our motivation is to gain insights on how people describe software defects and do a feasibility study on the possibility of building a predictive model (a classifier) for categorizing bug report based only on the titles to one of the predefined severity levels (bug importance). We observed that in general bug titles do not contain enough information for automatically predicting its importance with high accuracy. However, we noticed that two of the bug importance categories such as critical and enhancement have characteristics or features in the title that can be exploited to assign the correct severity level. We perform statistical analysis on part-of-speech, word frequency and distribution across various severity levels.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {9},
numpages = {6},
keywords = {bug report mining, data mining, text analytics},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754298,
author = {Prasad, Rajesh and Agarwal, Suneeta and Yadav, Ishadutta and Singh, Bharat},
title = {Efficient Bit-Parallel Multi-Patterns String Matching Algorithms for Limited Expression},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754298},
doi = {10.1145/1754288.1754298},
abstract = {The problem of searching occurrences of a pattern P[0...m-1] in the text T[0...n-1] with m ≤ n, where the symbols of P and T are drawn from some alphabet Σ of size σ, is called exact string matching problem. The problem of searching a set of patterns P0, P1, P2...Pr-1, r ≥ 1, in the given text T is called multi-pattern string matching problem. This problem has been previously solved by bit-parallel strings matching algorithms: shift-or and Backward non-deterministic DAWG matching (BNDM). In this paper, we extend BNDM algorithm with q-gram (B. Durian et al., 2008) for multiple patterns, where patterns are taken as "limited expressions". We define limited expression as subset of extended patterns excluding regular expression, optional and repeatable characters. Some examples are: patterns in case sensitive, patterns containing classes of characters etc. The set of r multiple patterns can be handled by converting into single pattern P by using either classes of characters or concatenating the characters of each patterns. We assume that each pattern is of equal size m and total length of pattern (after pre-processing) is less than or equal to word length (w) of computer used. We compare the performance of multi-patterns q-gram BNDM algorithm with already existing BNDM algorithm.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {10},
numpages = {6},
keywords = {BNDM, shift-or, multiple patterns, bit-parallelism, algorithm},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754299,
author = {Zacharias, Geevar C. and Lal, P. Sojan},
title = {Combining Singular Point and Co-Occurrence Matrix for Fingerprint Classification},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754299},
doi = {10.1145/1754288.1754299},
abstract = {Fingerprint classification is an important part of fingerprint identification system that works on large databases to increase its matching speed. In this paper, an automatic fingerprint classification method is proposed to classify the fingerprint images by combining singular points and Gray Level Co-occurrence Matrix (GLCM) features. Co-occurrence matrices can be used to extract features from the fingerprint image because these are composed of regular texture patterns. First, the fingerprint image is preprocessed and a unique reference point is detected to determine a Region-of-Interest (ROI). ROI is then partitioned into 4 different regions to extract 4 sets of 4 GLCM features from each region. To achieve this, 4 co-occurrence matrixes are computed from each region with a predefined set of parameters. A feature vector consisting of 64 features is used to train a feed-forward neural network for classifying the input image into 5 different classes. The accuracy of 97.14% with no rejection is achieved and the experiment result shows that the method is reliable for fingerprint classification.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {11},
numpages = {6},
keywords = {GLCM, fingerprint classification, co-occurrence matrix},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754300,
author = {Girisha, R. and Murali, S.},
title = {Self Shadow Elimination Algorithm for Surveillance Videos Using ANOVA F Test},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754300},
doi = {10.1145/1754288.1754300},
abstract = {Identifying moving objects from a video sequence is a fundamental and critical task in many computer vision applications and a robust segmentation of motion objects from the static background is generally required. Segmented foreground objects generally include their self shadows as foreground objects since the shadow intensity differs and gradually changes from the background in a video sequence. Moreover, self shadows are vague in nature and have no clear boundaries. To eliminate such shadows from motion segmented video sequences, we propose an algorithm based on inferential statistical one way ANalysis Of VAriance (ANOVA) F test. This statistical model can deal scenes with complex and time varying illuminations without restrictions on the number of light sources and surface orientations. Results obtained with different indoor and outdoor sequences show that algorithm can effectively and robustly detect associated self shadows from segmented frames.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {12},
numpages = {8},
keywords = {self shadows, critical value, motion segmentation, video surveillance, ANOVA F test},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754301,
author = {Ragunathan, T. and Reddy, P. Krishna},
title = {Performance Evaluation of Speculation-Based Protocol for Read-Only Transactions},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754301},
doi = {10.1145/1754288.1754301},
abstract = {In the literature, speculation-based protocols have been proposed to improve the performance of read-only transactions (ROTs) over the existing two-phase locking (2PL) and snapshot isolation(SI)-based protocols. In this paper, we have compared the performance of asynchronous speculation-based protocol with 2PL and SI-based protocols through analytical and simulation methods. The results show that asynchronous speculation-based protocol improves the performance over 2PL and SI-based protocols.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {13},
numpages = {4},
keywords = {performance evaluation, speculation, concurrency control, transaction management},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754302,
author = {Singh, Meena Dilip and Krishna, P. Radha and Saxena, Ashutosh},
title = {A Cryptography Based Privacy Preserving Solution to Mine Cloud Data},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754302},
doi = {10.1145/1754288.1754302},
abstract = {Due to increased adoption of cloud computing, there is a growing need of addressing the data privacy during mining. On the other hand, knowledge sharing is a key to survive many business organizations. Several attempts have been made to mine the data in distributed environment however, maintaining the privacy while mining the data over cloud is a challenging task. In this paper, we present an efficient and practical cryptographic based scheme that preserves privacy and mine the cloud data which is distributed in nature. In order to address the classification task, our approach uses k-NN classifier. We extend the Jaccard measure to find the similarity between two encrypted and distributed records by conducting an equality test. In addition, our approach accelerates mining by finding nearest neighbours at local and then at global level. The proposed approach avoids transmitting the original data and sharing of the key that is required in traditional crypto based privacy preserving data mining solutions.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {14},
numpages = {4},
keywords = {cryptography, private equality test, privacy preserving data mining, encrypted data},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754303,
author = {Sood, Sandeep K. and Sarje, Anil K. and Singh, Kuldip},
title = {An Improvement of Xu et al.'s Authentication Scheme Using Smart Cards},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754303},
doi = {10.1145/1754288.1754303},
abstract = {In 2009, Xu et al. found that Lee et al.'s [3] scheme is vulnerable to offline password guessing attack. Xu et al. also demonstrated that Lee and Chiu's [4] scheme is vulnerable to forgery attack. Furthermore, Lee and Chiu's scheme does not achieve mutual authentication and thus can not resist malicious server attack. Therefore, Xu et al. proposed an improved scheme that inherits the merits of Lee et al.'s and Lee and Chiu's schemes and resists different possible attacks. However, we found that Xu et al.'s scheme is vulnerable to forgery attack. This paper presents an improved scheme to resolve the aforementioned problem, while keeping the merits of Xu et al.'s scheme.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {15},
numpages = {5},
keywords = {password, authentication protocol, network security, smart card, cryptography, hash function},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754304,
author = {Bhadani, Abhay and Chaudhary, Sanjay},
title = {Performance Evaluation of Web Servers Using Central Load Balancing Policy over Virtual Machines on Cloud},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754304},
doi = {10.1145/1754288.1754304},
abstract = {Cloud Computing adds more power to the existing Internet technologies. Virtualization harnesses the power of the existing infrastructure and resources. With virtualization we can simultaneously run multiple instances of different commodity operating systems. Since we have limited processors and jobs work in concurrent fashion, overload situations can occur. Things become even more challenging in distributed environment. We propose Central Load Balancing Policy for Virtual Machines (CLBVM) to balance the load evenly in a distributed virtual machine/cloud computing environment. This work tries to compare the performance of web servers based on our CLBVM policy and independent virtual machine(VM) running on a single physical server using Xen Virtualizaion. The paper discusses the efficacy and feasibility of using this kind of policy for overall performance improvement.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {16},
numpages = {4},
keywords = {load balancing, cloud computing, resource allocation, virtualization, distributed computing, live migration},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754305,
author = {Sashi, K. and Thanamani, Antony Selvadoss},
title = {A New Dynamic Replication Algorithm for European Data Grid},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754305},
doi = {10.1145/1754288.1754305},
abstract = {The term grid computing refers to the emerging computational and networking infrastructure that is designed to provide pervasive and reliable access to data and computational resources over wide area network, across organizational domains. Grid computing has the potential to support different kinds of applications such as high energy physics, climate simulation astronomy and earth sciences. These applications handle large data sets that need to be used among different grid sites. Data Grids support data-intensive applications in wide area grid systems. To speed up data access, data grid systems replicate data in multiple locations so a user can access the data from a nearby site. The motivation of replication is that replication can improve data availability, data access performance, and load balancing. In this paper a new Dynamic Replication Algorithm (DRA) is proposed. DRA improves data availability by replicating files to different locations within the cluster, where a cluster is a network topological space where the sites are located closely and by storing the replica in the most popular site. Data access performance is increased by minimizing the job execution time and minimizing the network usage. It is implemented by using a data grid simulator, OptorSim Developed by European Data Grid projects.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {17},
numpages = {4},
keywords = {data grid, static replication, grid computing, European data grid, dynamic replication},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754306,
author = {Guru, D. S. and Harish, B. S. and Manjunath, S.},
title = {Symbolic Representation of Text Documents},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754306},
doi = {10.1145/1754288.1754306},
abstract = {This paper presents a novel method of representing a text document by the use of interval valued symbolic features. A method of classification of text documents based on the proposed representation is also presented. The newly proposed model significantly reduces the dimension of feature vectors and also the time taken to classify a given document. Further, extensive experimentations are conducted on vehicles-wikipedia datasets to evaluate the performance of the proposed model. The experimental results reveal that the obtained results are on par with the existing results for vehicles-wikipedia dataset. However, the advantage of the proposed model is that it takes relatively a less time for classification as it is based on a simple matching strategy.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {18},
numpages = {4},
keywords = {text classification, symbolic representation, pattern recognition, text document, classification},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754307,
author = {Guru, D. S. and Manjunath, S. and Kiranagi, Bapu B.},
title = {SVARS: Symbolic Video Archival and Retrieval System},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754307},
doi = {10.1145/1754288.1754307},
abstract = {In this paper, we present a novel approach for higher level semantic video archival and retrieval based on symbolic representation. Topological triangular spatial relationship has been introduced to study the higher level relationship (spatial and topological) among the objects in a frame. A two dimensional time series representation to preserve the variations in relationship among objects across frames is also proposed. A modification is recommended to make Dynamic Time Wrapping (DTW) suit for matching and retrieval of symbolic videos. To corroborate the efficacy of the proposed method an experimentation has been carried out on sports videos and results are encouraging.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {19},
numpages = {4},
keywords = {topological triangular spatial relationship, video archival and retrieval, symbolic representation, 2D - time series representation},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754308,
author = {Babu, Ramesh and Srinivasan, Jagannathan},
title = {Subsequent Patient Visit Detection in a High Volume OPD Using Record Linkage Techniques},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754308},
doi = {10.1145/1754288.1754308},
abstract = {Record or data linkage techniques are used to link records which represent the same entity (e.g. patient, customer, citation, etc.) in one or more data sets where a unique identifier for each entity is not available in all or any of the data sets to be linked. A data set that has undergone record linkage is said to be linked. We consider the problem of discovering subsequent patient visit records in a high volume Out Patient Department (OPD), which due to various reasons (including loss of patient ID card and lack of time in ascertaining link to prior visit) is recorded as individual visits. We present a solution to this problem by adapting record linkage techniques that are typically used for identifying duplicate records. An experimental study conducted on real data obtained from a hospital's Out Patient Department demonstrates the effectiveness of our scheme.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {20},
numpages = {5},
keywords = {deduplication, record linkage},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754309,
author = {Parichha, Barun Kumar and Gonsalves, T. A.},
title = {Remote Device Support in Thin Client Network},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754309},
doi = {10.1145/1754288.1754309},
abstract = {Remote device sharing to thin-client has now become a challenging and interesting field of study in computing technology. At present, thin clients run on protocols such as X-11, RDP, VNC which have only limited support for devices other than keyboard and mouse. The present literature does not reveal sufficient information to support such peripheral devices to thin clients. In this paper, we proposed an unique device support mechanism called "Universal extension of USB bus over IP network" for thin-clients. It allows the users to access the USB devices under constrained network bandwidth without adding any device specific driver to thin client.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {21},
numpages = {4},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754310,
author = {Sathiyamurthy, K. and Geetha, T. V.},
title = {Association of Domain Concepts with Educational Objectives for E-Learning},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754310},
doi = {10.1145/1754288.1754310},
abstract = {Rapid growth of web based courses for education and training impose challenges to e-learning systems to generate content according to the level of the learner. Automatic association of particular domain concept with context or pedagogical role of the e-learning materials is made difficult as much of them are in raw form. Blooms taxonomy categories level of educational learning. Identification of documents to particular level of this taxonomy enables e-learning systems to match learner needs. In this work domain ontology based on ACM classification and for pedagogical categorization context ontology was developed. Documents are annotated by their association of domain concepts with educational objectives based on devised algorithms for extraction and ranking of domain and context vocabularies.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {22},
numpages = {4},
keywords = {learning objects, pedagogy, blooms taxonomy},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754311,
author = {Subashini, P. and Krishnaveni, M. and Thakur, Suresh Kumar},
title = {Quantitative Performance Evaluation on Segmentation Methods for SAR Ship Images},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754311},
doi = {10.1145/1754288.1754311},
abstract = {Segmentation of target is an important step in forming realistic target models. To assist in classifying the relevant literature, two types of segmentation are identified where each type adds its own additional level of uniqueness. The first type is composed of the simplest forms of image analysis through thresholding, the second is characterized by region based in which the application meets the uncertainty models and optimization effects during segmentation process. The work proposed in this paper explores the strength and weaknesses of the methods and are analyzed for practical purposes. The progress towards the work is validated with SAR image database in which objective and subjective quantitative performance is clearly identified. The comparison is based on the potential performance measures. The methods show special strength in providing designers with an adequate degree of freedom in choosing the proper objects of the SAR image for their application purposes.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {23},
numpages = {5},
keywords = {region finding, threshold, metrics, segmentation, image quality, SAR},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754312,
author = {Hariharan, Shanmugasundaram and Srimathi, Ramachandran and Sivasubramanian, Murugan and Pavithra, Saranathan},
title = {Opinion Mining and Summarization of Reviews in Web Forums},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754312},
doi = {10.1145/1754288.1754312},
abstract = {The Internet has made life of every individual (web users) very simple and sophisticated. In recent years people use the web for many reasons like personal communication, entertainment, online shopping, general search and so on. Internet forums also act as a medium of exchange for sharing resources and knowledge. Though commercial review websites allow users to express their opinions in whatever way they feel, number of reviews for specific product available is enormous. Hence it becomes difficult for the customers to read all the reviews to make a decision. In this paper we propose an extraction technique to score the reviews and summarize the opinions to end user. Based on opinions mined it is decided as whether to recommend the product to the user or not. This paper mainly focuses on providing a methodology for mining the opinions using generic user focused reviews. The experiments performed were quite promising for the data set used.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {24},
numpages = {4},
keywords = {sentiment analysis, product reviews, summarization, recommendation, opinion mining},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754313,
author = {Gomathi, M. and Thangaraj, P.},
title = {Automated CAD for Detection of Lung Nodule Using CT Scans},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754313},
doi = {10.1145/1754288.1754313},
abstract = {The main objective of this paper is to evaluate the performance of the Computer-Aided Detection (CAD) system for automated nodule detection in lungs using CT scan images. The CAD system is applied to CT scans collected in a screening program for lung cancer detection. Each scan consists of a sequence of about 300 slices stored in DICOM (Digital Imaging and Communications in Medicine) format. All true nodules were detected and a very low false-positive detection rate was achieved. The automated extraction of the pulmonary parenchyma in CT images is the most important step in a CAD system. In this paper we describe a method, consisting of techniques which are helpful for the automatic identification of the pulmonary volume. The performance is evaluated as a fully automated computerized method for the detection of lung nodules in computed tomography (CT) scans},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {25},
numpages = {4},
keywords = {pulmonary nodules, lung cancer, computer assisted diagnosis, computer aided diagnosis, computed tomography, CT},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754314,
author = {Karthik, S. Venkatesh and Srikant, R. and Madhu, R. M.},
title = {Feature Selection &amp; Dominant Feature Selection for Product Reviews Using Meta-Heuristic Algorithms},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754314},
doi = {10.1145/1754288.1754314},
abstract = {In this paper, Harmony Search is introduced as a meta-heuristic that has a stronger presence with respect to intensification and a tuned version of Genetic algorithms, with respect to diversification. An approach to solving the Feature selection (FS) problem using Harmony Search is proposed. The problem of Dominant Feature Selection (DFS) is introduced with respect to product reviews and two solutions to the problem, one based on Genetic Algorithms and another, based on Harmony Search are proposed. By experimental evaluation, we conclude that the Feature Selection problem is best solved by a meta-heuristic that is stronger with respect to intensification (Harmony Search in our case) and that the Dominant Feature Selection is best solved by a meta-heuristic that is stronger with respect to diversification. This paper aims to give a brief guide to the judicious choice of meta-heuristics to solve problems in Text Mining.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {26},
numpages = {4},
keywords = {text mining, meta-heuristic algorithms, harmony search, feature selection, dominant feature selection, GA},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754315,
author = {Hota, Rudra N. and Jonna, Kishore and Krishna, P. Radha},
title = {On-Road Vehicle Detection by Cascaded Classifiers},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754315},
doi = {10.1145/1754288.1754315},
abstract = {We present an efficient algorithm for on-road vehicle (e.g. side and rear view of cars) detection problem using cascade of boosted classifiers. Adaptive boosting based classifier in cascaded structure is one of the few good approaches for object detection. This approach filters different non-target (negative) samples in different stages of cascaded structure according to their level of similarity with target object class. The boosted weak learners are quick and efficient for initial stages only, but in later stage of cascaded structure they are not efficient enough to remove the critical false alarms. In this paper, we propose a method of cascading complex features at the later stage of cascaded classifier to enhance the detection performance. We compared the performance of local and global texture features in combination with boosted haar like features. The best performance for on-road obstacle detection is achieved by Adaboost with Haar-like feature along with SVM and Histograms of Oriented Gradients (HOG) features.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {27},
numpages = {5},
keywords = {texture feature analysis, object detection, intelligent transport system, on-road obstacles, cascaded classifiers},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754316,
author = {Tiwari, Raj Gaurang and Husain, Mohd. and Gupta, Sandeep and Srivastava, Arun Pratap},
title = {A New Ant Colony Optimization Meta-Heuristic Algorithm to Tackle Large Optimization Problem.},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754316},
doi = {10.1145/1754288.1754316},
abstract = {NOTE FROM ACM: It has been determined that the contents of this paper plagiarize that of a previously published work. ACM has therefore shut off access to this paper.The previously published works that were plagiarized are:Optimizing large scale problems using multiple ant colonies algorithm based on pheromone evaluation technique A. Aljanaby, K.R. Ku Mahamud, and N.M. Norwawi may be found andA new multiple ant colonies optimization algorithm utilizing average pheromone evaluation mechanism by Alaa Aljanaby, Ku Ruhana Ku-Mahamud and Norita Md. Norwawi may be found  For further information, contact the },
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {28},
numpages = {4},
keywords = {artificial and swarm intelligence, ant colony optimization, combinatorial optimization problems, meta-heuristic algorithms},
location = {Bangalore, India},
series = {COMPUTE '10}
}

@inproceedings{10.1145/1754288.1754317,
author = {Anuradha, J. and Tisha and Ramachandran, Varun and Arulalan, K. V. and Tripathy, B. K.},
title = {Diagnosis of ADHD Using SVM Algorithm},
year = {2010},
isbn = {9781450300018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1754288.1754317},
doi = {10.1145/1754288.1754317},
abstract = {Attention Deficit Hyperactivity Disorder (ADHD) is a Disruptive Behaviour Disorder characterized by the presence of a set of chronic and impairing behaviour patterns that display abnormal levels of inattention, hyperactivity, or their combination. Since most individuals especially children display these behaviours from time to time, it is be difficult to differentiate behaviours that reflect ADHD from those that are a normal part of growing up which makes the diagnosis a tricky job. In this paper, we apply a well known artificial intelligence technique, the SVM algorithm, for the diagnosis of the disorder. The major advantage of using SVM is that it helps in controlling the complexity of the problem of diagnosing. There has not been much development or research on ADHD using SVM algorithm. Hence this is the first attempt at diagnosing the problems using the algorithm. To improve on the overall identification accuracy; we also make use of the GA-based, Feature Selection Algorithm. Genetic algorithms are known to give good solution to very complex problems. In conclusion, we expect that AI techniques like SVM will certainly play an essential role in future ADHD diagnosis applications.},
booktitle = {Proceedings of the Third Annual ACM Bangalore Conference},
articleno = {29},
numpages = {4},
location = {Bangalore, India},
series = {COMPUTE '10}
}

