@inproceedings{10.5555/2100584.2100585,
author = {Allen, David and Darwiche, Adnan},
title = {New Advances in Inference by Recursive Conditioning},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recursive Conditioning (RC) was introduced recently as an any-space algorithm for inference in Bayesian networks which can trade time for space by varying the size of its cache at the increment needed to store a floating point number. Under full caching, RC has an asymptotic time and space complexity which is comparable to mainstream algorithms based on variable elimination and clustering (exponential in the network treewidth and linear in its size). We show two main results about RC in this paper. First, we show that its actual space requirements under full caching are much more modest than those needed by mainstream methods and study the implications of this finding. Second, we show that RC can effectively deal with determinism in Bayesian networks by employing standard logical techniques, such as unit resolution, allowing a significant reduction in its time requirements in certain cases. We illustrate our results using a number of benchmark networks, including the very challenging ones that arise in genetic linkage analysis.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {2–10},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100586,
author = {Azari, David and Horvitz, Eric and Dumais, Susan and Brill, Eric},
title = {Web-Based Question Answering: A Decision-Making Perspective},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We investigate the use of probabilistic models and cost-benefit analyses to guide the operation of a Web-based question-answering system. We first provide an overview of research on questionanswering systems. Then, we present details about AskMSR, a prototype question-answering system that synthesizes answers from the results of queries to a Web search engine. We describe Bayesian analyses of the quality of answers generated by the system and show how we can endow the system with the ability to make decisions about the nature and number of queries that should be issued, by considering the expected value and cost of submitting the queries. Finally, we review the results of a set of experiments.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {11–19},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100587,
author = {Bacchus, Fahiem and Dalmao, Shannon and Pitassi, Toniann},
title = {Value Elimination: Bayesian Inference via Backtracking Search},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present Value Elimination, a new algorithm for Bayesian Inference. Given the same variable ordering information, Value Elimination can achieve performance that is within a constant factor of variable elimination or recursive conditioning, and on some problems it can perform exponentially better, irrespective of the variable ordering used by these algorithms. Value Elimination's other features include: (1) it can achieve the same space-time tradeoff guarantees as recursive conditioning; (2) it can utilize all of the logical reasoning techniques used in state of the art SAT solvers; these techniques allow it to obtain considerable extra mileage out of zero entries in the CPTs; (3) it can be naturally and easily extended to take advantage of context specific structure; and (4) it supports dynamic variable orderings which might be particularly advantageous in the presence of context specific structure. We have implemented a version of Value Elimination that demonstrates very promising performance, often being one or two orders of magnitude faster than a commercial Bayes inference engine, despite the fact that it does not as yet take advantage of context specific structure.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {20–28},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100588,
author = {Benferhat, Salem and Lagrue, Sylvain and Papini, Odile},
title = {A Possibilistic Handling of Partially Ordered Information},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In standard possibilistic logic, prioritized information are encoded by means of weighted knowledge bases. This paper proposes an extension of possibilistic logic to deal with partially ordered information which can be viewed as a family of possibilistic knowledge bases.We show that all basic notions of possibilistic logic have natural counterparts when dealing with partially ordered information. Furthermore, we propose an algorithm which computes plausible conclusions of a partially ordered knowledge base.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {29–36},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100589,
author = {Bidyuk, Bozhena and Dechter, Rina},
title = {An Empirical Study of W-Cutset Sampling for Bayesian Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The paper studies empirically the time-space trade-off between sampling and inference in the cutser sampling algorithm. The algorithm samples over a subset of nodes in a Bayesian network and applies exact inference over the rest. As the size of the sampling space decreases, requiring less samples for convergence, the time for generating each single sample increases. Algorithm w-cutset sampling selects a sampling set such that the induced-width of the network when the sampling set is observed is bounded by w, thus requiring inference whose complexity is exponentially bounded by w. In this paper, we investigate the performance of w-cutset sampling as a function of w. Our experiments over a range of randomly generated and real benchmarks, demonstrate the power of the cutset sampling idea and in particular show that an optimal balance between inference and sampling benefits substantially from restricting the cutset size, even at the cost of more complex inference.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {37–46},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100590,
author = {Bilmes, Jeff A. and Bartels, Chris},
title = {On Triangulating Dynamic Graphical Models},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper introduces improved methodology to triangulate dynamic graphical models and dynamic Bayesian networks (DBNs). In this approach, a standard DBN template can be modified so the repeating and unrolled graph section may dissect the original DBN time slice and may also span (and partially intersect) many such slices. We introduce the notion of a "boundary" which divides a graph into multi-slice partitions each of which has an interface, and define the "boundary algorithm", a method to find the best boundary (and corresponding interface) between partitions in such models. We prove that, after using this algorithm, the sizes of the best forwardand backward- interface (and also the corresponding fill-ins) are identical. The boundary algorithm allows for constrained elimination orders (and therefore graph triangulations) that are impossible using standard slice-by-slice constrained elimination. We describe the above using the Graphical Model ToolKit (GMTK)'s notion of dynamic graphical model, slightly generalizing standard DBN templates. We report triangulation results on hand-concocted graphs, novel speech recognition DBNs, and random graphs, and find that the boundary algorithm can significantly improve both tree width and graph weight.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {47–56},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100591,
author = {Bishop, Christopher M. and Svenskn, Markus},
title = {Bayesian Hierarchical Mixtures of Experts},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The Hierarchical Mixture of Experts (HME) is a well-known tree-structured model for regression and classification, based on soft probabilistic splits of the input space. In its original formulation its parameters are determined by maximum likelihood, which is prone to severe overfitting, including singularities in the likelihood function. Furthermore the maximum likelihood framework offers no natural metric for optimizing the complexity and structure of the tree. Previous attempts to provide a Bayesian treatment of the HME model have relied either on local Gaussian representations based on the Laplace approximation, or have modified the model so that it represents the joint distribution of both input and output variables, which can be wasteful of resources if the goal is prediction. In this paper we describe a fully Bayesian treatment of the original HME model based on variational inference. By combining 'local' and 'global' variational methods we obtain a rigorous lower bound on the marginal probability of the data under the model. This bound is optimized during the training phase, and its resulting value can be used for model order selection. We present results using this approach for data sets describing robot arm kinematics.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {57–64},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100592,
author = {Bobbio, Andrea and Montani, Stefania and Portinale, Luigi},
title = {Parametric Dependability Analysis through Probabilistic Horn Abduction},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Dependability modeling and evaluation is aimed at investigating that a system performs its function correctly in time. A usual way to achieve a high reliability is to design redundant systems that contain several replicas of the same subsystem. In order to provide compactness in system representation, parametric system modeling has been investigated in the jterature: a set of replicas of a given subsystem is parameterized so that only one representative instance is explicitly included in the model. While modeling aspects can be suitably addressed by these approaches, analytical tools working on parametric characterizations are often more difficult to be defined; the standard approach consists in "unfolding" the parametric model, in order to exploit standard analysis algorithms working at the unfolded "ground" level. In the present paper we consider the formalism of Parametric Fault Tree (PFT) and we show how it can be related to Probabilistic Horn Abduction (PHA). Since PHA is a framework where both modeling and analysis can be performed in a restricted firstorder language, we aim at showing that the conversion of a PFT into a PHA theory allows for an approach to dependability analysis directly exploiting parametric representation. We will show that classical qualitative and quantitative dependability measures can be characterized within PHA; this makes the PHA framework a candidate for PFT analysis, where also posterior probability computation (often neglected in standard Fault Tree analysis) can be naturally performed. A simple example of a multi-processor system with several replicated units is used to illustrate the approach.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {65–72},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100593,
author = {Bolt, Janneke H. and Renooij, Silja and van der Gaag, Linda C.},
title = {Upgrading Ambiguous Signs in QPNs},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A qualitative probabilistic network models the probabilistic relationships between its variables by means of signs. Nonmonotonic influences have associated an ambiguous sign. These ambiguous signs typically give rise to uninformative results upon inference. We argue that a non-monotonic influence can be associated with a more informative sign that indicates its effect in the current state of the network. To capture this effect, we introduce the concept of situational sign. Furthermore, if the network converts to a state in which all variables that provoke the nonmonotonicity have been observed, a nonmonotonic influence reduces to a monotonic one. We study the persistence and propagation of situational signs upon inference and give a method for establishing the sign of a reduced influence.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {73–80},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100594,
author = {Booth, Richard and Richter, Eva},
title = {On Revising Fuzzy Belief Bases},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We look at the problem of revising fuzzy belief bases, i.e., belief base revision in which both formulas in the base as well as revisioninput formulas can come attached with varying truth-degrees. Working within a very general framework for fuzzy logic which is able to capture certain types of uncertainty calculi as well as truth-functional fuzzy logics, we show how the idea of rational change from "crisp" base revision, as embodied by the idea of partial meet (base) revision, can be faithfully extended to revising fuzzy belief bases. We present and axiomatise an operation of partial meet fuzzy base revision and illustrate how the operation works in several important special instances of the framework.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {81–88},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100595,
author = {Boutilier, Craig and Das, Rajarshi and Kephart, Jeffrey O. and Tesauro, Gerald and Walsh, William E.},
title = {Cooperative Negotiation in Autonomic Systems Using Incremental Utility Elicitation},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Decentralized resource allocation is a key problem for large-scale autonomic (or self-managing) computing systems. Motivated by a data center scenario, we explore efficient techniques for resolving resource conflicts via cooperative negotiation. Rather than computing in advance the functional dependence of each element's utility upon the amount of resource it receives, which could be prohibitively expensive, each element's utility is elicited incrementally. Such incremental utility elicitation strategies require the evaluation of only a small set of sampled utility function points, yet they find near-optimal allocations with respect to a minimax regret criterion. We describe preliminary computational experiments that illustrate the benefit of our approach.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {89–97},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100596,
author = {Boutilier, Craig and Zemel, Richard S. and Marlin, Benjamin},
title = {Active Collaborative Filtering},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Collaborative filtering (CF) allows the preferences of multiple users to be pooled to make recommendations regarding unseen products. We consider in this paper the problem of online and interactive CF: given the current ratings associated with a user, what queries (new ratings) would most improve the quality of the recommendations made? We cast this in terms of expected value of information (EVOI); but the online computational cost of computing optimal queries is prohibitive. We show how ofline prototyping and computation of bounds on EVOI can be used to dramatically reduce the required online computation. The framework we develop is general, but we focus on derivations and empirical study in the specific case of the multiplecause vector quantization model.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {98–106},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100597,
author = {Chan, Hei and Darwiche, Adnan},
title = {Reasoning about Bayesian Network Classifiers},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Bayesian network classifiers are used in many fields, and one common class of classifiers are naive Bayes classifiers. In this paper, we introduce an approach for reasoning about Bayesian network classifiers in which we explicitly convert them into Ordered Decision Diagrams (ODDS), which are then used to reason about the properties of these classifiers. Specifically, we present an algorithm for converting any naive Bayes classifier into an ODD, and we show theoretically and experimentally that this algorithm can give us an ODD that is tractable in size cvcn given an intractable number of instances. Since ODDS are tractable representations of classifiers, our algorithm allows us to efficiently test the equivalence of two naive Bayes classifiers and characterize discrepancies between them. We also show a number of additional results including a count of distinct classifiers that can be induced by changing some CPT in a naive Bayes classifier, and the range of allowable changes to a CPT which keeps the current classifier unchanged.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {107–115},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100598,
author = {Chaudhuri, Sanjay and Richardson, Thomas},
title = {Using the Structure of D-Connecting Paths as a Qualitative Measure of the Strength of Dependence},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Pearl's concept of a d-connecting path is one of the foundations of the modern t h e ory of graphical models: the absence of a d-connecting path in a DAG indicates that conditional independence will hold in any distribution factorizing according to that graph. In this paper we show that in singlyconnected Gaussian DAGs it is possible to use the form of a d-connecting path to obtain qualitative information about the strength of conditional dependence. More precisely, the squared partial correlations between two given variables, conditioned on different subsets may be partially ordered by examining the relationship between the d-connecting path and the set of variables conditioned upon.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {116–123},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100599,
author = {Chickering, David Maxwell and Meek, Christopher and Heckerman, David},
title = {Large-Sample Learning of Bayesian Networks is NP-Hard},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, we provide new complexity results for algorithms that learn discretevariable Bayesian networks from data. Our results apply whenever the learning algorithm uses a scoring criterion that favors the simplest model able to represent the generative distribution exactly. Our results therefore hold whenever the learning algorithm uses a consistent scoring criterion and is applied to a sufficiently large dataset. We show that identifying high-scoring structures is NP-hard, even when we are given an independence oracle, an inference oracle, and/or an information oracle. Our negative results also apply when learning discrete-variable Bayesian networks in which each node has at most k parents, for all k ≥ 3.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {124–133},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100600,
author = {Chudova, Darya and Gaffney, Scott and Smyth, Padhraic},
title = {Probabilistic Models for Joint Clustering and Time-Warping of Multidimensional Curves},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper we present a family of models and learning algorithms that can simultaneously align and cluster sets of multidimensional curves measured on a discrete time grid. Our approach is based on a generative mixture model that allows both local nonlinear time warping and global linear shifts of the observed curves in both time and measurement spaces relative to the mean curves within the clusters. The resulting model can be viewed as a form of Bayesian network with a special temporal structure. The Expectation-Maximization (EM) algorithm is used to simultaneously recover both the curve models for each cluster, and the most likely alignments and cluster membership for each curve. We evaluate the methodology on two real-world data sets, and show that the Bayesian network models provide systematic improvements in predictive power over more conventional clustering approaches.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {134–141},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100601,
author = {de Cooman, Gert and Zaffalon, Marco},
title = {Updating with Incomplete Observations},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete (or setvalued). This is a fundamental problem, and of articular interest for Bayesian networks. Recently, Griinwald and Halpern have shown that commonly used updating strategies fail here, except under very special assumptions. We propose a new rule for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no or weak assumptions about the so-called incompleteness mechanism that produces incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we derive a new updating rule using coherence arguments. In general, our rule produces lower posterior probabilities, as well as partially determinate decisions. This is a logical consequence of the ignorance about the incompleteness mechanism. We show how the new rule can properly address the apparent paradox in the 'Monty Hall' puzzle. In addition, we apply it to the classification of new evidence in Bayesian networks constructed using expert knowledge. We provide an exact algorithm for this task with linear-time complexity, also for multiply connected nets.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {142–150},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100602,
author = {Corduneanu, Adrian and Jaakkola, Tommi},
title = {On Information Regularization},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We formulate a principle for classification with the knowledge of the marginal distribution over the data points (unlabeled data). The principle is cast in terms of Tikhonov style regularization where the regularization penalty articulates the way in which the marginal density should constrain otherwise unrestricted conditional distributions. Specifically, the regularization penalty penalizes any information introduced between the examples and labels beyond what is provided by the available labeled examples. The work extends (Szummer and Jaakkola, 2003) to multiple dimensions, providing a regularizer independent of the covering of the space used in the derivation. In addition we lay the learning theoretical framework for classification with information regularization and provide a sample complexity bound. We illustrate the regularization principle in practice by restricting the class of conditional distributions to be logistic regression models and constructing the regularization penalty from a finite set of unlabeled examples.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {151–158},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100603,
author = {Crick, Christopher and Pfeffer, Avi},
title = {Loopy Belief Propagation as a Basis for Communication in Sensor Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Sensor networks are an exciting new kind of computer system. Consisting of a large number of tiny, cheap computational devices physically distributed in an environment, they gather and process data about the environment in real time. One of the central questions in sensor networks is what to do with the data, i.e. how to reason with it and how to communicate it. This paper argues that the lessons of the UAI community, in particular that one should produce and communicate beliefs rather than raw sensor values, are highly relevant to sensor networks. We contend that loopy belief propagation is particularly well suited to communicating beliefs in sensor networks, due to its compact implementation and distributed nature. We investigate the ability of loopy belief propagation to function under the stressful conditions likely to prevail in sensor networks. Our experiments show that it performs well and degrades gracefully. It converges to appropriate beliefs even in highly asynchronous settings where some nodes communicate far less frequently than others; it continues to function if some nodes fail to participate in the propagation process; and it can track changes in the environment that occur while beliefs are propagating. As a result, we believe that sensor networks present an important application opportunity for UAI.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {159–166},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100604,
author = {Dash, Denver and Druzdzel, Marek J.},
title = {Robust Independence Testing for Constraint-Based Learning of Causal Structure},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper considers a method that combines ideas from Bayesian learning, Bayesian network inference, and classical hypothesis testing to produce a more reliable and robust test of independence for constraintbased (CB) learning of causal structure. Our method produces a smoothed contingency table Nxyz that can be used with any test of independence that relies on contingency table statistics. Nxyz can be calculated in the same asymptotic time and space required to calculate a standard contingency table, allows the specification of a prior distribution over parameters, and can be calculated when the database is incomplete. We provide theoretical justification for the procedure, and with synthetic data we demonstrate its benefits empirically over both a CB algorithm using the standard contingency table, and over a greedy Bayesian algorithm. We show that, even when used with noninformative priors, it results in better recovery of structural features and it produces networks with smaller KL-Divergence, especially as the number of nodes increases or the number of records decreases. Another benefit is the dramatic reduction in the probability that a CB algorithm will stall during the search, providing a remedy for an annoying problem plaguing CB learning when the database is small.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {167–174},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100605,
author = {Dechter, Rina and Mateescu, Robert},
title = {A Simple Insight into Iterative Belief Propagation's Success},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In non-ergodic belief networks the posterior belief of many queries given evidence may become zero. The paper shows that when belief propagation is applied iteratively over arbitrary networks (the so called, iterative or loopy belief propagation (IBP)) it is identical to an arc-consistency algorithm relative to zero-belief queries (namely assessing zero posterior probabilities). This implies that zero-belief conclusions derived by belief propagation converge and are sound. More importantly, it suggests that the inference power of IBP is as strong and as weak as that of arcconsistency. This allows the synthesis of belief networks for which belief propagation is useless on one hand, and focuses the investigation on classes of belief networks for which belief propagation may be zero-complete. Finally, we show empirically that IBP's accuracy is correlated with extreme probabilities, therefore explaining its success over coding applications.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {175–183},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100606,
author = {Drton, Mathias and Richardson, Thomas S.},
title = {A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical Models for Marginal Independence},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Graphical models with bi-directed edges (↔) represent marginal independence: the absence of an edge between two vertices indicates that the corresponding variables are marginally independent. In this paper, we consider maximum likelihood estimation in the case of continuous variables with a Gaussian joint distribution, sometimes termed a covariance graph model. We present a new fitting algorithm which exploits standard regression techniques and establish its convergence properties. Moreover, we contrast our procedure to existing estimation algorithms.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {184–191},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100607,
author = {Eiter, Thomas and Lukasiewicz, Thomas},
title = {Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present the language PC+ for probabilistic reasoning about actions, which is a generalization of the action language C+ that allows to deal with probabilistic as well as nondeterministic effects of actions. We define a formal semantics of PC+ in terms of probabilistic transitions between sets of states. Using a concept of a history and its belief state, we then show how several important problems in reasoning about actions can be concisely formulated in our formalism.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {192–199},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100608,
author = {Elidan, Gal and Friedman, Nir},
title = {The Information Bottleneck EM Algorithm},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Learning with hidden variables is a central challenge in probabilistic graphical models that has important implications for many real-life problems. The classical approach is using the Expectation Maximization (EM) algorithm. This algorithm, however, can get trapped in local maxima. In this paper we explore a new approach that is based on the Information Bottleneck principle. In this approach, we view the learning problem as a tradeoff between two information theoretic objectives. The first is to make the hidden variables uninformative about the identity of specific instances. The second is to make the hidden variables informative about the observed attributes. By exploring different tradeoffs between these two objectives, we can gradually converge on a high-scoring solution. As we show, the resulting, Information Bottleneck Expectation Maximization (IB-EM) algorithm, manages to find solutions that are superior to standard EM methods.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {200–208},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100609,
author = {Feng, Zhengzhu and Hansen, Eric A. and Zilberstein, Shlomo},
title = {Symbolic Generalization for On-Line Planning},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Symbolic representations have been used successfully in off-line planning algorithms for Markov decision processes. We show that they can also improve the performance of on-line planners. In addition to reducing computation time, symbolic generalization can reduce the amount of costly real-world interactions required for convergence. We introduce Symbolic Real-Time Dynamic Programming (or sRTDP), an extension of RTDP. After each step of on-line interaction with an environment, sRTDP uses symbolic model-checking techniques to generalizes its experience by updating a group of states rather than a single state. We examine two heuristic approaches to dynamic grouping of states and show that they accelerate the planning process significantly in terms of both CPU time and the number of steps of interaction with the environment.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {209–216},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100610,
author = {da Rocha, Jose Carlos Ferreira and Cozmanl, Fabio Gagliardi and de Campos, Cassio Polpo},
title = {Inference in Polytrees with Sets of Probabilities},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Inferences in directed acyclic graphs associated with probability intervals and sets of probabilities are NP-hard, even for polytrees. We propose: 1) an improvement on Tessem's A/R algorithm for inferences on polytrees associated with probability intervals; 2) a new algorithm for approximate inferences based on local search; 3) branch-and-bound algorithms that combine the previous techniques. The first two algorithms produce complementary approximate solutions, while branch-and-bound procedures can generate either exact or approximate solutions. We report improvements on existing techniques for inference with probability sets and intervals, in some cases reducing computational effort by several orders of magnitude.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {217–224},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100611,
author = {Finzi, Alberto and Lukasiewicz, Thomas},
title = {Structure-Based Causes and Explanations in the Independent Choice Logic},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper is directed towards combining Pearl's structural-model approach to causal reasoning with high-level formalisms for reasoning about actions. More precisely, we present a combination of Pearl's structural-model approach with Poole's independent choice logic. We show how probabilistic theories in the independent choice logic can be mapped to probabilistic causal models. This mapping provides the independent choice logic with appealing concepts of causality and explanation from the structural-model approach. We illustrate this along Halpern and Pearl's sophisticated notions of actual cause, explanation, and partial explanation. This mapping also adds first-order modeling capabilities and explicit actions to the structural-model approach.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {225–323},
numpages = {99},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100612,
author = {Flores, M. Julia and G\'{a}mez, Jos\'{e} A. and Olesen, Kristian G.},
title = {Incremental Compilation of Bayesian Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Most methods for exact probability propagation in Bayesian networks do not carry out the inference directly over the network, but over a secondary structure known as a junction tree or a join tree (JT). The process of obtaining a JT is usually termed compilation. As compilation is usually viewed as a whole process; each time the network is modified, a new compilation process has to be performed. The possibility of reusing an already existing JT in order to obtain the new one regarding only the modifications in the network has received only little attention in the literature. In this paper we present a method for incremental compilation of a Bayesian network, following the classical scheme in which triangulation plays the key role. In order to perform incremental compilation we propose to recompile only those parts of the JT which may have been affected by the network's modifications. To do so, we exploit the technique of maximal prime subgraph decomposition in determining the minimal subgraph(s) that have to be recompiled, and thereby the minimal subtree(s) of the JT that should be replaced by new subtree(s). We focus on structural modifications: addition and deletion of links and variables.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {233–240},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100613,
author = {Frank, Ari and Geiger, Dan and Yakhini, Zohar},
title = {A Distance-Based Branch and Bound Feature Selection Algorithm},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {There is no known efficient method for selecting k Gaussian features from n which achieve the lowest Bayesian classification error. We show an example of how greedy algorithms faced with this task are led to give results that are not optimal. This motivates us to propose a more robust approach. We present a Branch and Bound algorithm for finding a subset of k independent Gaussian features which minimizes the naive Bayesian classification error. Our algorithm uses additive monotonic distance measures to produce bounds for the Bayesian classification error in order to exclude many feature subsets from evaluation, while still returning an optimal solution. We test our method on synthetic data as well as data obtained from gene expression profiling.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {241–248},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100614,
author = {Frank, Eibe and Hall, Mark and Pfahringer, Bernhard},
title = {Locally Weighted Naive Bayes},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Despite its simplicity, the naive Bayes classifier has surprised machine learning researchers by exhibiting good performance on a variety of learning problems. Encouraged by these results, researchers have looked to overcome naive Bayes' primary weakness-attribute independence-and improve the performance of the algorithm. This paper presents a locally weighted version of naive Bayes that relaxes the independence assumption by learning local models at prediction time. Experimental results show that locally weighted naive Bayes rarely degrades accuracy compared to standard naive Bayes and, in many cases, improves accuracy dramatically. The main advantage of this method compared to other techniques for enhancing naive Bayes is its conceptual and computational simplicity.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {249–256},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100615,
author = {Frey, Brendan J.},
title = {Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The two most popular types of graphical model are Bayesian networks (BNs) and Markov random fields (MRFs). These types of model offer complementary properties in model construction, expressing conditional independencies, expressing arbitrary factorizations of joint distributions, and formulating messagepassing inference algorithms. We show how the notation and semantics of factor graphs (a relatively new type of graphical model) can be extended so as to combine the strengths of BNs and MRFs. Every BN or MRF can be easily converted to a factor graph that expresses the same conditional independencies, expresses the same factorization of the joint distribution, and can be used for probabilistic inference through application of a single, simple message-passing algorithm. We describe a modified "Bayes-ball" algorithm for establishing conditional independence in factor graphs, and we show that factor graphs form a strict superset of BNs and MRFs. In particular, we give an example of a commonly-used model fragment, whose independencies cannot be represented in a BN or an MRF, but can be represented in a factor graph. For readers who use chain graphs, we describe a further extension of factor graphs that enables them to represent properties of chain graphs.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {257–264},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100616,
author = {Gao, Yong},
title = {Phase Transition of Tractability in Constraint Satisfaction and Bayesian Network Inference},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Identifying tractable subclasses and designing efficient algorithms for these tractable classes are important topics in the study of constraint satisfaction and Bayesian network inference problems. In this paper we investigate the asymptotic average behavior of a typical tractable subclass characterized by the treewidth of the problems. We show that the property of having a bounded treewidth in the constraint satisfaction problem and Bayesian network inference problem has a phase transition that occurs while the underlying structures of problems are still sparse. This implies that algorithms making use of treewidth based structural knowledge only work efficiently in a limited range of random instances.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {265–271},
numpages = {7},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100617,
author = {Giang, Phan H. and Shenoy, Prakash P.},
title = {Decision Making with Partially Consonant Belief Functions},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper studies decision making for Walley's partially consonant belief functions (pcb). In a pcb, the set of foci are partitioned. Within each partition, foci are nested. The pcb class includes probability and possibility functions as extreme cases. We adopt an axiomatic system, similar in spirit to von Neumann and Morgenstern's axioms for preferences leading to the linear utility theory, for a preference relation on pcb lotteries. We Drove a representation theorem for this preference relation. Utility for a pcb lottery is a combination of linear utility for probabilistic lottery and binary utility for possibilistic lottery.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {272–280},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100618,
author = {Globerson, Amir and Chechik, Gal and Tishby, Naftali},
title = {Sufficient Dimensionality Reduction with Irrelevance Statistics},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The problem of unsupervised dimensionality reduction of stochastic variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Unfortunately, this problem is ill defined since natural datasets inherently contain alternative underlying structures. In this paper we address this problem by extending the recently introduced "Sufficient Dimensionality Reduction" feature extraction method [7], to use "side information" about irrelevant structures in the data. The use of such irrelevance information was recently successfully demonstrated in the context of clustering via the Information Bottleneck method [1]. Here we use this side-information framework to identify continuous features whose measurements are maximally informative for the main data set, but carry as little information as possible on the irrelevance data set. In statistical terms this can be understood as extracting statistics which are maximally sufficient for the main dataset, while simultaneously maximally ancillary for the irrelevance dataset. We formulate this problem as a tradeoff optimization problem and describe its analytic and algorithmic solutions. Our method is demonstrated on a synthetic example and on a real world application of face images, showing its superiority over other methods such as Oriented Principal Component Analysis.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {281–288},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100619,
author = {Gretton, Charles and Price, David and Thi\'{e}baux, Sylvie},
title = {Implementation and Comparison of Solution Methods for Decision Processes with Non-Markovian Rewards},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper examines a number of solution methods for decision processes with non-Markovian rewards(NMRDPs). They all exploit a temporal logic specification of the reward function to automatically translate the NMRDP into an equivalent Markov decision process (MDP) amenable to well-known MDP solution methods. They differ however in the representation of the target MDP and the class of MDP solution methods to which they are suited. As a result, they adopt different temporal logics and different translations. Unfortunately, no implementation of these methods nor experimental let alone comparative results have ever been reported. This paper is the first step towards filling this gap. We describe an integrated system for solving NMRDPs which implements these methods and several variants under a common interface; we use it to compare the various approaches and identify certain problem features favouring one over the other.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {289–296},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100620,
author = {Halpern, Joseph Y. and Pucella, Riccardo},
title = {A Logic for Reasoning about Evidence},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We introduce a logic for reasoning about evidence, that essentially views evidence as a function from prior beliefs (before making an observation) to posterior beliefs (after making the observation). We provide a sound and complete axiomatization for the logic, and consider the complexity of the decision problem. Although the reasoning in the logic is mainly propositional, we allow variables representing numbers and quantification over them. This expressive power seems necessary to capture important properties of evidence.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {297–304},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100621,
author = {Hauskrecht, Milos and Singliar, Tomas},
title = {Monte-Carlo Optimizations for Resource Allocation Problems in Stochastic Network Systems},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Real-world distributed systems and networks are often unreliable and subject to random failures of its components. Such a stochastic behavior affects adversely the complexity of optimization tasks performed routinely upon such systems. In this work we investigate Monte Carlo solutions for a class of two-stage optimization problems in stochastic networks in which the expected value of resources allocated before and after the occurence of stochastic failures needs to be optimized. The limitation of these problems is that their exact solutions are exponential in the number of unreliable network components: thus, exact methods do not scale-up well to large networks often seen in practice. We first show that Monte Carlo optimization methods can overcome the exponential bottleneck of exact methods. Next we support our theoretical findings on resource allocation experiments and show a very good scale-up potential of the methods on problems with large stochastic networks.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {305–312},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100622,
author = {Heskes, Tom and Albers, Kees and Kappen, Bert},
title = {Approximate Inference and Constrained Optimization},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms correspond to extrema of the Bethe and Kikuchi free energy (Yedidia et al., 2001). However, belief propagation does not always converge, which motivates approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP (Yuille, 2002) and UPS (Teh and Welling, 2002). Here we describe a class of algorithms that solves this typically non-convex constrained minimization problem through a sequence of convex constrained minimizations of upper bounds on the Kikuchi free energy. Intuitively one would expect tighter bounds to lead to faster algorithms, which is indeed convincingly demonstrated in our simulations. Several ideas are applied to obtain tight convex bounds that yield dramatic speed-ups over CCCP.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {313–320},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100623,
author = {Hopkins, Mark},
title = {LAYERWIDTH: Analysis of a New Metric for Directed Acyclic Graphs},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We analyze a new property of directed acyclic graphs (DAGs), called layerwidth, arising from a useful class of DAGs proposed by Eiter and Lukasiewicz for tractable causal reasoning. First, we establish that the complexity of deciding whether a given graph has a bounded layerwidth is NP-complete. Then we proceed to prove key properties of layerwidth that are helpful in efficiently computing the optimal layerwidth. Finally, we compare this new DAG property to two other important DAG properties: treewidth and bandwidth.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {321–328},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100624,
author = {Jin, Rong and Si, Luo and Zhai, ChengXiang},
title = {Preference-Based Graphic Models for Collaborative Filtering},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Collaborative filtering is a very useful general technique for exploiting the preference patterns of a group of users to predict the utility of items to a particular user. Previous research has studied several probabilistic graphic models for collaborative filtering with promising results. However, while these models have succeeded in capturing the similarity among users and items, none of them has considered the fact that users with similar interests in items can have very different rating patterns; some users tend to assign a higher rating to all items than other users. In this paper, we propose and study two new graphic models that address the distinction between user preferences and ratings. In one model, called the decoupled model, we introduce two different variables to decouple a user's preferences from hislher ratings. In the other, called the preference model, we model the orderings of items preferred by a user, rather than the user's numerical ratings of items. Empirical study over two datasets of movie ratings shows that, due to its appropriate modeling of the distinction between user preferences and ratings, the proposed decoupled model significantly outperforms all the five existing approaches that we compared with. The preference model, however, performs much worse than the decoupled model, suggesting that while explicit modeling of the underlying user preferences is very important for collaborative filtering, we can not afford ignoring the rating information completely.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {329–336},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100625,
author = {Lam, Shyong K. and Pennock, David M. and Cosley, Dan and Lawrence, Steve},
title = {1 Billion Pages = 1 Million Dollars? Mining the Web to Play "Who Wants to Be a Millionaire?"},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We exploit the redundancy and volume of information on the web to build a computerized player for the ABC TV game show "Who Wants To Be A Millionaire?". The player consists of a question-answering module and a decision-making module. The question-answering module utilizes question transformation techniques, natural language parsing, multiple information retrieval algorithms, and multiple search engines; results are combined in the spirit of ensemble learning using an adaptive weighting scheme. Empirically, the system correctly answers about 75% of questions from the Millionaire CD-ROM, 3rd edition--general-interest trivia questions often about popular culture and common knowledge. The decision-making module chooses from allowable actions in the game in order to maximize expected risk-adjusted winnings, where the estimated probability of answering correctly is a function of past performance and confidence in correctly answering the current question. When given a six question head start (i.e., when starting from the $2,000 level), we find that the system performs about as well on average as humans starting at the beginning. Our system demonstrates the potential of simple but well-chosen techniques for mining answers from unstructured information such as the web.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {337–345},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100626,
author = {Larkin, David},
title = {Approximate Decomposition: A Method for Bounding and Estimating Probabilistic and Deterministic Queries},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper, we introduce a method for approximating the solution to inference and optimization tasks in uncertain and deterministic reasoning. Such tasks are in general intractable for exact algorithms because of the large number of dependency relationships in their structure. Our method effectively maps such a dense problem to a sparser one which is in some sense "closest". Exact methods can be run on the sparser problem to derive bounds on the original answer, which can be quite sharp. On one large CPCS network, for example, we were able to calculate upper and lower bounds on the conditional probability of a variable, given evidence, that were almost identical in the average case.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {346–353},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100627,
author = {Lawrence, Gregory and Cowan, Noah and Russell, Stuart},
title = {Efficient Gradient Estimation for Motor Control Learning},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The task of estimating the gradient of a function in the presence of noise is central to several forms of reinforcement learning, including policy search methods. We present two techniques for reducing gradient estimation errors in the presence of observable input noise applied to the control signal. The first method extends the idea of a reinforcement baseline by fitting a local model to the response function whose gradient is being estimated; we show how to find the response surface model that minimizes the variance of the gradient estimate, and how to estimate the model from data. The second method improves this further by discounting components of the gradient vector that have high variance. These methods are applied to the problem of motor control learning, where actuator noise has a significant influence on behavior. In particular, we apply the techniques to learn locally optimal controllers for a dart-throwing task using a simulated three-link arm; we demonstrate that the proposed methods significantly improve the response function gradient estimate and, consequently, the learning curve, over existing methods.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {354–361},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100628,
author = {Lebanon, Guy},
title = {Learning Riemannian Metrics},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We consider the problem of learning a Riemannian metric associated with a given differentiable manifold and a set of points. Our approach to the problem involves choosing a metric from a parametric family that is based on maximizing the inverse volume of a given dataset of points. From a statistical perspective, it is related to maximum likelihood under a model that assigns probabilities inversely proportional to the Riemannian volume element. We discuss in detail learninga metric on the multinomial simplex where the metric candidates are pull-back metrics of the Fisher information under a continuous group of transformations. When applied to documents, the resulting geodesics resemble, but outperform, the TFIDF cosine similarity measure in classification.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {362–369},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100629,
author = {Liu, Liping and Shenoy, Catherine and Shenoy, Prakash P.},
title = {A Linear Belief Function Approach to Portfolio Evaluation},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We show how to use linear belief functions to represent market information and financial knowledge, including complete ignorance, statistical observations, subjective speculations, distributional assumptions, linear relations, and empirical asset pricing models. We then appeal to Dempster's rule of combination to integrate the knowledge for assessing an overall belief on portfolio performance, and to update this belief by incorporating additional information.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {370–377},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100630,
author = {Lizotte, Daniel J. and Madani, Omid and Greiner, Russell},
title = {Budgeted Learning of Nailve-Bayes Classifiers},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {There is almost always a cost associated with acquiring training data. We consider the situation where the learner, with a fixed budget, may 'purchase' data during training. In particular, we examine the case where observing the value of a feature of a training example has an associated cost, and the total cost of all feature values acquired during training must remain less than this fixed budget. This paper compares methods for sequentially choosing which feature value to purchase next, given the budget and user's current knowledge of Na\"{\i}ve Bayes model parameters. Whereas active learning has traditionally focused on myopic (greedy) approaches and uniform/round-robin policies for query selection, this paper shows that such methods are often suboptimal and presents a tractable method for incorporating knowledge of the budget in the information acquisition process.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {378–385},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100631,
author = {Lu, Fletcher and Schuurmans, Dale},
title = {Monte Carlo Matrix Inversion Policy Evaluation},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In 1950, Forsythe and Leibler (1950) introduced a statistical technique for finding the inverse of a matrix by characterizing the elements of the matrix inverse as expected values of a sequence of random walks. Barto and Duff (1994) subsequently showed relations between this technique and standard dynamic programming and temporal differencing methods. The advantage of the Monte Carlo matrix inversion (MCMI) approach is that it scales better with respect to state-space size than alternative techniques. In this paper, we introduce an algorithm for performing reinforcement learning policy evaluation using MCMI. We demonstrate that MCMI possesses accuracy similar to a maximum likelihood model-based policy evaluation approach but avoids ML's slow execution time. In fact, we show that MCMI executes at a similar runtime to temporal differencing (TD). We then illustrate a least-squares generalization technique for scaling up MCMI to large state spaces. We compare this leastsquares Monte Carlo matrix inversion (LS-MCMI) technique to the least-squares temporal differencing (LSTD) approach introduced by Bradtke and Barto (1996) demonstrating that both LS-MCMI and LSTD have similar runtime.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {386–393},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100632,
author = {Marinescu, Radu and Kask, Kalev and Dechter, Rina},
title = {Systematic vs. Non-Systematic Algorithms for Solving the MPE Task},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The paper explores the power of two systematic Branch and Bound search algorithms that exploit partition-based heuristics, BBBT (a new algorithm for which the heuristic information is constructed during search and allows dynamic variable/value ordering) and its predecessor BBMB (for which the heuristic information is pre-compiled) and compares them against a number of popular local search algorithms for the MPE problem as well as against the recently popular iterative belief propagation algorithms. We show empirically that the new Branch and Bound algorithm, BBBT demonstrates tremendous pruning of the search space far beyond its predecessor, BBMB which translates to impressive time saving for some classes of problems. Second, when viewed as approximation schemes, BBBT/BBMB together are highly competitive with the best known SLS algorithms and are superior, especially when the domain sizes increase beyond 2. The results also show that the class of belief propagation algorithms can outperform SLS, but they are quite inferior to BBMB/BBBT. As far as we know, BBBT/BBMB are currently among the best performing algorithms for solving the MPE task.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {394–402},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100633,
author = {McCallum, Andrew},
title = {Efficiently Inducing Features of Conditional Random Fields},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Conditional Random Fields (CRFs) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary, non-independent features of the input. Faced with this freedom, however, an important question remains: what features should be used? This paper presents an efficient feature induction method for CRFs. The method is founded on the principle of iteratively constructing feature conjunctions that would significantly increase conditional log-likelihood if added to the model. Automated feature induction enables not only improved accuracy and dramatic reduction in parameter count, but also the use of larger cliques, and more freedom to liberally hypothesize atomic input variables that may be relevant to a task. The method applies to linear-chain CRFs, as well as to more arbitrary CRF structures, such as Relational Markov Networks, where it corresponds to learning clique templates, and can also be understood as supervised structure learning. Experimental results on named entity extraction and noun phrase segmentation tasks are presented.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {403–410},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100634,
author = {Meek, Christopher and Chickering, David Maxwell},
title = {Practically Perfect},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We prove that perfect distributions exist when using a finite number of bits to represent the parameters of a Bayesian network. In addition, we provide an upper bound on the probability of sampling a non-perfect distribution when using a fixed number of bits for the parameters and that the upper bound approaches zero exponentially fast as one increases the number of bits. We also provide an upper bound on the number of bits needed to guarantee that a distribution sampled from a uniform Dirichlet distribution is perfect with probability greater than 1/2.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {411–416},
numpages = {6},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100635,
author = {Meuleau, Nicolas and Smith, David E.},
title = {Optimal Limited Contingency Planning},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {For a given problem, the optimal Markov policy over a finite horizon is a conditional plan containing a potentially large number of branches. However, there are applications where it is desirable to strictly limit the number of decision points and branches in a plan. This raises the question of how one goes about finding optimal plans containing only a limited number of branches. In this paper, we present an any-time algorithm for optimal k-contingency planning. It is the first optimal algorithm for limited contingency planning that is not an explicit enumeration of possible contingent plans. By modelling the problem as a partially observable Markov decision process, it implements the Bellman optimality principle and prunes the solution space. We present experimental results of applying this algorithm to some simple test cases.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {417–426},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100636,
author = {Mugica, Francisco and Nebot, Angela and G\'{o}mez, Pilar},
title = {Dealing with Uncertainty in Fuzzy Inductive Reasoning Methodology},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The aim of this research is to develop a strategy of reasoning under uncertainty in the context of the Fuzzy Inductive Reasoning methodology. This methodology allows the prediction of systems behavior by means of two different schemes. The first one corresponds to a pattern prediction scheme, based exclusively on pattern rules. The second one corresponds to a purely Sugeno inference system, i.e. Sugeno prediction scheme. The Sugeno fuzzy rules are automatically extracted from the pattern rules producing a compact representation of the system modelled. In this paper a mixed pattern/fuzzy rules scheme is studied to deal with uncertainty in such a way that the best of both perspectives is used. The proposed scheme is applied to a real biomedical system, i.e. the central nervous system control of the cardiovascular system.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {427–434},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100637,
author = {Nielsen, Jens D. and Ko\v{c}ka, Tom\'{a}\v{s} and Pe\~{n}a, Jose M.},
title = {On Local Optima in Learning Bayesian Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima when run repeatedly. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild conditions KES asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES finds a better local optimum than GES considerably often. Additionally, these results illustrate that the number of different local optima is usually huge.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {435–442},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100638,
author = {Nikovski, Daniel and Brand, Matthew},
title = {Marginalizing out Future Passengers in Group Elevator Control},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Group elevator scheduling is an NP-hard sequential decision-making problem with unbounded state spaces and substantial uncertainty. Decision-theoretic reasoning plays a surprisingly limited role in fielded systems. A new opportunity for probabilistic methods has opened with the recent discovery of a tractable solution for the expected waiting times of all passengers in the building, marginalized over all possible passenger itineraries [Nikovski and Brand, 2003]. Though commercially competitive, this solution does not contemplate future passengers. Yet in up-peak traffic, the effects of future passengers arriving at the lobby and entering elevator cars can dominate all waiting times. We develop a probabilistic model of how these arrivals affect the behavior of elevator cars at the lobby, and demonstrate how this model can be used to very significantly reduce the average waiting time of all passengers.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {443–450},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100639,
author = {Nodelman, Uri and Shelton, Christian R. and Koller, Daphne},
title = {Learning Continuous Time Bayesian Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Continuous time Bayesian networks (CTBN) describe structured stochastic processes with finitely many states that evolve nver continuous time. A CTBN is a directed (possibly cyclic) dependency graph over a set of variables, each of which represents a finite state continuous time Markov process whose transition model is a function of its parents. We address the problem of leaning parameters and structure of a CTBN from fully observed data. We define a conjugate prior for CTBNs and show how it can be used both for Bayesian parameter estimation and as the basis of a Bayesian score for structure learning. Because acyclicity is not a constraint in CTBNs, we can show that the structure leaning problem is significantly easier, both in theory and in practice, than structure leaning for dynamic Bayesian networks (DBNs). Furthermore, as CTBNs can tailor the parameters and dependency structure to the different time granularities of the evolution of different variables, they can provide a better fit to continuous-time processes than DBNs with a fixed time granularity.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {451–458},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100640,
author = {Park, James D. and Darwiche, Adnan},
title = {Solving MAP Exactly Using Systematic Search},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {MAP is the problem of finding a most probable instantiation of a set of variables in a Bayesian network, given some partial evidence about the complement of that set. Unlike posterior probabilities, or MPE (a special case of MAP), the time and space complexity of structure-based algorithms for MAP are not only exponential in the network treewidth, but in a larger parameter known as the constrained treewidth. In practice, this means that computing MAP can be orders of magnitude more expensive than computing posterior probabilities or MPE. We introduce in this paper a new, simple upper bound on the probability of a MAP solution, which is shown to be generally much tighter than existing bounds. We then use the proposed upper bound to develop a branch-andbound search algorithm for solving MAP exactly. Experimental results demonstrate that the search algorithm is able to solve many problems that are far beyond the reach of any structurebased method for MAP. For example, we show that the proposed algorithm can compute MAP exactly and efficiently for some networks whose constrained treewidth is more than 40.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {459–468},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100641,
author = {Perny, Patrice and Spanjaard, Olivier},
title = {An Axiomatic Approach to Robustness in Search Problems with Multiple Scenarios},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper is devoted to the the search of robust solutions in state space graphs when costs depend on scenarios. We first present axiomatic requirements for preference compatibility with the intuitive idea of robustness. This leads us to propose the Lorenz dominance rule as a basis for robustness analysis. Then, after presenting complexity results about the determination of robust solutions, we propose a new sophistication of A* specially designed to determine the set of robust paths in a state space graph. The behavior of the algorithm is illustrated on a small example. Finally, an axiomatic justification of the refinement of robustness by an OWA criterion is provided.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {469–476},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100642,
author = {Pineau, Joelle and Gordon, Geoff and Thrun, Sebastian},
title = {Policy-Contingent Abstraction for Robust Robot Control},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper presents a scalable control algorithm that enables a deployed mobile robot to make high-level control decisions under full consideration of its probabilistic belief. We draw on insights from the rich literature of structured robot controllers and hierarchical MDPs to propose PolCA, a hierarchical probabilistic control algorithm which learns both subtask-specific state abstractions and policies. The resulting controller has been successfully implemented onboard a mobile robotic assistant deployed in a nursing facility. To the best of our knowledge, this work is a unique instance of applying POMDPs to highlevel robotic control problems.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {477–484},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100643,
author = {Rosales, R\'{o}mer and Frey, Brendan},
title = {Learning Generative Models of Similarity Matrices},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Recently, spectral clustering (a.k.a. normalized graph cut) techniques have become popular for their potential ability at finding irregularly-shaped clusters in data. The input to these methods is a similarity measure between every pair of data points. If the clusters are well-separated, the eigenvectors of the similarity matrix can be used to identify the clusters, essentially by identifying groups of points that are related by transitive similarity relationships. However, these techniques fail when the clusters are noisy and not well-separated, or when the scale parameter that is used to map distances between points to similarities is not set correctly. Our approach to solving these problems is to introduce a generative probability model that explicitly models noise and can be trained in a maximum-likelihood fashion to estimate the scale parameter. Exact inference is computationally intractable, but we describe tractable, approximate techniques for inference and learning. Interestingly, it turns out that greedy inference and learning in one of our models with a fixed scale parameter is equivalent to spectral clustering. We examine several data sets, and demonstrate that our method finds better clusters compared with spectral clustering.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {485–492},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100644,
author = {Rosencrantz, Matt and Gordon, Geoffrey and Thrun, Sebastian},
title = {Decentralized Sensor Fusion with Distributed Particle Filters},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This paper presents a scalable Bayesian technique for decentralized state estimation from multiple platforms in dynamic environments. As has long been recognized, centralized architectures impose severe scaling limitations for distributed systems due to the enormous communication overheads. We propose a strictly decentralized approach in which only nearby platforms exchange information. They do so through an interactive communication protocol aimed at maximizing information flow. Our approach is evaluated in the context of a distributed surveillance scenario that arises in a robotic system for playing the game of laser tag. Our results, both from simulation and using physical robots, illustrate an unprecedented scaling capability to large teams of vehicles.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {493–500},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100645,
author = {Rusakov, Dmitry and Geiger, Dan},
title = {Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for Latent Models},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present two algorithms for analytic asymptotic evaluation of the marginal likelihood of data given a Bayesian network with hidden nodes. As shown by previous work, this evaluation is particularly hard because for these models asymptotic approximation of the marginal likelihood deviates from the standard BIC score. Our algorithms compute regular dimensionality drop for latent models and compute the non-standard approximation formulas for singular statistics for these models. The presented algorithms are implemented in Matlab and Maple and their usage is demonstrated on several examples.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {501–508},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100646,
author = {Salakhutdinov, Ruslan and Roweis, Sam and Ghahramani, Zoubin},
title = {On the Convergence of Bound Optimization Algorithms},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Many practitioners who use EM and related algorithms complain that they are sometimes slow. When does this happen, and what can be done about it? In this paper, we study the general class of bound optimization algorithms - including EM, Iterative Scaling, Non-negative Matrix Factorization, CCCP - and their relationship to direct optimization algorithms such as gradientbased methods for parameter learning. We derive a general relationship between the updates performed by bound optimization methods and those of gradient and second-order methods and identify analytic conditions under which bound optimization algorithms exhibit quasi-Newton behavior, and under which they possess poor, first-order convergence. Based on this analysis, we consider several specific algorithms, interpret and analyze their convergence properties and provide some recipes for preprocessing input to these algorithms to yield faster convergence behavior. We report empirical results supporting our analysis and showing that simple data preprocessing can result in dramatically improved performance of bound optimizers in practice.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {509–516},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100647,
author = {Costa, V\'{\i}tor Santos and Page, David and Qazi, Maleeha and Cussens, James},
title = {CLP(BN): Constraint Logic Programming for Probabilistic Knowledge},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In Datalog, missing values are represented by Skolem constants. More generally, in logic programming missing values, or existentially-quantified variables, are represented by terms built from Skolem functors. In an analogy to probabilistic relational models (PRMs), we wish to represent the joint probability distribution over missing values in a database or logic program using a Bayesian network. This paper presents an extension of logic programs that makes it possible to specify a joint probability distribution over terms built from Skolem functors in the program. Our extension is based on constraint logic programming (CLP), so we call the extended language CLP(BN). We show that CLP(BN) subsumes PRMs; this greater expressivity carries both advantages and disadvantages for CLP(BN). We also show that algorithms from inductive logic programming (ILP) can be used with only minor modification to learn CLP(BN) programs. An implementation of CLP(BN) is publicly available as part of YAP Prolog at http://www.cos.ufrj.br/~vitor/Yap/clpbn},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {517–524},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100648,
author = {Segal, Eran and Pe'er, Dana and Regev, Aviv and Koller, Daphne},
title = {Learning Module Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Methods for learning Bayesian networks can discover dependency structure between observed variables. Although these methods are useful in many applications, they run into computational and statistical problems in domains that involve a large number of variables. In this paper, we consider a solution that is applicable when many variables have similar behavior. We introduce a new class of models, module networks, that explicitly partition the variables into modules that share the same parents in the network and the same conditional probability distribution. We define the semantics of module networks, and describe an algorithm that learns the modules' composition and their dependency structure from data. Evaluation on real data in the domains of gene expression and the stock market shows that module networks generalize better than Bayesian networks, and that the learned module network structure reveals regularities that are obscured in learned Bayesian networks.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {525–534},
numpages = {10},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100649,
author = {Sharma, Rita and Poole, David},
title = {Efficient Inference in Large Discrete Domains},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {In this paper we examine the problem of inference in Bayesian Networks with discrete random variables that have very large or even unbounded domains. For example, in a domain where we are trying to identify a person, we may have variables that have as domains, the set of all names, the set of all postal codes, or the set of all credit card numbers. We cannot just have big tables of the conditional probabilities, but need compact representations. We provide an inference algorithm, based on variable elimination, for belief networks containing both large domain and normal discrete random variables. We use intensional (i.e., in terms of procedures) and extensional (in terms of listing the elements) representations of conditional probabilities and of the intermediate factors.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {535–542},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100650,
author = {Silva, Ricardo and Scheines, Richard and Glymour, Clark and Spirtes, Peter},
title = {Learning Measurement Models for Unobserved Variables},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Observed associations in a database may be due in whole or part to variations in unrecorded ("latent") variables. Identifying such variables and their causal relationships with one another is a principal goal in many scientific and practical domains. Previous work shows that, given a partition of observed variables such that members of a class share only a single latent common cause, standard search algorithms for causal Bayes nets can infer structural relations between latent variables. We introduce an algorithm for discovering such partitions when they exist. Uniquely among available procedures, the algorithm is (asymptotically) correct under standard assumptions in causal Bayes net search algorithms, requires no prior knowledge of the number of latent variables, and does not depend on the mathematical form of the relationships among the latent variables. We evaluate the algorithm on a variety of simulated data sets.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {543–550},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100651,
author = {Stewart, Benjamin and Ko, Jonathan and Fox, Dieter and konolige, Kurt},
title = {The Revisiting Problem in Mobile Robot Map Building: A Hierarchical Bayesian Approach},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present an application of hierarchical Bayesian estimation to robot map building. The revisiting problem occurs when a robot has to decide whether it is seeing a previously-built portion of a map, or is exploring new territory. This is a difficult decision problem, requiring the probability of being outside of the current known map. To estimate this probability, we model the structure of a "typical" environment as a hidden Markov model that generates sequences of views observed by a robot navigating through the environment. A Dirichlet prior over structural models is learned from previously explored environments. Whenever a robot explores a new environment, the posterior over the model is estimated by Dirichlet hyperparameters. Our approach is implemented and tested in the context of multi-robot map merging, a particularly difficult instance of the revisiting problem. Experiments with robot data show that the technique yields strong improvements over altemative methods.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {551–558},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100652,
author = {Storkey, Amos J. and Hambly, Nigel C. and Williams, Christopher K. I. and Mannt, Robert G.},
title = {Renewal Strings for Cleaning Astronomical Databases},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Large astronomical databases obtained from sky surveys such as the SuperCOSMOS Sky Surveys (SSS) invariably suffer from spurious records coming from artefactual effects of the telescope, satellites and junk objects in orbit around earth and physical defects on the photographic plate or CCD. Though relatively small in number these spurious records present a significant problem in many situations where they can become a large proportion of the records potentially of interest to a given astronomer. We have developed renewal strings, a probabilistic technique combining the Hough transform, renewal processes and hidden Markov models which has proven highly effective in this context. The methods are applied to the SSS data to develop a dataset of spurious object detections, along with confidence measures, which can allow this unwanted data to be removed from consideration. These methods are general and can be adapted to any other astronomical survey data.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {559–566},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100653,
author = {Wang, Shaojun and Schuurmans, Dale and Peng, Fuchun and Zhao, Yunxin},
title = {Boltzmann Machine Learning with the Latent Maximum Entropy Principle},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a new statistical learning paradigm for Boltzmann machines based on a new inference principle we have proposed: the latent maximum entropy principle (LME). LME is different both from Jaynes' maximum entropy principle and from standard maximum likelihood estimation. We demonstrate the LME principle by deriving new algorithms for Boltzmann machine parameter estimation, and show how a robust and rapidly convergent new variant of the EM algorithm can be developed. Our experiments show that estimation based on LME generally yields better results than maximum likelihood estimation when inferring models from small amounts of data.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {567–574},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100654,
author = {Welling, Max and Zemel, Richard S. and Hinton, Geoffrey E.},
title = {Efficient Parametric Projection Pursuit Density Estimation},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the "undercomplete product of experts" (UPoE), where each expert models a one dimensional projection of the data. The UPoE may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {575–582},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100655,
author = {Xing, Eric P. and Jordan, Michael I. and Russell, Stuart},
title = {A Generalized Mean Field Algorithm for Variational Inference in Exponential Families},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We present a ciass of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on overlapping clusters, our approach is based on nonoverlapping clusters. Unlike the cluster variational methods, the approach is proved to converge to a globally consistent set of marginals and a lower bound on the likelihood, while providing much of the flexibility associated with cluster variational methods. We present experiments that analyze the effect of different choices of clustering on inference quality, and compare GMF with belief propagation on several canonical models.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {583–591},
numpages = {9},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100656,
author = {Yamazaki, Keisuke and Watanabe, Sumio},
title = {Stochastic Complexity of Bayesian Networks},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Bayesian networks arc now used in enormous fields, for example. system diagnosis. data mining, clusterings etc. In spite of wide range of their applications, the statistical properties have not yet bcen clarified because the models are nonidentifiable and non-regular. In a Bayesian network. the set of parameters for a smaller model is an analytic set with singularities in the parameter space of a large model. Because of these singularities, the Fisher information matrices are not positive definite. In other words, the mathematical foundation for learning has not been constructed. In recent years, however, we have developed a method to analyze nonregular models by using algebraic geometry. This method revealed the relation between model's singularities and its statistical properties. In this paper, applying this method to Bayesian networks with latent variables, we clarify the orders of the stochastic complexities. Our result shows that their upper bound is smaller than thc dimension of the parameter space. This means that the Bayesian generalization error is also far smaller than that of a regular model, and that Schwarz's model selection criterion BIC needs to be improved for Bayesian networks.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {592–599},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100657,
author = {Yeang, Chen-Hsiang and Szummer, Martin},
title = {Markov Random Walk Representations with Continuous Distributions},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Representations based on random walks can exploit discrete data distributions for clustering and classification. We extend such representations from discrete to continuous distributions. Transition probabilities are now calculated using a diffusion equation with a diffusion coefficient that inversely depends on the data density. We relate this diffusion equation to a path integral and derive the corresponding path probability measure. The framework is useful for incorporating continuous data densities and prior knowledge.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {600–607},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100658,
author = {Young, Joel and Dean, Thomas},
title = {Exploiting Locality in Searching the Web},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Published experiments on spidering the Web suggest that, given training data in the form of a (relatively small) subgraph of the Web containing a subset of a selected class of target pages, it is possible to conduct a directed search and find additional target pages significantly faster (with fewer page retrievals) than by performing a blind or uninformed random or systematic search, e.g., breadthfirst search. If true, this claim motivates a number of practical applications. Unfortunately, these experiments were carried out in specialized domains or under conditions that are difficult to replicate. We present and apply an experimental framework designed to reexamine and resolve the basic claims of the earlier work, so that the supporting experiments can be replicated and built upon. We provide high-performance tools for building experimental spiders, make use of the ground truth and static nature of the WT10g TREC Web corpus, and rely on simple well understand machine learning techniques to conduct our experiments. In this paper, we describe the basic framework, motivate the experimental design, and report on our findings supporting and qualifying the conclusions of the earlier research.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {608–615},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100659,
author = {Yu, Kai and Schwaighofer, Anton and Tresp, Volker},
title = {Collaborative Ensemble Learning: Combining Collaborative and Content-Based Information Filtering via Hierarchical Bayes},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Collaborative filtering (CF) and content-based filtering (CBF) have widely been used information filtering applications, both approaches having their individual strengths and weaknesses. This paper proposes a novel probabilistic framework to unify CF and CBF, named collaborative ensemble learning. Based on content based probabilistic models for each user's preferences (the CBF idea), it combines a society of users' preferences to predict an active user's preferences (the CF idea). While retaining an intuitive explanation, the combination scheme can be interpreted as a hierarchical Bayesian approach in which a common prior distribution is learned from related experiments. It does not require a global training stage and thus can incrementally incorporate new data. We report results based on two data sets, the neuters-21578 text data set and a data base of user opionions on art images. For both data sets, collaborative ensemble achieved excellent performance in terms of recommendation accuracy. In addition to recommendation engines, collaborative ensemble learning is applicable to problems typically solved via classical hierarchical Bayes, like multisensor fusion and multitask learning.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {616–623},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100660,
author = {Yuan, Changhe and Druzdzel, Marek J.},
title = {An Importance Sampling Algorithm Based on Evidence Pre-Propagation},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Precision achieved by stochastic sampling algorithms for Bayesian networks typically deteriorates in face of extremely unlikely evidence. To address this problem, we propose the Evidence Pre-propagation Importance Sampling algorithm (EPIS-BN), an importance sampling algorithm that computes an approximate importance function using two techniques: loopy belief propagation [19, 25] and ε-cutoff heuristic [2]. We tested the performance of EPIS-BN on three large real Bayesian networks: ANDES [3], CPCS [21], and PATHFINDER[11]. We observed that on each of these networks the EPIS-BN algorithm outperforms AISBN [2], the current state of the art algorithm, while avoiding its costly learning stage.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {624–631},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

@inproceedings{10.5555/2100584.2100661,
author = {Zhang, Jiji and Spirtes, Peter},
title = {Strong Faithfulness and Uniform Consistency in Causal Inference},
year = {2002},
isbn = {0127056645},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {A fundamental question in causal inference is whether it is possible to reliably infer manipulation effects from observational data. There are a variety of senses of asymptotic reliability in the statistical literature, among which the most commonly discussed frequentist notions are pointwise consistency and uniform consistency (see, e.g. Bickel, Doksum [2001]). Uniform consistency is in general preferred to pointwise consistency because the former allows us to control the worst case error bounds with a finite sample size. In the sense of pointwise consistency, several reliable causal inference algorithms have been constructed under the Markov and Faithfulness assumptions [Pearl 2000, Spirtes et al. 2001]. In the sense of uniform consistency, however, reliable causal inference is impossible under the two assumptions when time order is unknown and/or latent confounders are present [Robins et al. 20001. In this paper we present two natural generalizations of the Faithfulness assumption in the context of structural equation models, under which we show that the typical algorithms in the literature (in some cases with modifications) are uniformly consistent even when the time order is unknown. We also discuss the situation where latent confounders may be present and the sense in which the Faithfulness assumption is a limiting case of the stronger assumptions.},
booktitle = {Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence},
pages = {632–639},
numpages = {8},
location = {Acapulco, Mexico},
series = {UAI'03}
}

