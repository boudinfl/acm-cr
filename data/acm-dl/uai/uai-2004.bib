@inproceedings{10.5555/1036843.1036920,
author = {Zitnick, C. Lawrence and Kanade, Takeo},
title = {Maximum Entropy for Collaborative Filtering},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Within the task of collaborative filtering two challenges for computing conditional probabilities exist. First, the amount of training data available is typically sparse with respect to the size of the domain. Thus, support for higher-order interactions is generally not present. Second, the variables that we are conditioning upon vary for each query. That is, users label different variables during each query. For this reason, there is no consistent input to output mapping. To address these problems we purpose a maximum entropy approach using a non-standard measure of entropy. This approach can be simplified to solving a set of linear equations that can be efficiently solved.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {636–643},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036919,
author = {Yuan, Changhe and Lu, Tsai-Ching and Druzdzel, Marek J.},
title = {Annealed MAP},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Maximum a Posteriori assignment (MAP) is the problem of finding the most probable instantiation of a set of variables given the partial evidence on the other variables in a Bayesian network. MAP has been shown to be a NP-hard problem [22], even for constrained networks, such as polytrees [18]. Hence, previous approaches often fail to yield any results for MAP problems in large complex Bayesian networks. To address this problem, we propose ANNEALEDMAP algorithm, a simulated annealing-based MAP algorithm. The ANNEALEDMAP algorithm simulates a non-homogeneous Markov chain whose invariant function is a probability density that concentrates itself on the modes of the target density. We tested this algorithm on several real Bayesian networks. The results show that, while maintaining good quality of the MAP solutions, the ANNEALEDMAP algorithm is also able to solve many problems that are beyond the reach of previous approaches.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {628–635},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036918,
author = {Yu, Huizhen and Bertsekas, Dimitri P.},
title = {Discretized Approximations for POMDP with Average Cost},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we propose a new lower approximation scheme for POMDP with discounted and average cost criterion. The approximating functions are determined by their values at a finite number of belief points, and can be computed efficiently using value iteration algorithms for finite-state MDP. While for discounted problems several lower approximation schemes have been proposed earlier, ours seems the first of its kind for average cost problems. We focus primarily on the average cost case, and we show that the corresponding approximation can be computed efficiently using multi-chain algorithms for finite-state MDP. We give a preliminary analysis showing that regardless of the existence of the optimal average cost <i>J*</i> in the POMDP, the approximation obtained is a lower bound of the liminf optimal average cost function, and can also be used to calculate an upper bound on the limsup optimal average cost function, as well as bounds on the cost of executing the stationary policy associated with the approximation. We show the convergence of the cost approximation, when the optimal average cost is constant and the optimal differential cost is continuous.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {619–627},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036917,
author = {Xiong, Xuejian and Chan, Kap Luk and Tan, Kian Lee},
title = {Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy Clustering},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, a similarity-driven cluster merging method is proposed for unsupervised fuzzy clustering. The cluster merging method is used to resolve the problem of cluster validation. Starting with an overspecified number of clusters in the data, pairs of similar clusters are merged based on the proposed similarity-driven cluster merging criterion. The similarity between clusters is calculated by a fuzzy cluster similarity matrix, while an adaptive threshold is used for merging. In addition, a modified generalized objective function is used for prototype-based fuzzy clustering. The function includes the <i>p</i>-norm distance measure as well as principal components of the clusters. The number of the principal components is determined automatically from the data being clustered. The properties of this unsupervised fuzzy clustering algorithm are illustrated by several experiments.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {611–618},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036916,
author = {Xing, Eric P. and Jordan, Michael I. and Russell, Stuart},
title = {Graph Partition Strategies for Generalized Mean Field Inference},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {An autonomous variational inference algorithm for arbitrary graphical models requires the ability to optimize variational approximations over the space of model parameters as well as over the choice of tractable families used for the variational approximation. In this paper, we present a novel combination of graph partitioning algorithms with a generalized mean field (GMF) inference algorithm. This combination optimizes over disjoint clustering of variables and performs inference using those clusters. We provide a formal analysis of the relationship between the graph cut and the GMF approximation, and explore several graph partition strategies empirically. Our empirical results provide rather clear support for a weighted version of MinCut as a useful clustering algorithm for GMF inference, which is consistent with the implications from the formal analysis.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {602–610},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036915,
author = {Wellner, Ben and McCallum, Andrew and Peng, Fuchun and Hay, Michael},
title = {An Integrated, Conditional Model of Information Extraction and Coreference with Application to Citation Matching},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Although information extraction and coreference resolution appear together in many applications, most current systems perform them as independent steps. This paper describes an approach to integrated inference for extraction and coreference based on conditionally-trained undirected graphical models. We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning. On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {593–601},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036914,
author = {Welling, Max},
title = {On the Choice of Regions for Generalized Belief Propagation},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Generalized belief propagation (GBP) has proven to be a promising technique for approximate inference tasks in AI and machine learning. However, the choice of a good set of clusters to be used in GBP has remained more of an art then a science until this day. This paper proposes a sequential approach to adding new clusters of nodes and their interactions (i.e. "regions") to the approximation. We first review and analyze the recently introduced region graphs and find that three kinds of operations ("split", "merge" and "death") leave the free energy and (under some conditions) the fixed points of GBP invariant. This leads to the notion of "weakly irreducible" regions as the natural candidates to be added to the approximation. Computational complexity of the GBP algorithm is controlled by restricting attention to regions with small "region-width". Combining the above with an efficient (i.e. local in the graph) measure to predict the improved accuracy of GBP leads to the sequential "region pursuit" algorithm for adding new regions bottom-up to the region graph. Experiments show that this algorithm can indeed perform close to optimally.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {585–592},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036913,
author = {Wang, Bo and Titterington, D. M.},
title = {Convergence and Asymptotic Normality of Variational Bayesian Approximations for Exponential Family Models with Missing Values},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We study the properties of variational Bayes approximations for exponential family models with missing values. It is shown that the iterative algorithm for obtaining the variational Bayesian estimator converges locally to the true value with probability 1 as the sample size becomes indefinitely large. Moreover, the variational posterior distribution is proved to be asymptotically normal.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {577–584},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036912,
author = {van der Gaag, Linda C. and Bodlaender, Hans L. and Feelders, Ad},
title = {Monotonicity in Bayesian Networks},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {For many real-life Bayesian networks, common knowledge dictates that the output established for the main variable of interest increases with higher values for the observable variables. We define two concepts of monotonicity to capture this type of knowledge. We say that a network is <i>isotone in distribution</i> if the probability distribution computed for the output variable given specific observations is stochastically dominated by any such distribution given higher-ordered observations; a network is <i>isotone in mode</i> if a probability distribution given higher observations has a higher mode. We show that establishing whether a network exhibits any of these properties of monotonicity is coNP<sup>PP</sup>-complete in general, and remains coNP-complete for poly-trees. We present an approximate algorithm for deciding whether a network is monotone in distribution and illustrate its application to a real-life network in oncology.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {569–576},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036911,
author = {Tian, Jin},
title = {Identifying Conditional Causal Effects},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper concerns the assessment of the effects of actions from a combination of nonexperimental data and causal assumptions encoded in the form of a directed acyclic graph in which some variables are presumed to be unobserved. We provide a procedure that systematically identifies cause effects between two sets of variables conditioned on some other variables, in time polynomial in the number of variables in the graph. The identifiable conditional causal effects are expressed in terms of the observed joint distribution.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {561–568},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036910,
author = {Thiesson, Bo and Chickering, David Maxwell and Heckerman, David and Meek, Christopher},
title = {ARMA Time-Series Modeling with Graphical Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We express the classic ARMA time-series model as a directed graphical model. In doing so, we find that the deterministic relationships in the model make it effectively impossible to use the EM algorithm for learning model parameters. To remedy this problem, we replace the deterministic relationships with Gaussian distributions having a small variance, yielding the stochastic ARMA (σARMA) model. This modification allows us to use the EM algorithm to learn parameters and to forecast, even in situations where some data is missing. This modification, in conjunction with the graphical-model approach, also allows us to include cross predictors in situations where there are multiple time series and/or additional non-temporal covariates. More surprising, experiments suggest that the move to stochastic ARMA yields improved accuracy through better smoothing. We demonstrate improvements afforded by cross prediction and better smoothing on real data.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {552–560},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036909,
author = {Tennenholtz, Moshe},
title = {Reputation Systems: An Axiomatic Approach},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about uncertainty and multi-agent systems. When the set of agents and the set of alternatives coincide, we get the so-called reputation systems setting. Famous types of reputation systems include page ranking in the context of search engines and traders ranking in the context of e-commerce. In this paper we present the first axiomatic study of reputation systems. We present three basic postulates that the desired/aggregated social ranking should satisfy and prove an impossibility theorem showing that no appropriate social ranking, satisfying all requirements, exists. Then we show that by relaxing any of these requirements an appropriate social ranking can be found. We first study reputation systems with (only) positive feedbacks. This setting refers to systems where agents' votes are interpreted as indications for the importance of other agents, as is the case in page ranking. Following this, we discuss the case of negative feedbacks, a most common situation in e-commerce settings, where traders may complain about the behavior of others. Finally, we discuss the case where both positive and negative feedbacks are available.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {544–551},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036908,
author = {Stauffer, Chris},
title = {Factored Latent Analysis for Far-Field Tracking Data},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper uses Factored Latent Analysis (FLA) to learn a factorized, segmental representation for observations of tracked objects over time. Factored Latent Analysis is latent class analysis in which the observation space is subdivided and each aspect of the original space is represented by a separate latent class model. One could simply treat these factors as completely independent and ignore their interdependencies or one could concatenate them together and attempt to learn latent class structure for the complete observation space. Alternatively, FLA allows the interdependencies to be exploited in estimating an effective model, which is also capable of representing a factored latent state. In this paper, FLA is used to learn a set of factored latent classes to represent different modalities of observations of tracked objects. Different characteristics of the state of tracked objects are each represented by separate latent class models, including normalized size, normalized speed, normalized direction, and position. This model also enables effective temporal segmentation of these sequences. This method is data-driven, unsupervised using only pairwise observation statistics. This data-driven and unsupervised activity classification technique exhibits good performance in multiple challenging environments.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {536–543},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036907,
author = {Smorodinsky, Rann and Tennenholtz, Moshe},
title = {Sequential Information Elicitation in Multi-Agent Systems},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We introduce the study of sequential information elicitation in strategic multi-agent systems. In an information elicitation setup a center attempts to compute the value of a function based on private information (a-k-a secrets) accessible to a set of agents. We consider the classical multi-party computation setup where each agent is interested in knowing the result of the function. However, in our setting each agent is strategic, and since acquiring information is costly, an agent may be tempted not spending the efforts of obtaining the information, free-riding on other agents' computations. A mechanism which elicits agents' secrets and performs the desired computation defines a game. A mechanism is 'appropriate' if there exists an equilibrium in which it is able to elicit (sufficiently many) agents' secrets and perform the computation, for all possible secret vectors. We characterize a general efficient procedure for determining an appropriate mechanism, if such mechanism exists. Moreover, we also address the existence problem, providing a polynomial algorithm for verifying the existence of an appropriate mechanism.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {528–535},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036906,
author = {Smith, Trey and Simmons, Reid},
title = {Heuristic Search Value Iteration for POMDPs},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present a novel POMDP planning algorithm called heuristic search value iteration (HSVI). HSVI is an anytime algorithm that returns a policy and a provable bound on its regret with respect to the optimal policy. HSVI gets its power by combining two well-known techniques: attention-focusing search heuristics and piecewise linear convex representations of the value function. HSVI's soundness and convergence have been proven. On some bench-mark problems from the literature, HSVI displays speedups of greater than 100 with respect to other state-of-the-art POMDP value iteration algorithms. We also apply HSVI to a new rover exploration problem 10 times larger than most POMDP problems in the literature.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {520–527},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036905,
author = {Singh, Satinder and James, Michael R. and Rudary, Matthew R.},
title = {Predictive State Representations: A New Theory for Modeling Dynamical Systems},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Modeling dynamical systems, both for control purposes and to make predictions about their behavior, is ubiquitous in science and engineering. Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems. The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the system as a set of predictions of observable outcomes of experiments one can do in the system. This makes PSRs rather different from history-based models such as n<sup>th</sup>-order Markov models and hidden-state-based models such as HMMs and POMDPs. We introduce an interesting construct, the system-dynamics matrix, and show how PSRs can be derived simply from it. We also use this construct to show formally that PSRs are more general than both n<sup>th</sup>-order Markov models and HMMs/POMDPs. Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {512–519},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036904,
author = {Shalizi, Cosma Rohilla and Shalizi, Kristina Lisa},
title = {Blind Construction of Optimal Nonlinear Recursive Predictors for Discrete Sequences},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present a new method for nonlinear prediction of discrete random sequences under minimal structural assumptions. We give a mathematical construction for optimal predictors of such processes, in the form of hidden Markov models. We then describe an algorithm, CSSR (Causal-State Splitting Reconstruction), which approximates the ideal predictor from data. We discuss the reliability of CSSR, its data requirements, and its performance in simulations. Finally, we compare our approach to existing methods using variable-length Markov models and cross-validated hidden Markov models, and show theoretically and experimentally that our method delivers results superior to the former and at least comparable to the latter.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {504–511},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036903,
author = {Schubert, Lenhart K.},
title = {A New Characterization of Probabilities in Bayesian Networks},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We characterize probabilities in Bayesian networks in terms of algebraic expressions called quasi-probabilities. These are arrived at by casting Bayesian networks as noisy AND-OR-NOT networks, and viewing the subnetworks that lead to a node as arguments for or against a node. Quasi-probabilities are in a sense the "natural" algebra of Bayesian networks: we can easily compute the marginal quasi-probability of any node recursively, in a compact form; and we can obtain the joint quasi-probability of any set of nodes by multiplying their marginals (using an idempotent product operator). Quasi-probabilities are easily manipulated to improve the efficiency of probabilistic inference. They also turn out to be representable as square-wave pulse trains, and joint and marginal distributions can be computed by multiplication and complementation of pulse trains.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {495–503},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036902,
author = {Rosen-Zvi, Michal and Griffiths, Thomas and Steyvers, Mark and Smyth, Padhraic},
title = {The Author-Topic Model for Authors and Documents},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications to computing similarity between authors and entropy of author output.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {487–494},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036901,
author = {Renooij, Silja and van der Gaag, Linda C.},
title = {Evidence-Invariant Sensitivity Bounds},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The sensitivities revealed by a sensitivity analysis of a probabilistic network typically depend on the entered evidence. For a real-life network therefore, the analysis is performed a number of times, with different evidence. Although efficient algorithms for sensitivity analysis exist, a complete analysis is often infeasible because of the large range of possible combinations of observations. In this paper we present a method for studying sensitivities that are invariant to the evidence entered. Our method builds upon the idea of establishing bounds between which a parameter can be varied without ever inducing a change in the most likely value of a variable of interest.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {479–486},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036900,
author = {Reeves, Daniel M. and Wellman, Michael P.},
title = {Computing Best-Response Strategies in Infinite Games of Incomplete Information},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We describe an algorithm for computing best-response strategies in a class of two-player infinite games of incomplete information, defined by payoffs piecewise linear in agents' types and actions, conditional on linear comparisons of agents' actions. We show that this class includes many well-known games including a variety of auctions and a novel allocation game. In some cases, the best-response algorithm can be iterated to compute Bayes-Nash equilibria. We demonstrate the efficacy of our approach on existing and new games.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {470–478},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036899,
author = {Ravikumar, Pradeep and Lafferty, John},
title = {Variational Chernoff Bounds for Graphical Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Recent research has made significant progress on the problem of bounding log partition functions for exponential family graphical models. Such bounds have associated dual parameters that are often used as heuristic estimates of the marginal probabilities required in inference and learning. However these variational estimates do not give rigorous bounds on marginal probabilities, nor do they give estimates for probabilities of more general events than simple marginals. In this paper we build on this recent work by deriving rigorous upper and lower bounds on event probabilities for graphical models. Our approach is based on the use of generalized Chernoff bounds to express bounds on event probabilities in terms of convex optimization problems; these optimization problems, in turn, require estimates of generalized log partition functions. Simulations indicate that this technique can result in useful, rigorous bounds to complement the heuristic variational estimates, with comparable computational cost.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {462–469},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036898,
author = {Ravikumar, Pradeep and Cohen, William W.},
title = {A Hierarchical Graphical Model for Record Linkage},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The task of matching co-referent records is known among other names as record linkage. For large record-linkage problems, often there is little or no labeled data available, but unlabeled data shows a reasonably clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods. In this paper, we describe a hierarchical graphical model framework for the record-linkage problem in an unsupervised setting. In addition to proposing new methods, we also cast existing unsupervised probabilistic record-linkage methods in this framework. Some of the techniques we propose to minimize overfitting in the above model are of interest in the general graphical model setting. We describe a method for incorporating monotonicity constraints in a graphical model. We also outline a bootstrapping approach of using "single-field" classifiers to noisily label latent variables in a hierarchical model. Experimental results show that our proposed unsupervised methods perform quite competitively even with fully supervised record-linkage methods.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {454–461},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036897,
author = {Pearl, Judea},
title = {Robustness of Causal Claims},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {A causal claim is any assertion that invokes causal relationships between variables, for example, that a drug has a certain effect on preventing a disease. Causal claims are established through a combination of data and a set of causal assumptions called a "causal model." A claim is robust when it is insensitive to violations of some of the causal assumptions embodied in the model. This paper gives a formal definition of this notion of robustness, and establishes a graphical condition for quantifying the degree of robustness of a given causal claim. Algorithms for computing the degree of robustness are also presented.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {446–453},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036896,
author = {Paskin, Mark A. and Guestrin, Carlos E.},
title = {Robust Probabilistic Inference in Distributed Systems},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Probabilistic inference problems arise naturally in distributed systems such as sensor networks and teams of mobile robots. Inference algorithms that use message passing are a natural fit for distributed systems, but they must be robust to the failure situations that arise in real-world settings, such as unreliable communication and node failures. Unfortunately, the popular sum--product algorithm can yield very poor estimates in these settings because the nodes' beliefs before convergence can be arbitrarily different from the correct posteriors. In this paper, we present a new message passing algorithm for probabilistic inference which provides several crucial guarantees that the standard sum--product algorithm does not. Not only does it converge to the correct posteriors, but it is also guaranteed to yield a principled approximation at any point before convergence. In addition, the computational complexity of the message passing updates depends only upon the model, and is independent of the network topology of the distributed system. We demonstrate the approach with detailed experimental results on a distributed sensor calibration task using data from an actual sensor network deployment.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {436–445},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036895,
author = {Orlitsky, Alon and Santhanam, Narayana P. and Viswanathan, Krishnamurthy and Zhang, Junan},
title = {On Modeling Profiles Instead of Values},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We consider the problem of estimating the distribution underlying an observed sample of data. Instead of maximum likelihood, which maximizes the probability of the observed values, we propose a different estimate, the <i>high-profile</i> distribution, which maximizes the probability of the observed <i>profile</i>---the number of symbols appearing any given number of times. We determine the high-profile distribution of several data samples, establish some of its general properties, and show that when the number of distinct symbols observed is small compared to the data size, the high-profile and maximum-likelihood distributions are roughly the same, but when the number of symbols is large, the distributions differ, and high-profile better explains the data.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {426–435},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036894,
author = {Nielsen, Rodney D.},
title = {MOB-ESP and Other Improvements in Probability Estimation},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {A key prerequisite to optimal reasoning under uncertainty in intelligent systems is to start with good class probability estimates. This paper improves on the current best probability estimation trees (Bagged-PETs) and also presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using several benchmark datasets and multiple metrics. These experiments show that MOB-ESP outputs significantly more accurate class probabilities than either the baseline B-PETs algorithm or the enhanced version presented here (EB-PETs). These results are based on metrics closely associated with the average accuracy of the predictions. MOB-ESP also provides much better probability rankings than B-PETs. The paper further suggests how these estimation techniques can be applied in concert with a broader category of classifiers.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {418–425},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036893,
author = {Narasimhan, Mukund and Bilmes, Jeff},
title = {PAC-Learning Bounded Tree-Width Graphical Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the class of strongly connected graphical models with tree-width at most <i>k</i> can be properly efficiently PAC-learnt with respect to the Kullback-Leibler Divergence. Previous approaches to this problem, such as those of Chow ([1]), and Hoffgen ([7]) have shown that this class is PAC-learnable by reducing it to a combinatorial optimization problem. However, for <i>k</i> &gt; 1, this problem is NP-complete ([15]), and so unless P=NP, these approaches will take exponential amounts of time. Our approach differs significantly from these, in that it first attempts to find approximate conditional independencies by solving (polynomially many) submodular optimization problems, and then using a dynamic programming formulation to combine the approximate conditional independence information to derive a graphical model with underlying graph of the tree-width specified. This gives us an efficient (polynomial time in the number of random variables) PAC-learning algorithm which requires only polynomial number of samples of the true distribution, and only polynomial running time.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {410–417},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036892,
author = {Nachman, Iftach and Elidan, Gal and Friedman, Nir},
title = {"Ideal Parent" Structure Learning for Continuous Variable Networks},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In recent years, there is a growing interest in learning Bayesian networks with continuous variables. Learning the structure of such networks is a computationally expensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks with hidden variables. We present a general method for significantly speeding the structure search algorithm for continuous variable networks with common parametric distributions. Importantly, our method facilitates the addition of new hidden variables into the network structure efficiently. We demonstrate the method on several data sets, both for learning structure on fully observable data, and for introducing new hidden variables during structure search.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {400–409},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036891,
author = {Murray, Iain and Ghahramani, Zoubin},
title = {Bayesian Learning in Undirected Graphical Models: Approximate MCMC Algorithms},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bayesian learning in undirected graphical models---computing posterior distributions over parameters and predictive quantities---is exceptionally difficult. We conjecture that for general undirected models, there are no tractable MCMC (Markov Chain Monte Carlo) schemes giving the correct equilibrium distribution over parameters. While this intractability, due to the partition function, is familiar to those performing parameter optimisation, Bayesian learning of posterior distributions over undirected model parameters has been unexplored and poses novel challenges. We propose several approximate MCMC schemes and test on fully observed binary models (Boltzmann machines) for a small coronary heart disease data set and larger artificial systems. While approximations must perform well on the model, their interaction with the sampling scheme is also important. Samplers based on variational mean-field approximations generally performed poorly, more advanced methods using loopy propagation, brief sampling and stochastic dynamics lead to acceptable parameter posteriors. Finally, we demonstrate these techniques on a Markov random field with hidden variables.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {392–399},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036890,
author = {McAllester, David and Collins, Michael and Pereira, Fernando},
title = {Case-Factor Diagrams for Structured Probabilistic Modeling},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We introduce a probabilistic formalism subsuming Markov random fields of bounded tree width and probabilistic context free grammars. Our models are based on a representation of Boolean formulas that we call case-factor diagrams (CFDs). CFDs are similar to binary decision diagrams (BDDs) but are concise for circuits of bounded tree width (unlike BDDs) and can concisely represent the set of parse trees over a given string under a given context free grammar (also unlike BDDs). A probabilistic model consists of a CFD defining a feasible set of Boolean assignments and a weight (or cost) for each individual Boolean variable. We give an inside-outside algorithm for simultaneously computing the marginal of each Boolean variable, and a Viterbi algorithm for finding the mininum cost variable assignment. Both algorithms run in time proportional to the size of the CFD.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {382–391},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036889,
author = {Mao, Yongyi and Kschischang, Frank R. and Frey, Brendan J.},
title = {Convolutional Factor Graphs as Probabilistic Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Based on a recent development in the area of error control coding, we introduce the notion of <i>convolutional factor graphs</i> (CFGs) as a new class of probabilistic graphical models. In this context, the conventional factor graphs are referred to as multiplicative factor graphs (MFGs). This paper shows that CFGs are natural models for probability functions when summation of independent latent random variables is involved. In particular, CFGs capture a large class of linear models, where the linearity is in the sense that the observed variables are obtained as a linear transformation of the latent variables taking arbitrary distributions. We use Gaussian models and independent factor models as examples to demonstrate the use of CFGs.The requirement of a linear transformation between latent variables (with certain independence restriction) and the observed variables, to an extent, limits the modelling flexibility of CFGs. This structural restriction however provides a powerful analytic tool to the framework of CFGs; that is, upon taking the Fourier transform of the function represented by the CFG, the resulting function is represented by a MFG with identical structure. This Fourier transform duality allows inference problems on a CFG to be solved on the corresponding dual MFG.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {374–381},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036888,
author = {Madsen, Anders L.},
title = {An Empirical Evaluation of Possible Variations of Lazy Propagation},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {As real-world Bayesian networks continue to grow larger and more complex, it is important to investigate the possibilities for improving the performance of existing algorithms of probabilistic inference. Motivated by examples, we investigate the dependency of the performance of Lazy propagation on the message computation algorithm.We show how Symbolic Probabilistic Inference (SPI) and Arc-Reversal (AR) can be used for computation of clique to clique messages in the addition to the traditional use of Variable Elimination (VE).In addition, the paper presents the results of an empirical evaluation of the performance of Lazy propagation using VE, SPI, and AR as the message computation algorithm. The results of the empirical evaluation show that for most networks, the performance of inference did not depend on the choice of message computation algorithm, but for some randomly generated networks the choice had an impact on both space and time performance. In the cases where the choice had an impact, AR produced the best results.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {366–373},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036887,
author = {Madani, Omid and Lizotte, Daniel J. and Greiner, Russell},
title = {Active Model Selection},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Classical learning assumes the learner is given a labeled data sample, from which it learns a model. The field of Active Learning deals with the situation where the learner begins not with a training sample, but instead with resources that it can use to obtain information to help identify the optimal model. To better understand this task, this paper presents and analyses the simplified "(budgeted) active model selection" version, which captures the pure exploration aspect of many active learning problems in a clean and simple problem formulation. Here the learner can use a fixed budget of "model probes" (where each probe evaluates the specified model on a random indistinguishable instance) to identify which of a given set of possible models has the highest expected accuracy. Our goal is a policy that sequentially determines which model to probe next, based on the information observed so far. We present a formal description of this task, and show that it is NP-hard in general. We then investigate a number of algorithms for this task, including several existing ones (eg, "Round-Robin", "Interval Estimation", "Gittins") as well as some novel ones (e.g., "Biased-Robin"), describing first their approximation properties and then their empirical performance on various problem instances. We observe empirically that the simple biased-robin algorithm significantly outperforms the other algorithms in the case of identical costs and priors.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {357–365},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036886,
author = {Li, Bing and Zha, Hongyuan and Chiaromonte, Francesca},
title = {Linear Contour Learning: A Method for Supervised Dimension Reduction},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We propose a novel approach to sufficient dimension reduction in regression, based on estimating contour directions of negligible variation for the response surface. These directions span the orthogonal complement of the minimal space relevant for the regression, and can be extracted according to a measure of the variation in the response, leading to <i>General Contour Regression</i> (GCR). In comparison to existing sufficient dimension reduction techniques, this contour-based methodology guarantees exhaustive estimation of the central space under ellipticity of the predictor distribution and very mild additional assumptions, while maintaining √<i>n</i>-consistency and computational ease. Moreover, it proves to be robust to departures from ellipticity. We also establish some useful population properties for GCR. Simulations to compare performance with that of standard techniques such as ordinary least squares, sliced inverse regression, principal hessian directions, and sliced average variance estimation confirm the advantages anticipated by theoretical analyses. We also demonstrate the use of contour-based methods on a data set concerning grades of students from Massachusetts colleges.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {349–356},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036885,
author = {Lebanon, Guy},
title = {An Extended \v{C}Encov-Campbell Characterization of Conditional Information Geometry},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We formulate and prove an axiomatic characterization of conditional information geometry, for both the normalized and the non-normalized cases. This characterization extends the axiomatic derivation of the Fisher geometry by \v{C}encov and Campbell to the cone of positive conditional models, and as a special case to the manifold of conditional distributions. Due to the close connection between the conditional <i>I</i>-divergence and the product Fisher information metric the characterization provides a new axiomatic interpretation of the primal problems underlying logistic regression and AdaBoost.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {341–348},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036884,
author = {Kuroki, Manabu and Cai, Zhihong},
title = {Selection of Identifiability Criteria for Total Effects by Using Path Diagrams},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Pearl has provided the back door criterion, the front door criterion and the conditional instrumental variable (IV) method as identifiability criteria for total effects. In some situations, these three criteria can be applied to identifying total effects simultaneously. For the purpose of increasing estimating accuracy, this paper compares the three ways of identifying total effects in terms of the asymptotic variance, and concludes that in some situations the superior of them can be recognized directly from the graph structure.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {333–340},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036883,
author = {Kuncheva, L. I. and Whitaker, C. J. and Cockcroft, P. D. and Hoare, Z. S. J.},
title = {Pre-Selection of Independent Binary Features: An Application to Diagnosing Scrapie in Sheep},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Suppose that the only available information in a multi-class problem are expert estimates of the conditional probabilities of occurrence for a set of binary features. The aim is to select a subset of features to be measured in subsequent data collection experiments. In the lack of any information about the dependencies between the features, we assume that all features are conditionally independent and hence choose the Naive Bayes classifier as the optimal classifier for the problem. Even in this (seemingly trivial) case of complete knowledge of the distributions, choosing an optimal feature subset is not straightforward. We discuss the properties and implementation details of Sequential Forward Selection (SFS) as a feature selection procedure for the current problem. A sensitivity analysis was carried out to investigate whether the same features are selected when the probabilities vary around the estimated values. The procedure is illustrated with a set of probability estimates for Scrapie in sheep.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {325–332},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036882,
author = {Kirshner, Sergey and Smyth, Padhraic and Robertson, Andrew W.},
title = {Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector Time Series},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We consider the problem of modeling discrete-valued vector time series data using extensions of Chow-Liu tree models to capture both dependencies across time and dependencies across variables. Conditional Chow-Liu tree models are introduced, as an extension to standard Chow-Liu trees, for modeling conditional rather than joint densities. We describe learning algorithms for such models and show how they can be used to learn parsimonious representations for the output distributions in hidden Markov models. These models are applied to the important problem of simulating and forecasting daily precipitation occurrence for networks of rain stations. To demonstrate the effectiveness of the models, we compare their performance versus a number of alternatives using historical precipitation data from Southwestern Australia and the Western United States. We illustrate how the structure and parameters of the models can be used to provide an improved meteorological interpretation of such data.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {317–324},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036881,
author = {Kim, Seyoung and Smyth, Padhraic and Luther, Stefan},
title = {Modeling Waveform Shapes with Random Effects Segmental Hidden Markov Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper we describe a general probabilistic framework for modeling waveforms such as heartbeats from ECG data. The model is based on segmental hidden Markov models (as used in speech recognition) with the addition of random effects to the generative model. The random effects component of the model handles shape variability across different waveforms within a general class of waveforms of similar shape. We show that this probabilistic model provides a unified framework for learning these models from sets of waveform data as well as parsing, classification, and prediction of new waveforms. We derive a computationally efficient EM algorithm to fit the model on multiple waveforms, and introduce a scoring method that evaluates a test waveform based on its shape. Results on two real-world data sets demonstrate that the random effects methodology leads to improved accuracy (compared to alternative approaches) on classification and segmentation of real-world waveforms.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {309–316},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036880,
author = {Kahn, Joseph M.},
title = {A Generative Bayesian Model for Aggregating Experts' Probabilities},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In order to improve forecasts, a decision-maker often combines probabilities given by various sources, such as human experts and machine learning classifiers. When few training data are available, aggregation can be improved by incorporating prior knowledge about the event being forecasted and about salient properties of the experts. To this end, we develop a generative Bayesian aggregation model for probabilistic classification. The model includes an event-specific prior, measures of individual experts' bias, calibration, accuracy, and a measure of dependence between experts. Rather than require absolute measures, we show that aggregation may be expressed in terms of <i>relative</i> accuracy between experts. The model results in a weighted logarithmic opinion pool (LogOps) that satisfies consistency criteria such as the external Bayesian property. We derive analytic solutions for independent and for exchangeable experts. Empirical tests demonstrate the model's use, comparing its accuracy with other aggregation methods.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {301–308},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036879,
author = {Jojic, Nebojsa and Caspi, Yaron and Reyes-Gomez, Manuel},
title = {Probabilistic Index Maps for Modeling Natural Signals},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {One of the major problems in modeling natural signals is that signals with very similar structure may locally have completely different measurements, e.g., images taken under different illumination conditions, or the speech signal captured in different environments. While there have been many successful attempts to address these problems in application-specific settings, we believe that underlying a large set of problems in signal representation is a representational deficiency of intensity-derived local measurements that are the basis of most efficient models. We argue that interesting structure in signals is better captured when the signal is defined as a matrix whose entries are discrete indices to a separate palette of possible measurements. In order to model the variability in signal structure, we define a signal class not by a single index map, but by a probability distribution over the index maps, which can be estimated from the data, and which we call probabilistic index maps. The existing algorithms can be adapted to work with this representation. Furthermore, the probabilistic index map representation leads to algorithms with computational costs proportional to either the size of the palette or the log of the size of the palette, making the cost of significantly increased invariance to non-structural changes quite bearable. We illustrate the benefits of the probabilistic index map representation in several applications in computer vision and speech processing.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {293–300},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036878,
author = {Jojic, Nebojsa and Jojic, Vladimir and Heckerman, David},
title = {Joint Discovery of Haplotype Blocks and Complex Trait Associations from SNP Sequences},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Haplotypes, the global patterns of DNA sequence variation, have important implications for identifying complex traits. Recently, blocks of limited haplotype diversity have been discovered in human chromosomes, intensifying the research on modelling the block structure as well as the transitions or co-occurrence of the alleles in these blocks as a way to compress the variability and infer the associations more robustly. The haplotype block structure analysis is typically complicated by the fact that the phase information for each SNP is missing, i.e., the observed allele pairs are not given in a consistent order across the sequence. The techniques for circumventing this require additional information, such as family data, or a more complex sequencing procedure. In this paper we present a hierarchical statistical model and the associated learning and inference algorithms that simultaneously deal with the allele ambiguity per locus, missing data, block estimation, and the complex trait association. While the block structure may differ from the structures inferred by other methods, which use the pedigree information or previously known alleles, the parameters we estimate, including the learned block structure and the estimated block transitions per locus, define a good model of variability in the set. The method is completely data-driven and can detect Chron's disease from the SNP data taken from the human chromosome 5q31 with the detection rate of 80% and a small error variance.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {286–292},
numpages = {7},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036877,
author = {Jin, Rong and Si, Luo},
title = {A Bayesian Approach toward Active Learning for Collaborative Filtering},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Collaborative filtering is a useful technique for exploiting the preference patterns of a group of users to predict the utility of items for the active user. In general, the performance of collaborative filtering depends on the number of rated examples given by the active user. The more the number of rated examples given by the active user, the more accurate the predicted ratings will be. Active learning provides an effective way to acquire the most informative rated examples from active users. Previous work on active learning for collaborative filtering only considers the expected loss function based on the estimated model, which can be misleading when the estimated model is inaccurate. This paper takes one step further by taking into account of the posterior distribution of the estimated model, which results in more robust active learning algorithm. Empirical studies with datasets of movie ratings show that when the number of ratings from the active user is restricted to be small, active learning methods only based on the estimated model don't perform well while the active learning method using the model distribution achieves substantially better performance.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {278–285},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036876,
author = {Hyafil, Nathanael and Boutilier, Craig},
title = {Regret Minimizing Equilibria and Mechanisms for Games with Strict Type Uncertainty},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Mechanism design has found considerable application to the construction of agent-interaction protocols. In the standard setting, the type (e.g., utility function) of an agent is not known by other agents, nor is it known by the mechanism designer. When this uncertainty is quantified probabilistically, a mechanism induces a game of incomplete information among the agents. However, in many settings, uncertainty over utility functions cannot easily be quantified. We consider the problem of incomplete information games in which type uncertainty is <i>strict</i> or unquantified. We propose the use of <i>minimax regret</i> as a decision criterion in such games, a robust approach for dealing with type uncertainty. We define <i>minimax-regret equilibria</i> and prove that these exist in mixed strategies for finite games. We also consider the problem of mechanism design in this framework by adopting minimax regret as an optimization criterion for the designer itself, and study automated optimization of such mechanisms.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {268–277},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036875,
author = {Howard, Andrew and Jebara, Tony},
title = {Dynamical Systems Trees},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We propose dynamical systems trees (DSTs) as a flexible class of models for describing multiple process that interact via a hierarchy of aggregating parent chains. DSTs extend Kalman filters, hidden Markov models and nonlinear dynamical systems to an interactive group scenario. Various individual processes interact as communities and sub-communities in a tree structure that is unrolled in time. To accommodate nonlinear temporal activity, each individual leaf process is modeled as a dynamical system containing discrete and/or continuous hidden states with discrete and/or Gaussian emissions. Subsequent higher level parent processes act like hidden Markov models and mediate the interaction between leaf processes or between other parent processes in the hierarchy. Aggregator chains are parents of child processes that they combine and mediate, yielding a compact overall parameterization. We provide tractable inference and learning algorithms for arbitrary DST topologies via an efficient structured mean-field algorithm. The diverse applicability of DSTs is demonstrated by experiments on gene expression data and by modeling group behavior in the setting of an American football game.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {260–267},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036874,
author = {Hooper, Peter M.},
title = {Dependent Dirichlet Priors and Optimal Linear Estimators for Belief Net Parameters},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {A Bayesian belief network is a model of a joint distribution over a finite set of variables, with a DAG structure representing immediate dependencies among the variables. For each node, a table of parameters (CP-table) represents local conditional probabilities, with rows indexed by conditioning events (assignments to parents). CP-table rows are usually modeled as independent random vectors, each assigned a Dirichlet prior distribution. The assumption that rows are independent permits a relatively simple analysis but may not reflect actual prior opinion about the parameters. Rows representing similar conditioning events often have similar conditional probabilities. This paper introduces a more flexible family of "dependent Dirichlet" prior distributions, where rows are not necessarily independent. Simple methods are developed to approximate the Bayes estimators of CP-table parameters with optimal linear estimators; i.e., linear combinations of sample proportions and prior means. This approach yields more efficient estimators by sharing information among rows. Improvements in efficiency can be substantial when a CP-table has many rows and samples sizes are small.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {251–259},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036873,
author = {Hamze, Firas and de Freitas, Nando},
title = {From Fields to Trees},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present new MCMC algorithms for computing the posterior distributions and expectations of the unknown variables in undirected graphical models with regular structure. For demonstration purposes, we focus on Markov Random Fields (MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to compute the posterior distribution of a particular tree exactly by conditioning on the remaining tree. These exact solutions allow us to construct efficient blocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree sampling is considerably more efficient than other partitioned sampling schemes and the naive Gibbs sampler, even in cases where loopy belief propagation fails to converge. We prove that tree sampling exhibits lower variance than the naive Gibbs sampler and other naive partitioning schemes using the theoretical measure of maximal correlation. We also construct new information theory tools for comparing different MCMC schemes and show that, under these, tree sampling is more efficient.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {243–250},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036872,
author = {Guestrin, Carlos and Hauskrecht, Milos and Kveton, Branislav},
title = {Solving Factored MDPs with Continuous and Discrete Variables},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Although many real-world stochastic planning problems are more naturally formulated by hybrid models with both discrete and continuous variables, current state-of-the-art methods cannot adequately address these problems. We present the first framework that can exploit problem structure for modeling and solving hybrid problems efficiently. We formulate these problems as hybrid Markov decision processes (MDPs with continuous and discrete state and action variables), which we assume can be represented in a factored way using a hybrid dynamic Bayesian network (hybrid DBN). This formulation also allows us to apply our methods to collaborative multiagent settings. We present a new linear program approximation method that exploits the structure of the hybrid MDP and lets us compute approximate value functions more efficiently. In particular, we describe a new factored discretization of continuous variables that avoids the exponential blow-up of traditional approaches. We provide theoretical bounds on the quality of such an approximation and on its scale-up potential. We support our theoretical arguments with experiments on a set of control problems with up to 28-dimensional continuous state space and 22-dimensional action space.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {235–242},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036871,
author = {Gr\"{u}nwald, Peter D. and Halpern, Joseph Y.},
title = {When Ignorance is Bliss},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {It is commonly-accepted wisdom that more information is better, and that information should never be ignored. Here we argue, using both a Bayesian and a non-Bayesian analysis, that in some situations you are better off ignoring information if your uncertainty is represented by a set of probability measures. These include situations in which the information <i>is</i> relevant for the prediction task at hand. In the non-Bayesian analysis, we show how ignoring information avoids <i>dilation,</i> the phenomenon that additional pieces of information sometimes lead to an increase in uncertainty. In the Bayesian analysis, we show that for small sample sizes and certain prediction tasks, the Bayesian posterior based on a non-informative prior yields worse predictions than simply ignoring the given information.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {226–234},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036870,
author = {Gretton, Charles and Thi\'{e}baux, Sylvie},
title = {Exploiting First-Order Regression in Inductive Policy Selection},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We consider the problem of computing optimal generalised policies for relational Markov decision processes. We describe an approach combining some of the benefits of purely inductive techniques with those of symbolic dynamic programming methods. The latter reason about the optimal value function using first-order decision-theoretic regression and formula rewriting, while the former, when provided with a suitable hypotheses language, are capable of generalising value functions or policies for small instances. Our idea is to use reasoning and in particular classical first-order regression to automatically generate a hypotheses language dedicated to the domain at hand, which is then used as input by an inductive solver. This approach avoids the more complex reasoning of symbolic dynamic programming while focusing the inductive solver's attention on concepts that are specifically relevant to the optimal value function for the domain considered.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {217–225},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036869,
author = {Greenwald, Amy and Boyan, Justin},
title = {Bidding under Uncertainty: Theory and Experiments},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper describes a study of agent bidding strategies, assuming combinatorial valuations for complementary and substitutable goods, in three auction environments: sequential auctions, simultaneous auctions, and the Trading Agent Competition (TAC) Classic hotel auction design, a hybrid of sequential and simultaneous auctions. The problem of bidding in sequential auctions is formulated as an MDP, and it is argued that expected marginal utility bidding is the optimal bidding policy. The problem of bidding in simultaneous auctions is formulated as a stochastic program, and it is shown by example that marginal utility bidding is not an optimal bidding policy, even in deterministic settings. Two alternative methods of approximating a solution to this stochastic program are presented: the first method, which relies on expected values, is optimal in deterministic environments; the second method, which samples the nondeterministic environment, is asymptotically optimal as the number of samples tends to infinity. Finally, experiments with these various bidding policies are described in the TAC Classic setting.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {209–216},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036868,
author = {Gogate, Vibhav and Dechter, Rina},
title = {A Complete Anytime Algorithm for Treewidth},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper, we present a Branch and Bound algorithm called <i>QuickBB</i> for computing the treewidth of an undirected graph. This algorithm performs a search in the space of perfect elimination ordering of vertices of the graph. The algorithm uses novel pruning and propagation techniques which are derived from the theory of graph minors and graph isomorphism. We present a new algorithm called minor-min-width for computing a lower bound on treewidth that is used within the branch and bound algorithm and which improves over earlier available lower bounds. Empirical evaluation of QuickBB on randomly generated graphs and benchmarks in Graph Coloring and Bayesian Networks shows that it is consistently better than complete algorithms like Quick-Tree [Shoikhet and Geiger, 1997] in terms of cpu time. QuickBB also has good anytime performance, being able to generate a better upper bound on treewidth of some graphs whose optimal treewidth could not be computed up to now.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {201–208},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036867,
author = {Globerson, Amir and Tishby, Naftali},
title = {The Minimum Information Principle for Discriminative Learning},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Exponential models of distributions are widely used in machine learning for classification and modelling. It is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. In this work, we argue that for classification tasks, mutual information is a more suitable information theoretic measure to be optimized. We show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classifiers. A game theoretic interpretation of our approach is then given, and several generalization bounds provided. We present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classification tasks. The results show that minimum information classifiers outperform the corresponding maximum entropy models.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {193–200},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036866,
author = {Giang, Phan H. and Sandilya, Sathyakama},
title = {Decision Making for Symbolic Probability},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper proposes a decision theory for a symbolic generalization of probability theory (SP). Darwiche and Ginsberg [2, 3] proposed SP to relax the requirement of using numbers for uncertainty while preserving desirable patterns of Bayesian reasoning. SP represents uncertainty by symbolic supports that are ordered partially rather than completely as in the case of standard probability. We show that a preference relation on acts that satisfies a number of intuitive postulates is represented by a utility function whose domain is a set of pairs of supports. We argue that a subjective interpretation is as useful and appropriate for SP as it is for numerical probability. It is useful because the subjective interpretation provides a basis for uncertainty elicitation. It is appropriate because we can provide a decision theory that explains how preference on acts is based on support comparison.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {185–192},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036865,
author = {Garcia, Luis David},
title = {Algebraic Statistics in Model Selection},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We develop the necessary theory in computational algebraic geometry to place Bayesian networks into the realm of algebraic statistics. We present an algebra-statistics dictionary focused on statistical modeling. In particular, we link the notion of <i>effective dimension</i> of a Bayesian network with the notion of <i>algebraic dimension</i> of a variety. We also obtain the independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables, via a generating set of an ideal of polynomials associated to the network. These results extend previous work on the subject. Finally, the relevance of these results for model selection is discussed.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {177–184},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036864,
author = {Gammerman, Alex and Kalnishkan, Yuri and Vovk, Vladimir},
title = {On-Line Prediction with Kernels and the Complexity Approximation Principle},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The paper describes an application of Aggregating Algorithm to the problem of regression. It generalizes earlier results concerned with plain linear regression to kernel techniques and presents an on-line algorithm which performs nearly as well as any oblivious kernel predictor. The paper contains the derivation of an estimate on the performance of this algorithm. The estimate is then used to derive an application of the Complexity Approximation Principle to kernel methods.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {170–176},
numpages = {7},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036863,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
title = {Metrics for Finite Markov Decision Processes},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {162–169},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036862,
author = {Feng, Zhengzhu and Dearden, Richard and Meuleau, Nicolas and Washington, Richard},
title = {Dynamic Programming for Structured Continuous Markov Decision Problems},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We describe an approach for exploiting structure in Markov Decision Processes with continuous state variables. At each step of the dynamic programming, the state space is dynamically partitioned into regions where the value function is the same throughout the region. We first describe the algorithm for piecewise constant representations. We then extend it to piecewise linear representations, using techniques from POMDPs to represent and reason about linear surfaces efficiently. We show that for complex, structured problems, our approach exploits the natural structure so that optimal solutions can be computed efficiently.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {154–161},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036861,
author = {Feng, Zhengzhu and Zilberstein, Shlomo},
title = {Region-Based Incremental Pruning for POMDPs},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present a major improvement to the incremental pruning algorithm for solving partially observable Markov decision processes. Our technique targets the cross-sum step of the dynamic programming (DP) update, a key source of complexity in POMDP algorithms. Instead of reasoning about the whole belief space when pruning the cross-sums, our algorithm divides the belief space into smaller regions and performs independent pruning in each region. We evaluate the benefits of the new technique both analytically and experimentally, and show that it produces very significant performance gains. The results contribute to the scalability of POMDP algorithms to domains that cannot be handled by the best existing techniques.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {146–153},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036860,
author = {Dubois, Didier and Fargier, H\'{e}l\`{e}ne},
title = {A Unified Framework for Order-of-Magnitude Confidence Relations},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The aim of this work is to provide a unified framework for ordinal representations of uncertainty lying at the crossroads between possibility and probability theories. Such confidence relations between events are commonly found in nonmonotonic reasoning, inconsistency management, or qualitative decision theory. They start either from probability theory, making it more qualitative, or from possibility theory, making it more expressive. We show these two trends converge to a class of genuine probability relations, numerically representable, that cumulate features of probability and possibility theories. We provide characterization results for these useful tools that preserve the qualitative nature of possibility rankings, while enjoying the power of expressivity of additive representations.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {138–145},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036859,
author = {Drton, Mathias and Richardson, Thomas S.},
title = {Iterative Conditional Fitting for Gaussian Ancestral Graph Models},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Ancestral graph models, introduced by Richardson and Spirtes (2002), generalize both Markov random fields and Bayesian networks to a class of graphs with a global Markov property that is closed under conditioning and marginalization. By design, ancestral graphs encode precisely the conditional independence structures that can arise from Bayesian networks with selection and unobserved (hidden/latent) variables. Thus, ancestral graph models provide a potentially very useful framework for exploratory model selection when unobserved variables might be involved in the data-generating process but no particular hidden structure can be specified. In this paper, we present the Iterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation in Gaussian ancestral graph models. The name reflects that in each step of the procedure a conditional distribution is estimated, subject to constraints, while a marginal distribution is held fixed. This approach is in duality to the well-known Iterative Proportional Fitting algorithm, in which marginal distributions are fitted while conditional distributions are held fixed.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {130–137},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036858,
author = {Dechter, Rina and Mateescu, Robert},
title = {Mixtures of Deterministic-Probabilistic Networks and Their AND/OR Search Space},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The paper introduces <i>mixed networks,</i> a new framework for expressing and reasoning with probabilistic and deterministic information. The framework combines belief networks with constraint networks, defining the semantics and graphical representation. We also introduce the AND/OR search space for graphical models, and develop a new linear space search algorithm. This provides the basis for understanding the benefits of processing the constraint information separately, resulting in the pruning of the search space. When the constraint part is tractable or has a small number of solutions, using the mixed representation can be exponentially more effective than using pure belief networks which model constraints as conditional probability tables.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {120–129},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036857,
author = {de Waal, Peter and van der Gaag, Linda C.},
title = {Stable Independence and Complexity of Representation},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The representation of independence relations generally builds upon the well-known semigraphoid axioms of independence. Recently, a representation has been proposed that captures a set of dominant statements of an independence relation from which any other statement can be generated by means of the axioms; the cardinality of this set is taken to indicate the complexity of the relation. Building upon the idea of dominance, we introduce the concept of stability to provide for a more compact representation of independence. We give an associated algorithm for establishing such a representation. We show that, with our concept of stability, many independence relations are found to be of lower complexity than with existing representations.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {112–119},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036856,
author = {Cozman, Fabio Gagliardi and de Campos, Cassio Polpo and Ide, Jaime Shinsuke and da Rocha, Jos\'{e} Carlos Ferreira},
title = {Propositional and Relational Bayesian Networks Associated with Imprecise and Qualitative Probabilistic Assessments},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper investigates a representation language with flexibility inspired by probabilistic logic and compactness inspired by relational Bayesian networks. The goal is to handle propositional and first-order constructs together with precise, imprecise, indeterminate and qualitative probabilistic assessments. The paper shows how this can be achieved through the theory of credal networks. New exact and approximate inference algorithms based on multilinear programming and iterated/loopy propagation of interval probabilities are presented; their superior performance, compared to existing ones, is shown empirically.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {104–111},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036855,
author = {Cooper, Gregory F. and Dash, Denver H. and Levander, John D. and Wong, Weng-Keen and Hogan, William R. and Wagner, Michael M.},
title = {Bayesian Biosurveillance of Disease Outbreaks},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Early, reliable detection of disease outbreaks is a critical problem today. This paper reports an investigation of the use of causal Bayesian networks to model spatio-temporal patterns of a non-contagious disease (respiratory anthrax infection) in a population of people. The number of parameters in such a network can become enormous, if not carefully managed. Also, inference needs to be performed in real time as population data stream in. We describe techniques we have applied to address both the modeling and inference challenges. A key contribution of this paper is the explication of assumptions and techniques that are sufficient to allow the scaling of Bayesian network modeling and inference to millions of nodes for real-time surveillance applications. The results reported here provide a proof-of-concept that Bayesian networks can serve as the foundation of a system that effectively performs Bayesian biosurveillance of disease outbreaks.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {94–103},
numpages = {10},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036854,
author = {Cobb, Barry R. and Shenoy, Prakash P.},
title = {Hybrid Influence Diagrams Using Mixtures of Truncated Exponentials},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Mixtures of truncated exponentials (MTE) potentials are an alternative to discretization for representing continuous chance variables in influence diagrams. Also, MTE potentials can be used to approximate utility functions. This paper introduces MTE influence diagrams, which can represent decision problems without restrictions on the relationships between continuous and discrete chance variables, without limitations on the distributions of continuous chance variables, and without limitations on the nature of the utility functions. In MTE influence diagrams, all probability distributions and the joint utility function (or its multiplicative factors) are represented by MTE potentials and decision nodes are assumed to have discrete state spaces. MTE influence diagrams are solved by variable elimination using a fusion algorithm.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {85–93},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036853,
author = {Ches\~{n}evar, Carlos I. and Simari, Guillermo R. and Alsinet, Teresa and Godo, Llu\'{\i}s},
title = {A Logic Programming Framework for Possibilistic Argumentation with Vague Knowledge},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Defeasible argumentation frameworks have evolved to become a sound setting to formalize commonsense, qualitative reasoning from incomplete and potentially inconsistent knowledge. Defeasible Logic Programming (DeLP) is a defeasible argumentation formalism based on an extension of logic programming. Although DeLP has been successfully integrated in a number of different real-world applications, DeLP cannot deal with explicit uncertainty, nor with vague knowledge, as defeasibility is directly encoded in the object language. This paper introduces P-DeLP, a new logic programming language that extends original DeLP capabilities for qualitative reasoning by incorporating the treatment of possibilistic uncertainty and fuzzy knowledge. Such features will be formalized on the basis of PGL, a possibilistic logic based on G\"{o}del fuzzy logic.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {76–84},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036852,
author = {Chan, Hei and Darwiche, Adnan},
title = {Sensitivity Analysis in Bayesian Networks: From Single to Multiple Parameters},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Previous work on sensitivity analysis in Bayesian networks has focused on single parameters, where the goal is to understand the sensitivity of queries to single parameter changes, and to identify single parameter changes that would enforce a certain query constraint. In this paper, we expand the work to multiple parameters which may be in the CPT of a single variable, or the CPTs of multiple variables. Not only do we identify the solution space of multiple parameter changes that would be needed to enforce a query constraint, but we also show how to find the optimal solution, that is, the one which disturbs the current probability distribution the least (with respect to a specific measure of disturbance). We characterize the computational complexity of our new techniques and discuss their applications to developing and debugging Bayesian networks, and to the problem of reasoning about the value (reliability) of new information.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {67–75},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036851,
author = {Buntine, Wray and Jakulin, Aleks},
title = {Applying Discrete PCA in Data Analysis},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {59–66},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036850,
author = {Brafman, Ronen I. and Domshlak, Carmel and Kogan, Tanya},
title = {Compact Value-Function Representations for Qualitative Preferences},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We consider the challenge of preference elicitation in systems that help users discover the most desirable item(s) within a given database. Past work on preference elicitation focused on structured models that provide a factored representation of users' preferences. Such models require less information to construct and support efficient reasoning algorithms. This paper makes two substantial contributions to this area: (1) Strong representation theorems for factored value functions. (2) A methodology that utilizes our representation results to address the problem of optimal item selection.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {51–58},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036849,
author = {Bidyuk, Bozhena and Dechter, Rina},
title = {On Finding Minimal <i>w</i>-Cutset},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {The complexity of a reasoning task over a graphical model is tied to the induced width of the underlying graph. It is well-known that the conditioning (assigning values) on a subset of variables yields a subproblem of the reduced complexity where instantiated variables are removed. If the assigned variables constitute a cycle-cutset, the rest of the network is singly-connected and therefore can be solved by linear propagation algorithms. A <i>w</i>-cutset is a generalization of a cycle-cutset defined as a subset of nodes such that the subgraph with cutset nodes removed has induced-width of <i>w</i> or less. In this paper we address the problem of finding a minimal <i>w</i>-cutset in a graph. We relate the problem to that of finding the minimal <i>w</i>-cutset of a tree-decomposition. The latter can be mapped to the well-known <i>set multi-cover</i> problem. This relationship yields a proof of NP-completeness on one hand and a greedy algorithm for finding a <i>w</i>-cutset of a tree decomposition on the other. Empirical evaluation of the algorithms is presented.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {43–50},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036848,
author = {Bhat, Navin A. R. and Leyton-Brown, Kevin},
title = {Computing Nash Equilibria of Action-Graph Games},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Action-graph games (AGGs) are a fully expressive game representation which can compactly express both strict and context-specific independence between players' utility functions. Actions are represented as nodes in a graph <i>G,</i> and the payoff to an agent who chose the action <i>s</i> depends only on the numbers of other agents who chose actions connected to <i>s.</i> We present algorithms for computing both symmetric and arbitrary equilibria of AGGs using a continuation method. We analyze the worst-case cost of computing the Jacobian of the payoff function, the exponential-time bottleneck step, and in all cases achieve exponential speedup. When the in-degree of <i>G</i> is bounded by a constant and the game is symmetric, the Jacobian can be computed in polynomial time.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {35–42},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036847,
author = {Bayer-Zubek, Valentina},
title = {Learning Diagnostic Policies from Examples by Systematic Search},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {A <i>diagnostic policy</i> specifies what test to perform next, based on the results of previous tests, and when to stop and make a diagnosis. Cost-sensitive diagnostic policies perform tradeoffs between (a) the <i>costs of tests</i> and (b) the <i>costs of misdiagnoses.</i> An optimal diagnostic policy minimizes the expected total cost. We formalize this diagnosis process as a Markov Decision Process (MDP). We investigate two types of algorithms for solving this MDP: systematic search based on the AO* algorithm and greedy search (particularly the Value of Information method). We investigate the issue of learning the MDP probabilities from examples, but only as they are relevant to the search for good policies. We do not learn nor assume a Bayesian network for the diagnosis process. Regularizers are developed that control overfitting and speed up the search. This research is the first that integrates overfitting prevention into systematic search. The paper has two contributions: it discusses the factors that make systematic search feasible for diagnosis, and it shows experimentally, on benchmark data sets, that systematic search methods produce better diagnostic policies than greedy methods.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {27–34},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036846,
author = {Anguelov, Dragomir and Koller, Daphne and Pang, Hoi-Cheung and Srinivasan, Praveen and Thrun, Sebastian},
title = {Recovering Articulated Object Models from 3D Range Data},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We address the problem of unsupervised learning of complex articulated object models from 3D range data. We describe an algorithm whose input is a set of meshes corresponding to different configurations of an articulated object. The algorithm automatically recovers a decomposition of the object into approximately rigid parts, the location of the parts in the different object instances, and the articulated object skeleton linking the parts. Our algorithm first registers all the meshes using an unsupervised non-rigid technique described in a companion paper. It then segments the meshes using a graphical model that captures the spatial contiguity of parts. The segmentation is done using the EM algorithm, iterating between finding a decomposition of the object into rigid parts, and finding the location of the parts in the object instances. Although the graphical model is densely connected, the object decomposition step can be performed optimally and efficiently, allowing us to identify a large number of object parts while avoiding local maxima. We demonstrate the algorithm on real world datasets, recovering a 15-part articulated model of a human puppet from just 7 different puppet configurations, as well as a 4 part model of a flexing arm where significant non-rigid deformation was present.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {18–26},
numpages = {9},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036845,
author = {Amgoud, Leila and Prade, Henri},
title = {Using Arguments for Making Decisions: A Possibilistic Logic Approach},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Humans currently use arguments for explaining choices which are already made, or for evaluating potential choices. Each potential choice has usually pros and cons of various strengths. In spite of the usefulness of arguments in a decision making process, there have been few formal proposals handling this idea if we except works by Fox and Parsons and by Bonet and Geffner. In this paper we propose a possibilistic logic framework where arguments are built from an uncertain knowledge base and a set of prioritized goals. The proposed approach can compute two kinds of decisions by distinguishing between pessimistic and optimistic attitudes. When the available, maybe uncertain, knowledge is consistent, as well as the set of prioritized goals (which have to be fulfilled as far as possible), the method for evaluating decisions on the basis of arguments agrees with the possibility theory-based approach to decision-making under uncertainty. Taking advantage of its relation with formal approaches to defeasible argumentation, the proposed framework can be generalized in case of partially inconsistent knowledge, or goal bases.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {10–17},
numpages = {8},
keywords = {argumentation, possibilistic logic, decision},
location = {Banff, Canada},
series = {UAI '04}
}

@inproceedings{10.5555/1036843.1036844,
author = {Altun, Yasemin and Smola, Alex J. and Hofmann, Thomas},
title = {Exponential Families for Conditional Random Fields},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {In this paper we define conditional random fields in reproducing kernel Hilbert spaces and show connections to Gaussian Process classification. More specifically, we prove decomposition results for undirected graphical models and we give constructions for kernels. Finally we present efficient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited efficiently in the optimization process.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {2–9},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@proceedings{10.5555/1036843,
title = {UAI '04: Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This year marks the 20th anniversary of the Conference of Uncertainty in Artificial Intelligence (UAI). From its beginnings as a small workshop, UAI has grown to become the leading conference in the field. It is now the primary international forum for presenting new results on the use of principled methods for reasoning under uncertainty within intelligent systems. The scope of UAI is wide, including, but not limited to, representation, automated reasoning, learning, decision making, and knowledge acquisition under uncertainty. This year's conference (UAI 2004) continues the tradition, including contributions that report on advances in these core areas, as well as insights derived from the construction and use of applications involving uncertain reasoning. This volume comprises the papers accepted for presentation at UAI 2004, held at the Banff Park Inn in Banff, Canada, from July 7 through 11, 2004. Papers appearing in this volume were subjected to rigorous review; three Program Committee members (or in some cases, auxiliary reviewers) reviewed each paper under the supervision of an Area Chair, who made recommendations to the Program Chairs. The assignment of Program Committee members to papers was based on their expertise and expressed interests in the papers, with an eye toward coverage of the relevant aspects of each paper. This year a record 253 papers were submitted to UAI, and 76 papers were accepted for plenary or poster presentation at the conference. All accepted papers appear in this volume. We are confident that the proceedings, like past UAI Conference Proceedings, will become an important archival reference for the field.Based on the recommendation of the program committee, we selected one paper for the recipient of the Best Paper Award and one as the recipient of the Best Student Paper Award. These awards were given for outstanding technical contributions. We are pleased to present the UAI 2004 Best Paper Award to David McAllester, Michael Collins, and Fernando Pereira for their paper The Case-Factor Complexity of Markov Random Fields and the 2004 Best Student Paper Award to Mathias Drton and Thomas Richardson for their paper Iterative Conditional Fitting for Gaussian Ancestral Graph Models. The runners-up for the Best Student Paper Award were Gal Elidan, Iftach Nachman, and Nir Friedman for their paper "Ideal Parent" Structure Learning for Continuous Variable Networks.In addition to the presentation of technical papers, we were very pleased to have five distinguished invited speakers: Ed George (University of Pennsylvania), Jon Kleinberg (Cornell University), Lillian Lee (Cornell University), Alon Orlitsky (University of California at San Diego), and Moshe Y. Vardi (Rice University).UAI 2004 also continued the tradition of offering a full-day course on Advanced Topics in Uncertainty in Artificial Intelligence consisting of tutorials by Ronen Brafman (Ben-Gurion University), Rina Dechter (University of California at Irvine), Nir Friedman (Hebrew University), and Martin Wainwright (University of California at Berkeley).The set of papers, invited talks, and full-day course topics illustrate both the depth and breadth of UAI techniques and applications. We are proud of the quality of this year's conference, and are looking forward to continued contributions and growth in the future.},
location = {Banff, Canada}
}

