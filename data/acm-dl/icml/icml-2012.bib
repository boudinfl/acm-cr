@inproceedings{10.5555/3042573.3042816,
author = {Ross, St\'{e}phane and Bagnell, J. Andrew},
title = {Agnostic System Identification for Model-Based Reinforcement Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1905–1912},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042815,
author = {Sarkar, Purnamrita and Chakrabarti, Deepayan and Jordan, Michael I.},
title = {Nonparametric Link Prediction in Dynamic Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a nonparametric link prediction algorithm for a sequence of graph snapshots over time. The model predicts links based on the features of its endpoints, as well as those of the local neighborhood around the endpoints. This allows for different types of neighborhoods in a graph, each with its own dynamics (e.g, growing or shrinking communities). We prove the consistency of our estimator, and give a fast implementation based on locality-sensitive hashing. Experiments with simulated as well as five real-world dynamic graphs show that we outperform the state of the art, especially when sharp fluctuations or nonlinearities are present.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1897–1904},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042814,
author = {Scherrer, Bruno and Ghavamzadeh, Mohammad and Gabillon, Victor and Geist, Matthieu},
title = {Approximate Modified Policy Iteration},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unifies those for approximate policy and value iteration. For the classification-based implementation, we develop a finite-sample analysis that shows that MPI's main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1889–1896},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042813,
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
title = {Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1881–1888},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042812,
author = {Chen, Shang-Tse and Lin, Hsuan-Tien and Lu, Chi-Jen},
title = {An Online Boosting Algorithm with Theoretical Justifications},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the task of online boosting -- combining online weak learners into an online strong learner. While batch boosting has a sound theoretical foundation, online boosting deserves more study from the theoretical perspective. In this paper, we carefully compare the differences between online and batch boosting, and propose a novel and reasonable assumption for the online weak learner. Based on the assumption, we design an online boosting algorithm with a strong theoretical guarantee by adapting from the offline SmoothBoost algorithm that matches the assumption closely. We further tackle the task of deciding the number of weak learners using established theoretical results for online convex programming and predicting with expert advice. Experiments on real-world data sets demonstrate that the proposed algorithm compares favorably with existing online boosting algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1873–1880},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042811,
author = {Hazan, Elad and Koren, Tomer},
title = {Linear Regression with Limited Observation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010).Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1865–1872},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042810,
author = {Farabet, Cl\'{e}ment and Couprie, Camille and Najman, Laurent and LeCun, Yann},
title = {Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. In parallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The system yields record accuracies on the the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on the Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320 \texttimes{} 240 image labeling in less than 1 second, including feature extraction.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1857–1864},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042809,
author = {Wagstaff, Kiri L.},
title = {Machine Learning That Matters},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1851–1856},
numpages = {6},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042808,
author = {Hazan, Elad and Kale, Satyen},
title = {Projection-Free Online Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The computational bottleneck in applying online learning to massive data sets is usually the projection step. We present efficient online learning algorithms that eschew projections in favor of much more efficient linear optimization steps using the Frank-Wolfe technique. We obtain a range of regret bounds for online convex optimization, with better bounds for specific cases such as stochastic online smooth convex optimization.Besides the computational advantage, other desirable features of our algorithms are that they are parameter-free in the stochastic case and produce sparse decisions. We apply our algorithms to computationally intensive applications of collaborative filtering, and show the theoretical improvements to be clearly visible on standard datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1843–1850},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042807,
author = {Mairal, Julien and Yu, Bin},
title = {Complexity Analysis of the Lasso Regularization Path},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The regularization path of the Lasso can be shown to be piecewise linear, making it possible to "follow" and explicitly compute the entire path. We analyze in this paper this popular strategy, and prove that its worst case complexity is exponential in the number of variables. We then oppose this pessimistic result to an (optimistic) approximate analysis: We show that an approximate path with at most O(1/√ε) linear segments can always be obtained, where every point on the path is guaranteed to be optimal up to a relative ε-duality gap. We complete our theoretical analysis with a practical algorithm to compute these approximate paths.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1835–1842},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042806,
author = {Feng, Jiashi and Xu, Huan and Yan, Shuicheng},
title = {Robust PCA in High-Dimension: A Deterministic Approach},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness - a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1827–1834},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042805,
author = {Balle, Borja and Quattoni, Ariadna and Carreras, Xavier},
title = {Local Loss Optimization in Operator Models: A New Insight into Spectral Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper re-visits the spectral method for learning latent variable models defined in terms of observable operators. We give a new perspective on the method, showing that operators can be recovered by minimizing a loss defined on a finite subset of the domain. This leads to a derivation of a non-convex optimization similar to the spectral method. We also propose a regularized convex relaxation of this optimization. In practice our experiments show that a continuous regularization parameter (in contrast with the discrete number of states in the original method) allows a better trade-off between accuracy and model complexity. We also prove that in general, a randomized strategy for choosing the local loss succeeds with high probability.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1819–1826},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042804,
author = {Rifai, Salah and Bengio, Yoshua and Dauphin, Yann N. and Vincent, Pascal},
title = {A Generative Process for Sampling Contractive Auto-Encoders},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1811–1818},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042803,
author = {Gr\"{u}new\"{a}lder, Steffen and Lever, Guy and Baldassarre, Luca and Patterson, Sam and Gretton, Arthur and Pontil, Massimilano},
title = {Conditional Mean Embeddings as Regressors},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We demonstrate an equivalence between reproducing kernel Hilbert space (RKHS) embeddings of conditional distributions and vector-valued regressors. This connection introduces a natural regularized loss function which the RKHS embeddings minimise, providing an intuitive understanding of the embeddings and a justification for their use. Furthermore, the equivalence allows the application of vector-valued regression methods and results to the problem of learning conditional distributions. Using this link we derive a sparse version of the embedding by considering alternative formulations. Further, by applying convergence results for vector-valued regression to the embedding problem we derive minimax convergence rates which are O(log(n)/n) - compared to current state of the art rates of O(n-1/4) - and are valid under milder and more intuitive assumptions. These minimax upper rates coincide with lower rates up to a logarithmic factor, showing that the embedding method achieves nearly optimal rates. We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over an incomplete Cholesky decomposition.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1803–1810},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042802,
author = {Ieong, Samuel and Mishra, Nina and Sheffet, Or},
title = {Predicting Preference Flips in Commerce Search},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Traditional approaches to ranking in web search follow the paradigm of rank-by-score: a learned function gives each query-URL combination an absolute score and URLs are ranked according to this score. This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other. Scoring contradicts prior work in behavioral economics that preference between items depends not only on the items but also on the presented alternatives. Thus, for the same query, preference between items A and B may depend on the presence or absence of item C. We propose a new model of ranking, the Random Shopper Model, that allows and explains such behavior. In this model, each feature is viewed as a Markov chain over the items to be ranked, and the goal is to find a weighting of the features that best reects their importance. We show that our model can be learned under the empirical risk minimization framework, and give an efficient learning algorithm. Experiments on commerce search logs demonstrate that our algorithm outperforms scoring-based approaches including regression and listwise ranking.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1795–1802},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042801,
author = {Kleiner, Ariel and Talwalkar, Ameet and Sarkar, Purnamrita and Jordan, Michael I.},
title = {The Big Data Bootstrap},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB's behavior, including a study of its statistical correctness, its large-scale implementation and performance, selection of hyperparameters, and performance on real data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1787–1794},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042800,
author = {Bart\'{o}k, G\'{a}bor and Zolghadr, Navid and Szepesv\'{a}ri, Csaba},
title = {An Adaptive Algorithm for Finite Stochastic Partial Monitoring},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both "easy" and "hard" problems. For easy problems, it additionally achieves logarithmic individual regret. Most importantly, the algorithm is adaptive in the sense that if the opponent strategy is in an "easy region" of the strategy space then the regret grows as if the problem was easy. As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O(√T) regret in Dynamic Pricing, proven to be hard by Bart\'{o}k et al. (2011).},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1779–1786},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042799,
author = {Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
title = {Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we address the following question: "Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?". An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1771–1778},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042798,
author = {Ko, Young Jun and Seeger, Matthias},
title = {Large Scale Variational Bayesian Inference for Structured Scale Mixture Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in nonfactorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial "sparse" methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with nonfactorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1763–1770},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042797,
author = {Pires, Bernardo \'{A}vila and Szepesv\'{a}ri, Csaba},
title = {Statistical Linear Estimation with Penalized Estimators: An Application to Reinforcement Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid data-splitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1755–1762},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042796,
author = {Arora, Raman and Dekel, Ofer and Tewari, Ambuj},
title = {Online Bandit Learning against an Adaptive Adversary: From Regret to Policy Regret},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm's ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm's actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm's performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary's memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1747–1754},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042795,
author = {Zhai, Yiteng and Tan, Mingkui and Tsang, Ivor W. and Ong, Yew-Soon},
title = {Discovering Support and Affiliated Features from Very High Dimensions},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions. Specifically, we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy. The benefits of the proposed algorithm are two-folds. First, it can identify the optimal discriminative and uncorrelated feature subset to the output labels, denoted here as Support Features, which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper. Second, during the learning process, the underlying group structures of correlated features associated with each support feature, denoted as Affiliated Features, can also be discovered without any additional cost. These affiliated features serve to improve the interpretations on the learning tasks. Extensive empirical studies on both synthetic and very high dimensional real-world datasets verify the validity and efficiency of the proposed method.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1739–1746},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042794,
author = {Liu, Han and Han, Fang and Yuan, Ming and Lafferty, John and Wasserman, Larry},
title = {The Nonparanormal Skeptic},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a semiparametric method we call the nonparanormal SKEPTIC for estimating high dimensional undirected graphical models. The underlying model is the nonparanormal family proposed by Liu et al. (2009). The method exploits nonparametric rank-based correlation coefficient estimators, including Spearman's rho and Kendall's tau. In high dimensional settings, we prove that the nonparanormal SKEPTIC achieves the optimal parametric rate of convergence for both graph and parameter estimation. This result suggests that the nonparanormal graphical model can be a safe replacement for the Gaussian graphical model, even when the data are Gaussian.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1731–1738},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042793,
author = {Kumar, Abhishek and Daum\'{e}, Hal},
title = {Learning Task Grouping and Overlap in Multi-Task Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. We propose a framework for multi-task learning that enables one to selectively share the information across the tasks. We assume that each task parameter vector is a linear combination of a finite number of underlying basis tasks. The coefficients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. Our model is based on the assumption that task parameters within a group lie in a low dimensional subspace but allows the tasks in different groups to overlap with each other in one or more bases. Experimental results on four datasets show that our approach outperforms competing methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1723–1730},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042792,
author = {Chaudhuri, Kamalika and Hsu, Daniel},
title = {Convergence Rates for Differentially Private Statistical Estimation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Differential privacy is a cryptographically-motivated definition of privacy which has gained significant attention over the past few years. Differentially private solutions enforce privacy by adding random noise to a function computed over the data, and the challenge in designing such algorithms is to control the added noise in order to optimize the privacy-accuracy-sample size tradeoff.This work studies differentially-private statistical estimation, and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators. Our results reveal a formal connection between differential privacy and the notion of Gross Error Sensitivity (GES) in robust statistics, by showing that the convergence rate of any differentially private approximation to an estimator that is accurate over a large class of distributions has to grow with the GES of the estimator. We then provide an upper bound on the convergence rate of a differentially private approximation to an estimator with bounded range and bounded GES. We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1715–1722},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042791,
author = {Azar, Mohammad Gheshlaghi and Munos, R\'{e}mi and Kappen, Hilbert J.},
title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor γ ∈ [0, 1) only O(N log(N/δ)/(1 - γ)3ε2)) samples are required to find an ε-optimal estimation of the action-value function with the probability 1 - δ. We also prove a matching lower bound of Θ(N log(N/δ)/((1 - γ)3ε2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, ε, δ and 1/(1-γ). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1 - γ).},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1707–1714},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042790,
author = {Wang, Huahua and Banerjee, Arindam},
title = {Online Alternating Direction Method},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Online optimization has emerged as powerful tool in large scale optimization. In this paper, we introduce efficient online algorithms based on the alternating directions method (ADM). We introduce a new proof technique for ADM in the batch setting, which yields the O(1/T) convergence rate of ADM and forms the basis of regret analysis in the online setting. We consider two scenarios in the online setting, based on whether the solution needs to lie in the feasible set or not. In both settings, we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions. Preliminary results are presented to illustrate the performance of the proposed algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1699–1706},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042789,
author = {Bracegirdle, Chris and Barber, David},
title = {Bayesian Conditional Cointegration},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Cointegration is an important topic for time-series, and describes a relationship between two series in which a linear combination is stationary. Classically, the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares. Subsequently a unit root test is performed on the residuals. A well-known deficiency of this classical approach is that it can lead to erroneous conclusions about the presence of cointegration. As an alternative, we present a framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach. Finally, we apply our technique to model segmented cointegration in which cointegration may exist only for limited time. In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1691–1698},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042788,
author = {Salazar, Esther and Cain, Matthew S. and Darling, Elise F. and Mitroff, Stephen F. and Carin, Lawrence},
title = {Inferring Latent Structure from Mixed Real and Categorical Relational Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider analysis of relational data (a matrix), in which the rows correspond to subjects (e.g., people) and the columns correspond to attributes. The elements of the matrix may be a mix of real and categorical. Each subject and attribute is characterized by a latent binary feature vector, and an inferred matrix maps each row-column pair of binary feature vectors to an observed matrix element. The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low-rank covariance matrix, and the Gaussian random variables are mapped to latent binary features via a probit link. The same type construction is applied jointly to the columns. The model infers latent, low-dimensional binary features associated with each row and each column, as well correlation structure between all rows and between all columns.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1683–1690},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042787,
author = {Xu, Zenglin and Yan, Feng and Qi, Yuan},
title = {Infinite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Tensor decomposition is a powerful computational tool for multiway data analysis. Many popular tensor decomposition approaches--such as the Tucker decomposition and CANDECOMP/ PARAFAC (CP)--amount to multilinear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose tensor-variate latent nonparametric Bayesian models, coupled with efficient inference methods, for multiway data analysis. We name these models InfTucker. Using these InfTucker models, we conduct Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, our new approaches handle both continuous and binary data in a probabilistic framework. Unlike previous Bayesian models on matrices and tensors, our models are based on latent Gaussian or t processes with nonlinear covariance functions. To efficiently learn the InfTucker models from data, we develop a variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. Our experimental results on chemometrics and social network datasets demonstrate that our new models achieved significantly higher prediction accuracy than the most state-of-art tensor decomposition approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1675–1682},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042786,
author = {Hazan, Tamir and Jaakkola, Tommi},
title = {On the Partition Function and Random Maximum A-Posteriori Perturbations},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we relate the partition function to the max-statistics of random variables. In particular, we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models. As a result, we can use efficient MAP solvers such as graph-cuts to evaluate the corresponding partition function. We show that our method excels in the typical "high signal - high coupling" regime that results in ragged energy landscapes difficult for alternative approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1667–1674},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042785,
author = {Schwing, Alexander G. and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},
title = {Efficient Structured Prediction with Latent Variables for General Graphical Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1659–1666},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042784,
author = {Tamar, Aviv and Di Castro, Dotan and Mannor, Shie},
title = {Policy Gradients with Variance Related Risk Criteria},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control. The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward. It is known that optimizing many of the variance related risk criteria is NP-hard. In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria. Our starting point is a new formula for the variance of the cost-togo in episodic tasks. Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost. We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1651–1658},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042783,
author = {Yin, Junming and Chen, Xi and Xing, Eric P.},
title = {Group Sparse Additive Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the non-parametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the l1/l2 norm to Hilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1643–1650},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042782,
author = {P\'{o}czos, Barnab\'{a}s and Ghahramani, Zoubin and Schneider, Jeff},
title = {Copula-Based Kernel Dependency Measures},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1635–1642},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042781,
author = {Chen, Minmin and Xu, Zhixiang and Weinberger, Kilian Q. and Sha, Fei},
title = {Marginalized Denoising Autoencoders for Domain Adaptation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Stacked denoising autoencoders (SDAs) have been successfully used to learn new representations for domain adaptation. Recently, they have attained record accuracy on standard benchmark tasks of sentiment analysis across different text domains. SDAs learn robust data representations by reconstruction, recovering original features from data that are artificially corrupted with noise. In this paper, we propose marginalized SDA (mSDA) that addresses two crucial limitations of SDAs: high computational cost and lack of scalability to high-dimensional features. In contrast to SDAs, our approach of mSDA marginalizes noise and thus does not require stochastic gradient descent or other optimization algorithms to learn parameters--in fact, they are computed in closed-form. Consequently, mSDA, which can be implemented in only 20 lines of MATLAB™, significantly speeds up SDAs by two orders of magnitude. Furthermore, the representations learnt by mSDA are as effective as the traditional SDAs, attaining almost identical accuracies in benchmark tasks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1627–1634},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042780,
author = {Boyd, Kendrick and Costa, V\'{\i}tor Santos and Davis, Jesse and Page, C. David},
title = {Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1619–1626},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042779,
author = {McCartin-Lim, Mark and McGregor, Andrew and Wang, Rui},
title = {Approximate Principal Direction Trees},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of lower-accuracy RP trees.APD trees use a small number of power-method iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the power-method and b) that O(log d) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1611–1618},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042778,
author = {Gr\"{u}new\"{a}lder, Steffen and Lever, Guy and Baldassarre, Luca and Pontil, Massimilano and Gretton, Arthur},
title = {Modelling Transition Dynamics in MDPs with RKHS Embeddings},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new, nonparametric approach to learning and representing transition dynamics in Markov decision processes (MDPs), which can be combined easily with dynamic programming methods for policy optimisation and value estimation. This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space (RKHS). Such representations bypass the need for estimating transition probabilities or densities, and apply to any domain on which kernels can be defined. This avoids the need to calculate intractable integrals, since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding. We provide guarantees for the proposed applications in MDPs: in the context of a value iteration algorithm, we prove convergence to either the optimal policy, or to the closest projection of the optimal policy in our model class (an RKHS), under reasonable assumptions. In experiments, we investigate a learning task in a typical classical control setting (the under-actuated pendulum), and on a navigation problem where only images from a sensor are observed. For policy optimisation we compare with least-squares policy iteration where a Gaussian process is used for value function estimation. For value estimation we also compare to the NPDP method. Our approach achieves better performance in all experiments.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1603–1610},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042777,
author = {Petrik, Marek},
title = {Approximate Dynamic Programming by Minimizing Distributionally Robust Bounds},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Approximate dynamic programming is a popular method for solving large Markov decision processes. This paper describes a new class of approximate dynamic programming (ADP) methods--distributionally robust ADP--that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss. This approach turns ADP into an optimization problem, for which we derive new mathematical program formulations and analyze its properties. DRADP improves on the theoretical guarantees of existing ADP methods--|it guarantees convergence and L1 norm-based error bounds. The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1595–1602},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042776,
author = {Gomez-Rodriguez, Manuel and Sch\"{o}lkopf, Bernhard},
title = {Submodular Inference of Diffusion Networks from Multiple Trees},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Diffusion and propagation of information, influence and diseases take place over increasingly larger networks. We observe when a node copies information, makes a decision or becomes infected but networks are often hidden or unobserved. Since networks are highly dynamic, changing and growing rapidly, we only observe a relatively small set of cascades before a network changes significantly. Scalable network inference based on a small cascade set is then necessary for understanding the rapidly evolving dynamics that govern diffusion. In this article, we develop a scalable approximation algorithm with provable near-optimal performance based on submodular maximization which achieves a high accuracy in such scenario, solving an open problem first introduced by Gomez-Rodriguez et al. (2010). Experiments on synthetic and real diffusion data show that our algorithm in practice achieves an optimal trade-off between accuracy and running time.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1587–1594},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042775,
author = {Jalali, Ali and Srebro, Nathan},
title = {Clustering Using Max-Norm Constrained Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We suggest using the max-norm as a convex surrogate constraint for clustering. We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear-norm relaxation, and study the effectiveness of our method, and other related convex relaxations, compared to other approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1579–1586},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042774,
author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
title = {Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Stochastic gradient descent (SGD) is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be O(log(T)/T), by running SGD for T iterations and returning the average point. However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that standard SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the optimality of SGD in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate with averaging might really be Ω(log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) rate, and no other change of the algorithm is necessary. We also present experimental results which support our findings, and point out open problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1571–1578},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042773,
author = {Su, Hao and Yu, Adams Wei and Fei-Fei, Li},
title = {Efficient Euclidean Projections onto the Intersection of Norm Balls},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Using sparse-inducing norms to learn robust models has received increasing attention from many fields for its attractive properties. Projection-based methods have been widely applied to learning tasks constrained by such norms. As a key building block of these methods, an efficient operator for Euclidean projection onto the intersection of l1 and l1,q norm balls (q = 2 or ∞) is proposed in this paper. We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic. Hence, a bisection algorithm is sufficient to solve the problem. We show that the time complexity of our solution is O(n + g log g) for q = 2 and O(n log n) for q = ∞, where n is the dimensionality of the vector to be projected and g is the number of disjoint groups; we confirm this complexity by experimentation. Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage. We further show that embedded with our efficient projection operator, projection-based algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1563–1570},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042772,
author = {Ye, Nan and Chai, Kian Ming A. and Lee, Wee Sun and Chieu, Hai Leong},
title = {Optimizing F-Measures: A Tale of Two Approaches},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1555–1562},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042771,
author = {Stulp, Freek and Sigaud, Olivier},
title = {Path Integral Policy Improvement with Covariance Matrix Adaptation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for "Path Integral Policy Improvement with Covariance Matrix Adaptation". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1547–1554},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042770,
author = {Samdani, Rajhans and Roth, Dan},
title = {Efficient Decomposed Learning for Structured Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Structured prediction is the cornerstone of several machine learning applications. Unfortunately, in structured prediction settings with expressive inter-variable interactions, exact inference-based learning algorithms, e.g. Structural SVM, are often intractable. We present a new way, Decomposed Learning (DecL), which performs efficient learning by restricting the inference step to a limited part of the structured spaces. We provide characterizations based on the structure, target parameters, and gold labels, under which DecL is equivalent to exact learning. We then show that in real world settings, where our theoretical assumptions may not completely hold, DecL-based algorithms are significantly more efficient and as accurate as exact learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1539–1546},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042769,
author = {Jawanpuria, Pratik and Nath, J. Saketha},
title = {A Convex Feature Learning Formulation for Latent Task Structure Discovery},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper considers the multi-task learning problem and in the setting where some relevant features could be shared across few related tasks. Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori. In real-world applications however, it is desirable to automatically discover the groups of related tasks that share a feature space. In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space. The main contribution is a convex formulation that employs a graph-based regularizer and simultaneously discovers few groups of related tasks, having close-by task parameters, as well as the feature space shared within each group. The regularizer encodes an important structure among the groups of tasks leading to an efficient algorithm for solving it: if there is no feature space under which a group of tasks has close-by task parameters, then there does not exist such a feature space for any of its supersets. An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented. The algorithm is guaranteed to solve the proposed formulation (within some precision) in a time polynomial in the number of groups of related tasks discovered. Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state-of-the-art multi-task learning algorithms in some cases.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1531–1538},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042768,
author = {Ouyang, Hua and Gray, Alexander},
title = {Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1523–1530},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042767,
author = {Mimno, David and Hoffman, Matthew D. and Blei, David M.},
title = {Sparse Stochastic Inference for Latent Dirichlet Allocation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1515–1522},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042766,
author = {Chen, Minhua and Carson, William and Rodrigues, Miguel and Calderbank, Robert and Carin, Lawrence},
title = {Communications Inspired Linear Discriminant Analysis},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the problem of supervised linear dimensionality reduction, taking an information-theoretic viewpoint. The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label. By harnessing a recent theoretical result on the gradient of mutual information, the above optimization problem can be solved directly using gradient descent, without requiring simplification of the objective function. Theoretical analysis and empirical comparison are made between the proposed method and two closely related methods, and comparisons are also made with a method in which R\'{e}nyi entropy is used to define the mutual information (in this case the gradient may be computed simply, under a special parameter setting). Relative to these alternative approaches, the proposed method achieves promising results on real datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1507–1514},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042765,
author = {Mysore, Gautham J. and Sahani, Maneesh},
title = {Variational Inference in Non-Negative Factorial Hidden Markov Models for Efficient Audio Source Separation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The past decade has seen substantial work on the use of non-negative matrix factorization and its probabilistic counterparts for audio source separation. Although able to capture audio spectral structure well, these models neglect the non-stationarity and temporal dynamics that are important properties of audio. The recently proposed non-negative factorial hidden Markov model (N-FHMM) introduces a temporal dimension and improves source separation performance. However, the factorial nature of this model makes the complexity of inference exponential in the number of sound sources. Here, we present a Bayesian variant of the N-FHMM suited to an efficient variational inference algorithm, whose complexity is linear in the number of sound sources. Our algorithm performs comparably to exact inference in the original N-FHMM but is significantly faster. In typical configurations of the N-FHMM, our method achieves around a 30x increase in speed.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1499–1506},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042764,
author = {Bellet, Aur\'{e}lien and Habrard, Amaury and Sebban, Marc},
title = {Similarity Learning for Provably Accurate Sparse Linear Classification},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of (ε, γ, τ)-good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1491–1498},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042763,
author = {McAfee, Lawrence and Olukotun, Kunle},
title = {Utilizing Static Analysis and Code Generation to Accelerate Neural Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {As datasets continue to grow, neural network (NN) applications are becoming increasingly limited by both the amount of available computational power and the ease of developing high-performance applications. Researchers often must have expert systems knowledge to make their algorithms run efficiently. Although available computing power increases rapidly each year, algorithm efficiency is not able to keep pace due to the use of general purpose compilers, which are not able to fully optimize specialized application domains. Within the domain of NNs, we have the added knowledge that network architecture remains constant during training, meaning the architecture's data structure can be statically optimized by a compiler. In this paper, we present SONNC, a compiler for NNs that utilizes static analysis to generate optimized parallel code. We show that SONNC's use of static optimizations make it able to outperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X. Additionally, we show that use of SONNC significantly reduces code complexity when using structurally sparse networks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1483–1490},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042762,
author = {Yu, Yaoliang and Neufeld, James and Kiros, Ryan and Zhang, Xinhua and Schuurmans, Dale},
title = {Regularizers versus Losses for Nonlinear Dimensionality Reduction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We demonstrate that almost all nonparametric dimensionality reduction methods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1475–1482},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042761,
author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
title = {Poisoning Attacks against Support Vector Machines},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1467–1474},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042760,
author = {Bachman, Philip and Precup, Doina},
title = {Improved Estimation in Time Varying Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing this variance, based on the idea of estimating simultaneously a transformed space for the model and locally adapted parameterizations expressed in the new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-specific bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1459–1466},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042759,
author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
title = {Safe Exploration in Markov Decision Processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1451–1458},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042758,
author = {Da Silva, Bruno Castro and Konidaris, George and Barto, Andrew G.},
title = {Learning Parameterized Skills},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1443–1450},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042757,
author = {Matuszek, Cynthia and FitzGerald, Nicholas and Zettlemoyer, Luke and Bo, Liefeng and Fox, Dieter},
title = {A Joint Model of Language and Perception for Grounded Attribute Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {As robots become more ubiquitous and capable, it becomes ever more important for untrained users to easily interact with them. Recently, this has led to study of the language grounding problem, where the goal is to extract representations of the meanings of natural language tied to the physical world. We present an approach for joint learning of language and perception models for grounded attribute induction. The perception model includes classifiers for physical characteristics and a language model based on a probabilistic categorial grammar that enables the construction of compositional meaning representations. We evaluate on the task of interpreting sentences that describe sets of objects in a physical workspace, and demonstrate accurate task performance and effective latent-variable concept induction in physical grounded scenes.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1435–1442},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042756,
author = {Naim, Iftekhar and Gildea, Daniel},
title = {Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic antiannealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1427–1431},
numpages = {5},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042755,
author = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
title = {Deep Lambertian Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1419–1426},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042754,
author = {Kim, Dae Il and Hughes, Michael C. and Sudderth, Erik B.},
title = {The Nonparametric Metadata Dependent Relational Model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We introduce the nonparametric metadata dependent relational (NMDR) model, a Bayesian nonparametric stochastic block model for network data. The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities. Learned regression models allow these memberships to depend on, and be predicted from, arbitrary node metadata. We develop efficient MCMC algorithms for learning NMDR models from partially observed node relationships. Retrospective MCMC methods allow our sampler to work directly with the infinite stick-breaking representation of the NMDR, avoiding the need for finite truncations. Our results demonstrate recovery of useful latent communities from real-world social and ecological networks, and the usefulness of metadata in link prediction tasks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1411–1418},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042753,
author = {Liu, Qiang and Ihler, Alexander},
title = {Distributed Parameter Estimation via Pseudo-Likelihood},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Estimating statistical models within sensor networks requires distributed algorithms, in which both data and computation are distributed across the nodes of the network. We propose a general approach for distributed learning based on combining local estimators defined by pseudo-likelihood components, encompassing a number of combination methods, and provide both theoretical and experimental analysis. We show that simple linear combination or max-voting methods, when combined with second-order information, are statistically competitive with more advanced and costly joint optimization. Our algorithms have many attractive properties including low communication and computational cost and "any-time" behavior.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1403–1410},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042752,
author = {Mau\'{a}, Denis Deratani and De Campos, Cassio Polpo},
title = {Anytime Marginal Maximum a Posteriori Inference},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper presents a new anytime algorithm for the marginal MAP problem in graphical models of bounded treewidth. We show asymptotic convergence and theoretical error bounds for any fixed step. Experiments show that it compares well to a state-of-the-art systematic search algorithm.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1395–1402},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042751,
author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
title = {Large-Scale Feature Learning with Spike-and-Slab Sparse Coding},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models' Transfer Learning Challenge.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1387–1394},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042750,
author = {Chen, Bo and Castro, Rui M. and Krause, Andreas},
title = {Joint Optimization and Variable Selection of High-Dimensional Gaussian Processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Maximizing high-dimensional, nonconvex functions through noisy observations is a notoriously hard problem, but one that arises in many applications. In this paper, we tackle this challenge by modeling the unknown function as a sample from a high-dimensional Gaussian process (GP) distribution. Assuming that the unknown function only depends on few relevant variables, we show that it is possible to perform joint variable selection and GP optimization. We provide strong performance guarantees for our algorithm, bounding the sample complexity of variable selection, and as well as providing cumulative regret bounds. We further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1379–1386},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042749,
author = {Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Thirion, Bertrand},
title = {Small-Sample Brain Mapping: Sparse Recovery on Spatially Correlated Designs with Randomization and Clustering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Functional neuroimaging can measure the brain's response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these dificulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1371–1378},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042748,
author = {Paisley, John and Blei, David M. and Jordan, Michael I.},
title = {Variational Bayesian Inference with Stochastic Search},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1363–1370},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042747,
author = {Bach, Francis and Lacoste-Julien, Simon and Obozinski, Guillaume},
title = {On the Equivalence between Herding and Conditional Gradient Algorithms},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We show that the herding procedure of Welling (2009b) takes exactly the form of a standard convex optimization algorithm-- namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. Our experiments shed more light on the learning bias of herding: they indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm approaches more often the maximum entropy distribution.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1355–1362},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042746,
author = {Dembczynski, Krzysztof and Kot\l{}owski, Wojciech and H\"{u}llermeier, Eyke},
title = {Consistent Multilabel Ranking through Univariate Loss Minimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of rank loss minimization in the setting of multilabel classification, which is usually tackled by means of convex surrogate losses defined on pairs of labels. Very recently, this approach was put into question by a negative result showing that commonly used pairwise surrogate losses, such as exponential and logistic losses, are inconsistent. In this paper, we show a positive result which is arguably surprising in light of the previous one: the simpler univariate variants of exponential and logistic surrogates (i.e., defined on single labels) are consistent for rank loss minimization. Instead of directly proving convergence, we give a much stronger result by deriving regret bounds and convergence rates. The proposed losses suggest efficient and scalable algorithms, which are tested experimentally.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1347–1354},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042745,
author = {Sohn, Kihyuk and Lee, Honglak},
title = {Learning Invariant Representations with Local Transformations},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Learning invariant representations is an important problem in machine learning and pattern recognition. In this paper, we present a novel framework of transformation-invariant feature learning by incorporating linear transformations into the feature learning algorithms. For example, we present the transformation-invariant restricted Boltzmann machine that compactly represents data by its weights and their transformations, which achieves invariance of the feature representation via probabilistic max pooling. In addition, we show that our transformation-invariant feature learning framework can also be extended to other unsupervised learning methods, such as autoencoders or sparse coding. We evaluate our method on several image classification benchmark datasets, such as MNIST variations, CIFAR-10, and STL-10, and show competitive or superior classification performance when compared to the state-of-the-art. Furthermore, our method achieves state-of-the-art performance on phone classification tasks with the TIMIT dataset, which demonstrates wide applicability of our proposed algorithms to other domains.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1339–1346},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042744,
author = {Kumar, Abhishek and Niculescu-Mizil, Alexandru and Kavukcoglu, Koray and Daum\'{e}, Hal},
title = {A Binary Classification Framework for Two-Stage Multiple Kernel Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of prespecified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1331–1338},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042743,
author = {Davis, Jesse and Costa, V\'{\i}tor Santos and Peissig, Peggy and Caldwell, Michael and Berg, Elizabeth},
title = {Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Learning from electronic medical records (EMR) is challenging due to their relational nature and the uncertain dependence between a patient's past and future health status. Statistical relational learning is a natural fit for analyzing EMRs but is less adept at handling their inherent latent structure, such as connections between related medications or diseases. One way to capture the latent structure is via a relational clustering of objects. We propose a novel approach that, instead of pre-clustering the objects, performs a demand-driven clustering during learning. We evaluate our algorithm on three real-world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication. We find that our approach is more accurate than performing no clustering, pre-clustering, and using expert-constructed medical heterarchies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1323–1330},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042742,
author = {Joulin, Armand and Bach, Francis},
title = {A Convex Relaxation for Weakly Supervised Classifiers},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning, as well as on clustering tasks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1315–1322},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042741,
author = {Bie\ss{}mann, Felix and Papaioannou, Jens-Michalis and Braun, Mikio and Harth, Andreas},
title = {Canonical Trends: Detecting Trend Setters in Web Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1307–1314},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042740,
author = {Xu, Zhixiang and Weinberger, Kilian Q. and Chapelle, Olivier},
title = {The Greedy Miser: Learning under Test-Time Budgets},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {As machine learning algorithms enter applications in industrial settings, there is increased interest in controlling their cpu-time during testing. The cpu-time consists of the running time of the algorithm and the extraction time of the features. The latter can vary drastically when the feature set is diverse. In this paper, we propose an algorithm, the Greedy Miser, that incorporates the feature extraction cost during training to explicitly minimize the cpu-time during testing. The algorithm is a straightforward extension of stagewise regression and is equally suitable for regression or multi-class classification. Compared to prior work, it is significantly more cost-effective and scales to larger data sets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1299–1306},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042739,
author = {Merchante, Luis Francisco S\'{a}nchez and Grandvalet, Yves and Govaert, G\'{e}rard},
title = {An Efficient Approach to Sparse Linear Discriminant Analysis},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal Scoring. It has an exact equivalence with penalized LDA, contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that selects the same features in all discriminant directions. Our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances. Besides prediction, the resulting sparse discriminant directions are also amenable to low-dimensional representations of data. Our algorithm is highly efficient for medium to large number of variables, and is thus particularly well suited to the analysis of gene expression data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1291–1298},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042738,
author = {Passos, Alexandre and Rai, Piyush and Wainer, Jacques and Daum\'{e}, Hal},
title = {Flexible Modeling of Latent Task Structures in Multitask Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the "right" latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, mean-regularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and real-world datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1283–1290},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042737,
author = {Shi, Yuan and Sha, Fei},
title = {Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the problem of unsupervised domain adaptation, which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain. Many existing approaches first learn domain-invariant features and then construct classifiers with them. We propose a novel approach that jointly learn the both. Specifically, while the method identifies a feature space where data in the source and the target domains are similarly distributed, it also learns the feature space discriminatively, optimizing an information-theoretic metric as an proxy to the expected misclassification error on the target domain. We show how this optimization can be effectively carried out with simple gradient-based methods and how hyperparameters can be cross-validated without demanding any labeled data from the target domain. Empirical studies on benchmark tasks of object recognition and sentiment analysis validated our modeling assumptions and demonstrated significant improvement of our method over competing ones in classification accuracies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1275–1282},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042736,
author = {Silver, David and Ciosek, Kamil},
title = {Compositional Planning Using Optimal Option Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macro-operators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intra- and inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1267–1274},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042735,
author = {Xiang, Qiaoliang and Mao, Qi and Chai, Kian Ming A. and Chieu, Hai Leong and Tsang, Ivor Wai-Hung and Zhao, Zhendong},
title = {A Split-Merge Framework for Comparing Clusterings},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Clustering evaluation measures are frequently used to evaluate the performance of algorithms. However, most measures are not properly normalized and ignore some information in the inherent structure of clusterings. We model the relation between two clusterings as a bipartite graph and propose a general component-based decomposition formula based on the components of the graph. Most existing measures are examples of this formula. In order to satisfy consistency in the component, we further propose a split-merge framework for comparing clusterings of different data sets. Our framework gives measures that are conditionally normalized, and it can make use of data point information, such as feature vectors and pairwise distances. We use an entropy-based instance of the framework and a coreference resolution data set to demonstrate empirically the utility of our framework over other measures.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1259–1266},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042734,
author = {Alamgir, Morteza and Von Luxburg, Ulrike},
title = {Shortest Path Distance in Random K-Nearest Neighbor Graphs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on Rd. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1251–1258},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042733,
author = {McDowell, Luke K. and Aha, David W.},
title = {Semi-Supervised Collective Classification via Hybrid Label Regularization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for collective classification (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of label regularization to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1243–1250},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042732,
author = {Karbasi, Amin and Ioannidis, Stratis and Massouli\'{e}, Laurent},
title = {Comparison-Based Learning with Rank Nets},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adaptive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the target in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum. We extend these results to the case of noisy oracles, and compare this strategy to prior art over multiple datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1235–1242},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042731,
author = {Xiao, Lin and Zhang, Tong},
title = {A Proximal-Gradient Homotopy Method for the l<sub>1</sub>-Regularized Least-Squares Problem},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the l1-regularized least-squares problem for sparse recovery and compressed sensing. Since the objective function is not strongly convex, standard proximal gradient methods only achieve sublinear convergence. We propose a homotopy continuation strategy, which employs a proximal gradient method to solve the problem with a sequence of decreasing regularization parameters. It is shown that under common assumptions in compressed sensing, the proposed method ensures that all iterates along the homotopy solution path are sparse, and the objective function is effectively strongly convex along the solution path. This observation allows us to obtain a global geometric convergence rate for the procedure. Empirical results are presented to support our theoretical analysis.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1227–1234},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042730,
author = {Du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
title = {Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In real-world classification problems, the class balance in the training dataset does not necessarily reflect that of the test dataset, which can cause significant estimation bias. If the class ratio of the test dataset is known, instance reweighting or resampling allows systematical bias correction. However, learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain. In this paper, we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data. We demonstrate the utility of the proposed approach through experiments.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1219–1226},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042729,
author = {Morvant, Emilie and Ko\c{c}o, Sokol and Ralaivola, Liva},
title = {PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class Classification},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we propose a PAC-Bayes bound for the generalization risk of the Gibbs classifier in the multi-class classification framework. The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate. Thanks to very recent and beautiful results on matrix concentration inequalities, we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper-bounded by its empirical risk plus a term depending on the number of training examples in each class. To the best of our knowledge, this is the first PAC-Bayes bounds based on confusion matrices.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1211–1218},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042728,
author = {Fujimaki, Ryohei and Hayashi, Kohei},
title = {Factorized Asymptotic Bayesian Hidden Markov Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1203–1210},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042727,
author = {Liu, Yan and Bahadori, Mohammad Taha and Li, Hongfei},
title = {Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Series Modeling},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In many applications of time series models, such as climate analysis and social media analysis, we are often interested in extreme events, such as heatwave, wind gust, and burst of topics. These time series data usually exhibit a heavy-tailed distribution rather than a Gaussian distribution. This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events. In this paper, we propose the Sparse-GEV model, a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions. Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies among multivariate extreme value time series. We demonstrate the superior performance of our algorithm to the state-of-art methods, including Granger causality, copula approach, and transfer entropy, on one synthetic dataset, one climate dataset and two Twitter datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1195–1202},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042726,
author = {Bonilla, Edwin V. and Robles-Kelly, Antonio},
title = {Discriminative Probabilistic Prototype Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where an input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people's work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmarks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1187–1194},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042725,
author = {Zhu, Jun},
title = {Max-Margin Nonparametric Latent Feature Models for Link Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a max-margin nonparametric latent feature relational model, which unites the ideas of max-margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension. By minimizing a hinge-loss using the linear expectation operator, we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function; by using a fully-Bayesian formulation, we can avoid tuning regularization constants. Experimental results on real datasets appear to demonstrate the benefits inherited from max-margin learning and fully-Bayesian nonparametric inference.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1179–1186},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042724,
author = {Shterev, Ivo D. and Dunson, David B.},
title = {Bayesian Watermark Attacks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper presents an application of statistical machine learning to the field of watermarking. We propose a new attack model on additive spread-spectrum watermarking systems. The proposed attack is based on Bayesian statistics. We consider the scenario in which a watermark signal is repeatedly embedded in specific, possibly chosen based on a secret message bitstream, segments (signals) of the host data. The host signal can represent a patch of pixels from an image or a video frame. We propose a probabilistic model that infers the embedded message bitstream and watermark signal, directly from the watermarked data, without access to the decoder. We develop an efficient Markov chain Monte Carlo sampler for updating the model parameters from their conjugate full conditional posteriors. We also provide a variational Bayesian solution, which further increases the convergence speed of the algorithm. Experiments with synthetic and real image signals demonstrate that the attack model is able to correctly infer a large part of the message bitstream and obtain a very accurate estimate of the watermark signal.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1171–1178},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042723,
author = {Dahl, George E. and Adams, Ryan P. and Larochelle, Hugo},
title = {Training Restricted Boltzmann Machines on Word Observations},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The restricted Boltzmann machine (RBM) is a flexible model for complex data. However, using RBMs for high-dimensional multinomial observations poses significant computational difficulties. In natural language processing applications, words are naturally modeled by K-ary discrete distributions, where K is determined by the vocabulary size and can easily be in the hundred thousands. The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K-way softmax visible units during block Gibbs updates, an operation that takes time linear in K. In this work, we address this issue with a more general class of Markov chain Monte Carlo operators on the visible units, yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n-grams using larger vocabularies than previously feasible with RBMs and by using the learned features to improve performance on chunking and sentiment classification tasks, achieving state-of-the-art results on the latter.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1163–1170},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042722,
author = {Ruderman, Avraham and Reid, Mark D. and Garc\'{\i}a-Garc\'{\i}a, Dar\'{\i}o and Petterson, James},
title = {Tighter Variational Representations of F-Divergences via Restriction to Probability Measures},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We show that the variational representations for f-divergences currently used in the literature can be tightened. This has implications to a number of methods recently proposed based on this representation. As an example application we use our tighter representation to derive a general f-divergence estimator based on two i.i.d. samples and derive the dual program for this estimator that performs well empirically. We also point out a connection between our estimator and MMD.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1155–1162},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042721,
author = {Yu, Yao-Liang and Szepesv\'{a}ri, Csaba},
title = {Analysis of Kernel Mean Matching under Covariate Shift},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In real supervised learning scenarios, it is not uncommon that the training and test sample follow different probability distributions, thus rendering the necessity to correct the sampling bias. Focusing on a particular covariate shift problem, we derive high probability confidence bounds for the kernel mean matching (KMM) estimator, whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel. By comparing KMM with the natural plug-in estimator, we establish the superiority of the former hence provide concrete evidence/ understanding to the effectiveness of KMM under covariate shift.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1147–1154},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042720,
author = {Wilson, Andrew Gordon and Knowles, David A. and Ghahramani, Zoubin},
title = {Gaussian Process Regression Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the nonparametric exibility of Gaussian processes. GPRN accommodates input (predictor) dependent signal and noise correlations between multiple output (response) variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on real datasets, including a 1000 dimensional gene expression dataset.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1139–1146},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042719,
author = {Kulis, Brian and Jordan, Michael I.},
title = {Revisiting K-Means: New Algorithms via Bayesian Nonparametrics},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Bayesian models offer great flexibility for clustering applications--Bayesian nonparametrics can be used for modeling infinite mixtures, and hierarchical Bayesian models can be utilized for sharing clusters across multiple data sets. For the most part, such flexibility is lacking in classical clustering methods such as k-means. In this paper, we revisit the k-means clustering algorithm from a Bayesian nonparametric viewpoint. Inspired by the asymptotic connection between k-means and mixtures of Gaussians, we show that a Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like clustering objective that includes a penalty for the number of clusters. We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We also discuss further extensions that highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1131–1138},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042718,
author = {Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
title = {Deep Mixtures of Factor Analysers},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1123–1130},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042717,
author = {Prasse, Paul and Sawade, Christoph and Landwehr, Niels and Scheffer, Tobias},
title = {Learning to Identify Regular Expressions That Describe Email Campaigns},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper addresses the problem of inferring a regular expression from a given set of strings that resembles, as closely as possible, the regular expression that a human expert would have written to identify the language. This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns. Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist. We model this task as a learning problem with structured output spaces and an appropriate loss function, derive a decoder and the resulting optimization problem, and a report on a case study conducted with an email service.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1115–1122},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042716,
author = {Avner, Orly and Mannor, Shie and Shamir, Ohad},
title = {Decoupling Exploration and Exploitation in Multi-Armed Bandits},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider a multi-armed bandit problem where the decision maker can explore and exploit different arms at every round. The exploited arm adds to the decision maker's cumulative reward (without necessarily observing the reward) while the explored arm reveals its value. We devise algorithms for this setup and show that the dependence on the number of arms, k, can be much better than the standard √k dependence, depending on the behavior of the arms' reward sequences. For the important case of piecewise stationary stochastic bandits, we show a significant improvement over existing algorithms. Our algorithms are based on a non-uniform sampling policy, which we show is essential to the success of any algorithm in the adversarial setup. Finally, we show some simulation results on an ultra-wide band channel selection inspired setting indicating the applicability of our algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1107–1114},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042715,
author = {Honorio, Jean},
title = {Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the convergence rate of stochastic optimization of exact (NP-hard) objectives, for which only biased estimates of the gradient are available. We motivate this problem in the context of learning the structure and parameters of Ising models. We first provide a convergence-rate analysis of deterministic errors for forward-backward splitting (FBS). We then extend our analysis to biased stochastic errors, by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS, but also proximal gradient (PG) methods. We derive some interesting conclusions: FBS requires only a logarithmically increasing number of random samples in order to converge (although at a very low rate); the required number of random samples is the same for the deterministic and the biased stochastic setting for FBS and basic PG; accelerated PG is not guaranteed to converge in the biased stochastic setting.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1099–1106},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042714,
author = {Vogt, Julia E. and Roth, Volker},
title = {A Complete Analysis of the l<sub>1,p</sub> Group-Lasso},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The Group-Lasso is a well-known tool for joint regularization in machine learning methods. While the l1,2 and the l1,∞ version have been studied in detail and efficient algorithms exist, there are still open questions regarding other l1,p variants. We characterize conditions for solutions of the l1,p Group-Lasso for all p-norms with 1 ≤ p ≤ ∞, and we present a unified active set algorithm. For all p-norms, a highly efficient projected gradient algorithm is presented. This new algorithm enables us to compare the prediction performance of many variants of the Group-Lasso in a multi-task learning setting, where the aim is to solve many learning problems in parallel which are coupled via the Group-Lasso constraint. We conduct large-scale experiments on synthetic data and on two real-world data sets. In accordance with theoretical characterizations of the different norms we observe that the weak-coupling norms with p between 1.5 and 2 consistently outperform the strong-coupling norms with p ≫ 2.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1091–1098},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042713,
author = {Laue, S\"{o}ren},
title = {A Hybrid Algorithm for Convex Semidefinite Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a hybrid algorithm for optimizing a convex, smooth function over the cone of positive semidefinite matrices. Our algorithm converges to the global optimal solution and can be used to solve general large-scale semidefinite programs and hence can be readily applied to a variety of machine learning problems. We show experimental results on three machine learning problems. Our approach outperforms state-of-the-art algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1083–1090},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042712,
author = {Zhao, Peilin and Wang, Jialei and Wu, Pengcheng and Jin, Rong and Hoi, Steven C. H.},
title = {Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel-Based Online Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Kernel-based online learning has often shown state-of-the-art performance for many online learning tasks. It, however, suffers from a major shortcoming, that is, the unbounded number of support vectors, making it nonscalable and unsuitable for applications with large-scale datasets. In this work, we study the problem of bounded kernel-based online learning that aims to constrain the number of support vectors by a predefined budget. Although several algorithms have been proposed in literature, they are neither computationally efficient due to their intensive budget maintenance strategy nor effective due to the use of simple Perceptron algorithm. To overcome these limitations, we propose a framework for bounded kernel-based online learning based on an online gradient descent approach. We propose two efficient algorithms of bounded online gradient descent (BOGD) for scalable kernel-based online learning: (i) BOGD by maintaining support vectors using uniform sampling, and (ii) BOGD++ by maintaining support vectors using nonuniform sampling. We present theoretical analysis of regret bound for both algorithms, and found promising empirical performance in terms of both efficacy and efficiency by comparing them to several well-known algorithms for bounded kernel-based online learning on large-scale datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1075–1082},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042711,
author = {Memisevic, Roland},
title = {On Multi-View Feature Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1067–1074},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042710,
author = {Xie, Ning and Hachiya, Hirotaka and Sugiyama, Masashi},
title = {Artist Agent: A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Oriental ink painting, called Sumi-e, is one of the most appealing painting styles that has attracted artists around the world. Major challenges in computer-based Sumi-e simulation are to abstract complex scene information and draw smooth and natural brush strokes. To automatically generate such strokes, we propose to model a brush as a reinforcement learning agent, and learn desired brush-trajectories by maximizing the sum of rewards in the policy search framework. We also elaborate on the design of actions, states, and rewards tailored for a Sumi-e agent. The effectiveness of our proposed approach is demonstrated through simulated Sumi-e experiments.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1059–1066},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042709,
author = {Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W. and Woodruff, David P.},
title = {Fast Approximation of Matrix Coherence and Statistical Leverage},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The statistical leverage scores of a data matrix are the squared row-norms of any matrix whose columns are obtained by orthogonalizing the columns of the data matrix; and, the coherence is the largest leverage score. These quantities play an important role in several machine learning algorithms because they capture the key structural nonuniformity of the data matrix that must be dealt with in developing efficient randomized algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n \texttimes{} d matrix A, with n ≫ d, and returns, as output, relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs in O(nd log n) time, as opposed to the O(nd2) time required by the na\"{\i}ve algorithm that involves computing an orthogonal basis for the range of A. This resolves an open question from (Drineas et al., 2006) and (Mohri &amp; Talwalkar, 2011); and our result leads to immediate improvements in coreset-based l2-regression, the estimation of the coherence of a matrix, and several related low-rank matrix problems. Interestingly, to achieve our result we judiciously apply random projections on both sides of A.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1051–1058},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042708,
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
title = {Information-Theoretic Semi-Supervised Metric Learning via Entropy Regularization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a general information-theoretic approach called SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, SERAPH is regularized by encouraging a low-rank projection induced from the metric. The optimization of SERAPH is solved efficiently and stably by an EM-like scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that SERAPH compares favorably with many well-known global and local metric learning methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1043–1050},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042707,
author = {Lanctot, Marc and Gibson, Richard and Burch, Neil and Zinkevich, Martin and Bowling, Michael},
title = {No-Regret Learning in Extensive-Form Games with Imperfect Recall},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Counterfactual Regret Minimization (CFR) is an efficient no-regret learning algorithm for decision problems modeled as extensive games. CFR's regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR's guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. We also show that CFR applied to any abstraction belonging to our class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1035–1042},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042706,
author = {Hennig, Philipp and Kiefel, Martin},
title = {Quasi-Newton Methods: A New Direction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that fit a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efficient use of available information at computational cost similar to its predecessors.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1027–1034},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042705,
author = {Storkey, Amos J. and Millin, Jonathan J. and Geras, Krzysztof J.},
title = {Isoelastic Agents and Wealth Updates in Machine Learning Markets},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recently, prediction markets have shown considerable promise for developing flexible mechanisms for machine learning. In this paper, agents with isoelastic utilities are considered. It is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha-mixtures, with a particular form of mixing component relating to each agent's wealth. We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents (through payoffs on prediction of training targets) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures. An iterative algorithm is given for market equilibrium computation. We demonstrate that inhomogeneous markets of agents with isoelastic utilities outperform state of the art aggregate classifiers such as random forests, as well as single classifiers (neural networks, decision trees) on a number of machine learning benchmarks, and show that isoelastic combination methods are generally better than their logarithmic counterparts.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1019–1026},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042704,
author = {Telgarsky, Matus and Dasgupta, Sanjoy},
title = {Agglomerative Bregman Clustering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1011–1018},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042703,
author = {Denchev, Vasil S. and Ding, Nan and Vishwanathan, S. V. N. and Neven, Hartmut},
title = {Robust Classification with Adiabatic Quantum Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a non-convex training objective for robust binary classification of data sets in which label noise is present. The design is guided by the intention of solving the resulting problem by adiabatic quantum optimization. Two requirements are imposed by the engineering constraints of existing quantum hardware: training problems are formulated as quadratic unconstrained binary optimization; and model parameters are represented as binary expansions of low bit-depth. In the present work we validate this approach by using a heuristic classical solver as a stand-in for quantum hardware. Testing on several popular data sets and comparing with a number of existing losses we find substantial advantages in robustness as measured by test error under increasing label noise. Robustness is enabled by the non-convexity of our hardware-compatible loss function, q-loss.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1003–1010},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042702,
author = {Koepke, Hoyt and Bilenko, Mikhail},
title = {Fast Prediction of New Feature Utility},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the new feature utility prediction problem: statistically testing whether adding a feature to the data representation can improve the accuracy of a current predictor. In many applications, identifying new features is the main pathway for improving performance. However, evaluating every potential feature by re-training the predictor can be costly. The paper describes an efficient, learner-independent technique for estimating new feature utility without re-training based on the current predictor's outputs. The method is obtained by deriving a connection between loss reduction potential and the new feature's correlation with the loss gradient of the current predictor. This leads to a simple yet powerful hypothesis testing procedure, for which we prove consistency. Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large-scale industrial dataset.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {995–1002},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042701,
author = {Vladymyrov, Max and Carreira-Perpi\~{n}\'{a}n, Miguel \'{A}.},
title = {Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Stochastic neighbor embedding (SNE) and related nonlinear manifold learning algorithms achieve high-quality low-dimensional representations of similarity data, but are notoriously slow to train. We propose a generic formulation of embedding algorithms that includes SNE and other existing algorithms, and study their relation with spectral methods and graph Laplacians. This allows us to define several partial-Hessian optimization strategies, characterize their global and local convergence, and evaluate them empirically. We achieve up to two orders of magnitude speedup over existing training methods with a strategy (which we call the spectral direction) that adds nearly no overhead to the gradient and yet is simple, scalable and applicable to several existing and future embedding algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {987–994},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042700,
author = {Yue, Yisong and Hong, Sue Ann and Guestrin, Carlos},
title = {Hierarchical Exploration for Accelerating Contextual Bandits},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user's preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {979–986},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042699,
author = {Arnosti, Nicholas A. and Danyluk, Andrea Pohoreckyj},
title = {Feature Selection via Probabilistic Outputs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by Shen et al. (2008) and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {971–978},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042698,
author = {Martens, James and Sutskever, Ilya and Swersky, Kevin},
title = {Estimating the Hessian by Back-Propagating Curvature},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {963–970},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042697,
author = {De Freitas, Nando and Smola, Alex J. and Zoghi, Masrour},
title = {Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al., 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, (Srinivas et al., 2010) proved that the regret vanishes at the approximate rate of O(1/√t), where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to O(e-τt/(ln t)d/4) with high probability. Here, d is the dimension of the search space and τ is a constant that depends on the behaviour of the objective function near its global maximum.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {955–962},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042696,
author = {Kim, Myunghwan and Leskovec, Jure},
title = {Latent Multi-Group Membership Graph Model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop the Latent Multi-group Membership Graph (LMMG) model, a model of networks with rich node feature structure. In the LMMG model, each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure. The LMMG can be used to summarize the network structure, to predict links between the nodes, and to predict missing features of a node. We derive efficient inference and learning algorithms and evaluate the predictive performance of the LMMG on several social and document network datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {947–954},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042695,
author = {Boukouvalas, Alexis and Barillec, Remi and Cornford, Dan},
title = {Direct Gaussian Process Quantile Regression Using Expectation Propagation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {939–946},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042694,
author = {Kong, Deguang and Ding, Chris and Huang, Heng and Nie, Feiping},
title = {An Iterative Locally Linear Embedding Algorithm},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Locally Linear embedding (LLE) is a popular dimension reduction method. In this paper, we systematically improve the two main steps of LLE: (A) learning the graph weights W, and (B) learning the embedding Y. We propose a sparse nonnegative W learning algorithm. We propose a weighted formulation for learning Y and show the results are identical to normalized cuts spectral clustering. We further propose to iterate the two steps in LLE repeatedly to improve the results. Extensive experiment results show that iterative LLE algorithm significantly improves both classification and clustering results.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {931–938},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042693,
author = {Fedorova, Valentina and Gammerman, Alex and Nouretdinov, Ilia and Vovk, Vladimir},
title = {Plug-in Martingales for Testing Exchangeability on-Line},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {A standard assumption in machine learning is the exchangeability of data, which is equivalent to assuming that the examples are generated from the same probability distribution independently. This paper is devoted to testing the assumption of exchangeability on-line: the examples arrive one by one, and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified. Such measures are provided by exchangeability martingales. We extend known techniques for constructing exchangeability martingales and show that our new method is competitive with the martingales introduced before. Finally we investigate the performance of our testing method on two benchmark datasets, USPS and Statlog Satellite data; for the former, the known techniques give satisfactory results, but for the latter our new more exible method becomes necessary.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {923–930},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042692,
author = {Guo, Yuhong and Xiao, Min},
title = {Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In many multilingual text classification problems, the documents in different languages often share the same set of categories. To reduce the labeling cost of training a classification model for each individual language, it is important to transfer the label knowledge gained from one language to another language by conducting cross language classification. In this paper we develop a novel subspace co-regularized multi-view learning method for cross language text classification. This method is built on parallel corpora produced by machine translation. It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents. Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms a number of inductive methods, domain adaptation methods, and multi-view learning methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {915–922},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042691,
author = {Jiang, Yun and Lim, Marcus and Saxena, Ashutosh},
title = {Learning Object Arrangements in 3D Scenes Using Human Context},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of learning object arrangements in a 3D scene. The key idea here is to learn how objects relate to human poses based on their affordances, ease of use and reachability. In contrast to modeling object-object relationships, modeling human-object relationships scales linearly in the number of objects. We design appropriate density functions based on 3D spatial features to capture this. We learn the distribution of human poses in a scene using a variant of the Dirichlet process mixture model that allows sharing of the density function parameters across the same object types. Then we can reason about arrangements of the objects in the room based on these meaningful human poses. In our extensive experiments on 20 different rooms with a total of 47 objects, our algorithm predicted correct placements with an average error of 1:6 meters from ground truth. In arranging five real scenes, it received a score of 4.3/5 compared to 3.7 for the best baseline method.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {907–914},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042690,
author = {Hu, Yuening and Zhai, Ke and Williamson, Sinead and Boyd-Graber, Jordan},
title = {Modeling Images Using Transformed Indian Buffet Processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {899–906},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042689,
author = {Makino, Takaki and Takeuchi, Johane},
title = {Apprenticeship Learning for Model Parameters of Partially Observable Environments},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider apprenticeship learning -- i.e., having an agent learn a task by observing an expert demonstrating the task -- in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {891–898},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042688,
author = {Xu, Min and Lafferty, John},
title = {Conditional Sparse Coding and Grouped Multivariate Regression},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the problem of multivariate regression where the data are naturally grouped, and a regression matrix is to be estimated for each group. We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups, and a sparse linear combination of the dictionary elements is estimated to form a model within each group. We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group. It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience. We propose an algorithm for conditional sparse coding, analyze its theoretical properties in terms of predictive accuracy, and present the results of simulation and brain imaging experiments that compare the new technique to reduced rank regression.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {883–890},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042687,
author = {Kolar, Mladen and Sharpnack, James},
title = {Variance Function Estimation in High-Dimensions},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the high-dimensional heteroscedastic regression model, where the mean and the log variance are modeled as a linear combination of input variables. Existing literature on high-dimensional linear regression models has largely ignored non-constant error variances, even though they commonly occur in a variety of applications ranging from biostatistics to finance. In this paper we study a class of nonconvex penalized pseudolikelihood estimators for both the mean and variance parameters. We show that the Heteroscedastic Iterative Penalized Pseudolikelihood Optimizer (HIPPO) achieves the oracle property, that is, we prove that the rates of convergence are the same as if the true model was known. We demonstrate numerical properties of the procedure on a simulation study and real world data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {875–882},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042686,
author = {Painter-Wakefield, Christopher and Parr, Ronald},
title = {Greedy Algorithms for Sparse Reinforcement Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning (RL) community to expand the reach and applicability of RL. One approach to the problem of feature selection is to impose a sparsity-inducing form of regularization on the learning method. Recent work on L1 regularization has adapted techniques from the supervised learning literature for use with RL. Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features. Such algorithms have many of the good properties of the L1 regularization methods, while also being extremely efficient and, in some cases, allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data. This paper considers variants of orthogonal matching pursuit (OMP) applied to reinforcement learning. The resulting algorithms are analyzed and compared experimentally with existing L1 regularized approaches. We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails; however, one variant, OMP-BRM, provides promising theoretical guarantees under certain assumptions on the feature dictionary. Another variant, OMP-TD, empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {867–874},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042685,
author = {Zhou, Mingyuan and Li, Lingbo and Dunson, David and Carin, Lawrence},
title = {Lognormal and Gamma Mixed Negative Binomial Regression},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of random effects, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {859–866},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042684,
author = {Pachauri, Deepti and Collins, Maxwell and Kondor, Risi and Singh, Vikas},
title = {Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Matching one set of objects to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem (QAP). The QAP is known to be notoriously hard, both in theory and in practice. Here, we investigate if this difficulty can be mitigated when some additional piece of information is available: (a) that all QAP instances of interest come from the same application, and (b) the correct solution for a set of such QAP instances is given. We propose a new approach to accelerate the solution of QAPs based on learning parameters for a modified objective function from prior QAP instances. A key feature of our approach is that it takes advantage of the algebraic structure of permutations, in conjunction with special methods for optimizing functions over the symmetric group Sn in Fourier space. Experiments show that in practical domains the new method can outperform existing approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {851–858},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042683,
author = {Garnett, Roman and Krishnamurthy, Yamuna and Xiong, Xuehan and Schneider, Jeff and Mann, Richard},
title = {Bayesian Optimal Active Search and Surveying},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider two active binary-classification problems with atypical objectives. In the first, active search, our goal is to actively uncover as many members of a given class as possible. In the second, active surveying, our goal is to actively query points to ultimately predict the proportion of a given class. Numerous real-world problems can be framed in these terms, and in either case typical model-based concerns such as generalization error are only of secondary importance.We approach these problems via Bayesian decision theory; after choosing natural utility functions, we derive the optimal policies. We provide three contributions. In addition to introducing the active surveying problem, we extend previous work on active search in two ways. First, we prove a novel theoretical result, that less-myopic approximations to the optimal policy can outperform more-myopic approximations by any arbitrary degree. We then derive bounds that for certain models allow us to reduce (in practice dramatically) the exponential search space required by a na\"{\i}ve implementation of the optimal policy, enabling further lookahead while still ensuring that optimal decisions are always made.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {843–850},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042682,
author = {Ji, Ming and Yang, Tianbao and Lin, Binbin and Jin, Rong and Han, Jiawei},
title = {A Simple Algorithm for Semi-Supervised Learning with Improved Generalization Error Bound},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {835–842},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042681,
author = {Desautels, Thomas and Krause, Andreas and Burdick, Joel},
title = {Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Can one parallelize complex exploration-exploitation tradeoffs? As an example, consider the problem of optimal high-throughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multiarmed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {827–834},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042680,
author = {Bachrach, Yoram and Minka, Tom and Guiver, John and Graepel, Thore},
title = {How to Grade a Test without Knowing the Answers: A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {819–826},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042679,
author = {Doppa, Janardhan Rao and Fern, Alan and Tadepalli, Prasad},
title = {Output Space Search for Structured Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we define the limited-discrepancy search space over structured outputs, which is able to leverage powerful classification learning algorithms to improve the search space quality. Second, we give a generic cost function learning approach, where the key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state-of-the-art structured-prediction performance.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {811–818},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042678,
author = {Spiliopoulou, Athina and Storkey, Amos},
title = {A Topic Model for Melodic Sequences},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre. This is a challenging task as one needs to capture not only the rich temporal structure evident in music, but also the complex statistical dependencies among different music components. To address this problem we introduce the Variable-gram Topic Model, which couples the latent topic formalism with a systematic model for contextual information. We evaluate the model on next-step prediction. Additionally, we present a novel way of model evaluation, where we directly compare model samples with data sequences using the Maximum Mean Discrepancy of string kernels, to assess how close is the model distribution to the data distribution. We show that the model has the highest performance under both evaluation measures when compared to LDA, the Topic Bigram and related non-topic models.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {803–810},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042677,
author = {Wang, Yi and Won, Kok Sung and Hsu, David and Lee, Wee Sun},
title = {Monte Carlo Bayesian Reinforcement Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Bayesian reinforcement learning (BRL) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a probability distribution over them. This paper presents Monte Carlo BRL (MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori a finite set of hypotheses for the model parameter values and forms a discrete partially observable Markov decision process (POMDP) whose state space is a cross product of the state space for the reinforcement learning task and the sampled model parameter space. The POMDP does not require conjugate distributions for belief representation, as earlier works do, and can be solved relatively easily with point-based approximation algorithms. MC-BRL naturally handles both fully and partially observable worlds. Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL task well with guaranteed performance.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {795–802},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042676,
author = {Sejdinovic, Dino and Gretton, Arthur and Sriperumbudur, Bharath and Fukumizu, Kenji},
title = {Hypothesis Testing Using Pairwise Distances and Associated Kernels},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. The equivalence holds when energy distances are computed with semimetrics of negative type, in which case a kernel may be defined such that the RKHS distance between distributions corresponds exactly to the energy distance. We determine the class of probability distributions for which kernels induced by semimetrics are characteristic (that is, for which embeddings of the distributions to an RKHS are injective). Finally, we investigate the performance of this family of kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {787–794},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042675,
author = {Sun, Peng and Reid, Mark D. and Zhou, Jie},
title = {AOSO-Logitboost: Adaptive One-vs-One Logitboost for Multi-Class Problem},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper presents an improvement to model learning when using multi-class LogitBoost for classification. Motivated by the statistical view, LogitBoost can be seen as additive tree regression. Two important factors in this setting are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the dense Hessian matrices that arise when computing tree node split gain and node value fittings. In general, this setting is too complicated for a tractable model learning algorithm. However, too aggressive simplification of the setting may lead to degraded performance. For example, the original LogitBoost is outperformed by ABC-LogitBoost due to the latter's more careful treatment of the above two factors.In this paper we propose techniques to address the two main difficulties of the LogitBoost setting: 1) we adopt a vector tree (i.e., each node value is vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block coordinate descent that exploits the dense Hessian when computing tree split gain and node values. Higher classification accuracy and faster convergence rates are observed for a range of public data sets when compared to both the original and the ABC-LogitBoost implementations.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {779–786},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042674,
author = {Peharz, Robert and Pernkopf, Franz},
title = {Exact Maximum Margin Structure Learning of Bayesian Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recently, there has been much interest in finding globally optimal Bayesian network structures. These techniques were developed for generative scores and can not be directly extended to discriminative scores, as desired for classification. In this paper, we propose an exact method for finding network structures maximizing the probabilistic soft margin, a successfully applied discriminative score. Our method is based on branch-and-bound techniques within a linear programming framework and maintains an any-time solution, together with worst-case sub-optimality bounds. We apply a set of order constraints for enforcing the network structure to be acyclic, which allows a compact problem representation and the use of general-purpose optimization techniques. In classification experiments, our methods clearly outperform generatively trained network structures and compete with support vector machines.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {771–778},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042673,
author = {Sato, Issei and Nakagawa, Hiroshi},
title = {Rethinking Collapsed Variational Bayes Inference for LDA},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a novel interpretation of the collapsed variational Bayes inference with a zero-order Taylor expansion approximation, called CVB0 inference, for latent Dirichlet allocation (LDA). We clarify the properties of the CVB0 inference by using the α-divergence. We show that the CVB0 inference is composed of two different divergence projections: α = 1 and -1. This interpretation will help shed light on CVB0 works.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {763–770},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042672,
author = {Kir\'{a}ly, Franz and Tomioka, Ryota},
title = {A Combinatorial Algebraic Approach for the Identifiability of Low-Rank Matrix Completion},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we review the problem of matrix completion and expose its intimate relations with algebraic geometry, combinatorics and graph theory. We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries, yielding theoretical constraints and new algorithms for the problem of matrix completion. We conclude by algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes, showing that the algebraic-combinatorial approach can lead to improvements over state-of-the-art matrix completion methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {755–762},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042671,
author = {Benbouzid, Djalel and Busa-Fekete, R\"{o}bert and K\'{e}gl, Bal\'{a}zs},
title = {Fast Classification Using Sparse Decision DAGs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark Web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {747–754},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042670,
author = {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nathan},
title = {The Kernelized Stochastic Batch Perceptron},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {739–746},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042669,
author = {Balakrishnan, Sivaraman and Puniyani, Kriti and Lafferty, John},
title = {Sparse Additive Functional and Kernel CCA},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Canonical Correlation Analysis (CCA) is a classical tool for finding correlations among the components of two random vectors. In recent years, CCA has been widely applied to the analysis of genomic data, where it is common for researchers to perform multiple assays on a single set of patient samples. Recent work has proposed sparse variants of CCA to address the high dimensionality of such data. However, classical and sparse CCA are based on linear models, and are thus limited in their ability to find general correlations. In this paper, we present two approaches to high-dimensional nonparametric CCA, building on recent developments in high-dimensional nonparametric regression. We present estimation procedures for both approaches, and analyze their theoretical properties in the high-dimensional setting. We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations, as well as through experiments with genomic data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {731–738},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042668,
author = {Hartikainen, Jouni and Sepp\"{a}nen, Mari and S\"{a}rkk\"{a}, Simo},
title = {State-Space Inference for Non-Linear Latent Force Models with Application to Satellite Orbit Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Latent force models (LFMs) are flexible models that combine mechanistic modelling principles (i.e., physical models) with nonparametric data-driven components. Several key applications of LFMs need nonlinearities, which results in analytically intractable inference. In this work we show how non-linear LFMs can be represented as nonlinear white noise driven state-space models and present an efficient non-linear Kalman filtering and smoothing based method for approximate state and parameter inference. We illustrate the performance of the proposed methodology via two simulated examples, and apply it to a real-world problem of long-term prediction of GPS satellite orbits.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {723–730},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042667,
author = {Chen, Changyou and Ding, Nan and Buntine, Wray},
title = {Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling. The dependency arises via superposition, subsampling and point transition on the underlying Poisson processes of these measures. The measures used include normalised generalised Gamma processes that demonstrate power law properties, unlike Dirichlet processes used previously in dynamic topic modeling. Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process. Experiments performed on news, blogs, academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {715–722},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042666,
author = {Yang, Zhirong and Oja, Erkki},
title = {Clustering by Low-Rank Doubly Stochastic Matrix Decomposition},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Clustering analysis by nonnegative low-rank approximations has achieved remarkable progress in the past decade. However, most approximation approaches in this direction are still restricted to matrix factorization. We propose a new low-rank learning method to improve the clustering performance, which is beyond matrix factorization. The approximation is based on a two-step bipartite random walk through virtual cluster nodes, where the approximation is formed by only cluster assigning probabilities. Minimizing the approximation error measured by Kullback-Leibler divergence is equivalent to maximizing the likelihood of a discriminative model, which endows our method with a solid probabilistic interpretation. The optimization is implemented by a relaxed Majorization-Minimization algorithm that is advantageous in finding good local minima. Furthermore, we point out that the regularized algorithm with Dirichlet prior only serves as initialization. Experimental results show that the new method has strong performance in clustering purity for various datasets, especially for large-scale manifold data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {707–714},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042665,
author = {Pletscher, Patrick and Wulff, Sharon},
title = {LPQP for MAP: Putting LP Solvers to Better Use},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {MAP inference for general energy functions remains a challenging problem. While most efforts are channeled towards improving the linear programming (LP) based relaxation, this work is motivated by the quadratic programming (QP) relaxation. We propose a novel MAP relaxation that penalizes the Kullback-Leibler divergence between the LP pairwise auxiliary variables, and QP equivalent terms given by the product of the unaries. We develop two efficient algorithms based on variants of this relaxation. The algorithms minimize the non-convex objective using belief propagation and dual decomposition as building blocks. Experiments on synthetic and real-world data show that the solutions returned by our algorithms substantially improve over the LP relaxation.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {699–706},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042664,
author = {Purushotham, Sanjay and Liu, Yan and Kuo, C.-C. Jay},
title = {Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Social network websites, such as Facebook, YouTube, Lastfm etc, have become a popular platform for users to connect with each other and share content or opinions. They provide rich information for us to study the influence of user's social circle in their decision process. In this paper, we are interested in examining the effectiveness of social network information to predict the user's ratings of items. We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks. A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data. Empirical experiments on two large-scale datasets show that our algorithm provides a more effective recommendation system than the state-of-the art approaches. Our results reveal interesting insight that the social circles have more influence on people's decisions about the usefulness of information (e.g., bookmarking preference on Delicious) than personal taste (e.g., music preference on Lastfm). We also examine and discuss solutions on potential information leak in many recommendation systems that utilize social information.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {691–698},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042663,
author = {Mohamed, Shakir and Heller, Katherine A. and Ghahramani, Zoubin},
title = {Bayesian and L<sub>1</sub> Approaches for Sparse Unsupervised Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The use of L1 regularisation for sparse learning has generated immense research interest, with many successful applications in diverse areas such as signal acquisition, image coding, genomics and collaborative filtering. While existing work highlights the many advantages of L1 methods, in this paper we find that L1 regularisation often dramatically under-performs in terms of predictive performance when compared to other methods for inferring sparsity. We focus on unsupervised latent variable models, and develop L1 minimising factor models, Bayesian variants of "L1", and Bayesian models with a stronger L0-like sparsity induced through spike-and-slab distributions. These spike-and-slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner, and avoid unnecessary shrinkage of non-zero values. We demonstrate on a number of data sets that in practice spike-and-slab Bayesian methods outperform L1 minimisation, even on a computational budget. We thus highlight the need to re-assess the wide use of L1 methods in sparsity-reliant applications, particularly when we care about generalising to previously unseen data, and provide an alternative that, over many varying conditions, provides improved generalisation performance.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {683–690},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042662,
author = {Kim, Dongwoo and Kim, Suin and Oh, Alice},
title = {Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We describe a nonparametric topic model for labeled data. The model uses a mixture of random measures (MRM) as a base distribution of the Dirichlet process (DP) of the HDP framework, so we call it the DP-MRM. To model labeled data, we define a DP distributed random measure for each label, and the resulting model generates an unbounded number of topics for each label. We apply DP-MRM on single-labeled and multi-labeled corpora of documents and compare the performance on label prediction with MedLDA, LDA-SVM, and Labeled-LDA. We further enhance the model by incorporating ddCRP and modeling multi-labeled images for image segmentation and object labeling, comparing the performance with nCuts and rddCRP.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {675–682},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042661,
author = {Duan, Lixin and Xu, Dong and Tsang, Ivor W.},
title = {Learning with Augmented Features for Heterogeneous Domain Adaptation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {667–674},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042660,
author = {Menon, Aditya Krishna and Jiang, Xiaoqian and Vembu, Shankar and Elkan, Charles and Ohno-Machado, Lucila},
title = {Predicting Accurate Probabilities with a Ranking Loss},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In many real-world applications of machine learning classifiers, it is essential to predict the probability of an example belonging to a particular class. This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss, followed by isotonic regression. This semi-parametric technique offers both good ranking and regression performance, and models a richer set of probability distributions than statistical workhorses such as logistic regression. We provide experimental results that show the effectiveness of this technique on real-world applications of probability prediction.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {659–666},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042659,
author = {Zhong, Mingjun and Girolami, Mark},
title = {A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-the-art performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {651–658},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042658,
author = {Shi, Qinfeng and Shen, Chunhua and Hill, Rhys and Van Den Hengel, Anton},
title = {Is Margin Preserved after Random Projection?},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Random projections have been applied in many machine learning algorithms. However, whether margin is preserved after random projection is non-trivial and not well studied. In this paper we analyse margin distortion after random projection, and give the conditions of margin preservation for binary classification problems. We also extend our analysis to margin for multiclass problems, and provide theoretical bounds on multiclass margin on the projected data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {643–650},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042657,
author = {Kolar, Mladen and Xing, Eric P.},
title = {Estimating Sparse Precision Matrices from Data with Missing Values},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study a simple two step procedure for estimating sparse precision matrices from data with missing values, which is tractable in high-dimensions and does not require imputation of the missing values. We provide rates of convergence for this estimator in the spectral norm, Frobenius norm and element-wise l∞ norm. Simulation studies show that this estimator compares favorably with the EM algorithm. Our results have important practical consequences as they show that standard tools for estimating sparse precision matrices can be used when data contains missing values, without resorting to the iterative EM algorithm that can be slow to converge in practice for large problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {635–642},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042656,
author = {Ravanbakhsh, Siamak and Yu, Chun-Nam and Greiner, Russell},
title = {A Generalized Loop Correction Method for Approximate Inference in Graphical Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based  approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {627–634},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042655,
author = {Jalali, Ali and Sanghavi, Sujay},
title = {Learning the Dependence Graph of Time Series with Latent Factors},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. We observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure of the observed variables - separating out the spurious interactions caused by the latent variables' time series. We develop a new convex optimization based method to do so in the case when the number of latent variables is smaller than the number of observed ones. For the case when the dependency structure between the observed variables is sparse, we theoretically establish a high-dimensional scaling result for structure recovery. We verify our theoretical result with both synthetic and real data (from the stock market).},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {619–626},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042654,
author = {Lin, Tong and Xie, Hanlin and Wang, Ling and Zha, Hongbin},
title = {Total Variation and Euler's Elastica for Supervised Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In recent years, total variation (TV) and Euler's elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler's elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {611–618},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042653,
author = {Kimura, Daisuke and Kashima, Hisashi},
title = {Fast Computation of Subpath Kernel for Trees},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The kernel method is a popular approach to analyzing structured data such as sequences, trees, and graphs; however, unordered trees have not been investigated extensively. Kimura et al. (2011) proposed a kernel function for unordered trees on the basis of their subpaths, which are vertical substructures of trees responsible for hierarchical information in them. Their kernel exhibits practically good performance in terms of accuracy and speed; however, linear-time computation is not guaranteed theoretically, unlike the case of the other unordered tree kernel proposed by Vishwanathan and Smola (2003). In this paper, we propose a theoretically guaranteed linear-time kernel computation algorithm that is also practically fast, and we present an efficient prediction algorithm whose running time depends only on the size of the input tree. Experimental results show that the proposed algorithms are quite efficient in practice.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {603–610},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042652,
author = {Lozano, Aur\'{e}lie C. and Swirszcz, Grzegorz},
title = {Multi-Level Lasso for Sparse Multi-Task Regression},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a flexible formulation for variable selection in multi-task regression to allow for discrepancies in the estimated sparsity patterns accross the multiple tasks, while leveraging the common structure among them. Our approach is based on an intuitive decomposition of the regression coe_cients into a product between a component that is common to all tasks and another component that captures task-specificity. This decomposition yields the Multi-level Lasso objective that can be solved efficiently via alternating optimization. The analysis of the "orthonormal design" case reveals some interesting insights on the nature of the shrinkage performed by our method, compared to that of related work. Theoretical guarantees are provided on the consistency of Multi-level Lasso. Simulations and empirical study of micro-array data further demonstrate the value of our framework.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {595–602},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042651,
author = {Sun, Yi and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
title = {On the Size of the Online Kernel Sparsification Dictionary},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We analyze the size of the dictionary constructed from online kernel sparsi_cation, using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator. Using this formula, we are able to connect the cardinality of the dictionary with the eigen-decay of the covariance operator. In particular, we show that under certain technical conditions, the size of the dictionary will always grow sublinearly in the number of data points, and, as a consequence, the kernel linear regressor constructed from the resulting dictionary is consistent.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {587–594},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042650,
author = {Gomez-Rodriguez, Manuel and Sch\"{o}lkopf, Bernhard},
title = {Influence Maximization in Continuous Time Diffusion Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The problem of finding the optimal set of source nodes in a diffusion network that maximizes the spread of information, influence, and diseases in a limited amount of time depends dramatically on the underlying temporal dynamics of the network. However, this still remains largely unexplored to date. To this end, given a network and its temporal dynamics, we first describe how continuous time Markov chains allow us to analytically compute the average total number of nodes reached by a diffusion process starting in a set of source nodes. We then show that selecting the set of most influential source nodes in the continuous time influence maximization problem is NP-hard and develop an efficient approximation algorithm with provable near-optimal performance. Experiments on synthetic and real diffusion networks show that our algorithm outperforms other state of the art algorithms by at least ∼20% and is robust across different network topologies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {579–586},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042649,
author = {Nowozin, Sebastian},
title = {Improved Information Gain Estimates for Decision Tree Induction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Ensembles of classification and regression trees remain popular machine learning methods because they define flexible non-parametric models that predict well and are computationally efficient both during training and testing. During induction of decision trees one aims to find predicates that are maximally informative about the prediction target. To select good predicates most approaches estimate an information-theoretic scoring function, the information gain, both for classification and regression problems. We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees. In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {571–578},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042648,
author = {Li, Bin and Hoi, Steven C. H.},
title = {On-Line Portfolio Selection with Moving Average Reversion},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {On-line portfolio selection has attracted increasing interests in machine learning and AI communities recently. Empirical evidence show that stock's high and low prices are temporary and stock price relatives are likely to follow the mean reversion phenomenon. While the existing mean reversion strategies are shown to achieve good empirical performance on many real datasets, they often make the single-period mean reversion assumption, which is not always satisfied, leading to poor performance in some real datasets. To overcome the limitation, this article proposes a multiple-period mean reversion, or so-called "Moving Average Reversion" (MAR), and a new on-line portfolio selection strategy named "On-Line Moving Average Reversion" (OLMAR), which exploits MAR by applying powerful online learning techniques. From our empirical results, we found that OLMAR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results, especially on the datasets where existing mean reversion algorithms failed. In addition to superior performance, OLMAR also runs extremely fast, further supporting its practical applicability to a wide range of applications.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {563–570},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042647,
author = {Defazio, Aaron J. and Caetano, Tib\'{e}rio S.},
title = {A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items, where each item is connected to those it is most similar to. The prediction of a user's rating on an item is then given by that rating of neighbouring items, weighted by their similarity. This paper presents a new neighbourhood approach which we call item fields, whereby an undirected graphical model is formed over the item graph. The resulting prediction rule is a simple generalization of the classical approaches, which takes into account non-local information in the graph, allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches. A fast approximate maximum entropy training method based on the Bethe approximation is presented, which uses a simple gradient ascent procedure. When using precomputed sufficient statistics on the Movielens datasets, our method is faster than maximum likelihood approaches by two orders of magnitude.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {555–562},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042646,
author = {Wang, Jiabing and Chen, Jiaye},
title = {Clustering to Maximize the Ratio of Split to Diameter},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Given a weighted and complete graph G = (V, E), V denotes the set of n objects to be clustered, and the weight d(u, v) associated with an edge (u, v) ∈ E denotes the dissimilarity between objects u and v. The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster, and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster. In this paper, we propose a new criterion for measuring the goodness of clusters--the ratio of the minimum split to the maximum diameter, and the objective is to maximize the ratio. For k = 2, we present an exact algorithm. For k ≥ 3, we prove that the problem is NP-hard and present a factor of 2 approximation algorithm on the precondition that the weights associated with E satisfy the triangle inequality. The worst-case runtime of both algorithms is O(n3). We compare the proposed algorithms with the Normalized Cut by applying them to image segmentation. The experimental results on both natural and synthetic images demonstrate the effectiveness of the proposed algorithms.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {547–554},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042645,
author = {Kalaitzis, Alfredo A. and Lawrence, Neil D.},
title = {Residual Component Analysis: Generalising PCA for More Flexible Inference in Linear-Gaussian Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Probabilistic principal component analysis (PPCA) seeks a low dimensional representation of a data set in the presence of independent spherical Gaussian noise. The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix. In this paper we consider the situation where the data variance is already partially explained by other factors, for example sparse conditional dependencies between the covariates, or temporal correlations leaving some residual variance. We decompose the residual variance into its components through a generalised eigenvalue problem, which we call residual component analysis (RCA). We explore a range of new algorithms that arise from the framework, including one that factorises the covariance of a Gaussian density into a low-rank and a sparse-inverse component. We illustrate the ideas on the recovery of a protein-signaling network, a gene expression time-series data set and the recovery of the human skeleton from motion capture 3-D cloud data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {539–546},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042644,
author = {Damianou, Andreas C. and Ek, Carl Henrik and Titsias, Michalis K. and Lawrence, Neil D.},
title = {Manifold Relevance Determination},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a "softly" shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {531–538},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042643,
author = {Takeda, Akiko and Mitsugi, Hiroyuki and Kanamori, Takafumi},
title = {A Unified Robust Classification Model},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {A wide variety of machine learning algorithms such as support vector machine (SVM), minimax probability machine (MPM), and Fisher discriminant analysis (FDA), exist for binary classification. The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach. This unified model has several benefits. One is that the extensions and improvements intended for SVM become applicable to MPM and FDA, and vice versa. Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model. We give a statistical interpretation of the unified classification model and propose a non-convex optimization algorithm that can be applied to nonconvex variants of existing learning methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {523–530},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042642,
author = {Araya-L\'{o}pez, Mauricio and Thomas, Vincent and Buffet, Olivier},
title = {Near-Optimal BRL Using Optimistic Local Transitions},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Model-based Bayesian Reinforcement Learning (BRL) allows a sound formalization of the problem of acting optimally while facing an unknown environment, i.e., avoiding the exploration-exploitation dilemma. However, algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms. This paper introduces BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is optimistic about the transition function. We analyze BOLT's sample complexity, and show that under certain parameters, the algorithm is near-optimal in the Bayesian sense with high probability. Then, experimental results highlight the key differences of this method compared to previous work.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {515–522},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042641,
author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
title = {Building High-Level Features Using Large Scale Unsupervised Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\texttimes{}200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {507–514},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042640,
author = {Wang, Yingjian and Carin, Lawrence},
title = {L\'{e}vy Measure Decompositions for the Beta and Gamma Processes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop new representations for the L\'{e}vy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on L\'{e}vy processes, as these are of increasing importance in machine learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {499–506},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042639,
author = {Wulsin, Drausin and Jensen, Shane and Litt, Brian},
title = {A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process--the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)--that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP's use in modeling seizures across multiple patients. We find the MLC-HDP's clustering to be comparable to independent human physician clusterings. To our knowledge, the MLC-HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {491–498},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042638,
author = {Zhong, Leon Wenliang and Kwok, James T.},
title = {Convex Multitask Learning with Flexible Task Clusters},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Traditionally, multitask learning (MTL) assumes that all the tasks are related. This can lead to negative transfer when tasks are indeed incoherent. Recently, a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships. However, they are limited to modeling these relationships at the task level, which may be restrictive in some applications. In this paper, we propose a novel MTL formulation that captures task relationships at the feature-level. Depending on the interactions among tasks and features, the proposed method construct different task clusters for different features, without even the need of pre-specifying the number of clusters. Computationally, the proposed formulation is strongly convex, and can be efficiently solved by accelerated proximal methods. Experiments are performed on a number of synthetic and real-world data sets. Under various degrees of task relationships, the accuracy of the proposed method is consistently among the best. Moreover, the feature-specific task clusters obtained agree with the known/plausible task structures of the data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {483–490},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042637,
author = {Levine, Sergey and Koltun, Vladlen},
title = {Continuous Inverse Optimal Control with Locally Optimal Examples},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {475–482},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042636,
author = {Liu, Wei and Wang, Jun and Mu, Yadong and Kumar, Sanjiv and Chang, Shih-Fu},
title = {Compact Hyperplane Hashing with Bilinear Functions},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Hyperplane hashing aims at rapidly searching nearest points to a hyperplane, and has shown practical impact in scaling up active learning with SVMs. Unfortunately, the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end, this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions, which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance, we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes, and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {467–474},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042635,
author = {Sch\"{o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
title = {On Causal and Anticausal Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {459–466},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042634,
author = {Mannor, Shie and Mebel, Ofir and Xu, Huan},
title = {Lightning Does Not Strike Twice: Robust MDPs with Coupled Uncertainty},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed "Lightning Does not Strike Twice," to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {451–458},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042633,
author = {Weston, Jason and Wang, Chong and Weiss, Ron and Berenzeig, Adam},
title = {Latent Collaborative Retrieval},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user's preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query \texttimes{} user \texttimes{} item tensor for training instead of the more traditional user \texttimes{} item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user's profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {443–450},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042632,
author = {Janzamin, Majid and Anandkumar, Animashree},
title = {High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we present a novel framework incorporating a combination of sparse models in different domains. We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse Gaussian independence model (with a sparse covariance matrix). We provide efficient methods for decomposition of the data into two domains, viz., Markov and independence domains. We characterize a set of sufficient conditions for identifiability and model consistency. Our decomposition method is based on a simple modification of the popular l1-penalized maximum-likelihood estimator (l1-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples n scales as n = Ω(d2 log p), where p is the number of variables and d is the maximum node degree in the Markov model. Our conditions for recovery are comparable to those of l1-MLE for consistent estimation of a sparse Markov model, and thus, we guarantee successful high-dimensional estimation of a richer class of models under comparable conditions. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {435–442},
numpages = {8},
keywords = {sparse graphical model selection, sparsistency, convex optimization, sparse covariance models, high-dimensional covariance estimation},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042631,
author = {Borboudakis, Giorgos and Tsamardinos, Ioannis},
title = {Incorporating Causal Prior Knowledge as Path-Constraints in Bayesian Networks and Maximal Ancestral Graphs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the incorporation of causal knowledge about the presence or absence of (possibly indirect) causal relations into a causal model. Such causal relations correspond to directed paths in a causal model. This type of knowledge naturally arises from experimental data, among others. Specifically, we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes: Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs. We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models. In simulated experiments, we show that often considering even a few causal facts leads to a significant number of new inferences. In a case study, we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network. The code is available at mensxmachina.org.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {427–434},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042630,
author = {Mnih, Andriy and Teh, Yee Whye},
title = {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well.We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {419–426},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042629,
author = {Yackley, Benjamin and Lane, Terran},
title = {Smoothness and Structure Learning by Proxy},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {As data sets grow in size, the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account. For instance, Bayesian networks, the model chosen in this paper, have a super-exponentially large search space for a fixed number of variables. One possible method to alleviate this problem is to use a proxy, such as a Gaussian Process regressor, in place of the true scoring function, training it on a selection of sampled networks. We prove here that the use of such a proxy is well-founded, as we can bound the smoothness of a commonly-used scoring function for Bayesian network structure learning. We show here that, compared to an identical search strategy using the network's exact scores, our proxy-based search is able to get equivalent or better scores on a number of data sets in a fraction of the time.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {411–418},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042628,
author = {Bowling, Michael and Zinkevich, Martin},
title = {On Local Regret},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Online learning aims to perform nearly as well as the best hypothesis in hindsight. For some hypothesis classes, though, even finding the best hypothesis offline is challenging. In such offline cases, local search techniques are often employed and only local optimality guaranteed. For online decision-making with such hypothesis classes, we introduce local regret, a generalization of regret that aims to perform nearly as well as only nearby hypotheses. We then present a general algorithm to minimize local regret with arbitrary locality graphs. We also show how the graph structure can be exploited to drastically speed learning. These algorithms are then demonstrated on a diverse set of online problems: online disjunct learning, online Max-SAT, and online decision tree learning.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {403–410},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042627,
author = {Palla, Konstantina and Knowles, David A. and Ghahramani, Zoubin},
title = {An Infinite Latent Attribute Model for Network Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Latent variable models for network data extract a summary of the relational structure underlying an observed network. The simplest possible models subdivide nodes of the network into clusters; the probability of a link between any two nodes then depends only on their cluster assignment. Currently available models can be classified by whether clusters are disjoint or are allowed to overlap. These models can explain a "flat" clustering structure. Hierarchical Bayesian models provide a natural approach to capture more complex dependencies. We propose a model in which objects are characterised by a latent feature vector. Each feature is itself partitioned into disjoint groups (subclusters), corresponding to a second layer of hierarchy. In experimental comparisons, the model achieves significantly improved predictive performance on social and biological link prediction tasks. The results indicate that models with a single layer hierarchy over-simplify real networks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {395–402},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042626,
author = {Gu, Haijie and Lafferty, John},
title = {Sequential Nonparametric Regression},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {387–394},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042625,
author = {Zhang, Yi and Schneider, Jeff},
title = {Maximum Margin Output Coding},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding.To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization.In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {379–386},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042624,
author = {Lou, Xinghua and Hamprecht, Fred A.},
title = {Structured Learning from Partial Annotations},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Structured learning is appropriate when predicting structured outputs such as trees, graphs, or sequences. Most prior work requires the training set to consist of complete trees, graphs or sequences. Specifying such detailed ground truth can be tedious or infeasible for large outputs. Our main contribution is a large margin formulation that makes structured learning from only partially annotated data possible. The resulting optimization problem is non-convex, yet can be efficiently solve by concave-convex procedure (CCCP) with novel speedup strategies. We apply our method to a challenging tracking-by-assignment problem of a variable number of divisible objects. On this benchmark, using only 25% of a full annotation we achieve a performance comparable to a model learned with a full annotation. Finally, we offer a unifying perspective of previous work using the hinge, ramp, or max loss for structured learning, followed by an empirical comparison on their practical performance.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {371–378},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042623,
author = {Han, Shaobo and Liao, Xuejun and Carin, Lawrence},
title = {Cross-Domain Multitask Learning with Latent Probit Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks. We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model (LPM) to jointly learn the domain transforms, and a probit classifier shared in the common domain. To learn meaningful task relatedness and avoid over-fitting in classification, we introduce sparsity in the domain transforms matrices, as well as in the common classifier parameters. We derive theoretical bounds for the estimation error of the classifier parameters in terms of the sparsity of domain transform matrices. An expectation-maximization algorithm is derived for learning the LPM. The effectiveness of the approach is demonstrated on several real datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {363–370},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042622,
author = {Scherrer, Chad and Halappanavar, Mahantesh and Tewari, Ambuj and Haglin, David},
title = {Scaling up Coordinate Descent Algorithms for Large l<sub>1</sub> Regularization Problems},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a generic framework for parallel coordinate descent (CD) algorithms that includes, as special cases, the original sequential algorithms Cyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm. We introduce two novel parallel algorithms that are also special cases--Thread-Greedy CD and Coloring-Based CD--and give performance measurements for an OpenMP implementation of these.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {355–362},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042621,
author = {Geist, Matthieu and Scherrer, Bruno and Lazaric, Alessandro and Ghavamzadeh, Mohammad},
title = {A Dantzig Selector Approach to Temporal Difference Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, l1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are well-suited for high-dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed-point problem, its integration with l1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {347–354},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042620,
author = {Rakotomamonjy, Alain},
title = {Sparse Support Vector Infinite Push},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we address the problem of embedded feature selection for ranking on top of the list problems. We pose this problem as a regularized empirical risk minimization with p-norm push loss function (p = ∞) and sparsity inducing regularizers. We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer. Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator. Experimental results on toy, DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {339–346},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042619,
author = {Shan, Hanhuai and Kattge, Jens and Reich, Peter B. and Banerjee, Arindam and Schrodt, Franziska and Reichstein, Markus},
title = {Gap Filling in the Plant Kingdom: Trait Prediction Using Hierarchical Probabilistic Matrix Factorization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Plant traits are a key to understanding and predicting the adaptation of ecosystems to environmental changes, which motivates the TRY project aiming at constructing a global database for plant traits and becoming a standard resource for the ecological community. Despite its unprecedented coverage, a large percentage of missing data substantially constrains joint trait analysis. Meanwhile, the trait data is characterized by the hierarchical phylogenetic structure of the plant kingdom. While factorization based matrix completion techniques have been widely used to address the missing data problem, traditional matrix factorization methods are unable to leverage the phylogenetic structure. We propose hierarchical probabilistic matrix factorization (HPMF), which effectively uses hierarchical phylogenetic information for trait prediction. We demonstrate HPMF's high accuracy, effectiveness of incorporating hierarchical structure and ability to capture trait correlation through experiments.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {331–338},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042618,
author = {Avron, Haim and Kale, Satyen and Kasiviswanathan, Shiva Prasad and Sindhwani, Vikas},
title = {Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization. Unlike existing approaches, our method executes very cheap iterations by combining low-rank stochastic subgradients with efficient incremental SVD updates, made possible by highly optimized and parallelizable dense linear algebra operations on small matrices. Our practical algorithms always maintain a low-rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings. Empirical comparisons confirm that our approach is highly competitive with several recently proposed state-of-the-art solvers for such problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {323–330},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042617,
author = {Azimi, Javad and Jalali, Ali and Fern, Xiaoli Z.},
title = {Hybrid Batch Bayesian Optimization},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Bayesian Optimization (BO) aims at optimizing an unknown function that is costly to evaluate. We focus on applications where concurrent function evaluations are possible. In such cases, BO could choose to either sequentially evaluate the function (sequential mode) or evaluate the function with multiple inputs at once (batch mode). The sequential mode generally leads to better optimization performance as each function evaluation is selected with more information, whereas the batch mode is more time efficient (smaller number of iterations). Our goal is to combine the strength of both settings. We systematically analyze BO using a Gaussian Process as the posterior estimator and provide a hybrid algorithm that dynamically switches between sequential and batch with variable batch sizes. We theoretically justify our algorithm and present experimental results on eight benchmark BO problems. The results show that our method achieves substantial speedup (up to 78%) compared to sequential, without suffering any significant performance loss.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {315–322},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042616,
author = {Azimi, Javad and Fern, Alan and Fern, Xiaoli Z. and Borradaile, Glencora and Heeringa, Brent},
title = {Batch Active Learning via Coordinated Matching},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a novel batch active learning method that leverages the availability of high-quality and efficient sequential active-learning policies by approximating their behavior when applied for k steps. Specifically, our algorithm uses Monte-Carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over k steps. The algorithm then selects k examples that best matches this distribution, leading to a combinatorial optimization problem that we term "bounded coordinated matching". While we show this problem is NP-hard, we give an efficient greedy solution, which inherits approximation bounds from supermodular minimization theory. Experiments on eight benchmark datasets show that the proposed approach is highly effective.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {307–314},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042615,
author = {Yger, Florian and Berar, Maxime and Gasso, Gilles and Rakotomamonjy, Alain},
title = {Adaptive Canonical Correlation Analysis Based on Matrix Manifolds},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we formulate the Canonical Correlation Analysis (CCA) problem on matrix manifolds. This framework provides a natural way for dealing with matrix constraints and tools for building efficient algorithms even in an adaptive setting. Finally, an adaptive CCA algorithm is proposed and applied to a change detection problem in EEG signals.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {299–306},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042614,
author = {Kriege, Nils and Mutzel, Petra},
title = {Subgraph Matching Kernels for Attributed Graphs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose graph kernels based on subgraph matchings, i.e. structure-preserving bijections between subgraphs. While recently proposed kernels based on common subgraphs (Wale et al., 2008; Shervashidze et al., 2009) in general can not be applied to attributed graphs, our approach allows to rate mappings of subgraphs by a exible scoring scheme comparing vertex and edge attributes by kernels. We show that subgraph matching kernels generalize several known kernels. To compute the kernel we propose a graph-theoretical algorithm inspired by a classical relation between common subgraphs of two graphs and cliques in their product graph observed by Levi (1973). Encouraging experimental results on a classification task of real-world graphs are presented.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {291–298},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042613,
author = {Balasubramanian, Krishnakumar and Lebanon, Guy},
title = {The Landmark Selection Method for Multiple Output Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Conditional modeling x → y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x → yL and (ii) yL → y. Composing these two models, we obtain a conditional model x → y that possesses convenient statistical properties. Multilabel classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {283–290},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042612,
author = {Rey, M\'{e}lanie and Roth, Volker},
title = {Copula Mixture Model for Dependency-Seeking Clustering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {275–282},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042611,
author = {Krishnamurthy, Akshay and Balakrishnan, Sivaraman and Xu, Min and Singh, Aarti},
title = {Efficient Active Algorithms for Hierarchical Clustering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets, while storage and processing power continue to lag behind. This motivates the need for algorithms that are efficient, both in terms of the number of measurements needed and running time. To combat the challenges associated with large datasets, we propose a general framework for active hierarchical clustering that repeatedly runs an off-the-shelf clustering algorithm on small subsets of the data and comes with guarantees on performance, measurement complexity and runtime complexity. We instantiate this framework with a simple spectral clustering algorithm and provide concrete results on its performance, showing that, under some assumptions, this algorithm recovers all clusters of size Ω(log n) using O(n log2 n) similarities and runs in O(n log3 n) time for a dataset of n objects. Through extensive experimentation we also demonstrate that this framework is practically alluring.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {267–274},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042610,
author = {Eban, Elad and Birnbaum, Aharon and Shalev-Shwartz, Shai and Globerson, Amir},
title = {Learning the Experts for Online Sequence Prediction},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of r experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of log(r). But, without firm prior knowledge on the problem, it is not clear how to choose a small set of good experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {259–266},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042609,
author = {Haider, Peter and Scheffer, Tobias},
title = {Finding Botnets Using Minimal Graph Clusterings},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the problem of identifying botnets and the IP addresses which they comprise, based on the observation of a fraction of the global email spam traffic. Observed mailing campaigns constitute evidence for joint botnet membership, they are represented by cliques in the graph of all messages. No evidence against an association of nodes is ever available. We reduce the problem of identifying botnets to a problem of finding a minimal clustering of the graph of messages. We directly model the distribution of clusterings given the input graph; this avoids potential errors caused by distributional assumptions of a generative model. We report on a case study in which we evaluate the model by its ability to predict the spam campaign that a given IP address is going to participate in.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {251–258},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042608,
author = {Reid, Mark D. and Williamson, Robert C. and Sun, Peng},
title = {The Convexity and Design of Composite Multiclass Losses},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider composite loss functions for multiclass prediction comprising a proper (i.e., Fisher-consistent) loss over probability distributions and an inverse link function. We establish conditions for their (strong) convexity and explore the implications. We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {243–250},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042607,
author = {Gershman, Samuel J. and Hoffman, Matthew D. and Blei, David M.},
title = {Nonparametric Variational Inference},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using multiple kernels allows the approximation to capture multiple modes of the posterior. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and MCMC approximations. The method is easy to apply to graphical models for which standard variational methods are difficult to derive.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {235–242},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042606,
author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
title = {PAC Subset Selection in Stochastic Multi-Armed Bandits},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This "subset selection" problem finds application in a variety of areas. In the authors' previous work (Kalyanakrishnan &amp; Stone, 2010), this problem is framed under a PAC setting (denoted "Explore-m"), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algorithms, in this paper, we design and analyze an algorithm ("LUCB") with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the well-known UCB algorithm for regret minimization. The expected sample complexity bound we show for LUCB is novel even for single-arm selection (Explore-1). We also give a lower bound on the worst case sample complexity of PAC algorithms for Explore-m.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {227–234},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042605,
author = {Sprechmann, Pablo and Bronstein, Alex and Sapiro, Guillermo},
title = {Learning Efficient Structured Sparse Models},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes. For this purpose, we propose an efficient feed forward architecture derived from the iteration of the block-coordinate algorithm. This architecture approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods. We also show that by using different training objective functions, the proposed learnable sparse encoders are not only restricted to be approximants of the exact sparse code for a pre-given dictionary, but can be rather used as full-featured sparse encoders or even modelers. A simple implementation shows several orders of magnitude speedup compared to the state-of-the-art exact optimization algorithms at minimal performance degradation, making the proposed framework suitable for real time and large-scale applications.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {219–226},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042604,
author = {Quadrianto, Novi and Chen, Chao and Lampert, Christoph H.},
title = {The Most Persistent Soft-Clique in a Set of Sampled Graphs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {When searching for characteristic subpatterns in potentially noisy graph data, it appears self-evident that having multiple observations would be better than having just one. However, it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge. In this work we address this challenge for the problem of finding maximum weighted cliques.We introduce the concept of most persistent soft-clique. This is subset of vertices, that 1) is almost fully or at least densely connected, 2) occurs in all or almost all graph instances, and 3) has the maximum weight. We present a measure of clique-ness, that essentially counts the number of edge missing to make a subset of vertices into a clique. With this measure, we show that the problem of finding the most persistent soft-clique problem can be cast either as: a) a max-min two person game optimization problem, or b) a min-min soft margin optimization problem. Both formulations lead to the same solution when using a partial Lagrangian method to solve the optimization problems. By experiments on synthetic data and on real social network data we show that the proposed method is able to reliably find soft cliques in graph data, even if that is distorted by random noise or unreliable observations.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {211–218},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042603,
author = {Mnih, Volodymyr and Hinton, Geoffrey},
title = {Learning to Label Aerial Images from Noisy Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {203–210},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042602,
author = {Parrish, Nathan and Gupta, Maya R.},
title = {Dimensionality Reduction by Local Discriminative Gaussians},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-of-the-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {195–202},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042601,
author = {Kumar, M. Pawan and Packer, Ben and Koller, Daphne},
title = {Modeling Latent Variable Uncertainty for Loss-Based Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider the problem of parameter estimation using weakly supervised datasets, where a training sample consists of the input and a partially specified annotation, which we refer to as the output. The missing information in the annotation is modeled using latent variables. Previous methods overburden a single distribution with two separate tasks: (i) modeling the uncertainty in the latent variables during training; and (ii) making accurate predictions for the output and the latent variables during testing. We propose a novel framework that separates the demands of the two tasks using two distributions: (i) a conditional distribution to model the uncertainty of the latent variables for a given input-output pair; and (ii) a delta distribution to predict the output and the latent variables for a given input. During learning, we encourage agreement between the two distributions by minimizing a loss-based dissimilarity coefficient. Our approach generalizes latent svm in two important ways: (i) it models the uncertainty over latent variables instead of relying on a pointwise estimate; and (ii) it allows the use of loss functions that depend on latent variables, which greatly increases its applicability. We demonstrate the efficacy of our approach on two challenging problems--object detection and action detection--using publicly available datasets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {187–194},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042600,
author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
title = {Off-Policy Actor-Critic},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in off-policy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {179–186},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042599,
author = {Crammer, Koby and Chechik, Gal},
title = {Adaptive Regularization for Weight Matrices},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Algorithms for learning distributions over weight-vectors, such as AROW (Crammer et al., 2009) were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as n4 with the dimension n of the matrix, and n tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {171–178},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042598,
author = {Wang, Yu-Xiang and Xu, Huan},
title = {Stability of Matrix Factorization for Collaborative Filtering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {163–170},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042597,
author = {Li, Ruijiang and Li, Bin and Zhang, Ke and Jin, Cheng and Xue, Xiangyang},
title = {Groupwise Constrained Reconstruction for Subspace Clustering},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Reconstruction based subspace clustering methods compute a self reconstruction matrix over the samples and use it for spectral clustering to obtain the final clustering result. Their success largely relies on the assumption that the underlying subspaces are independent, which, however, does not always hold in the applications with increasing number of subspaces. In this paper, we propose a novel reconstruction based subspace clustering model without making the subspace independence assumption. In our model, certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators, and the affinity matrix used for spectral clustering can be directly built from the posterior of the latent cluster indicators instead of the reconstruction matrix. Experimental results on both synthetic and realworld datasets show that the proposed model can outperform the state-of-the-art methods.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {155–162},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042596,
author = {Hannah, Lauren A. and Dunson, David B.},
title = {Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods (Hannah and Dunson, 2011; Magnani and Boyd, 2009) are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. We empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {147–154},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042595,
author = {Charlin, Laurent and Zemel, Richard and Boutilier, Craig},
title = {Active Learning for Matching Problems},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Effective learning of user preferences is critical to easing user burden in various types of matching problems. Equally important is active query selection to further reduce the amount of preference information users must provide. We address the problem of active learning of user preferences for matching problems, introducing a novel method for determining probabilistic matchings, and developing several new active learning strategies that are sensitive to the specific matching objective. Experiments with real-world data sets spanning diverse domains demonstrate that matching-sensitive active learning outperforms standard techniques.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {139–146},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042594,
author = {Zhang, Kai and Lan, Liang and Liu, Jun and Rauber, Andreas and Moerchen, Fabian},
title = {Inductive Kernel Low-Rank Decomposition with Priors: A Generalized Nystr\"{o}m Method},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work "transductively", i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an "inductive"-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\"{o}m method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposition. Empirical results demonstrate the efficacy and efficiency of the proposed method.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {131–138},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042593,
author = {Yang, Tianbao and Mahdavi, Mehrdad and Jin, Rong and Zhang, Lijun and Zhou, Yang},
title = {Multiple Kernel Learning from Noisy Labels by Stochastic Programming},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We study the problem of multiple kernel learning from noisy labels. This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples. Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments. We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem, and presenting a minimax formulation. We develop an efficient algorithm for solving the related convex-concave optimization problem with a fast convergence rate of O(1/T) where T is the number of iterations. Empirical studies on UCI data sets verify both the effectiveness and the efficiency of the proposed algorithm.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {123–130},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042592,
author = {Zanzotto, Fabio Massimo and Dell'Arciprete, Lorenzo},
title = {Distributed Tree Kernels},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {115–122},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042591,
author = {Wang, Jialei and Zhao, Peilin and Hoi, Steven C. H.},
title = {Exact Soft Confidence-Weighted Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we propose a new Soft Confidence-Weighted (SCW) online learning scheme, which enables the conventional confidence-weighted learning method to handle non-separable cases. Unlike the previous confidence-weighted learning algorithms, the proposed soft confidence-weighted learning method enjoys all the four salient properties: (i) large margin training, (ii) confidence weighting, (iii) capability to handle non-separable data, and (iv) adaptive margin. Our experimental results show that the proposed SCW algorithms significantly outperform the original CW algorithm. When comparing with a variety of state-of-the-art algorithms (including AROW, NAROW and NHERD), we found that SCW generally achieves better or at least comparable predictive accuracy, but enjoys significant advantage of computational efficiency (i.e., smaller number of updates and lower time cost).},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {107–114},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042590,
author = {Dundar, Murat and Akova, Feri and Qi, Yuan and Rajwa, Bartek},
title = {Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling. A Dirichlet process prior (DPP) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution. In an attempt to automatically discover potentially interesting class formations, the prior model is coupled with a suitably chosen data model, and sequential Monte Carlo sampling is used to perform online inference. Our research is driven by a biodetection application, where a new class of pathogen may suddenly appear, and the rapid increase in the number of samples originating from this class indicates the onset of an outbreak.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {99–106},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042589,
author = {G\"{o}nen, Mehmet},
title = {Bayesian Efficient Multiple Kernel Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources. Most of the previous research on such methods is focused on the computational efficiency issue. However, it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity. We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation, which allows us to combine hundreds or thousands of kernels very efficiently. We briefly explain how the proposed method can be extended for multiclass learning and semi-supervised learning. Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast, requiring less than a minute. On one bioinformatics and three image recognition data sets, our method outperforms previously reported results with better generalization performance.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {91–98},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042588,
author = {Ben-David, Shai and Loker, David and Srebro, Nathan and Sridharan, Karthik},
title = {Minimizing the Misclassification Error Rate Using a Surrogate Convex Loss},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassification error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {83–90},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042587,
author = {Fox, Roy and Tishby, Naftali},
title = {Bounded Planning in Passive POMDPs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {75–82},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042586,
author = {Dhillon, Paramveer S. and Rodu, Jordan and Foster, Dean P. and Ungar, Lyle H.},
title = {Two Step CCA: A New Spectral Method for Estimating Vector Models of Words},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank "dictionary" by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {67–74},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042585,
author = {Shivaswamy, Pannaga and Joachims, Thorsten},
title = {Online Structured Prediction via Coactive Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved - but not necessarily optimal - object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have O(1/√T) average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {59–66},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042584,
author = {Richard, Emile and Savalle, Pierre-Andr\'{e} and Vayatis, Nicolas},
title = {Estimation of Simultaneously Sparse and Low Rank Matrices},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low-rank at the same time. Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block-diagonal in the appropriate basis. We introduce a convex mixed penalty which involves l1-norm and trace norm simultaneously. We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix. We bound generalization error in the link prediction problem. We also develop proximal descent strategies to solve the optimization problem efficiently and evaluate performance on synthetic and real data sets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {51–58},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042583,
author = {Kalakrishnan, Mrinal and Righetti, Ludovic and Pastor, Peter and Schaal, Stefan},
title = {Learning Force Control Policies for Compliant Robotic Manipulation},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {49–50},
numpages = {2},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042582,
author = {He, Junfeng and Kumar, Sanjiv and Chang, Shih-Fu},
title = {On the Difficulty of Nearest Neighbor Search},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Fast approximate nearest neighbor(NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042581,
author = {Boots, Byron and Gordon, Geoffrey J.},
title = {Two-Manifold Problems with Applications to Nonlinear System Identification},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recently, there has been much interest in spectral approaches to learning manifolds-- so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {33–40},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042580,
author = {Gong, Dian and Zhao, Xuemei and Medioni, G\'{e}rard},
title = {Robust Multiple Manifolds Structure Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present a robust multiple manifolds structure learning (RMMSL) scheme to robustly estimate data structures under the multiple low intrinsic dimensional manifolds assumption. In the local learning stage, RMMSL efficiently estimates local tangent space by weighted low-rank matrix factorization. In the global learning stage, we propose a robust manifold clustering method based on local structure learning results. The proposed clustering method is designed to get the flattest manifolds clusters by introducing a novel curved-level similarity function. Our approach is evaluated and compared to state-of-the-art methods on synthetic data, handwritten digit images, human motion capture data and motorbike videos. We demonstrate the effectiveness of the proposed approach, which yields higher clustering accuracy, and produces promising results for challenging tasks of human motion segmentation and motion flow learning from videos.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {25–32},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042579,
author = {Liu, Chao and Wang, Yi-Min},
title = {TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper revisits the problem of analyzing multiple ratings given by different judges. Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic insights into our in-house well-trained judges. We generalize the well-known DAWIDSKENE model (Dawid &amp; Skene, 1979) to a spectrum of probabilistic models under the same "TrueLabel + Confusion" paradigm, and show that our proposed hierarchical Bayesian model, called HYBRIDCONFUSION, consistently outperforms DAWIDSKENE on both synthetic and real-world data sets.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {17–24},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042578,
author = {Bischof, Jonathan M. and Airoldi, Edoardo M.},
title = {Summarizing Topical Content with Word Frequency and Exclusivity},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the differential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of differential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {9–16},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042577,
author = {Malisiewicz, Tomasz and Shrivastava, Abhinav and Gupta, Abhinav and Efros, Alexei A.},
title = {Exemplar-SVMs for Visual Object Detection, Label Transfer and Image Retrieval},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Today's state-of-the-art visual object detection systems are based on three key components: 1) sophisticated features (to encode various visual invariances), 2) a powerful classifier (to build a discriminative object class model), and 3) lots of data (to use in large-scale hard-negative mining). While conventional wisdom tends to attribute the success of such methods to the ability of the classifier to generalize across the positive class instances, here we report on empirical findings suggesting that this might not necessarily be the case. We have experimented with a very simple idea: to learn a separate classifier for each positive object instance in the dataset (see Figure 1). In this setup, no generalization across the positive instances is possible by definition, and yet, surprisingly, we did not observe any drastic drop in performance compared to the standard, category-based approaches.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {7–8},
numpages = {2},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042576,
author = {Chambers, Nathanael and Jurafsky, Dan},
title = {Learning the Central Events and Participants in Unlabeled Text},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The majority of information on the Internet is expressed in written text. Understanding and extracting this information is crucial to building intelligent systems that can organize this knowledge. Today, most algorithms focus on learning atomic facts and relations. For instance, we can reliably extract facts like "Annapolis is a City" by observing redundant word patterns across a corpus. However, these facts do not capture richer knowledge like the way detonating a bomb is related to destroying a building, or that the perpetrator who was convicted must have been arrested. A structured model of these events and entities is needed for a deeper understanding of language. This talk describes unsupervised approaches to learning such rich knowledge.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {5–6},
numpages = {2},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042575,
author = {Kumar, Ranjitha and Talton, Jerry O. and Ahmad, Salman and Klemmer, Scott R.},
title = {Data-Driven Web Design},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This short paper summarizes challenges and opportunities of applying machine learning methods to Web design problems, and describes how structured prediction, deep learning, and probabilistic program induction can enable useful interactions for designers. We intend for these techniques to foster new work in data-driven Web design.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {3–4},
numpages = {2},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042573.3042574,
author = {Yu, Dong and Seide, Frank and Li, Gang},
title = {Conversational Speech Transcription Using Context-Dependent Deep Neural Networks},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, combine the classic artificial-neural-network HMMs with traditional context-dependent acoustic modeling and deep-belief-network pre-training. CD-DNN-HMMs greatly outperform conventional CD-GMM (Gaussian mixture model) HMMs: The word error rate is reduced by up to one third on the difficult benchmarking task of speaker-independent single-pass transcription of telephone conversations.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {1–2},
numpages = {2},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@proceedings{10.5555/3042573,
title = {ICML'12: Proceedings of the 29th International Coference on International Conference on Machine Learning},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
location = {Edinburgh, Scotland}
}

