@inproceedings{10.5555/3305890.3306115,
author = {Zoghi, Masrour and Tunys, Tomas and Ghavamzadeh, Mohammad and Kveton, Branislav and Szepesvari, Csaba and Wen, Zheng},
title = {Online Learning to Rank in Stochastic Click Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Online learning to rank is a core problem in information retrieval and machine learning. Many provably efficient algorithms have been recently proposed for this problem in specific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their impact on practice is limited, because all proposed algorithms are designed for specific click models and lack convergence guarantees in other models. In this work, we propose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encompasses two most fundamental click models, the cascade and position-based models. We derive a gap-dependent upper bound on the T-step regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outperforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4199–4208},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306114,
author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn\'{\i}k, Jan and Schmidhuber, J\"{u}rgen},
title = {Recurrent Highway Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with "deep" transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4189–4198},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306113,
author = {Zhu, Rongda and Wang, Lingxiao and Zhai, Chengxiang and Gu, Quanquan},
title = {High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a generic stochastic expectation-maximization (EM) algorithm for the estimation of high-dimensional latent variable models. At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the Q-function in the EM algorithm. Under a mild condition on the initialization, our algorithm is guaranteed to attain a linear convergence rate to the unknown parameter of the latent variable model, and achieve an optimal statistical rate up to a logarithmic factor for parameter estimation. Compared with existing high-dimensional EM algorithms, our algorithm enjoys a better computational complexity and is therefore more efficient. We apply our generic algorithm to two illustrative latent variable models: Gaussian mixture model and mixture of linear regression, and demonstrate the advantages of our algorithm by both theoretical analysis and numerical experiments. We believe that the proposed semi-stochastic gradient is of independent interest for general nonconvex optimization problems with bivariate structures.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4180–4188},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306112,
author = {Zhou, Hao Henry and Zhang, Yilin and Ithapu, Vamsi K. and Johnson, Sterling C. and University of Wisconsin-Madison, Grace Wahba and Singh, Vikas},
title = {When Can Multi-Site Datasets Be Pooled for Regression? Hypothesis Tests, ℓ<sub>2</sub>-Consistency and Neuroscience Applications},
year = {2017},
publisher = {JMLR.org},
abstract = {Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints. Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups. While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) - independent of the inference algorithms we use. In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression. We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens. With a focus on Alzheimer's disease studies, we present empirical results showing that in regimes suggested by our analysis, pooling a local dataset with data from an international study improves power.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4170–4179},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306111,
author = {Zhou, Yichi and Li, Jialian and Zhu, Jun},
title = {Identify the Nash Equilibrium in Static Games with Random Payoffs},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem on how to learn the pure Nash Equilibrium of a two-player zero-sum static game with random payoffs under unknown distributions via efficient payoff queries. We introduce a multi-armed bandit model to this problem due to its ability to find the best arm efficiently among random arms and propose two algorithms for this problem—LUCB-G based on the confidence bounds and a racing algorithm based on successive action elimination. We provide an analysis on the sample complexity lower bound when the Nash Equilibrium exists.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4160–4169},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306110,
author = {Zhou, Chaoxu and Gao, Wenbo and Goldfarb, Donald},
title = {Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a novel class of stochastic, adaptive methods for minimizing self-concordant functions which can be expressed as an expected value. These methods generate an estimate of the true objective function by taking the empirical mean over a sample drawn at each step, making the problem tractable. The use of adaptive step sizes eliminates the need for the user to supply a step size. Methods in this class include extensions of gradient descent (GD) and BFGS. We show that, given a suitable amount of sampling, the stochastic adaptive GD attains linear convergence in expectation, and with further sampling, the stochastic adaptive BFGS attains R-superlinear convergence. We present experiments showing that these methods compare favorably to SGD.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4150–4159},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306109,
author = {Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L. and Dhillon, Inderjit S.},
title = {Recovery Guarantees for One-Hidden-Layer Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs). We distill some properties of activation functions that lead to local strong convexity in the neighborhood of the ground-truth parameters for the 1NN squared-loss objective and most popular nonlinear activation functions satisfy the distilled properties, including rectified linear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also smooth, we show local linear convergence guarantees of gradient descent under a resampling rule. For homogeneous activations, we show tensor methods are able to initialize the parameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the ground truth with sample complexity d · log(1/ε) · poly(k, λ) and computational complexity n · d · poly(k, λ) for smooth homogeneous activations with high probability, where d is the dimension of the input, k (k ≤ d) is the number of hidden nodes, λ is a conditioning property of the ground-truth parameter matrix between the input layer and the hidden layer, ε is the targeted precision and n is the number of samples. To the best of our knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample complexity and computational complexity linear in the input dimension and logarithmic in the precision.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4140–4149},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306108,
author = {Zheng, Kai and Mou, Wenlong and Wang, Liwei},
title = {Collect at Once, Use Effectively: Making Non-Interactive Locally Private Learning Possible},
year = {2017},
publisher = {JMLR.org},
abstract = {Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev expansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under ℓ2-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under non-interactive LDP model.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4130–4139},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306107,
author = {Zheng, Shuxin and Meng, Qi and Wang, Taifeng and Chen, Wei and Yu, Nenghai and Ma, Zhi-Ming and Liu, Tie-Yan},
title = {Asynchronous Stochastic Gradient Descent with Delay Compensation},
year = {2017},
publisher = {JMLR.org},
abstract = {With the fast development of deep learning, it has become common to learn big neural networks using massive training data. Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulfill this task for its efficiency, which is, however, known to suffer from the problem of delayed gradients. That is, when a local worker adds its gradient to the global model, the global model may have been updated by other workers and this gradient becomes "delayed". We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is achieved by leveraging Taylor expansion of the gradient function and efficient approximation to the Hessian matrix of the loss function. We call the new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and the experimental results demonstrate that DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4120–4129},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306106,
author = {Zheng, Shuai and Kwok, James T.},
title = {Follow the Moving Leader in Deep Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep networks are highly nonlinear and difficult to optimize. During training, the parameter iterate may move from one local basin to another, or the data distribution may even change. Inspired by the close connection between stochastic optimization and online learning, we propose a variant of the follow the regularized leader (FTRL) algorithm called follow the moving leader (FTML). Unlike the FTRL family of algorithms, the recent samples are weighted more heavily in each iteration and so FTML can adapt more quickly to changes. We show that FTML enjoys the nice properties of RMSprop and Adam, while avoiding their pitfalls. Experimental results on a number of deep learning models and tasks demonstrate that FTML converges quickly, and outperforms other state-of-the-art optimizers.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4110–4119},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306105,
author = {Zhao, Mingmin and Yue, Shichao and Katabi, Dina and Jaakkola, Tommi S. and Bianchi, Matt T.},
title = {Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture},
year = {2017},
publisher = {JMLR.org},
abstract = {We focus on predicting sleep stages from radio measurements without any attached sensors on subjects. We introduce a new predictive model that combines convolutional and recurrent neural networks to extract sleep-specific subject-invariant features from RF signals and capture the temporal progression of sleep. A key innovation underlying our approach is a modified adversarial training regime that discards extraneous information specific to individuals or measurement conditions, while retaining all information relevant to the predictive task. We analyze our game theoretic setup and empirically demonstrate that our model achieves significant improvements over state-of-the-art solutions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4100–4109},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306104,
author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
title = {Learning Hierarchical Features from Deep Generative Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with some existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that does not suffer from these limitations. Our model is able to learn highly in-terpretable and disentangled hierarchical features on several natural image datasets with no task-specific regularization.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4091–4099},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306103,
author = {Zhao, Liang and Liao, Siyu and Wang, Yanzhi and Li, Zhe and Tang, Jian and Yuan, Bo},
title = {Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank},
year = {2017},
publisher = {JMLR.org},
abstract = {Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks. Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy. We formally study LDR matrices in deep learning. First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure. Finally, we propose back-propagation based training algorithm for general LDR neural networks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4082–4090},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306102,
author = {Zhao, He and Du, Lan and Buntine, Wray},
title = {Leveraging Node Attributes for Incomplete Relational Data},
year = {2017},
publisher = {JMLR.org},
abstract = {Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction. This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes. Extensive experiments show that our models achieve the state-of-the-art link prediction results, especially with highly incomplete relational data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4072–4081},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306101,
author = {Zhang, Teng and Zhou, Zhi-Hua},
title = {Multi-Class Optimal Margin Distribution Machine},
year = {2017},
publisher = {JMLR.org},
abstract = {Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the first-and second-order statistics can achieve superior performance. It still remains open for multi-class classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult. In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4063–4071},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306100,
author = {Zhang, Wenpeng and Zhao, Peilin and Zhu, Wenwu and Hoi, Steven C. H. and Zhang, Tong},
title = {Projection-Free Distributed Online Learning in Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efficiency in handling large-scale machine learning problems. However, none of existing studies has explored it in the distributed online learning setting, where locally light computation is assumed. In this paper, we fill this gap by proposing the distributed online conditional gradient algorithm, which eschews the expensive projection operation needed in its counterpart algorithms by exploiting much simpler linear optimization steps. We give a regret bound for the proposed algorithm as a function of the network size and topology, which will be smaller on smaller graphs or "well-connected" graphs. Experiments on two large-scale real-world datasets for a multiclass classification task confirm the computational benefit of the proposed algorithm and also verify the theoretical regret bound.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4054–4062},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306099,
author = {Zhang, Yuchen and Liang, Percy and Wainwright, Martin J.},
title = {Convexified Convolutional Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented in terms of a low-rank matrix, and the rank constraint can be relaxed so as to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, we find that CC-NNs achieve competitive or better performance than CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4044–4053},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306098,
author = {Zhang, Hantian and Li, Jerry and Kara, Kaan and Alistarh, Dan and Liu, Ji and Zhang, Ce},
title = {ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Recently there has been significant interest in training machine-learning models at low precision: by reducing precision, one can reduce computation and communication by one order of magnitude. We examine training at reduced precision, both from a theoretical and practical perspective, and ask: is it possible to train models at end-to-end low precision with provable guarantees? Can this lead to consistent order-of-magnitude speedups? We mainly focus on linear models, and the answer is yes for linear models. We develop a simple framework called ZipML based on one simple but novel strategy called double sampling. Our ZipML framework is able to execute training at low precision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of applications, and show that it enables an FPGA prototype that is up to 6.5x faster than an implementation using full 32-bit precision. We further develop a variance-optimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When applied to linear models together with double sampling, we save up to another 1.7x in data movement compared with uniform quantization. When training deep networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4035–4043},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306097,
author = {Zhang, Chenzi and Hu, Shuguang and Tang, Zhihao Gavin and Chan, T-H. Hubert},
title = {Re-Revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method},
year = {2017},
publisher = {JMLR.org},
abstract = {We revisit semi-supervised learning on hypergraphs. Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable. We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution. Moreover, we give a much simpler approach for solving the convex program based on the subgradient method. Our experiments on real-world datasets confirm that our confidence interval approach on hyper-graphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4026–4034},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306096,
author = {Zhang, Weizhong and Hong, Bin and Liu, Wei and Ye, Jieping and Cai, Deng and He, Xiaofei and Wang, Jie},
title = {Scaling up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction},
year = {2017},
publisher = {JMLR.org},
abstract = {Sparse support vector machine (SVM) is a popular classification technique that can simultaneously learn a small set of the most interpretable features and identify the support vectors. It has achieved great successes in many real-world applications. However, for large-scale problems involving a huge number of samples and extremely high-dimensional features, solving sparse SVM-s remains challenging. By noting that sparse SVMs induce sparsities in both feature and sample spaces, we propose a novel approach, which is based on accurate estimations of the primal and dual optima of sparse SVMs, to simultaneously identify the features and samples that are guaranteed to be irrelevant to the outputs. Thus, we can remove the identified inactive samples and features from the training phase, leading to substantial savings in both the memory usage and computational cost without sacrificing accuracy. To the best of our knowledge, the proposed method is the first static feature and sample reduction method for sparse SVM. Experiments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samples and 30 million features) demonstrate that our approach significantly outperforms state-of-the-art methods and the speedup gained by our approach can be orders of magnitude.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4016–4025},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306095,
author = {Zhang, Yizhe and Gan, Zhe and Fan, Kai and Chen, Zhi and Henao, Ricardo and Shen, Dinghan and Carin, Lawrence},
title = {Adversarial Feature Matching for Text Generation},
year = {2017},
publisher = {JMLR.org},
abstract = {The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long short-term memory network as generator, and a con-volutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {4006–4015},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306094,
author = {Zhang, Yizhe and Chen, Changyou and Gan, Zhe and Henao, Ricardo and Carin, Lawrence},
title = {Stochastic Gradient Monomial Gamma Sampler},
year = {2017},
publisher = {JMLR.org},
abstract = {Recent advances in stochastic gradient techniques have made it possible to estimate posterior distributions from large datasets via Markov Chain Monte Carlo (MCMC). However, when the target posterior is multimodal, mixing performance is often poor. This results in inadequate exploration of the posterior distribution. A framework is proposed to improve the sampling efficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A generalized kinetic function is leveraged, delivering superior stationary mixing, especially for multimodal distributions. Techniques are also discussed to overcome the practical issues introduced by this generalization. It is shown that the proposed approach is better at exploring complex multimodal posterior distributions, as demonstrated on multiple applications and in comparison with other stochastic gradient MCMC methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3996–4005},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306093,
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
title = {Continual Learning through Synaptic Intelligence},
year = {2017},
publisher = {JMLR.org},
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3987–3995},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306092,
author = {Zaheer, Manzil and Kottur, Satwik and Ahmed, Amr and Moura, Jos\'{e} and Smola, Alex},
title = {Canopy — Fast Sampling with Cover Trees},
year = {2017},
publisher = {JMLR.org},
abstract = {Hierarchical Bayesian models often capture distributions over a very large number of distinct atoms. The need for these models arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets that can be exploited to organize abundant unlabeled images. Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to approximate approaches. In this work, we propose Canopy, a sampler based on Cover Trees that is exact, has guaranteed runtime logarithmic in the number of atoms, and is provably polynomial in the inherent dimensionality of the underlying parameter space. In other words, the algorithm is as fast as search over a hierarchical data structure. We provide theory for Canopy and demonstrate its effectiveness on both synthetic and real datasets, consisting of over 100 million images.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3977–3986},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306091,
author = {Zaheer, Manzil and Ahmed, Amr and Smola, Alexander J},
title = {Latent LSTM Allocation Joint Clustering and Non-Linear Dynamic Modeling of Sequential Data},
year = {2017},
publisher = {JMLR.org},
abstract = {Recurrent neural networks, such as long-short term memory (LSTM) networks, are powerful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korpusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across different user types, LSTMs require a large number of parameters, notwithstanding the simplicity of the underlying dynamics, rendering it un-interpretable, which is highly undesirable in user modeling. The increase in complexity and parameters arises due to a large action space in which many of the actions have similar intent or topic. In this paper, we introduce Latent LSTM Allocation (LLA) for user modeling combining hierarchical Bayesian models with LSTMs. In LLA, each user is modeled as a sequence of actions, and the model jointly groups actions into topics and learns the temporal dynamics over the topic sequence, instead of action space directly. This leads to a model that is highly interpretable, concise, and can capture intricate dynamics. We present an efficient Stochastic EM inference algorithm for our model that scales to millions of users/documents. Our experimental evaluations show that the proposed model compares favorably with several state-of-the-art baselines.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3967–3976},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306090,
author = {Yoon, Jaehong and Hwang, Sung Ju},
title = {Combined Group and Exclusive Sparsity for Deep Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {The number of parameters in a deep neural network is usually very large, which helps with its learning capacity but also hinders its scalability and practicality due to memory/time inefficiency and overfitting. To resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the network to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the network. Specifically, we propose to use an exclusive sparsity regularization based on (1,2)-norm, which promotes competition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sparsity with the group sparsity based on (2,1)-norm, to promote both sharing and competition for features in training of a deep neural network. We validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3958–3966},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306089,
author = {Yen, Ian E.H. and Lee, Wei-Cheng and Chang, Sung-En and Suggala, Arun S. and Lin, Shou-De and Ravikumar, Pradeep},
title = {Latent Feature Lasso},
year = {2017},
publisher = {JMLR.org},
abstract = {The latent feature model (LFM), proposed in (Griffiths &amp; Ghahramani, 2005), but possibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of latent features. Thus, each instance has an associated latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on non-parametric LFMs, with priors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this complexity either still have computational complexity that is exponential, or sample complexity that is high-order polynomial w.r.t. the number of latent features. In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the data distribution.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3949–3957},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306088,
author = {Ye, Jianbo and Wang, James Z. and Li, Jia},
title = {A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization},
year = {2017},
publisher = {JMLR.org},
abstract = {Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research topic for gaining insights from a large set of structured objects. Despite being conceptually simple, WLM problems are computationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to compute. In this paper, we introduce a stochastic approach based on simulated annealing for solving WLMs. Particularly, we have developed a Gibbs sampler to approximate effectively and efficiently the partial gradients of a sequence of Wasserstein losses. Our new approach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM problems that often require multiple levels of iterations in which the oracle for computing the value and gradient of a loss function is embedded. We applied the method to optimal transport with Coulomb cost and the Wasserstein non-negative matrix factorization problem, and made comparisons with the existing method of entropy regularization.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3940–3948},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306087,
author = {Ye, Haishan and Luo, Luo and Zhang, Zhihua},
title = {Approximate Newton Methods and Their Local Convergence},
year = {2017},
publisher = {JMLR.org},
abstract = {Many machine learning models are reformulated as optimization problems. Thus, it is important to solve a large-scale optimization problem in big data applications. Recently, stochastic second order methods have emerged to attract much attention for optimization due to their efficiency at each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each iteration while commanding a high convergence rate. However, the convergence properties of these methods are still not well understood. There are also several important gaps between the current convergence theory and the performance in real applications. In this paper, we aim to fill these gaps. We propose a unifying framework to analyze local convergence properties of second order methods. Based on this framework, our theoretical analysis matches the performance in real applications.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3931–3939},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306086,
author = {Yang, Hongyu and Rudin, Cynthia and Seltzer, Margo},
title = {Scalable Bayesian Rule Lists},
year = {2017},
publisher = {JMLR.org},
abstract = {We present an algorithm for building probabilistic rule lists that is two orders of magnitude faster than previous work. Rule list algorithms are competitors for decision tree algorithms. They are associative classifiers, in that they are built from pre-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy splitting and pruning like decision tree algorithms, we aim to fully optimize over rule lists, striking a practical balance between accuracy, interpretability, and computational speed. The algorithm presented here uses a mixture of theoretical bounds (tight enough to have practical implications as a screening or bounding procedure), computational reuse, and highly tuned language libraries to achieve computational efficiency. Currently, for many practical problems, this method achieves better accuracy and sparsity than decision trees, with practical running times. The predictions in each leaf are probabilistic.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3921–3930},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306085,
author = {Yang, Eunho and Lozano, Aur\'{e}lie C.},
title = {Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity},
year = {2017},
publisher = {JMLR.org},
abstract = {Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity. Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions. We also study non-convex counterparts of sparse + group-sparse models. Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3911–3920},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306084,
author = {Yang, Tianbao and Lin, Qihang and Zhang, Lijun},
title = {A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper focuses on convex constrained optimization problems, where the solution is subject to a convex inequality constraint. In particular, we aim at challenging problems for which both projection into the constrained domain and a linear optimization under the inequality constraint are time-consuming, which render both projected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) expensive. In this paper, we develop projection reduced optimization algorithms for both smooth and non-smooth optimization with improved convergence rates under a certain regularity condition of the constraint function. We first present a general theory of optimization with only one projection. Its application to smooth optimization with only one projection yields O(1/ε) iteration complexity, which improves over the O(1/ε2) iteration complexity established before for non-smooth optimization and can be further reduced under strong convexity. Then we introduce a local error bound condition and develop faster algorithms for non-strongly convex optimization at the price of a logarithmic number of projections. In particular, we achieve an iteration complexity of \~{O}(1/ε2(1-θ)) for non-smooth optimization and \~{O}(1/ε1-θ) for smooth optimization, where θ ∈ (0,1] appearing the local error bound condition characterizes the functional local growth rate around the optimal solutions. Novel applications in solving the constrained ℓ1 minimization problem and a positive semi-definite constrained distance metric learning problem demonstrate that the proposed algorithms achieve significant speed-up compared with previous algorithms.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3901–3910},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306083,
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
title = {Tensor-Train Recurrent Neural Networks for Video Classification},
year = {2017},
publisher = {JMLR.org},
abstract = {The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3891–3900},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306082,
author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
title = {Improved Variational Autoencoders for Text Modeling Using Dilated Convolutions},
year = {2017},
publisher = {JMLR.org},
abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3881–3890},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306081,
author = {Yang, Haichuan and Gui, Shupeng and Ke, Chuyang and Stefankovic, Daniel and Fujimaki, Ryohei and Liu, Ji},
title = {On the Projection Operator to a Three-View Cardinality Constrained Set},
year = {2017},
publisher = {JMLR.org},
abstract = {The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for example, sparse learning, feature selection, and compressed sensing. To solve a cardinality constrained problem, the key challenge is to solve the projection onto the cardinality constraint set, which is NP-hard in general when there exist multiple overlapped cardinality constraints. In this paper, we consider the scenario where the overlapped cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many applications, such as identification of gene regulatory networks and task-worker assignment problem. We cast the projection into a linear programming, and show that for TVCS, the vertex solution of this linear programming is the solution for the original projection problem. We further prove that such solution can be found with the complexity proportional to the number of variables and constraints. We finally use synthetic experiments and two interesting applications in bioinformatics and crowdsourcing to validate the proposed TVCS model and method.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3871–3880},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306080,
author = {Yang, Bo and Fu, Xiao and Sidiropoulos, Nicholas D. and Hong, Mingyi},
title = {Towards K-Means-Friendly Spaces: Simultaneous Deep Learning and Clustering},
year = {2017},
publisher = {JMLR.org},
abstract = {Most learning approaches treat dimensionality reduction (DR) and clustering separately (i.e., sequentially), but recent research has shown that optimizing the two tasks jointly can substantially improve the performance of both. The premise behind the latter genre is that the data samples are obtained via linear transformation of latent representations that are easy to cluster; but in practice, the transformation from the latent space to the data can be more complicated. In this work, we assume that this transformation is an unknown and possibly nonlinear function. To recover the 'clustering-friendly' latent representations and to better cluster the data, we propose a joint DR and K-means clustering approach in which DR is accomplished via learning a deep neural network (DNN). The motivation is to keep the advantages of jointly optimizing the two tasks, while exploiting the deep neural network's ability to approximate any nonlinear function. This way, the proposed approach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint optimization criterion, and propose an effective and scalable algorithm to handle the formulated optimization problem. Experiments using different real datasets are employed to showcase the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3861–3870},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306079,
author = {Yang, Zhuoran and Balasubramanian, Krishnakumar and Liu, Han},
title = {High-Dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation},
year = {2017},
publisher = {JMLR.org},
abstract = {We consider estimating the parametric component of single index models in high dimensions. Compared with existing work, we do not require the covariate to be normally distributed. Utilizing Stein's Lemma, we propose estimators based on the score function of the covariate. Moreover, to handle score function and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators. Extensive numerical experiments are provided to back up our theory.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3851–3860},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306078,
author = {Xu, Zheng and Taylor, Gavin and Li, Hao and Figueiredo, M\'{a}rio A. T. and Yuan, Xiaoming and Goldstein, Tom},
title = {Adaptive Consensus ADMM for Distributed Optimization},
year = {2017},
publisher = {JMLR.org},
abstract = {The alternating direction method of multipliers (ADMM) is commonly used for distributed model fitting problems, but its performance and reliability depend strongly on user-defined penalty parameters. We study distributed ADMM methods that boost performance by using different fine-tuned algorithm parameters on each worker node. We present a O(1/k) convergence rate for adaptive ADMM methods with node-specific parameters, and propose adaptive consensus ADMM (ACADMM), which automatically tunes parameters without user oversight.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3841–3850},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306077,
author = {Xu, Hongteng and Luo, Dixin and Zha, Hongyuan},
title = {Learning Hawkes Processes from Short Doubly-Censored Event Sequences},
year = {2017},
publisher = {JMLR.org},
abstract = {Many real-world applications require robust algorithms to learn point processes based on a type of incomplete data — the so-called short doubly-censored (SDC) event sequences. We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis. Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method, sampling predecessors and successors for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both time-invariant and time-varying Hawkes processes.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3831–3840},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306076,
author = {Xu, Yi and Lin, Qihang and Yang, Tianbao},
title = {Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, a new theory is developed for first-order stochastic convex optimization, showing that the global convergence rate is sufficiently quantified by a local growth rate of the objective function in a neighborhood of the optimal solutions. In particular, if the objective function F(w) in the ε-sublevel set grows as fast as ||w - w*1/θ9, where w* represents the closest optimal solution to w and θ ∈ (0,1] quantifies the local growth rate, the iteration complexity of first-order stochastic optimization for achieving an e-optimal solution can be \~{O}(1/ε2(1-θ)), which is optimal at most up to a logarithmic factor. To achieve the faster global convergence, we develop two different accelerated stochastic subgradient methods by iteratively solving the original problem approximately in a local region around a historical solution with the size of the local region gradually decreasing as the solution approaches the optimal set. Besides the theoretical improvements, this work also include new contributions towards making the proposed algorithms practical: (i) we present practical variants of accelerated stochastic subgradient methods that can run without the knowledge of multiplicative growth constant and even the growth rate θ; (ii) we consider a broad family of problems in machine learning to demonstrate that the proposed algorithms enjoy faster convergence than traditional stochastic subgradient method. For example, when applied to the ℓ1 regularized empirical polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3821–3830},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306075,
author = {Xie, Pengtao and Singh, Aarti and Xing, Eric P.},
title = {Uncorrelation and Evenness: A New Diversity-Promoting Regularizer},
year = {2017},
publisher = {JMLR.org},
abstract = {Latent space models (LSMs) provide a principled and effective way to extract hidden patterns from observed data. To cope with two challenges in LSMs: (1) how to capture infrequent patterns when pattern frequency is imbalanced and (2) how to reduce model size without sacrificing their expressiveness, several studies have been proposed to "diversify" LSMs, which design regularizers to encourage the components therein to be "diverse". In light of the limitations of existing approaches, we design a new diversity-promoting regularizer by considering two factors: uncorrelation and evenness, which encourage the components to be uncorrelated and to play equally important roles in modeling data. Formally, this amounts to encouraging the co-variance matrix of the components to have more uniform eigenvalues. We apply the regularizer to two LSMs and develop an efficient optimization algorithm. Experiments on healthcare, image and text data demonstrate the effectiveness of the regularizer.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3811–3820},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306074,
author = {Xie, Pengtao and Deng, Yuntian and Zhou, Yi and Kumar, Abhimanu and Yu, Yaoliang and Zou, James and Xing, Eric P.},
title = {Learning Latent Space Models with Angular Constraints},
year = {2017},
publisher = {JMLR.org},
abstract = {The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile renders LSMs to be prone to overfitting. Several recent studies investigate a new type of regularization approach, which encourages components in LSMs to be diverse, for the sake of alleviating overfitting. While they have shown promising empirical effectiveness, in theory why larger "diversity" results in less overfitting is still unclear. To bridge this gap, we propose a new diversity-promoting approach that is both theoretically analyzable and empirically effective. Specifically, we use near-orthogonality to characterize "diversity" and impose angular constraints (ACs) on the components of LSMs to promote diversity. A generalization error analysis shows that larger diversity results in smaller estimation error and larger approximation error. An efficient ADMM algorithm is developed to solve the constrained LSM problems. Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversity-promoting approaches.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3799–3810},
numpages = {12},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306073,
author = {Xia, Yingce and Qin, Tao and Chen, Wei and Bian, Jiang and Yu, Nenghai and Liu, Tie-Yan},
title = {Dual Supervised Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach dual supervised learning. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3789–3798},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306072,
author = {Wu, Xi-Zhu and Zhou, Zhi-Hua},
title = {A Unified View of Multi-Label Performance Measures},
year = {2017},
publisher = {JMLR.org},
abstract = {Multi-label classification deals with the problem where each instance is associated with multiple class labels. Because evaluation in multi-label classification is more complicated than single-label setting, a number of performance measures have been proposed. It is noticed that an algorithm usually performs differently on different measures. Therefore, it is important to understand which algorithms perform well on which measure(s) and why. In this paper, we propose a unified margin view to revisit eleven performance measures in multi-label classification. In particular, we define label-wise margin and instance-wise margin, and prove that through maximizing these margins, different corresponding performance measures are to be optimized. Based on the defined margins, a max-margin approach called LIMO is designed and empirical results validate our theoretical findings.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3780–3788},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306071,
author = {Wrigley, Andrew and Lee, Wee Sun and Ye, Nan},
title = {Tensor Belief Propagation},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm. Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors. Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling. Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable. We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3771–3779},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306070,
author = {Winner, Kevin and Sujono, Debora and Sheldon, Dan},
title = {Exact Inference for Integer Latent-Variable Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Graphical models with latent count variables arise in a number of areas. However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables. Winner &amp; Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models. However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models. In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference. This substantially generalizes the class of models for which efficient, exact inference algorithms are available. Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data. Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3761–3770},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306069,
author = {Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W. and Colmenarejo, Sergio G\'{o}mez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha},
title = {Learned Optimizers That Scale and Generalize},
year = {2017},
publisher = {JMLR.org},
abstract = {Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3751–3760},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306068,
author = {White, Martha},
title = {Unifying Task Specification in Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3742–3750},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306067,
author = {Wen, Tsung-Hsien and Miao, Yishu and Blunsom, Phil and Young, Steve},
title = {Latent Intention Dialogue Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3732–3741},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306066,
author = {Wei, Pengfei and Sagarna, Ramon and Ke, Yiping and Ong, Yew-Soon and Goh, Chi-Keong},
title = {Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression},
year = {2017},
publisher = {JMLR.org},
abstract = {A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities. In this paper, we study different approaches based on Gaussian process models to solve the multi-source transfer regression problem. Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain. We theoretically show that using such a transfer co-variance function for general Gaussian process modelling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance. This leads us to propose TCMSStack, an integrated strategy incorporating the benefits of the transfer covariance function and stacking. Extensive experiments on one synthetic and two real-world datasets, with learning settings of up to 11 sources for the latter, demonstrate the effectiveness of our proposed TCMSStack.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3722–3731},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306065,
author = {Wang, Lingxiao and Zhang, Xiao and Gu, Quanquan},
title = {A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the (near) optimal sample complexity and statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3712–3721},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306064,
author = {Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
title = {Beyond Filters: Compact Feature Map for Portable Deep Model},
year = {2017},
publisher = {JMLR.org},
abstract = {Convolutional neural networks (CNNs) have shown extraordinary performance in a number of applications, but they are usually of heavy design for the accuracy reason. Beyond compressing the filters in CNNs, this paper focuses on the redundancy in the feature maps derived from the large number of filters in a layer. We propose to extract intrinsic representation of the feature maps and preserve the discriminability of the features. Circulant matrix is employed to formulate the feature map transformation, which only requires O(d log d) computation complexity to embed a d-dimensional feature map. The filter is then reconfigured to establish the mapping from original input to the new compact feature map, and the resulting network can preserve intrinsic information of the original network with significantly fewer parameters, which not only decreases the online memory for launching CNN but also accelerates the computation speed. Experiments on benchmark image datasets demonstrate the superiority of the proposed algorithm over state-of-the-art methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3703–3711},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306063,
author = {Wang, Jialei and Xiao, Lin},
title = {Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms},
year = {2017},
publisher = {JMLR.org},
abstract = {We consider empirical risk minimization of linear predictors with convex loss functions. Such problems can be reformulated as convex-concave saddle point problems and solved by primal-dual first-order algorithms. However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution. In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization. We also present dual-free variants of adaptive primal-dual algorithms that do not need the dual proximal mapping, which are especially suitable for logistic regression.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3694–3702},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306062,
author = {Wang, Yichen and Williams, Grady and Theodorou, Evangelos and Song, Le},
title = {Variational Policy for Guiding Point Processes},
year = {2017},
publisher = {JMLR.org},
abstract = {Temporal point processes have been widely applied to model event sequence data generated by online users. In this paper, we consider the problem of how to design the optimal control policy for point processes, such that the stochastic system driven by the point process is steered to a target state. In particular, we exploit the key insight to view the stochastic optimal control problem from the perspective of optimal measure and variational inference. We further propose a convex optimization framework and an efficient algorithm to update the policy adaptively to the current system state. Experiments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately and efficiently than other stochastic control methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3684–3693},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306061,
author = {Wang, Chong and Wang, Yining and Huang, Po-Sen and Mohamed, Abdelrahman and Zhou, Dengyong and Deng, Li},
title = {Sequence Modeling via Segmentations},
year = {2017},
publisher = {JMLR.org},
abstract = {Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3674–3683},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306060,
author = {Wang, Po-An and Lu, Chi-Jen},
title = {Tensor Decomposition via Simultaneous Power Iteration},
year = {2017},
publisher = {JMLR.org},
abstract = {Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem is the tensor power method. However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices. In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure. This allows us to achieve a better running time in the batch setting, as well as a lower sample complexity in the streaming setting.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3665–3673},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306059,
author = {Wang, Zi and Li, Chengtao and Jegelka, Stefanie and Kohli, Pushmeet},
title = {Batched High-Dimensional Bayesian Optimization via Structural Kernel Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Optimization of high-dimensional black-box functions is an extremely challenging problem. While Bayesian optimization has emerged as a popular approach for optimizing black-box functions, its applicability has been limited to low-dimensional problems due to its computational and statistical challenges arising from high-dimensional settings. In this paper, we propose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it properly for more efficient and effective BO, and (2) performing multiple evaluations in parallel to reduce the number of iterations required by the method. Our novel approach learns the latent structure with Gibbs sampling and constructs batched queries using determinantal point processes. Experimental validations on both synthetic and real-world functions demonstrate that the proposed method outperforms the existing state-of-the-art approaches.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3656–3664},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306058,
author = {Wang, Yixin and Kucukelbir, Alp and Blei, David M.},
title = {Robust Probabilistic Modeling with Bayesian Data Reweighting},
year = {2017},
publisher = {JMLR.org},
abstract = {Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protection against mismatch between a model's assumptions and reality. We propose a way to systematically detect and mitigate mismatch of a large class of probabilistic models. The idea is to raise the likelihood of each observation to a weight and then to infer both the latent variables and the weights from data. Inferring the weights allows a model to identify observations that match its assumptions and down-weight others. This enables robust inference and improves predictive accuracy. We study four different forms of mismatch with reality, ranging from missing latent groups to structure misspecification. A Poisson factorization analysis of the Movielens 1M dataset shows the benefits of this approach in a practical scenario.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3646–3655},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306057,
author = {Wang, Jialei and Kolar, Mladen and Srebro, Nathan and Zhang, Tong},
title = {Efficient Distributed Learning with Sparsity},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted ℓ1 regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3636–3645},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306056,
author = {Wang, Zi and Jegelka, Stefanie},
title = {Max-Value Entropy Search for Efficient Bayesian Optimization},
year = {2017},
publisher = {JMLR.org},
abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the arg max of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3627–3635},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306055,
author = {Wang, Lingxiao and Gu, Quanquan},
title = {Robust Gaussian Graphical Model Estimation with Arbitrary Corruption},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of estimating the high-dimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a robust estimator for the sparse precision matrix in the high- dimensional regime. At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In particular, we show that provided that the number of corrupted samples n2 for each variable satisfies n2 ≲ √n/√log d, where n is the sample size and d is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3617–3626},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306054,
author = {Wang, Shusen and Gittens, Alex and Mahoney, Michael W.},
title = {Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging},
year = {2017},
publisher = {JMLR.org},
abstract = {We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the "mass" in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3608–3616},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306053,
author = {Wang, Di and Fountoulakis, Kimon and Henzinger, Monika and Mahoney, Michael W. and Rao, Satish},
title = {Capacity Releasing Diffusion for Speed and Locality},
year = {2017},
publisher = {JMLR.org},
abstract = {Diffusions and related random walk procedures are of central importance in many areas of machine learning, data analysis, and applied mathematics. Because they spread mass agnostically at each step in an iterative manner, they can sometimes spread mass "too aggressively," thereby failing to find the "right" clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical spectral diffusion process. As an application, we use our CRD Process to develop an improved local algorithm for graph clustering. Our local graph clustering method can find local clusters in a model of clustering where one begins the CRD Process in a cluster whose vertices are connected better internally than externally by an O(log2 n) factor, where n is the number of nodes in the cluster. Thus, our CRD Process is the first local graph clustering algorithm that is not subject to the well-known quadratic Cheeger barrier. Our result requires a certain smoothness condition, which we expect to be an artifact of our analysis. Our empirical evaluation demonstrates improved results, in particular for realistic social graphs where there are moderately good—but not very good—clusters.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3598–3607},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306052,
author = {Wang, Yu-Xiang and Agarwal, Alekh and Dud\'{\i}k, Miroslav},
title = {Optimal and Adaptive Off-Policy Evaluation in Contextual Bandits},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the off-policy evaluation problem— estimating the value of a target policy using data collected by another policy—under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched up to constants by the inverse propensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is suboptimal. We then propose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of data sets, often outperforming prior work by orders of magnitude.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3589–3597},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306051,
author = {Walder, Christian J. and Bishop, Adrian N.},
title = {Fast Bayesian Intensity Estimation for the Permanental Process},
year = {2017},
publisher = {JMLR.org},
abstract = {The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process. In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process. In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood. We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3579–3588},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306050,
author = {Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
title = {On Orthogonality and Learning Recurrent Networks with Long Term Dependencies},
year = {2017},
publisher = {JMLR.org},
abstract = {It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during back-propagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3570–3578},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306049,
author = {Villegas, Ruben and Yang, Jimei and Zou, Yuliang and Sohn, Sungryull and Lin, Xunyu and Lee, Honglak},
title = {Learning to Generate Long-Term Future via Hierarchical Prediction},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy-based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human 3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3560–3569},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306048,
author = {Villacampa-Calvo, Carlos and Hernandez-Lobato, Daniel},
title = {Scalable Multi-Class Gaussian Process Classification Using Expectation Propagation},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper describes an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this type of training is used, the computational cost does not depend on the number of data instances N. Furthermore, extra assumptions in the approximate inference process make the memory cost independent of N. The consequence is that the proposed EP method can be used on datasets with millions of instances. We compare empirically this method with alternative approaches that approximate the required computations using variational inference. The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood. Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3550–3559},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306047,
author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
title = {FeUdal Networks for Hierarchical Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels - allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits - in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3540–3549},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306046,
author = {Vaswani, Sharan and Kveton, Branislav and Wen, Zheng and Ghavamzadeh, Mohammad and Lakshmanan, Laks V.S. and Schmidt, Mark},
title = {Model-Independent Online Learning for Influence Maximization},
year = {2017},
publisher = {JMLR.org},
abstract = {We consider influence maximization (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of "seed" users to expose the product to. While prior work assumes a known model of information diffusion, we propose a novel parametrization that not only makes our framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent analysis shows that our regret bound has a better (as compared to previous work) dependence on the size of the network. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3530–3539},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306045,
author = {Valera, Isabel and Ghahramani, Zoubin},
title = {Automatic Discovery of the Statistical Types of Variables in a Dataset},
year = {2017},
publisher = {JMLR.org},
abstract = {A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumption becomes too restrictive. Data are often heterogeneous, complex, and improperly or incompletely documented. Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset. In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3521–3529},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306044,
author = {Urschel, John and Brunel, Victor-Emmanuel and Moitra, Ankur and Rigollet, Philippe},
title = {Learning Determinantal Point Processes with Moments and Cycles},
year = {2017},
publisher = {JMLR.org},
abstract = {Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important. While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP. Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity. Finally, we give experimental results that confirm our theoretical findings.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3511–3520},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306043,
author = {Umlauft, Jonas and Hirche, Sandra},
title = {Learning Stable Stochastic Nonlinear Dynamical Systems},
year = {2017},
publisher = {JMLR.org},
abstract = {A data-driven identification of dynamical systems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from first principles in physics. However, meta-knowledge on the system's behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement generation. Therefore, this paper proposes a framework for learning stable stochastic systems from data. We focus on identifying a state-dependent coefficient form of the nonlinear stochastic model which is globally asymptotically stable according to probabilistic Lyapunov methods. We compare our approach to other state of the art methods on real-world datasets in terms of flexibility and stability.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3502–3510},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306042,
author = {Ubaru, Shashanka and Mazumdar, Arya},
title = {Multilabel Classification with Group Testing and Codes},
year = {2017},
publisher = {JMLR.org},
abstract = {In recent years, the multiclass and mutlilabel classification problems we encounter in many applications have very large (103 - 106) number of classes. However, each instance belongs to only one or few classes, i.e., the label vectors are sparse. In this work, we propose a novel approach based on group testing to solve such large multilabel classification problems with sparse label vectors. We describe various group testing constructions, and advocate the use of concatenated Reed Solomon codes and unbalanced bipartite expander graphs for extreme classification problems. The proposed approach has several advantages theoretically and practically over existing popular methods. Our method operates on the binary alphabet and can utilize the well-established binary classifiers for learning. The error correction capabilities of the codes are leveraged for the first time in the learning problem to correct prediction errors. Even if a linearly growing number of classifiers mis-classify, these errors are fully corrected. We establish Hamming loss error bounds for the approach. More importantly, our method utilizes a simple prediction algorithm and does not require matrix inversion or solving optimization problems making the algorithm very inexpensive. Numerical experiments with various datasets illustrate the superior performance of our method.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3492–3501},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306041,
author = {Tu, Stephen and Venkataraman, Shivaram and Wilson, Ashia C. and Gittens, Alex and Jordan, Michael I. and Recht, Benjamin},
title = {Breaking Locality Accelerates Block Gauss-Seidel},
year = {2017},
publisher = {JMLR.org},
abstract = {Recent work by Nesterov and Stich (2016) showed that momentum can be used to accelerate the rate of convergence for block Gauss-Seidel in the setting where a fixed partitioning of the coordinates is chosen ahead of time. We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any fixed partitioning. Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix sub-blocks are well-conditioned. Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3482–3491},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306040,
author = {Tsakiris, Manolis C. and Vidal, Ren\'{e}},
title = {Hyperplane Clustering via Dual Principal Component Pursuit},
year = {2017},
publisher = {JMLR.org},
abstract = {State-of-the-art methods for clustering data drawn from a union of subspaces are based on sparse and low-rank representation theory and convex optimization algorithms. Existing results guaranteeing the correctness of such methods require the dimension of the subspaces to be small relative to the dimension of the ambient space. When this assumption is violated, as is, e.g., in the case of hyperplanes, existing methods are either computationally too intensive (e.g., algebraic methods) or lack sufficient theoretical support (e.g., K-Hyperplanes or RANSAC). In this paper we provide theoretical and algorithmic contributions to the problem of clustering data from a union of hyperplanes, by extending a recent subspace learning method called Dual Principal Component Pursuit (DPCP) to the multi-hyperplane case. We give theoretical guarantees under which, the non-convex ℓ1 problem associated with DPCP admits a unique global minimizer equal to the normal vector of the most dominant hyperplane. Inspired by this insight, we propose sequential (RANSAC-style) and iterative (K-Hyperplanes-style) hyperplane learning DPCP algorithms, which, via experiments on synthetic and real data, are shown to outperform or be competitive to the state-of-the-art.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3472–3481},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306039,
author = {Trivedi, Rakshit and Dai, Hanjun and Wang, Yichen and Song, Le},
title = {Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs},
year = {2017},
publisher = {JMLR.org},
abstract = {The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embed-dings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3462–3471},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306038,
author = {Tripuraneni, Nilesh and Rowland, Mark and Ghahramani, Zoubin and Turner, Richard},
title = {Magnetic Hamiltonian Monte Carlo},
year = {2017},
publisher = {JMLR.org},
abstract = {Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3453–3461},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306037,
author = {Tosh, Christopher and Dasgupta, Sanjoy},
title = {Diameter-Based Active Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3444–3452},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306036,
author = {Tosatto, Samuele and Pirotta, Matteo and D'Eramo, Carlo and Restelli, Marcello},
title = {Boosted Fitted Q-Iteration},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper is about the study of B-FQI, an Approximated Value Iteration (AVI) algorithm that exploits a boosting procedure to estimate the action-value function in reinforcement learning problems. B-FQI is an iterative off-line algorithm that, given a dataset of transitions, builds an approximation of the optimal action-value function by summing the approximations of the Bellman residuals across all iterations. The advantage of such approach w.r.t. to other AVI methods is twofold: (1) while keeping the same function space at each iteration, B-FQI can represent more complex functions by considering an additive model; (2) since the Bellman residual decreases as the optimal value function is approached, regression problems become easier as iterations proceed. We study B-FQI both theoretically, providing also a finite-sample error upper bound for it, and empirically, by comparing its performance to the one of FQI in different domains and using different regression techniques.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3434–3443},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306035,
author = {Tompson, Jonathan and Schlachter, Kristofer and Sprechmann, Pablo and Perlin, Ken},
title = {Accelerating Eulerian Fluid Simulation with Convolutional Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing problem in applied mathematics, for which state-of-the-art methods require large compute resources. In this work, we propose a data-driven approach that leverages the approximation power of deep-learning with the precision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incompressible Euler equations using the standard operator splitting method, in which a large sparse linear system with many free parameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsupervised learning framework to solve the linear system. We present real-time 2D and 3D simulations that outperform recently proposed data-driven methods; the obtained results are realistic and show good generalization properties.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3424–3433},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306034,
author = {Tokui, Seiya and Sato, Issei},
title = {Evaluating the Variance of Likelihood-Ratio Gradient Estimators},
year = {2017},
publisher = {JMLR.org},
abstract = {The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance. Many types of baselines have been proposed, although their degree of optimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases. The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it. It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables. It is derived from the exchange property of the differentiation and integration. To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient. We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3414–3423},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306033,
author = {Tian, Yuandong},
title = {An Analytical Formula of Population Gradient for Two-Layered ReLU Network and Its Applications in Convergence and Critical Point Analysis},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we explore theoretical properties of training a two-layered ReLU network g(x; w) = ΣKj=1 σ(WTjX) with centered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w*. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters ("out-of-plane") are not isolated and form manifolds, and characterize in-plane critical-point-free regions for two ReLU case. On the other hand, convergence to w* for one ReLU node is guaranteed with at least (1 - ε)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O(ε/√d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w* (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3404–3413},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306032,
author = {Thi, Hoai An Le and Le, Hoai Minh and Phan, Duy Nhat and Tran, Bach},
title = {Stochastic DCA for the Large-Sum of Non-Convex Functions Problem and Its Application to Group Variable Selection in Classification},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we present a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of optimization problems whose objective function is a large sum of non-convex functions and a regularization term. We consider the ℓ2,0 regularization to deal with the group variables selection. By exploiting the special structure of the problem, we propose an efficient DC decomposition for which the corresponding stochastic DCA scheme is very inexpensive: it only requires the projection of points onto balls that is explicitly computed. As an application, we applied our algorithm for the group variables selection in multiclass logistic regression. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its superiority over well-known methods, with respect to classification accuracy, sparsity of solution as well as running time.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3394–3403},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306031,
author = {Telgarsky, Matus},
title = {Neural Networks and Rational Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {Neural networks and rational functions efficiently approximate each other. In more detail, it is shown here that for any ReLU network, there exists a rational function of degree O(poly log(1/ε)) which is ε-close, and similarly for any rational function there exists a ReLU network of size O(poly log(1/ε)) which is ε-close. By contrast, polynomials need degree Ω(poly(l/ε)) to approximate even a single ReLU. When converting a ReLU network to a rational function as above, the hidden constants depend exponentially on the number of layers, which is shown to be tight; in other words, a compositional representation can be beneficial even for rational functions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3387–3393},
numpages = {7},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306030,
author = {Tang, Junqi and Golbabaee, Mohammad and Davies, Mike E.},
title = {Gradient Projection Iterative Sketch for Large-Scale Constrained Least-Squares},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a randomized first order optimization algorithm Gradient Projection Iterative Sketch (GPIS) and an accelerated variant for efficiently solving large scale constrained Least Squares (LS). We provide the first theoretical convergence analysis for both algorithms. An efficient implementation using a tailored line-search scheme is also proposed. We demonstrate our methods' computational efficiency compared to the classical accelerated gradient method, and the variance-reduced stochastic gradient methods through numerical experiments in various large synthetic/real data sets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3377–3386},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306029,
author = {Tandon, Rashish and Lei, Qi and Dimakis, Alexandros G. and Karampatziakis, Nikos},
title = {Gradient Coding: Avoiding Stragglers in Distributed Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a novel coding theoretic framework for mitigating stragglers in distributed learning. We show how carefully replicating data blocks and coding across gradients can provide tolerance to failures and stragglers for synchronous Gradient Descent. We implement our schemes in python (using MPI) to run on Amazon EC2, and show how we compare against baseline approaches in running time and generalization error.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3368–3376},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306028,
author = {Tan, Zilong and Mukherjee, Sayan},
title = {Partitioned Tensor Factorizations for Learning Mixed Membership Models},
year = {2017},
publisher = {JMLR.org},
abstract = {We present an efficient algorithm for learning mixed membership models when the number of variables p is much larger than the number of hidden components k. This algorithm reduces the computational complexity of state-of-the-art tensor methods, which require decomposing an O (p3) tensor, to factorizing O(p/k) sub-tensors each of size O (k3). In addition, we address the issue of negative entries in the empirical method of moments based estimators. We provide sufficient conditions under which our approach has provable guarantees. Our approach obtains competitive empirical results on both simulated and real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3358–3367},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306027,
author = {Taieb, Souhaib Ben and Taylor, James W. and Hyndman, Rob J.},
title = {Coherent Probabilistic Forecasts for Hierarchical Time Series},
year = {2017},
publisher = {JMLR.org},
abstract = {Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels — the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3348–3357},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306026,
author = {Suzumura, Shinya and Nakagawa, Kazuya and Umezu, Yuta and Tsuda, Koji and Takeuchi, Ichiro},
title = {Selective Inference for Sparse High-Order Interaction Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Finding statistically significant high-order interactions in predictive modeling is important but challenging task because the possible number of high-order interactions is extremely large (e.g., &gt; 1017). In this paper we study feature selection and statistical inference for sparse high-order interaction models. Our main contribution is to extend recently developed selective inference framework for linear models to high-order interaction models by developing a novel algorithm for efficiently characterizing the selection event for the selective inference of high-order interactions. We demonstrate the effectiveness of the proposed algorithm by applying it to an HIV drug response prediction problem.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3338–3347},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306025,
author = {Suresh, Ananda Theertha and Yu, Felix X. and Kumar, Sanjiv and McMahan, H. Brendan},
title = {Distributed Mean Estimation with Limited Communication},
year = {2017},
publisher = {JMLR.org},
abstract = {Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike previous works, we make no probabilistic assumptions on the data. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of Θ (d/n) and uses a constant number of bits per dimension per client. We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n). We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd's algorithm for k-means and power iteration for PCA.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3329–3337},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306024,
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
title = {Axiomatic Attribution for Deep Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3319–3328},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306023,
author = {Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J. and Boots, Byron and Bagnell, J. Andrew},
title = {Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction},
year = {2017},
publisher = {JMLR.org},
abstract = {Recently, researchers have demonstrated state-of-the-art performance on sequential prediction problems using deep neural networks and Reinforcement Learning (RL). For some of these problems, oracles that can demonstrate good performance may be available during training, but are not used by plain RL methods. To take advantage of this extra information, we propose AggreVaTeD, an extension of the Imitation Learning (IL) approach of Ross &amp; Bagnell (2014). AggreVaTeD allows us to use expressive differentiable policy representations such as deep networks, while leveraging training-time oracles to achieve faster and more accurate solutions with less training data. Specifically, we present two gradient procedures that can learn neural network policies for several problems, including a sequential prediction task and several high-dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates that we can expect up to exponentially-lower sample complexity for learning with AggreVaTeD than with plain RL algorithms. Our results and theory indicate that IL (and AggreVaTeD in particular) can be a more effective strategy for sequential prediction than plain RL.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3309–3318},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306022,
author = {Sun, Xu and Ren, Xuancheng and Ma, Shuming and Wang, Houfeng},
title = {MeProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a simple yet effective technique for neural network learning. The forward propagation is computed as usual. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost. Surprisingly, experimental results demonstrate that we can update only 1-4% of the weights at each back propagation pass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3299–3308},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306021,
author = {Sun, Ke and Nielsen, Frank},
title = {Relative Fisher Information and Natural Gradient for Learning Large Modular Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner's structure turns large and complex. This paper makes a preliminary step towards anew direction. We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system. This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks. We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3289–3298},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306020,
author = {Sun, Wen and Dey, Debadeepta and Kapoor, Ashish},
title = {Safety-Aware Algorithms for Adversarial Contextual Bandit},
year = {2017},
publisher = {JMLR.org},
abstract = {In this work we study the safe sequential decision making problem under the setting of adversarial contextual bandits with sequential risk constraints. At each round, nature prepares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to pull an arm and receives the corresponding cost and risk associated with the pulled arm. In addition to minimizing the cumulative cost, for safety purposes, the learner needs to make safe decisions such that the average of the cumulative risk from all pulled arms should not be larger than a pre-defined threshold. To address this problem, we first study online convex programming in the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develop a meta algorithm leveraging online mirror descent for the full information setting and then extend it to contextual bandit with sequential risk constraints setting using expert advice. Our algorithms can achieve near-optimal regret in terms of minimizing the total cost, while successfully maintaining a sub-linear growth of accumulative risk constraint violation. We support our theoretical results by demonstrating our algorithm on a simple simulated robotics reactive control task.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3280–3288},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306019,
author = {Sugiyama, Mahito and Nakahara, Hiroyuki and Tsuda, Koji},
title = {Tensor Balancing on Statistical Manifold},
year = {2017},
publisher = {JMLR.org},
abstract = {We solve tensor balancing, rescaling an Nth order nonnegative tensor by multiplying N tensors of order N - 1 so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton's method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton's method, can be analytically obtained using the M\"{o}bius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3270–3279},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306018,
author = {Suggala, Arun Sai and Yang, Eunho and Ravikumar, Pradeep},
title = {Ordinal Graphical Models: A Tale of Two Approaches},
year = {2017},
publisher = {JMLR.org},
abstract = {Undirected graphical models or Markov random fields (MRFs) are widely used for modeling multivariate probability distributions. Much of the work on MRFs has focused on continuous variables, and nominal variables (that is, unordered categorical variables). However, data from many real world applications involve ordered categorical variables also known as ordinal variables, e.g., movie ratings on Netflix which can be ordered from 1 to 5 stars. With respect to univariate ordinal distributions, as we detail in the paper, there are two main categories of distributions; while there have been efforts to extend these to multivariate ordinal distributions, the resulting distributions are typically very complex, with either a large number of parameters, or with non-convex likelihoods. While there have been some work on tractable approximations, these do not come with strong statistical guarantees, and moreover are relatively computationally expensive. In this paper, we theoretically investigate two classes of graphical models for ordinal data, corresponding to the two main categories of univariate ordinal distributions. In contrast to previous work, our theoretical developments allow us to provide correspondingly two classes of estimators that are not only computationally efficient but also have strong statistical guarantees.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3260–3269},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306017,
author = {Stich, Sebastian U. and Raj, Anant and Jaggi, Martin},
title = {Approximate Steepest Coordinate Descent},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale optimization. The efficiency of this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor of up to n, the number of coordinates. In many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection. Numerical experiments with Lasso and Ridge regression show promising improvements, in line with our theoretical guarantees.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3251–3259},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306016,
author = {Stan, Serban and Zadimoghaddam, Morteza and Krause, Andreas and Karbasi, Amin},
title = {Probabilistic Submodular Maximization in Sub-Linear Time},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we consider optimizing submodular functions that are drawn from some unknown distribution. This setting arises, e.g., in recommender systems, where the utility of a subset of items may depend on a user-specific submodular utility function. In modern applications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough. As a remedy, we introduce the problem of sublinear time probabilistic submodular maximization: Given training examples of functions (e.g., via user feature vectors), we seek to reduce the ground set so that optimizing new functions drawn from the same distribution will provide almost as much value when restricted to the reduced ground set as when using the full set. We cast this problem as a two-stage sub-modular maximization and develop a novel efficient algorithm for this problem which offers a 1/2 (1 - 1/e2) approximation ratio for general monotone submodular functions and general matroid constraints. We demonstrate the effectiveness of our approach on several real-world applications where running the maximization problem on the reduced ground set leads to two orders of magnitude speed-up while incurring almost no loss.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3241–3250},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306015,
author = {Staib, Matthew and Jegelka, Stefanie},
title = {Robust Budget Allocation via Continuous Submodular Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {The optimal allocation of resources for maximizing influence, spread of information or coverage, has gained attention in the past years, in particular in machine learning and data mining. But in applications, the parameters of the problem are rarely known exactly, and using wrong parameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Bipartite Influence Maximization problem introduced by Alon et al. (2012) from a robust optimization perspective, where an adversary may choose the least favorable parameters within a confidence set. The resulting problem is a nonconvex-concave saddle point problem (or game). We show that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization problem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a problem can be solved to arbitrary precision ε.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3230–3240},
numpages = {11},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306014,
author = {Sivakumar, Vidyashankar and Banerjee, Arindam},
title = {High-Dimensional Structured Quantile Regression},
year = {2017},
publisher = {JMLR.org},
abstract = {Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(·) and consider the norm regularized quantile regression estimator. We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error. While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature. We perform experiments on synthetic data which support the theoretical results.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3220–3229},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306013,
author = {Singh, Shashank and P\'{o}czos, Barnab\'{a}s},
title = {Nonparanormal Information Estimation},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3210–3219},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306012,
author = {\c{S}im\v{S}ekli, Umut},
title = {Fractional Langevin Monte Carlo: Exploring Levy Driven Stochastic Differential Equations for Markov Chain Monte Carlo},
year = {2017},
publisher = {JMLR.org},
abstract = {Along with the recent advances in scalable Markov Chain Monte Carlo methods, sampling techniques that are based on Langevin diffusions have started receiving increasing attention. These so called Langevin Monte Carlo (LMC) methods are based on diffusions driven by a Brownian motion, which gives rise to Gaussian proposal distributions in the resulting algorithms. Even though these approaches have proven successful in many applications, their performance can be limited by the light-tailed nature of the Gaussian proposals. In this study, we extend classical LMC and develop a novel Fractional LMC (FLMC) framework that is based on a family of heavy-tailed distributions, called a-stable Levy distributions. As opposed to classical approaches, the proposed approach can possess large jumps while targeting the correct distribution, which would be beneficial for efficient exploration of the state space. We develop novel computational methods that can scale up to large-scale problems and we provide formal convergence analysis of the proposed scheme. Our experiments support our theory: FLMC can provide superior performance in multi-modal settings, improved convergence rates, and robustness to algorithm parameters.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3200–3209},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306011,
author = {Silver, David and van Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
title = {The Predictron: End-to-End Learning and Planning},
year = {2017},
publisher = {JMLR.org},
abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple "imagined" planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-to-end so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3191–3199},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306010,
author = {Si, Si and Zhang, Huan and Keerthi, S. Sathiya and Mahajan, Dhruv and Dhillon, Inderjit S. and Hsieh, Cho-Jui},
title = {Gradient Boosted Decision Trees for High Dimensional Sparse Output},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we study the gradient boosted decision trees (GBDT) when the output space is high dimensional and sparse. For example, in multilabel classification, the output space is a L-dimensional 0/1 vector, where L is number of labels that can grow to millions and beyond in many modern applications. We show that vanilla GBDT can easily run out of memory or encounter near-forever running time in this regime, and propose a new GBDT variant, GBDT-SPARSE, to resolve this problem by employing L0 regularization. We then discuss in detail how to utilize this sparsity to conduct GBDT training, including splitting the nodes, computing the sparse residual, and predicting in sub-linear time. Finally, we apply our algorithm to extreme multilabel classification problems, and show that the proposed GBDT-SPARSE achieves an order of magnitude improvements in model size and prediction time over existing methods, while yielding similar performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3182–3190},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306009,
author = {Shyam, Pranav and Gupta, Shubham and Dukkipati, Ambedkar},
title = {Attentive Recurrent Comparators},
year = {2017},
publisher = {JMLR.org},
abstract = {Rapid learning requires flexible representations to quickly adopt to new evidence. We develop a novel class of models called Attentive Recurrent Comparators (ARCs) that form representations of objects by cycling through them and making observations. Using the representations extracted by ARCs, we develop a way of approximating a dynamic representation space and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art performance with an error rate of 1.5%. This represents the first super-human result achieved for this task with a generic model that uses only pixel information.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3173–3181},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306008,
author = {Shu, Rui and Bui, Hung H. and Ghavamzadeh, Mohammad},
title = {Bottleneck Conditional Density Estimation},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce a new framework for training deep generative models for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. Crucially, we propose a new hybrid training method that blends the conditional generative model with a joint generative model. Hybrid blending is the key to effective training of the BCDE, which avoids overfitting and provides a novel mechanism for leveraging unlabeled data. We show that our hybrid training procedure enables models to achieve competitive results in the MNIST quadrant prediction task in the fully-supervised setting, and sets new benchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3164–3172},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306007,
author = {Shrivastava, Anshumali},
title = {Optimal Densification for Fast and Accurate Minwise Hashing},
year = {2017},
publisher = {JMLR.org},
abstract = {Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification (Shrivastava &amp; Li, 2014a;c) have shown that it is possible to compute k minwise hashes, of a vector with d nonzeros, in mere (d + k) computations, a significant improvement over the classical O(dk). These advances have led to an algorithmic improvement in the query complexity of traditional indexing algorithms based on minwise hashing. Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly poor accuracy compared to vanilla minwise hashing, especially when the data is sparse. In this paper, we provide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the proposed scheme is variance-optimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques. As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision probability as minwise hashing. Experimental evaluations on real sparse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will replace minwise hashing implementations in practice.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3154–3163},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306006,
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
title = {Learning Important Features through Propagating Activation Differences},
year = {2017},
publisher = {JMLR.org},
abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3145–3153},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306005,
author = {Shi, Tianlin Tim and Karpathy, Andrej and Fan, Linxi Jim and Hernandez, Jonathan and Liang, Percy},
title = {World of Bits: An Open-Domain Platform for Web-Based Agents},
year = {2017},
publisher = {JMLR.org},
abstract = {While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the open-domain realism of tasks in computer vision or natural language processing, which operate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a platform in which agents complete tasks on the Internet by performing low-level keyboard and mouse actions. The two main challenges are: (i) to curate a diverse set of natural web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reproducible despite the transience of the web. To tackle this, we develop a methodology in which crowdworkers create tasks defined by natural language questions and provide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reproducible offline approximation of the website. Finally, we show that agents trained via behavioral cloning and reinforcement learning can complete a range of web-based tasks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3135–3144},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306004,
author = {Shen, Li and Liu, Wei and Yuan, Ganzhao and Ma, Shiqian},
title = {GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics. The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity. In addition, we develop a new technique to establish the global convergence of the GSOS algorithm. To be specific, we first reformulate the iterations of GSOS as a two-step iterations algorithm by employing the tool of operator optimization theory. Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation. At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems. Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3125–3134},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306003,
author = {Shen, Jie and Li, Ping},
title = {On the Iteration Complexity of Support Recovery via Hard Thresholding Pursuit},
year = {2017},
publisher = {JMLR.org},
abstract = {Recovering the support of a sparse signal from its compressed samples has been one of the most important problems in high dimensional statistics. In this paper, we present a novel analysis for the hard thresholding pursuit (HTP) algorithm, showing that it exactly recovers the support of an arbitrary s-sparse signal within O(sk log k) iterations via a properly chosen proxy function, where k is the condition number of the problem. In stark contrast to the theoretical results in the literature, the iteration complexity we obtained holds without assuming the restricted isometry property, or relaxing the sparsity, or utilizing the optimality of the underlying signal. We further extend our result to a more challenging scenario, where the subproblem involved in HTP cannot be solved exactly. We prove that even in this setting, support recovery is possible and the computational complexity of HTP is established. Numerical study substantiates our theoretical results.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3115–3124},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306002,
author = {Sheffet, Or},
title = {Differentially Private Ordinary Least Squares},
year = {2017},
publisher = {JMLR.org},
abstract = {Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives t-values — representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the "Analyze Gauss" algorithm (Dwork et al., 2014).},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3105–3114},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306001,
author = {Sharan, Vatsal and Valiant, Gregory},
title = {Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use},
year = {2017},
publisher = {JMLR.org},
abstract = {The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is efficient and easy to implement, but often converges to poor local optima—particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS for a variety of tasks on synthetic data—including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion—and for computing word embeddings from a third-order word tri-occurrence tensor.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3095–3104},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3306000,
author = {Shamir, Ohad and Szlak, Liran},
title = {Online Learning with Local Permutations and Delayed Feedback},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly permute the order of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner's responses is not crucial, and on the other hand, might allow better learning and regret performance, by mitigating highly adversarial loss sequences. Also, with random permutations, this can be seen as a setting interpolating between adversarial and stochastic losses. In this paper, we consider the applicability of this setting to convex online learning with delayed feedback, in which the feedback on the prediction made in round t arrives with some delay τ. With such delayed feedback, the best possible regret bound is well-known to be O(√τT). We prove that by being able to permute losses by a distance of at most M (for M ≥ τ), the regret can be improved to O(√T(1 + √τ2/M)), using a Mirror-Descent based algorithm which can be applied for both Euclidean and non-Euclidean geometries. We also prove a lower bound, showing that for M &lt; τ/3, it is impossible to improve the standard O(√τT) regret bound by more than constant factors. Finally, we provide some experiments validating the performance of our algorithm.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3086–3094},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305999,
author = {Shalit, Uri and Johansson, Fredrik D. and Sontag, David},
title = {Estimating Individual Treatment Effect: Generalization Bounds and Algorithms},
year = {2017},
publisher = {JMLR.org},
abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3076–3085},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305998,
author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
title = {Failures of Gradient-Based Deep Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3067–3075},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305997,
author = {Sen, Rajat and Shanmugam, Karthikeyan and Dimakis, Alexandres G. and Shakkottai, Sanjay},
title = {Identifying Best Interventions through Online Importance Sampling},
year = {2017},
publisher = {JMLR.org},
abstract = {Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node V in an acyclic causal directed graph, to maximize the expected value of a target node Y (located downstream of V). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with K arms where each arm is a soft intervention at V, and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3057–3066},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305996,
author = {Selsam, Daniel and Liang, Percy and Dill, David L.},
title = {Developing Bug-Free Machine Learning Systems with Formal Mathematics},
year = {2017},
publisher = {JMLR.org},
abstract = {Noisy data, non-convex objectives, model misspecification, and numerical instability can all cause undesired behaviors in machine learning systems. As a result, detecting actual implementation errors can be extremely difficult. We demonstrate a methodology in which developers use an interactive proof assistant to both implement their system and to state a formal theorem defining what it means for their system to be correct. The process of proving this theorem interactively in the proof assistant exposes all implementation errors since any error in the program would cause the proof to fail. As a case study, we implement a new system, Certigrad, for optimizing over stochastic computation graphs, and we generate a formal (i.e. machine-checkable) proof that the gradients sampled by the system are unbiased estimates of the true mathematical gradients. We train a variational autoencoder using Certigrad and find the performance comparable to training the same model in TensorFlow.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3047–3056},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305995,
author = {Schlegel, Matthew and Pan, Yangchen and Chen, Jiecao and White, Martha},
title = {Adapting Kernel Representations Online Using Submodular Maximization},
year = {2017},
publisher = {JMLR.org},
abstract = {Kernel representations provide a nonlinear representation, through similarities to prototypes, but require only simple linear learning algorithms given those prototypes. In a continual learning setting, with a constant stream of observations, it is critical to have an efficient mechanism for sub-selecting prototypes amongst observations. In this work, we develop an approximately sub-modular criterion for this setting, and an efficient online greedy submodular maximization algorithm for optimizing the criterion. We extend streaming submodular maximization algorithms to continual learning, by removing the need for multiple passes—which is infeasible—and instead introducing the idea of coverage time. We propose a general block-diagonal approximation for the greedy update with our criterion, that enables updates linear in the number of prototypes. We empirically demonstrate the effectiveness of this approximation, in terms of approximation quality, significant runtime improvements, and effective prediction performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3037–3046},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305994,
author = {Seaman, Kevin and Bach, Francis and Bubeck, S\'{e}bastien and Lee, Yin Tat and Massouli\'{e}, Laurent},
title = {Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov's accelerated gradient descent is optimal and achieves a precision ε &gt; 0 in time O(√kg(1 + Δτ) ln(l/ε)), where kg is the condition number of the (global) function to optimize, Δ is the diameter of the network, and τ (resp. l) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision ε ε 0 in time O(√kl(1 + τ/√γ)ln(1/ε)), where kl is the condition number of the local functions and γ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3027–3036},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305993,
author = {Saxe, Andrew M. and Earle, Adam C. and Rosman, Benjamin},
title = {Hierarchy through Composition with Multitask LMDPs},
year = {2017},
publisher = {JMLR.org},
abstract = {Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3017–3026},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305992,
author = {Sakr, Charbel and Kim, Yongjune and Shanbhag, Naresh},
title = {Analytical Guarantees on Numerical Precision of Deep Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {The acclaimed successes of neural networks often overshadow their tremendous complexity. We focus on numerical precision - a key parameter defining the complexity of neural networks. First, we present theoretical bounds on the accuracy in presence of limited precision. Interestingly, these bounds can be computed via the back-propagation algorithm. Hence, by combining our theoretical analysis and the back-propagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to time-consuming fixed-point simulations. We provide numerical evidence showing how our approach allows us to maintain high accuracy but with lower complexity than state-of-the-art binary networks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3007–3016},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305991,
author = {Sakai, Tomoya and du Plessis, Marthinus Christoffel and Niu, Gang and Sugiyama, Masashi},
title = {Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data},
year = {2017},
publisher = {JMLR.org},
abstract = {Most of the semi-supervised classification methods developed so far use unlabeled data for regularization purposes under particular distributional assumptions such as the cluster assumption. In contrast, recently developed methods of classification from positive and unlabeled data (PU classification) use unlabeled data for risk evaluation, i.e., label information is directly extracted from unlabeled data. In this paper, we extend PU classification to also incorporate negative data and propose a novel semi-supervised classification approach. We establish generalization error bounds for our novel methods and show that the bounds decrease with respect to the number of unlabeled data without the distributional assumptions that are required in existing semi-supervised classification methods. Through experiments, we demonstrate the usefulness of the proposed methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2998–3006},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305990,
author = {Saito, Kuniaki and Ushiku, Yoshitaka and Harada, Tatsuya},
title = {Asymmetric Tri-Training for Unsupervised Domain Adaptation},
year = {2017},
publisher = {JMLR.org},
abstract = {It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in various domains is expensive. To learn discriminative representations for the target domain, we assume that artificially labeling the target samples can result in a good representation. Tri-training leverages three classifiers equally to provide pseudo-labels to unlabeled samples; however, the method does not assume labeling samples generated from a different domain. In this paper, we propose the use of an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels. In our work, we use three networks asymmetrically, and by asymmetric, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations. Our proposed method was shown to achieve a state-of-the-art performance on the benchmark digit recognition datasets for domain adaptation.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2988–2997},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305989,
author = {Safran, Itay and Shamir, Ohad},
title = {Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the L1 norm; and smooth non-linear functions. We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2979–2987},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305988,
author = {Rukat, Tammo and Holmes, Chris C. and Titsias, Michalis K. and Yau, Christopher},
title = {Bayesian Boolean Matrix Factorisation},
year = {2017},
publisher = {JMLR.org},
abstract = {Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2969–2978},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305987,
author = {Ruggieri, Salvatore},
title = {Enumerating Distinct Decision Trees},
year = {2017},
publisher = {JMLR.org},
abstract = {The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features. We provide an exact enumeration procedure of the subsets that lead to all and only the distinct decision trees. The procedure can be adopted to prune the search space of complete and heuristics search methods in wrapper models for feature selection. Based on this, we design a computational optimization of the sequential backward elimination heuristics with a performance improvement of up to 100 x.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2960–2968},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305986,
author = {Rubinstein, Benjamin I. P. and Ald\`{a}, Francesco},
title = {Pain-Free Random Differential Privacy with Sensitivity Sampling},
year = {2017},
publisher = {JMLR.org},
abstract = {Popular approaches to differential privacy, such as the Laplace and exponential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-private function. Bounding such sensitivity is often a prohibitively complex analytic calculation. As an alternative, we propose a straightforward sampler for estimating sensitivity of non-private mechanisms. Since our sensitivity estimates hold with high probability, any mechanism that would be (ε, δ)-differentially private under bounded global sensitivity automatically achieves (ε, δ, γ)-random differential privacy (Hall et al., 2012), without any target-specific calculations required. We demonstrate on worked example learners how our usable approach adopts a naturally-relaxed privacy guarantee, while achieving more accurate releases even for non-private functions that are black-box computer programs.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2950–2959},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305985,
author = {Ritter, Samuel and Barrett, David G.T. and Santoro, Adam and Botvinick, Matt M.},
title = {Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2940–2949},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305984,
author = {Riquelme, Carlos and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
title = {Active Learning for Accurate Estimation of Linear Models},
year = {2017},
publisher = {JMLR.org},
abstract = {We explore the sequential decision-making problem where the goal is to estimate a number of linear models uniformly well, given a shared budget of random contexts independently sampled from a known distribution. For each incoming context, the decision-maker selects one of the linear models and receives an observation that is corrupted by the unknown noise level of that model. We present Trace-UCB, an adaptive allocation algorithm that learns the models' noise levels while balancing contexts accordingly across them, and prove bounds for its simple regret in both expectation and high-probability. We extend the algorithm and its bounds to the high dimensional setting, where the number of linear models times the dimension of the contexts is more than the total budget of samples. Simulations with real data suggest that Trace-UCB is remarkably robust, outperforming a number of baselines even when its assumptions are violated.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2931–2939},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305983,
author = {Rippel, Oren and Bourdev, Lubomir},
title = {Real-Time Adaptive Image Compression},
year = {2017},
publisher = {JMLR.org},
abstract = {We present a machine learning-based approach to lossy image compression which outperforms all existing codecs, while running in real-time. Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG 2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of generic images across all quality levels. At the same time, our codec is designed to be lightweight and deployable: for example, it can encode or decode the Kodak dataset in around 10ms per image on GPU. Our architecture is an autoencoder featuring pyramidal analysis, an adaptive coding module, and regularization of the expected codelength. We also supplement our approach with adversarial training specialized towards use in a compression setting: this enables us to produce visually pleasing reconstructions for very low bitrates.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2922–2930},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305982,
author = {Reed, Scott and van den Oord, A\"{a}ron and Kalchbrenner, Nal and Colmenarejo, Sergio G\'{o}mez and Wang, Ziyu and Chen, Yutian and Belov, Dan and de Freitas, Nando},
title = {Parallel Multiscale Autoregressive Density Estimation},
year = {2017},
publisher = {JMLR.org},
abstract = {PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2912–2921},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305981,
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V. and Kurakin, Alexey},
title = {Large-Scale Evolution of Image Classifiers},
year = {2017},
publisher = {JMLR.org},
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2902–2911},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305980,
author = {Ravanbakhsh, Siamak and Schneider, Jeff and P\'{o}czos, Barnab\'{a}s},
title = {Equivariance through Parameter-Sharing},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer ϕW : ℝM → ℝN, we show that ϕW is equivariant with respect to G-action iff G explains the symmetries of the network parameters W. Inspired by this observation, we then propose two parameter-sharing schemes to induce the desirable symmetry on W. Our procedure for tying the parameters achieves G-equivariance and, under some conditions on the action of G, it guarantees sensitivity to all other permutation groups outside G.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2892–2901},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305979,
author = {Rana, Santu and Li, Cheng and Gupta, Sunil and Nguyen, Vu and Venkatesh, Svetha},
title = {High Dimensional Bayesian Optimization with Elastic Gaussian Process},
year = {2017},
publisher = {JMLR.org},
abstract = {Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or tuning hyperparameter of a machine learning algorithm. However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension - having only a few peaks marooned in a large terrain of almost flat surface. Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain. We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian process priors on the objective function as we leverage two underlying facts -a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method on both benchmark test functions and real-world case studies.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2883–2891},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305978,
author = {Rahmani, Mostafa and Atia, George},
title = {Innovation Pursuit: A New Approach to the Subspace Clustering Problem},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper presents a new scalable approach, termed Innovation Pursuit (iPursuit), to the problem of subspace clustering. iPursuit rests on a new geometrical idea whereby each subspace is identified based on its novelty with respect to the other subspaces. The subspaces are identified consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data. A detailed mathematical analysis is provided establishing sufficient conditions for the proposed approach to correctly cluster the data points. Moreover, the proposed direction search approach can be integrated with spectral clustering to yield a new variant of spectral-clustering-based algorithms. Remarkably, the proposed approach can provably yield exact clustering even when the subspaces have significant intersections. The numerical simulations demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms - more so for subspaces with significant intersections - along with substantial reductions in computational complexity.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2874–2882},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305977,
author = {Rahmani, Mostafa and Atia, George},
title = {Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery},
year = {2017},
publisher = {JMLR.org},
abstract = {A remarkably simple, yet powerful, algorithm termed Coherence Pursuit for robust Principal Component Analysis (PCA) is presented. In the proposed approach, an outlier is set apart from an inlier by comparing their coherence with the rest of the data points. As inliers lie in a low dimensional subspace, they are likely to have strong mutual coherence provided there are enough inliers. By contrast, outliers do not typically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data points. As Coherence Pursuit only involves one simple matrix multiplication, it is significantly faster than the state-of-the-art robust PCA algorithms. We provide a mathematical analysis of the proposed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the proposed method can recover the correct subspace even if the data is predominantly outliers. To the best of our knowledge, this is the first provable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly dependent outliers.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2864–2873},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305976,
author = {Raghunathan, Aditi and Valiant, Gregory and Zou, James},
title = {Estimating the Unseen from Multiple Populations},
year = {2017},
publisher = {JMLR.org},
abstract = {Given samples from a distribution, how many new elements should we expect to find if we continue sampling this distribution? This is an important and actively studied problem, with many applications ranging from unseen species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population j has an unknown distribution Dj from which we observe nj samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. Surprisingly, we prove that our estimator's accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions. We validate our methods and theory through extensive experiments. Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2855–2863},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305975,
author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Dickstein, Jascha Sohl},
title = {On the Expressive Power of Deep Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2847–2854},
numpages = {8},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305974,
author = {Raffel, Colin and Luong, Minh-Thang and Liu, Peter J. and Weiss, Ron J. and Eck, Douglas},
title = {Online and Linear-Time Attention by Enforcing Monotonic Alignments},
year = {2017},
publisher = {JMLR.org},
abstract = {Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems. However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity. Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time. We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2837–2846},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305973,
author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adri\`{a} Puigdom\`{e}nech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
title = {Neural Episodic Control},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2827–2836},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305972,
author = {Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
title = {Robust Adversarial Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H∞ control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced - that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper, Walker2d and Ant) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2817–2826},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305971,
author = {Pentina, Anastasia and Lampert, Christoph H.},
title = {Multi-Task Learning with Labeled and Unlabeled Tasks},
year = {2017},
publisher = {JMLR.org},
abstract = {In multi-task learning, a learner is given a collection of prediction tasks and needs to solve all of them. In contrast to previous work, which required that annotated training data is available for all tasks, we consider a new setting, in which for some tasks, potentially most of them, only un-labeled training data is provided. Consequently, to solve all tasks, information must be transferred between tasks with labels and tasks without labels. Focusing on an instance-based transfer method we analyze two variants of this setting: when the set of labeled tasks is fixed, and when it can be actively selected by the learner. We state and prove a generalization bound that covers both scenarios and derive from it an algorithm for making the choice of labeled tasks (in the active case) and for transferring information between the tasks in a principled way. We also illustrate the effectiveness of the algorithm by experiments on synthetic and real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2807–2816},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305970,
author = {Pennington, Jeffrey and Bahri, Yasaman},
title = {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
year = {2017},
publisher = {JMLR.org},
abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, ϕ, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1 - ϕ)2.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2798–2806},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305969,
author = {Peng, Hao and Zhe, Shandian and Zhang, Xiao and Qi, Yuan},
title = {Asynchronous Distributed Variational Gaussian Process for Regression},
year = {2017},
publisher = {JMLR.org},
abstract = {Gaussian processes (GPs) are powerful non-parametric function estimators. However, their applications are largely limited by the expensive computational cost of the inference procedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling up GPs to millions of samples, are still far from satisfactory for real-world large applications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the first Asynchronous Distributed Variational Gaussian Process inference for regression, on the recent large-scale machine learning platform, PARAM-ETERSERVER. ADVGP uses a novel, flexible variational framework based on a weight space augmentation, and implements the highly efficient, asynchronous proximal gradient optimization. While maintaining comparable or better predictive performance, ADVGP greatly improves upon the efficiency of the existing variational methods. With ADVGP, we effortlessly scale up GP regression to a real-world application with billions of samples and demonstrate an excellent, superior prediction accuracy to the popular linear models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2788–2797},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305968,
author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
title = {Curiosity-Driven Exploration by Self-Supervised Prediction},
year = {2017},
publisher = {JMLR.org},
abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2778–2787},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305967,
author = {Panahi, Ashkan and Dubhashi, Devdatt and Johansson, Fredrik D. and Bhattacharyya, Chiranjib},
title = {Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery},
year = {2017},
publisher = {JMLR.org},
abstract = {Standard clustering methods such as K-means, Gaussian mixture models, and hierarchical clustering, are beset by local minima, which are sometimes drastically suboptimal. Moreover the number of clusters K must be known in advance. The recently introduced sum-of-norms (SON) or Clusterpath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on proximal iterations to solve the SON problem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying proximity condition introduced in the approximation algorithms community (that covers paradigm cases such as Gaussian mixtures and planted partition models). We give experimental results to confirm that our algorithm scales much better than previous methods while producing clusters of comparable quality.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2769–2777},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305966,
author = {Pan, Yunpeng and Yan, Xinyan and Theodorou, Evangelos A. and Boots, Byron},
title = {Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control},
year = {2017},
publisher = {JMLR.org},
abstract = {Sparse Spectrum Gaussian Processes (SSGPs) are a powerful tool for scaling Gaussian processes (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inputs, precluding their use in many real-world robotics and engineering applications where accounting for input uncertainty is crucial. We address this problem by proposing two analytic moment-based approaches with closed-form expressions for SSGP regression with uncertain inputs. Our methods are more general and scalable than their standard GP counterparts, and are naturally applicable to multi-step prediction or uncertainty propagation. We show that efficient algorithms for Bayesian filtering and stochastic model predictive control can use these methods, and we evaluate our algorithms with comparative analyses and both real-world and simulated experiments.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2760–2768},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305965,
author = {Palla, Konstantina and Knowles, David and Ghahramani, Zoubin},
title = {A Birth-Death Process for Feature Allocation},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a Bayesian nonparametric prior over feature allocations for sequential data, the birth-death feature allocation process (BDFP). The BDFP models the evolution of the feature allocation of a set of N objects across a covari-ate (e.g. time) by creating and deleting features. A BDFP is exchangeable, projective, stationary and reversible, and its equilibrium distribution is given by the Indian buffet process (IBP). We show that the Beta process on an extended space is the de Finetti mixing distribution underlying the BDFP. Finally, we present the finite approximation of the BDFP, the Beta Event Process (BEP), that permits simplified inference. The utility of the BDFP as a prior is demonstrated on real world dynamic genomics and social network data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2751–2759},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305964,
author = {Pakman, Ari and Gilboa, Dar and Carlson, David and Paninski, Liam},
title = {Stochastic Bouncy Particle Sampler},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce a stochastic version of the nonreversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples which outperform previous approaches.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2741–2750},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305963,
author = {Pad, Pedram and Salehi, Farnood and Celis, Elisa and Thiran, Patrick and Unser, Michael},
title = {Dictionary Learning Based on Sparse Distribution Tomography},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new statistical dictionary learning algorithm for sparse signals that is based on an α-stable innovation model. The parameters of the underlying model—that is, the atoms of the dictionary, the sparsity index α and the dispersion of the transform-domain coefficients—are recovered using a new type of probability distribution tomography. Specifically, we drive our estimator with a series of random projections of the data, which results in an efficient algorithm. Moreover, since the projections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sparse signals that are not necessarily α-stable. We evaluate our algorithm by performing two types of experiments: image in-painting and image denoising. In both cases, we find that our approach is competitive with state-of-the-art dictionary learning techniques. Beyond the algorithm itself, two aspects of this study are interesting in their own right. The first is our statistical formulation of the problem, which unifies the topics of dictionary learning and independent component analysis. The second is a generalization of a classical theorem about isometries of ℓp-norms that constitutes the foundation of our approach.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2731–2740},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305962,
author = {Ostrovski, Georg and Bellemare, Marc G. and van den Oord, A\"{a}ron and Munos, R\'{e}mi},
title = {Count-Based Exploration with Neural Density Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2721–2730},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305961,
author = {Osogami, Takayuki and Kajino, Hiroshi and Sekiyama, Taro},
title = {Bidirectional Learning for Time-Series Models with Hidden Units},
year = {2017},
publisher = {JMLR.org},
abstract = {Hidden units can play essential roles in modeling time-series having long-term dependency or non-linearity but make it difficult to learn associated parameters. Here we propose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of parameters as the original (forward) model. Our key observation is that only a subset of the parameters is hard to learn, and that subset is complementary between the forward model and the backward model. By training both of the two models, we can effectively learn the values of the parameters that are hard to learn if only either of the two models is trained. We apply bidirectional learning to a dynamic Boltzmann machine extended with hidden units. Numerical experiments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2711–2720},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305960,
author = {Osband, Ian and Van Roy, Benjamin},
title = {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
year = {2017},
publisher = {JMLR.org},
abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an \~{O}(H√SAT) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of \~{O}(HS √AT) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2701–2710},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305959,
author = {Ongie, Greg and Willett, Rebecca and Nowak, Robert D. and Balzano, Laura},
title = {Algebraic Variety Models for High-Rank Matrix Completion},
year = {2017},
publisher = {JMLR.org},
abstract = {We consider a generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Many well-studied extensions of linear models, including affine subspaces and their union, can be described by a variety model, as well as a rich class of nonlinear quadratic and higher degree curves and surfaces. We study the sampling requirements for matrix completion under a variety model with a focus on a union of affine subspaces. We also propose an efficient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the matrix of monomial features, using the well-known "kernel trick" to avoid working directly with the high-dimensional monomial matrix. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds, and outperforms standard low rank matrix completion and sub-space clustering algorithms in experiments with real data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2691–2700},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305958,
author = {Omidshafiei, Shayegan and Pazis, Jason and Amato, Christopher and How, Jonathan P. and Vian, John},
title = {Deep Decentralized Multi-Task Multi-Agent Reinforcement Learning under Partial Observability},
year = {2017},
publisher = {JMLR.org},
abstract = {Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2681–2690},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305957,
author = {Oliva, Junier B. and P\'{o}czos, Barnab\'{a}s and Schneider, Jeff},
title = {The Statistical Recurrent Unit},
year = {2017},
publisher = {JMLR.org},
abstract = {Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters for both synthetic and real-world tasks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2671–2680},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305956,
author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
title = {Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2661–2670},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305955,
author = {Oglic, Dino and G\"{a}rtner, Thomas},
title = {Nystr\"{o}m Method with Kernel K-Means++ Samples as Landmarks},
year = {2017},
publisher = {JMLR.org},
abstract = {We investigate, theoretically and empirically, the effectiveness of kernel K-means++ samples as landmarks in the Nystr\"{o}m method for low-rank approximation of kernel matrices. Previous empirical studies (Zhang et al., 2008; Kumar et al., 2012) observe that the landmarks obtained using (kernel) K-means clustering define a good low-rank approximation of kernel matrices. However, the existing work does not provide a theoretical guarantee on the approximation error for this approach to landmark selection. We close this gap and provide the first bound on the approximation error of the Nystr\"{o}m method with kernel K-means++ samples as landmarks. Moreover, for the frequently used Gaussian kernel we provide a theoretically sound motivation for performing Lloyd refinements of kernel K-means++ landmarks in the instance space. We substantiate our theoretical results empirically by comparing the approach to several state-of-the-art algorithms.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2652–2660},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305954,
author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
title = {Conditional Image Synthesis with Auxiliary Classifier GANs},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2642–2651},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305953,
author = {Ochiai, Tsubasa and Watanabe, Shinji and Hori, Takaaki and Hershey, John R.},
title = {Multichannel End-to-End Speech Recognition},
year = {2017},
publisher = {JMLR.org},
abstract = {The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2632–2641},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305952,
author = {Ni, Xiuyan and Quadrianto, Novi and Wang, Yusu and Chen, Chao},
title = {Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data},
year = {2017},
publisher = {JMLR.org},
abstract = {Clustering data with both continuous and discrete attributes is a challenging task. Existing methods often lack a principled probabilistic formulation. In this paper, we propose a clustering method based on a tree-structured graphical model to describe the generation process of mixed-type data. Our tree-structured model factorizes into a product of pairwise interactions, and thus localizes the interaction between feature variables of different types. To provide a robust clustering method based on the tree-model, we adopt a topographical view and compute peaks of the density function and their attractive basins for clustering. Furthermore, we leverage the theory from topology data analysis to adaptively merge trivial peaks into large ones in order to achieve meaningful clusterings. Our method outperforms state-of-the-art methods on mixed-type data.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2622–2631},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305951,
author = {Nguyen, Lam M. and Liu, Jie and Scheinberg, Katya and Tak\'{a}\v{c}, Martin},
title = {SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2613–2621},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305950,
author = {Nguyen, Quynh and Hein, Matthias},
title = {The Loss Surface of Deep and Wide Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2603–2612},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305949,
author = {Neiswanger, Willie and Xing, Eric},
title = {Post-Inference Prior Swapping},
year = {2017},
publisher = {JMLR.org},
abstract = {While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, convenient priors that allow for computationally cheap or tractable inference are commonly used. In this paper, we investigate the following question: for a given model, is it possible to compute an inference result with any convenient false prior, and afterwards, given any target prior of interest, quickly transform this result into the target posterior? A potential solution is to use importance sampling (IS). However, we demonstrate that IS will fail for many choices of the target prior, depending on its parametric form and similarity to the false prior. Instead, we propose prior swapping, a method that leverages the pre-inferred false posterior to efficiently generate accurate posterior samples under arbitrary target priors. Prior swapping lets us apply less-costly inference algorithms to certain models, and incorporate new or updated prior information "post-inference". We give theoretical guarantees about our method, and demonstrate it empirically on a number of models and priors.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2594–2602},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305948,
author = {Neil, Daniel and Lee, Jun Haeng and Delbruck, Tobi and Liu, Shih-Chii},
title = {Delta Networks for Optimized Recurrent Network Computation},
year = {2017},
publisher = {JMLR.org},
abstract = {Many neural networks exhibit stability in their activation patterns over time in response to inputs from sensors operating under real-world conditions. By capitalizing on this property of natural signals, we propose a Recurrent Neural Network (RNN) architecture called a delta network in which each neuron transmits its value only when the change in its activation exceeds a threshold. The execution of RNNs as delta networks is attractive because their states must be stored and fetched at every timestep, unlike in convolutional neural networks (CNNs). We show that a naive run-time delta network implementation offers modest improvements on the number of memory accesses and computes, but optimized training techniques confer higher accuracy at higher speedup. With these optimizations, we demonstrate a 9X reduction in cost with negligible loss of accuracy for the TIDIGITS audio digit recognition benchmark. Similarly, on the large Wall Street Journal (WSJ) speech recognition benchmark, pretrained networks can also be greatly accelerated as delta networks and trained delta networks show a 5.7X improvement with negligible loss of accuracy. Finally, on an end-to-end CNN-RNN network trained for steering angle prediction in a driving dataset, the RNN cost can be reduced by a substantial 100X.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2584–2593},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305947,
author = {Namkoong, Hongseok and Sinha, Aman and Yadlowsky, Steve and Duchi, John C.},
title = {Adaptive Sampling Probabilities for Non-Smooth Optimization},
year = {2017},
publisher = {JMLR.org},
abstract = {Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data; their good behavior under random sampling is predicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examples (for SGD) are larger than others, there is a natural structure that can be exploited for quicker convergence. Yet adaptive variants often suffer nontrivial computational overhead. We present a framework that discovers and leverages such structural properties at a low computational cost. We employ a bandit optimization procedure that "learns" probabilities for sampling coordinates or examples in (non-smooth) optimization problems, allowing us to guarantee performance close to that of the optimal stationary sampling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adaptive counterparts, and we complement our analysis with experiments on several datasets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2574–2583},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305946,
author = {Nagamine, Tasha and Mesgarani, Nima},
title = {Understanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition},
year = {2017},
publisher = {JMLR.org},
abstract = {Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feedforward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2564–2573},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305945,
author = {Munkhdalai, Tsendsuren and Yu, Hong},
title = {Meta Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2554–2563},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305944,
author = {Mukkamala, Mahesh Chandra and Hein, Matthias},
title = {Variants of RMSProp and Adagrad with Logarithmic Regret Bounds},
year = {2017},
publisher = {JMLR.org},
abstract = {Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show √T-type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2545–2553},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305943,
author = {Mueller, Jonas and Gifford, David and Jaakkola, Tommi},
title = {Sequence to Better Sequence: Continuous Revision of Combinatorial Structures},
year = {2017},
publisher = {JMLR.org},
abstract = {We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome. Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions. To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module. Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes. By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural. These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2536–2544},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305942,
author = {Mroueh, Youssef and Sercu, Tom and Goel, Vaibhava},
title = {McGan: Mean and Covariance Feature Matching GAN},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and co-variance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2527–2535},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305941,
author = {Mou, Lili and Lu, Zhengdong and Li, Hang and Jin, Zhi},
title = {Coupling Distributed and Symbolic Execution for Natural Language Queries},
year = {2017},
publisher = {JMLR.org},
abstract = {Building neural networks to query a knowledge base (a table) with natural language is an emerging research topic in deep learning. An executor for table querying typically requires multiple steps of execution because queries may have complicated structures. In previous studies, researchers have developed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and explicit interpretability. A symbolic executor is efficient in execution, but is very difficult to train especially at initial stages. In this paper, we propose to couple distributed and symbolic execution for natural language queries, where the symbolic executor is pretrained with the distributed executor's intermediate execution results in a step-by-step fashion. Experiments show that our approach significantly outperforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interpretability.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2518–2526},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305940,
author = {Mollaysa, Amina and Strasser, Pablo and Kalousis, Alexandros},
title = {Regularising Non-Linear Models Using Feature Side-Information},
year = {2017},
publisher = {JMLR.org},
abstract = {Very often features come with their own vectorial descriptions which provide detailed information about their properties. We refer to these vectorial descriptions as feature side-information. In the standard learning scenario, input is represented as a vector of features and the feature side-information is most often ignored or used only for feature selection prior to model fitting. We believe that feature side-information which carries information about features intrinsic property will help improve model prediction if used in a proper way during learning process. In this paper, we propose a framework that allows for the incorporation of the feature side-information during the learning of very general model families to improve the prediction performance. We control the structures of the learned models so that they reflect features' similarities as these are defined on the basis of the side-information. We perform experiments on a number of benchmark datasets which show significant predictive performance gains, over a number of baselines, as a result of the exploitation of the side-information.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2508–2517},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305939,
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
title = {Variational Dropout Sparsifies Deep Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2498–2507},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305938,
author = {Mohajer, Soheil and Suh, Changho and Elmahdy, Adel},
title = {Active Learning for Top-<i>K</i> Rank Aggregation from Noisy Comparisons},
year = {2017},
publisher = {JMLR.org},
abstract = {We explore an active top-K ranking problem based on pairwise comparisons that are collected possibly in a sequential manner as per our design choice. We consider two settings: (1) top-K sorting in which the goal is to recover the top-K items in order out of n items; (2) top-K partitioning where only the set of top-K items is desired. Under a fairly general model which subsumes as special cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize upper bounds on the sample size required for top-K sorting as well as for top-K partitioning. As a consequence, we demonstrate that active ranking can offer significant multiplicative gains in sample complexity over passive ranking. Depending on the underlying stochastic noise model, such gain varies from around log n/log log n to n2 log n/log log n. We also present an algorithm that is applicable to both settings.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2488–2497},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305937,
author = {Mitrovic, Marko and Bun, Mark and Krause, Andreas and Karbasi, Amin},
title = {Differentially Private Submodular Maximization: Data Summarization in Disguise},
year = {2017},
publisher = {JMLR.org},
abstract = {Many data summarization applications are captured by the general framework of submodular maximization. As a consequence, a wide range of efficient approximation algorithms have been developed. However, when such applications involve sensitive data about individuals, their privacy concerns are not automatically addressed. To remedy this problem, we propose a general and systematic study of differentially private submodular maximization. We present privacy-preserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and p-extendible system constraints, with guarantees that are competitive with optimal solutions. Along the way, we analyze a new algorithm for non-monotone submodular maximization under a cardinality constraint, which is the first (even non-privately) to achieve a constant approximation ratio with a linear number of function evaluations. We additionally provide two concrete experiments to validate the efficacy of these algorithms.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2478–2487},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305936,
author = {Mitliagkas, Ioannis and Mackey, Lester},
title = {Improving Gibbs Sampler Scan Quality with DoGS},
year = {2017},
publisher = {JMLR.org},
abstract = {The pairwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs sampling. In this work, we use Dobrushin influence as the basis of a practical tool to certify and efficiently improve the quality of a discrete Gibbs sampler. Our Dobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection orders for a given sampling budget and variable subset of interest, explicit bounds on total variation distance to stationarity, and certifiable improvements over the standard systematic and uniform random scan Gibbs samplers. In our experiments with joint image segmentation and object recognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller sampling budgets than standard Gibbs samplers.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2469–2477},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305935,
author = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
title = {Prediction and Control with Temporal Segment Models},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2459–2468},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305934,
author = {Mirzasoleiman, Baharan and Karbasi, Amin and Krause, Andreas},
title = {Deletion-Robust Submodular Maximization: Data Summarization with "the Right to Be Forgotten"},
year = {2017},
publisher = {JMLR.org},
abstract = {How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an important challenge in online services, where the users generating the data may decide to exercise their right to restrict the service provider from using (part of) their data due to privacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization problem. We develop the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor approximation guarantee to the optimum solution. We evaluate the effectiveness of our approach on several real-world applications, including summarizing (1) streams of geo-coordinates (2); streams of images; and (3) click-stream log data, consisting of 45 million feature vectors from a news recommendation task.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2449–2458},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305933,
author = {Mirrokni, Vahab and Leme, Renato Paes and Vladu, Adrian and Wong, Sam Chiu-wai},
title = {Tight Bounds for Approximate Carath\'{e}Odory and Beyond},
year = {2017},
publisher = {JMLR.org},
abstract = {We present a deterministic nearly-linear time algorithm for approximating any point inside a convex polytope with a sparse convex combination of the polytope's vertices. Our result provides a constructive proof for the Approximate Caratheodory Problem (Barman, 2015), which states that any point inside a polytope contained in the ℓp ball of radius D can be approximated to within ε in ℓp norm by a convex combination of O(D2p/ε2) vertices of the polytope for p ≥ 2. While for the particular case of p = 2, this can be achieved by the well-known Perceptron algorithm, we follow a more principled approach which generalizes to arbitrary p ≥ 2; furthermore, this naturally extends to domains with more complicated geometry, as it is the case for providing an approximate Birkhoff-von Neumann decomposition. Secondly, we show that the sparsity bound is tight for ℓp norms, using an argument based on anti-concentration for the binomial distribution, thus resolving an open question posed by Barman. Experimentally, we verify that our deterministic optimization-based algorithms achieve in practice much better sparsity than previously known sampling-based algorithms. We also show how to apply our techniques to SVM training and rounding fractional points in matroid and flow polytopes.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2440–2448},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305932,
author = {Mirhoseini, Azalia and Pham, Hieu and Le, Quoc V. and Steiner, Benoit and Larsen, Rasmus and Zhou, Yuefeng and Kumar, Naveen and Norouzi, Mohammad and Bengio, Samy and Dean, Jeff},
title = {Device Placement Optimization with Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2430–2439},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305931,
author = {Miller, Andrew C. and Foti, Nicholas J. and Adams, Ryan P.},
title = {Variational Boosting: Iteratively Refining Posterior Approximations},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing a trade-off between computation time and accuracy. We expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that the resulting posterior inferences compare favorably to existing variational algorithms.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2420–2429},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305930,
author = {Miao, Yishu and Grefenstette, Edward and Blunsom, Phil},
title = {Discovering Discrete Latent Topics with Neural Variational Inference},
year = {2017},
publisher = {JMLR.org},
abstract = {Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2410–2419},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305929,
author = {Mhammedi, Zakaria and Hellicar, Andrew and Rahman, Ashfaqur and Bailey, James},
title = {Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections},
year = {2017},
publisher = {JMLR.org},
abstract = {The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2401–2409},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305928,
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
title = {Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2391–2400},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305927,
author = {Mei, Jiali and De Castro, Yohann and Goude, Yannig and H\'{e}brail, Georges},
title = {Nonnegative Matrix Factorization for Time Series Recovery from a Few Temporal Aggregates},
year = {2017},
publisher = {JMLR.org},
abstract = {Motivated by electricity consumption reconstitution, we propose a new matrix recovery method using nonnegative matrix factorization (NMF). The task tackled here is to reconstitute electricity consumption time series at a fine temporal scale from measures that are temporal aggregates of individual consumption. Contrary to existing NMF algorithms, the proposed method uses temporal aggregates as input data, instead of matrix entries. Furthermore, the proposed method is extended to take into account individual autocorrelation to provide better estimation, using a recent convex relaxation of quadratically constrained quadratic programs. Extensive experiments on synthetic and real-world electricity consumption datasets illustrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2382–2390},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305926,
author = {McNamara, Daniel and Balcan, Maria-Florina},
title = {Risk Bounds for Transferring Representations with and without Fine-Tuning},
year = {2017},
publisher = {JMLR.org},
abstract = {A popular machine learning strategy is the transfer of a representation (i.e. a feature extraction function) learned on a source task to a target task. Examples include the re-use of neural network weights or word embeddings. We develop sufficient conditions for the success of this approach. If the representation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an upper bound on target task risk via a VC dimension-based argument. We then consider using the representation from the source task to construct a prior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examples of our bounds using feedforward neural networks. Our results motivate a practical approach to weight transfer, which we validate with experiments.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2373–2381},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305925,
author = {McGill, Mason and Perona, Pietro},
title = {Deciding How to Decide: Dynamic Routing in Artificial Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: graphs of learned transformations through which different input signals may take different paths. Though some approaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become specialized to process distinct categories of images. Additionally, given a fixed computational budget, dynamically-routed networks tend to perform better than comparable statically-routed networks.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2363–2372},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305924,
author = {Maystre, Lucas and Grossglauser, Matthias},
title = {ChoiceRank: Identifying Preferences from Node Traffic in Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Understanding how users navigate in a network is of high interest in many applications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition probabilities. We cast it as a preference learning problem, and we study a model where choices follow Luce's axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n2) transition probabilities. We show how to make the inference problem well-posed regardless of the network's structure, and we present ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We apply the model to two clickstream datasets and show that it successfully recovers the transition probabilities using only the network structure and marginal (node-level) traffic data. Finally, we also consider an application to mobility networks and apply the model to one year of rides on New York City's bicycle-sharing system.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2354–2362},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305923,
author = {Maystre, Lucas and Grossglauser, Matthias},
title = {Just Sort It! A Simple and Effective Approach to Active Preference Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley-Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2344–2353},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305922,
author = {Masegosa, Andr\'{e}s and Nielsen, Thomas D. and Langseth, Helge and Ramos-L\'{o}pez, Dar\'{\i}o and Salmer\'{o}n, Antonio and Madsen, Anders L.},
title = {Bayesian Models of Data Streams with Hierarchical Power Priors},
year = {2017},
publisher = {JMLR.org},
abstract = {Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2334–2343},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305921,
author = {Mao, Xueyu and Sarkar, Purnamrita and Chakrabarti, Deepayan},
title = {On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations},
year = {2017},
publisher = {JMLR.org},
abstract = {The problem of finding overlapping communities in networks has gained much attention recently. Optimization-based approaches use non-negative matrix factorization (NMF) or variants, but the global optimum cannot be provably attained in general. Model-based approaches, such as the popular mixed membership stochastic blockmodel or MMSB (Airoldi et al., 2008), use parameters for each node to specify the overlapping communities, but standard inference techniques cannot guarantee consistency. We link the two approaches, by (a) establishing sufficient conditions for the symmetric NMF optimization to have a unique solution under MMSB, and (b) proposing a computationally efficient algorithm called GeoNMF that is provably optimal and hence consistent for a broad parameter regime. We demonstrate its accuracy on both simulated and real-world datasets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2324–2333},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305920,
author = {Malherbe, Cedric and Vayatis, Nicolas},
title = {Global Optimization of Lipschitz Functions},
year = {2017},
publisher = {JMLR.org},
abstract = {The goal of the paper is to design sequential strategies which lead to efficient optimization of an unknown function under the only assumption that it has a finite Lipschitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the expected minimax rate for their performance. We introduce and analyze a first algorithm called LIPO which assumes the Lipschitz constant to be known. Consistency, minimax rates for LIPO are proved, as well as fast rates under an additional H\"{o}lder like condition. An adaptive version of LIPO is also introduced for the more realistic setup where the Lipschitz constant is unknown and has to be estimated along with the optimization. Similar theoretical guarantees are shown to hold for the adaptive algorithm and a numerical assessment is provided at the end of the paper to illustrate the potential of this strategy with respect to state-of-the-art methods over typical benchmark problems for global optimization.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2314–2323},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305919,
author = {Mair, Sebastian and Boubekki, Ahc\`{e}ne and Brefeld, Ulf},
title = {Frame-Based Data Factorizations},
year = {2017},
publisher = {JMLR.org},
abstract = {Archetypal Analysis is the method of choice to compute interpretable matrix factorizations. Every data point is represented as a convex combination of factors, i.e., points on the boundary of the convex hull of the data. This renders computation inefficient. In this paper, we show that the set of vertices of a convex hull, the so-called frame, can be efficiently computed by a quadratic program. We provide theoretical and empirical results for our proposed approach and make use of the frame to accelerate Archetypal Analysis. The novel method yields similar reconstruction errors as baseline competitors but is much faster to compute.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2305–2313},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305918,
author = {Machado, Marlos C. and Bellemare, Marc G. and Bowling, Michael},
title = {A Laplacian Framework for Option Discovery in Reinforcement Learning},
year = {2017},
publisher = {JMLR.org},
abstract = {Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2295–2304},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305917,
author = {MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L. and Taylor, Matthew E. and Littman, Michael L.},
title = {Interactive Learning from Policy-Dependent Human Feedback},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false— whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2285–2294},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305916,
author = {Ma, Fan and Meng, Deyu and Xie, Qi and Li, Zina and Dong, Xuanyi},
title = {Self-Paced Co-Training},
year = {2017},
publisher = {JMLR.org},
abstract = {Co-training is a well-known semi-supervised learning approach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training process, labels of unlabeled instances in the training pool are very likely to be false especially in the initial training rounds, while the standard co-training algorithm utilizes a "draw without replacement" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its performance but also hampers its fundamental theory. Besides, there is no optimization model to explain what objective a co-training process optimizes. To these issues, in this study we design a new co-training algorithm named self-paced co-training (SPaCo) with a "draw with replacement" learning mode. The rationality of SPaCo can be proved under theoretical assumptions utilized in traditional co-training research, and furthermore, the algorithm exactly complies with the alternative optimization process for an optimization model of self-paced curriculum learning, which can be finely explained in robust learning manner. Experimental results substantiate the superiority of the proposed method as compared with current state-of-the-art co-training methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2275–2284},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305915,
author = {Ma, Yi-An and Foti, Nicholas J. and Fox, Emily B.},
title = {Stochastic Gradient MCMC Methods for Hidden Markov Models},
year = {2017},
publisher = {JMLR.org},
abstract = {Stochastic gradient MCMC (SG-MCMC) algorithms have proven useful in scaling Bayesian inference to large datasets under an assumption of i.i.d data. We instead develop an SG-MCMC algorithm to learn the parameters of hidden Markov models (HMMs) for time-dependent data. There are two challenges to applying SG-MCMC in this setting: The latent discrete states, and needing to break dependencies when considering minibatches. We consider a marginal likelihood representation of the HMM and propose an algorithm that harnesses the inherent memory decay of the process. We demonstrate the effectiveness of our algorithm on synthetic experiments and an ion channel recording data, with runtimes significantly outperforming batch MCMC.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2265–2274},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305914,
author = {Lyu, Yueming},
title = {Spherical Structured Feature Maps for Kernel Approximation},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose Spherical Structured Feature (SSF) maps to approximate shift and rotation invariant kernels as well as bth-order arc-cosine kernels (Cho &amp; Saul, 2009). We construct SSF maps based on the point set on d - 1 dimensional sphere Sd-1. We prove that the inner product of SSF maps are unbiased estimates for above kernels if asymptotically uniformly distributed point set on Sd-1 is given. According to (Brauchart &amp; Grabner, 2015), optimizing the discrete Riesz s-energy can generate asymptotically uniformly distributed point set on Sd-1. Thus, we propose an efficient coordinate decent method to find a local optimum of the discrete Riesz s-energy for SSF maps construction. Theoretically, SSF maps construction achieves linear space complexity and loglinear time complexity. Empirically, SSF maps achieve superior performance compared with other methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2256–2264},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305913,
author = {Lv, Kaifeng and Jiang, Shunhua and Li, Jian},
title = {Learning Gradient Descent: Better Generalization and Longer Horizons},
year = {2017},
publisher = {JMLR.org},
abstract = {Training deep neural networks is a highly non-trivial task, involving carefully selecting appropriate training algorithms, scheduling step sizes and tuning other hyperparameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use deep learning algorithms to exploit the landscape of the loss function of the training problem of interest, and learn how to optimize over it in an automatic way. In this paper, we propose a new learning-to-learn model and some useful and practical tricks. Our optimizer outperforms generic, hand-crafted optimization algorithms and state-of-the-art learning-to-learn optimizers by DeepMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including deep MLPs, CNNs, and simple LSTMs.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2247–2255},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305912,
author = {Luo, Ping},
title = {Learning Deep Architectures via Generalized Whitened Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation. However, the whitening transformation increases computation time. Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties. First, GWNN is able to learn compact representation to reduce computations. Second, it enables whitening transformation to be performed in a short period, preserving good conditioning. Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency. Extensive experiments on various datasets demonstrate the benefits of GWNN.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2238–2246},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305911,
author = {Loukas, Andreas},
title = {How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?},
year = {2017},
publisher = {JMLR.org},
abstract = {How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2228–2237},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305910,
author = {Louizos, Christos and Welling, Max},
title = {Multiplicative Normalizing Flows for Variational Bayesian Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende &amp; Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maal0e et al., 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2218–2227},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305909,
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
title = {Deep Transfer Learning with Joint Adaptation Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2208–2217},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305908,
author = {Livni, Roi and Carmon, Daniel and Globerson, Amir},
title = {Learning Infinite Layer Networks without the Kernel Trick},
year = {2017},
publisher = {JMLR.org},
abstract = {Infinite Layer Networks (ILN) have been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods. ILN are networks that integrate over infinitely many nodes within a single hidden layer. It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable. In this work we give an online algorithm for ILN, which avoids the kernel trick assumption. More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling. We provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values. Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comparable performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2198–2207},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305907,
author = {Liu, Hairong and Zhu, Zhenyao and Li, Xiangang and Satheesh, Sanjeev},
title = {Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling},
year = {2017},
publisher = {JMLR.org},
abstract = {Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed. These drawbacks usually result in sub-optimal performance of modeling sequences. In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC. While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences. Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency. We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2188–2197},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305906,
author = {Liu, Bo and Yuan, Xiao-Tong and Wang, Lezi and Liu, Qingshan and Metaxas, Dimitris N.},
title = {Dual Iterative Hard Thresholding: From Non-Convex Sparse Minimization to Non-Smooth Concave Maximization},
year = {2017},
publisher = {JMLR.org},
abstract = {Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice. As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form. It remains open to explore duality theory and algorithms in such a non-convex and NP-hard problem setting. In this paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization with ℓ2-regularized loss function and proposing an IHT-style algorithm for dual maximization. Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be e-quivalently solved in a dual formulation. The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by virtually all the existing primal IHT algorithms without sparsity relaxation. Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization. Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2179–2187},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305905,
author = {Liu, Hanxiao and Wu, Yuexin and Yang, Yiming},
title = {Analogical Inference for Multi-Relational Embeddings},
year = {2017},
publisher = {JMLR.org},
abstract = {Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledge-based inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the analogical properties of the embedded entities and relations. By formulating the learning objective in a differentiable fashion, our model enjoys both theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2168–2178},
numpages = {11},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305904,
author = {Liu, Tongliang and Lugosi, G\'{a}bor and Neu, Gergely and Tao, Dacheng},
title = {Algorithmic Stability and Hypothesis Complexity},
year = {2017},
publisher = {JMLR.org},
abstract = {We introduce a notion of algorithmic stability of learning algorithms—that we term argument stability—that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. The main result of the paper bounds the generalization error of any learning algorithm in terms of its argument stability. The bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2159–2167},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305903,
author = {Liu, Weiyang and Dai, Bo and Humayun, Ahmad and Tay, Charlene and Yu, Chen and Smith, Linda B. and Rehg, James M. and Song, Le},
title = {Iterative Machine Teaching},
year = {2017},
publisher = {JMLR.org},
abstract = {In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2149–2158},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305902,
author = {Liu, Li-Ping and Blei, David M.},
title = {Zero-Inflated Exponential Family Embeddings},
year = {2017},
publisher = {JMLR.org},
abstract = {Word embeddings are a widely-used tool to analyze language, and exponential family embeddings (Rudolph et al., 2016) generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally down-weights the zeros and dampens their influence on the model. Across many types of data— language, movie ratings, shopping histories, and bird watching logs—we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2140–2148},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305901,
author = {Lipor, John and Balzano, Laura},
title = {Leveraging Union of Subspace Structure to Improve Constrained Clustering},
year = {2017},
publisher = {JMLR.org},
abstract = {Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label. Sub-space clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits. While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible. We present a pairwise-constrained clustering algorithm that actively selects queries based on the union-of-subspaces model. The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary. We prove that points lying near the intersection of subspaces are points with low margin. Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2130–2139},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305900,
author = {Lindgren, Erik M. and Dimakis, Alexandros G. and Klivans, Adam},
title = {Exact MAP Inference by Avoiding Fractional Vertices},
year = {2017},
publisher = {JMLR.org},
abstract = {Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2120–2129},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305899,
author = {Li, Qunwei and Zhou, Yi and Liang, Yingbin and Varshney, Pramod K.},
title = {Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization},
year = {2017},
publisher = {JMLR.org},
abstract = {In this work, we investigate the accelerated proximal gradient method for nonconvex programming (APGnc). The method compares between a usual proximal gradient step and a linear extrapolation step, and accepts the one that has a lower function value to achieve a monotonic decrease. In specific, under a general nonsmooth and nonconvex setting, we provide a rigorous argument to show that the limit points of the sequence generated by APGnc are critical points of the objective function. Then, by exploiting the Kurdyka-undefinedojasiewicz (Kundefined) property for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further propose a stochastic variance reduced APGnc (SVRG-APGnc), and establish its linear convergence under a special case of the Kundefined property. We also extend the analysis to the inexact version of these methods and develop an adaptive momentum strategy that improves the numerical performance.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2111–2119},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305898,
author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
title = {Stochastic Modified Equations and Adaptive Stochastic Gradient Algorithms},
year = {2017},
publisher = {JMLR.org},
abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2101–2110},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305897,
author = {Li, Alexander Hanbo and Martin, Andrew},
title = {Forest-Type Regression with General Losses and Robust Forest},
year = {2017},
publisher = {JMLR.org},
abstract = {This paper introduces a new general framework for forest-type regression which allows the development of robust forest regressors by selecting from a large family of robust loss functions. In particular, when plugged in the squared error and quantile losses, it will recover the classical random forest (Breiman, 2001) and quantile random forest (Meinshausen, 2006). We then use robust loss functions to develop more robust forest-type regression algorithms. In the experiments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly improve the generalization performance of random forest.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2091–2100},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305896,
author = {Li, Ke and Malik, Jitendra},
title = {Fast <i>k</i>-Nearest Neighbour Search via Prioritized DCI},
year = {2017},
publisher = {JMLR.org},
abstract = {Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) (Li &amp; Malik, 2016) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear. In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality. In particular, a linear increase in intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space. We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods. In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2081–2090},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305895,
author = {Li, Lihong and Lu, Yu and Zhou, Dengyong},
title = {Provably Optimal Algorithms for Generalized Linear Contextual Bandits},
year = {2017},
publisher = {JMLR.org},
abstract = {Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in particular) have demonstrated stronger performance than linear models in many applications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we propose an upper confidence bound based algorithm for generalized linear contextual bandits, which achieves an \~{O}(√dT) regret over T rounds with d dimensional feature vectors. This regret matches the minimax lower bound, up to logarithmic terms, and improves on the best previous result by a √d factor, assuming the number of arms is fixed. A key component in our analysis is to establish a new, sharp finite-sample confidence bound for maximum-likelihood estimates in generalized linear models, which may be of independent interest. We also analyze a simpler upper confidence bound algorithm, which is useful in practice, and prove it to have optimal regret for certain cases.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2071–2080},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305894,
author = {Li, Yuanzhi and Liang, Yingyu},
title = {Provable Alternating Gradient Descent for Non-Negative Matrix Factorization with Strong Correlations},
year = {2017},
publisher = {JMLR.org},
abstract = {Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the ground-truth.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2062–2070},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305893,
author = {Li, Yingzhen and Gal, Yarin},
title = {Dropout Inference in Bayesian Neural Networks with Alpha-Divergences},
year = {2017},
publisher = {JMLR.org},
abstract = {To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a reparametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2052–2061},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305892,
author = {Levy, Dor and Wolf, Lior},
title = {Learning to Align the Source Code to the Compiled Object Code},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code. Our architecture learns the alignment between the two sequences - one being the translation of the other - by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains. Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines. We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2043–2051},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{10.5555/3305890.3305891,
author = {Lei, Qi and Yen, Ian E.H. and Wu, Chao-yuan and Dhillon, Inderjit S. and Ravikumar, Pradeep},
title = {Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization},
year = {2017},
publisher = {JMLR.org},
abstract = {We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy Primal-Dual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables. It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2034–2042},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@proceedings{10.5555/3305890,
title = {ICML'17: Proceedings of the 34th International Conference on Machine Learning - Volume 70},
year = {2017},
publisher = {JMLR.org},
location = {Sydney, NSW, Australia}
}

