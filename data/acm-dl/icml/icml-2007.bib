@inproceedings{10.1145/1273496.1273497,
author = {A\"{\i}meur, Esma and Brassard, Gilles and Gambs, S\'{e}bastien},
title = {Quantum Clustering Algorithms},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273497},
doi = {10.1145/1273496.1273497},
abstract = {By the term "quantization", we refer to the process of using quantum mechanics in order to improve a classical algorithm, usually by making it go faster. In this paper, we initiate the idea of quantizing clustering algorithms by using variations on a celebrated quantum algorithm due to Grover. After having introduced this novel approach to unsupervised learning, we illustrate it with a quantized version of three standard algorithms: divisive clustering, k-medians and an algorithm for the construction of a neighbourhood graph. We obtain a significant speedup compared to the classical approach.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1–8},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273498,
author = {Agarwal, Alekh and Chakrabarti, Soumen},
title = {Learning Random Walks to Rank Nodes in Graphs},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273498},
doi = {10.1145/1273496.1273498},
abstract = {Ranking nodes in graphs is of much recent interest. Edges, via the graph Laplacian, are used to encourage local smoothness of node scores in SVM-like formulations with generalization guarantees. In contrast, Page-rank variants are based on Markovian random walks. For directed graphs, there is no simple known correspondence between these views of scoring/ranking. Recent scalable algorithms for learning the Pagerank transition probabilities do not have generalization guarantees. In this paper we show some correspondence results between the Laplacian and the Pagerank approaches, and give new generalization guarantees for the latter. We enhance the Pagerank-learning approaches to use an additive margin. We also propose a general framework for rank-sensitive score-learning, and apply it to Laplacian smoothing. Experimental results are promising.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {9–16},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273499,
author = {Amit, Yonatan and Fink, Michael and Srebro, Nathan and Ullman, Shimon},
title = {Uncovering Shared Structures in Multiclass Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273499},
doi = {10.1145/1273496.1273499},
abstract = {This paper suggests a method for multiclass learning with many classes by simultaneously learning shared characteristics common to the classes, and predictors for the classes in terms of these characteristics. We cast this as a convex optimization problem, using trace-norm regularization and study gradient-based optimization both for the linear case and the kernelized setting.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {17–24},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273500,
author = {Ando, Rie Kubota and Zhang, Tong},
title = {Two-View Feature Generation Model for Semi-Supervised Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273500},
doi = {10.1145/1273496.1273500},
abstract = {We consider a setting for discriminative semi-supervised learning where unlabeled data are used with a generative model to learn effective feature representations for discriminative training. Within this framework, we revisit the two-view feature generation model of co-training and prove that the optimum predictor can be expressed as a linear combination of a few features constructed from unlabeled data. From this analysis, we derive methods that employ two views but are very different from co-training. Experiments show that our approach is more robust than co-training and EM, under various data generation conditions.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {25–32},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273501,
author = {Andrew, Galen and Gao, Jianfeng},
title = {Scalable Training of <i>L</i><sup>1</sup>-Regularized Log-Linear Models},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273501},
doi = {10.1145/1273496.1273501},
abstract = {The L-BFGS limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Efficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm Orthant-Wise Limited-memory Quasi-Newton (OWL-QN), based on L-BFGS, that can efficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than L-BFGS on the analogous L2-regularized problem. We also present a proof that OWL-QN is guaranteed to converge to a globally optimal parameter vector.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {33–40},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273502,
author = {Asharaf, S. and Murty, M. Narasimha and Shevade, S. K.},
title = {Multiclass Core Vector Machine},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273502},
doi = {10.1145/1273496.1273502},
abstract = {Even though several techniques have been proposed in the literature for achieving multiclass classification using Support Vector Machine(SVM), the scalability aspect of these approaches to handle large data sets still needs much of exploration. Core Vector Machine(CVM) is a technique for scaling up a two class SVM to handle large data sets. In this paper we propose a Multiclass Core Vector Machine(MCVM). Here we formulate the multiclass SVM problem as a Quadratic Programming(QP) problem defining an SVM with vector valued output. This QP problem is then solved using the CVM technique to achieve scalability to handle large data sets. Experiments done with several large synthetic and real world data sets show that the proposed MCVM technique gives good generalization performance as that of SVM at a much lesser computational expense. Further, it is observed that MCVM scales well with the size of the data set.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273503,
author = {Azran, Arik},
title = {The Rendezvous Algorithm: Multiclass Semi-Supervised Learning with Markov Random Walks},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273503},
doi = {10.1145/1273496.1273503},
abstract = {We consider the problem of multiclass classification where both labeled and unlabeled data points are given. We introduce and demonstrate a new approach for estimating a distribution over the missing labels where data points are viewed as nodes of a graph, and pairwise similarities are used to derive a transition probability matrix P for a Markov random walk between them. The algorithm associates each point with a particle which moves between points according to P. Labeled points are set to be absorbing states of the Markov random walk, and the probability of each particle to be absorbed by the different labeled points, as the number of steps increases, is then used to derive a distribution over the associated missing label. A computationally efficient algorithm to implement this is derived and demonstrated on both real and artificial data sets, including a numerical comparison with other methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {49–56},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273504,
author = {Babaria, Rashmin and Nath, J. Saketha and S, Krishnan and R, Sivaramakrishnan K and Bhattacharyya, Chiranjib and Murty, M. N.},
title = {Focused Crawling with Scalable Ordinal Regression Solvers},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273504},
doi = {10.1145/1273496.1273504},
abstract = {In this paper we propose a novel, scalable, clustering based Ordinal Regression formulation, which is an instance of a Second Order Cone Program (SOCP) with one Second Order Cone (SOC) constraint. The main contribution of the paper is a fast algorithm, CB-OR, which solves the proposed formulation more eficiently than general purpose solvers. Another main contribution of the paper is to pose the problem of focused crawling as a large scale Ordinal Regression problem and solve using the proposed CB-OR. Focused crawling is an efficient mechanism for discovering resources of interest on the web. Posing the problem of focused crawling as an Ordinal Regression problem avoids the need for a negative class and topic hierarchy, which are the main drawbacks of the existing focused crawling methods. Experiments on large synthetic and benchmark datasets show the scalability of CB-OR. Experiments also show that the proposed focused crawler outperforms the state-of-the-art.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {57–64},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273505,
author = {Hillel, Aharon Bar and Weinshall, Daphna},
title = {Learning Distance Function by Coding Similarity},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273505},
doi = {10.1145/1273496.1273505},
abstract = {We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is efficient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similarity-preserving projection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {65–72},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273506,
author = {Bhattacharya, Sourangshu and Bhattacharyya, Chiranjib and Chandra, Nagasuma},
title = {Structural Alignment Based Kernels for Protein Structure Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273506},
doi = {10.1145/1273496.1273506},
abstract = {Structural alignments are the most widely used tools for comparing proteins with low sequence similarity. The main contribution of this paper is to derive various kernels on proteins from structural alignments, which do not use sequence information. Central to the kernels is a novel alignment algorithm which matches substructures of fixed size using spectral graph matching techniques. We derive positive semi-definite kernels which capture the notion of similarity between substructures. Using these as base more sophisticated kernels on protein structures are proposed. To empirically evaluate the kernels we used a 40% sequence non-redundant structures from 15 different SCOP superfamilies. The kernels when used with SVMs show competitive performance with CE, a state of the art structure comparison program.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {73–80},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273507,
author = {Bickel, Steffen and Br\"{u}ckner, Michael and Scheffer, Tobias},
title = {Discriminative Learning for Differing Training and Test Distributions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273507},
doi = {10.1145/1273496.1273507},
abstract = {We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {81–88},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273508,
author = {Bordes, Antoine and Bottou, L\'{e}on and Gallinari, Patrick and Weston, Jason},
title = {Solving Multiclass Support Vector Machines with LaRank},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273508},
doi = {10.1145/1273496.1273508},
abstract = {Optimization algorithms for large margin multiclass recognizers are often too costly to handle ambitious problems with structured outputs and exponential numbers of classes. Optimization algorithms that rely on the full gradient are not effective because, unlike the solution, the gradient is not sparse and is very large. The LaRank algorithm sidesteps this difficulty by relying on a randomized exploration inspired by the perceptron algorithm. We show that this approach is competitive with gradient based optimizers on simple multiclass problems. Furthermore, a single LaRank pass over the training examples delivers test error rates that are nearly as good as those of the final solution.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {89–96},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273509,
author = {Bryan, Brent and McMahan, H. Brendan and Schafer, Chad M. and Schneider, Jeff},
title = {Efficiently Computing Minimax Expected-Size Confidence Regions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273509},
doi = {10.1145/1273496.1273509},
abstract = {Given observed data and a collection of parameterized candidate models, a 1 -- α confidence region in parameter space provides useful insight as to those models which are a good fit to the data, all while keeping the probability of incorrect exclusion below α. With complex models, optimally precise procedures (those with small expected size) are, in practice, difficult to derive; one solution is the Minimax Expected-Size (MES) confidence procedure. The key computational problem of MES is computing a minimax equilibria to a certain zero-sum game. We show that this game is convex with bilinear payoffs, allowing us to apply any convex game solver, including linear programming. Exploiting the sparsity of the matrix, along with using fast linear programming software, allows us to compute approximate minimax expected-size confidence regions orders of magnitude faster than previously published methods. We test these approaches by estimating parameters for a cosmological model.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {97–104},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273510,
author = {Bunescu, Razvan C. and Mooney, Raymond J.},
title = {Multiple Instance Learning for Sparse Positive Bags},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273510},
doi = {10.1145/1273496.1273510},
abstract = {We present a new approach to multiple instance learning (MIL) that is particularly effective when the positive bags are sparse (i.e. contain few positive instances). Unlike other SVM-based MIL methods, our approach more directly enforces the desired constraint that at least one of the instances in a positive bag is positive. Using both artificial and real-world data, we experimentally demonstrate that our approach achieves greater accuracy than state-of-the-art MIL methods when positive bags are sparse, and performs competitively when they are not. In particular, our approach is the best performing method for image region classification.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {105–112},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273511,
author = {Busse, Ludwig M. and Orbanz, Peter and Buhmann, Joachim M.},
title = {Cluster Analysis of Heterogeneous Rank Data},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273511},
doi = {10.1145/1273496.1273511},
abstract = {Cluster analysis of ranking data, which occurs in consumer questionnaires, voting forms or other inquiries of preferences, attempts to identify typical groups of rank choices. Empirically measured rankings are often incomplete, i.e. different numbers of filled rank positions cause heterogeneity in the data. We propose a mixture approach for clustering of heterogeneous rank data. Rankings of different lengths can be described and compared by means of a single probabilistic model. A maximum entropy approach avoids hidden assumptions about missing rank positions. Parameter estimators and an efficient EM algorithm for unsupervised inference are derived for the ranking mixture model. Experiments on both synthetic data and real-world data demonstrate significantly improved parameter estimates on heterogeneous data when the incomplete rankings are included in the inference process.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {113–120},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273512,
author = {Cao, Bin and Shen, Dou and Sun, Jian-Tao and Yang, Qiang and Chen, Zheng},
title = {Feature Selection in a Kernel Space},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273512},
doi = {10.1145/1273496.1273512},
abstract = {We address the problem of feature selection in a kernel space to select the most discriminative and informative features for classification and data analysis. This is a difficult problem because the dimension of a kernel space may be infinite. In the past, little work has been done on feature selection in a kernel space. To solve this problem, we derive a basis set in the kernel space as a first step for feature selection. Using the basis set, we then extend the margin-based feature selection algorithms that are proven effective even when many features are dependent. The selected features form a subspace of the kernel space, in which different state-of-the-art classification algorithms can be applied for classification. We conduct extensive experiments over real and simulated data to compare our proposed method with four baseline algorithms. Both theoretical analysis and experimental results validate the effectiveness of our proposed method.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {121–128},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273513,
author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
title = {Learning to Rank: From Pairwise Approach to Listwise Approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273513},
doi = {10.1145/1273496.1273513},
abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative filtering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as 'instances' in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach offers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as 'instances' in learning. The paper proposes a new probabilistic method for the approach. Specifically it introduces two probability models, respectively referred to as permutation probability and top k probability, to define a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {129–136},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273514,
author = {Cazzanti, Luca and Gupta, Maya R.},
title = {Local Similarity Discriminant Analysis},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273514},
doi = {10.1145/1273496.1273514},
abstract = {We propose a local, generative model for similarity-based classification. The method is applicable to the case that only pairwise similarities between samples are available. The classifier models the local class-conditional distribution using a maximum entropy estimate and empirical moment constraints. The resulting exponential class conditional-distributions are combined with class prior probabilities and misclassification costs to form the local similarity discriminant analysis (local SDA) classifier. We compare the performance of local SDA to a non-local version, to the local nearest centroid classifier, the nearest centroid classifier, k-NN, and to the recently-developed potential support vector machine (PSVM). Results show that local SDA is competitive with k-NN and the computationally-demanding PSVM while offering the advantages of a generative classifier.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {137–144},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273515,
author = {Chan, Antoni B. and Vasconcelos, Nuno and Lanckriet, Gert R. G.},
title = {Direct Convex Relaxations of Sparse SVM},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273515},
doi = {10.1145/1273496.1273515},
abstract = {Although support vector machines (SVMs) for binary classification give rise to a decision rule that only relies on a subset of the training data points (support vectors), it will in general be based on all available features in the input space. We propose two direct, novel convex relaxations of a non-convex sparse SVM formulation that explicitly constrains the cardinality of the vector of feature weights. One relaxation results in a quadratically-constrained quadratic program (QCQP), while the second is based on a semidefinite programming (SDP) relaxation. The QCQP formulation can be interpreted as applying an adaptive soft-threshold on the SVM hyperplane, while the SDP formulation learns a weighted inner-product (i.e. a kernel) that results in a sparse hyperplane. Experimental results show an increase in sparsity while conserving the generalization performance compared to a standard as well as a linear programming SVM.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {145–153},
numpages = {9},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273516,
author = {Chen, Xue-wen and Jeong, Jong Cheol},
title = {Minimum Reference Set Based Feature Selection for Small Sample Classifications},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273516},
doi = {10.1145/1273496.1273516},
abstract = {We address feature selection problems for classification of small samples and high dimensionality. A practical example is microarray-based cancer classification problems, where sample size is typically less than 100 and number of features is several thousands or higher. One of the commonly used methods in addressing this problem is recursive feature elimination (RFE) method, which utilizes the generalization capability embedded in support vector machines and is thus suitable for small samples problems. We propose a novel method using minimum reference set (MRS) generated by the nearest neighbor rule. MRS is the set of minimum number of samples that correctly classify all the training samples. It is related to structural risk minimization principle and thus leads to good generalization. The proposed MRS based method is compared to RFE method with several real datasets, and experimental results show that the MRS method produces better classification performance.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {153–160},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273517,
author = {Cheng, Li and Vishwanathan, S. V. N.},
title = {Learning to Compress Images and Videos},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273517},
doi = {10.1145/1273496.1273517},
abstract = {We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale suffice to recover the original image. A similar scheme is also applicable for compressing videos, where a single model can be used to predict color on many consecutive frames, leading to better compression. Existing algorithms for colorization -- the process of adding color to a grayscale image or video sequence -- are tedious, and require intensive human-intervention. We bypass these limitations by using a graph-based inductive semi-supervised learning module for colorization, and a simple active learning strategy to choose the representative pixels. Experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {161–168},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273518,
author = {Cortes, Corinna and Mohri, Mehryar and Rastogi, Ashish},
title = {Magnitude-Preserving Ranking Algorithms},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273518},
doi = {10.1145/1273496.1273518},
abstract = {This paper studies the learning problem of ranking when one wishes not just to accurately predict pairwise ordering but also preserve the magnitude of the preferences or the difference between ratings, a problem motivated by its key importance in the design of search engines, movie recommendation, and other similar ranking systems. We describe and analyze several algorithms for this problem and give stability bounds for their generalization error, extending previously known stability results to non-bipartite ranking and magnitude of preference-preserving algorithms. We also report the results of experiments comparing these algorithms on several datasets and compare these results with those obtained using an algorithm minimizing the pairwise misranking error and standard regression.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {169–176},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273519,
author = {d'Aspremont, Alexandre and Bach, Francis R. and Ghaoui, Laurent El},
title = {Full Regularization Path for Sparse Principal Component Analysis},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273519},
doi = {10.1145/1273496.1273519},
abstract = {Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients, with complexity O(n3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n3). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {177–184},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273520,
author = {Dai, Guang and Yeung, Dit-Yan},
title = {Kernel Selection Forl Semi-Supervised Kernel Machines},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273520},
doi = {10.1145/1273496.1273520},
abstract = {Existing semi-supervised learning methods are mostly based on either the cluster assumption or the manifold assumption. In this paper, we propose an integrated regularization framework for semi-supervised kernel machines by incorporating both the cluster assumption and the manifold assumption. Moreover, it supports kernel learning in the form of kernel selection. The optimization problem involves joint optimization over all the labeled and unlabeled data points, a convex set of basic kernels, and a discrete space of unknown labels for the unlabeled data. When the manifold assumption is incorporated, graph Laplacian kernels are used as the basic kernels for learning an optimal convex combination of graph Laplacian kernels. Comparison with related methods on the USPS data set shows very promising results.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {185–192},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273521,
author = {Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
title = {Boosting for Transfer Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273521},
doi = {10.1145/1273496.1273521},
abstract = {Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund &amp; Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {193–200},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273522,
author = {Davidson, Ian and Ravi, S. S.},
title = {Intractability and Clustering with Constraints},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273522},
doi = {10.1145/1273496.1273522},
abstract = {Clustering with constraints is a developing area of machine learning. Various papers have used constraints to enforce particular clusterings, seed clustering algorithms and even learn distance functions which are then used for clustering. We present intractability results for some constraint combinations and illustrate both formally and experimentally the implications of these results for using constraints with clustering.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {201–208},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273523,
author = {Davis, Jason V. and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S.},
title = {Information-Theoretic Metric Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273523},
doi = {10.1145/1273496.1273523},
abstract = {In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem---that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {209–216},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273524,
author = {Davis, Jesse and Costa, V\'{\i}tor Santos and Ray, Soumya and Page, David},
title = {An Integrated Approach to Feature Invention and Model Construction for Drug Activity Prediction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273524},
doi = {10.1145/1273496.1273524},
abstract = {We present a new machine learning approach for 3D-QSAR, the task of predicting binding affinities of molecules to target proteins based on 3D structure. Our approach predicts binding affinity by using regression on substructures discovered by relational learning. We make two contributions to the state-of-the-art. First, we use multiple-instance (MI) regression, which represents a molecule as a set of 3D conformations, to model activity. Second, the relational learning component employs the "Score As You Use" (SAYU) method to select substructures for their ability to improve the regression model. This is the first application of SAYU to multiple-instance, real-valued prediction. We evaluate our approach on three tasks and demonstrate that (i) SAYU outperforms standard coverage measures when selecting features for regression, (ii) the MI representation improves accuracy over standard single feature-vector encodings and (iii) combining SAYU with MI regression is more accurate for 3D-QSAR than either approach by itself.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {217–224},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273525,
author = {Delage, Erick and Mannor, Shie},
title = {Percentile Optimization in Uncertain Markov Decision Processes with Application to Efficient Exploration},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273525},
doi = {10.1145/1273496.1273525},
abstract = {Markov decision processes are an effective tool in modeling decision-making in uncertain dynamic environments. Since the parameters of these models are typically estimated from data, learned from experience, or designed by hand, it is not surprising that the actual performance of a chosen strategy often significantly differs from the designer's initial expectations due to unavoidable model uncertainty. In this paper, we present a percentile criterion that captures the trade-off between optimistic and pessimistic points of view on MDP with parameter uncertainty. We describe tractable methods that take parameter uncertainty into account in the process of decision making. Finally, we propose a cost-effective exploration strategy when it is possible to invest (money, time or computation efforts) in actions that will reduce the uncertainty in the parameters.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {225–232},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273526,
author = {Dietz, Laura and Bickel, Steffen and Scheffer, Tobias},
title = {Unsupervised Prediction of Citation Influences},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273526},
doi = {10.1145/1273496.1273526},
abstract = {Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {233–240},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273527,
author = {Doll\'{a}r, Piotr and Rabaud, Vincent and Belongie, Serge},
title = {Non-Isometric Manifold Learning: Analysis and an Algorithm},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273527},
doi = {10.1145/1273496.1273527},
abstract = {In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or 'unrolling' of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a representation of a nonlinear, possibly non-isometric manifold that allows for the manipulation of novel points. Central to this view of manifold learning is the concept of generalization beyond the training data. Drawing on concepts from supervised learning, we establish a framework for studying the problems of model assessment, model complexity, and model selection for manifold learning. We present an extension of a recent algorithm, Locally Smooth Manifold Learning (LSML), and show it has good generalization properties. LSML learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances, finding the projection of a point onto a manifold, recovering a manifold from points corrupted by noise, generating novel points on a manifold, and more.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {241–248},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273528,
author = {Dudik, Miroslav and Blei, David M. and Schapire, Robert E.},
title = {Hierarchical Maximum Entropy Density Estimation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273528},
doi = {10.1145/1273496.1273528},
abstract = {We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a real-world application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {249–256},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273529,
author = {Esposito, Roberto and Radicioni, Daniele P.},
title = {CarpeDiem: An Algorithm for the Fast Evaluation of SSL Classifiers},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273529},
doi = {10.1145/1273496.1273529},
abstract = {In this paper we present a novel algorithm, CarpeDiem. It significantly improves on the time complexity of Viterbi algorithm, preserving the optimality of the result. This fact has consequences on Machine Learning systems that use Viterbi algorithm during learning or classification. We show how the algorithm applies to the Supervised Sequential Learning task and, in particular, to the HMPerceptron algorithm. We illustrate CarpeDiem in full details, and provide experimental results that support the proposed approach.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {257–264},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273530,
author = {Farahmand, Amir massoud and Szepesv\'{a}ri, Csaba and Audibert, Jean-Yves},
title = {Manifold-Adaptive Dimension Estimation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273530},
doi = {10.1145/1273496.1273530},
abstract = {Intuitively, learning should be easier when the data points lie on a low-dimensional submanifold of the input space. Recently there has been a growing interest in algorithms that aim to exploit such geometrical properties of the data. Oftentimes these algorithms require estimating the dimension of the manifold first. In this paper we propose an algorithm for dimension estimation and study its finite-sample behaviour. The algorithm estimates the dimension locally around the data points using nearest neighbor techniques and then combines these local estimates. We show that the rate of convergence of the resulting estimate is independent of the dimension of the input space and hence the algorithm is "manifold-adaptive". Thus, when the manifold supporting the data is low dimensional, the algorithm can be exponentially more efficient than its counterparts that are not exploiting this property. Our computer experiments confirm the obtained theoretical results.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {265–272},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273531,
author = {Gelly, Sylvain and Silver, David},
title = {Combining Online and Offline Knowledge in UCT},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273531},
doi = {10.1145/1273496.1273531},
abstract = {The UCT algorithm learns a value function online using sample-based search. The TD(λ) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 x 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 x 9 Go program. Each technique significantly improves MoGo's playing strength.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {273–280},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273532,
author = {Gerber, Samuel and Tasdizen, Tolga and Whitaker, Ross},
title = {Robust Non-Linear Dimensionality Reduction Using Successive 1-Dimensional Laplacian Eigenmaps},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273532},
doi = {10.1145/1273496.1273532},
abstract = {Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the repeated eigendirections problem. We propose a novel approach that combines successive 1-dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {281–288},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273533,
author = {Geurts, Pierre and Wehenkel, Louis and d'Alch\'{e}-Buc, Florence},
title = {Gradient Boosting for Kernelized Output Spaces},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273533},
doi = {10.1145/1273496.1273533},
abstract = {A general framework is proposed for gradient boosting in supervised learning problems where the loss function is defined using a kernel over the output space. It extends boosting in a principled way to complex output spaces (images, text, graphs etc.) and can be applied to a general class of base learners working in kernelized output spaces. Empirical results are provided on three problems: a regression problem, an image completion task and a graph prediction problem. In these experiments, the framework is combined with tree-based base learners, which have interesting algorithmic properties. The results show that gradient boosting significantly improves these base learners and provides competitive results with other tree-based ensemble methods based on randomization.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {289–296},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273534,
author = {Ghavamzadeh, Mohammad and Engel, Yaakov},
title = {Bayesian Actor-Critic Algorithms},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273534},
doi = {10.1145/1273496.1273534},
abstract = {We present a new actor-critic learning model in which a Bayesian class of non-parametric critics, using Gaussian process temporal difference learning is used. Such critics model the state-action value function as a Gaussian process, allowing Bayes' rule to be used in computing the posterior distribution over state-action value functions, conditioned on the observed data. Appropriate choices of the prior covariance (kernel) between state-action values and of the parametrization of the policy allow us to obtain closed-form expressions for the posterior distribution of the gradient of the average discounted return with respect to the policy parameters. The posterior mean, which serves as our estimate of the policy gradient, is used to update the policy, while the posterior covariance allows us to gauge the reliability of the update.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {297–304},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273535,
author = {Globerson, Amir and Koo, Terry Y. and Carreras, Xavier and Collins, Michael},
title = {Exponentiated Gradient Algorithms for Log-Linear Structured Prediction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273535},
doi = {10.1145/1273496.1273535},
abstract = {Conditional log-linear models are a commonly used method for structured prediction. Efficient learning of parameters in these models is therefore an important problem. This paper describes an exponentiated gradient (EG) algorithm for training such models. EG is applied to the convex dual of the maximum likelihood objective; this results in both sequential and parallel update algorithms, where in the sequential algorithm parameters are updated in an online fashion. We provide a convergence proof for both algorithms. Our analysis also simplifies previous results on EG for max-margin models, and leads to a tighter bound on convergence rates. Experiments on a large-scale parsing task show that the proposed algorithm converges much faster than conjugate-gradient and L-BFGS approaches both in terms of optimization objective and test error.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {305–312},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273536,
author = {Grira, Nizar and Houle, Michael E.},
title = {Best of Both: A Hybridized Centroid-Medoid Clustering Heuristic},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273536},
doi = {10.1145/1273496.1273536},
abstract = {Although each iteration of the popular k-Means clustering heuristic scales well to larger problem sizes, it often requires an unacceptably-high number of iterations to converge to a solution. This paper introduces an enhancement of k-Means in which local search is used to accelerate convergence without greatly increasing the average computational cost of the iterations. The local search involves a carefully-controlled number of swap operations resembling those of the more robust k-Medoids clustering heuristic. We show empirically that the proposed method improves convergence results when compared to standard k-Means.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {313–320},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273537,
author = {Guo, Fan and Hanneke, Steve and Fu, Wenjie and Xing, Eric P.},
title = {Recovering Temporally Rewiring Networks: A Model-Based Approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273537},
doi = {10.1145/1273496.1273537},
abstract = {A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of hidden temporal exponential random graph models (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent time-specific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a Drosophila lifecycle gene expression data set, in comparison with a static counterpart of htERGM.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {321–328},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273538,
author = {Gupta, Rahul and Diwan, Ajit A. and Sarawagi, Sunita},
title = {Efficient Inference with Cardinality-Based Clique Potentials},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273538},
doi = {10.1145/1273496.1273538},
abstract = {Many collective labeling tasks require inference on graphical models where the clique potentials depend only on the number of nodes that get a particular label. We design efficient inference algorithms for various families of such potentials. Our algorithms are exact for arbitrary cardinality-based clique potentials on binary labels and for max-like and majority-like clique potentials on multiple labels. Moving towards more complex potentials, we show that inference becomes NP-hard even on cliques with homogeneous Potts potentials. We present a 13/15-approximation algorithm with runtime sub-quadratic in the clique size. In contrast, the best known previous guarantee for graphs with Potts potentials is only 0.5. We perform empirical comparisons on real and synthetic data, and show that our proposed methods are an order of magnitude faster than the well-known Tree-based re-parameterization (TRW) and graph-cut algorithms.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {329–336},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273539,
author = {H\'{e}rault, Romain and Grandvalet, Yves},
title = {Sparse Probabilistic Classifiers},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273539},
doi = {10.1145/1273496.1273539},
abstract = {The scores returned by support vector machines are often used as a confidence measures in the classification of new examples. However, there is no theoretical argument sustaining this practice. Thus, when classification uncertainty has to be assessed, it is safer to resort to classifiers estimating conditional probabilities of class labels. Here, we focus on the ambiguity in the vicinity of the boundary decision. We propose an adaptation of maximum likelihood estimation, instantiated on logistic regression. The model outputs proper conditional probabilities into a user-defined interval and is less precise elsewhere. The model is also sparse, in the sense that few examples contribute to the solution. The computational efficiency is thus improved compared to logistic regression. Furthermore, preliminary experiments show improvements over standard logistic regression and performances similar to support vector machines.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {337–344},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273540,
author = {Haider, Peter and Brefeld, Ulf and Scheffer, Tobias},
title = {Supervised Clustering of Streaming Data for Email Batch Detection},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273540},
doi = {10.1145/1273496.1273540},
abstract = {We address the problem of detecting batches of emails that have been created according to the same template. This problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages. The application matches the problem setting of supervised clustering, because examples of correct clusterings can be collected. Known decoding procedures for supervised clustering are cubic in the number of instances. When decisions cannot be reconsidered once they have been made --- owing to the streaming nature of the data --- then the decoding problem can be solved in linear time. We devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering. We study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {345–352},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273541,
author = {Hanneke, Steve},
title = {A Bound on the Label Complexity of Agnostic Active Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273541},
doi = {10.1145/1273496.1273541},
abstract = {We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the A2 algorithm proposed by Balcan, Beygelzimer &amp; Langford (Balcan et al., 2006). This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {353–360},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273542,
author = {Hoi, Steven C. H. and Jin, Rong and Lyu, Michael R.},
title = {Learning Nonparametric Kernel Matrices from Pairwise Constraints},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273542},
doi = {10.1145/1273496.1273542},
abstract = {Many kernel learning methods have to assume parametric forms for the target kernel functions, which significantly limits the capability of kernels in fitting diverse patterns. Some kernel learning methods assume the target kernel matrix to be a linear combination of parametric kernel matrices. This assumption again importantly limits the flexibility of the target kernel matrices. The key challenge with nonparametric kernel learning arises from the difficulty in linking the nonparametric kernels to the input patterns. In this paper, we resolve this problem by introducing the graph Laplacian of the observed data as a regularizer when optimizing the kernel matrix with respect to the pairwise constraints. We formulate the problem into Semi-Definite Programs (SDP), and propose an efficient algorithm to solve the SDP problem. The extensive evaluation on clustering with pairwise constraints shows that the proposed nonparametric kernel learning method is more effective than other state-of-the-art kernel learning techniques.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {361–368},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273543,
author = {Jaeger, Manfred},
title = {Parameter Learning for Relational Bayesian Networks},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273543},
doi = {10.1145/1273496.1273543},
abstract = {We present a method for parameter learning in relational Bayesian networks (RBNs). Our approach consists of compiling the RBN model into a computation graph for the likelihood function, and to use this likelihood graph to perform the necessary computations for a gradient ascent likelihood optimization procedure. The method can be applied to all RBN models that only contain differentiable combining rules. This includes models with non-decomposable combining rules, as well as models with weighted combinations or nested occurrences of combining rules. Experimental results on artificial random graph data explores the feasibility of the approach both for complete and incomplete data.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {369–376},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273544,
author = {Ji, Shihao and Carin, Lawrence},
title = {Bayesian Compressive Sensing and Projection Optimization},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273544},
doi = {10.1145/1273496.1273544},
abstract = {This paper introduces a new problem for which machine-learning tools may make an impact. The problem considered is termed "compressive sensing", in which a real signal of dimension N is measured accurately based on K &lt;&lt; N real measurements. This is achieved under the assumption that the underlying signal has a sparse representation in some basis (e.g., wavelets). In this paper we demonstrate how techniques developed in machine learning, specifically sparse Bayesian regression and active learning, may be leveraged to this new problem. We also point out future research directions in compressive sensing of interest to the machine-learning community.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {377–384},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273545,
author = {Johns, Jeff and Mahadevan, Sridhar},
title = {Constructing Basis Functions from Directed Graphs for Value Function Approximation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273545},
doi = {10.1145/1273496.1273545},
abstract = {Basis functions derived from an undirected graph connecting nearby samples from a Markov decision process (MDP) have proven useful for approximating value functions. The success of this technique is attributed to the smoothness of the basis functions with respect to the state space geometry. This paper explores the properties of bases created from directed graphs which are a more natural fit for expressing state connectivity. Digraphs capture the effect of non-reversible MDPs whose value functions may not be smooth across adjacent states. We provide an analysis using the Dirichlet sum of the directed graph Laplacian to show how the smoothness of the basis functions is affected by the graph's invariant distribution. Experiments in discrete and continuous MDPs with non-reversible actions demonstrate a significant improvement in the policies learned using directed graph bases.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {385–392},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273546,
author = {Kersting, Kristian and Plagemann, Christian and Pfaff, Patrick and Burgard, Wolfram},
title = {Most Likely Heteroscedastic Gaussian Process Regression},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273546},
doi = {10.1145/1273496.1273546},
abstract = {This paper presents a novel Gaussian process (GP) approach to regression with input-dependent noise rates. We follow Goldberg et al.'s approach and model the noise variance using a second GP in addition to the GP governing the noise-free output value. In contrast to Goldberg et al., however, we do not use a Markov chain Monte Carlo method to approximate the posterior noise variance but a most likely noise approach. The resulting model is easy to implement and can directly be used in combination with various existing extensions of the standard GPs such as sparse approximations. Extensive experiments on both synthetic and real-world data, including a challenging perception problem in robotics, show the effectiveness of most likely heteroscedastic GP regression.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {393–400},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273547,
author = {Kim, Kye-Hyeon and Choi, Seungjin},
title = {Neighbor Search with Global Geometry: A Minimax Message Passing Algorithm},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273547},
doi = {10.1145/1273496.1273547},
abstract = {Neighbor search is a fundamental task in machine learning, especially in classification and retrieval. Efficient nearest neighbor search methods have been widely studied, with their emphasis on data structures but most of them did not consider the underlying global geometry of a data set. Recent graph-based semi-supervised learning methods capture the global geometry, but suffer from scalability and parameter tuning problems. In this paper we present a (nearest) neighbor search method where the underlying global geometry is incorporated and the parameter tuning is not required. To this end, we introduce deterministic walks as a deterministic counterpart of Markov random walks, leading us to use the minimax distance as a global dissimilarity measure. Then we develop a message passing algorithm for efficient minimax distance calculation, which scales linearly in both time and space. Empirical study reveals the useful behavior of the method in image retrieval and semi-supervised learning.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {401–408},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273548,
author = {Kim, Minyoung and Pavlovic, Vladimir},
title = {A Recursive Method for Discriminative Mixture Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273548},
doi = {10.1145/1273496.1273548},
abstract = {We consider the problem of learning density mixture models for classification. Traditional learning of mixtures for density estimation focuses on models that correctly represent the density at all points in the sample space. Discriminative learning, on the other hand, aims at representing the density at the decision boundary. We introduce a novel discriminative learning method for mixtures of generative models. Unlike traditional discriminative learning methods that often resort to computationally demanding gradient search optimization, the proposed method is highly efficient as it reduces to generative learning of individual mixture components on weighted data. Hence it is particularly suited to domains with complex component models, such as hidden Markov models or Bayesian networks in general, that are usually too complex for effective gradient search. We demonstrate the benefits of the proposed method in a comprehensive set of evaluations on time-series sequence classification problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {409–416},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273549,
author = {Kirshner, Sergey and Smyth, Padhraic},
title = {Infinite Mixtures of Trees},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273549},
doi = {10.1145/1273496.1273549},
abstract = {Finite mixtures of tree-structured distributions have been shown to be efficient and effective in modeling multivariate distributions. Using Dirichlet processes, we extend this approach to allow countably many tree-structured mixture components. The resulting Bayesian framework allows us to deal with the problem of selecting the number of mixture components by computing the posterior distribution over the number of components and integrating out the components by Bayesian model averaging. We apply the proposed framework to identify the number and the properties of predominant precipitation patterns in historical archives of climate data.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {417–423},
numpages = {7},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273550,
author = {Klami, Arto and Kaski, Samuel},
title = {Local Dependent Components},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273550},
doi = {10.1145/1273496.1273550},
abstract = {We introduce a mixture of probabilistic canonical correlation analyzers model for analyzing local correlations, or more generally mutual statistical dependencies, in cooccurring data pairs. The model extends the traditional canonical correlation analysis and its probabilistic interpretation in three main ways. First, a full Bayesian treatment enables analysis of small samples (large p, small n, a crucial problem in bioinformatics, for instance), and rigorous estimation of the degree of dependency and independency. Secondly, the mixture formulation generalizes the method from global linearity to the more reasonable assumption of different kinds of dependencies for different kinds of data. As a third novel extension the method decomposes the variation in the data into shared and data set-specific components.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {425–432},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273551,
author = {Kok, Stanley and Domingos, Pedro},
title = {Statistical Predicate Invention},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273551},
doi = {10.1145/1273496.1273551},
abstract = {We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the objects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {433–440},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273552,
author = {Kr\"{a}mer, Nicole and Braun, Mikio L.},
title = {Kernelizing PLS, Degrees of Freedom, and Efficient Model Selection},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273552},
doi = {10.1145/1273496.1273552},
abstract = {Kernelizing partial least squares (PLS), an algorithm which has been particularly popular in chemometrics, leads to kernel PLS which has several interesting properties, including a sub-cubic runtime for learning, and an iterative construction of directions which are relevant for predicting the outputs. We show that the kernelization of PLS introduces interesting properties not found in ordinary PLS, giving novel insights into the workings of kernel PLS and the connections to kernel ridge regression and conjugate gradient descent methods. Furthermore, we show how to correctly define the degrees of freedom for kernel PLS and how to efficiently compute an unbiased estimate. Finally, we address the practical problem of model selection. We demonstrate how to use the degrees of freedom estimate to perform effective model selection, and discuss how to implement crossvalidation schemes efficiently.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {441–448},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273553,
author = {Krause, Andreas and Guestrin, Carlos},
title = {Nonmyopic Active Learning of Gaussian Processes: An Exploration-Exploitation Approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273553},
doi = {10.1145/1273496.1273553},
abstract = {When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {449–456},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273554,
author = {Kropotov, Dmitry and Vetrov, Dmitry},
title = {On One Method of Non-Diagonal Regularization in Sparse Bayesian Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273554},
doi = {10.1145/1273496.1273554},
abstract = {In the paper we propose a new type of regularization procedure for training sparse Bayesian methods for classification. Transforming Hessian matrix of log-likelihood function to diagonal form with further regularization of its eigenvectors allows us to optimize evidence explicitly as a product of one-dimensional integrals. The process of automatic regularization coefficients determination then converges in one iteration. We show how to use the proposed approach for Gaussian and Laplace priors. Both algorithms show comparable performance with the state-of-the-art Relevance Vector Machines (RVM) but require less time for training and produce more sparse decision rules (in terms of degrees of freedom).},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {457–464},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273555,
author = {Kuzmin, Dima and Warmuth, Manfred K.},
title = {Online Kernel PCA with Entropic Matrix Updates},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273555},
doi = {10.1145/1273496.1273555},
abstract = {A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can be kernelized. This is important because the bounds provable for these algorithms are logarithmic in the feature dimension (provided that the 2-norm of feature vectors is bounded by a constant). The main problem we focus on is the kernelization of an online PCA algorithm which belongs to this family of updates.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {465–472},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273556,
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273556},
doi = {10.1145/1273496.1273556},
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {473–480},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273557,
author = {Lawrence, Neil D. and Moore, Andrew J.},
title = {Hierarchical Gaussian Process Latent Variable Models},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273557},
doi = {10.1145/1273496.1273557},
abstract = {The Gaussian process latent variable model (GP-LVM) is a powerful approach for probabilistic modelling of high dimensional data through dimensional reduction. In this paper we extend the GP-LVM through hierarchies. A hierarchical model (such as a tree) allows us to express conditional independencies in the data as well as the manifold structure. We first introduce Gaussian process hierarchies through a simple dynamical model, we then extend the approach to a more complex hierarchy which is applied to the visualisation of human motion data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {481–488},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273558,
author = {Lee, Su-In and Chatalbashev, Vassil and Vickrey, David and Koller, Daphne},
title = {Learning a Meta-Level Prior for Feature Relevance from Multiple Related Tasks},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273558},
doi = {10.1145/1273496.1273558},
abstract = {In many prediction tasks, selecting relevant features is essential for achieving good generalization performance. Most feature selection algorithms consider all features to be a priori equally likely to be relevant. In this paper, we use transfer learning---learning on an ensemble of related tasks---to construct an informative prior on feature relevance. We assume that features themselves have meta-features that are predictive of their relevance to the prediction task, and model their relevance as a function of the meta-features using hyperparameters (called meta-priors). We present a convex optimization algorithm for simultaneously learning the meta-priors and feature weights from an ensemble of related prediction tasks which share a similar relevance structure. Our approach transfers the "meta-priors" among different tasks, which makes it possible to deal with settings where tasks have nonoverlapping features or the relevance of the features vary over the tasks. We show that learning feature relevance improves performance on two real data sets which illustrate such settings: (1) predicting ratings in a collaborative filtering task, and (2) distinguishing arguments of a verb in a sentence.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {489–496},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273559,
author = {Leskovec, Jure and Faloutsos, Christos},
title = {Scalable Modeling of Real Graphs Using Kronecker Multiplication},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273559},
doi = {10.1145/1273496.1273559},
abstract = {Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use "Kronecker graphs", which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {497–504},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273560,
author = {Li, Bin and Chi, Mingmin and Fan, Jianping and Xue, Xiangyang},
title = {Support Cluster Machine},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273560},
doi = {10.1145/1273496.1273560},
abstract = {For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {505–512},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273561,
author = {Li, Fuxin and Yang, Jian and Wang, Jue},
title = {A Transductive Framework of Distance Metric Learning by Spectral Dimensionality Reduction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273561},
doi = {10.1145/1273496.1273561},
abstract = {Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 105 samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {513–520},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273562,
author = {Ding, Chris and Li, Tao},
title = {Adaptive Dimension Reduction Using Discriminant Analysis and <i>K</i>-Means Clustering},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273562},
doi = {10.1145/1273496.1273562},
abstract = {We combine linear discriminant analysis (LDA) and K-means clustering into a coherent framework to adaptively select the most discriminative subspace. We use K-means clustering to generate class labels and use LDA to do subspace selection. The clustering process is thus integrated with the subspace selection process and the data are then simultaneously clustered while the feature subspaces are selected. We show the rich structure of the general LDA-Km framework by examining its variants and their relationships to earlier approaches. Relations among PCA, LDA, K-means are clarified. Extensive experimental results on real-world datasets show the effectiveness of our approach.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {521–528},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273563,
author = {Li, Wenye and Lee, Kin-Hong and Leung, Kwong-Sak},
title = {Large-Scale RLSC Learning without Agony},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273563},
doi = {10.1145/1273496.1273563},
abstract = {The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrix-vector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating projection framework. We will report signi ficant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {529–536},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273564,
author = {Li, Xin and Cheung, William K. W. and Liu, Jiming and Wu, Zhili},
title = {A Novel Orthogonal NMF-Based Belief Compression for POMDPs},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273564},
doi = {10.1145/1273496.1273564},
abstract = {High dimensionality of POMDP's belief state space is one major cause that makes the underlying optimal policy computation intractable. Belief compression refers to the methodology that projects the belief state space to a low-dimensional one to alleviate the problem. In this paper, we propose a novel orthogonal non-negative matrix factorization (O-NMF) for the projection. The proposed O-NMF not only factors the belief state space by minimizing the reconstruction error, but also allows the compressed POMDP formulation to be efficiently computed (due to its orthogonality) in a value-directed manner so that the value function will take same values for corresponding belief states in the original and compressed state spaces. We have tested the proposed approach using a number of benchmark problems and the empirical results confirms its effectiveness in achieving substantial computational cost saving in policy computation.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {537–544},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273565,
author = {Liang, Percy and Jordan, Michael I. and Taskar, Ben},
title = {A Permutation-Augmented Sampler for DP Mixture Models},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273565},
doi = {10.1145/1273496.1273565},
abstract = {We introduce a new inference algorithm for Dirichlet process mixture models. While Gibbs sampling and variational methods focus on local moves, the new algorithm makes more global moves. This is done by introducing a permutation of the data points as an auxiliary variable. The algorithm is a blocked sampler which alternates between sampling the clustering and sampling the permutation. The key to the efficiency of this approach is that it is possible to use dynamic programming to consider all exponentially many clusterings consistent with a given permutation. We also show that random projections can be used to effectively sample the permutation. The result is a stochastic hill-climbing algorithm that yields burn-in times significantly smaller than those of collapsed Gibbs sampling.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {545–552},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273566,
author = {Liao, Xuejun and Li, Hui and Carin, Lawrence},
title = {Quadratically Gated Mixture of Experts for Incomplete Data Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273566},
doi = {10.1145/1273496.1273566},
abstract = {We introduce quadratically gated mixture of experts (QGME), a statistical model for multi-class nonlinear classification. The QGME is formulated in the setting of incomplete data, where the data values are partially observed. We show that the missing values entail joint estimation of the data manifold and the classifier, which allows adaptive imputation during classifier learning. The expectation maximization (EM) algorithm is derived for joint likelihood maximization, with adaptive imputation performed analytically in the E-step. The performance of QGME is evaluated on three benchmark data sets and the results show that the QGME yields significant improvements over competing methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {553–560},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273567,
author = {Lin, Chih-Jen and Weng, Ruby C. and Keerthi, S. Sathiya},
title = {Trust Region Newton Methods for Large-Scale Logistic Regression},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273567},
doi = {10.1145/1273496.1273567},
abstract = {Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {561–568},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273568,
author = {Long, Bo and Zhang, Zhongfei (Mark) and Wu, Xiaoyun and Yu, Philip S.},
title = {Relational Clustering by Symmetric Convex Coding},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273568},
doi = {10.1145/1273496.1273568},
abstract = {Relational data appear frequently in many machine learning applications. Relational data consist of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects, and are usually stored in relation matrices and typically no other knowledge is available. Although relational clustering can be formulated as graph partitioning in some applications, this formulation is not adequate for general relational data. In this paper, we propose a general model for relational clustering based on symmetric convex coding. The model is applicable to all types of relational data and unifies the existing graph partitioning formulation. Under this model, we derive two alternative bound optimization algorithms to solve the symmetric convex coding under two popular distance functions, Euclidean distance and generalized I-divergence. Experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {569–576},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273569,
author = {Ma, Yong and Lao, Shihong and Takikawa, Erina and Kawade, Masato},
title = {Discriminant Analysis in Correlation Similarity Measure Space},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273569},
doi = {10.1145/1273496.1273569},
abstract = {Correlation is one of the most widely used similarity measures in machine learning like Euclidean and Mahalanobis distances. However, compared with proposed numerous discriminant learning algorithms in distance metric space, only a very little work has been conducted on this topic using correlation similarity measure. In this paper, we propose a novel discriminant learning algorithm in correlation measure space, Correlation Discriminant Analysis (CDA). In this framework, based on the definitions of within-class correlation and between-class correlation, the optimum transformation can be sought for to maximize the difference between them, which is in accordance with good classification performance empirically. Under different cases of the transformation, different implementations of the algorithm are given. Extensive empirical evaluations of CDA demonstrate its advantage over alternative methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {577–584},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273570,
author = {Mahadevan, Sridhar},
title = {Adaptive Mesh Compression in 3D Computer Graphics Using Multiscale Manifold Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273570},
doi = {10.1145/1273496.1273570},
abstract = {This paper investigates compression of 3D objects in computer graphics using manifold learning. Spectral compression uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging application domain: object models can have &gt; 105 vertices, and reliably computing the basis functions on large graphs is numerically challenging. In this paper, we introduce a novel multiscale manifold learning approach to 3D mesh compression using diffusion wavelets, a general extension of wavelets to graphs with arbitrary topology. Unlike the "global" nature of Laplacian bases, diffusion wavelet bases are compact, and multiscale in nature. We decompose large graphs using a fast graph partitioning method, and combine local multiscale wavelet bases computed on each subgraph. We present results showing that multiscale diffusion wavelets bases are superior to the Laplacian bases for adaptive compression of large 3D objects.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {585–592},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273571,
author = {Mann, Gideon S. and McCallum, Andrew},
title = {Simple, Robust, Scalable Semi-Supervised Learning via Expectation Regularization},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273571},
doi = {10.1145/1273496.1273571},
abstract = {Although semi-supervised learning has been an active area of research, its use in deployed applications is still relatively rare because the methods are often difficult to implement, fragile in tuning, or lacking in scalability. This paper presents expectation regularization, a semi-supervised learning method for exponential family parametric models that augments the traditional conditional label-likelihood objective function with an additional term that encourages model predictions on unlabeled data to match certain expectations---such as label priors. The method is extremely easy to implement, scales as well as logistic regression, and can handle non-independent features. We present experiments on five different data sets, showing accuracy improvements over other semi-supervised methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {593–600},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273572,
author = {Marthi, Bhaskara},
title = {Automatic Shaping and Decomposition of Reward Functions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273572},
doi = {10.1145/1273496.1273572},
abstract = {This paper investigates the problem of automatically learning how to restructure the reward function of a Markov decision process so as to speed up reinforcement learning. We begin by describing a method that learns a shaped reward function given a set of state and temporal abstractions. Next, we consider decomposition of the per-timestep reward in multieffector problems, in which the overall agent can be decomposed into multiple units that are concurrently carrying out various tasks. We show by example that to find a good reward decomposition, it is often necessary to first shape the rewards appropriately. We then give a function approximation algorithm for solving both problems together. Standard reinforcement learning algorithms can be augmented with our methods, and we show experimentally that in each case, significantly faster learning results.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {601–608},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273573,
author = {Masnadi-Shirazi, Hamed and Vasconcelos, Nuno},
title = {Asymmetric Boosting},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273573},
doi = {10.1145/1273496.1273573},
abstract = {A cost-sensitive extension of boosting, denoted as asymmetric boosting, is presented. Unlike previous proposals, the new algorithm is derived from sound decision-theoretic principles, which exploit the statistical interpretation of boosting to determine a principled extension of the boosting loss. Similarly to AdaBoost, the cost-sensitive extension minimizes this loss by gradient descent on the functional space of convex combinations of weak learners, and produces large margin detectors. It is shown that asymmetric boosting is fully compatible with AdaBoost, in the sense that it becomes the latter when errors are weighted equally. Experimental evidence is provided to demonstrate the claims of cost-sensitivity and large margin. The algorithm is also applied to the computer vision problem of face detection, where it is shown to outperform a number of previous heuristic proposals for cost-sensitive boosting (AdaCost, CSB0, CSB1, CSB2, asymmetric-AdaBoost, AdaC1, AdaC2 and AdaC3).},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {609–619},
numpages = {11},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273574,
author = {McNeill, Graham and Vijayakumar, Sethu},
title = {Linear and Nonlinear Generative Probabilistic Class Models for Shape Contours},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273574},
doi = {10.1145/1273496.1273574},
abstract = {We introduce a robust probabilistic approach to modeling shape contours based on a lowdimensional, nonlinear latent variable model. In contrast to existing techniques that use objective functions in data space without explicit noise models, we are able to extract complex shape variation from noisy data. Most approaches to learning shape models slide observed data points around fixed contours and hence, require a correctly labeled 'reference shape' to prevent degenerate solutions. In our method, unobserved curves are reparameterized to explain the fixed data points, so this problem does not arise. The proposed algorithms are suitable for use with arbitrary basis functions and are applicable to both open and closed shapes; their effectiveness is demonstrated through illustrative examples, quantitative assessment on benchmark data sets and a visualization task.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {617–624},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273575,
author = {Mihalkova, Lilyana and Mooney, Raymond J.},
title = {Bottom-up Learning of Markov Logic Network Structure},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273575},
doi = {10.1145/1273496.1273575},
abstract = {Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a "propositional" Markov network learning method to construct "template" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {625–632},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273576,
author = {Mimno, David and Li, Wei and McCallum, Andrew},
title = {Mixtures of Hierarchical Topics with Pachinko Allocation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273576},
doi = {10.1145/1273496.1273576},
abstract = {The four-level pachinko allocation model (PAM) (Li &amp; McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM---an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLDA's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {633–640},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273577,
author = {Mnih, Andriy and Hinton, Geoffrey},
title = {Three New Graphical Models for Statistical Language Modelling},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273577},
doi = {10.1145/1273496.1273577},
abstract = {The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {641–648},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273578,
author = {Moschitti, Alessandro and Zanzotto, Fabio Massimo},
title = {Fast and Effective Kernels for Relational Learning from Texts},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273578},
doi = {10.1145/1273496.1273578},
abstract = {In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {649–656},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273579,
author = {Mosci, Sofia and Rosasco, Lorenzo and Verri, Alessandro},
title = {Dimensionality Reduction and Generalization},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273579},
doi = {10.1145/1273496.1273579},
abstract = {In this paper we investigate the regularization property of Kernel Principal Component Analysis (KPCA), by studying its application as a preprocessing step to supervised learning problems. We show that performing KPCA and then ordinary least squares on the projected data, a procedure known as kernel principal component regression (KPCR), is equivalent to spectral cut-off regularization, the regularization parameter being exactly the number of principal components to keep. Using probabilistic estimates for integral operators we can prove error estimates for KPCR and propose a parameter choice procedure allowing to prove consistency of the algorithm.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {657–664},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273580,
author = {Mylonakis, Markos and Sima'an, Khalil and Hwa, Rebecca},
title = {Unsupervised Estimation for Noisy-Channel Models},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273580},
doi = {10.1145/1273496.1273580},
abstract = {Shannon's Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize the original message and a channel model to describe the channel's corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisy-channel. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {665–672},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273581,
author = {Nelson, Blaine and Cohen, Ira},
title = {Revisiting Probabilistic Models for Clustering with Pair-Wise Constraints},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273581},
doi = {10.1145/1273496.1273581},
abstract = {We revisit recently proposed algorithms for probabilistic clustering with pair-wise constraints between data points. We evaluate and compare existing techniques in terms of robustness to misspecified constraints. We show that the technique that strictly enforces the given constraints, namely the chunklet model, produces poor results even under a small number of misspecified constraints. We further show that methods that penalize constraint violation are more robust to misspecified constraints but have undesirable local behaviors. Based on this evaluation, we propose a new learning technique, extending the chunklet model to allow soft constraints represented by an intuitive measure of confidence in the constraint.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {673–680},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273582,
author = {Nguyen, Nam and Guo, Yunsong},
title = {Comparisons of Sequence Labeling Algorithms and Extensions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273582},
doi = {10.1145/1273496.1273582},
abstract = {In this paper, we survey the current state-of-art models for structured learning problems, including Hidden Markov Model (HMM), Conditional Random Fields (CRF), Averaged Perceptron (AP), Structured SVMs (SVMstruct), Max Margin Markov Networks (M3N), and an integration of search and learning algorithm (SEARN). With all due tuning efforts of various parameters of each model, on the data sets we have applied the models to, we found that SVMstruct enjoys better performance compared with the others. In addition, we also propose a new method which we call the Structured Learning Ensemble (SLE) to combine these structured learning models. Empirical results show that our SLE algorithm provides more accurate solutions compared with the best results of the individual models.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273583,
author = {Ni, Kai and Carin, Lawrence and Dunson, David},
title = {Multi-Task Learning for Sequential Data via IHMMs and the Nested Dirichlet Process},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273583},
doi = {10.1145/1273496.1273583},
abstract = {A new hierarchical nonparametric Bayesian model is proposed for the problem of multitask learning (MTL) with sequential data. Sequential data are typically modeled with a hidden Markov model (HMM), for which one often must choose an appropriate model structure (number of states) before learning. Here we model sequential data from each task with an infinite hidden Markov model (iHMM), avoiding the problem of model selection. The MTL for iHMMs is implemented by imposing a nested Dirichlet process (nDP) prior on the base distributions of the iHMMs. The nDP-iHMM MTL method allows us to perform task-level clustering and data-level clustering simultaneously, with which the learning for individual iHMMs is enhanced and between-task similarities are learned. Learning and inference for the nDP-iHMM MTL are based on a Gibbs sampler. The effectiveness of the framework is demonstrated using synthetic data as well as real music data.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {689–696},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273584,
author = {Nilsson, Jens and Sha, Fei and Jordan, Michael I.},
title = {Regression on Manifolds Using Kernel Dimension Reduction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273584},
doi = {10.1145/1273496.1273584},
abstract = {We study the problem of discovering a manifold that best preserves information relevant to a nonlinear regression. Solving this problem involves extending and uniting two threads of research. On the one hand, the literature on sufficient dimension reduction has focused on methods for finding the best linear subspace for nonlinear regression; we extend this to manifolds. On the other hand, the literature on manifold learning has focused on unsupervised dimensionality reduction; we extend this to the supervised setting. Our approach to solving the problem involves combining the machinery of kernel dimension reduction with Laplacian eigenmaps. Specifically, we optimize cross-covariance operators in kernel feature spaces that are induced by the normalized graph Laplacian. The result is a highly flexible method in which no strong assumptions are made on the regression function or on the distribution of the covariates. We illustrate our methodology on the analysis of global temperature data and image manifolds.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {697–704},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273585,
author = {Osentoski, Sarah and Mahadevan, Sridhar},
title = {Learning State-Action Basis Functions for Hierarchical MDPs},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273585},
doi = {10.1145/1273496.1273585},
abstract = {This paper introduces a new approach to action-value function approximation by learning basis functions from a spectral decomposition of the state-action manifold. This paper extends previous work on using Laplacian bases for value function approximation by using the actions of the agent as part of the representation when creating basis functions. The approach results in a nonlinear learned representation particularly suited to approximating action-value functions, without incurring the wasteful duplication of state bases in previous work. We discuss two techniques to create state-action graphs: off-policy and on-policy. We show that these graphs have a greater expressive power and have better performance over state-based Laplacian basis functions in domains modeled as Semi-Markov Decision Processes (SMDPs). We present a simple graph partitioning method to scale the approach to large discrete MDPs.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {705–712},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273586,
author = {P, Yogananda A and Murthy, M Narasimha and Gopal, Lakshmi},
title = {A Fast Linear Separability Test by Projection of Positive Points on Subspaces},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273586},
doi = {10.1145/1273496.1273586},
abstract = {A geometric and non parametric procedure for testing if two finite set of points are linearly separable is proposed. The Linear Separability Test is equivalent to a test that determines if a strictly positive point h &gt; 0 exists in the range of a matrix A (related to the points in the two finite sets). The algorithm proposed in the paper iteratively checks if a strictly positive point exists in a subspace by projecting a strictly positive vector with equal co-ordinates (p), on the subspace. At the end of each iteration, the subspace is reduced to a lower dimensional subspace. The test is completed within r ≤ min(n, d + 1) steps, for both linearly separable and non separable problems (r is the rank of A, n is the number of points and d is the dimension of the space containing the points). The worst case time complexity of the algorithm is O(nr3) and space complexity of the algorithm is O(nd). A small review of some of the prominent algorithms and their time complexities is included. The worst case computational complexity of our algorithm is lower than the worst case computational complexity of Simplex, Perceptron, Support Vector Machine and Convex Hull Algorithms, if d<n2/3.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {713–720},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273587,
author = {Pandey, Sandeep and Chakrabarti, Deepayan and Agarwal, Deepak},
title = {Multi-Armed Bandit Problems with Dependent Arms},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273587},
doi = {10.1145/1273496.1273587},
abstract = {We provide a framework to exploit dependencies among arms in multi-armed bandit problems, when the dependencies are in the form of a generative model on clusters of arms. We find an optimal MDP-based policy for the discounted reward case, and also give an approximation of it with formal error guarantee. We discuss lower bounds on regret in the undiscounted reward scenario, and propose a general two-level bandit policy for it. We propose three different instantiations of our general policy and provide theoretical justifications of how the regret of the instantiated policies depend on the characteristics of the clusters. Finally, we empirically demonstrate the efficacy of our policies on large-scale real-world and synthetic data, and show that they significantly outperform classical policies designed for bandits with independent arms.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {721–728},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273588,
author = {Parker, Charles and Fern, Alan and Tadepalli, Prasad},
title = {Learning for Efficient Retrieval of Structured Data with Noisy Queries},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273588},
doi = {10.1145/1273496.1273588},
abstract = {Increasingly large collections of structured data necessitate the development of efficient, noise-tolerant retrieval tools. In this work, we consider this issue and describe an approach to learn a similarity function that is not only accurate, but that also increases the effectiveness of retrieval data structures. We present an algorithm that uses functional gradient boosting to maximize both retrieval accuracy and the retrieval efficiency of vantage point trees. We demonstrate the effectiveness of our approach on two datasets, including a moderately sized real-world dataset of folk music.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {729–736},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273589,
author = {Parr, Ronald and Painter-Wakefield, Christopher and Li, Lihong and Littman, Michael},
title = {Analyzing Feature Generation for Value-Function Approximation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273589},
doi = {10.1145/1273496.1273589},
abstract = {We analyze a simple, Bellman-error-based approach to generating basis functions for value-function approximation. We show that it generates orthogonal basis functions that provably tighten approximation error bounds. We also illustrate the use of this approach in the presence of noise on some sample problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {737–744},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273590,
author = {Peters, Jan and Schaal, Stefan},
title = {Reinforcement Learning by Reward-Weighted Regression for Operational Space Control},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273590},
doi = {10.1145/1273496.1273590},
abstract = {Many robot control problems of practical importance, including operational space control, can be reformulated as immediate reward reinforcement learning problems. However, few of the known optimization or reinforcement learning algorithms can be used in online learning control for robots, as they are either prohibitively slow, do not scale to interesting domains of complex robots, or require trying out policies generated by random search, which are infeasible for a physical system. Using a generalization of the EM-base reinforcement learning framework suggested by Dayan &amp; Hinton, we reduce the problem of learning with immediate rewards to a reward-weighted regression problem with an adaptive, integrated reward transformation for faster convergence. The resulting algorithm is efficient, learns smoothly without dangerous jumps in solution space, and works well in applications of complex high degree-of-freedom robots.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {745–750},
numpages = {6},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273591,
author = {Phua, Chee Wee and Fitch, Robert},
title = {Tracking Value Function Dynamics to Improve Reinforcement Learning with Piecewise Linear Function Approximation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273591},
doi = {10.1145/1273496.1273591},
abstract = {Reinforcement learning algorithms can become unstable when combined with linear function approximation. Algorithms that minimize the mean-square Bellman error are guaranteed to converge, but often do so slowly or are computationally expensive. In this paper, we propose to improve the convergence speed of piecewise linear function approximation by tracking the dynamics of the value function with the Kalman filter using a random-walk model. We cast this as a general framework in which we implement the TD, Q-Learning and MAXQ algorithms for different domains, and report empirical results demonstrating improved learning speed over previous methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {751–758},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273592,
author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
title = {Self-Taught Learning: Transfer Learning from Unlabeled Data},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273592},
doi = {10.1145/1273496.1273592},
abstract = {We present a new machine learning framework called "self-taught learning" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {759–766},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273593,
author = {Rakhlin, Alexander and Abernethy, Jacob and Bartlett, Peter L.},
title = {Online Discovery of Similarity Mappings},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273593},
doi = {10.1145/1273496.1273593},
abstract = {We consider the problem of choosing, sequentially, a map which assigns elements of a set A to a few elements of a set B. On each round, the algorithm suffers some cost associated with the chosen assignment, and the goal is to minimize the cumulative loss of these choices relative to the best map on the entire sequence. Even though the offline problem of finding the best map is provably hard, we show that there is an equivalent online approximation algorithm, Randomized Map Prediction (RMP), that is efficient and performs nearly as well. While drawing upon results from the "Online Prediction with Expert Advice" setting, we show how RMP can be utilized as an online approach to several standard batch problems. We apply RMP to online clustering as well as online feature selection and, surprisingly, RMP often outperforms the standard batch algorithms on these problems.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {767–774},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273594,
author = {Rakotomamonjy, Alain and Bach, Francis and Canu, St\'{e}phane and Grandvalet, Yves},
title = {More Efficiency in Multiple Kernel Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273594},
doi = {10.1145/1273496.1273594},
abstract = {An efficient and general multiple kernel learning (MKL) algorithm has been recently proposed by Sonnenburg et al. (2006). This approach has opened new perspectives since it makes the MKL approach tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs several iterations before converging towards a reasonable solution. In this paper, we address the MKL problem through an adaptive 2-norm regularization formulation. Weights on each kernel matrix are included in the standard SVM empirical risk minimization problem with a l1 constraint to encourage sparsity. We propose an algorithm for solving this problem and provide an new insight on MKL algorithms based on block 1-norm regularization by showing that the two approaches are equivalent. Experimental results show that the resulting algorithm converges rapidly and its efficiency compares favorably to other MKL algorithms.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {775–782},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273595,
author = {Rattigan, Matthew J. and Maier, Marc and Jensen, David},
title = {Graph Clustering with Network Structure Indices},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273595},
doi = {10.1145/1273496.1273595},
abstract = {Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k-medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {783–790},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273596,
author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
title = {Restricted Boltzmann Machines for Collaborative Filtering},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273596},
doi = {10.1145/1273496.1273596},
abstract = {Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {791–798},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273597,
author = {Shah, Mohak},
title = {Sample Compression Bounds for Decision Trees},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273597},
doi = {10.1145/1273496.1273597},
abstract = {We propose a formulation of the Decision Tree learning algorithm in the Compression settings and derive tight generalization error bounds. In particular, we propose Sample Compression and Occam's Razor bounds. We show how such bounds, unlike the VC dimension or Rademacher complexities based bounds, are more general and can also perform a margin-sparsity trade-off to obtain better classifers. Potentially, these risk bounds can also guide the model selection process and replace traditional pruning strategies.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {799–806},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273598,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
title = {Pegasos: Primal Estimated Sub-GrAdient SOlver for SVM},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273598},
doi = {10.1145/1273496.1273598},
abstract = {We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy ε is \~{O}(1/ε). In contrast, previous analyses of stochastic gradient descent methods require Ω (1/ε2) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is \~{O} (d/(λε)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273599,
author = {Song, Le and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M.},
title = {A Dependence Maximization View of Clustering},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273599},
doi = {10.1145/1273496.1273599},
abstract = {We propose a family of clustering algorithms based on the maximization of dependence between the input variables and their cluster labels, as expressed by the Hilbert-Schmidt Independence Criterion (HSIC). Under this framework, we unify the geometric, spectral, and statistical dependence views of clustering, and subsume many existing algorithms as special cases (e.g. k-means and spectral clustering). Distinctive to our framework is that kernels can also be applied on the labels, which can endow them with particular structures. We also obtain a perturbation bound on the change in k-means clustering.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {815–822},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273600,
author = {Song, Le and Smola, Alex and Gretton, Arthur and Borgwardt, Karsten M. and Bedo, Justin},
title = {Supervised Feature Selection via Dependence Estimation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273600},
doi = {10.1145/1273496.1273600},
abstract = {We introduce a framework for filtering features that employs the Hilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence between the features and the labels. The key idea is that good features should maximise such dependence. Feature selection for various supervised learning problems (including classification and regression) is unified under this framework, and the solutions can be approximated using a backward-elimination algorithm. We demonstrate the usefulness of our method on both artificial and real world datasets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {823–830},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273601,
author = {Sriperumbudur, Bharath K. and Torres, David A. and Lanckriet, Gert R. G.},
title = {Sparse Eigen Methods by D.C. Programming},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273601},
doi = {10.1145/1273496.1273601},
abstract = {Eigenvalue problems are rampant in machine learning and statistics and appear in the context of classification, dimensionality reduction, etc. In this paper, we consider a cardinality constrained variational formulation of generalized eigenvalue problem with sparse principal component analysis (PCA) as a special case. Using l1-norm approximation to the cardinality constraint, previous methods have proposed both convex and non-convex solutions to the sparse PCA problem. In contrast, we propose a tighter approximation that is related to the negative log-likelihood of a Student's t-distribution. The problem is then framed as a d.c. (difference of convex functions) program and is solved as a sequence of locally convex programs. We show that the proposed method not only explains more variance with sparse loadings on the principal directions but also has better scalability compared to other methods. We demonstrate these results on a collection of datasets of varying dimensionality, two of which are high-dimensional gene datasets where the goal is to find few relevant genes that explain as much variance as possible.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {831–838},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273602,
author = {Stern, David and Herbrich, Ralf and Graepel, Thore},
title = {Learning to Solve Game Trees},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273602},
doi = {10.1145/1273496.1273602},
abstract = {We apply probability theory to the task of proving whether a goal can be achieved by a player in an adversarial game. Such problems are solved by searching the game tree. We view this tree as a graphical model which yields a distribution over the (Boolean) outcome of the search before it terminates. Experiments show that a best-first search algorithm guided by this distribution explores a similar number of nodes as Proof-Number Search to solve Go problems. Knowledge is incorporated into search by using domain-specific models to provide prior distributions over the values of leaf nodes of the game tree. These are surrogate for the unexplored parts of the tree. The parameters of these models can be learned from previous search trees. Experiments on Go show that the speed of problem solving can be increased by orders of magnitude by this technique but care must be taken to avoid over-fitting.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {839–846},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273603,
author = {Sun, Jianyong and Kab\'{a}n, Ata and Raychaudhury, Somak},
title = {Robust Mixtures in the Presence of Measurement Errors},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273603},
doi = {10.1145/1273496.1273603},
abstract = {We develop a mixture-based approach to robust density modeling and outlier detection for experimental multivariate data that includes measurement error information. Our model is designed to infer atypical measurements that are not due to errors, aiming to retrieve potentially interesting peculiar objects. Since exact inference is not possible in this model, we develop a tree-structured variational EM solution. This compares favorably against a fully factorial approximation scheme, approaching the accuracy of a Markov-Chain-EM, while maintaining computational simplicity. We demonstrate the benefits of including measurement errors in the model, in terms of improved outlier detection rates in varying measurement uncertainty conditions. We then use this approach for detecting peculiar quasars from an astrophysical survey, given photometric measurements with errors.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {847–854},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273604,
author = {Sun, Xiaohai and Janzing, Dominik and Sch\"{o}lkopf, Bernhard and Fukumizu, Kenji},
title = {A Kernel-Based Causal Learning Algorithm},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273604},
doi = {10.1145/1273496.1273604},
abstract = {We describe a causal learning method, which employs measuring the strength of statistical dependences in terms of the Hilbert-Schmidt norm of kernel-based cross-covariance operators. Following the line of the common faithfulness assumption of constraint-based causal learning, our approach assumes that a variable Z is likely to be a common effect of X and Y, if conditioning on Z increases the dependence between X and Y. Based on this assumption, we collect "votes" for hypothetical causal directions and orient the edges by the majority principle. In most experiments with known causal structures, our method provided plausible results and outperformed the conventional constraint-based PC algorithm.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {855–862},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273605,
author = {Sutton, Charles and McCallum, Andrew},
title = {Piecewise Pseudolikelihood for Efficient Training of Conditional Random Fields},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273605},
doi = {10.1145/1273496.1273605},
abstract = {Discriminative training of graphical models can be expensive if the variables have large cardinality, even if the graphical structure is tractable. In such cases, pseudolikelihood is an attractive alternative, because its running time is linear in the variable cardinality, but on some data its accuracy can be poor. Piecewise training (Sutton &amp; McCallum, 2005) can have better accuracy but does not scale as well in the variable cardinality. In this paper, we introduce piecewise pseudolikelihood, which retains the computational efficiency of pseudolikelihood but can have much better accuracy. On several benchmark NLP data sets, piecewise pseudolikelihood has better accuracy than standard pseudolikelihood, and in many cases nearly equivalent to maximum likelihood, with five to ten times less training time than batch CRF training.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {863–870},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273606,
author = {Sutton, Richard S. and Koop, Anna and Silver, David},
title = {On the Role of Tracking in Stationary Environments},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273606},
doi = {10.1145/1273496.1273606},
abstract = {It is often thought that learning algorithms that track the best solution, as opposed to converging to it, are important only on nonstationary problems. We present three results suggesting that this is not so. First we illustrate in a simple concrete example, the Black and White problem, that tracking can perform better than any converging algorithm on a stationary problem. Second, we show the same point on a larger, more realistic problem, an application of temporal difference learning to computer Go. Our third result suggests that tracking in stationary problems could be important for metalearning research (e.g., learning to learn, feature selection, transfer). We apply a metalearning algorithm for step-size adaptation, IDBD (Sutton, 1992a), to the Black and White problem, showing that meta-learning has a dramatic long-term effect on performance whereas, on an analogous converging problem, meta-learning has only a small second-order effect. This small result suggests a way of eventually overcoming a major obstacle to meta-learning research: the lack of an independent methodology for task selection.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {871–878},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273607,
author = {Taylor, Matthew E. and Stone, Peter},
title = {Cross-Domain Transfer for Reinforcement Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273607},
doi = {10.1145/1273496.1273607},
abstract = {A typical goal for transfer learning algorithms is to utilize knowledge gained in a source task to learn a target task faster. Recently introduced transfer methods in reinforcement learning settings have shown considerable promise, but they typically transfer between pairs of very similar tasks. This work introduces Rule Transfer, a transfer algorithm that first learns rules to summarize a source task policy and then leverages those rules to learn faster in a target task. This paper demonstrates that Rule Transfer can effectively speed up learning in Keepaway, a benchmark RL problem in the robot soccer domain, based on experience from source tasks in the gridworld domain. We empirically show, through the use of three distinct transfer metrics, that Rule Transfer is effective across these domains.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {879–886},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273608,
author = {Titov, Ivan and Henderson, James},
title = {Incremental Bayesian Networks for Structure Prediction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273608},
doi = {10.1145/1273496.1273608},
abstract = {We propose a class of graphical models appropriate for structure prediction problems where the model structure is a function of the output structure. Incremental Sigmoid Belief Networks (ISBNs) avoid the need to sum over the possible model structures by using directed arcs and incrementally specifying the model structure. Exact inference in such directed models is not tractable, but we derive two efficient approximations based on mean field methods, which prove effective in artificial experiments. We then demonstrate their effectiveness on a benchmark natural language parsing task, where they achieve state-of-the-art accuracy. Also, the model which is a closer approximation to an ISBN has better parsing accuracy, suggesting that ISBNs are an appropriate abstract model of structure prediction tasks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {887–894},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273609,
author = {Tomioka, Ryota and Aihara, Kazuyuki},
title = {Classifying Matrices with a Spectral Regularization},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273609},
doi = {10.1145/1273496.1273609},
abstract = {We propose a method for the classification of matrices. We use a linear classifier with a novel regularization scheme based on the spectral l1-norm of its coefficient matrix. The spectral regularization not only provides a principled way of complexity control but also enables automatic determination of the rank of the coefficient matrix. Using the Linear Matrix Inequality technique, we formulate the inference task as a single convex optimization problem. We apply our method to the motor-imagery EEG classification problem. The method not only improves upon conventional methods in the classification performance but also determines a subspace in the signal that concentrates discriminative information without any additional feature extraction step. The method can be easily generalized to regression problems by changing the loss function. Connections to other methods are also discussed.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {895–902},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273610,
author = {Tsampouka, Petroula and Shawe-Taylor, John},
title = {Approximate Maximum Margin Algorithms with Rules Controlled by the Number of Mistakes},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273610},
doi = {10.1145/1273496.1273610},
abstract = {We present a family of incremental Perceptron-like algorithms (PLAs) with margin in which both the "effective" learning rate, defined as the ratio of the learning rate to the length of the weight vector, and the misclassification condition are entirely controlled by rules involving (powers of) the number of mistakes. We examine the convergence of such algorithms in a finite number of steps and show that under some rather mild conditions there exists a limit of the parameters involved in which convergence leads to classification with maximum margin. An experimental comparison of algorithms belonging to this family with other large margin PLAs and decomposition SVMs is also presented.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {903–910},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273611,
author = {Tsang, Ivor W. and Kocsor, Andras and Kwok, James T.},
title = {Simpler Core Vector Machines with Enclosing Balls},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273611},
doi = {10.1145/1273496.1273611},
abstract = {The core vector machine (CVM) is a recent approach for scaling up kernel methods based on the notion of minimum enclosing ball (MEB). Though conceptually simple, an efficient implementation still requires a sophisticated numerical solver. In this paper, we introduce the enclosing ball (EB) problem where the ball's radius is fixed and thus does not have to be minimized. We develop efficient (1 + e)-approximation algorithms that are simple to implement and do not require any numerical solver. For the Gaussian kernel in particular, a suitable choice of this (fixed) radius is easy to determine, and the center obtained from the (1 + e)-approximation of this EB problem is close to the center of the corresponding MEB. Experimental results show that the proposed algorithm has accuracies comparable to the other large-scale SVM implementations, but can handle very large data sets and is even faster than the CVM in general.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {911–918},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273612,
author = {Tsuda, Koji},
title = {Entire Regularization Paths for Graph Data},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273612},
doi = {10.1145/1273496.1273612},
abstract = {Graph data such as chemical compounds and XML documents are getting more common in many application domains. A main difficulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible subgraph patterns, the dimensionality gets too large for usual statistical methods. We propose an efficient method to select a small number of salient patterns by regularization path tracking. The generation of useless patterns is minimized by progressive extension of the search space. In experiments, it is shown that our technique is considerably more efficient than a simpler approach based on frequent substructure mining.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {919–926},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273613,
author = {Urtasun, Raquel and Darrell, Trevor},
title = {Discriminative Gaussian Process Latent Variable Model for Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273613},
doi = {10.1145/1273496.1273613},
abstract = {Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {927–934},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273614,
author = {Van Hulse, Jason and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Experimental Perspectives on Learning from Imbalanced Data},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273614},
doi = {10.1145/1273496.1273614},
abstract = {We present a comprehensive suite of experimentation on the subject of learning from imbalanced data. When classes are imbalanced, many learning algorithms can suffer from the perspective of reduced performance. Can data sampling be used to improve the performance of learners built from imbalanced data? Is the effectiveness of sampling related to the type of learner? Do the results change if the objective is to optimize different performance metrics? We address these and other issues in this work, showing that sampling in many cases will improve classifier performance.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {935–942},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273615,
author = {Wachman, Gabriel and Khardon, Roni},
title = {Learning from Interpretations: A Rooted Kernel for Ordered Hypergraphs},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273615},
doi = {10.1145/1273496.1273615},
abstract = {The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph. Experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ILP methods, and is competitive with state-of-the-art graph kernels. The experiments also demonstrate that the encoding of graph data can affect performance dramatically, a fact that can be useful beyond kernel methods.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {943–950},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273616,
author = {Wang, Gang and Yeung, Dit-Yan and Lochovsky, Frederick H.},
title = {A Kernel Path Algorithm for Support Vector Machines},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273616},
doi = {10.1145/1273496.1273616},
abstract = {The choice of the kernel function which determines the mapping between the input space and the feature space is of crucial importance to kernel methods. The past few years have seen many efforts in learning either the kernel function or the kernel matrix. In this paper, we address this model selection issue by learning the hyperparameter of the kernel function for a support vector machine (SVM). We trace the solution path with respect to the kernel hyperparameter without having to train the model multiple times. Given a kernel hyperparameter value and the optimal solution obtained for that value, we find that the solutions of the neighborhood hyperparameters can be calculated exactly. However, the solution path does not exhibit piecewise linearity and extends nonlinearly. As a result, the breakpoints cannot be computed in advance. We propose a method to approximate the breakpoints. Our method is both efficient and general in the sense that it can be applied to many kernel functions in common use.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {951–958},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273617,
author = {Wang, Hua-Yan and Zha, Hongbin and Qin, Hong},
title = {Dirichlet Aggregation: Unsupervised Learning towards an Optimal Metric for Proportional Data},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273617},
doi = {10.1145/1273496.1273617},
abstract = {Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Major features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a "real" global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {959–966},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273618,
author = {Wang, Huan and Yan, Shuicheng and Huang, Thomas and Liu, Jianzhuang and Tang, Xiaoou},
title = {Transductive Regression Piloted by Inter-Manifold Relations},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273618},
doi = {10.1145/1273496.1273618},
abstract = {In this paper, we present a novel semisupervised regression algorithm working on multiclass data that may lie on multiple manifolds. Unlike conventional manifold regression algorithms that do not consider the class distinction of samples, our method introduces the class information to the regression process and tries to exploit the similar configurations shared by the label distribution of multi-class data. To utilize the correlations among data from different classes, we develop a cross-manifold label propagation process and employ labels from different classes to enhance the regression performance. The interclass relations are coded by a set of intermanifold graphs and a regularization item is introduced to impose inter-class smoothness on the possible solutions. In addition, the algorithm is further extended with the kernel trick for predicting labels of the out-of-sample data even without class information. Experiments on both synthesized data and real world problems validate the effectiveness of the proposed framework for semisupervised regression.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {967–974},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273619,
author = {Wang, Jack M. and Fleet, David J. and Hertzmann, Aaron},
title = {Multifactor Gaussian Process Models for Style-Content Separation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273619},
doi = {10.1145/1273496.1273619},
abstract = {We introduce models for density estimation with multiple, hidden, continuous factors. In particular, we propose a generalization of multilinear models using nonlinear basis functions. By marginalizing over the weights, we obtain a multifactor form of the Gaussian process latent variable model. In this model, each factor is kernelized independently, allowing nonlinear mappings from any particular factor to the data. We learn models for human locomotion data, in which each pose is generated by factors representing the person's identity, gait, and the current state of motion. We demonstrate our approach using time-series prediction, and by synthesizing novel animation from the model.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {975–982},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273620,
author = {Wang, Li and Zhu, Ji and Zou, Hui},
title = {Hybrid Huberized Support Vector Machines for Microarray Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273620},
doi = {10.1145/1273496.1273620},
abstract = {The large number of genes and the relatively small number of samples are typical characteristics for microarray data. These characteristics pose challenges for both sample classification and relevant gene selection. The support vector machine (SVM) is a widely used classification technique, and previous studies have demonstrated its superior classification performance in microarray analysis. However, a major limitation is that the SVM can not perform automatic gene selection. To overcome this limitation, we propose the hybrid huberized support vector machine (HHSVM). The HHSVM uses the huberized hinge loss function and the elastic-net penalty. It has two major benefits: 1. automatic gene selection; 2. the grouping effect, where highly correlated genes tend to be selected/removed together. We also develop an efficient algorithm that computes the entire regularized solution path for HHSVM. We have applied our method to real microarray data and achieved promising results.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {983–990},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273621,
author = {Wang, Liwei and Yang, Cheng and Feng, Jufu},
title = {On Learning with Dissimilarity Functions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273621},
doi = {10.1145/1273496.1273621},
abstract = {We study the problem of learning a classification task in which only a dissimilarity function of the objects is accessible. That is, data are not represented by feature vectors but in terms of their pairwise dissimilarities. We investigate the sufficient conditions for dissimilarity functions to allow building accurate classifiers. Our results have the advantages that they apply to unbounded dissimilarities and are invariant to order-preserving transformations. The theory immediately suggests a learning paradigm: construct an ensemble of decision stumps each depends on a pair of examples, then find a convex combination of them to achieve a large margin. We next develop a practical algorithm called Dissimilarity based Boosting (DBoost) for learning with dissimilarity functions under the theoretical guidance. Experimental results demonstrate that DBoost compares favorably with several existing approaches on a variety of databases and under different conditions.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {991–998},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273622,
author = {Warmuth, Manfred K.},
title = {Winnowing Subspaces},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273622},
doi = {10.1145/1273496.1273622},
abstract = {We generalize the Winnow algorithm for learning disjunctions to learning subspaces of low rank. Subspaces are represented by symmetric projection matrices. The online algorithm maintains its uncertainty about the hidden low rank projection matrix as a symmetric positive definite matrix. This matrix is updated using a version of the Matrix Exponentiated Gradient algorithm that is based on matrix exponentials and matrix logarithms. As in the case of the Winnow algorithm, the bounds are logarithmic in the dimension n of the problem, but linear in the rank r of the hidden subspace. We show that the algorithm can be adapted to handle arbitrary matrices of any dimension via a reduction.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {999–1006},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273623,
author = {Werner, Tom\'{a}\v{s}},
title = {What is Decreased by the Max-Sum Arc Consistency Algorithm?},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273623},
doi = {10.1145/1273496.1273623},
abstract = {Inference tasks in Markov random fields (MRFs) are closely related to the constraint satisfaction problem (CSP) and its soft generalizations. In particular, MAP inference in MRF is equivalent to the weighted (maxsum) CSP. A well-known tool to tackle CSPs are arc consistency algorithms, a.k.a. relaxation labeling. A promising approach to MAP inference in MRFs is linear programming relaxation solved by sequential treereweighted message passing (TRW-S). There is a not widely known algorithm equivalent to TRW-S, max-sum diffusion, which is slower but very simple. We give two theoretical results. First, we show that arc consistency algorithms and max-sum diffusion become the same thing if formulated in an abstractalgebraic way. Thus, we argue that max-sum arc consistency algorithm or max-sum relaxation labeling is a more suitable name for max-sum diffusion. Second, we give a criterion that strictly decreases during these algorithms. It turns out that every class of equivalent problems contains a unique problem that is minimal w.r.t. this criterion.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1007–1014},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273624,
author = {Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
title = {Multi-Task Reinforcement Learning: A Hierarchical Bayesian Approach},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273624},
doi = {10.1145/1273496.1273624},
abstract = {We consider the problem of multi-task reinforcement learning, where the agent needs to solve a sequence of Markov Decision Processes (MDPs) chosen randomly from a fixed but unknown distribution. We model the distribution over MDPs using a hierarchical Bayesian infinite mixture model. For each novel MDP, we use the previously learned distribution as an informed prior for modelbased Bayesian reinforcement learning. The hierarchical Bayesian framework provides a strong prior that allows us to rapidly infer the characteristics of new environments based on previous environments, while the use of a nonparametric model allows us to quickly adapt to environments we have not encountered before. In addition, the use of infinite mixtures allows for the model to automatically learn the number of underlying MDP components. We evaluate our approach and show that it leads to significant speedups in convergence to an optimal policy after observing only a small number of tasks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1015–1022},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273625,
author = {Wipf, David and Nagarajan, Srikantan},
title = {Beamforming Using the Relevance Vector Machine},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273625},
doi = {10.1145/1273496.1273625},
abstract = {Beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere. The widely-used minimum variance adaptive beamformer (MVAB) creates such filters using a sample covariance estimate; however, the quality of this estimate deteriorates when the sources are correlated or the number of samples n is small. Herein, a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine (RVM), a Bayesian method for learning sparse models from possibly overcomplete feature sets. We prove that this substitution has the natural ability to remove the undesirable effects of correlations or limited data. When n becomes large and assuming uncorrelated sources, this method reduces to the exact MVAB. Simulations using direction-of-arrival data support these conclusions. Additionally, RVMs can potentially enhance a variety of traditional signal processing methods that rely on robust sample covariance estimates.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1023–1030},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273626,
author = {Woznica, Adam and Kalousis, Alexandros and Hilario, Melanie},
title = {Learning to Combine Distances for Complex Representations},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273626},
doi = {10.1145/1273496.1273626},
abstract = {The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1031–1038},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273627,
author = {Wu, Mingrui and Yu, Kai and Yu, Shipeng and Sch\"{o}lkopf, Bernhard},
title = {Local Learning Projections},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273627},
doi = {10.1145/1273496.1273627},
abstract = {This paper presents a Local Learning Projection (LLP) approach for linear dimensionality reduction. We first point out that the well known Principal Component Analysis (PCA) essentially seeks the projection that has the minimal global estimation error. Then we propose a dimensionality reduction algorithm that leads to the projection with the minimal local estimation error, and elucidate its advantages for classification tasks. We also indicate that LLP keeps the local information in the sense that the projection value of each point can be well estimated based on its neighbors and their projection values. Experimental results are provided to validate the effectiveness of the proposed algorithm.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1039–1046},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273628,
author = {Xu, Yuehua and Fern, Alan},
title = {On Learning Linear Ranking Functions for Beam Search},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273628},
doi = {10.1145/1273496.1273628},
abstract = {Beam search is used to maintain tractability in large search spaces at the expense of completeness and optimality. We study supervised learning of linear ranking functions for controlling beam search. The goal is to learn ranking functions that allow for beam search to perform nearly as well as unconstrained search while gaining computational efficiency. We first study the computational complexity of the learning problem, showing that even for exponentially large search spaces the general consistency problem is in NP. We also identify tractable and intractable subclasses of the learning problem. Next, we analyze the convergence of recently proposed and modified online learning algorithms. We first provide a counter-example to an existing convergence result and then introduce alternative notions of "margin" that do imply convergence. Finally, we study convergence properties for ambiguous training data.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1047–1054},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273629,
author = {Xuan, Xiang and Murphy, Kevin},
title = {Modeling Changing Dependency Structure in Multivariate Time Series},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273629},
doi = {10.1145/1273496.1273629},
abstract = {We show how to apply the efficient Bayesian changepoint detection techniques of Fearnhead in the multivariate setting. We model the joint density of vector-valued observations using undirected Gaussian graphical models, whose structure we estimate. We show how we can exactly compute the MAP segmentation, as well as how to draw perfect samples from the posterior over segmentations, simultaneously accounting for uncertainty about the number and location of changepoints, as well as uncertainty about the covariance structure. We illustrate the technique by applying it to financial data and to bee tracking data.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1055–1062},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273630,
author = {Xue, Ya and Dunson, David and Carin, Lawrence},
title = {The Matrix Stick-Breaking Process for Flexible Multi-Task Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273630},
doi = {10.1145/1273496.1273630},
abstract = {In multi-task learning our goal is to design regression or classification models for each of the tasks and appropriately share information between tasks. A Dirichlet process (DP) prior can be used to encourage task clustering. However, the DP prior does not allow local clustering of tasks with respect to a subset of the feature vector without making independence assumptions. Motivated by this problem, we develop a new multitask-learning prior, termed the matrix stick-breaking process (MSBP), which encourages cross-task sharing of data. However, the MSBP allows separate clustering and borrowing of information for the different feature components. This is important when tasks are more closely related for certain features than for others. Bayesian inference proceeds by a Gibbs sampling algorithm and the approach is illustrated using a simulated example and a multi-national application.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1063–1070},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273631,
author = {Yairi, Takehisa},
title = {Map Building without Localization by Dimensionality Reduction Techniques},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273631},
doi = {10.1145/1273496.1273631},
abstract = {This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of objects so that they maximally preserve the local proximity of the objects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate "visibility-only" and "bearing-only" localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1071–1078},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273632,
author = {Yamazaki, Keisuke and Kawanabe, Motoaki and Watanabe, Sumio and Sugiyama, Masashi and M\"{u}ller, Klaus-Robert},
title = {Asymptotic Bayesian Generalization Error When Training and Test Distributions Are Different},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273632},
doi = {10.1145/1273496.1273632},
abstract = {In supervised learning, we commonly assume that training and test data are sampled from the same distribution. However, this assumption can be violated in practice and then standard machine learning techniques perform poorly. This paper focuses on revealing and improving the performance of Bayesian estimation when the training and test distributions are different. We formally analyze the asymptotic Bayesian generalization error and establish its upper bound under a very general setting. Our important finding is that lower order terms---which can be ignored in the absence of the distribution change---play an important role under the distribution change. We also propose a novel variant of stochastic complexity which can be used for choosing an appropriate model and hyper-parameters under a particular distribution change.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1079–1086},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273633,
author = {Ye, Jieping},
title = {Least Squares Linear Discriminant Analysis},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273633},
doi = {10.1145/1273496.1273633},
abstract = {Linear Discriminant Analysis (LDA) is a well-known method for dimensionality reduction and classification. LDA in the binaryclass case has been shown to be equivalent to linear regression with the class label as the output. This implies that LDA for binary-class classifications can be formulated as a least squares problem. Previous studies have shown certain relationship between multivariate linear regression and LDA for the multi-class case. Many of these studies show that multivariate linear regression with a specific class indicator matrix as the output can be applied as a preprocessing step for LDA. However, directly casting LDA as a least squares problem is challenging for the multi-class case. In this paper, a novel formulation for multivariate linear regression is proposed. The equivalence relationship between the proposed least squares formulation and LDA for multi-class classifications is rigorously established under a mild condition, which is shown empirically to hold in many applications involving high-dimensional data. Several LDA extensions based on the equivalence relationship are discussed.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1087–1093},
numpages = {7},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273634,
author = {Ye, Jieping and Chen, Jianhui and Ji, Shuiwang},
title = {Discriminant Kernel and Regularization Parameter Learning via Semidefinite Programming},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273634},
doi = {10.1145/1273496.1273634},
abstract = {Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1095–1102},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273635,
author = {Yu, Shipeng and Tresp, Volker and Yu, Kai},
title = {Robust Multi-Task Learning with <i>t</i>-Processes},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273635},
doi = {10.1145/1273496.1273635},
abstract = {Most current multi-task learning frameworks ignore the robustness issue, which means that the presence of "outlier" tasks may greatly reduce overall system performance. We introduce a robust framework for Bayesian multitask learning, t-processes (TP), which are a generalization of Gaussian processes (GP) for multi-task learning. TP allows the system to effectively distinguish good tasks from noisy or outlier tasks. Experiments show that TP not only improves overall system performance, but can also serve as an indicator for the "informativeness" of different tasks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1103–1110},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273636,
author = {Zhang, Jian and Yan, Rong},
title = {On the Value of Pairwise Constraints in Classification and Consistency},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273636},
doi = {10.1145/1273496.1273636},
abstract = {In this paper we consider the problem of classification in the presence of pairwise constraints, which consist of pairs of examples as well as a binary variable indicating whether they belong to the same class or not. We propose a method which can effectively utilize pairwise constraints to construct an estimator of the decision boundary, and we show that the resulting estimator is sign-insensitive consistent with respect to the optimal linear decision boundary. We also study the asymptotic variance of the estimator and extend the method to handle both labeled and pairwise examples in a natural way. Several experiments on simulated datasets and real world classification datasets are conducted. The results not only verify the theoretical properties of the proposed method but also demonstrate its practical value in applications.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1111–1118},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273637,
author = {Zhang, Kai and Tsang, Ivor W. and Kwok, James T.},
title = {Maximum Margin Clustering Made Practical},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273637},
doi = {10.1145/1273496.1273637},
abstract = {Maximum margin clustering (MMC) is a recent large margin unsupervised learning approach that has often outperformed conventional clustering methods. Computationally, it involves non-convex optimization and has to be relaxed to different semidefinite programs (SDP). However, SDP solvers are computationally very expensive and only small data sets can be handled by MMC so far. To make MMC more practical, we avoid SDP relaxations and propose in this paper an efficient approach that performs alternating optimization directly on the original non-convex problem. A key step to avoid premature convergence is on the use of SVR with the Laplacian loss, instead of SVM with the hinge loss, in the inner optimization subproblem. Experiments on a number of synthetic and real-world data sets demonstrate that the proposed approach is often more accurate, much faster and can handle much larger data sets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1119–1126},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273638,
author = {Zhang, Kun and Chan, Laiwan},
title = {Nonlinear Independent Component Analysis with Minimal Nonlinear Distortion},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273638},
doi = {10.1145/1273496.1273638},
abstract = {Nonlinear ICA may not result in nonlinear blind source separation, since solutions to nonlinear ICA are highly non-unique. In practice, the nonlinearity in the data generation procedure is usually not strong. Thus it is reasonable to select the solution with the mixing procedure close to linear. In this paper we propose to solve nonlinear ICA with the "minimal nonlinear distortion" principle. This is achieved by incorporating a regularization term to minimize the mean square error between the mixing mapping and the best-fitting linear one. As an application, the proposed method helps to identify linear, non-Gaussian, and acyclic causal models when mild nonlinearity exists in the data generation procedure. Using this method to separate daily returns of a set of stocks, we successfully identify their linear causal relations. The resulting causal relations give some interesting insights into the stock market.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1127–1134},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273639,
author = {Zhang, Wei and Xue, Xiangyang and Sun, Zichen and Guo, Yue-Fei and Lu, Hong},
title = {Optimal Dimensionality of Metric Space for Classification},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273639},
doi = {10.1145/1273496.1273639},
abstract = {In many real-world applications, Euclidean distance in the original space is not good due to the curse of dimensionality. In this paper, we propose a new method, called Discriminant Neighborhood Embedding (DNE), to learn an appropriate metric space for classification given finite training samples. We define a discriminant adjacent matrix in favor of classification task, i.e., neighboring samples in the same class are squeezed but those in different classes are separated as far as possible. The optimal dimensionality of the metric space can be estimated by spectral analysis in the proposed method, which is of great significance for high-dimensional patterns. Experiments with various datasets demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1135–1142},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273640,
author = {Zhang, Xinhua and Aberdeen, Douglas and Vishwanathan, S. V. N.},
title = {Conditional Random Fields for Multi-Agent Reinforcement Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273640},
doi = {10.1145/1273496.1273640},
abstract = {Conditional random fields (CRFs) are graphical models for modeling the probability of labels given the observations. They have traditionally been trained with using a set of observation and label pairs. Underlying all CRFs is the assumption that, conditioned on the training data, the labels are independent and identically distributed (iid). In this paper we explore the use of CRFs in a class of temporal learning algorithms, namely policy-gradient reinforcement learning (RL). Now the labels are no longer iid. They are actions that update the environment and affect the next observation. From an RL point of view, CRFs provide a natural way to model joint actions in a decentralized Markov decision process. They define how agents can communicate with each other to choose the optimal joint action. Our experiments include a synthetic network alignment problem, a distributed sensor network, and road traffic control; clearly outperforming RL methods which do not model the proper joint policy.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1143–1150},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273641,
author = {Zhao, Zheng and Liu, Huan},
title = {Spectral Feature Selection for Supervised and Unsupervised Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273641},
doi = {10.1145/1273496.1273641},
abstract = {Feature selection aims to reduce dimensionality for building comprehensible learning models with good generalization performance. Feature selection algorithms are largely studied separately according to the type of learning: supervised or unsupervised. This work exploits intrinsic properties underlying supervised and unsupervised feature selection algorithms, and proposes a unified framework for feature selection based on spectral graph theory. The proposed framework is able to generate families of algorithms for both supervised and unsupervised feature selection. And we show that existing powerful algorithms such as ReliefF (supervised) and Laplacian Score (unsupervised) are special cases of the proposed framework. To the best of our knowledge, this work is the first attempt to unify supervised and unsupervised feature selection, and enable their joint study under a general framework. Experiments demonstrated the efficacy of the novel algorithms derived from the framework.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1151–1157},
numpages = {7},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273642,
author = {Zhou, Dengyong and Burges, Christopher J. C.},
title = {Spectral Clustering and Transductive Learning with Multiple Views},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273642},
doi = {10.1145/1273496.1273642},
abstract = {We consider spectral clustering and transductive inference for data with multiple views. A typical example is the web, which can be described by either the hyperlinks between web pages or the words occurring in web pages. When each view is represented as a graph, one may convexly combine the weight matrices or the discrete Laplacians for each graph, and then proceed with existing clustering or classification techniques. Such a solution might sound natural, but its underlying principle is not clear. Unlike this kind of methodology, we develop multiview spectral clustering via generalizing the normalized cut from a single view to multiple views. We further build multiview transductive inference on the basis of multiview spectral clustering. Our framework leads to a mixture of Markov chains defined on every graph. The experimental evaluation on real-world web classification demonstrates promising results that validate our method.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1159–1166},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273643,
author = {Zhou, Zhi-Hua and Xu, Jun-Ming},
title = {On the Relation between Multi-Instance Learning and Semi-Supervised Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273643},
doi = {10.1145/1273496.1273643},
abstract = {Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1167–1174},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273644,
author = {Zhu, Jun and Nie, Zaiqing and Zhang, Bo and Wen, Ji-Rong},
title = {Dynamic Hierarchical Markov Random Fields and Their Application to Web Data Extraction},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273644},
doi = {10.1145/1273496.1273644},
abstract = {Hierarchical models have been extensively studied in various domains. However, existing models assume fixed model structures or incorporate structural uncertainty generatively. In this paper, we propose Dynamic Hierarchical Markov Random Fields (DHMRFs) to incorporate structural uncertainty in a discriminative manner. DHMRFs consist of two parts -- structure model and class label model. Both are defined as exponential family distributions. Conditioned on observations, DHMRFs relax the independence assumption as made in directed models. As exact inference is intractable, a variational method is developed to learn parameters and to find the MAP model structure and label assignment. We apply the model to a real-world web data extraction task, which automatically extracts product items for sale on the Web. The results show promise.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1175–1182},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273645,
author = {Zien, Alexander and Brefeld, Ulf and Scheffer, Tobias},
title = {Transductive Support Vector Machines for Structured Variables},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273645},
doi = {10.1145/1273496.1273645},
abstract = {We study the problem of learning kernel machines transductively for structured output variables. Transductive learning can be reduced to combinatorial optimization problems over all possible labelings of the unlabeled data. In order to scale transductive learning to structured variables, we transform the corresponding non-convex, combinatorial, constrained optimization problems into continuous, unconstrained optimization problems. The discrete optimization parameters are eliminated and the resulting differentiable problems can be optimized efficiently. We study the effectiveness of the generalized TSVM on multiclass classification and label-sequence learning problems empirically.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1183–1190},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1145/1273496.1273646,
author = {Zien, Alexander and Ong, Cheng Soon},
title = {Multiclass Multiple Kernel Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273646},
doi = {10.1145/1273496.1273646},
abstract = {In many applications it is desirable to learn from several kernels. "Multiple kernel learning" (MKL) allows the practitioner to optimize over linear combinations of kernels. By enforcing sparse coefficients, it also generalizes feature selection to kernel selection. We propose MKL for joint feature maps. This provides a convenient and principled way for MKL with multiclass problems. In addition, we can exploit the joint feature map to learn kernels on output spaces. We show the equivalence of several different primal formulations including different regularizers. We present several optimization methods, and compare a convex quadratically constrained quadratic program (QCQP) and two semi-infinite linear programs (SILPs) on toy data, showing that the SILPs are faster than the QCQP. We then demonstrate the utility of our method by applying the SILP to three real world datasets.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {1191–1198},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

