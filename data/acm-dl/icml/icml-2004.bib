@inproceedings{10.1145/1015330.1015430,
author = {Abbeel, Pieter and Ng, Andrew Y.},
title = {Apprenticeship Learning via Inverse Reinforcement Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015430},
doi = {10.1145/1015330.1015430},
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {1},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015343,
author = {Agarwal, Ankur and Triggs, Bill},
title = {Learning to Track 3D Human Motion from Silhouettes},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015343},
doi = {10.1145/1015330.1015343},
abstract = {We describe a sparse Bayesian regression method for recovering 3D human body motion directly from silhouettes extracted from monocular video sequences. No detailed body shape model is needed, and realism is ensured by training on real human motion capture data. The tracker estimates 3D body pose by using Relevance Vector Machine regression to combine a learned autoregressive dynamical model with robust shape descriptors extracted automatically from image silhouettes. We studied several different combination methods, the most effective being to learn a nonlinear observation-update correction based on joint regression with respect to the predicted state and the observations. We demonstrate the method on a 54-parameter full body pose model, both quantitatively using motion capture based test sequences, and qualitatively on a test video sequence.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {2},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015379,
author = {Ahn, Jong-Hoon and Choi, Seungjin and Oh, Jong-Hoon},
title = {A Multiplicative Up-Propagation Algorithm},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015379},
doi = {10.1145/1015330.1015379},
abstract = {We present a generalization of the nonnegative matrix factorization (NMF), where a multilayer generative network with nonnegative weights is used to approximate the observed nonnegative data. The multilayer generative network with nonnegativity constraints, is learned by a multiplicative uppropagation algorithm, where the weights in each layer are updated in a multiplicative fashion while the mismatch ratio is propagated from the bottom to the top layer. The monotonic convergence of the multiplicative up-propagation algorithm is shown. In contrast to NMF, the multiplicative uppropagation is an algorithm that can learn hierarchical representations, where complex higher-level representations are defined in terms of less complex lower-level representations. The interesting behavior of our algorithm is demonstrated with face image data.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {3},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015433,
author = {Altun, Yasemin and Hofmann, Thomas and Smola, Alexander J.},
title = {Gaussian Process Classification for Segmenting and Annotating Sequences},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015433},
doi = {10.1145/1015330.1015433},
abstract = {Many real-world classification tasks involve the prediction of multiple, inter-dependent class labels. A prototypical case of this sort deals with prediction of a sequence of labels for a sequence of observations. Such problems arise naturally in the context of annotating and segmenting observation sequences. This paper generalizes Gaussian Process classification to predict multiple labels by taking dependencies between neighboring labels into account. Our approach is motivated by the desire to retain rigorous probabilistic semantics, while overcoming limitations of parametric methods like Conditional Random Fields, which exhibit conceptual and computational difficulties in high-dimensional input spaces. Experiments on named entity recognition and pitch accent prediction tasks demonstrate the competitiveness of our approach.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {4},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015397,
author = {Appice, Annalisa and Ceci, Michelangelo and Rawles, Simon and Flach, Peter},
title = {Redundant Feature Elimination for Multi-Class Problems},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015397},
doi = {10.1145/1015330.1015397},
abstract = {We consider the problem of eliminating redundant Boolean features for a given data set, where a feature is redundant if it separates the classes less well than another feature or set of features. Lavra\v{c} et al. proposed the algorithm REDUCE that works by pairwise comparison of features, i.e., it eliminates a feature if it is redundant with respect to another feature. Their algorithm operates in an ILP setting and is restricted to two-class problems. In this paper we improve their method and extend it to multiple classes. Central to our approach is the notion of a neighbourhood of examples: a set of examples of the same class where the number of different features between examples is relatively small. Redundant features are eliminated by applying a revised version of the REDUCE method to each pair of neighbourhoods of different class. We analyse the performance of our method on a range of data sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {5},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015424,
author = {Bach, Francis R. and Lanckriet, Gert R. G. and Jordan, Michael I.},
title = {Multiple Kernel Learning, Conic Duality, and the SMO Algorithm},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015424},
doi = {10.1145/1015330.1015424},
abstract = {While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {6},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015378,
author = {Bahamonde, Antonio and Bay\'{o}n, Gustavo F. and D\'{\i}ez, Jorge and Quevedo, Jos\'{e} Ram\'{o}n and Luaces, Oscar and del Coz, Juan Jos\'{e} and Alonso, Jaime and Goyache, F\'{e}lix},
title = {Feature Subset Selection for Learning Preferences: A Case Study},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015378},
doi = {10.1145/1015330.1015378},
abstract = {In this paper we tackle a real world problem, the search of a function to evaluate the merits of beef cattle as meat producers. The independent variables represent a set of live animals' measurements; while the outputs cannot be captured with a single number, since the available experts tend to assess each animal in a relative way, comparing animals with the other partners in the same batch. Therefore, this problem can not be solved by means of regression methods; our approach is to learn the preferences of the experts when they order small groups of animals. Thus, the problem can be reduced to a binary classification, and can be dealt with a Support Vector Machine (SVM) improved with the use of a feature subset selection (FSS) method. We develop a method based on Recursive Feature Elimination (RFE) that employs an adaptation of a metric based method devised for model selection (ADJ). Finally, we discuss the extension of the resulting method to more general settings, and provide a comparison with other possible alternatives.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {7},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015431,
author = {Banerjee, Arindam and Dhillon, Inderjit and Ghosh, Joydeep and Merugu, Srujana},
title = {An Information Theoretic Analysis of Maximum Likelihood Mixture Estimation for Exponential Families},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015431},
doi = {10.1145/1015330.1015431},
abstract = {An important task in unsupervised learning is maximum likelihood mixture estimation (MLME) for exponential families. In this paper, we prove a mathematical equivalence between this MLME problem and the rate distortion problem for Bregman divergences. We also present new theoretical results in rate distortion theory for Bregman divergences. Further, an analysis of the problems as a trade-off between compression and preservation of information is presented that yields the information bottleneck method as an interesting special case.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {8},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015394,
author = {Basilico, Justin and Hofmann, Thomas},
title = {Unifying Collaborative and Content-Based Filtering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015394},
doi = {10.1145/1015330.1015394},
abstract = {Collaborative and content-based filtering are two paradigms that have been applied in the context of recommender systems and user preference prediction. This paper proposes a novel, unified approach that systematically integrates all available training information such as past user-item ratings as well as attributes of items or users to learn a prediction function. The key ingredient of our method is the design of a suitable kernel or similarity function between user-item pairs that allows simultaneous generalization across the user and item dimensions. We propose an on-line algorithm (JRank) that generalizes perceptron learning. Experimental results on the EachMovie data set show significant improvements over standard approaches.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {9},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015398,
author = {Baskiotis, Nicolas and Sebag, Mich\`{e}le},
title = {C4.5 Competence Map: A Phase Transition-Inspired Approach},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015398},
doi = {10.1145/1015330.1015398},
abstract = {How to determine a priori whether a learning algorithm is suited to a learning problem instance is a major scientific and technological challenge. A first step toward this goal, inspired by the Phase Transition (PT) paradigm developed in the Constraint Satisfaction domain, is presented in this paper.Based on the PT paradigm, extensive and principled experiments allow for constructing the Competence Map associated to a learning algorithm, describing the regions where this algorithm on average fails or succeeds. The approach is illustrated on the long and widely used C4.5 algorithm. A non trivial failure region in the landscape of k-term DNF languages is observed and some interpretations are offered for the experimental results.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {10},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015360,
author = {Bilenko, Mikhail and Basu, Sugato and Mooney, Raymond J.},
title = {Integrating Constraints and Metric Learning in Semi-Supervised Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015360},
doi = {10.1145/1015330.1015360},
abstract = {Semi-supervised clustering employs a small amount of labeled data to aid unsupervised learning. Previous work in the area has utilized supervised data in one of two approaches: 1) constraint-based methods that guide the clustering algorithm towards a better grouping of the data, and 2) distance-function learning methods that adapt the underlying similarity metric used by the clustering algorithm. This paper provides new methods for the two approaches as well as presents a new semi-supervised clustering algorithm that integrates both of these techniques in a uniform, principled framework. Experimental results demonstrate that the unified approach produces better clusters than both individual approaches as well as previously proposed semi-supervised clustering algorithms.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {11},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015439,
author = {Blei, David M. and Jordan, Michael I.},
title = {Variational Methods for the Dirichlet Process},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015439},
doi = {10.1145/1015330.1015439},
abstract = {Variational inference methods, including mean field methods and loopy belief propagation, have been widely used for approximate probabilistic inference in graphical models. While often less accurate than MCMC, variational methods provide a fast deterministic approximation to marginal and conditional probabilities. Such approximations can be particularly useful in high dimensional problems where sampling methods are too slow to be effective. A limitation of current methods, however, is that they are restricted to parametric probabilistic models. MCMC does not have such a limitation; indeed, MCMC samplers have been developed for the Dirichlet process (DP), a nonparametric distribution on distributions (Ferguson, 1973) that is the cornerstone of Bayesian nonparametric statistics (Escobar &amp; West, 1995; Neal, 2000). In this paper, we develop a mean-field variational approach to approximate inference for the Dirichlet process, where the approximate posterior is based on the truncated stick-breaking construction (Ishwaran &amp; James, 2001). We compare our approach to DP samplers for Gaussian DP mixture models.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {12},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015429,
author = {Blum, Avrim and Lafferty, John and Rwebangira, Mugizi Robert and Reddy, Rajashekar},
title = {Semi-Supervised Learning Using Randomized Mincuts},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015429},
doi = {10.1145/1015330.1015429},
abstract = {In many application domains there is a large amount of unlabeled data but only a very limited amount of labeled training data. One general approach that has been explored for utilizing this unlabeled data is to construct a graph on all the data points based on distance relationships among examples, and then to use the known labels to perform some type of graph partitioning. One natural partitioning to use is the minimum cut that agrees with the labeled data (Blum &amp; Chawla, 2001), which can be thought of as giving the most probable label assignment if one views labels as generated according to a Markov Random Field on the graph. Zhu et al. (2003) propose a cut based on a relaxation of this field, and Joachims (2003) gives an algorithm based on finding an approximate min-ratio cut.In this paper, we extend the mincut approach by adding randomness to the graph structure. The resulting algorithm addresses several short-comings of the basic mincut approach, and can be given theoretical justification from both a Markov random field perspective and from sample complexity considerations. In cases where the graph does not have small cuts for a given classification problem, randomization may not help. However, our experiments on several datasets show that when the structure of the graph supports small cuts, this can result in highly accurate classifiers with good accuracy/coverage tradeoffs. In addition, we are able to achieve good performance with a very simple graph-construction procedure.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {13},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015416,
author = {Bohte, Sander M. and Breitenbach, Markus and Grudic, Gregory Z.},
title = {Nonparametric Classification with Polynomial MPMC Cascades},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015416},
doi = {10.1145/1015330.1015416},
abstract = {A new class of nonparametric algorithms for high-dimensional binary classification is proposed using cascades of low dimensional polynomial structures. Construction of polynomial cascades is based on Minimax Probability Machine Classification (MPMC), which results in direct estimates of classification accuracy, and provides a simple stopping criteria that does not require expensive cross-validation measures. This Polynomial MPMC Cascade (PMC) algorithm is constructed in linear time with respect to the input space dimensionality, and linear time in the number of examples, making it a potentially attractive alternative to algorithms like support vector machines and standard MPMC. Experimental evidence is given showing that, compared to state-of-the-art classifiers, PMCs are competitive; inherently fast to compute; not prone to overfitting; and generally yield accurate estimates of the maximum error rate on unseen data.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {14},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015338,
author = {Bouckaert, Remco R.},
title = {Estimating Replicability of Classifier Learning Experiments},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015338},
doi = {10.1145/1015330.1015338},
abstract = {Replicability of machine learning experiments measures how likely it is that the outcome of one experiment is repeated when performed with a different randomization of the data. In this paper, we present an estimator of replicability of an experiment that is efficient. More precisely, the estimator is unbiased and has lowest variance in the class of estimators formed by a linear combination of outcomes of experiments on a given data set.We gathered empirical data for comparing experiments consisting of different sampling schemes and hypothesis tests. Both factors are shown to have an impact on replicability of experiments. The data suggests that sign tests should not be used due to low replicability. Ranked sum tests show better performance, but the combination of a sorted runs sampling scheme with a t-test gives the most desirable performance judged on Type I and II error and replicability.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {15},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015350,
author = {Brefeld, Ulf and Scheffer, Tobias},
title = {Co-EM Support Vector Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015350},
doi = {10.1145/1015330.1015350},
abstract = {Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Co-EM outperforms co-training for many problems, but it requires the underlying learner to estimate class probabilities, and to learn from probabilistically labeled data. Therefore, co-EM has so far only been studied with naive Bayesian learners. We cast linear classifiers into a probabilistic framework and develop a co-EM version of the Support Vector Machine. We conduct experiments on text classification problems and compare the family of semi-supervised support vector algorithms under different conditions, including violations of the assumptions underlying multi-view learning. For some problems, such as course web page classification, we observe the most accurate results reported so far.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {16},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015331,
author = {Brinker, Klaus},
title = {Active Learning of Label Ranking Functions},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015331},
doi = {10.1145/1015330.1015331},
abstract = {The effort necessary to construct labeled sets of examples in a supervised learning scenario is often disregarded, though in many applications, it is a time-consuming and expensive procedure. While this already constitutes a major issue in classification learning, it becomes an even more serious problem when dealing with the more complex target domain of total orders over a set of alternatives. Considering both the pairwise decomposition and the constraint classification technique to represent label ranking functions, we introduce a novel generalization of pool-based active learning to address this problem.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {17},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015432,
author = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
title = {Ensemble Selection from Libraries of Models},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015432},
doi = {10.1145/1015330.1015432},
abstract = {We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using different learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or ROC Area. Experiments with seven test problems and ten metrics demonstrate the benefit of ensemble selection.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {18},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015334,
author = {Castillo, Lourdes Pe\~{n}a and Wrobel, Stefan},
title = {A Comparative Study on Methods for Reducing Myopia of Hill-Climbing Search in Multirelational Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015334},
doi = {10.1145/1015330.1015334},
abstract = {Hill-climbing search is the most commonly used search algorithm in ILP systems because it permits the generation of theories in short running times. However, a well known drawback of this greedy search strategy is its myopia. Macro-operators (or macros for short), a recently proposed technique to reduce the search space explored by exhaustive search, can also be argued to reduce the myopia of hill-climbing search by automatically performing a variable-depth look-ahead in the search space. Surprisingly, macros have not been employed in a greedy learner. In this paper, we integrate macros into a hill-climbing learner. In a detailed comparative study in several domains, we show that indeed a hill-climbing learner using macros performs significantly better than current state-of-the-art systems involving other techniques for reducing myopia, such as fixed-depth look-ahead, template-based look-ahead, beam-search, or determinate literals. In addition, macros, in contrast to some of the other approaches, can be computed fully automatically and do not require user involvement nor special domain properties such as determinacy.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {19},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015391,
author = {Chang, Hong and Yeung, Dit-Yan},
title = {Locally Linear Metric Adaptation for Semi-Supervised Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015391},
doi = {10.1145/1015330.1015391},
abstract = {Many supervised and unsupervised learning algorithms are very sensitive to the choice of an appropriate distance metric. While classification tasks can make use of class label information for metric learning, such information is generally unavailable in conventional clustering tasks. Some recent research sought to address a variant of the conventional clustering problem called semi-supervised clustering, which performs clustering in the presence of some background knowledge or supervisory information expressed as pairwise similarity or dissimilarity constraints. However, existing metric learning methods for semi-supervised clustering mostly perform global metric learning through a linear transformation. In this paper, we propose a new metric learning method which performs nonlinear transformation globally but linear transformation locally. In particular, we formulate the learning problem as an optimization problem and present two methods for solving it. Through some toy data sets, we show empirically that our locally linear metric adaptation (LLMA) method can handle some difficult cases that cannot be handled satisfactorily by previous methods. We also demonstrate the effectiveness of our method on some real data sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {20},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015354,
author = {Chu, Wei and Ghahramani, Zoubin and Wild, David L.},
title = {A Graphical Model for Protein Secondary Structure Prediction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015354},
doi = {10.1145/1015330.1015354},
abstract = {In this paper, we present a graphical model for protein secondary structure prediction. This model extends segmental semi-Markov models (SSMM) to exploit multiple sequence alignment profiles which contain information from evolutionarily related sequences. A novel parameterized model is proposed as the likelihood function for the SSMM to capture the segmental conformation. By incorporating the information from long range interactions in \ss{}-sheets, this model is capable of carrying out inference on contact maps. The numerical results on benchmark data sets show that incorporating the profiles results in substantial improvements and the generalization performance is promising.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {21},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015419,
author = {Climer, Sharlee and Zhang, Weixiong},
title = {Take a Walk and Cluster Genes: A TSP-Based Approach to Optimal Rearrangement Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015419},
doi = {10.1145/1015330.1015419},
abstract = {Cluster analysis is a fundamental problem and technique in many areas related to machine learning. In this paper, we consider rearrangement clustering, which is the problem of finding sets of objects that share common or similar features by arranging the rows (objects) of a matrix (specifying object features) in such a way that adjacent objects are similar to each other (based on a similarity measure of the features) so as to maximize the overall similarity. Based on formulating this problem as the Traveling Salesman Problem (TSP), we develop a new TSP-based optimal clustering algorithm called TSPCluster. We overcome a flaw that is inherent in previous approaches by relaxing restrictions on dissimilarities between clusters. Our new algorithm has three important features: finding the optimal k clusters for a given k, automatically detecting cluster borders, and ascertaining a set of most viable clustering results that make good balances among maximizing the overall similarity within clusters and dissimilarity between clusters. We apply TSPCluster to cluster and display ~500 genes of flowering plant Arabidopsis which are regulated under various abiotic stress conditions. We compare TSPCluster to the bond energy algorithm and two existing clustering algorithms. Our TSPCluster code is available at (Climer &amp; Zhang, 2004).},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {22},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015415,
author = {Collobert, Ronan and Bengio, Samy},
title = {Links between Perceptrons, MLPs and SVMs},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015415},
doi = {10.1145/1015330.1015415},
abstract = {We propose to study links between three important classification algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We first study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous findings, which yields better performances in generalization and speed than the other models.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {23},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015351,
author = {Conitzer, Vincent and Sandholm, Tuomas},
title = {Communication Complexity as a Lower Bound for Learning in Games},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015351},
doi = {10.1145/1015330.1015351},
abstract = {A fast-growing body of research in the AI and machine learning communities addresses learning in games, where there are multiple learners with different interests. This research adds to more established research on learning in games conducted in economics. In part because of a clash of fields, there are widely varying requirements on learning algorithms in this domain. The goal of this paper is to demonstrate how communication complexity can be used as a lower bound on the required learning time or cost. Because this lower bound does not assume any requirements on the learning algorithm, it is universal, applying under any set of requirements on the learning algorithm.We characterize exactly the communication complexity of various solution concepts from game theory, namely Nash equilibrium, iterated dominant strategies (both strict and weak), and backwards induction. This gives the tighest lower bounds on learning in games that can be obtained with this method.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {24},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015434,
author = {Cortes, Corinna and Mohri, Mehryar},
title = {Distribution Kernels Based on Moments of Counts},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015434},
doi = {10.1145/1015330.1015434},
abstract = {Many applications in text and speech processing require the analysis of distributions of variable-length sequences. We recently introduced a general kernel framework, rational kernels, to extend kernel methods to the analysis of such variable-length sequences or more generally weighted automata. These kernels are efficient to compute and have been successfully used in applications such as spoken-dialog classification using Support Vector Machines.However, the rational kernels previously introduced do not fully encompass distributions over alternate sequences. Prior similarity measures between two weighted automata are based only on the expected counts of co-occurring subsequences and ignore similarities (or dissimilarities) in higher order moments of the distributions of these counts.In this paper, we introduce a new family of rational kernels, moment kernels, that precisely exploit this additional information. These kernels are distribution kernels based on moments of counts of strings. We describe efficient algorithms to compute moment kernels and apply them to several difficult spoken-dialog classification tasks. Our experiments show that using the second moment of the counts of n-gram sequences consistently improves the classification accuracy in these tasks.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {25},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015399,
author = {Crammer, Koby and Chechik, Gal},
title = {A Needle in a Haystack: Local One-Class Optimization},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015399},
doi = {10.1145/1015330.1015399},
abstract = {This paper addresses the problem of finding a small and coherent subset of points in a given data. This problem, sometimes referred to as one-class or set covering, requires to find a small-radius ball that covers as many data points as possible. It rises naturally in a wide range of applications, from finding gene-modules to extracting documents' topics, where many data points are irrelevant to the task at hand, or in applications where only positive examples are available. Most previous approaches to this problem focus on identifying and discarding a possible set of outliers. In this paper we adopt an opposite approach which directly aims to find a small set of coherently structured regions, by using a loss function that focuses on local properties of the data. We formalize the learning task as an optimization problem using the Information-Bottleneck principle. An algorithm to solve this optimization problem is then derived and analyzed. Experiments on gene expression data and a text document corpus demonstrate the merits of our approach.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {26},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015374,
author = {Dekel, Ofer and Keshet, Joseph and Singer, Yoram},
title = {Large Margin Hierarchical Classification},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015374},
doi = {10.1145/1015330.1015374},
abstract = {We present an algorithmic framework for supervised classification learning where the set of labels is organized in a predefined hierarchical structure. This structure is encoded by a rooted tree which induces a metric over the label set. Our approach combines ideas from large margin kernel methods and Bayesian analysis. Following the large margin principle, we associate a prototype with each label in the tree and formulate the learning task as an optimization problem with varying margin constraints. In the spirit of Bayesian methods, we impose similarity requirements between the prototypes corresponding to adjacent labels in the hierarchy. We describe new online and batch algorithms for solving the constrained optimization problem. We derive a worst case loss-bound for the online algorithm and provide generalization analysis for its batch counterpart. We demonstrate the merits of our approach with a series of experiments on synthetic, text and speech data.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {27},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015428,
author = {Dietterich, Thomas G. and Ashenfelter, Adam and Bulatov, Yaroslav},
title = {Training Conditional Random Fields via Gradient Tree Boosting},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015428},
doi = {10.1145/1015330.1015428},
abstract = {Conditional Random Fields (CRFs; Lafferty, McCallum, &amp; Pereira, 2001) provide a flexible and powerful model for learning to assign labels to elements of sequences in such applications as part-of-speech tagging, text-to-speech mapping, protein and DNA sequence analysis, and information extraction from web pages. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features. This paper describes a new method for training CRFs by applying Friedman's (1999) gradient tree boosting method. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees. Regression trees are learned by stage-wise optimizations similar to Adaboost, but with the objective of maximizing the conditional likelihood P(Y|X) of the CRF model. By growing regression trees, interactions among features are introduced only as needed, so although the parameter space is potentially immense, the search algorithm does not explicitly consider the large space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially like previous algorithms based on iterative scaling and gradient descent.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {28},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015408,
author = {Ding, Chris and He, Xiaofeng},
title = {<i>K</i>-Means Clustering via Principal Component Analysis},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015408},
doi = {10.1145/1015330.1015408},
abstract = {Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction. K-means clustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noise-reduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective techniques for K-means data clustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5% of the optimal values.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {29},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015407,
author = {Ding, Chris and He, Xiaofeng},
title = {Linearized Cluster Assignment via Spectral Ordering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015407},
doi = {10.1145/1015330.1015407},
abstract = {Spectral clustering uses eigenvectors of the Laplacian of the similarity matrix. They are most conveniently applied to 2-way clustering problems. When applying to multi-way clustering, either the 2-way spectral clustering is recursively applied or an embedding to spectral space is done and some other methods are used to cluster the points. Here we propose and study a K-way cluster assignment method. The method transforms the problem to find valleys and peaks of a 1-D quantity called cluster crossing, which measures the symmetric cluster overlap across a cut point along a linear ordering of the data points. The method can either determine K clusters in one shot or recursively split a current cluster into several smaller ones. We show that a linear ordering based on a distance sensitive objective has a continuous solution which is the eigenvector of the Laplacian, showing the close relationship between clustering and ordering. The method relies on the connectivity matrix constructed as the truncated spectral expansion of the similarity matrix, useful for revealing cluster structure. The method is applied to newsgroups to illustrate introduced concepts; experiments show it outperforms the recursive 2-way clustering and the standard K-means clustering.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {30},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015358,
author = {D'Souza, Aaron and Vijayakumar, Sethu and Schaal, Stefan},
title = {The Bayesian Backfitting Relevance Vector Machine},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015358},
doi = {10.1145/1015330.1015358},
abstract = {Traditional non-parametric statistical learning techniques are often computationally attractive, but lack the same generalization and model selection abilities as state-of-the-art Bayesian algorithms which, however, are usually computationally prohibitive. This paper makes several important contributions that allow Bayesian learning to scale to more complex, real-world learning scenarios. Firstly, we show that backfitting --- a traditional non-parametric, yet highly efficient regression tool --- can be derived in a novel formulation within an expectation maximization (EM) framework and thus can finally be given a probabilistic interpretation. Secondly, we show that the general framework of sparse Bayesian learning and in particular the relevance vector machine (RVM), can be derived as a highly efficient algorithm using a Bayesian version of backfitting at its core. As we demonstrate on several regression and classification benchmarks, Bayesian backfitting offers a compelling alternative to current regression methods, especially when the size and dimensionality of the data challenge computational resources.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {31},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015413,
author = {Eliazar, Austin I. and Parr, Ronald},
title = {Learning Probabilistic Motion Models for Mobile Robots},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015413},
doi = {10.1145/1015330.1015413},
abstract = {Machine learning methods are often applied to the problem of learning a map from a robot's sensor data, but they are rarely applied to the problem of learning a robot's motion model. The motion model, which can be influenced by robot idiosyncrasies and terrain properties, is a crucial aspect of current algorithms for Simultaneous Localization and Mapping (SLAM). In this paper we concentrate on generating the correct motion model for a robot by applying EM methods in conjunction with a current SLAM algorithm. In contrast to previous calibration approaches, we not only estimate the mean of the motion, but also the interdependencies between motion terms, and the variances in these terms. This can be used to provide a more focused proposal distribution to a particle filter used in a SLAM algorithm, which can reduce the resources needed for localization while decreasing the chance of losing track of the robot's position. We validate this approach by recovering a good motion model despite initialization with a poor one. Further experiments validate the generality of the learned model in similar circumstances.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {32},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015373,
author = {Esmeir, Saher and Markovitch, Shaul},
title = {Lookahead-Based Algorithms for Anytime Induction of Decision Trees},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015373},
doi = {10.1145/1015330.1015373},
abstract = {The majority of the existing algorithms for learning decision trees are greedy---a tree is induced top-down, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Furthermore, the greedy algorithms require a fixed amount of time and are not able to generate a better tree if additional time is available. To overcome this problem, we present two lookahead-based algorithms for anytime induction of decision trees, thus allowing tradeoff between tree quality and learning time. The first one is depth-k lookahead, where a larger time allocation permits larger k. The second algorithm uses a novel strategy for evaluating candidate splits; a stochastic version of ID3 is repeatedly invoked to estimate the size of the tree in which each split results, and the one that minimizes the expected size is preferred. Experimental results indicate that for several hard concepts, our proposed approach exhibits good anytime behavior and yields significantly better decision trees when more time is available.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {33},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015386,
author = {Esposito, Roberto and Saitta, Lorenza},
title = {A Monte Carlo Analysis of Ensemble Classification},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015386},
doi = {10.1145/1015330.1015386},
abstract = {In this paper we extend previous results providing a theoretical analysis of a new Monte Carlo ensemble classifier. The framework allows us to characterize the conditions under which the ensemble approach can be expected to outperform the single hypothesis classifier. Moreover, we provide a closed form expression for the distribution of the true ensemble accuracy, as well as of its mean and variance. We then exploit this result in order to analyze the expected error behavior in a particularly interesting case.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {34},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015420,
author = {Fern, Alan and Givan, Robert},
title = {Relational Sequential Inference with Reliable Observations},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015420},
doi = {10.1145/1015330.1015420},
abstract = {We present a trainable sequential-inference technique for processes with large state and observation spaces and relational structure. Our method assumes "reliable observations", i.e. that each process state persists long enough to be reliably inferred from the observations it generates. We introduce the idea of a "state-inference function" (from observation sequences to underlying hidden states) for representing knowledge about a process and develop an efficient sequential-inference algorithm, utilizing this function, that is correct for processes that generate reliable observations consistent with the state-inference function. We describe a representation for state-inference functions in relational domains and give a corresponding supervised learning algorithm. Experiments, in relational video interpretation, show that our technique provides significantly improved accuracy and speed relative to a variety of recent, hand-coded, non-trainable systems.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {35},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015414,
author = {Fern, Xiaoli Zhang and Brodley, Carla E.},
title = {Solving Cluster Ensemble Problems by Bipartite Graph Partitioning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015414},
doi = {10.1145/1015330.1015414},
abstract = {A critical problem in cluster ensemble research is how to combine multiple clusterings to yield a final superior clustering result. Leveraging advanced graph partitioning techniques, we solve this problem by reducing it to a graph partitioning problem. We introduce a new reduction method that constructs a bipartite graph from a given cluster ensemble. The resulting graph models both instances and clusters of the ensemble simultaneously as vertices in the graph. Our approach retains all of the information provided by a given ensemble, allowing the similarity among instances and the similarity among clusters to be considered collectively in forming the final clustering. Further, the resulting graph partitioning problem can be solved efficiently. We empirically evaluate the proposed approach against two commonly used graph formulations and show that it is more robust and achieves comparable or better performance in comparison to its competitors.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {36},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015395,
author = {Ferri, C\'{e}sar and Flach, Peter and Hern\'{a}ndez-Orallo, Jos\'{e}},
title = {Delegating Classifiers},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015395},
doi = {10.1145/1015330.1015395},
abstract = {A sensible use of classifiers must be based on the estimated reliability of their predictions. A cautious classifier would delegate the difficult or uncertain predictions to other, possibly more specialised, classifiers. In this paper we analyse and develop this idea of delegating classifiers in a systematic way. First, we design a two-step scenario where a first classifier chooses which examples to classify and delegates the difficult examples to train a second classifier. Secondly, we present an iterated scenario involving an arbitrary number of chained classifiers. We compare these scenarios to classical ensemble methods, such as bagging and boosting. We show experimentally that our approach is not far behind these methods in terms of accuracy, but with several advantages: (i) improved efficiency, since each classifier learns from fewer examples than the previous one; (ii) improved comprehensibility, since each classification derives from a single classifier; and (iii) the possibility to simplify the overall multi-classifier by removing the parts that lead to delegation.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {37},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015356,
author = {Forman, George},
title = {A Pitfall and Solution in Multi-Class Feature Selection for Text Classification},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015356},
doi = {10.1145/1015330.1015356},
abstract = {Information Gain is a well-known and empirically proven method for high-dimensional feature selection. We found that it and other existing methods failed to produce good results on an industrial text classification problem. On investigating the root cause, we find that a large class of feature scoring methods suffers a pitfall: they can be blinded by a surplus of strongly predictive features for some classes, while largely ignoring features needed to discriminate difficult classes. In this paper we demonstrate this pitfall hurts performance even for a relatively uniform text classification task. Based on this understanding, we present solutions inspired by round-robin scheduling that avoid this pitfall, without resorting to costly wrapper methods. Empirical evaluation on 19 datasets shows substantial improvements.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {38},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015363,
author = {Frank, Eibe and Kramer, Stefan},
title = {Ensembles of Nested Dichotomies for Multi-Class Problems},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015363},
doi = {10.1145/1015330.1015363},
abstract = {Nested dichotomies are a standard statistical technique for tackling certain polytomous classification problems with logistic regression. They can be represented as binary trees that recursively split a multi-class classification task into a system of dichotomies and provide a statistically sound way of applying two-class learning algorithms to multi-class problems (assuming these algorithms generate class probability estimates). However, there are usually many candidate trees for a given problem and in the standard approach the choice of a particular tree is based on domain knowledge that may not be available in practice. An alternative is to treat every system of nested dichotomies as equally likely and to form an ensemble classifier based on this assumption. We show that this approach produces more accurate classifications than applying C4.5 and logistic regression directly to multi-class problems. Our results also show that ensembles of nested dichotomies produce more accurate classifiers than pairwise classification if both techniques are used with C4.5, and comparable results for logistic regression. Compared to error-correcting output codes, they are preferable if logistic regression is used, and comparable in the case of C4.5. An additional benefit is that they generate class probability estimates. Consequently they appear to be a good general-purpose method for applying binary classifiers to multi-class problems.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {39},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015409,
author = {Fung, Glenn and Dundar, Murat and Bi, Jinbo and Rao, Bharat},
title = {A Fast Iterative Algorithm for Fisher Discriminant Using Heterogeneous Kernels},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015409},
doi = {10.1145/1015330.1015409},
abstract = {We propose a fast iterative classification algorithm for Kernel Fisher Discriminant (KFD) using heterogeneous kernel models. In contrast with the standard KFD that requires the user to predefine a kernel function, we incorporate the task of choosing an appropriate kernel into the optimization problem to be solved. The choice of kernel is defined as a linear combination of kernels belonging to a potentially large family of different positive semidefinite kernels. The complexity of our algorithm does not increase significantly with respect to the number of kernels on the kernel family. Experiments on several benchmark datasets demonstrate that generalization performance of the proposed algorithm is not significantly different from that achieved by the standard KFD in which the kernel parameters have been tuned using cross validation. We also present results on a real-life colon cancer dataset that demonstrate the efficiency of the proposed method.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {40},
numpages = {8},
keywords = {Binary Classification, Heterogeneous Kernels, Mathematical Programming, Linear Fisher Discriminant},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015388,
author = {Gabrilovich, Evgeniy and Markovitch, Shaul},
title = {Text Categorization with Many Redundant Features: Using Aggressive Feature Selection to Make SVMs Competitive with C4.5},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015388},
doi = {10.1145/1015330.1015388},
abstract = {Text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features. Most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed. We describe a class of text categorization problems that are characterized with many redundant features. Even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy. We develop a novel measure that captures feature redundancy, and use it to analyze a large collection of datasets. We show that for problems plagued with numerous redundant features the performance of C4.5 is significantly superior to that of SVM, while aggressive feature selection allows SVM to beat C4.5 by a narrow margin.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {41},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015361,
author = {Gao, Sheng and Wu, Wen and Lee, Chin-Hui and Chua, Tat-Seng},
title = {A MFoM Learning Approach to Robust Multiclass Multi-Label Text Categorization},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015361},
doi = {10.1145/1015330.1015361},
abstract = {We propose a multiclass (MC) classification approach to text categorization (TC). To fully take advantage of both positive and negative training examples, a maximal figure-of-merit (MFoM) learning algorithm is introduced to train high performance MC classifiers. In contrast to conventional binary classification, the proposed MC scheme assigns a uniform score function to each category for each given test sample, and thus the classical Bayes decision rules can now be applied. Since all the MC MFoM classifiers are simultaneously trained, we expect them to be more robust and work better than the binary MFoM classifiers, which are trained separately and are known to give the best TC performance. Experimental results on the Reuters-21578 TC task indicate that the MC MFoM classifiers achieve a micro-averaging F1 value of 0.377, which is significantly better than 0.138, obtained with the binary MFoM classifiers, for the categories with less than 4 training samples. Furthermore, for all 90 categories, most with large training sizes, the MC MFoM classifiers give a micro-averaging F1 value of 0.888, better than 0.884, obtained with the binary MFoM classifiers.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {42},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015352,
author = {Gilad-Bachrach, Ran and Navot, Amir and Tishby, Naftali},
title = {Margin Based Feature Selection - Theory and Algorithms},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015352},
doi = {10.1145/1015330.1015352},
abstract = {Feature selection is the task of choosing a small set out of a given set of features that capture the relevant properties of the data. In the context of supervised classification problems the relevance is determined by the given labels on the training data. A good choice of features is a key for building compact and accurate classifiers. In this paper we introduce a margin based feature selection criterion and apply it to measure the quality of sets of features. Using margins we devise novel selection algorithms for multi-class classification problems and provide theoretical generalization bound. We also study the well known Relief algorithm and show that it resembles a gradient ascent over our margin criterion. We apply our new algorithm to various datasets and show that our new Simba algorithm, which directly optimizes the margin, outperforms Relief.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {43},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015406,
author = {Goldenberg, Anna and Moore, Andrew},
title = {Tractable Learning of Large Bayes Net Structures from Sparse Data},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015406},
doi = {10.1145/1015330.1015406},
abstract = {This paper addresses three questions. Is it useful to attempt to learn a Bayesian network structure with hundreds of thousands of nodes? How should such structure search proceed practically? The third question arises out of our approach to the second: how can Frequent Sets (Agrawal et al., 1993), which are extremely popular in the area of descriptive data mining, be turned into a probabilistic model?Large sparse datasets with hundreds of thousands of records and attributes appear in social networks, warehousing, supermarket transactions and web logs. The complexity of structural search made learning of factored probabilistic models on such datasets unfeasible. We propose to use Frequent Sets to significantly speed up the structural search. Unlike previous approaches, we not only cache n-way sufficient statistics, but also exploit their local structure. We also present an empirical evaluation of our algorithm applied to several massive datasets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {44},
numpages = {8},
keywords = {statistical learning, Bayesian networks/graphical models, Bayes Net structure learning},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015367,
author = {Gramacy, Robert B. and Lee, Herbert K. H. and Macready, William G.},
title = {Parameter Space Exploration with Gaussian Process Trees},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015367},
doi = {10.1145/1015330.1015367},
abstract = {Computer experiments often require dense sweeps over input parameters to obtain a qualitative understanding of their response. Such sweeps can be prohibitively expensive, and are unnecessary in regions where the response is easy predicted; well-chosen designs could allow a mapping of the response with far fewer simulation runs. Thus, there is a need for computationally inexpensive surrogate models and an accompanying method for selecting small designs. We explore a general methodology for addressing this need that uses non-stationary Gaussian processes. Binary trees partition the input space to facilitate non-stationarity and a Bayesian interpretation provides an explicit measure of predictive uncertainty that can be used to guide sampling. Our methods are illustrated on several examples, including a motivating example involving computational fluid dynamics simulation of a NASA reentry vehicle.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {45},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015339,
author = {Grossman, Daniel and Domingos, Pedro},
title = {Learning Bayesian Network Classifiers by Maximizing Conditional Likelihood},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015339},
doi = {10.1145/1015330.1015339},
abstract = {Bayesian networks are a powerful probabilistic representation, and their use for classification has received considerable attention. However, they tend to perform poorly when learned in the standard way. This is attributable to a mismatch between the objective function used (likelihood or a function thereof) and the goal of classification (maximizing accuracy or conditional likelihood). Unfortunately, the computational cost of optimizing structure and parameters for conditional likelihood is prohibitive. In this paper we show that a simple approximation---choosing structures by maximizing conditional likelihood while setting parameters by maximum likelihood---yields good results. On a large suite of benchmark datasets, this approach produces better class probability estimates than naive Bayes, TAN, and generatively-trained Bayesian networks.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {46},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015417,
author = {Ham, Jihun and Lee, Daniel D. and Mika, Sebastian and Sch\"{o}lkopf, Bernhard},
title = {A Kernel View of the Dimensionality Reduction of Manifolds},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015417},
doi = {10.1145/1015330.1015417},
abstract = {We interpret several well-known algorithms for dimensionality reduction of manifolds as kernel methods. Isomap, graph Laplacian eigenmap, and locally linear embedding (LLE) all utilize local neighborhood information to construct a global embedding of the manifold. We show how all three algorithms can be described as kernel PCA on specially constructed Gram matrices, and illustrate the similarities and differences between the algorithms with representative examples.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {47},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015421,
author = {Hardin, Douglas and Tsamardinos, Ioannis and Aliferis, Constantin F.},
title = {A Theoretical Characterization of Linear SVM-Based Feature Selection},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015421},
doi = {10.1145/1015330.1015421},
abstract = {Most prevalent techniques in Support Vector Machine (SVM) feature selection are based on the intuition that the weights of features that are close to zero are not required for optimal classification. In this paper we show that indeed, in the sample limit, the irrelevant variables (in a theoretical and optimal sense) will be given zero weight by a linear SVM, both in the soft and the hard margin case. However, SVM-based methods have certain theoretical disadvantages too. We present examples where the linear SVM may assign zero weights to strongly relevant variables (i.e., variables required for optimal estimation of the distribution of the target variable) and where weakly relevant features (i.e., features that are superfluous for optimal feature selection given other features) may get non-zero weights. We contrast and theoretically compare with Markov-Blanket based feature selection algorithms that do not have such disadvantages in a broad class of distributions and could also be used for causal discovery.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {48},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015366,
author = {Herschtal, Alan and Raskutti, Bhavani},
title = {Optimising Area under the ROC Curve Using Gradient Descent},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015366},
doi = {10.1145/1015330.1015366},
abstract = {This paper introduces RankOpt, a linear binary classifier which optimises the area under the ROC curve (the AUC). Unlike standard binary classifiers, RankOpt adopts the AUC statistic as its objective function, and optimises it directly using gradient descent. The problems with using the AUC statistic as an objective function are that it is non-differentiable, and of complexity O(n2) in the number of data observations. RankOpt uses a differentiable approximation to the AUC which is accurate, and computationally efficient, being of complexity O(n.) This enables the gradient descent to be performed in reasonable time. The performance of RankOpt is compared with a number of other linear binary classifiers, over a number of different classification problems. In almost all cases it is found that the performance of RankOpt is significantly better than the other classifiers tested.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {49},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015389,
author = {Hertz, Tomer and Bar-Hillel, Aharon and Weinshall, Daphna},
title = {Boosting Margin Based Distance Functions for Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015389},
doi = {10.1145/1015330.1015389},
abstract = {The performance of graph based clustering methods critically depends on the quality of the distance function used to compute similarities between pairs of neighboring nodes. In this paper we learn distance functions by training binary classifiers with margins. The classifiers are defined over the product space of pairs of points and are trained to distinguish whether two points come from the same class or not. The signed margin is used as the distance value. Our main contribution is a distance learning method (DistBoost), which combines boosting hypotheses over the product space with a weak learner based on partitioning the original feature space. Each weak hypothesis is a Gaussian mixture model computed using a semi-supervised constrained EM algorithm, which is trained using both unlabeled and labeled data. We also consider SVM and decision trees boosting as margin based classifiers in the product space. We experimentally compare the margin based distance functions with other existing metric learning methods, and with existing techniques for the direct incorporation of constraints into various clustering algorithms. Clustering performance is measured on some benchmark databases from the UCI repository, a sample from the MNIST database, and a data set of color images of animals. In most cases the DistBoost algorithm significantly and robustly outperformed its competitors.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {50},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015365,
author = {Huang, Kaizhu and Yang, Haiqin and King, Irwin and Lyu, Michael R.},
title = {Learning Large Margin Classifiers Locally and Globally},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015365},
doi = {10.1145/1015330.1015365},
abstract = {A new large margin classifier, named Maxi-Min Margin Machine (M4) is proposed in this paper. This new classifier is constructed based on both a "local: and a "global" view of data, while the most popular large margin classifier, Support Vector Machine (SVM) and the recently-proposed important model, Minimax Probability Machine (MPM) consider data only either locally or globally. This new model is theoretically important in the sense that SVM and MPM can both be considered as its special case. Furthermore, the optimization of M4 can be cast as a sequential conic programming problem, which can be solved efficiently. We describe the M4 model definition, provide a clear geometrical interpretation, present theoretical justifications, propose efficient solving methods, and perform a series of evaluations on both synthetic data sets and real world benchmark data sets. Its comparison with SVM and MPM also demonstrates the advantages of our new model.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {51},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015377,
author = {Jakulin, Aleks and Bratko, Ivan},
title = {Testing the Significance of Attribute Interactions},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015377},
doi = {10.1145/1015330.1015377},
abstract = {Attribute interactions are the irreducible dependencies between attributes. Interactions underlie feature relevance and selection, the structure of joint probability and classification models: if and only if the attributes interact, they should be connected. While the issue of 2-way interactions, especially of those between an attribute and the label, has already been addressed, we introduce an operational definition of a generalized n-way interaction by highlighting two models: the reductionistic part-to-whole approximation, where the model of the whole is reconstructed from models of the parts, and the holistic reference model, where the whole is modelled directly. An interaction is deemed significant if these two models are significantly different. In this paper, we propose the Kirkwood superposition approximation for constructing part-to-whole approximations. To model data, we do not assume a particular structure of interactions, but instead construct the model by testing for the presence of interactions. The resulting map of significant interactions is a graphical model learned from the data. We confirm that the P-values computed with the assumption of the asymptotic X2 distribution closely match those obtained with the boot-strap.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {52},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015359,
author = {James, Michael R. and Singh, Satinder},
title = {Learning and Discovery of Predictive State Representations in Dynamical Systems with Reset},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015359},
doi = {10.1145/1015330.1015359},
abstract = {Predictive state representations (PSRs) are a recently proposed way of modeling controlled dynamical systems. PSR-based models use predictions of observable outcomes of tests that could be done on the system as their state representation, and have model parameters that define how the predictive state representation changes over time as actions are taken and observations noted. Learning PSR-based models requires solving two subproblems: 1) discovery of the tests whose predictions constitute state, and 2) learning the model parameters that define the dynamics. So far, there have been no results available on the discovery subproblem while for the learning subproblem an approximate-gradient algorithm has been proposed (Singh et al., 2003) with mixed results (it works on some domains and not on others). In this paper, we provide the first discovery algorithm and a new learning algorithm for linear PSRs for the special class of controlled dynamical systems that have a reset operation. We provide experimental verification of our algorithms. Finally, we also distinguish our work from prior work by Jaeger (2000) on observable operator models (OOMs).},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {53},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015336,
author = {Janodet, Jean-Christophe and Nock, Richard and Sebban, Marc and Suchier, Henri-Maxime},
title = {Boosting Grammatical Inference with Confidence Oracles},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015336},
doi = {10.1145/1015330.1015336},
abstract = {In this paper we focus on the adaptation of boosting to grammatical inference. We aim at improving the performance of state merging algorithms in the presence of noisy data by using, in the update rule, additional information provided by an oracle. This strategy requires the construction of a new weighting scheme that takes into account the confidence in the labels of the examples. We prove that our new framework preserves the theoretical properties of boosting. Using the state merging algorithm RPNI*, we describe an experimental study on various datasets, showing a dramatic improvement of performances.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {54},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015426,
author = {Jebara, Tony},
title = {Multi-Task Feature and Kernel Selection for SVMs},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015426},
doi = {10.1145/1015330.1015426},
abstract = {We compute a common feature selection or kernel selection configuration for multiple support vector machines (SVMs) trained on different yet inter-related datasets. The method is advantageous when multiple classification tasks and differently labeled datasets exist over a common input space. Different datasets can mutually reinforce a common choice of representation or relevant features for their various classifiers. We derive a multi-task representation learning approach using the maximum entropy discrimination formalism. The resulting convex algorithms maintain the global solution properties of support vector machines. However, in addition to multiple SVM classification/regression parameters they also jointly estimate an optimal subset of features or optimal combination of kernels. Experiments are shown on standardized datasets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {55},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015357,
author = {Jenkins, Odest Chadwicke and Matari\'{c}, Maja J.},
title = {A Spatio-Temporal Extension to Isomap Nonlinear Dimension Reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015357},
doi = {10.1145/1015330.1015357},
abstract = {We present an extension of Isomap nonlinear dimension reduction (Tenenbaum et al., 2000) for data with both spatial and temporal relationships. Our method, ST-Isomap, augments the existing Isomap framework to consider temporal relationships in local neighborhoods that can be propagated globally via a shortest-path mechanism. Two instantiations of ST-Isomap are presented for sequentially continuous and segmented data. Results from applying ST-Isomap to real-world data collected from human motion performance and humanoid robot teleoperation are also presented.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {56},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015370,
author = {Jin, Rong and Liu, Huan},
title = {Robust Feature Induction for Support Vector Machines},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015370},
doi = {10.1145/1015330.1015370},
abstract = {The goal of feature induction is to automatically create nonlinear combinations of existing features as additional input features to improve classification accuracy. Typically, nonlinear features are introduced into a support vector machine (SVM) through a nonlinear kernel function. One disadvantage of such an approach is that the feature space induced by a kernel function is usually of high dimension and therefore will substantially increase the chance of over-fitting the training data. Another disadvantage is that nonlinear features are induced implicitly and therefore are difficult for people to understand which induced features are critical to the classification performance. In this paper, we propose a boosting-style algorithm that can explicitly induces important nonlinear features for SVMs. We present empirical studies with discussion to show that this approach is effective in improving classification accuracy for SVMs. The comparison with an SVM model using nonlinear kernels also indicates that this approach is effective and robust, particularly when the number of training data is small.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {57},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015383,
author = {Kashima, Hisashi and Tsuboi, Yuta},
title = {Kernel-Based Discriminative Learning Algorithms for Labeling Sequences, Trees, and Graphs},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015383},
doi = {10.1145/1015330.1015383},
abstract = {We introduce a new perceptron-based discriminative learning algorithm for labeling structured data such as sequences, trees, and graphs. Since it is fully kernelized and uses pointwise label prediction, large features, including arbitrary number of hidden variables, can be incorporated with polynomial time complexity. This is in contrast to existing labelers that can handle only features of a small number of hidden variables, such as Maximum Entropy Markov Models and Conditional Random Fields. We also introduce several kernel functions for labeling sequences, trees, and graphs and efficient algorithms for them.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {58},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015401,
author = {Kersting, Kristian and Otterlo, Martijn Van and De Raedt, Luc},
title = {Bellman Goes Relational},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015401},
doi = {10.1145/1015330.1015401},
abstract = {Motivated by the interest in relational reinforcement learning, we introduce a novel relational Bellman update operator called REBEL. It employs a constraint logic programming language to compactly represent Markov decision processes over relational domains. Using REBEL, a novel value iteration algorithm is developed in which abstraction (over states and actions) plays a major role. This framework provides new insights into relational reinforcement learning. Convergence results as well as experiments are presented.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {59},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015364,
author = {Kim, Yongdai and Kim, Jinseog},
title = {Gradient LASSO for Feature Selection},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015364},
doi = {10.1145/1015330.1015364},
abstract = {LASSO (Least Absolute Shrinkage and Selection Operator) is a useful tool to achieve the shrinkage and variable selection simultaneously. Since LASSO uses the L1 penalty, the optimization should rely on the quadratic program (QP) or general non-linear program which is known to be computational intensive. In this paper, we propose a gradient descent algorithm for LASSO. Even though the final result is slightly less accurate, the proposed algorithm is computationally simpler than QP or non-linear program, and so can be applied to large size problems. We provide the convergence rate of the algorithm, and illustrate it with simulated models as well as real data sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {60},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015410,
author = {Kok, Jelle R. and Vlassis, Nikos},
title = {Sparse Cooperative Q-Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015410},
doi = {10.1145/1015330.1015410},
abstract = {Learning in multiagent systems suffers from the fact that both the state and the action space scale exponentially with the number of agents. In this paper we are interested in using Q-learning to learn the coordinated actions of a group of cooperative agents, using a sparse representation of the joint state-action space of the agents. We first examine a compact representation in which the agents need to explicitly coordinate their actions only in a predefined set of states. Next, we use a coordination-graph approach in which we represent the Q-values by value rules that specify the coordination dependencies of the agents at particular states. We show how Q-learning can be efficiently applied to learn a coordinated policy for the agents in the above framework. We demonstrate the proposed method on the predator-prey domain, and we compare it with other related multiagent Q-learning methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {61},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015448,
author = {Koppel, Moshe and Schler, Jonathan},
title = {Authorship Verification as a One-Class Classification Problem},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015448},
doi = {10.1145/1015330.1015448},
abstract = {In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the "depth of difference" between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {62},
numpages = {7},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015344,
author = {Krause, Nir and Singer, Yoram},
title = {Leveraging the Margin More Carefully},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015344},
doi = {10.1145/1015330.1015344},
abstract = {Boosting is a popular approach for building accurate classifiers. Despite the initial popular belief, boosting algorithms do exhibit overfitting and are sensitive to label noise. Part of the sensitivity of boosting algorithms to outliers and noise can be attributed to the unboundedness of the margin-based loss functions that they employ. In this paper we describe two leveraging algorithms that build on boosting techniques and employ a bounded loss function of the margin. The first algorithm interleaves the expectation maximization (EM) algorithm with boosting steps. The second algorithm decomposes a non-convex loss into a difference of two convex losses. We prove that both algorithms converge to a stationary point. We also analyze the generalization properties of the algorithms using the Rademacher complexity. We describe experiments with both synthetic data and natural data (OCR and text) that demonstrate the merits of our framework, in particular robustness to outliers.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {63},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015337,
author = {Lafferty, John and Zhu, Xiaojin and Liu, Yan},
title = {Kernel Conditional Random Fields: Representation and Clique Selection},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015337},
doi = {10.1145/1015330.1015337},
abstract = {Kernel conditional random fields (KCRFs) are introduced as a framework for discriminative modeling of graph-structured data. A representer theorem for conditional graphical models is given which shows how kernel conditional random fields arise from risk minimization procedures defined using Mercer kernels on labeled graphs. A procedure for greedily selecting cliques in the dual representation is then proposed, which allows sparse representations. By incorporating kernels and implicit feature spaces into conditional graphical models, the framework enables semi-supervised learning algorithms for structured data through the use of graph kernels. The framework and clique selection methods are demonstrated in synthetic data experiments, and are also applied to the problem of protein secondary structure prediction.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {64},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015382,
author = {Lawrence, Neil D. and Platt, John C.},
title = {Learning to Learn with the Informative Vector Machine},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015382},
doi = {10.1145/1015330.1015382},
abstract = {This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {65},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015333,
author = {Lebanon, Guy and Lafferty, John},
title = {Hyperplane Margin Classifiers on the Multinomial Manifold},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015333},
doi = {10.1145/1015330.1015333},
abstract = {The assumptions behind linear classifiers for categorical data are examined and reformulated in the context of the multinomial manifold, the simplex of multinomial models furnished with the Riemannian structure induced by the Fisher information. This leads to a new view of hyperplane classifiers which, together with a generalized margin concept, shows how to adapt existing margin-based hyperplane models to multinomial geometry. Experiments show the new classification framework to be effective for text classification, where the categorical structure of the data is modeled naturally within the multinomial family.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {66},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015362,
author = {Lee, Jianguo and Wang, Jingdong and Zhang, Changshui and Bian, Zhaoqi},
title = {Probabilistic Tangent Subspace: A Unified View},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015362},
doi = {10.1145/1015330.1015362},
abstract = {Tangent Distance (TD) is one classical method for invariant pattern classification. However, conventional TD need pre-obtain tangent vectors, which is difficult except for image objects. This paper extends TD to more general pattern classification tasks. The basic assumption is that tangent vectors can be approximately represented by the pattern variations. We propose three probabilistic subspace models to encode the variations: the linear subspace, nonlinear subspace, and manifold subspace models. These three models are addressed in a unified view, namely Probabilistic Tangent Subspace (PTS). Experiments show that PTS can achieve promising classification performance in non-image data sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {67},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015404,
author = {Li, Tao and Ma, Sheng and Ogihara, Mitsunori},
title = {Entropy-Based Criterion in Categorical Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015404},
doi = {10.1145/1015330.1015404},
abstract = {Entropy-type measures for the heterogeneity of clusters have been used for a long time. This paper studies the entropy-based criterion in clustering categorical data. It first shows that the entropy-based criterion can be derived in the formal framework of probabilistic clustering models and establishes the connection between the criterion and the approach based on dissimilarity co-efficients. An iterative Monte-Carlo procedure is then presented to search for the partitions minimizing the criterion. Experiments are conducted to show the effectiveness of the proposed procedure.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {68},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015369,
author = {Ling, Charles X. and Yang, Qiang and Wang, Jianning and Zhang, Shichao},
title = {Decision Trees with Minimal Costs},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015369},
doi = {10.1145/1015330.1015369},
abstract = {We propose a simple, novel and yet effective method for building and testing decision trees that minimizes the sum of the misclassification and test costs. More specifically, we first put forward an original and simple splitting criterion for attribute selection in tree building. Our tree-building algorithm has many desirable properties for a cost-sensitive learning system that must account for both types of costs. Then, assuming that the test cases may have a large number of missing values, we design several intelligent test strategies that can suggest ways of obtaining the missing values at a cost in order to minimize the total cost. We experimentally compare these strategies and C4.5, and demonstrate that our new algorithms significantly outperform C4.5 and its variations. In addition, our algorithm's complexity is similar to that of C4.5, and is much lower than that of previous work. Our work is useful for many diagnostic tasks which must factor in the misclassification and test costs for obtaining missing information.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {69},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015446,
author = {Mah\'{e}, Pierre and Ueda, Nobuhisa and Akutsu, Tatsuya and Perret, Jean-Luc and Vert, Jean-Philippe},
title = {Extensions of Marginalized Graph Kernels},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015446},
doi = {10.1145/1015330.1015446},
abstract = {Positive definite kernels between labeled graphs have recently been proposed. They enable the application of kernel methods, such as support vector machines, to the analysis and classification of graphs, for example, chemical compounds. These graph kernels are obtained by marginalizing a kernel between paths with respect to a random walk model on the graph vertices along the edges. We propose two extensions of these graph kernels, with the double goal to reduce their computation time and increase their relevance as measure of similarity between graphs. First, we propose to modify the label of each vertex by automatically adding information about its environment with the use of the Morgan algorithm. Second, we suggest a modification of the random walk model to prevent the walk from coming back to a vertex that was just visited. These extensions are then tested on benchmark experiments of chemical compounds classification, with promising results.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {70},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015355,
author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
title = {Dynamic Abstraction in Reinforcement Learning via Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015355},
doi = {10.1145/1015330.1015355},
abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {71},
numpages = {8},
keywords = {Options, Clustering, Hierarchical Reinforcement Learning, Reinforcement Learning, Q-Learning},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015402,
author = {Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N.},
title = {Bias and Variance in Value Function Estimation},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015402},
doi = {10.1145/1015330.1015402},
abstract = {We consider the bias and variance of value function estimation that are caused by using an empirical model instead of the true model. We analyze these bias and variance for Markov processes from a classical (frequentist) statistical point of view, and in a Bayesian setting. Using a second order approximation, we provide explicit expressions for the bias and variance in terms of the transition counts and the reward statistics. We present supporting experiments with artificial Markov chains and with a large transactional database provided by a mail-order catalog firm.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {72},
numpages = {8},
keywords = {Markov Processes, Bayesian Estimation, Variance, Bias, Reinforcement Learning},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015437,
author = {Marlin, Benjamin and Zemel, Richard S.},
title = {The Multiple Multiplicative Factor Model for Collaborative Filtering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015437},
doi = {10.1145/1015330.1015437},
abstract = {We describe a class of causal, discrete latent variable models called Multiple Multiplicative Factor models (MMFs). A data vector is represented in the latent space as a vector of factors that have discrete, non-negative expression levels. Each factor proposes a distribution over the data vector. The distinguishing feature of MMFs is that they combine the factors' proposed distributions multiplicatively, taking into account factor expression levels. The product formulation of MMFs allow factors to specialize to a subset of the items, while the causal generative semantics mean MMFs can readily accommodate missing data. This makes MMFs distinct from both directed models with mixture semantics and undirected product models. In this paper we present empirical results from the collaborative filtering domain showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {73},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015385,
author = {Melville, Prem and Mooney, Raymond J.},
title = {Diverse Ensembles for Active Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015385},
doi = {10.1145/1015330.1015385},
abstract = {Query by Committee is an effective approach to selective sampling in which disagreement amongst an ensemble of hypotheses is used to select data for labeling. Query by Bagging and Query by Boosting are two practical implementations of this approach that use Bagging and Boosting, respectively, to build the committees. For effective active learning, it is critical that the committee be made up of consistent hypotheses that are very different from each other. DECORATE is a recently developed method that directly constructs such diverse committees using artificial training data. This paper introduces ACTIVE-DECORATE, which uses DECORATE committees to select good training examples. Extensive experimental results demonstrate that, in general, ACTIVE-DECORATE outperforms both Query by Bagging and Query by Boosting.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {74},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015390,
author = {Merke, Artur and Schoknecht, Ralf},
title = {Convergence of Synchronous Reinforcement Learning with Linear Function Approximation},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015390},
doi = {10.1145/1015330.1015390},
abstract = {Synchronous reinforcement learning (RL) algorithms with linear function approximation are representable as inhomogeneous matrix iterations of a special form (Schoknecht &amp; Merke, 2003). In this paper we state conditions of convergence for general inhomogeneous matrix iterations and prove that they are both necessary and sufficient. This result extends the work presented in (Schoknecht &amp; Merke, 2003), where only a sufficient condition of convergence was proved. As the condition of convergence is necessary and sufficient, the new result is suitable to prove convergence and divergence of RL algorithms with function approximation. We use the theorem to deduce a new concise proof of convergence for the synchronous residual gradient algorithm (Baird, 1995). Moreover, we derive a counterexample for which the uniform RL algorithm (Merke &amp; Schoknecht, 2002) diverges. This yields a negative answer to the open question if the uniform RL algorithm converges for arbitrary multiple transitions.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {75},
numpages = {6},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015384,
author = {Morales, Eduardo F. and Sammut, Claude},
title = {Learning to Fly by Combining Reinforcement Learning with Behavioural Cloning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015384},
doi = {10.1145/1015330.1015384},
abstract = {Reinforcement learning deals with learning optimal or near optimal policies while interacting with the environment. Application domains with many continuous variables are difficult to solve with existing reinforcement learning methods due to the large search space. In this paper, we use a relational representation to define powerful abstractions that allow us to incorporate domain knowledge and re-use previously learned policies in other similar problems. We also describe how to learn useful actions from human traces using a behavioural cloning approach combined with an exploration phase. Since several conflicting actions may be induced for the same abstract state, reinforcement learning is used to learn an optimal policy over this reduced space. It is shown experimentally how a combination of behavioural cloning and reinforcement learning using a relational representation is powerful enough to learn how to fly an aircraft through different points in space and different turbulence conditions.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {76},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015447,
author = {Nattee, Cholwich and Sinthupinyo, Sukree and Numao, Masayuki and Okada, Takashi},
title = {Learning First-Order Rules from Data with Multiple Parts: Applications on Mining Chemical Compound Data},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015447},
doi = {10.1145/1015330.1015447},
abstract = {Inductive learning of first-order theory based on examples has serious bottleneck in the enormous hypothesis search space needed, making existing learning approaches perform poorly when compared to the propositional approach. Moreover, in order to choose the appropiate candidates, all Inductive Logic Programming (ILP) systems only use quantitive information, e.g. number of examples covered and length of rules, which is insufficient for search space having many similar candidates. This paper introduces a novel approach to improve ILP by incorporating the qualitative information into the search heuristics by focusing only on a kind of data where one instance consists of several parts, as well as relations among parts. This approach aims to find the hypothesis describing each class by using both individual and relational characteristics of parts of examples. This kind of data can be found in various domains, especially in representing chemical compound structure. Each compound is composed of atoms as parts, and bonds as relations between two atoms. We apply the proposed approach for discovering rules describing the activity of compounds from their structures from two real-world datasets: mutagenicity in nitroaromatic compounds and dopamine antagonist compounds. The results were compared to the existing method using ten-fold cross validation, and we found that the proposed method significantly produced more accurate results in prediction.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {77},
numpages = {9},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015435,
author = {Ng, Andrew Y.},
title = {Feature Selection, <i>L</i><sub>1</sub> vs. <i>L</i><sub>2</sub> Regularization, and Rotational Invariance},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015435},
doi = {10.1145/1015330.1015435},
abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {78},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015349,
author = {Nguyen, Hieu T. and Smeulders, Arnold},
title = {Active Learning Using Pre-Clustering},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015349},
doi = {10.1145/1015330.1015349},
abstract = {The paper is concerned with two-class active learning. While the common approach for collecting data in active learning is to select samples close to the classification boundary, better performance can be achieved by taking into account the prior data distribution. The main contribution of the paper is a formal framework that incorporates clustering into active learning. The algorithm first constructs a classifier on the set of the cluster representatives, and then propagates the classification decision to the other samples via a local noise model. The proposed model allows to select the most representative samples as well as to avoid repeatedly labeling samples in the same cluster. During the active learning process, the clustering is adjusted using the coarse-to-fine strategy in order to balance between the advantage of large clusters and the accuracy of the data representation. The results of experiments in image databases show a better performance of our algorithm compared to the current methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {79},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015438,
author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
title = {Decentralized Detection and Classification Using Kernel Methods},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015438},
doi = {10.1145/1015330.1015438},
abstract = {We consider the problem of decentralized detection under constraints on the number of bits that can be transmitted by each sensor. In contrast to most previous work, in which the joint distribution of sensor observations is assumed to be known, we address the problem when only a set of empirical samples is available. We propose a novel algorithm using the framework of empirical risk minimization and marginalized kernels, and analyze its computational and statistical properties both theoretically and empirically. We provide an efficient implementation of the algorithm, and demonstrate its performance on both simulated and real data sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {80},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015443,
author = {Ong, Cheng Soon and Mary, Xavier and Canu, St\'{e}phane and Smola, Alexander J.},
title = {Learning with Non-Positive Kernels},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015443},
doi = {10.1145/1015330.1015443},
abstract = {In this paper we show that many kernel methods can be adapted to deal with indefinite kernels, that is, kernels which are not positive semidefinite. They do not satisfy Mercer's condition and they induce associated functional spaces called Reproducing Kernel Kreundefinedn Spaces (RKKS), a generalization of Reproducing Kernel Hilbert Spaces (RKHS).Machine learning in RKKS shares many "nice" properties of learning in RKHS, such as orthogonality and projection. However, since the kernels are indefinite, we can no longer minimize the loss, instead we stabilize it. We show a general representer theorem for constrained stabilization and prove generalization bounds by computing the Rademacher averages of the kernel class. We list several examples of indefinite kernels and investigate regularization methods to solve spline interpolation. Some preliminary experiments with indefinite kernels for spline smoothing are reported for truncated spectral factorization, Landweber-Fridman iterations, and MR-II.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {81},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015375,
author = {Peltonen, Jaakko and Sinkkonen, Janne and Kaski, Samuel},
title = {Sequential Information Bottleneck for Finite Data},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015375},
doi = {10.1145/1015330.1015375},
abstract = {The sequential information bottleneck (sIB) algorithm clusters co-occurrence data such as text documents vs. words. We introduce a variant that models sparse co-occurrence data by a generative process. This turns the objective function of sIB, mutual information, into a Bayes factor, while keeping it intact asymptotically, for non-sparse data. Experimental performance of the new algorithm is comparable to the original sIB for large data sets, and better for smaller, sparse sets.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {82},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015412,
author = {Phillips, Steven J. and Dud\'{\i}k, Miroslav and Schapire, Robert E.},
title = {A Maximum Entropy Approach to Species Distribution Modeling},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015412},
doi = {10.1145/1015330.1015412},
abstract = {We study the problem of modeling species geographic distributions, a critical problem in conservation biology. We propose the use of maximum-entropy techniques for this problem, specifically, sequential-update algorithms that can handle a very large number of features. We describe experiments comparing maxent with a standard distribution-modeling tool, called GARP, on a dataset containing observation data for North American breeding birds. We also study how well maxent performs as a function of the number of training examples and training time, analyze the use of regularization to avoid overfitting when the number of examples is small, and explore the interpretability of models constructed using maxent.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {83},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015372,
author = {Potts, Duncan},
title = {Incremental Learning of Linear Model Trees},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015372},
doi = {10.1145/1015330.1015372},
abstract = {A linear model tree is a decision tree with a linear functional model in each leaf. Previous model tree induction algorithms have operated on the entire training set, however there are many situations when an incremental learner is advantageous. In this paper we demonstrate that model trees can be induced incrementally using an algorithm that scales linearly with the number of examples. An incremental node splitting rule is presented, together with incremental methods for stopping the growth of the tree and pruning. Empirical testing in three domains, where the emphasis is on learning a dynamic model of the environment, shows that the algorithm can learn a more accurate approximation from fewer examples than other incremental methods. In addition the induced models are smaller, and the learner requires less prior knowledge about the domain.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {84},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015418,
author = {Qi, Yuan (Alan) and Minka, Thomas P. and Picard, Rosalind W. and Ghahramani, Zoubin},
title = {Predictive Automatic Relevance Determination by Expectation Propagation},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015418},
doi = {10.1145/1015330.1015418},
abstract = {In many real-world classification problems the input contains a large number of potentially irrelevant features. This paper proposes a new Bayesian framework for determining the relevance of input features. This approach extends one of the most successful Bayesian methods for feature selection and sparse learning, known as Automatic Relevance Determination (ARD). ARD finds the relevance of features by optimizing the model marginal likelihood, also known as the evidence. We show that this can lead to overfitting. To address this problem, we propose Predictive ARD based on estimating the predictive performance of the classifier. While the actual leave-one-out predictive performance is generally very costly to compute, the expectation propagation (EP) algorithm proposed by Minka provides an estimate of this predictive performance as a side-effect of its iterations. We exploit this in our algorithm to do feature selection, and to select data points in a sparse Bayesian kernel classifier. Moreover, we provide two other improvements to previous algorithms, by replacing Laplace's approximation with the generally more accurate EP, and by incorporating the fast optimization algorithm proposed by Faul and Tipping. Our experiments show that our method based on the EP estimate of predictive performance is more accurate on test data than relevance determination by optimizing the evidence.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {85},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015392,
author = {Ray, Soumya and Page, David},
title = {Sequential Skewing: An Improved Skewing Algorithm},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015392},
doi = {10.1145/1015330.1015392},
abstract = {This paper extends previous work on the Skewing algorithm, a promising approach that allows greedy decision tree induction algorithms to handle problematic functions such as parity functions with a lower run-time penalty than Lookahead. A deficiency of the previously proposed algorithm is its inability to scale up to high dimensional problems. In this paper, we describe a modified algorithm that scales better with increasing numbers of variables. We present experiments with randomly generated Boolean functions that evaluate the algorithm's response to increasing dimensions. We also evaluate the algorithm on a challenging real world biomedical problem, that of SH3 domain binding. Our results indicate that our algorithm almost always outperforms an information gain-based decision tree learner.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {86},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015403,
author = {Rosales, R\'{o}mer and Achan, Kannan and Frey, Brendan},
title = {Learning to Cluster Using Local Neighborhood Structure},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015403},
doi = {10.1145/1015330.1015403},
abstract = {This paper introduces an approach for clustering/classification which is based on the use of local, high-order structure present in the data. For some problems, this local structure might be more relevant for classification than other measures of point similarity used by popular unsupervised and semi-supervised clustering methods. Under this approach, changes in the class label are associated to changes in the local properties of the data. Using this idea, we also pursue to learn how to cluster given examples of clustered data (including from different datasets). We make these concepts formal by presenting a probability model that captures their fundamentals and show that in this setting, learning to cluster is a well defined and tractable task. Based on probabilistic inference methods, we then present an algorithm for computing the posterior probability distribution of class labels for each data point. Experiments in the domain of spatial grouping and functional gene classification are used to illustrate and test these concepts.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {87},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015441,
author = {Rosencrantz, Matthew and Gordon, Geoff and Thrun, Sebastian},
title = {Learning Low Dimensional Predictive Representations},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015441},
doi = {10.1145/1015330.1015441},
abstract = {Predictive state representations (PSRs) have recently been proposed as an alternative to partially observable Markov decision processes (POMDPs) for representing the state of a dynamical system (Littman et al., 2001). We present a learning algorithm that learns a PSR from observational data. Our algorithm produces a variant of PSRs called transformed predictive state representations (TPSRs). We provide an efficient principal-components-based algorithm for learning a TPSR, and show that TPSRs can perform well in comparison to Hidden Markov Models learned with Baum-Welch in a real world robot tracking task for low dimensional representations and long prediction horizons.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {88},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015400,
author = {Rosset, Saharon},
title = {Model Selection via the AUC},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015400},
doi = {10.1145/1015330.1015400},
abstract = {We present a statistical analysis of the AUC as an evaluation criterion for classification scoring models. First, we consider significance tests for the difference between AUC scores of two algorithms on the same test set. We derive exact moments under simplifying assumptions and use them to examine approximate practical methods from the literature. We then compare AUC to empirical misclassification error when the prediction goal is to minimize future error rate. We show that the AUC may be preferable to empirical error even in this case and discuss the tradeoff between approximation error and estimation error underlying this phenomenon.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {89},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015387,
author = {R\"{u}ckert, Ulrich and Kramer, Stefan},
title = {Towards Tight Bounds for Rule Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015387},
doi = {10.1145/1015330.1015387},
abstract = {While there is a lot of empirical evidence showing that traditional rule learning approaches work well in practice, it is nearly impossible to derive analytical results about their predictive accuracy. In this paper, we investigate rule-learning from a theoretical perspective. We show that the application of McAllester's PAC-Bayesian bound to rule learning yields a practical learning algorithm, which is based on ensembles of weighted rule sets. Experiments with the resulting learning algorithm show not only that it is competitive with state-of-the-art rule learners, but also that its error rate can often be bounded tightly. In fact, the bound turns out to be tighter than one of the "best" bounds for a practical learning scheme known so far (the Set Covering Machine). Finally, we prove that the bound can be further improved by allowing the learner to abstain from uncertain predictions.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {90},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015411,
author = {Rudary, Matthew and Singh, Satinder and Pollack, Martha E.},
title = {Adaptive Cognitive Orthotics: Combining Reinforcement Learning and Constraint-Based Temporal Reasoning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015411},
doi = {10.1145/1015330.1015411},
abstract = {Reminder systems support people with impaired prospective memory and/or executive function, by providing them with reminders of their functional daily activities. We integrate temporal constraint reasoning with reinforcement learning (RL) to build an adaptive reminder system and in a simulated environment demonstrate that it can personalize to a user and adapt to both short- and long-term changes. In addition to advancing the application domain, our integrated algorithm contributes to research on temporal constraint reasoning by showing how RL can select an optimal policy from amongst a set of temporally consistent ones, and it contributes to the work on RL by showing how temporal constraint reasoning can be used to dramatically reduce the space of actions from which an RL agent needs to learn.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {91},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015340,
author = {Ryabko, Daniil},
title = {Online Learning of Conditionally I.I.D. Data},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015340},
doi = {10.1145/1015330.1015340},
abstract = {In this work we consider the task of relaxing the i.i.d assumption in online pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Online pattern recognition is predicting a sequence of labels based on objects given for each label and on examples (pairs of objects and labels) learned so far. Traditionally, this task is considered under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over under a much weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects only, while the only condition on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. distributed examples.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {92},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015380,
author = {Scully, Ted and Madden, Michael G. and Lyons, Gerard},
title = {Coalition Calculation in a Dynamic Agent Environment},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015380},
doi = {10.1145/1015330.1015380},
abstract = {We consider a dynamic market-place of self-interested agents with differing capabilities. A task to be completed is proposed to the agent population. An agent attempts to form a coalition of agents to perform the task. Before proposing a coalition, the agent must determine the optimal set of agents with whom to enter into a coalition for this task; we refer to this activity as coalition calculation. To determine the optimal coalition, the agent must have a means of calculating the value of any given coalition. Multiple metrics (cost, time, quality etc.) determine the true value of a coalition. However, because of conflicting metrics, differing metric importance and the tendency of metric importance to vary over time, it is difficult to obtain a true valuation of a given coalition. Previous work has not addressed these issues. We present a solution based on the adaptation of a multi-objective optimization evolutionary algorithm. In order to obtain a true valuation of any coalition, we use the concept of Pareto dominance coupled with a distance weighting algorithm. We determine the Pareto optimal set of coalitions and then use an instance-based learning algorithm to select the optimal coalition. We show through empirical evaluation that the proposed technique is capable of eliciting metric importance and adapting to metric variation over time.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {93},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015376,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Ng, Andrew Y.},
title = {Online and Batch Learning of Pseudo-Metrics},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015376},
doi = {10.1145/1015330.1015376},
abstract = {We describe and analyze an online algorithm for supervised learning of pseudo-metrics. The algorithm receives pairs of instances and predicts their similarity according to a pseudo-metric. The pseudo-metrics we use are quadratic forms parameterized by positive semi-definite matrices. The core of the algorithm is an update rule that is based on successive projections onto the positive semi-definite cone and onto half-space constraints imposed by the examples. We describe an efficient procedure for performing these projections, derive a worst case mistake bound on the similarity predictions, and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators. The online algorithm also serves as a building block for deriving a large-margin batch algorithm. We demonstrate the merits of the proposed approach by conducting experiments on MNIST dataset and on document filtering.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {94},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015353,
author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Barto, Andrew G.},
title = {Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015353},
doi = {10.1145/1015330.1015353},
abstract = {We present a new method for automatically creating useful temporal abstractions in reinforcement learning. We argue that states that allow the agent to transition to a different region of the state space are useful subgoals, and propose a method for identifying them using the concept of relative novelty. When such a state is identified, a temporally-extended activity (e.g., an option) is generated that takes the agent efficiently to this state. We illustrate the utility of the method in a number of tasks.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {95},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015371,
author = {Sminchisescu, Cristian and Jepson, Allan},
title = {Generative Modeling for Continuous Non-Linearly Embedded Visual Inference},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015371},
doi = {10.1145/1015330.1015371},
abstract = {Many difficult visual perception problems, like 3D human motion estimation, can be formulated in terms of inference using complex generative models, defined over high-dimensional state spaces. Despite progress, optimizing such models is difficult because prior knowledge cannot be flexibly integrated in order to reshape an initially designed representation space. Nonlinearities, inherent sparsity of high-dimensional training sets, and lack of global continuity makes dimensionality reduction challenging and low-dimensional search inefficient. To address these problems, we present a learning and inference algorithm that restricts visual tracking to automatically extracted, non-linearly embedded, low-dimensional spaces. This formulation produces a layered generative model with reduced state representation, that can be estimated using efficient continuous optimization methods. Our prior flattening method allows a simple analytic treatment of low-dimensional intrinsic curvature constraints, and allows consistent interpolation operations. We analyze reduced manifolds for human interaction activities, and demonstrate that the algorithm learns continuous generative models that are useful for tracking and for the reconstruction of 3D human motion in monocular video.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {96},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015381,
author = {Strens, Malcolm},
title = {Efficient Hierarchical MCMC for Policy Search},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015381},
doi = {10.1145/1015330.1015381},
abstract = {Many inference and optimization tasks in machine learning can be solved by sampling approaches such as Markov Chain Monte Carlo (MCMC) and simulated annealing. These methods can be slow if a single target density query requires many runs of a simulation (or a complete sweep of a training data set). We introduce a hierarchy of MCMC samplers that allow most steps to be taken in the solution space using only a small sample of simulation runs (or training examples). This is shown to accelerate learning in a policy search optimization task.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {97},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015393,
author = {Su, Ting and Dy, Jennifer G.},
title = {Automated Hierarchical Mixtures of Probabilistic Principal Component Analyzers},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015393},
doi = {10.1145/1015330.1015393},
abstract = {Many clustering algorithms fail when dealing with high dimensional data. Principal component analysis (PCA) is a popular dimensionality reduction algorithm. However, it assumes a single multivariate Gaussian model, which provides a global linear projection of the data. Mixture of probabilistic principal component analyzers (PPCA) provides a better model to the clustering paradigm. It provides a local linear PCA projection for each multivariate Gaussian cluster component. We extend this model to build hierarchical mixtures of PPCA. Hierarchical clustering provides a flexible representation showing relationships among clusters in various perceptual levels. We introduce an automated hierarchical mixture of PPCA algorithm, which utilizes the integrated classification likelihood as a criterion for splitting and stopping the addition of hierarchical levels. An automated approach requires automated methods for initialization, determining the number of principal component dimensions, and determining when to split clusters. We address each of these in the paper. This automated approach results in a coarse to fine local component model with varying projections and with different number of dimensions for each cluster.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {98},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015422,
author = {Sutton, Charles and Rohanimanesh, Khashayar and McCallum, Andrew},
title = {Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015422},
doi = {10.1145/1015330.1015422},
abstract = {In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random fields (DCRFs), a generalization of linear-chain conditional random fields (CRFs) in which each time slice contains a set of state variables and edges---a distributed state representation as in dynamic Bayesian networks (DBNs)---and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {99},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015445,
author = {Szepesv\'{a}ri, Csaba and Smart, William D.},
title = {Interpolation-Based Q-Learning},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015445},
doi = {10.1145/1015330.1015445},
abstract = {We consider a variant of Q-learning in continuous state spaces under the total expected discounted cost criterion combined with local function approximation methods. Provided that the function approximator satisfies certain interpolation properties, the resulting algorithm is shown to converge with probability one. The limit function is shown to satisfy a fixed point equation of the Bellman type, where the fixed point operator depends on the stationary distribution of the exploration policy and the function approximation method. The basic algorithm is extended in several ways. In particular, a variant of the algorithm is obtained that is shown to converge in probability to the optimal Q function. Preliminary computer simulations are presented that confirm the validity of the approach.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {100},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015405,
author = {Tao, Qingping and Scott, Stephen and Vinodchandran, N. V. and Osugi, Thomas Takeo},
title = {SVM-Based Generalized Multiple-Instance Learning via Approximate Box Counting},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015405},
doi = {10.1145/1015330.1015405},
abstract = {The multiple-instance learning (MIL) model has been very successful in application areas such as drug discovery and content-based image-retrieval. Recently, a generalization of this model and an algorithm for this generalization were introduced, showing significant advantages over the conventional MIL model in certain application areas. Unfortunately, this algorithm is inherently inefficient, preventing scaling to high dimensions. We reformulate this algorithm using a kernel for a support vector machine, reducing its time complexity from exponential to polynomial. Computing the kernel is equivalent to counting the number of axis-parallel boxes in a discrete, bounded space that contain at least one point from each of two multisets P and Q. We show that this problem is #P-complete, but then give a fully polynomial randomized approximation scheme (FPRAS) for it. Finally, we empirically evaluate our kernel.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {101},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015444,
author = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne},
title = {Learning Associative Markov Networks},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015444},
doi = {10.1145/1015330.1015444},
abstract = {Markov networks are extensively used to model complex sequential, spatial, and relational interactions in fields as diverse as image processing, natural language analysis, and bioinformatics. However, inference and learning in general Markov networks is intractable. In this paper, we focus on learning a large subclass of such models (called associative Markov networks) that are tractable or closely approximable. This subclass contains networks of discrete variables with K labels each and clique potentials that favor the same labels for all variables in the clique. Such networks capture the "guilt by association" pattern of reasoning present in many domains, in which connected ("associated") variables tend to have the same label. Our approach exploits a linear programming relaxation for the task of finding the best joint assignment in such networks, which provides an approximate quadratic program (QP) for the problem of learning a margin-maximizing Markov network. We show that for associative Markov network over binary-valued variables, this approximate QP is guaranteed to return an optimal parameterization for Markov networks of arbitrary topology. For the nonbinary case, optimality is not guaranteed, but the relaxation produces good solutions in practice. Experimental results with hypertext and newswire classification show significant advantages over standard approaches.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {102},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015442,
author = {Toutanova, Kristina and Manning, Christopher D. and Ng, Andrew Y.},
title = {Learning Random Walk Models for Inducing Word Dependency Distributions},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015442},
doi = {10.1145/1015330.1015442},
abstract = {Many NLP tasks rely on accurately estimating word dependency probabilities P(ω1|ω2), where the words w1 and w2 have a particular relationship (such as verb-object). Because of the sparseness of counts of such dependencies, smoothing and the ability to use multiple sources of knowledge are important challenges. For example, if the probability P(N|V) of noun N being the subject of verb V is high, and V takes similar objects to V', and V' is synonymous to V", then we want to conclude that P(N|V") should also be reasonably high---even when those words did not cooccur in the training data.To capture these higher order relationships, we propose a Markov chain model, whose stationary distribution is used to give word probability estimates. Unlike the manually defined random walks used in some link analysis algorithms, we show how to automatically learn a rich set of parameters for the Markov chain's transition probabilities. We apply this model to the task of prepositional phrase attachment, obtaining an accuracy of 87.54%.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {103},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015341,
author = {Tsochantaridis, Ioannis and Hofmann, Thomas and Joachims, Thorsten and Altun, Yasemin},
title = {Support Vector Machine Learning for Interdependent and Structured Output Spaces},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015341},
doi = {10.1145/1015330.1015341},
abstract = {Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {104},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015427,
author = {Vural, Volkan and Dy, Jennifer G.},
title = {A Hierarchical Method for Multi-Class Support Vector Machines},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015427},
doi = {10.1145/1015330.1015427},
abstract = {We introduce a framework, which we call Divide-by-2 (DB2), for extending support vector machines (SVM) to multi-class problems. DB2 offers an alternative to the standard one-against-one and one-against-rest algorithms. For an N class problem, DB2 produces an N − 1 node binary decision tree where nodes represent decision boundaries formed by N − 1 SVM binary classifiers. This tree structure allows us to present a generalization and a time complexity analysis of DB2. Our analysis and related experiments show that, DB2 is faster than one-against-one and one-against-rest algorithms in terms of testing time, significantly faster than one-against-rest in terms of training time, and that the cross-validation accuracy of DB2 is comparable to these two methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {105},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015345,
author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
title = {Learning a Kernel Matrix for Nonlinear Dimensionality Reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015345},
doi = {10.1145/1015330.1015345},
abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {106},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015396,
author = {Welling, Max and Rosen-Zvi, Michal and Teh, Yee Whye},
title = {Approximate Inference by Markov Chains on Union Spaces},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015396},
doi = {10.1145/1015330.1015396},
abstract = {A standard method for approximating averages in probabilistic models is to construct a Markov chain in the product space of the random variables with the desired equilibrium distribution. Since the number of configurations in this space grows exponentially with the number of random variables we often need to represent the distribution with samples. In this paper we show that if one is interested in averages over single variables only, an alternative Markov chain defined on the much smaller "union space", which can be evolved exactly, becomes feasible. The transition kernel of this Markov chain is based on conditional distributions for pairs of variables and we present ways to approximate them using approximate inference algorithms such as mean field, factorized neighbors and belief propagation. Robustness to these approximations and error bounds on the estimates follow from stability analysis for Markov chains. We also present ideas on a new class of algorithms that iterate between increasingly accurate estimates for conditional and marginal distributions. Experiments validate the proposed methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {107},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015346,
author = {Wierstra, Daan and Wiering, Marco},
title = {Utile Distinction Hidden Markov Models},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015346},
doi = {10.1145/1015330.1015346},
abstract = {This paper addresses the problem of constructing good action selection policies for agents acting in partially observable environments, a class of problems generally known as Partially Observable Markov Decision Processes. We present a novel approach that uses a modification of the well-known Baum-Welch algorithm for learning a Hidden Markov Model (HMM) to predict both percepts and utility in a non-deterministic world. This enables an agent to make decisions based on its previous history of actions, observations, and rewards. Our algorithm, called Utile Distinction Hidden Markov Models (UDHMM), handles the creation of memory well in that it tends to create perceptual and utility distinctions only when needed, while it can still discriminate states based on histories of arbitrary length. The experimental results in highly stochastic problem domains show very good performance.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {108},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015440,
author = {Wingate, David and Seppi, Kevin D.},
title = {P3VI: A Partitioned, Prioritized, Parallel Value Iterator},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015440},
doi = {10.1145/1015330.1015440},
abstract = {We present an examination of the state-of-the-art for using value iteration to solve large-scale discrete Markov Decision Processes. We introduce an architecture which combines three independent performance enhancements (the intelligent prioritization of computation, state partitioning, and massively parallel processing) into a single algorithm. We show that each idea improves performance in a different way, meaning that algorithm designers do not have to trade one improvement for another. We give special attention to parallelization issues, discussing how to efficiently partition states, distribute partitions to processors, minimize message passing and ensure high scalability. We present experimental results which demonstrate that this approach solves large problems in reasonable time.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {109},
numpages = {8},
keywords = {reinforcement learning, value iteration, Asynchronous dynamic programming},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015436,
author = {Wu, Pengcheng and Dietterich, Thomas G.},
title = {Improving SVM Accuracy by Training on Auxiliary Data Sources},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015436},
doi = {10.1145/1015330.1015436},
abstract = {The standard model of supervised learning assumes that training and test data are drawn from the same underlying distribution. This paper explores an application in which a second, auxiliary, source of data is available drawn from a different distribution. This auxiliary data is more plentiful, but of significantly lower quality, than the training and test data. In the SVM framework, a training example has two roles: (a) as a data point to constrain the learning process and (b) as a candidate support vector that can form part of the definition of the classifier. The paper considers using the auxiliary data in either (or both) of these roles. This auxiliary data framework is applied to a problem of classifying images of leaves of maple and oak trees using a kernel derived from the shapes of the leaves. Experiments show that when the training data set is very small, training with auxiliary data can produce large improvements in accuracy, even when the auxiliary data is significantly different from the training (and test) data. The paper also introduces techniques for adjusting the kernel scores of the auxiliary data points to make them more comparable to the training data points.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {110},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015423,
author = {Xing, Eric and Sharan, Roded and Jordan, Michael I.},
title = {Bayesian Haplo-Type Inference via the Dirichlet Process},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015423},
doi = {10.1145/1015330.1015423},
abstract = {The problem of inferring haplotypes from genotypes of single nucleotide polymorphisms (SNPs) is essential for the understanding of genetic variation within and among populations, with important applications to the genetic analysis of disease propensities and other complex traits. The problem can be formulated as a mixture model, where the mixture components correspond to the pool of haplotypes in the population. The size of this pool is unknown; indeed, knowing the size of the pool would correspond to knowing something significant about the genome and its history. Thus methods for fitting the genotype mixture must crucially address the problem of estimating a mixture with an unknown number of mixture components. In this paper we present a Bayesian approach to this problem based on a nonparametric prior known as the Dirichlet process. The model also incorporates a likelihood that captures statistical errors in the haplotype/genotype relationship. We apply our approach to the analysis of both simulated and real genotype data, and compare to extant methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {111},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015347,
author = {Ye, Jieping},
title = {Generalized Low Rank Approximations of Matrices},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015347},
doi = {10.1145/1015330.1015347},
abstract = {We consider the problem of computing low rank approximations of matrices. The novelty of our approach is that the low rank approximations are on a sequence of matrices. Unlike the problem of low rank approximations of a single matrix, which was well studied in the past, the proposed algorithm in this paper does not admit a closed form solution in general. We did extensive experiments on face image data to evaluate the effectiveness of the proposed algorithm and compare the computed low rank approximations with those obtained from traditional Singular Value Decomposition based method.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {112},
numpages = {8},
keywords = {low rank approximation, classification, Singular Value Decomposition (SVD)},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015348,
author = {Ye, Jieping and Janardan, Ravi and Li, Qi and Park, Haesun},
title = {Feature Extraction via Generalized Uncorrelated Linear Discriminant Analysis},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015348},
doi = {10.1145/1015330.1015348},
abstract = {Feature extraction is important in many applications, such as text and image retrieval, because of high dimensionality. Uncorrelated Linear Discriminant Analysis (ULDA) was recently proposed for feature extraction. The extracted features via ULDA were shown to be statistically uncorrelated, which is desirable for many applications. In this paper, we will first propose the ULDA/QR algorithm to simplify the previous implementation of ULDA. Then we propose the ULDA/GSVD algorithm, based on a novel optimization criterion, to address the singularity problem. It is applicable for undersampled problem, where the data dimension is much larger than the data size, such as text and image retrieval. The novel criterion used in ULDA/GSVD is the perturbed version of the one from ULDA/QR, while surprisingly, the solution to ULDA/GSVD is shown to be independent of the amount of perturbation applied. We did extensive experiments on text and face image data to show the effectiveness of ULDA/GSVD and compare with other popular feature extraction algorithms.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {113},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015425,
author = {Zadrozny, Bianca},
title = {Learning and Evaluating Classifiers under Sample Selection Bias},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015425},
doi = {10.1145/1015330.1015425},
abstract = {Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {114},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015335,
author = {Zhang, Jian and Yang, Yiming},
title = {Probabilistic Score Estimation with Piecewise Logistic Regression},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015335},
doi = {10.1145/1015330.1015335},
abstract = {Well-calibrated probabilities are necessary in many applications like probabilistic frameworks or cost-sensitive tasks. Based on previous success of asymmetric Laplace method in calibrating text classifiers' scores, we propose to use piecewise logistic regression, which is a simple extension of standard logistic regression, as an alternative method in the discriminative family. We show that both methods have the flexibility to be piecewise linear functions in log-odds, but they are based on quite different assumptions. We evaluated asymmetric Laplace method, piecewise logistic regression and standard logistic regression over standard text categorization collections (Reuters-21578 and TRECAP) with three classifiers (SVM, Naive Bayes and Logistic Regression Classifier), and observed that piecewise logistic regression performs significantly better than the other two methods in the log-loss metric.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {115},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015332,
author = {Zhang, Tong},
title = {Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015332},
doi = {10.1145/1015330.1015332},
abstract = {Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {116},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015342,
author = {Zhang, Zhihua and Kwok, James T. and Yeung, Dit-Yan},
title = {Surrogate Maximization/Minimization Algorithms for AdaBoost and the Logistic Regression Model},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015342},
doi = {10.1145/1015330.1015342},
abstract = {Surrogate maximization (or minimization) (SM) algorithms are a family of algorithm that can be regarded as a generalization of expectation-maximization (EM) algorithms. There are three major approaches to the construction of surrogate function, all relying on the convexity of some function. In this paper, we solve the boosting problem by proposing SM algorithms for the corresponding optimization problem. Specifically, for AdaBoost, we derive an SM algorithm that can be shown to be identical to the algorithm proposed by Collins et al. (2002) based on Bregman distance. More importantly, for LogitBoost (or logistic boosting), we use several methods to construct different surrogate functions which result in different SM algorithms. By combining multiple methods, we are able to derive an SM algorithm that is also the same as an algorithm derived by Collins et al. (2002). Our approach based on SM algorithms is much simpler and convergence results follow naturally.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {117},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@inproceedings{10.1145/1015330.1015368,
author = {Zhang, Zhihua and Yeung, Dit-Yan and Kwok, James T.},
title = {Bayesian Inference for Transductive Learning of Kernel Matrix Using the Tanner-Wong Data Augmentation Algorithm},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015368},
doi = {10.1145/1015330.1015368},
abstract = {In kernel methods, an interesting recent development seeks to learn a good kernel from empirical data automatically. In this paper, by regarding the transductive learning of the kernel matrix as a missing data problem, we propose a Bayesian hierarchical model for the problem and devise the Tanner-Wong data augmentation algorithm for making inference on the model. The Tanner-Wong algorithm is closely related to Gibbs sampling, and it also bears a strong resemblance to the expectation-maximization (EM) algorithm. For an efficient implementation, we propose a simplified Bayesian hierarchical model and the corresponding Tanner-Wong algorithm. We express the relationship between the kernel on the input space and the kernel on the output space as a symmetric-definite generalized eigenproblem. Based on this eigenproblem, an efficient approach to choosing the base kernel matrices is presented. The effectiveness of our Bayesian model with the Tanner-Wong algorithm is demonstrated through some classification experiments showing promising results.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {118},
numpages = {8},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

