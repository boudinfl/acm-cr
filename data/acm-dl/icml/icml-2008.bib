@inproceedings{10.1145/1390156.1390157,
author = {Adams, Ryan Prescott and Stegle, Oliver},
title = {Gaussian Process Product Models for Nonparametric Nonstationarity},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390157},
doi = {10.1145/1390156.1390157},
abstract = {Stationarity is often an unrealistic prior assumption for Gaussian process regression. One solution is to predefine an explicit nonstationary covariance function, but such covariance functions can be difficult to specify and require detailed prior knowledge of the nonstationarity. We propose the Gaussian process product model (GPPM) which models data as the pointwise product of two latent Gaussian processes to nonparametrically infer nonstationary variations of amplitude. This approach differs from other nonparametric approaches to covariance function inference in that it operates on the outputs rather than the inputs, resulting in a significant reduction in computational cost and required data for inference. We present an approximate inference scheme using Expectation Propagation. This variational approximation yields convenient GP hyperparameter selection and compact approximate predictive distributions.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1–8},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390158,
author = {Allauzen, Cyril and Mohri, Mehryar and Talwalkar, Ameet},
title = {Sequence Kernels for Predicting Protein Essentiality},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390158},
doi = {10.1145/1390156.1390158},
abstract = {The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database, which includes several Pfam domains. We then present novel and general domain-based sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem -- several thousands of domains with individual domains sometimes containing thousands of sequences -- by representing and efficiently computing these kernels using automata. We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality, while requiring no manual tuning.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {9–16},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390159,
author = {An, Qi and Wang, Chunping and Shterev, Ivo and Wang, Eric and Carin, Lawrence and Dunson, David B.},
title = {Hierarchical Kernel Stick-Breaking Process for Multi-Task Image Analysis},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390159},
doi = {10.1145/1390156.1390159},
abstract = {The kernel stick-breaking process (KSBP) is employed to segment general imagery, imposing the condition that patches (small blocks of pixels) that are spatially proximate are more likely to be associated with the same cluster (segment). The number of clusters is not set a priori and is inferred from the hierarchical Bayesian model. Further, KSBP is integrated with a shared Dirichlet process prior to simultaneously model multiple images, inferring their inter-relationships. This latter application may be useful for sorting and learning relationships between multiple images. The Bayesian inference algorithm is based on a hybrid of variational Bayesian analysis and local sampling. In addition to providing details on the model and associated inference framework, example results are presented for several image-analysis problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {17–24},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390160,
author = {Bach, Francis R.},
title = {Graph Kernels between Point Clouds},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390160},
doi = {10.1145/1390156.1390160},
abstract = {Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow one to use kernel methods for such objects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on probabilistic graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {25–32},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390161,
author = {Bach, Francis R.},
title = {Bolasso: Model Consistent Lasso Estimation through the Bootstrap},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390161},
doi = {10.1145/1390156.1390161},
abstract = {We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {33–40},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390162,
author = {Barrett, Leon and Narayanan, Srini},
title = {Learning All Optimal Policies with Multiple Criteria},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390162},
doi = {10.1145/1390156.1390162},
abstract = {We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once. The algorithm can be viewed as an extension to standard reinforcement learning for MDPs where instead of repeatedly backing up maximal expected rewards, we back up the set of expected rewards that are maximal for some set of linear preferences (given by a weight vector, w). We present the algorithm along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function. The solution reduces to the standard value iteration algorithm for a specific weight vector, w.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {41–47},
numpages = {7},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390163,
author = {Bergeron, Charles and Zaretzki, Jed and Breneman, Curt and Bennett, Kristin P.},
title = {Multiple Instance Ranking},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390163},
doi = {10.1145/1390156.1390163},
abstract = {This paper introduces a novel machine learning model called multiple instance ranking (MIRank) that enables ranking to be performed in a multiple instance learning setting. The motivation for MIRank stems from the hydrogen abstraction problem in computational chemistry, that of predicting the group of hydrogen atoms from which a hydrogen is abstracted (removed) during metabolism. The model predicts the preferred hydrogen group within a molecule by ranking the groups, with the ambiguity of not knowing which hydrogen atom within the preferred group is actually abstracted. This paper formulates MIRank in its general context and proposes an algorithm for solving MIRank problems using successive linear programming. The method outperforms multiple instance classification models on several real and synthetic datasets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {48–55},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390164,
author = {Bickel, Steffen and Bogojeska, Jasmina and Lengauer, Thomas and Scheffer, Tobias},
title = {Multi-Task Learning for HIV Therapy Screening},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390164},
doi = {10.1145/1390156.1390164},
abstract = {We address the problem of learning classifiers for a large number of tasks. We derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task. Our work is motivated by the problem of predicting the outcome of a therapy attempt for a patient who carries an HIV virus with a set of observed genetic properties. Such predictions need to be made for hundreds of possible combinations of drugs, some of which use similar biochemical mechanisms. Multi-task learning enables us to make predictions even for drug combinations with few or no training examples and substantially improves the overall prediction accuracy.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {56–63},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390165,
author = {Biggs, Michael and Ghodsi, Ali and Vavasis, Stephen},
title = {Nonnegative Matrix Factorization via Rank-One Downdate},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390165},
doi = {10.1145/1390156.1390165},
abstract = {Nonnegative matrix factorization (NMF) was popularized as a tool for data mining by Lee and Seung in 1999. NMF attempts to approximate a matrix with nonnegative entries by a product of two low-rank matrices, also with nonnegative entries. We propose an algorithm called rank-one downdate (R1D) for computing an NMF that is partly motivated by the singular value decomposition. This algorithm computes the dominant singular values and vectors of adaptively determined sub-matrices of a matrix. On each iteration, R1D extracts a rank-one submatrix from the original matrix according to an objective function. We establish a theoretical result that maximizing this objective function corresponds to correctly classifying articles in a nearly separable corpus. We also provide computational experiments showing the success of this method in identifying features in realistic datasets. The method is also much faster than other NMF routines.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {64–71},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390166,
author = {Bowling, Michael and Johanson, Michael and Burch, Neil and Szafron, Duane},
title = {Strategy Evaluation in Extensive Games with Importance Sampling},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390166},
doi = {10.1145/1390156.1390166},
abstract = {Typically agent evaluation is done through Monte Carlo estimation. However, stochastic agent decisions and stochastic outcomes can make this approach inefficient, requiring many samples for an accurate estimate. We present a new technique that can be used to simultaneously evaluate many strategies while playing a single strategy in the context of an extensive game. This technique is based on importance sampling, but utilizes two new mechanisms for significantly reducing variance in the estimates. We demonstrate its effectiveness in the domain of poker, where stochasticity makes traditional evaluation problematic.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {72–79},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390167,
author = {Bryan, Brent and Schneider, Jeff},
title = {Actively Learning Level-Sets of Composite Functions},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390167},
doi = {10.1145/1390156.1390167},
abstract = {Scientists frequently have multiple types of experiments and data sets on which they can test the validity of their parameterized models and locate plausible regions for the model parameters. By examining multiple data sets, scientists can obtain inferences which typically are much more informative than the deductions derived from each of the data sources independently. Several standard data combination techniques result in target functions which are a weighted sum of the observed data sources. Thus, computing constraints on the plausible regions of the model parameter space can be formulated as finding a level set of a target function which is the sum of observable functions. We propose an active learning algorithm for this problem which selects both a sample (from the parameter space) and an observable function upon which to compute the next sample. Empirical tests on synthetic functions and on real data for an eight parameter cosmological model show that our algorithm significantly reduces the number of samples required to identify the desired level-set.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {80–87},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390168,
author = {Caron, Fran\c{c}ois and Doucet, Arnaud},
title = {Sparse Bayesian Nonparametric Regression},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390168},
doi = {10.1145/1390156.1390168},
abstract = {One of the most common problems in machine learning and statistics consists of estimating the mean response Xβ from a vector of observations y assuming y = Xβ + ε where X is known, β is a vector of parameters of interest and ε a vector of stochastic errors. We are particularly interested here in the case where the dimension K of β is much higher than the dimension of y. We propose some flexible Bayesian models which can yield sparse estimates of β. We show that as K → ∞ these models are closely related to a class of L\'{e}vy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {88–95},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390169,
author = {Caruana, Rich and Karampatziakis, Nikos and Yessenalina, Ainur},
title = {An Empirical Evaluation of Supervised Learning in High Dimensions},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390169},
doi = {10.1145/1390156.1390169},
abstract = {In this paper we perform an empirical evaluation of supervised learning on high-dimensional data. We evaluate performance on three metrics: accuracy, AUC, and squared loss and study the effect of increasing dimensionality on the performance of the learning algorithms. Our findings are consistent with previous studies for problems of relatively low dimension, but suggest that as dimensionality increases the relative performance of the learning algorithms changes. To our surprise, the method that performs consistently well across all dimensions is random forests, followed by neural nets, boosted trees, and SVMs.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {96–103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390170,
author = {Catanzaro, Bryan and Sundaram, Narayanan and Keutzer, Kurt},
title = {Fast Support Vector Machine Training and Classification on Graphics Processors},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390170},
doi = {10.1145/1390156.1390170},
abstract = {Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training running on a GPU, using the Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LIBSVM (5-24x over our own CPU based SVM classifier).},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {104–111},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390171,
author = {Cayton, Lawrence},
title = {Fast Nearest Neighbor Retrieval for Bregman Divergences},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390171},
doi = {10.1145/1390156.1390171},
abstract = {We present a data structure enabling efficient nearest neighbor (NN) retrieval for bregman divergences. The family of bregman divergences includes many popular dissimilarity measures including KL-divergence (relative entropy), Mahalanobis distance, and Itakura-Saito divergence. These divergences present a challenge for efficient NN retrieval because they are not, in general, metrics, for which most NN data structures are designed. The data structure introduced in this work shares the same basic structure as the popular metric ball tree, but employs convexity properties of bregman divergences in place of the triangle inequality. Experiments demonstrate speedups over brute-force search of up to several orders of magnitude.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {112–119},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390172,
author = {Cevikalp, Hakan and Triggs, Bill and Polikar, Robi},
title = {Nearest Hyperdisk Methods for High-Dimensional Classification},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390172},
doi = {10.1145/1390156.1390172},
abstract = {In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and kernel methods to have irregular decision boundaries. One solution is to "fill in the holes" by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models. Methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied. Here we propose a method based on the bounding hyperdisk of each class - the intersection of the affine hull and the smallest bounding hypersphere of its training samples. We argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres: they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high-dimensional convex hulls. We show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on non-Euclidean distance metrics. Experiments on several classification problems show promising results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {120–127},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390173,
author = {Chen, David L. and Mooney, Raymond J.},
title = {Learning to Sportscast: A Test of Grounded Language Acquisition},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390173},
doi = {10.1145/1390156.1390173},
abstract = {We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {128–135},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390174,
author = {Chen, Jianhui and Ye, Jieping},
title = {Training SVM with Indefinite Kernels},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390174},
doi = {10.1145/1390156.1390174},
abstract = {Similarity matrices generated from many applications may not be positive semidefinite, and hence can't fit into the kernel machine framework. In this paper, we study the problem of training support vector machines with an indefinite kernel. We consider a regularized SVM formulation, in which the indefinite kernel matrix is treated as a noisy observation of some unknown positive semidefinite one (proxy kernel) and the support vectors and the proxy kernel can be computed simultaneously. We propose a semi-infinite quadratically constrained linear program formulation for the optimization, which can be solved iteratively to find a global optimum solution. We further propose to employ an additional pruning strategy, which significantly improves the efficiency of the algorithm, while retaining the convergence property of the algorithm. In addition, we show the close relationship between the proposed formulation and multiple kernel learning. Experiments on a collection of benchmark data sets demonstrate the efficiency and effectiveness of the proposed algorithm.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {136–143},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390175,
author = {Coates, Adam and Abbeel, Pieter and Ng, Andrew Y.},
title = {Learning for Control from Multiple Demonstrations},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390175},
doi = {10.1145/1390156.1390175},
abstract = {We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {144–151},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390176,
author = {Coleman, Tom and Saunderson, James and Wirth, Anthony},
title = {Spectral Clustering with Inconsistent Advice},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390176},
doi = {10.1145/1390156.1390176},
abstract = {Clustering with advice (often known as constrained clustering) has been a recent focus of the data mining community. Success has been achieved incorporating advice into the k-means and spectral clustering frameworks. Although the theory community has explored inconsistent advice, it has not yet been incorporated into spectral clustering. Extending work of De Bie and Cristianini, we set out a framework for finding minimum normalised cuts, subject to inconsistent advice.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {152–159},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390177,
author = {Collobert, Ronan and Weston, Jason},
title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390178,
author = {Corrada-Emmanuel, Andr\'{e}s and Schultz, Howard},
title = {Autonomous Geometric Precision Error Estimation in Low-Level Computer Vision Tasks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390178},
doi = {10.1145/1390156.1390178},
abstract = {Errors in map-making tasks using computer vision are sparse. We demonstrate this by considering the construction of digital elevation models that employ stereo matching algorithms to triangulate real-world points. This sparsity, coupled with a geometric theory of errors recently developed by the authors, allows for autonomous agents to calculate their own precision independently of ground truth. We connect these developments with recent advances in the mathematics of sparse signal reconstruction or compressed sensing. The theory presented here extends the autonomy of 3-D model reconstructions discovered in the 1990s to their errors.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {168–175},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390179,
author = {Cortes, Corinna and Mohri, Mehryar and Pechyony, Dmitry and Rastogi, Ashish},
title = {Stability of Transductive Regression Algorithms},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390179},
doi = {10.1145/1390156.1390179},
abstract = {This paper uses the notion of algorithmic stability to derive novel generalization bounds for several families of transductive regression algorithms, both by using convexity and closed-form solutions. Our analysis helps compare the stability of these algorithms. It suggests that several existing algorithms might not be stable but prescribes a technique to make them stable. It also reports the results of experiments with local transductive regression demonstrating the benefit of our stability bounds for model selection, in particular for determining the radius of the local neighborhood used by the algorithm.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {176–183},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390180,
author = {Crammer, Koby and Talukdar, Partha Pratim and Pereira, Fernando},
title = {A Rate-Distortion One-Class Model and Its Applications to Clustering},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390180},
doi = {10.1145/1390156.1390180},
abstract = {In one-class classification we seek a rule to find a coherent subset of instances similar to a few positive examples in a large pool of instances. The problem can be formulated and analyzed naturally in a rate-distortion framework, leading to an efficient algorithm that compares well with two previous one-class methods. The model can be also be extended to remove background clutter in clustering to improve cluster purity.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {184–191},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390181,
author = {Cunningham, John P. and Shenoy, Krishna V. and Sahani, Maneesh},
title = {Fast Gaussian Process Methods for Point Process Intensity Estimation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390181},
doi = {10.1145/1390156.1390181},
abstract = {Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive framework within which to infer underlying intensity functions. The result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts. However, a naive implementation will become computationally infeasible in any problem of reasonable size, both in memory and run time requirements. We demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {192–199},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390182,
author = {Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
title = {Self-Taught Clustering},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390182},
doi = {10.1145/1390156.1390182},
abstract = {This paper focuses on a new clustering task, called self-taught clustering. Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of target unlabeled data with the help of a large amount of auxiliary unlabeled data. The target and auxiliary data can be different in topic distribution. We show that even when the target data are not sufficient to allow effective learning of a high quality feature representation, it is possible to learn the useful features with the help of the auxiliary data on which the target data can be clustered effectively. We propose a co-clustering based self-taught clustering algorithm to tackle this problem, by clustering the target and auxiliary data simultaneously to allow the feature representation from the auxiliary data to influence the target data through a common set of features. Under the new data representation, clustering on the target data can be improved. Our experiments on image clustering show that our algorithm can greatly outperform several state-of-the-art clustering methods when utilizing irrelevant unlabeled auxiliary data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {200–207},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390183,
author = {Dasgupta, Sanjoy and Hsu, Daniel},
title = {Hierarchical Sampling for Active Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390183},
doi = {10.1145/1390156.1390183},
abstract = {We present an active learning scheme that exploits cluster structure in data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {208–215},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390184,
author = {Dekel, Ofer and Shamir, Ohad},
title = {Learning to Classify with Missing and Corrupted Features},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390184},
doi = {10.1145/1390156.1390184},
abstract = {After a classifier is trained using a machine learning algorithm and put to use in a real world system, it often faces noise which did not appear in the training data. Particularly, some subset of features may be missing or may become corrupted. We present two novel machine learning techniques that are robust to this type of classification-time noise. First, we solve an approximation to the learning problem using linear programming. We analyze the tightness of our approximation and prove statistical risk bounds for this approach. Second, we define the online-learning variant of our problem, address this variant using a modified Perceptron, and obtain a statistical learning algorithm using an online-to-batch technique. We conclude with a set of experiments that demonstrate the effectiveness of our algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {216–223},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390185,
author = {Dembczy\'{n}ski, Krzysztof and Kot\l{}owski, Wojciech and S\l{}owi\'{n}ski, Roman},
title = {Maximum Likelihood Rule Ensembles},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390185},
doi = {10.1145/1390156.1390185},
abstract = {We propose a new rule induction algorithm for solving classification problems via probability estimation. The main advantage of decision rules is their simplicity and good interpretability. While the early approaches to rule induction were based on sequential covering, we follow an approach in which a single decision rule is treated as a base classifier in an ensemble. The ensemble is built by greedily minimizing the negative loglikelihood which results in estimating the class conditional probability distribution. The introduced approach is compared with other decision rule induction algorithms such as SLIPPER, LRI and RuleFit.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {224–231},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390186,
author = {Dick, Uwe and Haider, Peter and Scheffer, Tobias},
title = {Learning from Incomplete Data with Infinite Imputations},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390186},
doi = {10.1145/1390156.1390186},
abstract = {We address the problem of learning decision functions from training data in which some attribute values are unobserved. This problem can arise, for instance, when training data is aggregated from multiple sources, and some sources record only a subset of attributes. We derive a generic joint optimization problem in which the distribution governing the missing values is a free parameter. We show that the optimal solution concentrates the density mass on finitely many imputations, and provide a corresponding algorithm for learning from incomplete data. We report on empirical results on benchmark data, and on the email spam application that motivates our work.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {232–239},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390187,
author = {Diuk, Carlos and Cohen, Andre and Littman, Michael L.},
title = {An Object-Oriented Representation for Efficient Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390187},
doi = {10.1145/1390156.1390187},
abstract = {Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well-known Taxi domain, plus a real-life videogame.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {240–247},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390188,
author = {Donmez, Pinar and Carbonell, Jaime G.},
title = {Optimizing Estimated Loss Reduction for Active Sampling in Rank Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390188},
doi = {10.1145/1390156.1390188},
abstract = {Learning to rank is becoming an increasingly popular research area in machine learning. The ranking problem aims to induce an ordering or preference relations among a set of instances in the input space. However, collecting labeled data is growing into a burden in many rank applications since labeling requires eliciting the relative ordering over the set of alternatives. In this paper, we propose a novel active learning framework for SVM-based and boosting-based rank learning. Our approach suggests sampling based on maximizing the estimated loss differential over unlabeled data. Experimental results on two benchmark corpora show that the proposed model substantially reduces the labeling effort, and achieves superior performance rapidly with as much as 30% relative improvement over the margin-based sampling baseline.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {248–255},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390189,
author = {Doshi, Finale and Pineau, Joelle and Roy, Nicholas},
title = {Reinforcement Learning with Limited Reinforcement: Using Bayes Risk for Active Learning in POMDPs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390189},
doi = {10.1145/1390156.1390189},
abstract = {Partially Observable Markov Decision Processes (POMDPs) have succeeded in planning domains that require balancing actions that increase an agent's knowledge and actions that increase an agent's reward. Unfortunately, most POMDPs are defined with a large number of parameters which are difficult to specify only from domain knowledge. In this paper, we present an approximation approach that allows us to treat the POMDP model parameters as additional hidden state in a "model-uncertainty" POMDP. Coupled with model-directed queries, our planner actively learns good policies. We demonstrate our approach on several POMDP problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {256–263},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390190,
author = {Dredze, Mark and Crammer, Koby and Pereira, Fernando},
title = {Confidence-Weighted Linear Classification},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390190},
doi = {10.1145/1390156.1390190},
abstract = {We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {264–271},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390191,
author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
title = {Efficient Projections onto the <i>l</i><sub>1</sub>-Ball for Learning in High Dimensions},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390191},
doi = {10.1145/1390156.1390191},
abstract = {We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the l1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with l1 projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {272–279},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390192,
author = {Dugas, Charles and Gadoury, David},
title = {Pointwise Exact Bootstrap Distributions of Cost Curves},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390192},
doi = {10.1145/1390156.1390192},
abstract = {Cost curves have recently been introduced as an alternative or complement to ROC curves in order to visualize binary classifiers performance. Of importance to both cost and ROC curves is the computation of confidence intervals along with the curves themselves so that the reliability of a classifier's performance can be assessed. Computing confidence intervals for the difference in performance between two classifiers allows the determination of whether one classifier performs significantly better than another. A simple procedure to obtain confidence intervals for costs or the difference between two costs, under various operating conditions, is to perform bootstrap resampling of the test set. In this paper, we derive exact bootstrap distributions for these values and use these dstributions to obtain confidence intervals, under various operating conditions. Performances of these confidence intervals are measured in terms of coverage accuracies. Simulations show excellent results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {280–287},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390193,
author = {Dundar, M. Murat and Wolf, Matthias and Lakare, Sarang and Salganicoff, Marcos and Raykar, Vikas C.},
title = {Polyhedral Classifier for Target Detection: A Case Study: Colorectal Cancer},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390193},
doi = {10.1145/1390156.1390193},
abstract = {In this study we introduce a novel algorithm for learning a polyhedron to describe the target class. The proposed approach takes advantage of the limited subclass information made available for the negative samples and jointly optimizes multiple hyperplane classifiers each of which is designed to classify positive samples from a subclass of the negative samples. The flat faces of the polyhedron provides robustness whereas multiple faces contributes to the flexibility required to deal with complex datasets. Apart from improving the prediction accuracy of the system, the proposed polyhedral classifier also provides run-time speedups as a by-product when executed in a cascaded framework in real-time. We evaluate the performance of the proposed technique on a real-world Colon dataset both in terms of prediction accuracy and online execution speed.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {288–295},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390194,
author = {Epshteyn, Arkady and Vogel, Adam and DeJong, Gerald},
title = {Active Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390194},
doi = {10.1145/1390156.1390194},
abstract = {When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {296–303},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390195,
author = {Finley, Thomas and Joachims, Thorsten},
title = {Training Structural SVMs When Exact Inference is Intractable},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390195},
doi = {10.1145/1390156.1390195},
abstract = {While discriminative training (e.g., CRF, structural SVM) holds much promise for machine translation, image segmentation, and clustering, the complex inference these applications require make exact training intractable. This leads to a need for approximate training methods. Unfortunately, knowledge about how to perform efficient and effective approximate training is limited. Focusing on structural SVMs, we provide and explore algorithms for two different classes of approximate training algorithms, which we call undergenerating (e.g., greedy) and overgenerating (e.g., relaxations) algorithms. We provide a theoretical and empirical analysis of both types of approximate trained structural SVMs, focusing on fully connected pairwise Markov random fields. We find that models trained with overgenerating methods have theoretic advantages over undergenerating methods, are empirically robust relative to their undergenerating brethren, and relaxed trained models favor non-fractional predictions from relaxed predictors.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {304–311},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390196,
author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
title = {An HDP-HMM for Systems with State Persistence},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390196},
doi = {10.1145/1390156.1390196},
abstract = {The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a sticky extension which allows more robust learning of smoothly varying dynamics. Using DP mixtures, this formulation also allows learning of more complex, multimodal emission distributions. We further develop a sampling algorithm that employs a truncated approximation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with synthetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {312–319},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390197,
author = {Franc, Vojt\v{e}ch and Sonnenburg, Soeren},
title = {Optimized Cutting Plane Algorithm for Support Vector Machines},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390197},
doi = {10.1145/1390156.1390197},
abstract = {We have developed a new Linear Support Vector Machine (SVM) training algorithm called OCAS. Its computational effort scales linearly with the sample size. In an extensive empirical evaluation OCAS significantly outperforms current state of the art SVM solvers, like SVMlight, SVMperf and BMRM, achieving speedups of over 1,000 on some datasets over SVMlight and 20 over SVMperf, while obtaining the same precise Support Vector solution. OCAS even in the early optimization steps shows often faster convergence than the so far in this domain prevailing approximative methods SGD and Pegasos. Effectively parallelizing OCAS we were able to train on a dataset of size 15 million examples (itself about 32GB in size) in just 671 seconds --- a competing string kernel SVM required 97,484 seconds to train on 10 million examples sub-sampled from this dataset.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {320–327},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390198,
author = {Franc, Vojt\v{e}ch and Laskov, Pavel and M\"{u}ller, Klaus-Robert},
title = {Stopping Conditions for Exact Computation of Leave-One-out Error in Support Vector Machines},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390198},
doi = {10.1145/1390156.1390198},
abstract = {We propose a new stopping condition for a Support Vector Machine (SVM) solver which precisely reflects the objective of the Leave-One-Out error computation. The stopping condition guarantees that the output on an intermediate SVM solution is identical to the output of the optimal SVM solution with one data point excluded from the training set. A simple augmentation of a general SVM training algorithm allows one to use a stopping criterion equivalent to the proposed sufficient condition. A comprehensive experimental evaluation of our method shows consistent speedup of the exact LOO computation by our method, up to the factor of 13 for the linear kernel. The new algorithm can be seen as an example of constructive guidance of an optimization algorithm towards achieving the best attainable expected risk at optimal computational cost.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {328–335},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390199,
author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
title = {Reinforcement Learning in the Presence of Rare Events},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390199},
doi = {10.1145/1390156.1390199},
abstract = {We consider the task of reinforcement learning in an environment in which rare significant events occur independently of the actions selected by the controlling agent. If these events are sampled according to their natural probability of occurring, convergence of conventional reinforcement learning algorithms is likely to be slow, and the learning algorithms may exhibit high variance. In this work, we assume that we have access to a simulator, in which the rare event probabilities can be artificially altered. Then, importance sampling can be used to learn with this simulation data. We introduce algorithms for policy evaluation, using both tabular and function approximation representations of the value function. We prove that in both cases, the reinforcement learning algorithms converge. In the tabular case, we also analyze the bias and variance of our approach compared to TD-learning. We evaluate empirically the performance of the algorithm on random Markov Decision Processes, as well as on a large network planning task.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {336–343},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390200,
author = {Gomes, Ryan and Welling, Max and Perona, Pietro},
title = {Memory Bounded Inference in Topic Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390200},
doi = {10.1145/1390156.1390200},
abstract = {What type of algorithms and statistical techniques support learning from very large datasets over long stretches of time? We address this question through a memory bounded version of a variational EM algorithm that approximates inference in a topic model. The algorithm alternates two phases: "model building" and "model compression" in order to always satisfy a given memory constraint. The model building phase expands its internal representation (the number of topics) as more data arrives through Bayesian model selection. Compression is achieved by merging data-items in clumps and only caching their sufficient statistics. Empirically, the resulting algorithm is able to handle datasets that are orders of magnitude larger than the standard batch version.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {344–351},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390201,
author = {G\"{o}nen, Mehmet and Alpaydin, Ethem},
title = {Localized Multiple Kernel Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390201},
doi = {10.1145/1390156.1390201},
abstract = {Recently, instead of selecting a single kernel, multiple kernel learning (MKL) has been proposed which uses a convex combination of kernels, where the weight of each kernel is optimized during training. However, MKL assigns the same weight to a kernel over the whole input space. In this paper, we develop a localized multiple kernel learning (LMKL) algorithm using a gating model for selecting the appropriate kernel function locally. The localizing gating model and the kernel-based classifier are coupled and their optimization is done in a joint manner. Empirical results on ten benchmark and two bioinformatics data sets validate the applicability of our approach. LMKL achieves statistically similar accuracy results compared with MKL by storing fewer support vectors. LMKL can also combine multiple copies of the same kernel function localized in different parts. For example, LMKL with multiple linear kernels gives better accuracy results than using a single linear kernel on bioinformatics data sets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {352–359},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390202,
author = {Gordon, Geoffrey J. and Greenwald, Amy and Marks, Casey},
title = {No-Regret Learning in Convex Games},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390202},
doi = {10.1145/1390156.1390202},
abstract = {Quite a bit is known about minimizing different kinds of regret in experts problems, and how these regret types relate to types of equilibria in the multiagent setting of repeated matrix games. Much less is known about the possible kinds of regret in online convex programming problems (OCPs), or about equilibria in the analogous multiagent setting of repeated convex games. This gap is unfortunate, since convex games are much more expressive than matrix games, and since many important machine learning problems can be expressed as OCPs. In this paper, we work to close this gap: we analyze a spectrum of regret types which lie between external and swap regret, along with their corresponding equilibria, which lie between coarse correlated and correlated equilibrium. We also analyze algorithms for minimizing these regret types. As examples of our framework, we derive algorithms for learning correlated equilibria in polyhedral convex games and extensive-form correlated equilibria in extensive-form games. The former is exponentially more efficient than previous algorithms, and the latter is the first of its type.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {360–367},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390203,
author = {Haffari, Gholamreza and Wang, Yang and Wang, Shaojun and Mori, Greg and Jiao, Feng},
title = {Boosting with Incomplete Information},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390203},
doi = {10.1145/1390156.1390203},
abstract = {In real-world machine learning problems, it is very common that part of the input feature vector is incomplete: either not available, missing, or corrupted. In this paper, we present a boosting approach that integrates features with incomplete information and those with complete information to form a strong classifier. By introducing hidden variables to model missing information, we form loss functions that combine fully labeled data with partially labeled data to effectively learn normalized and unnormalized models. The primal problems of the proposed optimization problems with these loss functions are provided to show their close relationship and the motivations behind them. We use auxiliary functions to bound the change of the loss functions and derive explicit parameter update rules for the learning algorithms. We demonstrate encouraging results on two real-world problems --- visual object recognition in computer vision and named entity recognition in natural language processing --- to show the effectiveness of the proposed boosting approach.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {368–375},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390204,
author = {Hamm, Jihun and Lee, Daniel D.},
title = {Grassmann Discriminant Analysis: A Unifying View on Subspace-Based Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390204},
doi = {10.1145/1390156.1390204},
abstract = {In this paper we propose a discriminant learning framework for problems in which data consist of linear subspaces instead of vectors. By treating subspaces as basic elements, we can make learning algorithms adapt naturally to the problems with linear invariant structures. We propose a unifying view on the subspace-based learning method by formulating the problems on the Grassmann manifold, which is the set of fixed-dimensional linear subspaces of a Euclidean space. Previous methods on the problem typically adopt an inconsistent strategy: feature extraction is performed in the Euclidean space while non-Euclidean distances are used. In our approach, we treat each sub-space as a point in the Grassmann space, and perform feature extraction and classification in the same space. We show feasibility of the approach by using the Grassmann kernel functions such as the Projection kernel and the Binet-Cauchy kernel. Experiments with real image databases show that the proposed method performs well compared with state-of-the-art algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {376–383},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390205,
author = {Heigold, Georg and Deselaers, Thomas and Schl\"{u}ter, Ralf and Ney, Hermann},
title = {Modified MMI/MPE: A Direct Evaluation of the Margin in Speech Recognition},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390205},
doi = {10.1145/1390156.1390205},
abstract = {In this paper we show how common speech recognition training criteria such as the Minimum Phone Error criterion or the Maximum Mutual Information criterion can be extended to incorporate a margin term. Different margin-based training algorithms have been proposed to refine existing training algorithms for general machine learning problems. However, for speech recognition, some special problems have to be addressed and all approaches proposed either lack practical applicability or the inclusion of a margin term enforces significant changes to the underlying model, e.g. the optimization algorithm, the loss function, or the parameterization of the model. In our approach, the conventional training criteria are modified to incorporate a margin term. This allows us to do large-margin training in speech recognition using the same efficient algorithms for accumulation and optimization and to use the same software as for conventional discriminative training. We show that the proposed criteria are equivalent to Support Vector Machines with suitable smooth loss functions, approximating the non-smooth hinge loss function or the hard error (e.g. phone error). Experimental results are given for two different tasks: the rather simple digit string recognition task Sietill which severely suffers from overfitting and the large vocabulary European Parliament Plenary Sessions English task which is supposed to be dominated by the risk and the generalization does not seem to be such an issue.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {384–391},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390206,
author = {Heller, Katherine A. and Williamson, Sinead and Ghahramani, Zoubin},
title = {Statistical Models for Partial Membership},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390206},
doi = {10.1145/1390156.1390206},
abstract = {We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data (Gasch &amp; Eisen, 2002). Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {392–399},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390207,
author = {Hoi, Steven C. H. and Jin, Rong},
title = {Active Kernel Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390207},
doi = {10.1145/1390156.1390207},
abstract = {Identifying the appropriate kernel function/matrix for a given dataset is essential to all kernel-based learning techniques. A number of kernel learning algorithms have been proposed to learn kernel functions or matrices from side information (e.g., either labeled examples or pairwise constraints). However, most previous studies are limited to "passive" kernel learning in which side information is provided beforehand. In this paper we present a framework of Active Kernel Learning (AKL) that actively identifies the most informative pairwise constraints for kernel learning. The key challenge of active kernel learning is how to measure the informativeness of an example pair given its class label is unknown. To this end, we propose a min-max approach for active kernel learning that selects the example pair that results in a large classification margin regardless of its assigned class label. We furthermore approximate the related optimization problem into a convex programming problem. We evaluate the effectiveness of the proposed algorithm by comparing it to two other implementations of active kernel learning. Empirical study with nine datasets on semi-supervised data clustering shows that the proposed algorithm is more effective than its competitors.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {400–407},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390208,
author = {Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and Keerthi, S. Sathiya and Sundararajan, S.},
title = {A Dual Coordinate Descent Method for Large-Scale Linear SVM},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390208},
doi = {10.1145/1390156.1390208},
abstract = {In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an ε-accurate solution in O(log(1/ε)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate descent implementation.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {408–415},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390209,
author = {Huynh, Tuyen N. and Mooney, Raymond J.},
title = {Discriminative Structure and Parameter Learning for Markov Logic Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390209},
doi = {10.1145/1390156.1390209},
abstract = {Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {416–423},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390210,
author = {Hyv\"{a}rinen, Aapo and Shimizu, Shohei and Hoyer, Patrik O.},
title = {Causal Modelling Combining Instantaneous and Lagged Effects: An Identifiable Model Based on Non-Gaussianity},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390210},
doi = {10.1145/1390156.1390210},
abstract = {Causal analysis of continuous-valued variables typically uses either autoregressive models or linear Gaussian Bayesian networks with instantaneous effects. Estimation of Gaussian Bayesian networks poses serious identifiability problems, which is why it was recently proposed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. We show that such a non-Gaussian model is identifiable without prior knowledge of network structure, and we propose an estimation method shown to be consistent. This approach also points out how neglecting instantaneous effects can lead to completely wrong estimates of the autoregressive coefficients.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {424–431},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390211,
author = {Jong, Nicholas K. and Stone, Peter},
title = {Hierarchical Model-Based Reinforcement Learning: <span class="smallcaps SmallerCapital">R-Max</span> + MAXQ},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390211},
doi = {10.1145/1390156.1390211},
abstract = {Hierarchical decomposition promises to help scale reinforcement learning algorithms naturally to real-world problems by exploiting their underlying structure. Model-based algorithms, which provided the first finite-time convergence guarantees for reinforcement learning, may also play an important role in coping with the relative scarcity of data in large environments. In this paper, we introduce an algorithm that fully integrates modern hierarchical and model-learning methods in the standard reinforcement learning setting. Our algorithm, R-maxq, inherits the efficient model-based exploration of the R-max algorithm and the opportunities for abstraction provided by the MAXQ framework. We analyze the sample complexity of our algorithm, and our experiments in a standard simulation environment illustrate the advantages of combining hierarchies and models.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {432–439},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390212,
author = {Kakade, Sham M. and Shalev-Shwartz, Shai and Tewari, Ambuj},
title = {Efficient Bandit Algorithms for Online Multiclass Prediction},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390212},
doi = {10.1145/1390156.1390212},
abstract = {This paper introduces the Banditron, a variant of the Perceptron [Rosenblatt, 1958], for the multiclass bandit setting. The multiclass bandit setting models a wide range of practical supervised learning applications where the learner only receives partial feedback (referred to as "bandit" feedback, in the spirit of multi-armed bandit models) with respect to the true label (e.g. in many web applications users often only provide positive "click" feedback which does not necessarily fully disclose a true label). The Banditron has the ability to learn in a multiclass classification setting with the "bandit" feedback which only reveals whether or not the prediction made by the algorithm was correct or not (but does not necessarily reveal the true label). We provide (relative) mistake bounds which show how the Banditron enjoys favorable performance, and our experiments demonstrate the practicality of the algorithm. Furthermore, this paper pays close attention to the important special case when the data is linearly separable --- a problem which has been exhaustively studied in the full information setting yet is novel in the bandit setting.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {440–447},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390213,
author = {Karlen, Michael and Weston, Jason and Erkan, Ayse and Collobert, Ronan},
title = {Large Scale Manifold Transduction},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390213},
doi = {10.1145/1390156.1390213},
abstract = {We show how the regularizer of Transductive Support Vector Machines (TSVM) can be trained by stochastic gradient descent for linear models and multi-layer architectures. The resulting methods can be trained online, have vastly superior training and testing speed to existing TSVM algorithms, can encode prior knowledge in the network architecture, and obtain competitive error rates. We then go on to propose a natural generalization of the TSVM loss function that takes into account neighborhood and manifold information directly, unifying the two-stage Low Density Separation method into a single criterion, and leading to state-of-the-art results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {448–455},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390214,
author = {Kersting, Kristian and Driessens, Kurt},
title = {Non-Parametric Policy Gradients: A Unified Treatment of Propositional and Relational Domains},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390214},
doi = {10.1145/1390156.1390214},
abstract = {Policy gradient approaches are a powerful instrument for learning how to interact with the environment. Existing approaches have focused on propositional and continuous domains only. Without extensive feature engineering, it is difficult - if not impossible - to apply them within structured domains, in which e.g. there is a varying number of objects and relations among them. In this paper, we describe a non-parametric policy gradient approach - called NPPG - that overcomes this limitation. The key idea is to apply Friedmann's gradient boosting: policies are represented as a weighted sum of regression models grown in an stage-wise optimization. Employing off-the-shelf regression learners, NPPG can deal with propositional, continuous, and relational domains in a unified way. Our experimental results show that it can even improve on established results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {456–463},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390215,
author = {Kirshner, Sergey and P\'{o}czos, Barnab\'{a}s},
title = {ICA and ISA Using Schweizer-Wolff Measure of Dependence},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390215},
doi = {10.1145/1390156.1390215},
abstract = {We propose a new algorithm for independent component and independent subspace analysis problems. This algorithm uses a contrast based on the Schweizer-Wolff measure of pairwise dependence (Schweizer &amp; Wolff, 1981), a non-parametric measure computed on pairwise ranks of the variables. Our algorithm frequently outperforms state of the art ICA methods in the normal setting, is significantly more robust to outliers in the mixed signals, and performs well even in the presence of noise. Our method can also be used to solve independent subspace analysis (ISA) problems by grouping signals recovered by ICA methods. We provide an extensive empirical evaluation using simulated, sound, and image data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {464–471},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390216,
author = {Klementiev, Alexandre and Roth, Dan and Small, Kevin},
title = {Unsupervised Rank Aggregation with Distance-Based Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390216},
doi = {10.1145/1390156.1390216},
abstract = {The need to meaningfully combine sets of rankings often comes up when one deals with ranked data. Although a number of heuristic and supervised learning approaches to rank aggregation exist, they require domain knowledge or supervised ranked data, both of which are expensive to acquire. In order to address these limitations, we propose a mathematical and algorithmic framework for learning to aggregate (partial) rankings without supervision. We instantiate the framework for the cases of combining permutations and combining top-k lists, and propose a novel metric for the latter. Experiments in both scenarios demonstrate the effectiveness of the proposed formalism.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {472–479},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390217,
author = {Kohli, Pushmeet and Shekhovtsov, Alexander and Rother, Carsten and Kolmogorov, Vladimir and Torr, Philip},
title = {On Partial Optimality in Multi-Label MRFs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390217},
doi = {10.1145/1390156.1390217},
abstract = {We consider the problem of optimizing multilabel MRFs, which is in general NP-hard and ubiquitous in low-level computer vision. One approach for its solution is to formulate it as an integer linear programming and relax the integrality constraints. The approach we consider in this paper is to first convert the multi-label MRF into an equivalent binary-label MRF and then to relax it. The resulting relaxation can be efficiently solved using a maximum flow algorithm. Its solution provides us with a partially optimal labelling of the binary variables. This partial labelling is then easily transferred to the multi-label problem. We study the theoretical properties of the new relaxation and compare it with the standard one. Specifically, we compare tightness, and characterize a subclass of problems where the two relaxations coincide. We propose several combined algorithms based on the technique and demonstrate their performance on challenging computer vision problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {480–487},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390218,
author = {Kolter, J. Zico and Coates, Adam and Ng, Andrew Y. and Gu, Yi and DuHadway, Charles},
title = {Space-Indexed Dynamic Programming: Learning to Follow Trajectories},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390218},
doi = {10.1145/1390156.1390218},
abstract = {We consider the task of learning to accurately follow a trajectory in a vehicle such as a car or helicopter. A number of dynamic programming algorithms such as Differential Dynamic Programming (DDP) and Policy Search by Dynamic Programming (PSDP), can efficiently compute non-stationary policies for these tasks --- such policies in general are well-suited to trajectory following since they can easily generate different control actions at different times in order to follow the trajectory. However, a weakness of these algorithms is that their policies are time-indexed, in that they apply different policies depending on the current time. This is problematic since 1) the current time may not correspond well to where we are along the trajectory and 2) the uncertainty over states can prevent these algorithms from finding any good policies at all. In this paper we propose a method for space-indexed dynamic programming that overcomes both these difficulties. We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable (i.e., how far along the trajectory we are) rather than as a function of time. We then use these space-indexed dynamical systems to derive space-indexed version of the DDP and PSDP algorithms. Finally, we show that these algorithms perform well on a variety of control tasks, both in simulation and on real systems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {488–495},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390219,
author = {Kondor, Risi and Borgwardt, Karsten M.},
title = {The Skew Spectrum of Graphs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390219},
doi = {10.1145/1390156.1390219},
abstract = {The central issue in representing graph-structured data instances in learning algorithms is designing features which are invariant to permuting the numbering of the vertices. We present a new system of invariant graph features which we call the skew spectrum of graphs. The skew spectrum is based on mapping the adjacency matrix of any (weigted, directed, unlabeled) graph to a function on the symmetric group and computing bispectral invariants. The reduced form of the skew spectrum is computable in O(n3) time, and experiments show that on several benchmark datasets it can outperform state of the art graph kernels.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {496–503},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390220,
author = {Ku\v{z}elka, Ond\v{r}ej and \v{Z}elezn\'{y}, Filip},
title = {Fast Estimation of First-Order Clause Coverage through Randomization and Maximum Likelihood},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390220},
doi = {10.1145/1390156.1390220},
abstract = {In inductive logic programming, θ-subsumption is a widely used coverage test. Unfortunately, testing θ-subsumption is NP-complete, which represents a crucial efficiency bottleneck for many relational learners. In this paper, we present a probabilistic estimator of clause coverage, based on a randomized restarted search strategy. Under a distribution assumption, our algorithm can estimate clause coverage without having to decide subsumption for all examples. We implement this algorithm in program ReCovEr. On generated graph data and real-world datasets, we show that ReCovEr provides reasonably accurate estimates while achieving dramatic runtimes improvements compared to a state-of-the-art algorithm.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {504–511},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390221,
author = {Lan, Yanyan and Liu, Tie-Yan and Qin, Tao and Ma, Zhiming and Li, Hang},
title = {Query-Level Stability and Generalization in Learning to Rank},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390221},
doi = {10.1145/1390156.1390221},
abstract = {This paper is concerned with the generalization ability of learning to rank algorithms for information retrieval (IR). We point out that the key for addressing the learning problem is to look at it from the viewpoint of query. We define a number of new concepts, including query-level loss, query-level risk, and query-level stability. We then analyze the generalization ability of learning to rank algorithms by giving query-level generalization bounds to them using query-level stability as a tool. Such an analysis is very helpful for us to derive more advanced algorithms for IR. We apply the proposed theory to the existing algorithms of Ranking SVM and IRSVM. Experimental results on the two algorithms verify the correctness of the theoretical analysis.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {512–519},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390222,
author = {Landwehr, Niels},
title = {Modeling Interleaved Hidden Processes},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390222},
doi = {10.1145/1390156.1390222},
abstract = {Hidden Markov models assume that observations in time series data stem from some hidden process that can be compactly represented as a Markov chain. We generalize this model by assuming that the observed data stems from multiple hidden processes, whose outputs interleave to form the sequence of observations. Exact inference in this model is NP-hard. However, a tractable and effective inference algorithm is obtained by extending structured approximate inference methods used in factorial hidden Markov models. The proposed model is evaluated in an activity recognition domain, where multiple activities interleave and together generate a stream of sensor observations. It is shown to be more accurate than a standard hidden Markov model in this domain.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {520–527},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390223,
author = {Langford, John and Strehl, Alexander and Wortman, Jennifer},
title = {Exploration Scavenging},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390223},
doi = {10.1145/1390156.1390223},
abstract = {We examine the problem of evaluating a policy in the contextual bandit setting using only observations collected during the execution of another policy. We show that policy evaluation can be impossible if the exploration policy chooses actions based on the side information provided at each time step. We then propose and prove the correctness of a principled method for policy evaluation which works when this is not the case, even when the exploration policy is deterministic, as long as each action is explored sufficiently often. We apply this general technique to the problem of offline evaluation of internet advertising policies. Although our theoretical results hold only when the exploration policy chooses ads independent of side information, an assumption that is typically violated by commercial systems, we show how clever uses of the theory provide non-trivial and realistic applications. We also provide an empirical demonstration of the effectiveness of our techniques on real ad placement data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {528–535},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390224,
author = {Larochelle, Hugo and Bengio, Yoshua},
title = {Classification Using Discriminative Restricted Boltzmann Machines},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390224},
doi = {10.1145/1390156.1390224},
abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {536–543},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390225,
author = {Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
title = {Transfer of Samples in Batch Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390225},
doi = {10.1145/1390156.1390225},
abstract = {The main objective of transfer in reinforcement learning is to reduce the complexity of learning the solution of a target task by effectively reusing the knowledge retained from solving a set of source tasks. In this paper, we introduce a novel algorithm that transfers samples (i.e., tuples 〈s, a, s', r〉) from source to target tasks. Under the assumption that tasks have similar transition models and reward functions, we propose a method to select samples from the source tasks that are mostly similar to the target task, and, then, to use them as input for batch reinforcement-learning algorithms. As a result, the number of samples an agent needs to collect from the target task to learn its solution is reduced. We empirically show that, following the proposed approach, the transfer of samples is effective in reducing the learning complexity, even when some source tasks are significantly different from the target task.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {544–551},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390226,
author = {Lebanon, Guy and Zhao, Yang},
title = {Local Likelihood Modeling of Temporal Text Streams},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390226},
doi = {10.1145/1390156.1390226},
abstract = {Temporal text data is often generated by a time-changing process or distribution. Such a drift in the underlying distribution cannot be captured by stationary likelihood techniques. We consider the application of local likelihood methods to generative and conditional modeling of temporal document sequences. We examine the asymptotic bias and variance and present an experimental study using the RCV1 dataset containing a temporal sequence of Reuters news stories.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {552–559},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390227,
author = {Li, Lihong},
title = {A Worst-Case Comparison between Temporal Difference and Residual Gradient with Linear Function Approximation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390227},
doi = {10.1145/1390156.1390227},
abstract = {Residual gradient (RG) was proposed as an alternative to TD(0) for policy evaluation when function approximation is used, but there exists little formal analysis comparing them except in very limited cases. This paper employs techniques from online learning of linear functions and provides a worst-case (non-probabilistic) analysis to compare these two types of algorithms when linear function approximation is used. No statistical assumptions are made on the sequence of observations, so the analysis applies to non-Markovian and even adversarial domains as well. In particular, our results suggest that RG may result in smaller temporal differences, while TD(0) is more likely to yield smaller prediction errors. These phenomena can be observed even in two simple Markov chain examples that are non-adversarial.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {560–567},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390228,
author = {Li, Lihong and Littman, Michael L. and Walsh, Thomas J.},
title = {Knows What It Knows: A Framework for Self-Aware Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390228},
doi = {10.1145/1390156.1390228},
abstract = {We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and active-learning problems. We catalog several KWIK-learnable classes and open problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {568–575},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390229,
author = {Li, Zhenguo and Liu, Jianzhuang and Tang, Xiaoou},
title = {Pairwise Constraint Propagation by Semidefinite Programming for Semi-Supervised Classification},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390229},
doi = {10.1145/1390156.1390229},
abstract = {We consider the general problem of learning from both pairwise constraints and unlabeled data. The pairwise constraints specify whether two objects belong to the same class or not, known as the must-link constraints and the cannot-link constraints. We propose to learn a mapping that is smooth over the data graph and maps the data onto a unit hypersphere, where two must-link objects are mapped to the same point while two cannot-link objects are mapped to be orthogonal. We show that such a mapping can be achieved by formulating a semidefinite programming problem, which is convex and can be solved globally. Our approach can effectively propagate pairwise constraints to the whole data set. It can be directly applied to multi-class classification and can handle data labels, pairwise constraints, or a mixture of them in a unified framework. Promising experimental results are presented for classification tasks on a variety of synthetic and real data sets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {576–583},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390230,
author = {Liang, Percy and Jordan, Michael I.},
title = {An Asymptotic Analysis of Generative, Discriminative, and Pseudolikelihood Estimators},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390230},
doi = {10.1145/1390156.1390230},
abstract = {Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood, e.g., joint, conditional, and pseudolikelihood. In this paper, we present a unified framework for studying these estimators, which allows us to compare their relative (statistical) efficiencies. Our asymptotic analysis suggests that modeling more of the data tends to reduce variance, but at the cost of being more sensitive to model misspecification. We present experiments validating our analysis.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {584–591},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390231,
author = {Liang, Percy and Daum\'{e}, Hal and Klein, Dan},
title = {Structure Compilation: Trading Structure for Features},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390231},
doi = {10.1145/1390156.1390231},
abstract = {Structured models often achieve excellent performance but can be slow at test time. We investigate structure compilation, where we replace structure with features, which are often computationally simpler but unfortunately statistically more complex. We analyze this tradeoff theoretically and empirically on three natural language processing tasks. We also introduce a simple method to transfer predictive power from structure to features via unlabeled data, while incurring a minimal statistical penalty.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {592–599},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390232,
author = {Loeff, Nicolas and Forsyth, David and Ramachandran, Deepak},
title = {ManifoldBoost: Stagewise Function Approximation for Fully-, Semi- and Un-Supervised Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390232},
doi = {10.1145/1390156.1390232},
abstract = {We describe a manifold learning framewor that naturally accommodates supervised learning, partially supervised learning and unsupervised clustering as particular cases. Our method chooses a function by minimizing loss subject to a manifold regularization penalty. This augmented cost is minimized using a greedy, stagewise, functional minimization procedure, as in Gradientboost. Each stage of boosting is fast and efficient. We demonstrate our approach using both radial basis function approximations and trees. The performance of our method is at the state of the art on many standard semi-supervised learning benchmarks, and we produce results for large scale datasets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {600–607},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390233,
author = {Long, Philip M. and Servedio, Rocco A.},
title = {Random Classification Noise Defeats All Convex Potential Boosters},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390233},
doi = {10.1145/1390156.1390233},
abstract = {A broad class of boosting algorithms can be interpreted as performing coordinate-wise gradient descent to minimize some potential function of the margins of a data set. This class includes AdaBoost, LogitBoost, and other widely used and well-studied boosters. In this paper we show that for a broad class of convex potential functions, any such boosting algorithm is highly susceptible to random classification noise. We do this by showing that for any such booster and any nonzero random classification noise rate η, there is a simple data set of examples which is efficiently learnable by such a booster if there is no noise, but which cannot be learned to accuracy better than 1/2 if there is random classification noise at rate η. This negative result is in contrast with known branching program based boosters which do not fall into the convex potential function framework and which can provably learn to high accuracy in the presence of random classification noise.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {608–615},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390234,
author = {Lu, Haiping and Plataniotis, Konstantinos N. and Venetsanopoulos, Anastasios N.},
title = {Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390234},
doi = {10.1145/1390156.1390234},
abstract = {Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel unsupervised dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the UMPCA on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {616–623},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390235,
author = {Lu, Zhengdong and Leen, Todd K. and Huang, Yonghong and Erdogmus, Deniz},
title = {A Reproducing Kernel Hilbert Space Framework for Pairwise Time Series Distances},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390235},
doi = {10.1145/1390156.1390235},
abstract = {A good distance measure for time series needs to properly incorporate the temporal structure, and should be applicable to sequences with unequal lengths. In this paper, we propose a distance measure as a principled solution to the two requirements. Unlike the conventional feature vector representation, our approach represents each time series with a summarizing smooth curve in a reproducing kernel Hilbert space (RKHS), and therefore translate the distance between time series into distances between curves. Moreover we propose to learn the kernel of this RKHS from a population of time series with discrete observations using Gaussian process-based non-parametric mixed-effect models. Experiments on two vastly different real-world problems show that the proposed distance measure leads to improved classification accuracy over the conventional distance measures.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {624–631},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390236,
author = {Makino, Takaki and Takagi, Toshihisa},
title = {On-Line Discovery of Temporal-Difference Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390236},
doi = {10.1145/1390156.1390236},
abstract = {We present an algorithm for on-line, incremental discovery of temporal-difference (TD) networks. The key contribution is the establishment of three criteria to expand a node in TD network: a node is expanded when the node is well-known, independent, and has a prediction error that requires further explanation. Since none of these criteria requires centralized calculation operations, they are easily computed in a parallel and distributed manner, and scalable for bigger problems compared to other discovery methods of predictive state representations. Through computer experiments, we demonstrate the empirical effectiveness of our algorithm.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {632–639},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390237,
author = {Martins, Andr\'{e} F. T. and Figueiredo, M\'{a}rio A. T. and Aguiar, Pedro M. Q. and Smith, Noah A. and Xing, Eric P.},
title = {Nonextensive Entropic Kernels},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390237},
doi = {10.1145/1390156.1390237},
abstract = {Positive definite kernels on probability measures have been recently applied in structured data classification problems. Some of these kernels are related to classic information theoretic quantities, such as mutual information and the Jensen-Shannon divergence. Meanwhile, driven by recent advances in Tsallis statistics, nonextensive generalizations of Shannon's information theory have been proposed. This paper bridges these two trends. We introduce the Jensen-Tsallis q-difference, a generalization of the Jensen-Shannon divergence. We then define a new family of nonextensive mutual information kernels, which allow weights to be assigned to their arguments, and which includes the Boolean, Jensen-Shannon, and linear kernels as particular cases. We illustrate the performance of these kernels on text categorization tasks.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {640–647},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390238,
author = {Mehta, Neville and Ray, Soumya and Tadepalli, Prasad and Dietterich, Thomas},
title = {Automatic Discovery and Transfer of MAXQ Hierarchies},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390238},
doi = {10.1145/1390156.1390238},
abstract = {We present an algorithm, HI-MAT (Hierarchy Induction via Models And Trajectories), that discovers MAXQ task hierarchies by applying dynamic Bayesian network models to a successful trajectory from a source reinforcement learning task. HI-MAT discovers subtasks by analyzing the causal and temporal relationships among the actions in the trajectory. Under appropriate assumptions, HI-MAT induces hierarchies that are consistent with the observed trajectory and have compact value-function tables employing safe state abstractions. We demonstrate empirically that HI-MAT constructs compact hierarchies that are comparable to manually-engineered hierarchies and facilitate significant speedup in learning when transferred to a target task.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {648–655},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390239,
author = {Meka, Raghu and Jain, Prateek and Caramanis, Constantine and Dhillon, Inderjit S.},
title = {Rank Minimization via Online Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390239},
doi = {10.1145/1390156.1390239},
abstract = {Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework (Zinkevich, 2003). In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {656–663},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390240,
author = {Melo, Francisco S. and Meyn, Sean P. and Ribeiro, M. Isabel},
title = {An Analysis of Reinforcement Learning with Function Approximation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390240},
doi = {10.1145/1390156.1390240},
abstract = {We address the problem of computing the optimal Q-function in Markov decision problems with infinite state-space. We analyze the convergence properties of several variations of Q-learning when combined with function approximation, extending the analysis of TD-learning in (Tsitsiklis &amp; Van Roy, 1996a) to stochastic control settings. We identify conditions under which such approximate methods converge with probability 1. We conclude with a brief discussion on the general applicability of our results and compare them with several related works.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {664–671},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390241,
author = {Mnih, Volodymyr and Szepesv\'{a}ri, Csaba and Audibert, Jean-Yves},
title = {Empirical Bernstein Stopping},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390241},
doi = {10.1145/1390156.1390241},
abstract = {Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and they can stop early, saving valuable resources. We consider problems where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are efficient. We provide upper bounds on the sample complexity of the new rules, as well as empirical results on model selection and boosting in the filtering setting.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {672–679},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390242,
author = {Kumar, M. Pawan and Torr, P. H. S.},
title = {Efficiently Solving Convex Relaxations for MAP Estimation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390242},
doi = {10.1145/1390156.1390242},
abstract = {The problem of obtaining the maximum a posteriori (MAP) estimate of a discrete random field is of fundamental importance in many areas of Computer Science. In this work, we build on the tree reweighted message passing (TRW) framework of (Kolmogorov, 2006; Wainwright et al., 2005). TRW iteratively optimizes the Lagrangian dual of a linear programming relaxation for MAP estimation. We show how the dual formulation of TRW can be extended to include cycle inequalities (Barahona &amp; Mahjoub, 1986) and some recently proposed second order cone (SOC) constraints (Kumar et al., 2007). We propose efficient iterative algorithms for solving the resulting duals. Similar to the method described in (Kolmogorov, 2006), these algorithms are guaranteed to converge. We test our approach on a large set of synthetic data, as well as real data. Our experiments show that the additional constraints (i.e. cycle inequalities and SOC constraints) provide better results in cases where the TRW framework fails (namely MAP estimation for non-submodular energy functions).},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {680–687},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390243,
author = {Narayanamurthy, Shravan Matthur and Ravindran, Balaraman},
title = {On the Hardness of Finding Symmetries in Markov Decision Processes},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390243},
doi = {10.1145/1390156.1390243},
abstract = {In this work we address the question of finding symmetries of a given MDP. We show that the problem is Isomorphism Complete, that is, the problem is polynomially equivalent to verifying whether two graphs are isomorphic. Apart from the theoretical importance of this result it has an important practical application. The reduction presented can be used together with any off-the-shelf Graph Isomorphism solver, which performs well in the average case, to find symmetries of an MDP. In fact, we present results of using NAutY (the best Graph Isomorphism solver currently available), to find symmetries of MDPs.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {688–695},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390244,
author = {Nijssen, Siegfried},
title = {Bayes Optimal Classification for Decision Trees},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390244},
doi = {10.1145/1390156.1390244},
abstract = {We present an algorithm for exact Bayes optimal classification from a hypothesis space of decision trees satisfying leaf constraints. Our contribution is that we reduce this classification problem to the problem of finding a rule-based classifier with appropriate weights. We show that these rules and weights can be computed in linear time from the output of a modified frequent itemset mining algorithm, which means that we can compute the classifier in practice, despite the exponential worst-case complexity. In experiments we compare the Bayes optimal predictions with those of the maximum a posteriori hypothesis.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {696–703},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390245,
author = {Nowozin, Sebastian and Bakir, G\"{o}khan},
title = {A Decoupled Approach to Exemplar-Based Unsupervised Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390245},
doi = {10.1145/1390156.1390245},
abstract = {A recent trend in exemplar based unsupervised learning is to formulate the learning problem as a convex optimization problem. Convexity is achieved by restricting the set of possible prototypes to training exemplars. In particular, this has been done for clustering, vector quantization and mixture model density estimation. In this paper we propose a novel algorithm that is theoretically and practically superior to these convex formulations. This is possible by posing the unsupervised learning problem as a single convex "master problem" with non-convex subproblems. We show that for the above learning tasks the subproblems are extremely well-behaved and can be solved efficiently.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {704–711},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390246,
author = {O'Brien, Deirdre B. and Gupta, Maya R. and Gray, Robert M.},
title = {Cost-Sensitive Multi-Class Classification from Probability Estimates},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390246},
doi = {10.1145/1390156.1390246},
abstract = {For two-class classification, it is common to classify by setting a threshold on class probability estimates, where the threshold is determined by ROC curve analysis. An analog for multi-class classification is learning a new class partitioning of the multiclass probability simplex to minimize empirical misclassification costs. We analyze the interplay between systematic errors in the class probability estimates and cost matrices for multiclass classification. We explore the effect on the class partitioning of five different transformations of the cost matrix. Experiments on benchmark datasets with naive Bayes and quadratic discriminant analysis show the effectiveness of learning a new partition matrix compared to previously proposed methods.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {712–719},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390247,
author = {Orabona, Francesco and Keshet, Joseph and Caputo, Barbara},
title = {The Projectron: A Bounded Kernel-Based Perceptron},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390247},
doi = {10.1145/1390156.1390247},
abstract = {We present a discriminative online algorithm with a bounded memory growth, which is based on the kernel-based Perceptron. Generally, the required memory of the kernel-based Perceptron for storing the online hypothesis is not bounded. Previous work has been focused on discarding part of the instances in order to keep the memory bounded. In the proposed algorithm the instances are not discarded, but projected onto the space spanned by the previous online hypothesis. We derive a relative mistake bound and compare our algorithm both analytically and empirically to the state-of-the-art Forgetron algorithm (Dekel et al, 2007). The first variant of our algorithm, called Projectron, outperforms the Forgetron. The second variant, called Projectron++, outperforms even the Perceptron.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {720–727},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390248,
author = {Ouyang, Hua and Gray, Alex},
title = {Learning Dissimilarities by Ranking: From SDP to QP},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390248},
doi = {10.1145/1390156.1390248},
abstract = {We consider the problem of learning dissimilarities between points via formulations which preserve a specified ordering between points rather than the numerical values of the dissimilarities. Dissimilarity ranking (d-ranking) learns from instances like "A is more similar to B than C is to D" or "The distance between E and F is larger than that between G and H". Three formulations of d-ranking problems are presented and new algorithms are presented for two of them, one by semidefinite programming (SDP) and one by quadratic programming (QP). Among the novel capabilities of these approaches are out-of-sample prediction and scalability to large problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {728–735},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390249,
author = {Paiement, Jean-Fran\c{c}ois and Grandvalet, Yves and Bengio, Samy and Eck, Douglas},
title = {A Distance Model for Rhythms},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390249},
doi = {10.1145/1390156.1390249},
abstract = {Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {736–743},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390250,
author = {Palatucci, Mark and Carlson, Andrew},
title = {On the Chance Accuracies of Large Collections of Classifiers},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390250},
doi = {10.1145/1390156.1390250},
abstract = {We provide a theoretical analysis of the chance accuracies of large collections of classifiers. We show that on problems with small numbers of examples, some classifier can perform well by random chance, and we derive a theorem to explicitly calculate this accuracy. We use this theorem to provide a principled feature selection criterion for sparse, high-dimensional problems. We evaluate this method on microarray and fMRI datasets and show that it performs very close to the optimal accuracy obtained from an oracle. We also show that on the fMRI dataset this technique chooses relevant features successfully while another state-of-the-art method, the False Discovery Rate (FDR), completely fails at standard significance levels.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {744–751},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390251,
author = {Parr, Ronald and Li, Lihong and Taylor, Gavin and Painter-Wakefield, Christopher and Littman, Michael L.},
title = {An Analysis of Linear Models, Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390251},
doi = {10.1145/1390156.1390251},
abstract = {We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {752–759},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390252,
author = {Puolam\"{a}ki, Kai and Ajanki, Antti and Kaski, Samuel},
title = {Learning to Learn Implicit Queries from Gaze Patterns},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390252},
doi = {10.1145/1390156.1390252},
abstract = {In the absence of explicit queries, an alternative is to try to infer users' interests from implicit feedback signals, such as clickstreams or eye tracking. The interests, formulated as an implicit query, can then be used in further searches. We formulate this task as a probabilistic model, which can be interpreted as a kind of transfer or meta-learning. The probabilistic model is demonstrated to outperform an earlier kernel-based method in a small-scale information retrieval task.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {760–767},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390253,
author = {Qi, Yuting and Liu, Dehong and Dunson, David and Carin, Lawrence},
title = {Multi-Task Compressive Sensing with Dirichlet Process Priors},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390253},
doi = {10.1145/1390156.1390253},
abstract = {Compressive sensing (CS) is an emerging £eld that, under appropriate conditions, can signi£cantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing "task". In this paper we propose a novel multitask compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {768–775},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390254,
author = {Quadrianto, Novi and Smola, Alex J. and Caetano, Tiberio S. and Le, Quoc V.},
title = {Estimating Labels from Label Proportions},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390254},
doi = {10.1145/1390156.1390254},
abstract = {Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, also with known label proportions. This problem appears in areas like e-commerce, spam filtering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {776–783},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390255,
author = {Radlinski, Filip and Kleinberg, Robert and Joachims, Thorsten},
title = {Learning Diverse Rankings with Multi-Armed Bandits},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390255},
doi = {10.1145/1390156.1390255},
abstract = {Algorithms for learning to rank Web documents usually assume a document's relevance is independent of other documents. This leads to learned ranking functions that produce rankings with redundant results. In contrast, user studies have shown that diversity at high ranks is often preferred. We present two online learning algorithms that directly learn a diverse ranking of documents based on users' clicking behavior. We show that these algorithms minimize abandonment, or alternatively, maximize the probability that a relevant document is found in the top k positions of a ranking. Moreover, one of our algorithms asymptotically achieves optimal worst-case performance even if users' interests change.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {784–791},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390256,
author = {Ranzato, Marc' Aurelio and Szummer, Martin},
title = {Semi-Supervised Learning of Compact Document Representations with Deep Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390256},
doi = {10.1145/1390156.1390256},
abstract = {Finding good representations of text documents is crucial in information retrieval and classification systems. Today the most popular document representation is based on a vector of word counts in the document. This representation neither captures dependencies between related words, nor handles synonyms or polysemous words. In this paper, we propose an algorithm to learn text document representations based on semi-supervised autoencoders that are stacked to form a deep network. The model can be trained efficiently on partially labeled corpora, producing very compact representations of documents, while retaining as much class information and joint word statistics as possible. We show that it is advantageous to exploit even a few labeled samples during training.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {792–799},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390257,
author = {Ravikumar, Pradeep and Agarwal, Alekh and Wainwright, Martin J.},
title = {Message-Passing for Graph-Structured Linear Programs: Proximal Projections, Convergence and Rounding Schemes},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390257},
doi = {10.1145/1390156.1390257},
abstract = {A large body of past work has focused on the first-order tree-based LP relaxation for the MAP problem in Markov random fields. This paper develops a family of super-linearly convergent LP solvers based on proximal minimization schemes using Bregman divergences that exploit the underlying graphical structure, and so scale well to large problems. All of our algorithms have a double-loop character, with the outer loop corresponding to the proximal sequence, and an inner loop of cyclic Bregman divergences used to compute each proximal update. The inner loop updates are distributed and respect the graph structure, and thus can be cast as message-passing algorithms. We establish various convergence guarantees for our algorithms, illustrate their performance, and also present rounding schemes with provable optimality guarantees.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {800–807},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390258,
author = {Raykar, Vikas C. and Krishnapuram, Balaji and Bi, Jinbo and Dundar, Murat and Rao, R. Bharat},
title = {Bayesian Multiple Instance Learning: Automatic Feature Selection and Inductive Transfer},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390258},
doi = {10.1145/1390156.1390258},
abstract = {We propose a novel Bayesian multiple instance learning (MIL) algorithm. This algorithm automatically identifies the relevant feature subset, and utilizes inductive transfer when learning multiple (conceptually related) classifiers. Experimental results indicate that the proposed MIL method is more accurate than previous MIL algorithms and selects a much smaller set of useful features. Inductive transfer further improves the accuracy of the classifier as compared to learning each task individually.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {808–815},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390259,
author = {Reisinger, Joseph and Stone, Peter and Miikkulainen, Risto},
title = {Online Kernel Selection for Bayesian Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390259},
doi = {10.1145/1390156.1390259},
abstract = {Kernel-based Bayesian methods for Reinforcement Learning (RL) such as Gaussian Process Temporal Difference (GPTD) are particularly promising because they rigorously treat uncertainty in the value function and make it easy to specify prior knowledge. However, the choice of prior distribution significantly affects the empirical performance of the learning agent, and little work has been done extending existing methods for prior model selection to the online setting. This paper develops Replacing-Kernel RL, an online model selection method for GPTD using sequential Monte-Carlo methods. Replacing-Kernel RL is compared to standard GPTD and tile-coding on several RL domains, and is shown to yield significantly better asymptotic performance for many different kernel families. Furthermore, the resulting kernels capture an intuitively useful notion of prior state covariance that may nevertheless be difficult to capture manually.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {816–823},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390260,
author = {Ren, Lu and Dunson, David B. and Carin, Lawrence},
title = {The Dynamic Hierarchical Dirichlet Process},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390260},
doi = {10.1145/1390156.1390260},
abstract = {The dynamic hierarchical Dirichlet process (dHDP) is developed to model the time-evolving statistical properties of sequential data sets. The data collected at any time point are represented via a mixture associated with an appropriate underlying model, in the framework of HDP. The statistical properties of data collected at consecutive time points are linked via a random parameter that controls their probabilistic similarity. The sharing mechanisms of the time-evolving data are derived, and a relatively simple Markov Chain Monte Carlo sampler is developed. Experimental results are presented to demonstrate the model.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {824–831},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390261,
author = {Rish, Irina and Grabarnik, Genady and Cecchi, Guillermo and Pereira, Francisco and Gordon, Geoffrey J.},
title = {Closed-Form Supervised Dimensionality Reduction with Generalized Linear Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390261},
doi = {10.1145/1390156.1390261},
abstract = {We propose a family of supervised dimensionality reduction (SDR) algorithms that combine feature extraction (dimensionality reduction) with learning a predictive model in a unified optimization framework, using data- and class-appropriate generalized linear models (GLMs), and handling both classification and regression problems. Our approach uses simple closed-form update rules and is provably convergent. Promising empirical results are demonstrated on a variety of high-dimensional datasets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {832–839},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390262,
author = {Rosset, Saharon},
title = {Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390262},
doi = {10.1145/1390156.1390262},
abstract = {Modeling of conditional quantiles requires specification of the quantile being estimated and can thus be viewed as a parameterized predictive modeling problem. Quantile loss is typically used, and it is indeed parameterized by a quantile parameter. In this paper we show how to follow the path of cross validated solutions to regularized kernel quantile regression. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on data. This algorithm allows us to efficiently solve the whole family of bi-level problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {840–847},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390263,
author = {Roth, Volker and Fischer, Bernd},
title = {The Group-Lasso for Generalized Linear Models: Uniqueness of Solutions and Efficient Algorithms},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390263},
doi = {10.1145/1390156.1390263},
abstract = {The Group-Lasso method for finding important explanatory factors suffers from the potential non-uniqueness of solutions and also from high computational costs. We formulate conditions for the uniqueness of Group-Lasso solutions which lead to an easily implementable test procedure that allows us to identify all potentially active groups. These results are used to derive an efficient algorithm that can deal with input dimensions in the millions and can approximate the solution path efficiently. The derived methods are applied to large-scale learning problems where they exhibit excellent performance and where the testing procedure helps to avoid misinterpretations of the solutions.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {848–855},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390264,
author = {Sahbi, Hichem and Audibert, Jean-Yves and Rabarisoa, Jaonary and Keriven, Renaud},
title = {Robust Matching and Recognition Using Context-Dependent Kernels},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390264},
doi = {10.1145/1390156.1390264},
abstract = {The success of kernel methods including support vector machines (SVMs) strongly depends on the design of appropriate kernels. While initially kernels were designed in order to handle fixed-length data, their extension to unordered, variable-length data became more than necessary for real pattern recognition problems such as object recognition and bioinformatics.We focus in this paper on object recognition using a new type of kernel referred to as "context-dependent". Objects, seen as constellations of local features (interest points, regions, etc.), are matched by minimizing an energy function mixing (1) a fidelity term which measures the quality of feature matching, (2) a neighborhood criterion which captures the object geometry and (3) a regularization term. We will show that the fixed-point of this energy is a "context-dependent" kernel ("CDK") which also satisfies the Mercer condition. Experiments conducted on object recognition show that when plugging our kernel in SVMs, we clearly outperform SVMs with "context-free" kernels.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {856–863},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390265,
author = {Sakuma, Jun and Kobayashi, Shigenobu and Wright, Rebecca N.},
title = {Privacy-Preserving Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390265},
doi = {10.1145/1390156.1390265},
abstract = {We consider the problem of distributed reinforcement learning (DRL) from private perceptions. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also should be kept private. Conventional DRL algorithms can handle multiple agents, but do not necessarily guarantee privacy preservation and may not guarantee optimality. In this work, we design cryptographic solutions that achieve optimal policies without requiring the agents to share their private information.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {864–871},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390266,
author = {Salakhutdinov, Ruslan and Murray, Iain},
title = {On the Quantitative Analysis of Deep Belief Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390266},
doi = {10.1145/1390156.1390266},
abstract = {Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {872–879},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390267,
author = {Salakhutdinov, Ruslan and Mnih, Andriy},
title = {Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390267},
doi = {10.1145/1390156.1390267},
abstract = {Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction accuracy than PMF models trained using MAP estimation.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {880–887},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390268,
author = {Sarawagi, Sunita and Gupta, Rahul},
title = {Accurate Max-Margin Training for Structured Output Spaces},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390268},
doi = {10.1145/1390156.1390268},
abstract = {Tsochantaridis et al. (2005) proposed two formulations for maximum margin training of structured spaces: margin scaling and slack scaling. While margin scaling has been extensively used since it requires the same kind of MAP inference as normal structured prediction, slack scaling is believed to be more accurate and better-behaved. We present an efficient variational approximation to the slack scaling method that solves its inference bottleneck while retaining its accuracy advantage over margin scaling.We further argue that existing scaling approaches do not separate the true labeling comprehensively while generating violating constraints. We propose a new max-margin trainer PosLearn that generates violators to ensure separation at each position of a decomposable loss function. Empirical results on real datasets illustrate that PosLearn can reduce test error by up to 25% over margin scaling and 10% over slack scaling. Further, PosLearn violators can be generated more efficiently than slack violators; for many structured tasks the time required is just twice that of MAP inference.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {888–895},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390269,
author = {Sarkar, Purnamrita and Moore, Andrew W. and Prakash, Amit},
title = {Fast Incremental Proximity Search in Large Graphs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390269},
doi = {10.1145/1390156.1390269},
abstract = {In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries on the fly without caching any information about the entire graph. We present empirical results on a 600, 000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {896–903},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390270,
author = {Schnall-Levin, Michael and Chindelevitch, Leonid and Berger, Bonnie},
title = {Inverting the Viterbi Algorithm: An Abstract Framework for Structure Design},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390270},
doi = {10.1145/1390156.1390270},
abstract = {Probabilistic grammatical formalisms such as hidden Markov models (HMMs) and stochastic context-free grammars (SCFGs) have been extensively studied and widely applied in a number of fields. Here, we introduce a new algorithmic problem on HMMs and SCFGs that arises naturally from protein and RNA design, and which has not been previously studied. The problem can be viewed as an inverse to the one solved by the Viterbi algorithm on HMMs or by the CKY algorithm on SCFGs. We study this problem theoretically and obtain the first algorithmic results. We prove that the problem is NP-complete, even for a 3-letter emission alphabet, via a reduction from 3-SAT, a result that has implications for the hardness of RNA secondary structure design. We then develop a number of approaches for making the problem tractable. In particular, for HMMs we develop a branch-and-bound algorithm, which can be shown to have fixed-parameter tractable worst-case running time, exponential in the number of states of the HMM but linear in the length of the structure. We also show how to cast the problem as a Mixed Integer Linear Program.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {904–911},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390271,
author = {Seeger, Matthias W. and Nickisch, Hannes},
title = {Compressed Sensing and Bayesian Experimental Design},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390271},
doi = {10.1145/1390156.1390271},
abstract = {We relate compressed sensing (CS) with Bayesian experimental design and provide a novel efficient approximate method for the latter, based on expectation propagation. In a large comparative study about linearly measuring natural images, we show that the simple standard heuristic of measuring wavelet coefficients top-down systematically outperforms CS methods using random measurements; the sequential projection optimisation approach of (Ji &amp; Carin, 2007) performs even worse. We also show that our own approximate Bayesian method is able to learn measurement filters on full images efficiently which outperform the wavelet heuristic. To our knowledge, ours is the first successful attempt at "learning compressed sensing" for images of realistic size. In contrast to common CS methods, our framework is not restricted to sparse signals, but can readily be applied to other notions of signal complexity or noise models. We give concrete ideas how our method can be scaled up to large signal representations.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {912–919},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390272,
author = {Seldin, Yevgeny and Tishby, Naftali},
title = {Multi-Classification by Categorical Features via Clustering},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390272},
doi = {10.1145/1390156.1390272},
abstract = {We derive a generalization bound for multi-classification schemes based on grid clustering in categorical parameter product spaces. Grid clustering partitions the parameter space in the form of a Cartesian product of partitions for each of the parameters. The derived bound provides a means to evaluate clustering solutions in terms of the generalization power of a built-on classifier. For classification based on a single feature the bound serves to find a globally optimal classification rule. Comparison of the generalization power of individual features can then be used for feature ranking. Our experiments show that in this role the bound is much more precise than mutual information or normalized correlation indices.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {920–927},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390273,
author = {Shalev-Shwartz, Shai and Srebro, Nathan},
title = {SVM Optimization: Inverse Dependence on Training Set Size},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390273},
doi = {10.1145/1390156.1390273},
abstract = {We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {928–935},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390274,
author = {Shi, Tao and Belkin, Mikhail and Yu, Bin},
title = {Data Spectroscopy: Learning Mixture Models Using Eigenspaces of Convolution Operators},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390274},
doi = {10.1145/1390156.1390274},
abstract = {In this paper we develop a spectral framework for estimating mixture distributions, specifically Gaussian mixture models. In physics, spectroscopy is often used for the identification of substances through their spectrum. Treating a kernel function K(x, y) as "light" and the sampled data as "substance", the spectrum of their interaction (eigenvalues and eigenvectors of the kernel matrix K) unveils certain aspects of the underlying parametric distribution p, such as the parameters of a Gaussian mixture. Our approach extends the intuitions and analyses underlying the existing spectral techniques, such as spectral clustering and Kernel Principal Components Analysis (KPCA).We construct algorithms to estimate parameters of Gaussian mixture models, including the number of mixture components, their means and covariance matrices, which are important in many practical applications. We provide a theoretical framework and show encouraging experimental results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {936–943},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390275,
author = {Shin, Kilho and Kuboyama, Tetsuji},
title = {A Generalization of Haussler's Convolution Kernel: Mapping Kernel},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390275},
doi = {10.1145/1390156.1390275},
abstract = {Haussler's convolution kernel provides a successful framework for engineering new positive semidefinite kernels, and has been applied to a wide range of data types and applications. In the framework, each data object represents a finite set of finer grained components. Then, Haussler's convolution kernel takes a pair of data objects as input, and returns the sum of the return values of the predetermined primitive positive semidefinite kernel calculated for all the possible pairs of the components of the input data objects. On the other hand, the mapping kernel that we introduce in this paper is a natural generalization of Haussler's convolution kernel, in that the input to the primitive kernel moves over a predetermined subset rather than the entire cross product. Although we have plural instances of the mapping kernel in the literature, their positive semidefiniteness was investigated in case-by-case manners, and worse yet, was sometimes incorrectly concluded. In fact, there exists a simple and easily checkable necessary and sufficient condition, which is generic in the sense that it enables us to investigate the positive semidefiniteness of an arbitrary instance of the mapping kernel. This is the first paper that presents and proves the validity of the condition. In addition, we introduce two important instances of the mapping kernel, which we refer to as the size-of-index-structure-distribution kernel and the editcost-distribution kernel. Both of them are naturally derived from well known (dis)similarity measurements in the literature (e.g. the maximum agreement tree, the edit distance), and are reasonably expected to improve the performance of the existing measures by evaluating their distributional features rather than their peak (maximum/minimum) features.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {944–951},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390276,
author = {Shringarpure, Suyash and Xing, Eric P.},
title = {<i>MStruct</i>: A New Admixture Model for Inference of Population Structure in Light of Both Genetic Admixing and Allele Mutations},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390276},
doi = {10.1145/1390156.1390276},
abstract = {Traditional methods for analyzing population structure, such as the Structure program, ignore the influence of mutational effects. We propose mStruct, an admixture of population-specific mixtures of inheritance models, that addresses the task of structure inference and mutation estimation jointly through a hierarchical Bayesian framework, and a variational algorithm for inference. We validated our method on synthetic data, and used it to analyze the HGDP-CEPH cell line panel of microsatellites used in (Rosenberg et al., 2002) and the HGDP SNP data used in (Conrad et al., 2006). A comparison of the structural maps of world populations estimated by mStruct and Structure is presented, and we also report potentially interesting mutation patterns in world populations estimated by mStruct, which is not possible by Structure.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {952–959},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390277,
author = {Sigg, Christian D. and Buhmann, Joachim M.},
title = {Expectation-Maximization for Sparse and Non-Negative PCA},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390277},
doi = {10.1145/1390156.1390277},
abstract = {We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal sign. This problem is known as sparse and non-negative principal component analysis (PCA), and has many applications including dimensionality reduction and feature selection. Based on expectation-maximization for probabilistic PCA, we present an algorithm for any combination of these constraints. Its complexity is at most quadratic in the number of dimensions of the data. We demonstrate significant improvements in performance and computational efficiency compared to other constrained PCA algorithms, on large data sets from biology and computer vision. Finally, we show the usefulness of non-negative sparse PCA for unsupervised feature selection in a gene clustering task.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {960–967},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390278,
author = {Silver, David and Sutton, Richard S. and M\"{u}ller, Martin},
title = {Sample-Based Learning and Search with Permanent and Transient Memories},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390278},
doi = {10.1145/1390156.1390278},
abstract = {We present a reinforcement learning architecture, Dyna-2, that encompasses both sample-based learning and sample-based search, and that generalises across states during both learning and search. We apply Dyna-2 to high performance Computer Go. In this domain the most successful planning methods are based on sample-based search algorithms, such as UCT, in which states are treated individually, and the most successful learning methods are based on temporal-difference learning algorithms, such as Sarsa, in which linear function approximation is used. In both cases, an estimate of the value function is formed, but in the first case it is transient, computed and then discarded after each move, whereas in the second case it is more permanent, slowly accumulating over many moves and games. The idea of Dyna-2 is for the transient planning memory and the permanent learning memory to remain separate, but for both to be based on linear function approximation and both to be updated by Sarsa. To apply Dyna-2 to 9x9 Computer Go, we use a million binary features in the function approximator, based on templates matching small fragments of the board. Using only the transient memory, Dyna-2 performed at least as well as UCT. Using both memories combined, it significantly outperformed UCT. Our program based on Dyna-2 achieved a higher rating on the Computer Go Online Server than any handcrafted or traditional search based program.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {968–975},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390279,
author = {Sindhwani, Vikas and Rosenberg, David S.},
title = {An RKHS for Multi-View Learning and Manifold Co-Regularization},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390279},
doi = {10.1145/1390156.1390279},
abstract = {Inspired by co-training, many multi-view semi-supervised kernel methods implement the following idea: find a function in each of multiple Reproducing Kernel Hilbert Spaces (RKHSs) such that (a) the chosen functions make similar predictions on unlabeled examples, and (b) the average prediction given by the chosen functions performs well on labeled examples. In this paper, we construct a single RKHS with a data-dependent "co-regularization" norm that reduces these approaches to standard supervised learning. The reproducing kernel for this RKHS can be explicitly derived and plugged into any kernel method, greatly extending the theoretical and algorithmic scope of coregularization. In particular, with this development, the Rademacher complexity bound for co-regularization given in (Rosenberg &amp; Bartlett, 2007) follows easily from wellknown results. Furthermore, more refined bounds given by localized Rademacher complexity can also be easily applied. We propose a co-regularization based algorithmic alternative to manifold regularization (Belkin et al., 2006; Sindhwani et al., 2005a) that leads to major empirical improvements on semi-supervised tasks. Unlike the recently proposed transductive approach of (Yu et al., 2008), our RKHS formulation is truly semi-supervised and naturally extends to unseen test data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {976–983},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390280,
author = {Sokolovska, Nataliya and Capp\'{e}, Olivier and Yvon, Fran\c{c}ois},
title = {The Asymptotics of Semi-Supervised Learning in Discriminative Probabilistic Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390280},
doi = {10.1145/1390156.1390280},
abstract = {Semi-supervised learning aims at taking advantage of unlabeled data to improve the efficiency of supervised learning procedures. For discriminative models however, this is a challenging task. In this contribution, we introduce an original methodology for using unlabeled data through the design of a simple semi-supervised objective function. We prove that the corresponding semi-supervised estimator is asymptotically optimal. The practical consequences of this result are discussed for the case of the logistic regression model.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {984–991},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390281,
author = {Song, Le and Zhang, Xinhua and Smola, Alex and Gretton, Arthur and Sch\"{o}lkopf, Bernhard},
title = {Tailoring Density Estimation via Reproducing Kernel Moment Matching},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390281},
doi = {10.1145/1390156.1390281},
abstract = {Moment matching is a popular means of parametric density estimation. We extend this technique to nonparametric estimation of mixture models. Our approach works by embedding distributions into a reproducing kernel Hilbert space, and performing moment matching in that space. This allows us to tailor density estimators to a function class of interest (i.e., for which we would like to compute expectations). We show our density estimation approach is useful in applications such as message compression in graphical models, and image classification and retrieval.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {992–999},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390282,
author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
title = {Detecting Statistical Interactions with Additive Groves of Trees},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390282},
doi = {10.1145/1390156.1390282},
abstract = {Discovering additive structure is an important step towards understanding a complex multi-dimensional function because it allows the function to be expressed as the sum of lower-dimensional components. When variables interact, however, their effects are not additive and must be modeled and interpreted simultaneously. We present a new approach for the problem of interaction detection. Our method is based on comparing the performance of unrestricted and restricted prediction models, where restricted models are prevented from modeling an interaction in question. We show that an additive model-based regression ensemble, Additive Groves, can be restricted appropriately for use with this framework, and thus has the right properties for accurately detecting variable interactions.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1000–1007},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390283,
author = {Sriperumbudur, Bharath K. and Lang, Omer A. and Lanckriet, Gert R. G.},
title = {Metric Embedding for Kernel Classification Rules},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390283},
doi = {10.1145/1390156.1390283},
abstract = {In this paper, we consider a smoothing kernel based classification rule and propose an algorithm for optimizing the performance of the rule by learning the bandwidth of the smoothing kernel along with a data-dependent distance metric. The data-dependent distance metric is obtained by learning a function that embeds an arbitrary metric space into a Euclidean space while minimizing an upper bound on the resubstitution estimate of the error probability of the kernel classification rule. By restricting this embedding function to a reproducing kernel Hilbert space, we reduce the problem to solving a semidefinite program and show the resulting kernel classification rule to be a variation of the k-nearest neighbor rule. We compare the performance of the kernel rule (using the learned data-dependent distance metric) to state-of-the-art distance metric learning algorithms (designed for k-nearest neighbor classification) on some benchmark datasets. The results show that the proposed rule has either better or as good classification accuracy as the other metric learning algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1008–1015},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390284,
author = {Su, Jiang and Zhang, Harry and Ling, Charles X. and Matwin, Stan},
title = {Discriminative Parameter Learning for Bayesian Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390284},
doi = {10.1145/1390156.1390284},
abstract = {Bayesian network classifiers have been widely used for classification problems. Given a fixed Bayesian network structure, parameters learning can take two different approaches: generative and discriminative learning. While generative parameter learning is more efficient, discriminative parameter learning is more effective. In this paper, we propose a simple, efficient, and effective discriminative parameter learning method, called Discriminative Frequency Estimate (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art discriminative parameter learning method ELR in accuracy, but is significantly more efficient.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1016–1023},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390285,
author = {Sun, Liang and Ji, Shuiwang and Ye, Jieping},
title = {A Least Squares Formulation for Canonical Correlation Analysis},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390285},
doi = {10.1145/1390156.1390285},
abstract = {Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multi-dimensional variables. It projects both sets of variables into a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction, in which one of the multi-dimensional variables is derived from the class label. It has been shown that CCA can be formulated as a least squares problem in the binaryclass case. However, their relationship in the more general setting remains unclear. In this paper, we show that, under a mild condition which tends to hold for high-dimensional data, CCA in multi-label classifications can be formulated as a least squares problem. Based on this equivalence relationship, we propose several CCA extensions including sparse CCA using 1-norm regularization. Experiments on multi-label data sets confirm the established equivalence relationship. Results also demonstrate the effectiveness of the proposed CCA extensions.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1024–1031},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390286,
author = {Syed, Umar and Bowling, Michael and Schapire, Robert E.},
title = {Apprenticeship Learning Using Linear Programming},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390286},
doi = {10.1145/1390156.1390286},
abstract = {In apprenticeship learning, the goal is to learn a policy in a Markov decision process that is at least as good as a policy demonstrated by an expert. The difficulty arises in that the MDP's true reward function is assumed to be unknown. We show how to frame apprenticeship learning as a linear programming problem, and show that using an off-the-shelf LP solver to solve this problem results in a substantial improvement in running time over existing methods---up to two orders of magnitude faster in our experiments. Additionally, our approach produces stationary policies, while all existing methods for apprenticeship learning output policies that are "mixed", i.e. randomized combinations of stationary policies. The technique used is general enough to convert any mixed policy to a stationary policy.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1032–1039},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390287,
author = {Szafranski, Marie and Grandvalet, Yves and Rakotomamonjy, Alain},
title = {Composite Kernel Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390287},
doi = {10.1145/1390156.1390287},
abstract = {The Support Vector Machine (SVM) is an acknowledged powerful tool for building classifiers, but it lacks flexibility, in the sense that the kernel is chosen prior to learning. Multiple Kernel Learning (MKL) enables to learn the kernel, from an ensemble of basis kernels, whose combination is optimized in the learning process. Here, we propose Composite Kernel Learning to address the situation where distinct components give rise to a group structure among kernels. Our formulation of the learning problem encompasses several setups, putting more or less emphasis on the group structure. We characterize the convexity of the learning problem, and provide a general wrapper algorithm for computing solutions. Finally, we illustrate the behavior of our method on multi-channel data where groups correpond to channels.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1040–1047},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390288,
author = {Szita, Istv\'{a}n and L\H{o}rincz, Andr\'{a}s},
title = {The Many Faces of Optimism: A Unifying Approach},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390288},
doi = {10.1145/1390156.1390288},
abstract = {The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1048–1055},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390289,
author = {Takeda, Akiko and Sugiyama, Masashi},
title = {<i>ν</i>-Support Vector Machine as Conditional Value-at-Risk Minimization},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390289},
doi = {10.1145/1390156.1390289},
abstract = {The ν-support vector classification (ν-SVC) algorithm was shown to work well and provide intuitive interpretations, e.g., the parameter ν roughly specifies the fraction of support vectors. Although ν corresponds to a fraction, it cannot take the entire range between 0 and 1 in its original form. This problem was settled by a non-convex extension of ν-SVC and the extended method was experimentally shown to generalize better than original ν-SVC. However, its good generalization performance and convergence properties of the optimization algorithm have not been studied yet. In this paper, we provide new theoretical insights into these issues and propose a novel ν-SVC algorithm that has guaranteed generalization performance and convergence properties.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1056–1063},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390290,
author = {Tieleman, Tijmen},
title = {Training Restricted Boltzmann Machines Using Approximations to the Likelihood Gradient},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390290},
doi = {10.1145/1390156.1390290},
abstract = {A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1064–1071},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390291,
author = {Ueno, Tsuyoshi and Kawanabe, Motoaki and Mori, Takeshi and Maeda, Shin-ichi and Ishii, Shin},
title = {A Semiparametric Statistical Approach to Model-Free Policy Evaluation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390291},
doi = {10.1145/1390156.1390291},
abstract = {Reinforcement learning (RL) methods based on least-squares temporal difference (LSTD) have been developed recently and have shown good practical performance. However, the quality of their estimation has not been well elucidated. In this article, we discuss LSTD-based policy evaluation from the new view-point of semiparametric statistical inference. In fact, the estimator can be obtained from a particular estimating function which guarantees its convergence to the true value asymptotically, without specifying a model of the environment. Based on these observations, we 1) analyze the asymptotic variance of an LSTD-based estimator, 2) derive the optimal estimating function with the minimum asymptotic estimation variance, and 3) derive a suboptimal estimator to reduce the computational burden in obtaining the optimal estimating function.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1072–1079},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390292,
author = {Urtasun, Raquel and Fleet, David J. and Geiger, Andreas and Popovi\'{c}, Jovan and Darrell, Trevor J. and Lawrence, Neil D.},
title = {Topologically-Constrained Latent Variable Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390292},
doi = {10.1145/1390156.1390292},
abstract = {In dimensionality reduction approaches, the data are typically embedded in a Euclidean latent space. However for some data sets this is inappropriate. For example, in human motion data we expect latent spaces that are cylindrical or a toroidal, that are poorly captured with a Euclidean space. In this paper, we present a range of approaches for embedding data in a non-Euclidean latent space. Our focus is the Gaussian Process latent variable model. In the context of human motion modeling this allows us to (a) learn models with interpretable latent directions enabling, for example, style/content separation, and (b) generalise beyond the data set enabling us to learn transitions between motion styles even though such transitions are not present in the data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1080–1087},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390293,
author = {Van Gael, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
title = {Beam Sampling for the Infinite Hidden Markov Model},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390293},
doi = {10.1145/1390156.1390293},
abstract = {The infinite hidden Markov model is a non-parametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the infinite Hidden Markov model called beam sampling. Beam sampling combines slice sampling, which limits the number of states considered at each time step to a finite number, with dynamic programming, which samples whole state trajectories efficiently. Our algorithm typically outperforms the Gibbs sampler and is more robust. We present applications of iHMM inference using the beam sampler on changepoint detection and text prediction problems.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1088–1095},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390294,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and Composing Robust Features with Denoising Autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390294},
doi = {10.1145/1390156.1390294},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1096–1103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390295,
author = {Vovk, Vladimir and Zhdanov, Fedor},
title = {Prediction with Expert Advice for the Brier Game},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390295},
doi = {10.1145/1390156.1390295},
abstract = {We show that the Brier game of prediction is mixable and find the optimal learning rate and substitution function for it. The resulting prediction algorithm is applied to predict results of football and tennis matches. The theoretical performance guarantee turns out to be rather tight on these data sets, especially in the case of the more extensive tennis data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1104–1111},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390296,
author = {Walder, Christian and Kim, Kwang In and Sch\"{o}lkopf, Bernhard},
title = {Sparse Multiscale Gaussian Process Regression},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390296},
doi = {10.1145/1390156.1390296},
abstract = {Most existing sparse Gaussian process (g.p.) models seek computational advantages by basing their computations on a set of m basis functions that are the covariance function of the g.p. with one of its two inputs fixed. We generalise this for the case of Gaussian covariance function, by basing our computations on m Gaussian basis functions with arbitrary diagonal covariance matrices (or length scales). For a fixed number of basis functions and any given criteria, this additional flexibility permits approximations no worse and typically better than was previously possible. We perform gradient based optimisation of the marginal likelihood, which costs O(m2n) time where n is the number of data points, and compare the method to various other sparse g.p. methods. Although we focus on g.p. regression, the central idea is applicable to all kernel based algorithms, and we also provide some results for the support vector machine (s.v.m.) and kernel ridge regression (k.r.r.). Our approach outperforms the other methods, particularly for the case of very few basis functions, i. e. a very high sparsity ratio.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1112–1119},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390297,
author = {Wang, Chang and Mahadevan, Sridhar},
title = {Manifold Alignment Using Procrustes Analysis},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390297},
doi = {10.1145/1390156.1390297},
abstract = {In this paper we introduce a novel approach to manifold alignment, based on Procrustes analysis. Our approach differs from "semi-supervised alignment" in that it results in a mapping that is defined everywhere - when used with a suitable dimensionality reduction method - rather than just on the training data points. We describe and evaluate our approach both theoretically and experimentally, providing results showing useful knowledge transfer from one domain to another. Novel applications of our method including cross-lingual information retrieval and transfer learning in Markov decision processes are presented.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1120–1127},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390298,
author = {Wang, Hua-Yan and Yang, Qiang and Qin, Hong and Zha, Hongbin},
title = {Dirichlet Component Analysis: Feature Extraction for Compositional Data},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390298},
doi = {10.1145/1390156.1390298},
abstract = {We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated "correlations" while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1128–1135},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390299,
author = {Wang, Hua-Yan and Yang, Qiang and Zha, Hongbin},
title = {Adaptive P-Posterior Mixture-Model Kernels for Multiple Instance Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390299},
doi = {10.1145/1390156.1390299},
abstract = {In multiple instance learning (MIL), how the instances determine the bag-labels is an essential issue, both algorithmically and intrinsically. In this paper, we show that the mechanism of how the instances determine the bag-labels is different for different application domains, and does not necessarily obey the traditional assumptions of MIL. We therefore propose an adaptive framework for MIL that adapts to different application domains by learning the domain-specific mechanisms merely from labeled bags. Our approach is especially attractive when we are encountered with novel application domains, for which the mechanisms may be different and unknown. Specifically, we exploit mixture models to represent the composition of each bag and an adaptable kernel function to represent the relationship between the bags. We validate on synthetic MIL datasets that the kernel function automatically adapts to different mechanisms of how the instances determine the bag-labels. We also compare our approach with state-of-the-art MIL techniques on real-world benchmark datasets.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1136–1143},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390300,
author = {Wang, Jun and Jebara, Tony and Chang, Shih-Fu},
title = {Graph Transduction via Alternating Minimization},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390300},
doi = {10.1145/1390156.1390300},
abstract = {Graph transduction methods label input data by learning a classification function that is regularized to exhibit smoothness along a graph over labeled and unlabeled samples. In practice, these algorithms are sensitive to the initial set of labels provided by the user. For instance, classification accuracy drops if the training set contains weak labels, if imbalances exist across label classes or if the labeled portion of the data is not chosen at random. This paper introduces a propagation algorithm that more reliably minimizes a cost function over both a function on the graph and a binary label matrix. The cost function generalizes prior work in graph transduction and also introduces node normalization terms for resilience to label imbalances. We demonstrate that global minimization of the function is intractable but instead provide an alternating minimization scheme that incrementally adjusts the function and the labels towards a reliable local minimum. Unlike prior methods, the resulting propagation of labels does not prematurely commit to an erroneous labeling and obtains more consistent labels. Experiments are shown for synthetic and real classification tasks including digit and text recognition. A substantial improvement in accuracy compared to state of the art semi-supervised methods is achieved. The advantage are even more dramatic when labeled instances are limited.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1144–1151},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390301,
author = {Wang, Wei and Zhou, Zhi-Hua},
title = {On Multi-View Active Learning and the Combination with Semi-Supervised Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390301},
doi = {10.1145/1390156.1390301},
abstract = {Multi-view learning has become a hot topic during the past few years. In this paper, we first characterize the sample complexity of multi-view active learning. Under the α-expansion assumption, we get an exponential improvement in the sample complexity from usual \~{O}(1/ε) to \~{O}(log 1/ε), requiring neither strong assumption on data distribution such as the data is distributed uniformly over the unit sphere in Rd nor strong assumption on hypothesis class such as linear separators through the origin. We also give an upper bound of the error rate when the α-expansion assumption does not hold. Then, we analyze the combination of multi-view active learning and semi-supervised learning and get a further improvement in the sample complexity. Finally, we study the empirical behavior of the two paradigms, which verifies that the combination of multi-view active learning and semi-supervised learning is efficient.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1152–1159},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390302,
author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
title = {Fast Solvers and Efficient Implementations for Distance Metric Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390302},
doi = {10.1145/1390156.1390302},
abstract = {In this paper we study how to improve nearest neighbor classification by learning a Mahalanobis distance metric. We build on a recently proposed framework for distance metric learning known as large margin nearest neighbor (LMNN) classification. Our paper makes three contributions. First, we describe a highly efficient solver for the particular instance of semidefinite programming that arises in LMNN classification; our solver can handle problems with billions of large margin constraints in a few hours. Second, we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnified by learning low dimensional representations of the input space. Third, we show how to learn different Mahalanobis distance metrics in different parts of the input space. For large data sets, the use of locally adaptive distance metrics leads to even lower error rates.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1160–1167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390303,
author = {Weston, Jason and Ratle, Fr\'{e}d\'{e}ric and Collobert, Ronan},
title = {Deep Learning via Semi-Supervised Embedding},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390303},
doi = {10.1145/1390156.1390303},
abstract = {We show how nonlinear embedding algorithms popular for use with shallow semi-supervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1168–1175},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390304,
author = {Wingate, David and Singh, Satinder},
title = {Efficiently Learning Linear-Linear Exponential Family Predictive Representations of State},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390304},
doi = {10.1145/1390156.1390304},
abstract = {Exponential Family PSR (EFPSR) models capture stochastic dynamical systems by representing state as the parameters of an exponential family distribution over a shortterm window of future observations. They are appealing from a learning perspective because they are fully observed (meaning expressions for maximum likelihood do not involve hidden quantities), but are still expressive enough to both capture existing models and predict new models. While maximum-likelihood learning algorithms for EFPSRs exist, they are not computationally feasible. We present a new, computationally efficient, learning algorithm based on an approximate likelihood function. The algorithm can be interpreted as attempting to induce stationary distributions of observations, features and states which match their empirically observed counterparts. The approximate likelihood, and the idea of matching stationary distributions, may apply to other models.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1176–1183},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390305,
author = {Wolfe, Jason and Haghighi, Aria and Klein, Dan},
title = {Fully Distributed EM for Very Large Datasets},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390305},
doi = {10.1145/1390156.1390305},
abstract = {In EM and related algorithms, E-step computations distribute easily, because data items are independent given parameters. For very large data sets, however, even storing all of the parameters in a single node for the M-step can be impractical. We present a framework that fully distributes the entire EM procedure. Each node interacts only with parameters relevant to its data, sending messages to other nodes along a junction-tree topology. We demonstrate improvements over a MapReduce topology, on two tasks: word alignment and topic modeling.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1184–1191},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390306,
author = {Xia, Fen and Liu, Tie-Yan and Wang, Jue and Zhang, Wensheng and Li, Hang},
title = {Listwise Approach to Learning to Rank: Theory and Algorithm},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390306},
doi = {10.1145/1390156.1390306},
abstract = {This paper aims to conduct a study on the listwise approach to learning to rank. The listwise approach learns a ranking function by taking individual lists as instances and minimizing a loss function defined on the predicted list and the ground-truth list. Existing work on the approach mainly focused on the development of new algorithms; methods such as RankCosine and ListNet have been proposed and good performances by them have been observed. Unfortunately, the underlying theory was not sufficiently studied so far. To amend the problem, this paper proposes conducting theoretical analysis of learning to rank algorithms through investigations on the properties of the loss functions, including consistency, soundness, continuity, differentiability, convexity, and efficiency. A sufficient condition on consistency for ranking is given, which seems to be the first such result obtained in related research. The paper then conducts analysis on three loss functions: likelihood loss, cosine loss, and cross entropy loss. The latter two were used in RankCosine and ListNet. The use of the likelihood loss leads to the development of a new listwise method called ListMLE, whose loss function offers better properties, and also leads to better experimental results.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1192–1199},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390307,
author = {Yaman, Fusun and Walsh, Thomas J. and Littman, Michael L. and desJardins, Marie},
title = {Democratic Approximation of Lexicographic Preference Models},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390307},
doi = {10.1145/1390156.1390307},
abstract = {Previous algorithms for learning lexicographic preference models (LPMs) produce a "best guess" LPM that is consistent with the observations. Our approach is more democratic: we do not commit to a single LPM. Instead, we approximate the target using the votes of a collection of consistent LPMs. We present two variations of this method---variable voting and model voting---and empirically show that these democratic algorithms outperform the existing methods. We also introduce an intuitive yet powerful learning bias to prune some of the possible LPMs. We demonstrate how this learning bias can be used with variable and model voting and show that the learning bias improves the learning curve significantly, especially when the number of observations is small.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1200–1207},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390308,
author = {Yao, Hengshuai and Liu, Zhi-Qiang},
title = {Preconditioned Temporal Difference Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390308},
doi = {10.1145/1390156.1390308},
abstract = {This paper extends many of the recent popular policy evaluation algorithms to a generalized framework that includes least-squares temporal difference (LSTD) learning, least-squares policy evaluation (LSPE) and a variant of incremental LSTD (iLSTD). The basis of this extension is a preconditioning technique that solves a stochastic model equation. This paper also studies three significant issues of the new framework: it presents a new rule of step-size that can be computed online, provides an iterative way to apply preconditioning, and reduces the complexity of related algorithms to near that of temporal difference (TD) learning.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1208–1215},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390309,
author = {Yu, Jin and Vishwanathan, S. V. N. and G\"{u}nter, Simon and Schraudolph, Nicol N.},
title = {A Quasi-Newton Approach to Non-Smooth Convex Optimization},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390309},
doi = {10.1145/1390156.1390309},
abstract = {We extend the well-known BFGS quasi-Newton method and its limited-memory variant LBFGS to the optimization of non-smooth convex objectives. This is done in a rigorous fashion by generalizing three components of BFGS to subdifferentials: The local quadratic model, the identification of a descent direction, and the Wolfe line search conditions. We apply the resulting subLBFGS algorithm to L2-regularized risk minimization with binary hinge loss, and its direction-finding component to L1-regularized risk minimization with logistic loss. In both settings our generic algorithms perform comparable to or better than their counterparts in specialized state-of-the-art solvers.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1216–1223},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390310,
author = {Yue, Yisong and Joachims, Thorsten},
title = {Predicting Diverse Subsets Using Structural SVMs},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390310},
doi = {10.1145/1390156.1390310},
abstract = {In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively showing more information with the presented results. Secondly, queries are often ambiguous at some level. For example, the query "Jaguar" can refer to many different topics (such as the car or feline). A set of documents with high topic diversity ensures that fewer users abandon the query because no results are relevant to them. Unlike existing approaches to learning retrieval functions, we present a method that explicitly trains to diversify results. In particular, we formulate the learning problem of predicting diverse subsets and derive a training method based on structural SVMs.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1224–1231},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390311,
author = {Zhang, Kai and Tsang, Ivor W. and Kwok, James T.},
title = {Improved Nystr\"{o}m Low-Rank Approximation and Error Analysis},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390311},
doi = {10.1145/1390156.1390311},
abstract = {Low-rank matrix approximation is an effective tool in alleviating the memory and computational burdens of kernel methods and sampling, as the mainstream of such algorithms, has drawn considerable attention in both theory and practice. This paper presents detailed studies on the Nystr\"{o}m sampling scheme and in particular, an error analysis that directly relates the Nystr\"{o}m approximation quality with the encoding powers of the landmark points in summarizing the data. The resultant error bound suggests a simple and efficient sampling scheme, the k-means clustering algorithm, for Nystr\"{o}m low-rank approximation. We compare it with state-of-the-art approaches that range from greedy schemes to probabilistic sampling. Our algorithm achieves significant performance gains in a number of supervised/unsupervised learning tasks including kernel PCA and least squares SVM.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1232–1239},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390312,
author = {Zhang, Zhenjie and Dai, Bing Tian and Tung, Anthony K. H.},
title = {Estimating Local Optimums in EM Algorithm over Gaussian Mixture Model},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390312},
doi = {10.1145/1390156.1390312},
abstract = {EM algorithm is a very popular iteration-based method to estimate the parameters of Gaussian Mixture Model from a large observation set. However, in most cases, EM algorithm is not guaranteed to converge to the global optimum. Instead, it stops at some local optimums, which can be much worse than the global optimum. Therefore, it is usually required to run multiple procedures of EM algorithm with different initial configurations and return the best solution. To improve the efficiency of this scheme, we propose a new method which can estimate an upper bound on the logarithm likelihood of the local optimum, based on the current configuration after the latest EM iteration. This is accomplished by first deriving some region bounding the possible locations of local optimum, followed by some upper bound estimation on the maximum likelihood. With this estimation, we can terminate an EM algorithm procedure if the estimated local optimum is definitely worse than the best solution seen so far. Extensive experiments show that our method can effectively and efficiently accelerate conventional multiple restart EM algorithm.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1240–1247},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390313,
author = {Zhao, Bin and Wang, Fei and Zhang, Changshui},
title = {Efficient Multiclass Maximum Margin Clustering},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390313},
doi = {10.1145/1390156.1390313},
abstract = {This paper presents a cutting plane algorithm for multiclass maximum margin clustering (MMC). The proposed algorithm constructs a nested sequence of successively tighter relaxations of the original MMC problem, and each optimization problem in this sequence could be efficiently solved using the constrained concave-convex procedure (CCCP). Experimental evaluations on several real world datasets show that our algorithm converges much faster than existing MMC methods with guaranteed accuracy, and can thus handle much larger datasets efficiently.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1248–1255},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{10.1145/1390156.1390314,
author = {Zhu, Jun and Xing, Eric P. and Zhang, Bo},
title = {Laplace Maximum Margin Markov Networks},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390314},
doi = {10.1145/1390156.1390314},
abstract = {We propose Laplace max-margin Markov networks (LapM3N), and a general class of Bayesian M3N (BM3N) of which the LapM3N is a special case with sparse structural bias, for robust structured prediction. BM3N generalizes extant structured prediction rules based on point estimator to a Bayes-predictor using a learnt distribution of rules. We present a novel Structured Maximum Entropy Discrimination (SMED) formalism for combining Bayesian and max-margin learning of Markov networks for structured prediction, and our approach subsumes the conventional M3N as a special case. An efficient learning algorithm based on variational inference and standard convex-optimization solvers for M3N, and a generalization bound are offered. Our method outperforms competing ones on both synthetic and real OCR data.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1256–1263},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

