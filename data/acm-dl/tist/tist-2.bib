@article{10.1145/2508037.2508046,
author = {Wolf, Hannes and Herrmann, Klaus and Rothermel, Kurt},
title = {Dealing with Uncertainty: Robust Workflow Navigation in the Healthcare Domain},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508046},
doi = {10.1145/2508037.2508046},
abstract = {Processes in the healthcare domain are characterized by coarsely predefined recurring procedures that are flexibly adapted by the personnel to suite-specific situations. In this setting, a workflow management system that gives guidance and documents the personnel's actions can lead to a higher quality of care, fewer mistakes, and higher efficiency. However, most existing workflow management systems enforce rigid inflexible workflows and rely on direct manual input. Both are inadequate for healthcare processes. In particular, direct manual input is not possible in most cases since (1) it would distract the personnel even in critical situations and (2) it would violate fundamental hygiene principles by requiring disinfected doctors and nurses to touch input devices. The solution could be activity recognition systems that use sensor data (e.g., audio and acceleration data) to infer the current activities by the personnel and provide input to a workflow (e.g., informing it that a certain activity is finished now). However, state-of-the-art activity recognition technologies have difficulties in providing reliable information.We describe a comprehensive framework tailored for flexible human-centric healthcare processes that improves the reliability of activity recognition data. We present a set of mechanisms that exploit the application knowledge encoded in workflows in order to reduce the uncertainty of this data, thus enabling unobtrusive robust healthcare workflows. We evaluate our work based on a real-world case study and show that the robustness of unobtrusive healthcare workflows can be increased to an absolute value of up to 91% (compared to only 12% with a classical workflow system). This is a major breakthrough that paves the way towards future IT-enabled healthcare systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {65},
numpages = {23},
keywords = {business process management, workflow mining, Activity recognition, subjective logic, particle filters, Bayesian Networks, uncertain real-world context}
}

@article{10.1145/2508037.2508045,
author = {Rashidi, Parisa and Cook, Diane J.},
title = {COM: A Method for Mining and Monitoring Human Activity Patterns in Home-Based Health Monitoring Systems},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508045},
doi = {10.1145/2508037.2508045},
abstract = {The increasing aging population in the coming decades will result in many complications for society and in particular for the healthcare system due to the shortage of healthcare professionals and healthcare facilities. To remedy this problem, researchers have pursued developing remote monitoring systems and assisted living technologies by utilizing recent advances in sensor and networking technology, as well as in the data mining and machine learning fields. In this article, we report on our fully automated approach for discovering and monitoring patterns of daily activities. Discovering and tracking patterns of daily activities can provide unprecedented opportunities for health monitoring and assisted living applications, especially for older adults and individuals with mental disabilities. Previous approaches usually rely on preselected activities or labeled data to track and monitor daily activities. In this article, we present a fully automated approach by discovering natural activity patterns and their variations in real-life data. We will show how our activity discovery component can be integrated with an activity recognition component to track and monitor various daily activity patterns. We also provide an activity visualization component to allow caregivers to visually observe and examine the activity patterns using a user-friendly interface. We validate our algorithms using real-life data obtained from two apartments during a three-month period.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {64},
numpages = {20},
keywords = {smart environments, health monitoring, sequence mining, Assisted living technology}
}

@article{10.1145/2508037.2508044,
author = {Batal, Iyad and Valizadegan, Hamed and Cooper, Gregory F. and Hauskrecht, Milos},
title = {A Temporal Pattern Mining Approach for Classifying Electronic Health Record Data},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508044},
doi = {10.1145/2508037.2508044},
abstract = {We study the problem of learning classification models from complex multivariate temporal data encountered in electronic health record systems. The challenge is to define a good set of features that are able to represent well the temporal aspect of the data. Our method relies on temporal abstractions and temporal pattern mining to extract the classification features. Temporal pattern mining usually returns a large number of temporal patterns, most of which may be irrelevant to the classification task. To address this problem, we present the Minimal Predictive Temporal Patterns framework to generate a small set of predictive and nonspurious patterns. We apply our approach to the real-world clinical task of predicting patients who are at risk of developing heparin-induced thrombocytopenia. The results demonstrate the benefit of our approach in efficiently learning accurate classifiers, which is a key step for developing intelligent clinical monitoring systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {63},
numpages = {22},
keywords = {time-interval patterns, classification, Temporal pattern mining, temporal abstractions, multivariate time series}
}

@article{10.1145/2508037.2508043,
author = {Reddy, Chandan K. and Yang, Cristopher C.},
title = {Introduction to the Special Section on Intelligent Systems for Health Informatics},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508043},
doi = {10.1145/2508037.2508043},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {62},
numpages = {3}
}

@article{10.1145/2508037.2508042,
author = {Chen, Chao and Zhu, Qiusha and Lin, Lin and Shyu, Mei-Ling},
title = {Web Media Semantic Concept Retrieval via Tag Removal and Model Fusion},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508042},
doi = {10.1145/2508037.2508042},
abstract = {Multimedia data on social websites contain rich semantics and are often accompanied with user-defined tags. To enhance Web media semantic concept retrieval, the fusion of tag-based and content-based models can be used, though it is very challenging. In this article, a novel semantic concept retrieval framework that incorporates tag removal and model fusion is proposed to tackle such a challenge. Tags with useful information can facilitate media search, but they are often imprecise, which makes it important to apply noisy tag removal (by deleting uncorrelated tags) to improve the performance of semantic concept retrieval. Therefore, a multiple correspondence analysis (MCA)-based tag removal algorithm is proposed, which utilizes MCA's ability to capture the relationships among nominal features and identify representative and discriminative tags holding strong correlations with the target semantic concepts. To further improve the retrieval performance, a novel model fusion method is also proposed to combine ranking scores from both tag-based and content-based models, where the adjustment of ranking scores, the reliability of models, and the correlations between the intervals divided on the ranking scores and the semantic concepts are all considered. Comparative results with extensive experiments on the NUS-WIDE-LITE as well as the NUS-WIDE-270K benchmark datasets with 81 semantic concepts show that the proposed framework outperforms baseline results and the other comparison methods with each component being evaluated separately.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {61},
numpages = {22},
keywords = {multimedia semantic concept retrieval, multiple correspondence analysis (MCA), noisy tag removal, Social tags, model fusion}
}

@article{10.1145/2508037.2508041,
author = {Biancalana, Claudio and Gasparetti, Fabio and Micarelli, Alessandro and Sansonetti, Giuseppe},
title = {Social Semantic Query Expansion},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508041},
doi = {10.1145/2508037.2508041},
abstract = {Weak semantic techniques rely on the integration of Semantic Web techniques with social annotations and aim to embrace the strengths of both. In this article, we propose a novel weak semantic technique for query expansion. Traditional query expansion techniques are based on the computation of two-dimensional co-occurrence matrices. Our approach proposes the use of three-dimensional matrices, where the added dimension is represented by semantic classes (i.e., categories comprising all the terms that share a semantic property) related to the folksonomy extracted from social bookmarking services, such as delicious and StumbleUpon. The results of an indepth experimental evaluation performed on both artificial datasets and real users show that our approach outperforms traditional techniques, such as relevance feedback and personalized PageRank, so confirming the validity and usefulness of the categorization of the user needs and preferences in semantic classes. We also present the results of a questionnaire aimed to know the users opinion regarding the system. As one drawback of several query expansion techniques is their high computational costs, we also provide a complexity analysis of our system, in order to show its capability of operating in real time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {60},
numpages = {43},
keywords = {query expansion, Social Semantic Web, information retrieval}
}

@article{10.1145/2508037.2508039,
author = {Li, Xi and Hu, Weiming and Shen, Chunhua and Zhang, Zhongfei and Dick, Anthony and Hengel, Anton Van Den},
title = {A Survey of Appearance Models in Visual Object Tracking},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508039},
doi = {10.1145/2508037.2508039},
abstract = {Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models.To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic.The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {58},
numpages = {48},
keywords = {Visual object tracking, appearance model, features, statistical modeling}
}

@article{10.1145/2508037.2508038,
author = {Jiang, Daxin and Pei, Jian and Li, Hang},
title = {Mining Search and Browse Logs for Web Search: A Survey},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2508037.2508038},
doi = {10.1145/2508037.2508038},
abstract = {Huge amounts of search log data have been accumulated at Web search engines. Currently, a popular Web search engine may receive billions of queries and collect terabytes of records about user search behavior daily. Beside search log data, huge amounts of browse log data have also been collected through client-side browser plugins. Such massive amounts of search and browse log data provide great opportunities for mining the wisdom of crowds and improving Web search. At the same time, designing effective and efficient methods to clean, process, and model log data also presents great challenges.In this survey, we focus on mining search and browse log data for Web search. We start with an introduction to search and browse log data and an overview of frequently-used data summarizations in log mining. We then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, document ranking, user understanding, and monitoring and feedback. For each aspect, we survey the major tasks, fundamental principles, and state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {57},
numpages = {37},
keywords = {query understanding, user understanding, browse log, Search logs, feedbacks, survey, monitoring, document understanding, Web search, document ranking, log mining}
}

@article{10.1145/2501603,
author = {Cena, Federica and Dattolo, Antonina and Lops, Pasquale and Vassileva, Julita},
title = {Perspectives in Semantic Adaptive Social Web},
year = {2013},
issue_date = {September 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2501603},
doi = {10.1145/2501603},
abstract = {The Social Web is now a successful reality with its quickly growing number of users and applications. Also the Semantic Web, which started with the objective of describing Web resources in a machine-processable way, is now outgrowing the research labs and is being massively exploited in many websites, incorporating high-quality user-generated content and semantic annotations. The primary goal of this special section is to showcase some recent research at the intersection of the Social Web and the Semantic Web that explores the benefits that adaptation and personalization have to offer in the Web of the future, the so-called Social Adaptive Semantic Web. We have selected two articles out of fourteen submissions based on the quality of the articles and we present the main lessons learned from the overall analysis of these submissions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {59},
numpages = {8},
keywords = {Semantic Web, adaptation, Social Web}
}

@article{10.1145/2483669.2483689,
author = {Marathe, Achla and Pan, Zhengzheng and Apolloni, Andrea},
title = {Analysis of Friendship Network and Its Role in Explaining Obesity},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483689},
doi = {10.1145/2483669.2483689},
abstract = {We employ Add Health data to show that friendship networks, constructed from mutual friendship nominations, are important in building weight perception, setting weight goals, and measuring social marginalization among adolescents and young adults. We study the relationship between individuals' perceived weight status, actual weight status, weight status relative to friends' weight status, and weight goals. This analysis helps us understand how individual weight perceptions might be formed, what these perceptions do to the weight goals, and how friends' relative weight affects weight perception and weight goals. Combining this information with individuals' friendship network helps determine the influence of social relationships on weight-related variables. Multinomial logistic regression results indicate that relative status is indeed a significant predictor of perceived status, and perceived status is a significant predictor of weight goals. We also address the issue of causality between actual weight status and social marginalization (as measured by the number of friends) and show that obesity precedes social marginalization in time rather than the other way around. This lends credence to the hypothesis that obesity leads to social marginalization not vice versa. Attributes of the friendship network can provide new insights into effective interventions for combating obesity since adolescent friendships provide an important social context for weight-related behaviors.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {56},
numpages = {21},
keywords = {friendship network, relative weight, obesity, Add Health, causality, perceived weight}
}

@article{10.1145/2483669.2483688,
author = {Saito, Kazumi and Kimura, Masahiro and Ohara, Kouzou and Motoda, Hiroshi},
title = {Detecting Changes in Information Diffusion Patterns over Social Networks},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483688},
doi = {10.1145/2483669.2483688},
abstract = {We addressed the problem of detecting the change in behavior of information diffusion over a social network which is caused by an unknown external situation change using a small amount of observation data in a retrospective setting. The unknown change is assumed effectively reflected in changes in the parameter values in the probabilistic information diffusion model, and the problem is reduced to detecting where in time and how long this change persisted and how big this change is. We solved this problem by searching the change pattern that maximizes the likelihood of generating the observed information diffusion sequences, and in doing so we devised a very efficient general iterative search algorithm using the derivative of the likelihood which avoids parameter value optimization during each search step. This is in contrast to the naive learning algorithm in that it has to iteratively update the patten boundaries, each requiring the parameter value optimization and thus is very inefficient. We tested this algorithm for two instances of the probabilistic information diffusion model which has different characteristics. One is of information push style and the other is of information pull style. We chose Asynchronous Independent Cascade (AsIC) model as the former and Value-weighted Voter (VwV) model as the latter. The AsIC is the model for general information diffusion with binary states and the parameter to detect its change is diffusion probability and the VwV is the model for opinion formation with multiple states and the parameter to detect its change is opinion value. The results tested on these two models using four real-world network structures confirm that the algorithm is robust enough and can efficiently identify the correct change pattern of the parameter values. Comparison with the naive method that finds the best combination of change boundaries by an exhaustive search through a set of randomly selected boundary candidates shows that the proposed algorithm far outperforms the native method both in terms of accuracy and computation time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {55},
numpages = {23},
keywords = {information diffusion, Change point detection, social networks, parameter learning}
}

@article{10.1145/2483669.2483687,
author = {Fridman, Natalie and Kaminka, Gal A.},
title = {Using Qualitative Reasoning for Social Simulation of Crowds},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483687},
doi = {10.1145/2483669.2483687},
abstract = {The ability to model and reason about the potential violence level of a demonstration is important to the police decision making process. Unfortunately, existing knowledge regarding demonstrations is composed of partial qualitative descriptions without complete and precise numerical information. In this article we describe a first attempt to use qualitative reasoning techniques to model demonstrations. To our knowledge, such techniques have never been applied to modeling and reasoning regarding crowd behaviors, nor in particular demonstrations. We develop qualitative models consistent with the partial, qualitative social science literature, allowing us to model the interactions between different factors that influence violence in demonstrations. We then utilize qualitative simulation to predict the potential eruption of violence, at various levels, based on a description of the demographics, environmental settings, and police responses. We incrementally present and compare three such qualitative models. The results show that while two of these models fail to predict the outcomes of real-world events reported and analyzed in the literature, one model provides good results. We also examine whether a popular machine learning algorithm (decision tree learning) can be used. While the results show that the decision trees provide improved predictions, we show that the QR models can be more sensitive to changes, and can account for what-if scenarios, in contrast to decision trees. Moreover, we introduce a novel analysis algorithm that analyzes the QR simulations, to automatically determine the factors that are most important in influencing the outcome in specific real-world demonstrations. We show that the algorithm identifies factors that correspond to experts' analysis of these events.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {54},
numpages = {21},
keywords = {Demonstrations, social simulation, qualitative reasoning}
}

@article{10.1145/2483669.2483686,
author = {Gintis, Herbert},
title = {Markov Models of Social Dynamics: Theory and Applications},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483686},
doi = {10.1145/2483669.2483686},
abstract = {This article shows how agent-based models of social dynamics can be treated rigorously and analytically as finite Markov processes, and their long-run properties are then given by an expanded version of the ergodic theorem for Markov processes. A Markov process model of a simplified market economy shows the fruitfulness of this approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {53},
numpages = {19}
}

@article{10.1145/2483669.2483685,
author = {Hung, Benjamin W.K. and Kolitz, Stephan E. and Ozdaglar, Asuman},
title = {Optimization-Based Influencing of Village Social Networks in a Counterinsurgency},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483685},
doi = {10.1145/2483669.2483685},
abstract = {This article considers the nonlethal targeting assignment problem in the counterinsurgency in Afghanistan, the problem of deciding on the people whom U.S. forces should engage through outreach, negotiations, meetings, and other interactions in order to ultimately win the support of the population in their area of operations. We propose two models: (1) the Afghan counterinsurgency (COIN) social influence model, to represent how attitudes of local leaders are affected by repeated interactions with other local leaders, insurgents, and counterinsurgents, and (2) the nonlethal targeting model, a NonLinear Programming (NLP) optimization formulation that identifies a strategy for assigning k U.S. agents to produce the greatest arithmetic mean of the expected long-term attitude of the population. We demonstrate in an experiment the merits of the optimization model in nonlethal targeting, which performs significantly better than both doctrine-based and random methods of assignment in a large network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {52},
numpages = {22},
keywords = {Social network, agent modeling, opinion dynamics, counterinsurgency, network optimization}
}

@article{10.1145/2483669.2483684,
author = {Yang, Shanchieh Jay and Nau, Dana and Salerno, John},
title = {Introduction to the Special Section on Social Computing, Behavioral-Cultural Modeling, and Prediction},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483684},
doi = {10.1145/2483669.2483684},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {51},
numpages = {2}
}

@article{10.1145/2483669.2483683,
author = {Chin, Alvin and Xu, Bin and Wang, Hao and Chang, Lele and Wang, Hao and Zhu, Lijun},
title = {Connecting People through Physical Proximity and Physical Resources at a Conference},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483683},
doi = {10.1145/2483669.2483683},
abstract = {This work investigates how to bridge the gap between offline and online behaviors at a conference and how the physical resources in the conference (the physical objects used in the conference for gathering attendees together in engaging an activity such as rooms, sessions, and papers) can be used to help facilitate social networking. We build Find and Connect, a system that integrates offline activities and interactions captured in real time with online connections in a conference environment, to provide a list of potential people one should connect to for forming an ephemeral social network. We investigate how social connections can be established and integrated with physical resources through positioning technology, and the relationship between physical proximity encounters and online social connections. Results from our two datasets of two trials, one at the UIC/ATC 2010 conference and GCJK internal marketing event, show that social connections that are reciprocal in relationship, such as friendship and exchanged contacts, have tighter, denser, and highly clustered networks compared to unidirectional relationships such as follow. We discover that there is a positive relationship between physical proximity encounters and online social connections before the social connection is made for friends, but a negative relationship for after the social connection is made. The first indicates social selection is strong, and the second indicates social influence is weak. Even though our dataset is sparse, nonetheless we believe our work is promising and novel which is worthy of future research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {50},
numpages = {21},
keywords = {Ephemeral social network, physical proximity, mobile social network, resource, social networking}
}

@article{10.1145/2483669.2483682,
author = {Yan, Zhixian and Chakraborty, Dipanjan and Parent, Christine and Spaccapietra, Stefano and Aberer, Karl},
title = {Semantic Trajectories: Mobility Data Computation and Annotation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483682},
doi = {10.1145/2483669.2483682},
abstract = {With the large-scale adoption of GPS equipped mobile sensing devices, positional data generated by moving objects (e.g., vehicles, people, animals) are being easily collected. Such data are typically modeled as streams of spatio-temporal (x,y,t) points, called trajectories. In recent years trajectory management research has progressed significantly towards efficient storage and indexing techniques, as well as suitable knowledge discovery. These works focused on the geometric aspect of the raw mobility data. We are now witnessing a growing demand in several application sectors (e.g., from shipment tracking to geo-social networks) on understanding the semantic behavior of moving objects. Semantic behavior refers to the use of semantic abstractions of the raw mobility data, including not only geometric patterns but also knowledge extracted jointly from the mobility data and the underlying geographic and application domains information. The core contribution of this article lies in a semantic model and a computation and annotation platform for developing a semantic approach that progressively transforms the raw mobility data into semantic trajectories enriched with segmentations and annotations. We also analyze a number of experiments we did with semantic trajectories in different domains.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {49},
numpages = {38},
keywords = {trajectory computing, trajectory annotation, hidden Markov model, trajectory segmentation, spatial join, Spatio-temporal/structured/semantic trajectory, map matching}
}

@article{10.1145/2483669.2483681,
author = {Wei, Ling-Yin and Peng, Wen-Chih and Lee, Wang-Chien},
title = {Exploring Pattern-Aware Travel Routes for Trajectory Search},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483681},
doi = {10.1145/2483669.2483681},
abstract = {With the popularity of positioning devices, Web 2.0 technology, and trip sharing services, many users are willing to log and share their trips on the Web. Thus, trip planning Web sites are able to provide some new services by inferring Regions-Of-Interest (ROIs) and recommending popular travel routes from trip trajectories. We argue that simply providing some travel routes consisting of popular ROIs to users is not sufficient. To tour around a wide geographical area, for example, a city, some users may prefer a trip to visit as many ROIs as possible, while others may like to stop by only a few ROIs for an in-depth visit. We refer to a trip fitting the former user group as an in-breadth trip and a trip suitable for the latter user group as an in-depth trip. Prior studies on trip planning have focused on mining ROIs and travel routes without considering these different preferences. In this article, given a spatial range and a user preference of depth/breadth specified by a user, we develop a Pattern-Aware Trajectory Search (PATS) framework to retrieve the top K trajectories passing through popular ROIs. PATS is novel because the returned travel trajectories, discovered from travel patterns hidden in trip trajectories, may represent the most valuable travel experiences of other travelers fitting the user's trip preference in terms of depth or breadth. The PATS framework comprises two components: travel behavior exploration and trajectory search. The travel behavior exploration component determines a set of ROIs along with their attractive scores by considering not only the popularity of the ROIs but also the travel sequential relationships among the ROIs. To capture the travel sequential relationships among ROIs and to derive their attractive scores, a user movement graph is constructed. For the trajectory search component of PATS, we formulate two trajectory score functions, the depth-trip score function and the breadth-trip score function, by taking into account the number of ROIs in a trajectory and their attractive scores. Accordingly, we propose an algorithm, namely, Bounded Trajectory Search (BTS), to efficiently retrieve the top K trajectories based on the two trajectory scores. The PATS framework is evaluated by experiments and user studies using a real dataset. The experimental results demonstrate the effectiveness and the efficiency of the proposed PATS framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {48},
numpages = {25},
keywords = {trajectory search, data mining, route planning, Trajectory pattern mining}
}

@article{10.1145/2483669.2483680,
author = {Shi, Yue and Serdyukov, Pavel and Hanjalic, Alan and Larson, Martha},
title = {Nontrivial Landmark Recommendation Using Geotagged Photos},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483680},
doi = {10.1145/2483669.2483680},
abstract = {Online photo-sharing sites provide a wealth of information about user behavior and their potential is increasing as it becomes ever-more common for images to be associated with location information in the form of geotags. In this article, we propose a novel approach that exploits geotagged images from an online community for the purpose of personalized landmark recommendation. Under our formulation of the task, recommended landmarks should be relevant to user interests and additionally they should constitute nontrivial recommendations. In other words, recommendations of landmarks that are highly popular and frequently visited and can be easily discovered through other information sources such as travel guides should be avoided in favor of recommendations that relate to users' personal interests. We propose a collaborative filtering approach to the personalized landmark recommendation task within a matrix factorization framework. Our approach, WMF-CR, combines weighted matrix factorization and category-based regularization. The integrated weights emphasize the contribution of nontrivial landmarks in order to focus the recommendation model specifically on the generation of nontrivial recommendations. They support the judicious elimination of trivial landmarks from consideration without also discarding information valuable for recommendation. Category-based regularization addresses the sparse data problem, which is arguably even greater in the case of our landmark recommendation task than in other recommendation scenarios due to the limited amount of travel experience recorded in the online image set of any given user. We use category information extracted from Wikipedia in order to provide the system with a method to generalize the semantics of landmarks and allow the model to relate them not only on the basis of identity, but also on the basis of topical commonality. The proposed approach is computational scalable, that is, its complexity is linear with the number of observed preferences in the user-landmark preference matrix and the number of nonzero similarities in the category-based landmark similarity matrix. We evaluate the approach on a large collection of geotagged photos gathered from Flickr. Our experimental results demonstrate that WMF-CR outperforms several state-of-the-art baseline approaches in recommending nontrivial landmarks. Additionally, they demonstrate that the approach is well suited for addressing data sparseness and provides particular performance improvement in the case of users who have limited travel experience, that is, have visited only few cities or few landmarks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {47},
numpages = {27},
keywords = {geotag, nontrivial recommendation, Collaborative filtering, location-based recommendation, social media application}
}

@article{10.1145/2483669.2483679,
author = {Schuster, Daniel and Rosi, Alberto and Mamei, Marco and Springer, Thomas and Endler, Markus and Zambonelli, Franco},
title = {Pervasive Social Context: Taxonomy and Survey},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483679},
doi = {10.1145/2483669.2483679},
abstract = {As pervasive computing meets social networks, there is a fast growing research field called pervasive social computing. Applications in this area exploit the richness of information arising out of people using sensor-equipped pervasive devices in their everyday life combined with intense use of different social networking services. We call this set of information pervasive social context. We provide a taxonomy to classify pervasive social context along the dimensions space, time, people, and information source (STiPI) as well as commenting on the type and reason for creating such context. A survey of recent research shows the applicability and usefulness of the taxonomy in classifying and assessing applications and systems in the area of pervasive social computing. Finally, we present some research challenges in this area and illustrate how they affect the systems being surveyed.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {46},
numpages = {22},
keywords = {context awareness, survey, taxonomy, Pervasive computing, social networks}
}

@article{10.1145/2483669.2483678,
author = {Yu, Zhiwen and Zhang, Daqing and Eagle, Nathan and Cook, Diane},
title = {Introduction to the Special Section on Intelligent Systems for Socially Aware Computing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483678},
doi = {10.1145/2483669.2483678},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {45},
numpages = {3}
}

@article{10.1145/2483669.2483677,
author = {Bouamor, Houda and Max, Aur\'{e}elien and Vilnat, Anne},
title = {Multitechnique Paraphrase Alignment: A Contribution to Pinpointing Sub-Sentential Paraphrases},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483677},
doi = {10.1145/2483669.2483677},
abstract = {This work uses parallel monolingual corpora for a detailed study of the task of sub-sentential paraphrase acquisition. We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited type for studies on paraphrasing. We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, their combinations, as well as four monolingual corpus types of varying comparability. We report, under all conditions, a significant improvement over all techniques by validating candidate paraphrases using a maximum entropy classifier. An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {44},
numpages = {27},
keywords = {Paraphrase acquisition, paraphrase corpora}
}

@article{10.1145/2483669.2483676,
author = {Burrows, Steven and Potthast, Martin and Stein, Benno},
title = {Paraphrase Acquisition via Crowdsourcing and Machine Learning},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483676},
doi = {10.1145/2483669.2483676},
abstract = {To paraphrase means to rewrite content while preserving the original meaning. Paraphrasing is important in fields such as text reuse in journalism, anonymizing work, and improving the quality of customer-written reviews. This article contributes to paraphrase acquisition and focuses on two aspects that are not addressed by current research: (1) acquisition via crowdsourcing, and (2) acquisition of passage-level samples. The challenge of the first aspect is automatic quality assurance; without such a means the crowdsourcing paradigm is not effective, and without crowdsourcing the creation of test corpora is unacceptably expensive for realistic order of magnitudes. The second aspect addresses the deficit that most of the previous work in generating and evaluating paraphrases has been conducted using sentence-level paraphrases or shorter; these short-sample analyses are limited in terms of application to plagiarism detection, for example. We present the Webis Crowd Paraphrase Corpus 2011 (Webis-CPC-11), which recently formed part of the PAN 2010 international plagiarism detection competition. This corpus comprises passage-level paraphrases with 4067 positive samples and 3792 negative samples that failed our criteria, using Amazon's Mechanical Turk for crowdsourcing. In this article, we review the lessons learned at PAN 2010, and explain in detail the method used to construct the corpus. The empirical contributions include machine learning experiments to explore if passage-level paraphrases can be identified in a two-class classification problem using paraphrase similarity features, and we find that a k-nearest-neighbor classifier can correctly distinguish between paraphrased and nonparaphrased samples with 0.980 precision at 0.523 recall. This result implies that just under half of our samples must be discarded (remaining 0.477 fraction), but our cost analysis shows that the automation we introduce results in a 18% financial saving and over 100 hours of time returned to the researchers when repeating a similar corpus design. On the other hand, when building an unrelated corpus requiring, say, 25% training data for the automated component, we show that the financial outcome is cost neutral, while still returning over 70 hours of time to the researchers. The work presented here is the first to join the paraphrasing and plagiarism communities.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {43},
numpages = {21},
keywords = {plagiarism, cost analysis, corpus, Mechanical Turk, Paraphrase generation}
}

@article{10.1145/2483669.2483675,
author = {Moon, Taesun and Erk, Katrin},
title = {An Inference-Based Model of Word Meaning in Context as a Paraphrase Distribution},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483675},
doi = {10.1145/2483669.2483675},
abstract = {Graded models of word meaning in context characterize the meaning of individual usages (occurrences) without reference to dictionary senses. We introduce a novel approach that frames the task of computing word meaning in context as a probabilistic inference problem. The model represents the meaning of a word as a probability distribution over potential paraphrases, inferred using an undirected graphical model. Evaluated on paraphrasing tasks, the model achieves state-of-the-art performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {28},
keywords = {probabilistic inference, Semantics, probabilistic graphical models, loopy belief propagation, lexical semantics, paraphrases}
}

@article{10.1145/2483669.2483674,
author = {Cohn, Trevor and Lapata, Mirella},
title = {An Abstractive Approach to Sentence Compression},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483674},
doi = {10.1145/2483669.2483674},
abstract = {In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {35},
keywords = {language models, machine translation, transduction, Language generation, paraphrases, synchronous grammars, sentence compression}
}

@article{10.1145/2483669.2483673,
author = {Madnani, Nitin and Dorr, Bonnie J.},
title = {Generating Targeted Paraphrases for Improved Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483673},
doi = {10.1145/2483669.2483673},
abstract = {Today's Statistical Machine Translation (SMT) systems require high-quality human translations for parameter tuning, in addition to large bitexts for learning the translation units. This parameter tuning usually involves generating translations at different points in the parameter space and obtaining feedback against human-authored reference translations as to how good the translations. This feedback then dictates what point in the parameter space should be explored next. To measure this feedback, it is generally considered wise to have multiple (usually 4) reference translations to avoid unfair penalization of translation hypotheses which could easily happen given the large number of ways in which a sentence can be translated from one language to another. However, this reliance on multiple reference translations creates a problem since they are labor intensive and expensive to obtain. Therefore, most current MT datasets only contain a single reference. This leads to the problem of reference sparsity. In our previously published research, we had proposed the first paraphrase-based solution to this problem and evaluated its effect on Chinese-English translation. In this article, we first present extended results for that solution on additional source languages. More importantly, we present a novel way to generate “targeted” paraphrases that yields substantially larger gains (up to 2.7 BLEU points) in translation quality when compared to our previous solution (up to 1.6 BLEU points). In addition, we further validate these improvements by supplementing with human preference judgments obtained via Amazon Mechanical Turk.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {40},
numpages = {25},
keywords = {Natural language processing, paraphrasing, machine translation}
}

@article{10.1145/2483669.2483672,
author = {Marton, Yuval},
title = {Distributional Phrasal Paraphrase Generation for Statistical Machine Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483672},
doi = {10.1145/2483669.2483672},
abstract = {Paraphrase generation has been shown useful for various natural language processing tasks, including statistical machine translation. A commonly used method for paraphrase generation is pivoting [Callison-Burch et al. 2006], which benefits from linguistic knowledge implicit in the sentence alignment of parallel texts, but has limited applicability due to its reliance on parallel texts. Distributional paraphrasing [Marton et al. 2009a] has wider applicability, is more language-independent, but doesn't benefit from any linguistic knowledge. Nevertheless, we show that using distributional paraphrasing can yield greater gains in translation tasks. We report method improvements leading to higher gains than previously published, of almost 2 Bleu points, and provide implementation details, complexity analysis, and further insight into this method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {32},
keywords = {Semantic similarity, SMT, semantic distance, paraphrase generation, statistical machine translation}
}

@article{10.1145/2483669.2483671,
author = {Resnik, Philip and Buzek, Olivia and Kronrod, Yakov and Hu, Chang and Quinn, Alexander J. and Bederson, Benjamin B.},
title = {Using Targeted Paraphrasing and Monolingual Crowdsourcing to Improve Translation},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483671},
doi = {10.1145/2483669.2483671},
abstract = {Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation, which makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e., paraphrases) with only monolingual knowledge of the source language. Formal evaluation demonstrates that this approach can yield substantial improvements in translation quality, and the idea has been integrated into a broader framework for monolingual collaborative translation that produces fully accurate, fully fluent translations for a majority of sentences in a real-world translation task, with no involvement of human bilingual speakers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {38},
numpages = {21},
keywords = {translation, paraphrase, machine translation, wisdom of crowds, human computation, Monolingual, crowdsourcing, translation interface}
}

@article{10.1145/2483669.2483670,
author = {Wang, Haifeng and Dolan, Bill and Szpektor, Idan and Zhao, Shiqi},
title = {Introduction to Special Section on Paraphrasing},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2483669.2483670},
doi = {10.1145/2483669.2483670},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {2}
}

@article{10.1145/2438653.2438671,
author = {Tran, Vien and Nguyen, Khoi and Son, Tran Cao and Pontelli, Enrico},
title = {A Conformant Planner Based on Approximation: C<span class="smallcaps SmallerCapital">p</span>A(H)},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438671},
doi = {10.1145/2438653.2438671},
abstract = {This article describes the planner CpA(H), the recipient of the Best Nonobservable Nondeterministic Planner Award in the “Uncertainty Track” of the 6th International Planning Competition (IPC), 2008. The article presents the various techniques that help CpA(H) to achieve the level of performance and scalability exhibited in the competition. The article also presents experimental results comparing CpA(H) with state-of-the-art conformant planners.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {36},
numpages = {38},
keywords = {international planning competition, approximations, conformant planning, Plan generation}
}

@article{10.1145/2438653.2438670,
author = {Song, Xuan and Shao, Xiaowei and Zhang, Quanshi and Shibasaki, Ryosuke and Zhao, Huijing and Cui, Jinshi and Zha, Hongbin},
title = {A Fully Online and Unsupervised System for Large and High-Density Area Surveillance: Tracking, Semantic Scene Learning and Abnormality Detection},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438670},
doi = {10.1145/2438653.2438670},
abstract = {For reasons of public security, an intelligent surveillance system that can cover a large, crowded public area has become an urgent need. In this article, we propose a novel laser-based system that can simultaneously perform tracking, semantic scene learning, and abnormality detection in a fully online and unsupervised way. Furthermore, these three tasks cooperate with each other in one framework to improve their respective performances. The proposed system has the following key advantages over previous ones: (1) It can cover quite a large area (more than 60\texttimes{}35m), and simultaneously perform robust tracking, semantic scene learning, and abnormality detection in a high-density situation. (2) The overall system can vary with time, incrementally learn the structure of the scene, and perform fully online abnormal activity detection and tracking. This feature makes our system suitable for real-time applications. (3) The surveillance tasks are carried out in a fully unsupervised manner, so that there is no need for manual labeling and the construction of huge training datasets. We successfully apply the proposed system to the JR subway station in Tokyo, and demonstrate that it can cover an area of 60\texttimes{}35m, robustly track more than 150 targets at the same time, and simultaneously perform online semantic scene learning and abnormality detection with no human intervention.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {35},
numpages = {21},
keywords = {Surveillance, abnormality detection, Multitarget tracking, semantic scene learning}
}

@article{10.1145/2438653.2438669,
author = {Wang, Zhengxiang and Hu, Yiqun and Chia, Liang-Tien},
title = {Learning Image-to-Class Distance Metric for Image Classification},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438669},
doi = {10.1145/2438653.2438669},
abstract = {Image-To-Class (I2C) distance is a novel distance used for image classification and has successfully handled datasets with large intra-class variances. However, it uses Euclidean distance for measuring the distance between local features in different classes, which may not be the optimal distance metric in real image classification problems. In this article, we propose a distance metric learning method to improve the performance of I2C distance by learning per-class Mahalanobis metrics in a large margin framework. Our I2C distance is adaptive to different classes by combining with the learned metric for each class. These multiple per-class metrics are learned simultaneously by forming a convex optimization problem with the constraints that the I2C distance from each training image to its belonging class should be less than the distances to other classes by a large margin. A subgradient descent method is applied to efficiently solve this optimization problem. For efficiency and scalability to large-scale problems, we also show how to simplify the method to learn a diagonal matrix for each class. We show in experiments that our learned Mahalanobis I2C distance can significantly outperform the original Euclidean I2C distance as well as other distance metric learning methods in several prevalent image datasets, and our simplified diagonal matrices can preserve the performance but significantly speed up the metric learning procedure for large-scale datasets. We also show in experiment that our method is able to correct the class imbalance problem, which usually leads the NN-based methods toward classes containing more training images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {34},
numpages = {22},
keywords = {Image-to-class distance, image classification, distance metric learning, nearest-neighbor classification}
}

@article{10.1145/2438653.2438668,
author = {Tabia, Hedi and Daoudi, Mohamed and Vandeborre, Jean-Philippe and Colot, Olivier},
title = {A Parts-Based Approach for Automatic 3D Shape Categorization Using Belief Functions},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438668},
doi = {10.1145/2438653.2438668},
abstract = {Grouping 3D objects into (semantically) meaningful categories is a challenging and important problem in 3D mining and shape processing. Here, we present a novel approach to categorize 3D objects. The method described in this article, is a belief-function-based approach and consists of two stages: the training stage, where 3D objects in the same category are processed and a set of representative parts is constructed, and the labeling stage, where unknown objects are categorized. The experimental results obtained on the Tosca-Sumner and the Shrec07 datasets show that the system efficiently performs in categorizing 3D models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {33},
numpages = {16},
keywords = {Multimedia data, object recognition, classification, belief functions, 3D categorization}
}

@article{10.1145/2438653.2438667,
author = {Wang, Fei-Yue and Wong, Pak Kin},
title = {Intelligent Systems and Technology for Integrative and Predictive Medicine: An ACP Approach},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438667},
doi = {10.1145/2438653.2438667},
abstract = {One of the principal goals in medicine is to determine and implement the best treatment for patients through fastidious estimation of the effects and benefits of therapeutic procedures. The inherent complexities of physiological and pathological networks that span across orders of magnitude in time and length scales, however, represent fundamental hurdles in determining effective treatments for patients. Here we argue for a new approach, called the ACP-based approach, that combines artificial (societies), computational (experiments), and parallel (execution) methods in intelligent systems and technology for integrative and predictive medicine, or more generally, precision medicine and smart health management. The advent of artificial societies that collect the clinically relevant information in prognostics and therapeutics provides a promising platform for organizing and experimenting complex physiological systems toward integrative medicine. The ability of computational experiments to analyze distinct, interactive systems such as the host mechanisms, pathological pathways, and therapeutic strategies, as well as other factors using the artificial systems, will enable control and management through parallel execution of real and arficial systems concurrently within the integrative medicine context. The development of this framework in integrative medicine, fueled by close collaborations between physicians, engineers, and scientists, will result in preventive and predictive practices of a personal, proactive, and precise nature, including rational combinatorial treatments, adaptive therapeutics, and patient-oriented disease management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {32},
numpages = {6}
}

@article{10.1145/2438653.2438666,
author = {Ehara, Yo and Shimizu, Nobuyuki and Ninomiya, Takashi and Nakagawa, Hiroshi},
title = {Personalized Reading Support for Second-Language Web Documents},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438666},
doi = {10.1145/2438653.2438666},
abstract = {A novel intelligent interface eases the browsing of Web documents written in the second languages of users. It automatically predicts words unfamiliar to the user by a collective intelligence method and glosses them with their meaning in advance. If the prediction succeeds, the user does not need to consult a dictionary; even if it fails, the user can correct the prediction. The correction data are collected and used to improve the accuracy of further predictions. The prediction is personalized in that every user's language ability is estimated by a state-of-the-art language testing model, which is trained in a practical response time with only a small sacrifice of prediction accuracy. The system was evaluated in terms of prediction accuracy and reading simulation. The reading simulation results show that this system can reduce the number of clicks for most readers with insufficient vocabulary to read documents and can significantly reduce the remaining number of unfamiliar words after the prediction and glossing for all users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {19},
keywords = {glossing systems, logistic regression, Web pages, item response theory, Reading support}
}

@article{10.1145/2438653.2438665,
author = {Yen, Neil Y. and Shih, Timothy K. and Jin, Qun},
title = {LONET: An Interactive Search Network for Intelligent Lecture Path Generation},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438665},
doi = {10.1145/2438653.2438665},
abstract = {Sharing resources and information on the Internet has become an important activity for education. In distance learning, instructors can benefit from resources, also known as Learning Objects (LOs), to create plenteous materials for specific learning purposes. Our repository (called the MINE Registry) has been developed for storing and sharing learning objects, around 22,000 in total, in the past few years. To enhance reusability, one significant concept named Reusability Tree was implemented to trace the process of changes. Also, weighting and ranking metrics have been proposed to enhance the searchability in the repository. Following the successful implementation, this study goes further to investigate the relationships between LOs from a perspective of social networks. The LONET (Learning Object Network), as an extension of Reusability Tree, is newly proposed and constructed to clarify the vague reuse scenario in the past, and to summarize collaborative intelligence through past interactive usage experiences. We define a social structure in our repository based on past usage experiences from instructors, by proposing a set of metrics to evaluate the interdependency such as prerequisites and references. The structure identifies usage experiences and can be graphed in terms of implicit and explicit relations among learning objects. As a practical contribution, an adaptive algorithm is proposed to mine the social structure in our repository. The algorithm generates adaptive routes, based on past usage experiences, by computing possible interactive input, such as search criteria and feedback from instructors, and assists them in generating specific lectures.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {30},
numpages = {27},
keywords = {distance learning, social network analysis, ranking, interactive search, Learning object network, SCORM, repository, lecture path}
}

@article{10.1145/2438653.2438664,
author = {Folsom-Kovarik, Jeremiah T. and Sukthankar, Gita and Schatz, Sae},
title = {Tractable POMDP Representations for Intelligent Tutoring Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438664},
doi = {10.1145/2438653.2438664},
abstract = {With Partially Observable Markov Decision Processes (POMDPs), Intelligent Tutoring Systems (ITSs) can model individual learners from limited evidence and plan ahead despite uncertainty. However, POMDPs need appropriate representations to become tractable in ITSs that model many learner features, such as mastery of individual skills or the presence of specific misconceptions. This article describes two POMDP representations—state queues and observation chains—that take advantage of ITS task properties and let POMDPs scale to represent over 100 independent learner features. A real-world military training problem is given as one example. A human study (n = 14) provides initial validation for the model construction. Finally, evaluating the experimental representations with simulated students helps predict their impact on ITS performance. The compressed representations can model a wide range of simulated problems with instructional efficacy equal to lossless representations. With improved tractability, POMDP ITSs can accommodate more numerous or more detailed learner states and inputs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {29},
numpages = {22},
keywords = {intelligent tutoring systems, computer-based training, Partially observable Markov decision processes}
}

@article{10.1145/2438653.2438663,
author = {Li, Qing and Luo, Xiangfeng and Wenyin, Liu and Conati, Cristina},
title = {Introduction to the Special Section on Intelligent Tutoring and Coaching Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438663},
doi = {10.1145/2438653.2438663},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {28},
numpages = {2}
}

@article{10.1145/2438653.2438662,
author = {Falcone, Rino and Piunti, Michele and Venanzi, Matteo and Castelfranchi, Cristiano},
title = {From Manifesta to Krypta: The Relevance of Categories for Trusting Others},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438662},
doi = {10.1145/2438653.2438662},
abstract = {In this article we consider the special abilities needed by agents for assessing trust based on inference and reasoning. We analyze the case in which it is possible to infer trust towards unknown counterparts by reasoning on abstract classes or categories of agents shaped in a concrete application domain. We present a scenario of interacting agents providing a computational model implementing different strategies to assess trust. Assuming a medical domain, categories, including both competencies and dispositions of possible trustees, are exploited to infer trust towards possibly unknown counterparts. The proposed approach for the cognitive assessment of trust relies on agents' abilities to analyze heterogeneous information sources along different dimensions. Trust is inferred based on specific observable properties (manifesta), namely explicitly readable signals indicating internal features (krypta) regulating agents' behavior and effectiveness on specific tasks. Simulative experiments evaluate the performance of trusting agents adopting different strategies to delegate tasks to possibly unknown trustees, while experimental results show the relevance of this kind of cognitive ability in the case of open multiagent systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {27},
numpages = {24},
keywords = {open systems, Trust by reasoning, cognitive analysis, fuzzy cognitive maps}
}

@article{10.1145/2438653.2438661,
author = {Burnett, Chris and Norman, Timothy J. and Sycara, Katia},
title = {Stereotypical Trust and Bias in Dynamic Multiagent Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438661},
doi = {10.1145/2438653.2438661},
abstract = {Large-scale multiagent systems have the potential to be highly dynamic. Trust and reputation are crucial concepts in these environments, as it may be necessary for agents to rely on their peers to perform as expected, and learn to avoid untrustworthy partners. However, aspects of highly dynamic systems introduce issues which make the formation of trust relationships difficult. For example, they may be short-lived, precluding agents from gaining the necessary experiences to make an accurate trust evaluation. This article describes a new approach, inspired by theories of human organizational behavior, whereby agents generalize their experiences with previously encountered partners as stereotypes, based on the observable features of those partners and their behaviors. Subsequently, these stereotypes are applied when evaluating new and unknown partners. Furthermore, these stereotypical opinions can be communicated within the society, resulting in the notion of stereotypical reputation. We show how this approach can complement existing state-of-the-art trust models, and enhance the confidence in the evaluations that can be made about trustees when direct and reputational information is lacking or limited. Furthermore, we show how a stereotyping approach can help agents detect unwanted biases in the reputational opinions they receive from others in the society.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {26},
numpages = {22},
keywords = {Trust, multiagent systems, stereotypes}
}

@article{10.1145/2438653.2438660,
author = {Erriquez, Elisabetta and Hoek, Wiebe van der and Wooldridge, Michael},
title = {Building and Using Social Structures: A Case Study Using the Agent ART Testbed},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438660},
doi = {10.1145/2438653.2438660},
abstract = {This article investigates the conjecture that agents who make decisions in scenarios where trust is important can benefit from the use of a social structure, representing the social relationships that exist between agents. We propose techniques that can be used by agents to initially build and then progressively update such a structure in the light of experience. We describe an implementation of our techniques in the domain of the Agent ART testbed: we take two existing agents for this domain (“Simplet” and “Connected”) and compare their performance with versions that use our social structure (“SocialSimplet” and “SocialConnected”). We show that SocialSimplet and SocialConnected outperform their counterparts with respect to the quality of the interactions, the number of rounds won in a competition, and the total utility gained.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {25},
numpages = {20},
keywords = {trading competition, trust, Agents}
}

@article{10.1145/2438653.2438659,
author = {Zhang, Jie and Cohen, Robin},
title = {A Framework for Trust Modeling in Multiagent Electronic Marketplaces with Buying Advisors to Consider Varying Seller Behavior and the Limiting of Seller Bids},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438659},
doi = {10.1145/2438653.2438659},
abstract = {In this article, we present a framework of use in electronic marketplaces that allows buying agents to model the trustworthiness of selling agents in an effective way, making use of seller ratings provided by other buying agents known as advisors. The trustworthiness of the advisors is also modeled, using an approach that combines both personal and public knowledge and allows the relative weighting to be adjusted over time. Through a series of experiments that simulate e-marketplaces, including ones where sellers may vary their behavior over time, we are able to demonstrate that our proposed framework delivers effective seller recommendations to buyers, resulting in important buyer profit. We also propose limiting seller bids as a method for promoting seller honesty, thus facilitating successful selection of sellers by buyers, and demonstrate the value of this approach through experimental results. Overall, this research is focused on the technological aspects of electronic commerce and specifically on technology that would be used to manage trust.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {22},
keywords = {trust modeling, Multiagent systems, electronic commerce applications, social networks}
}

@article{10.1145/2438653.2438658,
author = {Falcone, Rino and Singh, Munindar P.},
title = {Introduction to Special Section on Trust in Multiagent Systems},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438658},
doi = {10.1145/2438653.2438658},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {23},
numpages = {2},
keywords = {Trust}
}

@article{10.1145/2438653.2438657,
author = {Baldoni, Matteo and Baroglio, Cristina and Marengo, Elisa and Patti, Viviana},
title = {Constitutive and Regulative Specifications of Commitment Protocols: A Decoupled Approach},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438657},
doi = {10.1145/2438653.2438657},
abstract = {Interaction protocols play a fundamental role in multiagent systems. In this work, after analyzing the trends that are emerging not only from research on multiagent interaction protocols but also from neighboring fields, like research on workflows and business processes, we propose a novel definition of commitment-based interaction protocols, that is characterized by the decoupling of the constitutive and the regulative specifications and that explicitly foresees a representation of the latter based on constraints among commitments. A clear distinction between the two representations has many advantages, mainly residing in a greater openness of multiagent systems, and an easier reuse of protocols and of action definitions. A language, named 2CL, for writing regulative specifications is also given together with a designer-oriented graphical notation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {22},
numpages = {25},
keywords = {commitments, Interaction protocols, constraints among commitments, constitutive and regulative specifications}
}

@article{10.1145/2438653.2438656,
author = {Gerard, Scott N. and Singh, Munindar P.},
title = {Formalizing and Verifying Protocol Refinements},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438656},
doi = {10.1145/2438653.2438656},
abstract = {A (business) protocol describes, in high-level terms, a pattern of communication between two or more participants, specifically via the creation and manipulation of the commitments between them. In this manner, a protocol offers both flexibility and rigor: a participant may communicate in any way it chooses as long as it discharges all of its activated commitments. Protocols thus promise benefits in engineering cross-organizational business processes. However, software engineering using protocols presupposes a formalization of protocols and a notion of the refinement of one protocol by another. Refinement for protocols is both intuitively obvious (e.g., PayViaCheck is clearly a kind of Pay) and technically nontrivial (e.g., compared to Pay, PayViaCheck involves different participants exchanging different messages). This article formalizes protocols and their refinement. It develops Proton, an analysis tool for protocol specifications that overlays a model checker to compute whether one protocol refines another with respect to a stated mapping. Proton and its underlying theory are evaluated by formalizing several protocols from the literature and verifying all and only the expected refinements.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {21},
numpages = {27},
keywords = {agent communication, Commitments, verification of multiagent systems}
}

@article{10.1145/2438653.2438655,
author = {Chopra, Amit K. and Artikis, Alexander and Bentahar, Jamal and Colombetti, Marco and Dignum, Frank and Fornara, Nicoletta and Jones, Andrew J. I. and Singh, Munindar P. and Yolum, Pinar},
title = {Research Directions in Agent Communication},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438655},
doi = {10.1145/2438653.2438655},
abstract = {Increasingly, software engineering involves open systems consisting of autonomous and heterogeneous participants or agents who carry out loosely coupled interactions. Accordingly, understanding and specifying communications among agents is a key concern. A focus on ways to formalize meaning distinguishes agent communication from traditional distributed computing: meaning provides a basis for flexible interactions and compliance checking.Over the years, a number of approaches have emerged with some essential and some irrelevant distinctions drawn among them. As agent abstractions gain increasing traction in the software engineering of open systems, it is important to resolve the irrelevant and highlight the essential distinctions, so that future research can be focused in the most productive directions.This article is an outcome of extensive discussions among agent communication researchers, aimed at taking stock of the field and at developing, criticizing, and refining their positions on specific approaches and future challenges. This article serves some important purposes, including identifying (1) points of broad consensus; (2) points where substantive differences remain; and (3) interesting directions of future work.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {20},
numpages = {23},
keywords = {Communication}
}

@article{10.1145/2438653.2438654,
author = {Chopra, Amit K. and Artikis, Alexander and Bentahar, Jamal and Dignum, Frank},
title = {Introduction to the Special Section on Agent Communication},
year = {2013},
issue_date = {March 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2438653.2438654},
doi = {10.1145/2438653.2438654},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {19},
numpages = {1}
}

@article{10.1145/2414425.2414443,
author = {Song, Xuan and Zhao, Huijing and Cui, Jinshi and Shao, Xiaowei and Shibasaki, Ryosuke and Zha, Hongbin},
title = {An Online System for Multiple Interacting Targets Tracking: Fusion of Laser and Vision, Tracking and Learning},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414443},
doi = {10.1145/2414425.2414443},
abstract = {Multitarget tracking becomes significantly more challenging when the targets are in close proximity or frequently interact with each other. This article presents a promising online system to deal with these problems. The novelty of this system is that laser and vision are integrated with tracking and online learning to complement each other in one framework: when the targets do not interact with each other, the laser-based independent trackers are employed and the visual information is extracted simultaneously to train some classifiers online for “possible interacting targets”. When the targets are in close proximity, the classifiers learned online are used alongside visual information to assist in tracking. Therefore, this mode of cooperation not only deals with various tough problems encountered in tracking, but also ensures that the entire process can be completely online and automatic. Experimental results demonstrate that laser and vision fully display their respective advantages in our system, and it is easy for us to obtain a good trade-off between tracking accuracy and the time-cost factor.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {18},
numpages = {21},
keywords = {sensor fusion, Multiple targets tracking}
}

@article{10.1145/2414425.2414442,
author = {Okada, Isamu and Yamamoto, Hitoshi},
title = {Mathematical Description and Analysis of Adaptive Risk Choice Behavior},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414442},
doi = {10.1145/2414425.2414442},
abstract = {Which risk should one choose when facing alternatives with different levels of risk? We discuss here adaptive processes in such risk choice behavior by generalizing the study of Roos et al. [2010]. We deal with an n-choice game in which every player sequentially chooses n times of lotteries of which there are two types: a safe lottery and a risky lottery. We analyze this model in more detail by elaborating the game. Based on the results of mathematical analysis, replicator dynamics analysis, and numerical simulations, we derived some salient features of risk choice behavior. We show that all the risk strategies can be divided into two groups: persistence and nonpersistence. We also proved that the dynamics with perturbation in which a mutation is installed is globally asymptotically stable to a unique equilibrium point for any initial population. The numerical simulations clarify that the number of persistent strategies seldom increases regardless of the increase in n, and suggest that a rarity of dominant choice strategies is widely observed in many social contexts. These facts not only go hand-in-hand with some well-known insights from prospect theory, but may also provide some theoretical hypotheses for various fields such as behavioral economics, ecology, sociology, and consumer behavioral theory.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {17},
numpages = {21},
keywords = {decision theory, sequentiality, Adaptive process, winner-takes-all, risk, risk choice strategy, replicator dynamics, mutation}
}

@article{10.1145/2414425.2414441,
author = {Shi, Yue and Larson, Martha and Hanjalic, Alan},
title = {Mining Contextual Movie Similarity with Matrix Factorization for Context-Aware Recommendation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414441},
doi = {10.1145/2414425.2414441},
abstract = {Context-aware recommendation seeks to improve recommendation performance by exploiting various information sources in addition to the conventional user-item matrix used by recommender systems. We propose a novel context-aware movie recommendation algorithm based on joint matrix factorization (JMF). We jointly factorize the user-item matrix containing general movie ratings and other contextual movie similarity matrices to integrate contextual information into the recommendation process. The algorithm was developed within the scope of the mood-aware recommendation task that was offered by the Moviepilot mood track of the 2010 context-aware movie recommendation (CAMRa) challenge. Although the algorithm could generalize to other types of contextual information, in this work, we focus on two: movie mood tags and movie plot keywords. Since the objective in this challenge track is to recommend movies for a user given a specified mood, we devise a novel mood-specific movie similarity measure for this purpose. We enhance the recommendation based on this measure by also deploying the second movie similarity measure proposed in this article that takes into account the movie plot keywords. We validate the effectiveness of the proposed JMF algorithm with respect to the recommendation performance by carrying out experiments on the Moviepilot challenge dataset. We demonstrate that exploiting contextual information in JMF leads to significant improvement over several state-of-the-art approaches that generate movie recommendations without using contextual information. We also demonstrate that our proposed mood-specific movie similarity is better suited for the task than the conventional mood-based movie similarity measures. Finally, we show that the enhancement provided by the movie similarity capturing the plot keywords is particularly helpful in improving the recommendation to those users who are significantly more active in rating the movies than other users.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {16},
numpages = {19},
keywords = {context-aware recommendation, recommender systems, matrix factorization, mood-specific movie similarity, Collaborative filtering}
}

@article{10.1145/2414425.2414440,
author = {Liu, Nathan N. and He, Luheng and Zhao, Min},
title = {Social Temporal Collaborative Ranking for Context Aware Movie Recommendation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414440},
doi = {10.1145/2414425.2414440},
abstract = {Most existing collaborative filtering models only consider the use of user feedback (e.g., ratings) and meta data (e.g., content, demographics). However, in most real world recommender systems, context information, such as time and social networks, are also very important factors that could be considered in order to produce more accurate recommendations. In this work, we address several challenges for the context aware movie recommendation tasks in CAMRa 2010: (1) how to combine multiple heterogeneous forms of user feedback? (2) how to cope with dynamic user and item characteristics? (3) how to capture and utilize social connections among users? For the first challenge, we propose a novel ranking based matrix factorization model to aggregate explicit and implicit user feedback. For the second challenge, we extend this model to a sequential matrix factorization model to enable time-aware parametrization. Finally, we introduce a network regularization function to constrain user parameters based on social connections. To the best of our knowledge, this is the first study that investigates the collective modeling of social and temporal dynamics. Experiments on the CAMRa 2010 dataset demonstrated clear improvements over many baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {15},
numpages = {26},
keywords = {recommender systems, context awareness, user feedback, Collaborative filtering}
}

@article{10.1145/2414425.2414439,
author = {Bellog\'{\i}n, Alejandro and Cantador, Iv\'{a}n and D\'{\i}ez, Fernando and Castells, Pablo and Chavarriaga, Enrique},
title = {An Empirical Comparison of Social, Collaborative Filtering, and Hybrid Recommenders},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414439},
doi = {10.1145/2414425.2414439},
abstract = {In the Social Web, a number of diverse recommendation approaches have been proposed to exploit the user generated contents available in the Web, such as rating, tagging, and social networking information. In general, these approaches naturally require the availability of a wide amount of these user preferences. This may represent an important limitation for real applications, and may be somewhat unnoticed in studies focusing on overall precision, in which a failure to produce recommendations gets blurred when averaging the obtained results or, even worse, is just not accounted for, as users with no recommendations are typically excluded from the performance calculations. In this article, we propose a coverage metric that uncovers and compensates for the incompleteness of performance evaluations based only on precision. We use this metric together with precision metrics in an empirical comparison of several social, collaborative filtering, and hybrid recommenders. The obtained results show that a better balance between precision and coverage can be achieved by combining social-based filtering (high accuracy, low coverage) and collaborative filtering (low accuracy, high coverage) recommendation techniques. We thus explore several hybrid recommendation approaches to balance this trade-off. In particular, we compare, on the one hand, techniques integrating collaborative and social information into a single model, and on the other, linear combinations of recommenders. For the last approach, we also propose a novel strategy to dynamically adjust the weight of each recommender on a user-basis, utilizing graph measures as indicators of the target user's connectedness and relevance in a social network.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {14},
numpages = {29},
keywords = {collaborative filtering, user coverage, social networks, Recommender Systems, random walk, hybrid recommenders, graph theory}
}

@article{10.1145/2414425.2414438,
author = {Said, Alan and Berkovsky, Shlomo and De Luca, Ernesto W.},
title = {Introduction to Special Section on CAMRa2010: Movie Recommendation in Context},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414438},
doi = {10.1145/2414425.2414438},
abstract = {The challenge and workshop on Context-Aware Movie Recommendation (CAMRa2010) were conducted jointly in 2010 with the Recommender Systems conference. The challenge focused on three context-aware recommendation scenarios: time-based, mood-based, and social recommendation. The participants were provided with anonymized datasets from two real-world online movie recommendation communities and competed against each other for obtaining the highest accuracy of recommendations. The datasets contained contextual features, such as tags, annotation, social relationsips, and comments, normally not available in public recommendation datasets. More than 40 teams from 21 countries participated in the challenge. Their participation was summarized by 10 papers published by the workshop, which have been extended and revised for this special section. In this preface we overview the challenge datasets, tasks, evaluation metrics, and the obtained outcomes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {13},
numpages = {9},
keywords = {social network analysis, context-aware recommendations, Recommender systems, datasets, context-awareness, user modeling}
}

@article{10.1145/2414425.2414437,
author = {Chen, Yu-Chih and Lin, Yu-Shi and Shen, Yu-Chun and Lin, Shou-De},
title = {A Modified Random Walk Framework for Handling Negative Ratings and Generating Explanations},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414437},
doi = {10.1145/2414425.2414437},
abstract = {The concept of random walk (RW) has been widely applied in the design of recommendation systems. RW-based approaches are effective in handling locality problem and taking extra information, such as the relationships between items or users, into consideration. However, the traditional RW-based approach has a serious limitation in handling bidirectional opinions. The propagation of positive and negative information simultaneously in a graph is nontrivial using random walk. To address the problem, this article presents a novel and efficient RW-based model that can handle both positive and negative comments with the guarantee of convergence. Furthermore, we argue that a good recommendation system should provide users not only a list of recommended items but also reasonable explanations for the decisions. Therefore, we propose a technique that generates explanations by backtracking the influential paths and subgraphs. The results of experiments on the MovieLens and Netflix datasets show that our model significantly outperforms state-of-the-art RW-based algorithms, and is capable of improving the overall performance in the ensemble with other models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {12},
numpages = {21},
keywords = {Ranking, Explanation, Integration, Random Walk, Collaborative Filtering}
}

@article{10.1145/2414425.2414436,
author = {Gedikli, Fatih and Jannach, Dietmar},
title = {Improving Recommendation Accuracy Based on Item-Specific Tag Preferences},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414436},
doi = {10.1145/2414425.2414436},
abstract = {In recent years, different proposals have been made to exploit Social Web tagging information to build more effective recommender systems. The tagging data, for example, were used to identify similar users or were viewed as additional information about the recommendable items. Recent research has indicated that “attaching feelings to tags” is experienced by users as a valuable means to express which features of an item they particularly like or dislike. When following such an approach, users would therefore not only add tags to an item as in usual Web 2.0 applications, but also attach a preference (affect) to the tag itself, expressing, for example, whether or not they liked a certain actor in a given movie. In this work, we show how this additional preference data can be exploited by a recommender system to make more accurate predictions.In contrast to previous work, which also relied on so-called tag preferences to enhance the predictive accuracy of recommender systems, we argue that tag preferences should be considered in the context of an item. We therefore propose new schemes to infer and exploit context-specific tag preferences in the recommendation process. An evaluation on two different datasets reveals that our approach is capable of providing more accurate recommendations than previous tag-based recommender algorithms and recent tag-agnostic matrix factorization techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {11},
numpages = {19},
keywords = {Recommender systems, algorithms, tagging, social web}
}

@article{10.1145/2414425.2414435,
author = {Biancalana, Claudio and Gasparetti, Fabio and Micarelli, Alessandro and Sansonetti, Giuseppe},
title = {An Approach to Social Recommendation for Context-Aware Mobile Services},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414435},
doi = {10.1145/2414425.2414435},
abstract = {Nowadays, several location-based services (LBSs) allow their users to take advantage of information from the Web about points of interest (POIs) such as cultural events or restaurants. To the best of our knowledge, however, none of these provides information taking into account user preferences, or other elements, in addition to location, that contribute to define the context of use. The provided suggestions do not consider, for example, time, day of week, weather, user activity or means of transport. This article describes a social recommender system able to identify user preferences and information needs, thus suggesting personalized recommendations related to POIs in the surroundings of the user's current location. The proposed approach achieves the following goals: (i) to supply, unlike the current LBSs, a methodology for identifying user preferences and needs to be used in the information filtering process; (ii) to exploit the ever-growing amount of information from social networking, user reviews, and local search Web sites; (iii) to establish procedures for defining the context of use to be employed in the recommendation of POIs with low effort. The flexibility of the architecture is such that our approach can be easily extended to any category of POI. Experimental tests carried out on real users enabled us to quantify the benefits of the proposed approach in terms of performance improvement.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {10},
numpages = {31},
keywords = {ubiquitous computing, user modeling, Social recommender system}
}

@article{10.1145/2414425.2414434,
author = {Zhang, Weishi and Ding, Guiguang and Chen, Li and Li, Chunping and Zhang, Chengbo},
title = {Generating Virtual Ratings from Chinese Reviews to Augment Online Recommendations},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414434},
doi = {10.1145/2414425.2414434},
abstract = {Collaborative filtering (CF) recommenders based on User-Item rating matrix as explicitly obtained from end users have recently appeared promising in recommender systems. However, User-Item rating matrix is not always available or very sparse in some web applications, which has critical impact to the application of CF recommenders. In this article we aim to enhance the online recommender system by fusing virtual ratings as derived from user reviews. Specifically, taking into account of Chinese reviews' characteristics, we propose to fuse the self-supervised emotion-integrated sentiment classification results into CF recommenders, by which the User-Item Rating Matrix can be inferred by decomposing item reviews that users gave to the items. The main advantage of this approach is that it can extend CF recommenders to some web applications without user rating information. In the experiments, we have first identified the self-supervised sentiment classification's higher precision and recall by comparing it with traditional classification methods. Furthermore, the classification results, as behaving as virtual ratings, were incorporated into both user-based and item-based CF algorithms. We have also conducted an experiment to evaluate the proximity between the virtual and real ratings and clarified the effectiveness of the virtual ratings. The experimental results demonstrated the significant impact of virtual ratings on increasing system's recommendation accuracy in different data conditions (i.e., conditions with real ratings and without).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {9},
numpages = {17},
keywords = {Information retrieval, sentiment analysis, online recommendation}
}

@article{10.1145/2414425.2414433,
author = {Quijano-Sanchez, Lara and Recio-Garcia, Juan A. and Diaz-Agudo, Belen and Jimenez-Diaz, Guillermo},
title = {Social Factors in Group Recommender Systems},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414433},
doi = {10.1145/2414425.2414433},
abstract = {In this article we review the existing techniques in group recommender systems and we propose some improvement based on the study of the different individual behaviors when carrying out a decision-making process. Our method includes an analysis of group personality composition and trust between each group member to improve the accuracy of group recommenders. This way we simulate the argumentation process followed by groups of people when agreeing on a common activity in a more realistic way. Moreover, we reflect how they expect the system to behave in a long term recommendation process. This is achieved by including a memory of past recommendations that increases the satisfaction of users whose preferences have not been taken into account in previous recommendations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {8},
numpages = {30},
keywords = {personality, trust, Memory, recommender systems, social networks}
}

@article{10.1145/2414425.2414432,
author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
title = {Introduction to the Special Section on Social Recommender Systems},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414432},
doi = {10.1145/2414425.2414432},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {7},
numpages = {2}
}

@article{10.1145/2414425.2414431,
author = {Shen, Keyi and Wu, Jianmin and Zhang, Ya and Han, Yiping and Yang, Xiaokang and Song, Li and Gu, Xiao},
title = {Reorder User's Tweets},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414431},
doi = {10.1145/2414425.2414431},
abstract = {Twitter displays the tweets a user received in a reversed chronological order, which is not always the best choice. As Twitter is full of messages of very different qualities, many informative or relevant tweets might be flooded or displayed at the bottom while some nonsense buzzes might be ranked higher. In this work, we present a supervised learning method for personalized tweets reordering based on user interests. User activities on Twitter, in terms of tweeting, retweeting, and replying, are leveraged to obtain the training data for reordering models. Through exploring a rich set of social and personalized features, we model the relevance of tweets by minimizing the pairwise loss of relevant and irrelevant tweets. The tweets are then reordered according to the predicted relevance scores. Experimental results with real twitter user activities demonstrated the effectiveness of our method. The new method achieved above 30% accuracy gain compared with the default ordering in twitter based on time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {6},
numpages = {17},
keywords = {twitter, Personalization, retweet, reorder}
}

@article{10.1145/2414425.2414430,
author = {Han, Bo and Cook, Paul and Baldwin, Timothy},
title = {Lexical Normalization for Social Media Text},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414430},
doi = {10.1145/2414425.2414430},
abstract = {Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this article, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalizing lexical variants. Our method uses a classifier to detect lexical variants, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn't require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {5},
numpages = {27},
keywords = {short text message, microblog, text preprocessing, lexical normalization, Text analysis}
}

@article{10.1145/2414425.2414429,
author = {Chang, Yi and Dong, Anlei and Kolari, Pranam and Zhang, Ruiqiang and Inagaki, Yoshiyuki and Diaz, Fernanodo and Zha, Hongyuan and Liu, Yan},
title = {Improving Recency Ranking Using Twitter Data},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414429},
doi = {10.1145/2414425.2414429},
abstract = {In Web search and vertical search, recency ranking refers to retrieving and ranking documents by both relevance and freshness. As impoverished in-links and click information is the the biggest challenge for recency ranking, we advocate the use of Twitter data to address the challenge in this article. We propose a method to utilize Twitter TinyURL to detect fresh and high-quality documents, and leverage Twitter data to generate novel and effective features for ranking. The empirical experiments demonstrate that the proposed approach effectively improves a commercial search engine for both Web search ranking and tweet vertical ranking.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {4},
numpages = {24},
keywords = {tweet ranking, Twitter, Recency ranking}
}

@article{10.1145/2414425.2414428,
author = {Liu, Xiaohua and Wei, Furu and Zhang, Shaodian and Zhou, Ming},
title = {Named Entity Recognition for Tweets},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414428},
doi = {10.1145/2414425.2414428},
abstract = {Two main challenges of Named Entity Recognition (NER) for tweets are the insufficient information in a tweet and the lack of training data. We propose a novel method consisting of three core elements: (1) normalization of tweets; (2) combination of a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model; and (3) semisupervised learning framework. The tweet normalization preprocessing corrects common ill-formed words using a global linear model. The KNN-based classifier conducts prelabeling to collect global coarse evidence across tweets while the CRF model conducts sequential labeling to capture fine-grained information encoded in a tweet. The semisupervised learning plus the gazetteers alleviate the lack of training data. Extensive experiments show the advantages of our method over the baselines as well as the effectiveness of normalization, KNN, and semisupervised learning.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {3},
numpages = {15},
keywords = {Semisupervised learning, model combination, tweet normalization}
}

@article{10.1145/2414425.2414427,
author = {Cheng, Zhiyuan and Caverlee, James and Lee, Kyumin},
title = {A Content-Driven Framework for Geolocating Microblog Users},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414427},
doi = {10.1145/2414425.2414427},
abstract = {Highly dynamic real-time microblog systems have already published petabytes of real-time human sensor data in the form of status updates. However, the lack of user adoption of geo-based features per user or per post signals that the promise of microblog services as location-based sensing systems may have only limited reach and impact. Thus, in this article, we propose and evaluate a probabilistic framework for estimating a microblog user's location based purely on the content of the user's posts. Our framework can overcome the sparsity of geo-enabled features in these services and bring augmented scope and breadth to emerging location-based personalized information services. Three of the key features of the proposed approach are: (i) its reliance purely on publicly available content; (ii) a classification component for automatically identifying words in posts with a strong local geo-scope; and (iii) a lattice-based neighborhood smoothing model for refining a user's location estimate. On average we find that the location estimates converge quickly, placing 51% of users within 100 miles of their actual location.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {2},
numpages = {27},
keywords = {Twitter, location-based estimation, spatial data mining, text mining, Microblog}
}

@article{10.1145/2414425.2414426,
author = {King, Irwin and Nejdl, Wolfgang},
title = {Introduction to the Special Section on Twitter and Microblogging Services},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2414425.2414426},
doi = {10.1145/2414425.2414426},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {1},
numpages = {2}
}

@article{10.1145/2337542.2337562,
author = {Mandrake, Lukas and Rebbapragada, Umaa and Wagstaff, Kiri L. and Thompson, David and Chien, Steve and Tran, Daniel and Pappalardo, Robert T. and Gleeson, Damhnait and Casta\~{n}o, Rebecca},
title = {Surface Sulfur Detection via Remote Sensing and Onboard Classification},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337562},
doi = {10.1145/2337542.2337562},
abstract = {Orbital remote sensing provides a powerful way to efficiently survey targets such as the Earth and other planets and moons for features of interest. One such feature of astrobiological relevance is the presence of surface sulfur deposits. These deposits have been observed to be associated with microbial activity at the Borup Fiord glacial springs in Canada, a location that may provide an analogue to other icy environments such as Europa.This article evaluates automated classifiers for detecting sulfur in remote sensing observations by the hyperion spectrometer on the EO-1 spacecraft. We determined that a data-driven machine learning solution was needed because the sulfur could not be detected by simply matching observations to sulfur lab spectra. We also evaluated several methods (manual and automated) for identifying the most relevant attributes (spectral wavelengths) needed for successful sulfur detection. Our findings include (1) the Borup Fiord sulfur deposits were best modeled as containing two sub-populations: sulfur on ice and sulfur on rock; (2) as expected, classifiers using Gaussian kernels outperformed those based on linear kernels, and should be adopted when onboard computational constraints permit; and (3) Recursive Feature Elimination selected sensible and effective features for use in the computationally constrained environment onboard EO-1. This study helped guide the selection of algorithm parameters and configuration for the classification system currently operational on EO-1. Finally, we discuss implications for a similar onboard classification system for a future Europa orbiter.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {77},
numpages = {20},
keywords = {support vector machines, feature selection, Remote sensing}
}

@article{10.1145/2337542.2337561,
author = {Wang, Zhenxing and Chan, Laiwan},
title = {Learning Causal Relations in Multivariate Time Series Data},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337561},
doi = {10.1145/2337542.2337561},
abstract = {Many applications naturally involve time series data and the vector autoregression (VAR), and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series. In the first part of this work, we show that the SVAR method is incapable of identifying contemporaneous causal relations for Gaussian process. In addition, least squares estimators become unreliable when the scales of the problems are large and observations are limited. In the remaining part, we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations, and avoid high-order statistical tests. The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large, and high-order statistical tests are required by Bayesian network learning algorithms in this case. To overcome the difficulty, we show that the search space of conditioning sets d-separating two vertices should be a subset of the Markov blankets. Based on this fact, we propose an algorithm enabling us to learn Bayesian networks locally, and make the largest order of statistical tests independent of the scales of the problems. Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {76},
numpages = {28},
keywords = {Bayesian networks, graphical models, causal modeling, VAR}
}

@article{10.1145/2337542.2337560,
author = {Zhang, Xiaoqin Shelley and Shrestha, Bhavesh and Yoon, Sungwook and Kambhampati, Subbarao and DiBona, Phillip and Guo, Jinhong K. and McFarlane, Daniel and Hofmann, Martin O. and Whitebread, Kenneth and Appling, Darren Scott and Whitaker, Elizabeth T. and Trewhitt, Ethan B. and Ding, Li and Michaelis, James R. and McGuinness, Deborah L. and Hendler, James A. and Doppa, Janardhan Rao and Parker, Charles and Dietterich, Thomas G. and Tadepalli, Prasad and Wong, Weng-Keen and Green, Derek and Rebguns, Anton and Spears, Diana and Kuter, Ugur and Levine, Geoff and DeJong, Gerald and MacTavish, Reid L. and Onta\~{n}\'{o}n, Santiago and Radhakrishnan, Jainarayan and Ram, Ashwin and Mostafa, Hala and Zafar, Huzaifa and Zhang, Chongjie and Corkill, Daniel and Lesser, Victor and Song, Zhexuan},
title = {An Ensemble Architecture for Learning Complex Problem-Solving Techniques from Demonstration},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337560},
doi = {10.1145/2337542.2337560},
abstract = {We present a novel ensemble architecture for learning problem-solving techniques from a very small number of expert solutions and demonstrate its effectiveness in a complex real-world domain. The key feature of our “Generalized Integrated Learning Architecture” (GILA) is a set of heterogeneous independent learning and reasoning (ILR) components, coordinated by a central meta-reasoning executive (MRE). The ILRs are weakly coupled in the sense that all coordination during learning and performance happens through the MRE. Each ILR learns independently from a small number of expert demonstrations of a complex task. During performance, each ILR proposes partial solutions to subproblems posed by the MRE, which are then selected from and pieced together by the MRE to produce a complete solution. The heterogeneity of the learner-reasoners allows both learning and problem solving to be more effective because their abilities and biases are complementary and synergistic. We describe the application of this novel learning and problem solving architecture to the domain of airspace management, where multiple requests for the use of airspaces need to be deconflicted, reconciled, and managed automatically. Formal evaluations show that our system performs as well as or better than humans after learning from the same training data. Furthermore, GILA outperforms any individual ILR run in isolation, thus demonstrating the power of the ensemble architecture for learning and problem solving.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {75},
numpages = {38},
keywords = {Ensemble architecture, learning from demonstration, complex problem-solving}
}

@article{10.1145/2337542.2337559,
author = {Strohmaier, Markus and Helic, Denis and Benz, Dominik and K\"{o}rner, Christian and Kern, Roman},
title = {Evaluation of Folksonomy Induction Algorithms},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337559},
doi = {10.1145/2337542.2337559},
abstract = {Algorithms for constructing hierarchical structures from user-generated metadata have caught the interest of the academic community in recent years. In social tagging systems, the output of these algorithms is usually referred to as folksonomies (from folk-generated taxonomies). Evaluation of folksonomies and folksonomy induction algorithms is a challenging issue complicated by the lack of golden standards, lack of comprehensive methods and tools as well as a lack of research and empirical/simulation studies applying these methods. In this article, we report results from a broad comparative study of state-of-the-art folksonomy induction algorithms that we have applied and evaluated in the context of five social tagging systems. In addition to adopting semantic evaluation techniques, we present and adopt a new technique that can be used to evaluate the usefulness of folksonomies for navigation. Our work sheds new light on the properties and characteristics of state-of-the-art folksonomy induction algorithms and introduces a new pragmatic approach to folksonomy evaluation, while at the same time identifying some important limitations and challenges of folksonomy evaluation. Our results show that folksonomy induction algorithms specifically developed to capture intuitions of social tagging systems outperform traditional hierarchical clustering techniques. To the best of our knowledge, this work represents the largest and most comprehensive evaluation study of state-of-the-art folksonomy induction algorithms to date.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {74},
numpages = {22},
keywords = {taxonomies, Folksonomies, social tagging systems, evaluation}
}

@article{10.1145/2337542.2337558,
author = {Tang, Xuning and Yang, Christopher C.},
title = {Ranking User Influence in Healthcare Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337558},
doi = {10.1145/2337542.2337558},
abstract = {Due to the revolutionary development of Web 2.0 technology, individual users have become major contributors of Web content in online social media. In light of the growing activities, how to measure a user’s influence to other users in online social media becomes increasingly important. This research need is urgent especially in the online healthcare community since positive influence can be beneficial while negative influence may cause-negative impact on other users of the same community. In this article, a research framework was proposed to study user influence within the online healthcare community. We proposed a new approach to incorporate users’ reply relationship, conversation content and response immediacy which capture both explicit and implicit interaction between users to identify influential users of online healthcare community. A weighted social network is developed to represent the influence between users. We tested our proposed techniques thoroughly on two medical support forums. Two algorithms UserRank and Weighted in-degree are benchmarked with PageRank and in-degree. Experiment results demonstrated the validity and effectiveness of our proposed approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {73},
numpages = {21},
keywords = {social media analytics, User influence, ranking algorithm, social network, social computing, Web mining, online healthcare community}
}

@article{10.1145/2337542.2337557,
author = {Lampos, Vasileios and Cristianini, Nello},
title = {Nowcasting Events from the Social Web with Statistical Learning},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337557},
doi = {10.1145/2337542.2337557},
abstract = {We present a general methodology for inferring the occurrence and magnitude of an event or phenomenon by exploring the rich amount of unstructured textual information on the social part of the Web. Having geo-tagged user posts on the microblogging service of Twitter as our input data, we investigate two case studies. The first consists of a benchmark problem, where actual levels of rainfall in a given location and time are inferred from the content of tweets. The second one is a real-life task, where we infer regional Influenza-like Illness rates in the effort of detecting timely an emerging epidemic disease. Our analysis builds on a statistical learning framework, which performs sparse learning via the bootstrapped version of LASSO to select a consistent subset of textual features from a large amount of candidates. In both case studies, selected features indicate close semantic correlation with the target topics and inference, conducted by regression, has a significant performance, especially given the short length --approximately one year-- of Twitter’s data time series.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {72},
numpages = {22},
keywords = {Event detection, LASSO, social network mining, feature selection, sparse learning, Twitter}
}

@article{10.1145/2337542.2337556,
author = {Wang, Haofen and Fu, Linyun and Jin, Wei and Yu, Yong},
title = {EachWiki: Facilitating Wiki Authoring by Annotation Suggestion},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337556},
doi = {10.1145/2337542.2337556},
abstract = {Wikipedia, one of the best-known wikis and the world’s largest free online encyclopedia, has embraced the power of collaborative editing to harness collective intelligence. However, using such a wiki to create high-quality articles is not as easy as people imagine, given for instance the difficulty of reusing knowledge already available in Wikipedia. As a result, the heavy burden of upbuilding and maintaining the ever-growing online encyclopedia still rests on a small group of people. In this article, we aim at facilitating wiki authoring by providing annotation recommendations, thus lightening the burden of both contributors and administrators. We leverage the collective wisdom of the users by exploiting Semantic Web technologies with Wikipedia data and adopt a unified algorithm to support link, category, and semantic relation recommendation. A prototype system named EachWiki is proposed and evaluated. The experimental results show that it has achieved considerable improvements in terms of effectiveness, efficiency and usability. The proposed approach can also be applied to other wiki-based collaborative editing systems.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {71},
numpages = {18},
keywords = {semantic relation suggestion, Link suggestion, category suggestion}
}

@article{10.1145/2337542.2337555,
author = {Li, Xiaonan and Li, Chengkai and Yu, Cong},
title = {Entity-Relationship Queries over Wikipedia},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337555},
doi = {10.1145/2337542.2337555},
abstract = {Wikipedia is the largest user-generated knowledge base. We propose a structured query mechanism, entity-relationship query, for searching entities in the Wikipedia corpus by their properties and interrelationships. An entity-relationship query consists of multiple predicates on desired entities. The semantics of each predicate is specified with keywords. Entity-relationship query searches entities directly over text instead of preextracted structured data stores. This characteristic brings two benefits: (1) Query semantics can be intuitively expressed by keywords; (2) It only requires rudimentary entity annotation, which is simpler than explicitly extracting and reasoning about complex semantic information before query-time. We present a ranking framework for general entity-relationship queries and a position-based Bounded Cumulative Model (BCM) for accurate ranking of query answers. We also explore various weighting schemes for further improving the accuracy of BCM. We test our ideas on a 2008 version of Wikipedia using a collection of 45 queries pooled from INEX entity ranking track and our own crafted queries. Experiments show that the ranking and weighting schemes are both effective, particularly on multipredicate queries.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {70},
numpages = {20},
keywords = {Entity search and ranking, Wikipedia, structured entity query}
}

@article{10.1145/2337542.2337554,
author = {Carmel, David and Roitman, Haggai and Yom-Tov, Elad},
title = {On the Relationship between Novelty and Popularity of User-Generated Content},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337554},
doi = {10.1145/2337542.2337554},
abstract = {This work deals with the task of predicting the popularity of user-generated content. We demonstrate how the novelty of newly published content plays an important role in affecting its popularity. More specifically, we study three dimensions of novelty. The first one, termed contemporaneous novelty, models the relative novelty embedded in a new post with respect to contemporary content that was generated by others. The second type of novelty, termed self novelty, models the relative novelty with respect to the user’s own contribution history. The third type of novelty, termed discussion novelty, relates to the novelty of the comments associated by readers with respect to the post content. We demonstrate the contribution of the new novelty measures to estimating blog-post popularity by predicting the number of comments expected for a fresh post. We further demonstrate how novelty based measures can be utilized for predicting the citation volume of academic papers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {69},
numpages = {19},
keywords = {Popularity, novelty, user-generated content}
}

@article{10.1145/2337542.2337553,
author = {Potthast, Martin and Stein, Benno and Loose, Fabian and Becker, Steffen},
title = {Information Retrieval in the Commentsphere},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337553},
doi = {10.1145/2337542.2337553},
abstract = {This article studies information retrieval tasks related to Web comments. Prerequisite of such a study and a main contribution of the article is a unifying survey of the research field. We identify the most important retrieval tasks related to comments, namely filtering, ranking, and summarization. Within these tasks, we distinguish two paradigms according to which comments are utilized and which we designate as comment-targeting and comment-exploiting. Within the first paradigm, the comments themselves form the retrieval targets. Within the second paradigm, the commented items form the retrieval targets (i.e., comments are used as an additional information source to improve the retrieval performance for the commented items). We report on four case studies to demonstrate the exploration of the commentsphere under information retrieval aspects: comment filtering, comment ranking, comment summarization and cross-media retrieval. The first three studies deal primarily with comment-targeting retrieval, while the last one deals with comment-exploiting retrieval. Throughout the article, connections to information retrieval research are pointed out.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {68},
numpages = {21},
keywords = {Web comments, commentsphere, comment-based retrieval, survey}
}

@article{10.1145/2337542.2337552,
author = {Trivedi, Anusua and Rai, Piyush and Daum\'{e}, Hal and Duvall, Scott L.},
title = {Leveraging Social Bookmarks from Partially Tagged Corpus for Improved Web Page Clustering},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337552},
doi = {10.1145/2337542.2337552},
abstract = {Automatic clustering of Web pages helps a number of information retrieval tasks, such as improving user interfaces, collection clustering, introducing diversity in search results, etc. Typically, Web page clustering algorithms use only features extracted from the page-text. However, the advent of social-bookmarking Web sites, such as StumbleUpon.com and Delicious.com, has led to a huge amount of user-generated content such as the social tag information that is associated with the Web pages. In this article, we present a subspace based feature extraction approach that leverages the social tag information to complement the page-contents of a Web page for extracting beter features, with the goal of improved clustering performance. In our approach, we consider page-text and tags as two separate views of the data, and learn a shared subspace that maximizes the correlation between the two views. Any clustering algorithm can then be applied in this subspace. We then present an extension that allows our approach to be applicable even if the Web page corpus is only partially tagged, that is, when the social tags are present for not all, but only for a small number of Web pages. We compare our subspace based approach with a number of baselines that use tag information in various other ways, and show that the subspace based approach leads to improved performance on the Web page clustering task. We also discuss some possible future work including an active learning extension that can help in choosing which Web pages to get tags for, if we only can get the social tags for only a small number of Web pages.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {67},
numpages = {18},
keywords = {information retrieval, social bookmarking, Web page clustering, kernel methods, canonical correlation analysis}
}

@article{10.1145/2337542.2337551,
author = {Paltoglou, Georgios and Thelwall, Mike},
title = {Twitter, MySpace, Digg: Unsupervised Sentiment Analysis in Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337551},
doi = {10.1145/2337542.2337551},
abstract = {Sentiment analysis is a growing area of research with significant applications in both industry and academia. Most of the proposed solutions are centered around supervised, machine learning approaches and review-oriented datasets. In this article, we focus on the more common informal textual communication on the Web, such as online discussions, tweets and social network comments and propose an intuitive, less domain-specific, unsupervised, lexicon-based approach that estimates the level of emotional intensity contained in text in order to make a prediction. Our approach can be applied to, and is tested in, two different but complementary contexts: subjectivity detection and polarity classification. Extensive experiments were carried on three real-world datasets, extracted from online social Web sites and annotated by human evaluators, against state-of-the-art supervised approaches. The results demonstrate that the proposed algorithm, even though unsupervised, outperforms machine learning solutions in the majority of cases, overall presenting a very robust and reliable solution for sentiment analysis of informal communication on the Web.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {66},
numpages = {19},
keywords = {social media, Opinion mining, sentiment analysis}
}

@article{10.1145/2337542.2337550,
author = {Cortizo, Jos\'{e} Carlos and Carrero, Francisco and Cantador, Iv\'{a}n and Troyano, Jos\'{e} Antonio and Rosso, Paolo},
title = {Introduction to the Special Section on Search and Mining User-Generated Content},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337550},
doi = {10.1145/2337542.2337550},
abstract = {The primary goal of this special section of ACM Transactions on Intelligent Systems and Technology is to foster research in the interplay between Social Media, Data/Opinion Mining and Search, aiming to reflect the actual developments in technologies that exploit user-generated content.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {65},
numpages = {3},
keywords = {information retrieval, data mining, user-generated contents, text mining, social media, opinion mining, Search}
}

@article{10.1145/2337542.2337549,
author = {Sizov, Sergej},
title = {Latent Geospatial Semantics of Social Media},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337549},
doi = {10.1145/2337542.2337549},
abstract = {Multimodal understanding of shared content is an important success factor for many Web 2.0 applications and platforms. This article addresses the fundamental question of geo-spatial awareness in social media applications. In this context, we introduce an approach for improved characterization of social media by combining text features (e.g., tags as a prominent example of short, unstructured text labels) with spatial knowledge (e.g., geotags, coordinates of images, and videos). Our model-based framework GeoFolk combines these two aspects in order to construct better algorithms for content management, retrieval, and sharing. We demonstrate in systematic studies the benefits of this combination for a broad spectrum of scenarios related to social media: recommender systems, automatic content organization and filtering, and event detection. Furthermore, we establish a simple and technically sound model that can be seen as a reference baseline for future research in the field of geotagged social media.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {64},
numpages = {20},
keywords = {tagging, geotagging, social media, Web2.0}
}

@article{10.1145/2337542.2337548,
author = {Yin, Zhijun and Cao, Liangliang and Gu, Quanquan and Han, Jiawei},
title = {Latent Community Topic Analysis: Integration of Community Discovery with Topic Modeling},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337548},
doi = {10.1145/2337542.2337548},
abstract = {This article studies the problem of latent community topic analysis in text-associated graphs. With the development of social media, a lot of user-generated content is available with user networks. Along with rich information in networks, user graphs can be extended with text information associated with nodes. Topic modeling is a classic problem in text mining and it is interesting to discover the latent topics in text-associated graphs. Different from traditional topic modeling methods considering links, we incorporate community discovery into topic analysis in text-associated graphs to guarantee the topical coherence in the communities so that users in the same community are closely linked to each other and share common latent topics. We handle topic modeling and community discovery in the same framework. In our model we separate the concepts of community and topic, so one community can correspond to multiple topics and multiple communities can share the same topic. We compare different methods and perform extensive experiments on two real datasets. The results confirm our hypothesis that topics could help understand community structure, while community structure could help model topics.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {63},
numpages = {21},
keywords = {community discovery, Topic modeling}
}

@article{10.1145/2337542.2337547,
author = {Lerman, Kristina and Hogg, Tad},
title = {Using Stochastic Models to Describe and Predict Social Dynamics of Web Users},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337547},
doi = {10.1145/2337542.2337547},
abstract = {The popularity of content in social media is unequally distributed, with some items receiving a disproportionate share of attention from users. Predicting which newly-submitted items will become popular is critically important for both the hosts of social media content and its consumers. Accurate and timely prediction would enable hosts to maximize revenue through differential pricing for access to content or ad placement. Prediction would also give consumers an important tool for filtering the content. Predicting the popularity of content in social media is challenging due to the complex interactions between content quality and how the social media site highlights its content. Moreover, most social media sites selectively present content that has been highly rated by similar users, whose similarity is indicated implicitly by their behavior or explicitly by links in a social network. While these factors make it difficult to predict popularity a priori, stochastic models of user behavior on these sites can allow predicting popularity based on early user reactions to new content. By incorporating the various mechanisms through which web sites display content, such models improve on predictions that are based on simply extrapolating from the early votes. Specifically, for one such site, the news aggregator Digg, we show how a stochastic model distinguishes the effect of the increased visibility due to the network from how interested users are in the content. We find a wide range of interest, distinguishing stories primarily of interest to users in the network (“niche interests”) from those of more general interest to the user community. This distinction is useful for predicting a story’s eventual popularity from users’ early reactions to the story.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {62},
numpages = {33},
keywords = {social dynamics, social networks, social news, Social media, modeling, prediction}
}

@article{10.1145/2337542.2337546,
author = {Wang, Guan and Xie, Sihong and Liu, Bing and Yu, Philip S.},
title = {Identify Online Store Review Spammers via Social Review Graph},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337546},
doi = {10.1145/2337542.2337546},
abstract = {Online shopping reviews provide valuable information for customers to compare the quality of products, store services, and many other aspects of future purchases. However, spammers are joining this community trying to mislead consumers by writing fake or unfair reviews to confuse the consumers. Previous attempts have used reviewers’ behaviors such as text similarity and rating patterns, to detect spammers. These studies are able to identify certain types of spammers, for instance, those who post many similar reviews about one target. However, in reality, there are other kinds of spammers who can manipulate their behaviors to act just like normal reviewers, and thus cannot be detected by the available techniques.In this article, we propose a novel concept of review graph to capture the relationships among all reviewers, reviews and stores that the reviewers have reviewed as a heterogeneous graph. We explore how interactions between nodes in this graph could reveal the cause of spam and propose an iterative computation model to identify suspicious reviewers. In the review graph, we have three kinds of nodes, namely, reviewer, review, and store. We capture their relationships by introducing three fundamental concepts, the trustiness of reviewers, the honesty of reviews, and the reliability of stores, and identifying their interrelationships: a reviewer is more trustworthy if the person has written more honesty reviews; a store is more reliable if it has more positive reviews from trustworthy reviewers; and a review is more honest if many other honest reviews support it. This is the first time such intricate relationships have been identified for spam detection and captured in a graph model. We further develop an effective computation method based on the proposed graph model. Different from any existing approaches, we do not use an review text information. Our model is thus complementary to existing approaches and able to find more difficult and subtle spamming activities, which are agreed upon by human judges after they evaluate our results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {61},
numpages = {21},
keywords = {Spammer detection, review graph}
}

@article{10.1145/2337542.2337545,
author = {Carmel, David and Uziel, Erel and Guy, Ido and Mass, Yosi and Roitman, Haggai},
title = {Folksonomy-Based Term Extraction for Word Cloud Generation},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337545},
doi = {10.1145/2337542.2337545},
abstract = {In this work we study the task of term extraction for word cloud generation in sparsely tagged domains, in which manual tags are scarce. We present a folksonomy-based term extraction method, called tag-boost, which boosts terms that are frequently used by the public to tag content. Our experiments with tag-boost based term extraction over different domains demonstrate tremendous improvement in word cloud quality, as reflected by the agreement between manual tags of the testing items and the cloud’s terms extracted from the items’ content. Moreover, our results demonstrate the high robustness of this approach, as compared to alternative cloud generation methods that exhibit a high sensitivity to data sparseness. Additionally, we show that tag-boost can be effectively applied even in nontagged domains, by using an external rich folksonomy borrowed from a well-tagged domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {60},
numpages = {20},
keywords = {keyword extraction, tag-boost, Tag-cloud generation}
}

@article{10.1145/2337542.2337544,
author = {Herda\u{g}delen, Ama\c{c} and Baroni, Marco},
title = {Bootstrapping a Game with a Purpose for Commonsense Collection},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337544},
doi = {10.1145/2337542.2337544},
abstract = {Text mining has been very successful in extracting huge amounts of commonsense knowledge from data, but the extracted knowledge tends to be extremely noisy. Manual construction of knowledge repositories, on the other hand, tends to produce high-quality data in very small amounts. We propose an architecture to combine the best of both worlds: A game with a purpose that induces humans to clean up data automatically extracted by text mining. First, a text miner trained on a set of known commonsense facts harvests many more candidate facts from corpora. Then, a simple slot-machine-with-a-purpose game presents these candidate facts to the players for verification by playing. As a result, a new dataset of high precision commonsense knowledge is created. This combined architecture is able to produce significantly better commonsense facts than the state-of-the-art text miner alone. Furthermore, we report that bootstrapping (i.e., training the text miner on the output of the game) improves the subsequent performance of the text miner.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {59},
numpages = {24},
keywords = {commonsense, Games with a purpose, Facebook, natural language processing, knowledge extraction}
}

@article{10.1145/2337542.2337543,
author = {Gabrilovich, Evgeniy and Su, Zhong and Tang, Jie},
title = {Introduction to the Special Section on Computational Models of Collective Intelligence in the Social Web},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/2337542.2337543},
doi = {10.1145/2337542.2337543},
journal = {ACM Trans. Intell. Syst. Technol.},
month = sep,
articleno = {58},
numpages = {2}
}

@article{10.1145/2168752.2168771,
author = {Rendle, Steffen},
title = {Factorization Machines with LibFM},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168771},
doi = {10.1145/2168752.2168771},
abstract = {Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {57},
numpages = {22},
keywords = {Factorization model, factorization machine, collaborative filtering, recommender system, tensor factorization, matrix factorization}
}

@article{10.1145/2168752.2168770,
author = {Zheng, Yan-Tao and Zha, Zheng-Jun and Chua, Tat-Seng},
title = {Mining Travel Patterns from Geotagged Photos},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168770},
doi = {10.1145/2168752.2168770},
abstract = {Recently, the phenomenal advent of photo-sharing services, such as Flickr and Panoramio, have led to volumous community-contributed photos with text tags, timestamps, and geographic references on the Internet. The photos, together with their time- and geo-references, become the digital footprints of photo takers and implicitly document their spatiotemporal movements. This study aims to leverage the wealth of these enriched online photos to analyze people’s travel patterns at the local level of a tour destination. Specifically, we focus our analysis on two aspects: (1) tourist movement patterns in relation to the regions of attractions (RoA), and (2) topological characteristics of travel routes by different tourists. To do so, we first build a statistically reliable database of travel paths from a noisy pool of community-contributed geotagged photos on the Internet. We then investigate the tourist traffic flow among different RoAs by exploiting the Markov chain model. Finally, the topological characteristics of travel routes are analyzed by performing a sequence clustering on tour routes. Testings on four major cities demonstrate promising results of the proposed system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {56},
numpages = {18},
keywords = {geotagged photos, Travel pattern mining}
}

@article{10.1145/2168752.2168769,
author = {Xu, Jun-Ming and Zhu, Xiaojin and Rogers, Timothy T.},
title = {Metric Learning for Estimating Psychological Similarities},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168769},
doi = {10.1145/2168752.2168769},
abstract = {An important problem in cognitive psychology is to quantify the perceived similarities between stimuli. Previous work attempted to address this problem with multidimensional scaling (MDS) and its variants. However, there are several shortcomings of the MDS approaches. We propose Yada, a novel general metric-learning procedure based on two-alternative forced-choice behavioral experiments. Our method learns forward and backward nonlinear mappings between an objective space in which the stimuli are defined by the standard feature vector representation and a subjective space in which the distance between a pair of stimuli corresponds to their perceived similarity. We conduct experiments on both synthetic and real human behavioral datasets to assess the effectiveness of Yada. The results show that Yada outperforms several standard embedding and metric-learning algorithms, both in terms of likelihood and recovery error.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {55},
numpages = {22},
keywords = {subjective distance, embedding, Metric learning}
}

@article{10.1145/2168752.2168768,
author = {Zhang, Yu and Yeung, Dit-Yan},
title = {Transfer Metric Learning with Semi-Supervised Extension},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168768},
doi = {10.1145/2168752.2168768},
abstract = {Distance metric learning plays a very crucial role in many data mining algorithms because the performance of an algorithm relies heavily on choosing a good metric. However, the labeled data available in many applications is scarce, and hence the metrics learned are often unsatisfactory. In this article, we consider a transfer-learning setting in which some related source tasks with labeled data are available to help the learning of the target task. We first propose a convex formulation for multitask metric learning by modeling the task relationships in the form of a task covariance matrix. Then we regard transfer learning as a special case of multitask learning and adapt the formulation of multitask metric learning to the transfer-learning setting for our method, called transfer metric learning (TML). In TML, we learn the metric and the task covariances between the source tasks and the target task under a unified convex formulation. To solve the convex optimization problem, we use an alternating method in which each subproblem has an efficient solution. Moreover, in many applications, some unlabeled data is also available in the target task, and so we propose a semi-supervised extension of TML called STML to further improve the generalization performance by exploiting the unlabeled data based on the manifold assumption. Experimental results on some commonly used transfer-learning applications demonstrate the effectiveness of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {54},
numpages = {28},
keywords = {multitask learning, transfer learning, Metric learning, semi-supervised learning}
}

@article{10.1145/2168752.2168767,
author = {Zhai, Deming and Chang, Hong and Shan, Shiguang and Chen, Xilin and Gao, Wen},
title = {Multiview Metric Learning with Global Consistency and Local Smoothness},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168767},
doi = {10.1145/2168752.2168767},
abstract = {In many real-world applications, the same object may have different observations (or descriptions) from multiview observation spaces, which are highly related but sometimes look different from each other. Conventional metric-learning methods achieve satisfactory performance on distance metric computation of data in a single-view observation space, but fail to handle well data sampled from multiview observation spaces, especially those with highly nonlinear structure. To tackle this problem, we propose a new method called Multiview Metric Learning with Global consistency and Local smoothness (MVML-GL) under a semisupervised learning setting, which jointly considers global consistency and local smoothness. The basic idea is to reveal the shared latent feature space of the multiview observations by embodying global consistency constraints and preserving local geometric structures. Specifically, this framework is composed of two main steps. In the first step, we seek a global consistent shared latent feature space, which not only preserves the local geometric structure in each space but also makes those labeled corresponding instances as close as possible. In the second step, the explicit mapping functions between the input spaces and the shared latent space are learned via regularized locally linear regression. Furthermore, these two steps both can be solved by convex optimizations in closed form. Experimental results with application to manifold alignment on real-world datasets of pose and facial expression demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {53},
numpages = {22},
keywords = {global consistency, Metric learning, local smoothness, multiview learning}
}

@article{10.1145/2168752.2168766,
author = {Hoi, Steven C. H. and Jin, Rong and Tang, Jinhui and Zhou, Zhi-Hua},
title = {Introduction to the Special Section on Distance Metric Learning in Intelligent Systems},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168766},
doi = {10.1145/2168752.2168766},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {52},
numpages = {2}
}

@article{10.1145/2168752.2168765,
author = {Hayden, David S. and Chien, Steve and Thompson, David R. and Casta\~{n}o, Rebecca},
title = {Using Clustering and Metric Learning to Improve Science Return of Remote Sensed Imagery},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168765},
doi = {10.1145/2168752.2168765},
abstract = {Current and proposed remote space missions, such as the proposed aerial exploration of Titan by an aerobot, often can collect more data than can be communicated back to Earth. Autonomous selective downlink algorithms can choose informative subsets of data to improve the science value of these bandwidth-limited transmissions. This requires statistical descriptors of the data that reflect very abstract and subtle distinctions in science content. We propose a metric learning strategy that teaches algorithms how best to cluster new data based on training examples supplied by domain scientists. We demonstrate that clustering informed by metric learning produces results that more closely match multiple scientists’ labelings of aerial data than do clusterings based on random or periodic sampling. A new metric-learning strategy accommodates training sets produced by multiple scientists with different and potentially inconsistent mission objectives. Our methods are fit for current spacecraft processors (e.g., RAD750) and would further benefit from more advanced spacecraft processor architectures, such as OPERA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {51},
numpages = {19},
keywords = {selective data return, Onboard data analysis, information retrieval, space exploration, clustering}
}

@article{10.1145/2168752.2168764,
author = {Estlin, Tara A. and Bornstein, Benjamin J. and Gaines, Daniel M. and Anderson, Robert C. and Thompson, David R. and Burl, Michael and Casta\~{n}o, Rebecca and Judd, Michele},
title = {AEGIS Automated Science Targeting for the MER Opportunity Rover},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168764},
doi = {10.1145/2168752.2168764},
abstract = {The Autonomous Exploration for Gathering Increased Science (AEGIS) system enables automated data collection by planetary rovers. AEGIS software was uploaded to the Mars Exploration Rover (MER) mission’s Opportunity rover in December 2009 and has successfully demonstrated automated onboard targeting based on scientist-specified objectives. Prior to AEGIS, images were transmitted from the rover to the operations team on Earth; scientists manually analyzed the images, selected geological targets for the rover’s remote-sensing instruments, and then generated a command sequence to execute the new measurements. AEGIS represents a significant paradigm shift---by using onboard data analysis techniques, the AEGIS software uses scientist input to select high-quality science targets with no human in the loop. This approach allows the rover to autonomously select and sequence targeted observations in an opportunistic fashion, which is particularly applicable for narrow field-of-view instruments (such as the MER Mini-TES spectrometer, the MER Panoramic camera, and the 2011 Mars Science Laboratory (MSL) ChemCam spectrometer). This article provides an overview of the AEGIS automated targeting capability and describes how it is currently being used onboard the MER mission Opportunity rover.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {50},
numpages = {19},
keywords = {spacecraft autonomy, Data analysis, autonomous science}
}

@article{10.1145/2168752.2168763,
author = {Wagstaff, Kiri L. and Panetta, Julian and Ansar, Adnan and Greeley, Ronald and Pendleton Hoffer, Mary and Bunte, Melissa and Sch\"{o}rghofer, Norbert},
title = {Dynamic Landmarking for Surface Feature Identification and Change Detection},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168763},
doi = {10.1145/2168752.2168763},
abstract = {Given the large volume of images being sent back from remote spacecraft, there is a need for automated analysis techniques that can quickly identify interesting features in those images. Feature identification in individual images and automated change detection in multiple images of the same target are valuable for scientific studies and can inform subsequent target selection. We introduce a new approach to orbital image analysis called dynamic landmarking. It focuses on the identification and comparison of visually salient features in images. We have evaluated this approach on images collected by five Mars orbiters. These evaluations were motivated by three scientific goals: to study fresh impact craters, dust devil tracks, and dark slope streaks on Mars. In the process we also detected a different kind of surface change that may indicate seasonally exposed bedforms. These experiences also point the way to how this approach could be used in an onboard setting to analyze and prioritize data as it is collected.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {49},
numpages = {22},
keywords = {salience, Change detection, image analysis}
}

@article{10.1145/2168752.2168762,
author = {Chien, Steve and Cesta, Amedeo},
title = {Introduction to the Special Section on Artificial Intelligence in Space},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168762},
doi = {10.1145/2168752.2168762},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {48},
numpages = {2}
}

@article{10.1145/2168752.2168761,
author = {Leung, Clement H. C. and Chan, Alice W. S. and Milani, Alfredo and Liu, Jiming and Li, Yuanxi},
title = {Intelligent Social Media Indexing and Sharing Using an Adaptive Indexing Search Engine},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168761},
doi = {10.1145/2168752.2168761},
abstract = {Effective sharing of diverse social media is often inhibited by limitations in their search and discovery mechanisms, which are particularly restrictive for media that do not lend themselves to automatic processing or indexing. Here, we present the structure and mechanism of an adaptive search engine which is designed to overcome such limitations. The basic framework of the adaptive search engine is to capture human judgment in the course of normal usage from user queries in order to develop semantic indexes which link search terms to media objects semantics. This approach is particularly effective for the retrieval of multimedia objects, such as images, sounds, and videos, where a direct analysis of the object features does not allow them to be linked to search terms, for example, nontextual/icon-based search, deep semantic search, or when search terms are unknown at the time the media repository is built. An adaptive search architecture is presented to enable the index to evolve with respect to user feedback, while a randomized query-processing technique guarantees avoiding local minima and allows the meaningful indexing of new media objects and new terms. The present adaptive search engine allows for the efficient community creation and updating of social media indexes, which is able to instill and propagate deep knowledge into social media concerning the advanced search and usage of media resources. Experiments with various relevance distribution settings have shown efficient convergence of such indexes, which enable intelligent search and sharing of social media resources that are otherwise hard to discover.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {47},
numpages = {27},
keywords = {Adaptive indexing, multimedia semantics, social media, evolutionary computation, relevance feedback, genetic algorithms}
}

@article{10.1145/2168752.2168760,
author = {Zhang, Ning and Duan, Ling-Yu and Li, Lingfang and Huang, Qingming and Du, Jun and Gao, Wen and Guan, Ling},
title = {A Generic Approach for Systematic Analysis of Sports Videos},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168760},
doi = {10.1145/2168752.2168760},
abstract = {Various innovative and original works have been applied and proposed in the field of sports video analysis. However, individual works have focused on sophisticated methodologies with particular sport types and there has been a lack of scalable and holistic frameworks in this field. This article proposes a solution and presents a systematic and generic approach which is experimented on a relatively large-scale sports consortia. The system aims at the event detection scenario of an input video with an orderly sequential process. Initially, domain knowledge-independent local descriptors are extracted homogeneously from the input video sequence. Then the video representation is created by adopting a bag-of-visual-words (BoW) model. The video’s genre is first identified by applying the k-nearest neighbor (k-NN) classifiers on the initially obtained video representation, and various dissimilarity measures are assessed and evaluated analytically. Subsequently, an unsupervised probabilistic latent semantic analysis (PLSA)-based approach is employed at the same histogram-based video representation, characterizing each frame of video sequence into one of four view groups, namely closed-up-view, mid-view, long-view, and outer-field-view. Finally, a hidden conditional random field (HCRF) structured prediction model is utilized for interesting event detection. From experimental results, k-NN classifier using KL-divergence measurement demonstrates the best accuracy at 82.16% for genre categorization. Supervised SVM and unsupervised PLSA have average classification accuracies at 82.86% and 68.13%, respectively. The HCRF model achieves 92.31% accuracy using the unsupervised PLSA based label input, which is comparable with the supervised SVM based input at an accuracy of 93.08%. In general, such a systematic approach can be widely applied in processing massive videos generically.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {46},
numpages = {29},
keywords = {Genre categorization, View classification, Event detection, Generic framework}
}

@article{10.1145/2168752.2168759,
author = {Berretti, Stefano and Del Bimbo, Alberto and Pala, Pietro},
title = {Distinguishing Facial Features for Ethnicity-Based 3D Face Recognition},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168759},
doi = {10.1145/2168752.2168759},
abstract = {Among different approaches for 3D face recognition, solutions based on local facial characteristics are very promising, mainly because they can manage facial expression variations by assigning different weights to different parts of the face. However, so far, a few works have investigated the individual relevance that local features play in 3D face recognition with very simple solutions applied in the practice.In this article, a local approach to 3D face recognition is combined with a feature selection model to study the relative relevance of different regions of the face for the purpose of discriminating between different subjects. The proposed solution is experimented using facial scans of the Face Recognition Grand Challenge dataset. Results of the experimentation are two-fold: they quantitatively demonstrate the assumption that different regions of the face have different relevance for face discrimination and also show that the relevance of facial regions changes for different ethnic groups.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {45},
numpages = {20},
keywords = {Feature selection, iso-geodesic stripes, ethnicity-based learning, 3D face recognition}
}

@article{10.1145/2168752.2168758,
author = {Ji, Rongrong and Yao, Hongxun and Tian, Qi and Xu, Pengfei and Sun, Xiaoshuai and Liu, Xianming},
title = {Context-Aware Semi-Local Feature Detector},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168758},
doi = {10.1145/2168752.2168758},
abstract = {How can interest point detectors benefit from contextual cues? In this articles, we introduce a context-aware semi-local detector (CASL) framework to give a systematic answer with three contributions: (1) We integrate the context of interest points to recurrently refine their detections. (2) This integration boosts interest point detectors from the traditionally local scale to a semi-local scale to discover more discriminative salient regions. (3) Such context-aware structure further enables us to bring forward category learning (usually in the subsequent recognition phase) into interest point detection to locate category-aware, meaningful salient regions.Our CASL detector consists of two phases. The first phase accumulates multiscale spatial correlations of local features into a difference of contextual Gaussians (DoCG) field. DoCG quantizes detector context to highlight contextually salient regions at a semi-local scale, which also reveals visual attentions to a certain extent. The second phase locates contextual peaks by mean shift search over the DoCG field, which subsequently integrates contextual cues into feature description. This phase enables us to integrate category learning into mean shift search kernels. This learning-based CASL mechanism produces more category-aware features, which substantially benefits the subsequent visual categorization process. We conducted experiments in image search, object characterization, and feature detector repeatability evaluations, which reported superior discriminability and comparable repeatability to state-of-the-art works.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {44},
numpages = {27},
keywords = {knowledge representation, learning-based feature extraction, context-aware feature, mean shift, Internet, contextual Gaussian field, multimedia systems, Semi-local feature, image analysis, supervised kernel learning}
}

@article{10.1145/2168752.2168757,
author = {Zhang, Shengping and Yao, Hongxun and Sun, Xin and Liu, Shaohui},
title = {Robust Visual Tracking Using an Effective Appearance Model Based on Sparse Coding},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168757},
doi = {10.1145/2168752.2168757},
abstract = {Intelligent video surveillance is currently one of the most active research topics in computer vision, especially when facing the explosion of video data captured by a large number of surveillance cameras. As a key step of an intelligent surveillance system, robust visual tracking is very challenging for computer vision. However, it is a basic functionality of the human visual system (HVS). Psychophysical findings have shown that the receptive fields of simple cells in the visual cortex can be characterized as being spatially localized, oriented, and bandpass, and it forms a sparse, distributed representation of natural images. In this article, motivated by these findings, we propose an effective appearance model based on sparse coding and apply it in visual tracking. Specifically, we consider the responses of general basis functions extracted by independent component analysis on a large set of natural image patches as features and model the appearance of the tracked target as the probability distribution of these features. In order to make the tracker more robust to partial occlusion, camouflage environments, pose changes, and illumination changes, we further select features that are related to the target based on an entropy-gain criterion and ignore those that are not. The target is finally represented by the probability distribution of those related features. The target search is performed by minimizing the Matusita distance between the distributions of the target model and a candidate using Newton-style iterations. The experimental results validate that the proposed method is more robust and effective than three state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {43},
numpages = {18},
keywords = {Intelligent visual surveillance, appearance model, sparse coding}
}

@article{10.1145/2168752.2168756,
author = {Suk, Myunghoon and Ramadass, Ashok and Jin, Yohan and Prabhakaran, B.},
title = {Video Human Motion Recognition Using a Knowledge-Based Hybrid Method Based on a Hidden Markov Model},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168756},
doi = {10.1145/2168752.2168756},
abstract = {Human motion recognition in video data has several interesting applications in fields such as gaming, senior/assisted-living environments, and surveillance. In these scenarios, we may have to consider adding new motion classes (i.e., new types of human motions to be recognized), as well as new training data (e.g., for handling different type of subjects). Hence, both the accuracy of classification and training time for the machine learning algorithms become important performance parameters in these cases. In this article, we propose a knowledge-based hybrid (KBH) method that can compute the probabilities for hidden Markov models (HMMs) associated with different human motion classes. This computation is facilitated by appropriately mixing features from two different media types (3D motion capture and 2D video). We conducted a variety of experiments comparing the proposed KBH for HMMs and the traditional Baum-Welch algorithms. With the advantage of computing the HMM parameter in a noniterative manner, the KBH method outperforms the Baum-Welch algorithm both in terms of accuracy as well as in reduced training time. Moreover, we show in additional experiments that the KBH method also outperforms the linear support vector machine (SVM).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {42},
numpages = {29},
keywords = {hidden Markov model, video human motion recognition, human-computer interaction, 3D motion capture}
}

@article{10.1145/2168752.2168755,
author = {Ewerth, Ralph and M\"{u}hling, Markus and Freisleben, Bernd},
title = {Robust Video Content Analysis via Transductive Learning},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168755},
doi = {10.1145/2168752.2168755},
abstract = {Reliable video content analysis is an essential prerequisite for effective video search. An important current research question is how to develop robust video content analysis methods that produce satisfactory results for a large variety of video sources, distribution platforms, genres, and content. The work presented in this article exploits the observation that the appearance of objects and events is often related to a particular video sequence, episode, program, or broadcast. This motivates our idea of considering the content analysis task for a single video or episode as a transductive setting: the final classification model must be optimal for the given video only, and not in general, as expected for inductive learning. For this purpose, the unlabeled video test data have to be used in the learning process. In this article, a transductive learning framework for robust video content analysis based on feature selection and ensemble classification is presented. In contrast to related transductive approaches for video analysis (e.g., for concept detection), the framework is designed in a general manner and not only for a single task. The proposed framework is applied to the following video analysis tasks: shot boundary detection, face recognition, semantic video retrieval, and semantic indexing of computer game sequences. Experimental results for diverse video analysis tasks and large test sets demonstrate that the proposed transductive framework improves the robustness of the underlying state-of-the-art approaches, whereas transductive support vector machines do not solve particular tasks in a satisfactory manner.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {41},
numpages = {26},
keywords = {cut detection, concept detection, Transductive learning, face recognition, robustness, ensemble classification, self-supervised learning, robust video content analysis}
}

@article{10.1145/2168752.2168754,
author = {Yang, Yi-Hsuan and Chen, Homer H.},
title = {Machine Recognition of Music Emotion: A Review},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168754},
doi = {10.1145/2168752.2168754},
abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {40},
numpages = {30},
keywords = {Music emotion recognition}
}

@article{10.1145/2168752.2168753,
author = {Hua, Xian-Sheng and Tian, Qi and Del Bimbo, Alberto and Jain, Ramesh},
title = {Introduction to the Special Section on Intelligent Multimedia Systems and Technology Part II},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/2168752.2168753},
doi = {10.1145/2168752.2168753},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {39},
numpages = {2}
}

@article{10.1145/2089094.2089114,
author = {Peng, Wei and Sun, Tong and Revankar, Shriram and Li, Tao},
title = {Mining the “Voice of the Customer” for Business Prioritization},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089114},
doi = {10.1145/2089094.2089114},
abstract = {To gain competitiveness and sustained growth in the 21st century, most businesses are on a mission to become more customer-centric. In order to succeed in this endeavor, it is crucial not only to synthesize and analyze the VOC (the VOice of the Customer) data (i.e., the feedbacks or requirements raised by customers), but also to quickly turn these data into actionable knowledge. Although there are many technologies being developed in this complex problem space, most existing approaches in analyzing customer requests are ad hoc, time-consuming, error-prone, people-based processes which hardly scale well as the quantity of customer information explodes. This often results in the slow response to customer requests. In this article, in order to mine VOC to extract useful knowledge for the best product or service quality, we develop a hybrid framework that integrates domain knowledge with data-driven approaches to analyze the semi-structured customer requests. The framework consists of capturing functional features, discovering the overlap or correlation among the features, and identifying the evolving feature trend by using the knowledge transformation model. In addition, since understanding the relative importance of the individual customer request is very critical and has a direct impact on the effective prioritization in the development process, we develop a novel semantic enhanced link-based ranking (SELRank) algorithm for relatively rating/ranking both customer requests and products. The framework has been successfully applied on Xerox Office Group Feature Enhancement Requirements (XOG FER) datasets to analyze customer requests.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {38},
numpages = {17},
keywords = {business prioritization, text mining, Voice of the customer, ranking}
}

@article{10.1145/2089094.2089113,
author = {Zhou, Ke and Bai, Jing and Zha, Hongyuan and Xue, Gui-Rong},
title = {Leveraging Auxiliary Data for Learning to Rank},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089113},
doi = {10.1145/2089094.2089113},
abstract = {In learning to rank, both the quality and quantity of the training data have significant impacts on the performance of the learned ranking functions. However, in many applications, there are usually not sufficient labeled training data for the construction of an accurate ranking model. It is therefore desirable to leverage existing training data from other tasks when learning the ranking function for a particular task, an important problem which we tackle in this article utilizing a boosting framework with transfer learning. In particular, we propose to adaptively learn transferable representations called super-features from the training data of both the target task and the auxiliary task. Those super-features and the coefficients for combining them are learned in an iterative stage-wise fashion. Unlike previous transfer learning methods, the super-features can be adaptively learned by weak learners from the data. Therefore, the proposed framework is sufficiently flexible to deal with complicated common structures among different learning tasks. We evaluate the performance of the proposed transfer learning method for two datasets from the Letor collection and one dataset collected from a commercial search engine, and we also compare our methods with several existing transfer learning methods. Our results demonstrate that the proposed method can enhance the ranking functions of the target tasks utilizing the training data from the auxiliary tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {37},
numpages = {21},
keywords = {transfer learning, experimental evaluation, super-features, convergence analysis, search engine, learning to rank, Relevance}
}

@article{10.1145/2089094.2089112,
author = {Zhang, Weinan and Wang, Dingquan and Xue, Gui-Rong and Zha, Hongyuan},
title = {Advertising Keywords Recommendation for Short-Text Web Pages Using Wikipedia},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089112},
doi = {10.1145/2089094.2089112},
abstract = {Advertising keywords recommendation is an indispensable component for online advertising with the keywords selected from the target Web pages used for contextual advertising or sponsored search. Several ranking-based algorithms have been proposed for recommending advertising keywords. However, for most of them performance is still lacking, especially when dealing with short-text target Web pages, that is, those containing insufficient textual information for ranking. In some cases, short-text Web pages may not even contain enough keywords for selection. A natural alternative is then to recommend relevant keywords not present in the target Web pages. In this article, we propose a novel algorithm for advertising keywords recommendation for short-text Web pages by leveraging the contents of Wikipedia, a user-contributed online encyclopedia. Wikipedia contains numerous entities with related entities on a topic linked to each other. Given a target Web page, we propose to use a content-biased PageRank on the Wikipedia graph to rank the related entities. Furthermore, in order to recommend high-quality advertising keywords, we also add an advertisement-biased factor into our model. With these two biases, advertising keywords that are both relevant to a target Web page and valuable for advertising are recommended. In our experiments, several state-of-the-art approaches for keyword recommendation are compared. The experimental results demonstrate that our proposed approach produces substantial improvement in the precision of the top 20 recommended keywords on short-text Web pages over existing approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {36},
numpages = {25},
keywords = {topic-sensitive PageRank, advertising keywords recommendation, Wikipedia, Contextual advertising}
}

@article{10.1145/2089094.2089111,
author = {Li, Xueying and Cao, Huanhuan and Chen, Enhong and Tian, Jilei},
title = {Learning to Infer the Status of Heavy-Duty Sensors for Energy-Efficient Context-Sensing},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089111},
doi = {10.1145/2089094.2089111},
abstract = {With the prevalence of smart mobile devices with multiple sensors, the commercial application of intelligent context-aware services becomes more and more attractive. However, limited by the battery capacity, the energy efficiency of context-sensing is the bottleneck for the success of context-aware applications. Though several previous studies for energy-efficient context-sensing have been reported, none of them can be applied to multiple types of high-energy-consuming sensors. Moreover, applying machine learning technologies to energy-efficient context-sensing is underexplored too. In this article, we propose to leverage machine learning technologies for improving the energy efficiency of multiple high-energy-consuming context sensors by trading off the sensing accuracy. To be specific, we try to infer the status of high-energy-consuming sensors according to the outputs of software-based sensors and the physical sensors that are necessary to work all the time for supporting the basic functions of mobile devices. If the inference indicates the high-energy-consuming sensor is in a stable status, we avoid the unnecessary invocation and instead use the latest invoked value as the estimation. The experimental results on real datasets show that the energy efficiency of GPS sensing and audio-level sensing are significantly improved by the proposed approach while the sensing accuracy is over 90%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {35},
numpages = {23},
keywords = {Context-sensing, energy efficiency, machine learning}
}

@article{10.1145/2089094.2089110,
author = {Shakarian, Paulo and Dickerson, John P. and Subrahmanian, V. S.},
title = {Adversarial Geospatial Abduction Problems},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089110},
doi = {10.1145/2089094.2089110},
abstract = {Geospatial Abduction Problems (GAPs) involve the inference of a set of locations that “best explain” a given set of locations of observations. For example, the observations might include locations where a serial killer committed murders or where insurgents carried out Improvised Explosive Device (IED) attacks. In both these cases, we would like to infer a set of locations that explain the observations, for example, the set of locations where the serial killer lives/works, and the set of locations where insurgents locate weapons caches. However, unlike all past work on abduction, there is a strong adversarial component to this; an adversary actively attempts to prevent us from discovering such locations. We formalize such abduction problems as a two-player game where both players (an “agent” and an “adversary”) use a probabilistic model of their opponent (i.e., a mixed strategy). There is asymmetry as the adversary can choose both the locations of the observations and the locations of the explanation, while the agent (i.e., us) tries to discover these. In this article, we study the problem from the point of view of both players. We define reward functions axiomatically to capture the similarity between two sets of explanations (one corresponding to the locations chosen by the adversary, one guessed by the agent). Many different reward functions can satisfy our axioms. We then formalize the Optimal Adversary Strategy (OAS) problem and the Maximal Counter-Adversary strategy (MCA) and show that both are NP-hard, that their associated counting complexity problems are #P-hard, and that MCA has no fully polynomial approximation scheme unless P=NP. We show that approximation guarantees are possible for MCA when the reward function satisfies two simple properties (zero-starting and monotonicity) which many natural reward functions satisfy. We develop a mixed integer linear programming algorithm to solve OAS and two algorithms to (approximately) compute MCA; the algorithms yield different approximation guarantees and one algorithm assumes a monotonic reward function. Our experiments use real data about IED attacks over a 21-month period in Baghdad. We are able to show that both the MCA algorithms work well in practice; while MCA-GREEDY-MONO is both highly accurate and slightly faster than MCA-LS, MCA-LS (to our surprise) always completely and correctly maximized the expected benefit to the agent while running in an acceptable time period.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {34},
numpages = {35},
keywords = {spatial reasoning, Abduction}
}

@article{10.1145/2089094.2089109,
author = {Shi, Lixin and Zhao, Yuhang and Tang, Jie},
title = {Batch Mode Active Learning for Networked Data},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089109},
doi = {10.1145/2089094.2089109},
abstract = {We study a novel problem of batch mode active learning for networked data. In this problem, data instances are connected with links and their labels are correlated with each other, and the goal of batch mode active learning is to exploit the link-based dependencies and node-specific content information to actively select a batch of instances to query the user for learning an accurate model to label unknown instances in the network. We present three criteria (i.e., minimum redundancy, maximum uncertainty, and maximum impact) to quantify the informativeness of a set of instances, and formalize the batch mode active learning problem as selecting a set of instances by maximizing an objective function which combines both link and content information. As solving the objective function is NP-hard, we present an efficient algorithm to optimize the objective function with a bounded approximation rate. To scale to real large networks, we develop a parallel implementation of the algorithm. Experimental results on both synthetic datasets and real-world datasets demonstrate the effectiveness and efficiency of our approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {33},
numpages = {25},
keywords = {network classification, combine link and content, Batch mode active learning}
}

@article{10.1145/2089094.2089108,
author = {Kolomvatsos, Kostas and Anagnostopoulos, Christos and Hadjiefthymiades, Stathes},
title = {A Fuzzy Logic System for Bargaining in Information Markets},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089108},
doi = {10.1145/2089094.2089108},
abstract = {Future Web business models involve virtual environments where entities interact in order to sell or buy information goods. Such environments are known as Information Markets (IMs). Intelligent agents are used in IMs for representing buyers or information providers (sellers). We focus on the decisions taken by the buyer in the purchase negotiation process with sellers. We propose a reasoning mechanism on the offers (prices of information goods) issued by sellers based on fuzzy logic. The buyer’s knowledge on the negotiation process is modeled through fuzzy sets. We propose a fuzzy inference engine dealing with the decisions that the buyer takes on each stage of the negotiation process. The outcome of the proposed reasoning method indicates whether the buyer should accept or reject the sellers’ offers. Our findings are very promising for the efficiency of automated transactions undertaken by intelligent agents.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {32},
numpages = {26},
keywords = {negotiation process, Fuzzy systems}
}

@article{10.1145/2089094.2089107,
author = {Ma, Huadong and Zeng, Chengbin and Ling, Charles X.},
title = {A Reliable People Counting System via Multiple Cameras},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089107},
doi = {10.1145/2089094.2089107},
abstract = {Reliable and real-time people counting is crucial in many applications. Most previous works can only count moving people from a single camera, which cannot count still people or can fail badly when there is a crowd (i.e., heavy occlusion occurs). In this article, we build a system for robust and fast people counting under occlusion through multiple cameras. To improve the reliability of human detection from a single camera, we use a dimensionality reduction method on the multilevel edge and texture features to handle the large variations in human appearance and poses. To accelerate the detection speed, we propose a novel two-stage cascade-of-rejectors method. To handle the heavy occlusion in crowded scenes, we present a fusion method with error tolerance to combine human detection from multiple cameras. To improve the speed and accuracy of moving people counting, we combine our multiview fusion detection method with particle tracking to count the number of people moving in/out the camera view (“border control”). Extensive experiments and analyses show that our method outperforms state-of-the-art techniques in single- and multicamera datasets for both speed and reliability. We also design a deployed system for fast and reliable people (still or moving) counting by using multiple cameras.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {31},
numpages = {22},
keywords = {human detection, People counting, video surveillance, multiple cameras}
}

@article{10.1145/2089094.2089106,
author = {Bifet, Albert and Frank, Eibe and Holmes, Geoff and Pfahringer, Bernhard},
title = {Ensembles of Restricted Hoeffding Trees},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089106},
doi = {10.1145/2089094.2089106},
abstract = {The success of simple methods for classification shows that is is often not necessary to model complex attribute interactions to obtain good classification accuracy on practical problems. In this article, we propose to exploit this phenomenon in the data stream context by building an ensemble of Hoeffding trees that are each limited to a small subset of attributes. In this way, each tree is restricted to model interactions between attributes in its corresponding subset. Because it is not known a priori which attribute subsets are relevant for prediction, we build exhaustive ensembles that consider all possible attribute subsets of a given size. As the resulting Hoeffding trees are not all equally important, we weigh them in a suitable manner to obtain accurate classifications. This is done by combining the log-odds of their probability estimates using sigmoid perceptrons, with one perceptron per class. We propose a mechanism for setting the perceptrons’ learning rate using the change detection method for data streams, and also use to reset ensemble members (i.e., Hoeffding trees) when they no longer perform well. Our experiments show that the resulting ensemble classifier outperforms bagging for data streams in terms of accuracy when both are used in conjunction with adaptive naive Bayes Hoeffding trees, at the expense of runtime and memory consumption. We also show that our stacking method can improve the performance of a bagged ensemble.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {30},
numpages = {20},
keywords = {Data streams, decision trees, ensemble methods}
}

@article{10.1145/2089094.2089105,
author = {Li, Peipei and Wu, Xindong and Hu, Xuegang},
title = {Mining Recurring Concept Drifts with Limited Labeled Streaming Data},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089105},
doi = {10.1145/2089094.2089105},
abstract = {Tracking recurring concept drifts is a significant issue for machine learning and data mining that frequently appears in real-world stream classification problems. It is a challenge for many streaming classification algorithms to learn recurring concepts in a data stream environment with unlabeled data, and this challenge has received little attention from the research community. Motivated by this challenge, this article focuses on the problem of recurring contexts in streaming environments with limited labeled data. We propose a semi-supervised classification algorithm for data streams with REcurring concept Drifts and Limited LAbeled data, called REDLLA, in which a decision tree is adopted as the classification model. When growing a tree, a clustering algorithm based on k-means is installed to produce concept clusters and unlabeled data are labeled in the method of majority-class at leaves. In view of deviations between history and new concept clusters, potential concept drifts are distinguished and recurring concepts are maintained. Extensive studies on both synthetic and real-world data confirm the advantages of our REDLLA algorithm over three state-of-the-art online classification algorithms of CVFDT, DWCDS, and CDRDT and several known online semi-supervised algorithms, even in the case with more than 90% unlabeled data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {29},
numpages = {32},
keywords = {Data stream, decision tree, clustering, concept drift}
}

@article{10.1145/2089094.2089104,
author = {Hajimirsadeghi, Hossein and Ahmadabadi, Majid Nili and Araabi, Babak Nadjar and Moradi, Hadi},
title = {Conceptual Imitation Learning in a Human-Robot Interaction Paradigm},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089104},
doi = {10.1145/2089094.2089104},
abstract = {In general, imitation is imprecisely used to address different levels of social learning from high-level knowledge transfer to low-level regeneration of motor commands. However, true imitation is based on abstraction and conceptualization. This article presents a model for conceptual imitation through interaction with the teacher to abstract spatio-temporal demonstrations based on their functional meaning. Abstraction, concept acquisition, and self-organization of proto-symbols are performed through an incremental and gradual learning algorithm. In this algorithm, Hidden Markov Models (HMMs) are used to abstract perceptually similar demonstrations. However, abstract (relational) concepts emerge as a collection of HMMs irregularly scattered in the perceptual space but showing the same functionality. Performance of the proposed algorithm is evaluated in two experimental scenarios. The first one is a human-robot interaction task of imitating signs produced by hand movements. The second one is a simulated interactive task of imitating whole body motion patterns of a humanoid model. Experimental results show efficiency of our model for concept extraction, proto-symbol emergence, motion pattern recognition, prediction, and generation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {28},
numpages = {25},
keywords = {Imitation, human-robot interaction, concept learning, hidden Markov model}
}

@article{10.1145/2089094.2089103,
author = {Sugiyama, Masashi and Yang, Qiang},
title = {Introduction to the Special Section on the 2nd Asia Conference on Machine Learning (ACML 2010)},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089103},
doi = {10.1145/2089094.2089103},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {27},
numpages = {1}
}

@article{10.1145/2089094.2089102,
author = {Rohrdantz, Christian and Hao, Ming C. and Dayal, Umeshwar and Haug, Lars-Erik and Keim, Daniel A.},
title = {Feature-Based Visual Sentiment Analysis of Text Document Streams},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089102},
doi = {10.1145/2089094.2089102},
abstract = {This article describes automatic methods and interactive visualizations that are tightly coupled with the goal to enable users to detect interesting portions of text document streams. In this scenario the interestingness is derived from the sentiment, temporal density, and context coherence that comments about features for different targets (e.g., persons, institutions, product attributes, topics, etc.) have. Contributions are made at different stages of the visual analytics pipeline, including novel ways to visualize salient temporal accumulations for further exploration. Moreover, based on the visualization, an automatic algorithm aims to detect and preselect interesting time interval patterns for different features in order to guide analysts. The main target group for the suggested methods are business analysts who want to explore time-stamped customer feedback to detect critical issues. Finally, application case studies on two different datasets and scenarios are conducted and an extensive evaluation is provided for the presented intelligent visual interface for feature-based sentiment exploration over time.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {26},
numpages = {25},
keywords = {text mining, Document time series, visual analytics, sentiment analysis}
}

@article{10.1145/2089094.2089101,
author = {Liu, Shixia and Zhou, Michelle X. and Pan, Shimei and Song, Yangqiu and Qian, Weihong and Cai, Weijia and Lian, Xiaoxiao},
title = {TIARA: Interactive, Topic-Based Visual Text Summarization and Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089101},
doi = {10.1145/2089094.2089101},
abstract = {We are building an interactive visual text analysis tool that aids users in analyzing large collections of text. Unlike existing work in visual text analytics, which focuses either on developing sophisticated text analytic techniques or inventing novel text visualization metaphors, ours tightly integrates state-of-the-art text analytics with interactive visualization to maximize the value of both. In this article, we present our work from two aspects. We first introduce an enhanced, LDA-based topic analysis technique that automatically derives a set of topics to summarize a collection of documents and their content evolution over time. To help users understand the complex summarization results produced by our topic analysis technique, we then present the design and development of a time-based visualization of the results. Furthermore, we provide users with a set of rich interaction tools that help them further interpret the visualized results in context and examine the text collection from multiple perspectives. As a result, our work offers three unique contributions. First, we present an enhanced topic modeling technique to provide users with a time-sensitive and more meaningful text summary. Second, we develop an effective visual metaphor to transform abstract and often complex text summarization results into a comprehensible visual representation. Third, we offer users flexible visual interaction tools as alternatives to compensate for the deficiencies of current text summarization techniques. We have applied our work to a number of text corpora and our evaluation shows promise, especially in support of complex text analyses.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {25},
numpages = {28},
keywords = {Text analytics, topic model, stacked graph, interactive text visualization, text summarization, text trend chart}
}

@article{10.1145/2089094.2089100,
author = {Zhang, Yi and Li, Tao},
title = {DClusterE: A Framework for Evaluating and Understanding Document Clustering Using Visualization},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089100},
doi = {10.1145/2089094.2089100},
abstract = {Over the last decade, document clustering, as one of the key tasks in information organization and navigation, has been widely studied. Many algorithms have been developed for addressing various challenges in document clustering and for improving clustering performance. However, relatively few research efforts have been reported on evaluating and understanding document clustering results. In this article, we present DClusterE, a comprehensive and effective framework for document clustering evaluation and understanding using information visualization. DClusterE integrates cluster validation with user interactions and offers rich visualization tools for users to examine document clustering results from multiple perspectives. In particular, through informative views including force-directed layout view, matrix view, and cluster view, DClusterE provides not only different aspects of document inter/intra-clustering structures, but also the corresponding relationship between clustering results and the ground truth. Additionally, DClusterE supports general user interactions such as zoom in/out, browsing, and interactive access of the documents at different levels. Two new techniques are proposed to implement DClusterE: (1) A novel multiplicative update algorithm (MUA) for matrix reordering to generate narrow-banded (or clustered) nonzero patterns from documents. Combined with coarse seriation, MUA is able to provide better visualization of the cluster structures. (2) A Mallows-distance-based algorithm for establishing the relationship between the clustering results and the ground truth, which serves as the basis for coloring schemes. Experiments and user studies are conducted to demonstrate the effectiveness and efficiency of DClusterE.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {24},
keywords = {performance evaluation, clustering, visualization, Document analysis}
}

@article{10.1145/2089094.2089099,
author = {Gretarsson, Brynjar and O’Donovan, John and Bostandjiev, Svetlin and H\"{o}llerer, Tobias and Asuncion, Arthur and Newman, David and Smyth, Padhraic},
title = {TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089099},
doi = {10.1145/2089094.2089099},
abstract = {We present TopicNets, a Web-based system for visual and interactive analysis of large sets of documents using statistical topic models. A range of visualization types and control mechanisms to support knowledge discovery are presented. These include corpus- and document-specific views, iterative topic modeling, search, and visual filtering. Drill-down functionality is provided to allow analysts to visualize individual document sections and their relations within the global topic space. Analysts can search across a dataset through a set of expansion techniques on selected document and topic nodes. Furthermore, analysts can select relevant subsets of documents and perform real-time topic modeling on these subsets to interactively visualize topics at various levels of granularity, allowing for a better understanding of the documents. A discussion of the design and implementation choices for each visual analysis technique is presented. This is followed by a discussion of three diverse use cases in which TopicNets enables fast discovery of information that is otherwise hard to find. These include a corpus of 50,000 successful NSF grant proposals, 10,000 publications from a large research center, and single documents including a grant proposal and a PhD thesis.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {23},
numpages = {26},
keywords = {Topic modeling, text visualization, graph visualization}
}

@article{10.1145/2089094.2089098,
author = {Candan, K. Sel\c{c}uk and Di Caro, Luigi and Sapino, Maria Luisa},
title = {PhC: Multiresolution Visualization and Exploration of Text Corpora with Parallel Hierarchical Coordinates},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089098},
doi = {10.1145/2089094.2089098},
abstract = {The high-dimensional nature of the textual data complicates the design of visualization tools to support exploration of large document corpora. In this article, we first argue that the Parallel Coordinates (PC) technique, which can map multidimensional vectors onto a 2D space in such a way that elements with similar values are represented as similar poly-lines or curves in the visualization space, can be used to help users discern patterns in document collections. The inherent reduction in dimensionality during the mapping from multidimensional points to 2D lines, however, may result in visual complications. For instance, the lines that correspond to clusters of objects that are separate in the multidimensional space may overlap each other in the 2D space; the resulting increase in the number of crossings would make it hard to distinguish the individual document clusters. Such crossings of lines and overly dense regions are significant sources of visual clutter, thus avoiding them may help interpret the visualization. In this article, we note that visual clutter can be significantly reduced by adjusting the resolution of the individual term coordinates by clustering the corresponding values. Such reductions in the resolution of the individual term-coordinates, however, will lead to a certain degree of information loss and thus the appropriate resolution for the term-coordinates has to be selected carefully. Thus, in this article we propose a controlled clutter reduction approach, called Parallel hierarchical Coordinates (or PhC), for reducing the visual clutter in PC-based visualizations of text corpora. We define visual clutter and information loss measures and provide extensive evaluations that show that the proposed PhC provides significant visual gains (i.e., multiple orders of reductions in visual clutter) with small information loss during visualization and exploration of document collections.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {22},
numpages = {36},
keywords = {Document set visualization, clutter reduction, parallel coordinates}
}

@article{10.1145/2089094.2089097,
author = {Thai, Vinhtuan and Rouille, Pierre-Yves and Handschuh, Siegfried},
title = {Visual Abstraction and Ordering in Faceted Browsing of Text Collections},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089097},
doi = {10.1145/2089094.2089097},
abstract = {Faceted navigation is a technique for the exploration and discovery of a collection of resources, which can be of various types including text documents. While being information-rich resources, documents are usually not treated as content-bearing items in faceted browsing interfaces, and yet the required clean metadata is not always available or matches users’ interest. In addition, the existing linear listing paradigm for representing result items from the faceted filtering process makes it difficult for users to traverse or compare across facet values in different orders of importance to them. In this context, we report in this article a visual support toward faceted browsing of a collection of documents based on a set of entities of interest to users. Our proposed approach involves using a multi-dimensional visualization as an alternative to the linear listing of focus items. In this visualization, visual abstraction based on a combination of a conceptual structure and the structural equivalence of documents can be simultaneously used to deal with a large number of items. Furthermore, the approach also enables visual ordering based on the importance of facet values to support prioritized, cross-facet comparisons of focus items. A user study was conducted and the results suggest that interfaces using the proposed approach can support users better in exploratory tasks and were also well-liked by the participants of the study, with the hybrid interface combining the multi-dimensional visualization with the linear listing receiving the most favorable ratings.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {21},
numpages = {24},
keywords = {Faceted browsing, visual exploration, text collections}
}

@article{10.1145/2089094.2089096,
author = {Cui, Weiwei and Qu, Huamin and Zhou, Hong and Zhang, Wenbin and Skiena, Steve},
title = {Watch the Story Unfold with TextWheel: Visualization of Large-Scale News Streams},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089096},
doi = {10.1145/2089094.2089096},
abstract = {Keyword-based searching and clustering of news articles have been widely used for news analysis. However, news articles usually have other attributes such as source, author, date and time, length, and sentiment which should be taken into account. In addition, news articles and keywords have complicated macro/micro relations, which include relations between news articles (i.e., macro relation), relations between keywords (i.e., micro relation), and relations between news articles and keywords (i.e., macro-micro relation). These macro/micro relations are time varying and pose special challenges for news analysis.In this article we present a visual analytics system for news streams which can bring multiple attributes of the news articles and the macro/micro relations between news streams and keywords into one coherent analytical context, all the while conveying the dynamic natures of news streams. We introduce a new visualization primitive called TextWheel which consists of one or multiple keyword wheels, a document transportation belt, and a dynamic system which connects the wheels and belt. By observing the TextWheel and its content changes, some interesting patterns can be detected. We use our system to analyze several news corpora related to some major companies and the results demonstrate the high potential of our method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {17},
keywords = {macro-micro relation, text visualization, Document analysis}
}

@article{10.1145/2089094.2089095,
author = {Liu, Shixia and Zhou, Michelle X. and Carenini, Giuseppe and Qu, Huamin},
title = {Introduction to the Special Section on Intelligent Visual Interfaces for Text Analysis},
year = {2012},
issue_date = {February 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/2089094.2089095},
doi = {10.1145/2089094.2089095},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {19},
numpages = {3}
}

@article{10.1145/2036264.2036282,
author = {Sukthankar, Gita and Sycara, Katia},
title = {Activity Recognition for Dynamic Multi-Agent Teams},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036282},
doi = {10.1145/2036264.2036282},
abstract = {This article addresses the problem of activity recognition for dynamic, physically embodied agent teams. We define team activity recognition as the process of identifying team behaviors from traces of agent positions over time; for many physical domains, military or athletic, coordinated team behaviors create distinctive spatio-temporal patterns that can be used to identify low-level action sequences. This article focuses on the novel problem of recovering agent-to-team assignments for complex team tasks where team composition, the mapping of agents into teams, changes over time. We suggest two methods for improving the computational efficiency of the multi-agent plan recognition process in these cases of changing team composition; our proposed approach is robust to sensor observation noise and errors in behavior classification.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {18},
numpages = {24},
keywords = {teamwork, plan recognition, Activity recognition, multi-agent systems}
}

@article{10.1145/2036264.2036281,
author = {Liao, Zhen and Jiang, Daxin and Chen, Enhong and Pei, Jian and Cao, Huanhuan and Li, Hang},
title = {Mining Concept Sequences from Large-Scale Search Logs for Context-Aware Query Suggestion},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036281},
doi = {10.1145/2036264.2036281},
abstract = {Query suggestion plays an important role in improving usability of search engines. Although some recently proposed methods provide query suggestions by mining query patterns from search logs, none of them models the immediately preceding queries as context systematically, and uses context information effectively in query suggestions. Context-aware query suggestion is challenging in both modeling context and scaling up query suggestion using context. In this article, we propose a novel context-aware query suggestion approach. To tackle the challenges, our approach consists of two stages. In the first, offline model-learning stage, to address data sparseness, queries are summarized into concepts by clustering a click-through bipartite. A concept sequence suffix tree is then constructed from session data as a context-aware query suggestion model. In the second, online query suggestion stage, a user’s search context is captured by mapping the query sequence submitted by the user to a sequence of concepts. By looking up the context in the concept sequence suffix tree, we suggest to the user context-aware queries. We test our approach on large-scale search logs of a commercial search engine containing 4.0 billion Web queries, 5.9 billion clicks, and 1.87 billion search sessions. The experimental results clearly show that our approach outperforms three baseline methods in both coverage and quality of suggestions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {17},
numpages = {40},
keywords = {context-aware, session data, Query suggestion, click-through data}
}

@article{10.1145/2036264.2036280,
author = {Liu, Zhanyi and Wang, Haifeng and Wu, Hua and Li, Sheng},
title = {Two-Word Collocation Extraction Using Monolingual Word Alignment Method},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036280},
doi = {10.1145/2036264.2036280},
abstract = {Statistical bilingual word alignment has been well studied in the field of machine translation. This article adapts the bilingual word alignment algorithm into a monolingual scenario to extract collocations from monolingual corpus, based on the fact that the words in a collocation tend to co-occur in similar contexts as in bilingual word alignment. First, the monolingual corpus is replicated to generate a parallel corpus, in which each sentence pair consists of two identical sentences. Next, the monolingual word alignment algorithm is employed to align potentially collocated words. Finally, the aligned word pairs are ranked according to the alignment scores and candidates with higher scores are extracted as collocations. We conducted experiments on Chinese and English corpora respectively. Compared to previous approaches that use association measures to extract collocations from co-occurrence word pairs within a given window, our method achieves higher precision and recall. According to human evaluation, our method achieves precisions of 62% on a Chinese corpus and 64% on an English corpus. In particular, we can extract collocations with longer spans, achieving a higher precision of 83% on the long-span (&gt; 6 words) Chinese collocations.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {16},
numpages = {29},
keywords = {statistical word alignment, Collocation extraction}
}

@article{10.1145/2036264.2036279,
author = {Tang, Lei and Wang, Xufei and Liu, Huan},
title = {Group Profiling for Understanding Social Structures},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036279},
doi = {10.1145/2036264.2036279},
abstract = {The prolific use of participatory Web and social networking sites is reshaping the ways in which people interact with one another. It has become a vital part of human social life in both the developed and developing world. People sharing certain similarities or affiliates tend to form communities within social media. At the same time, they participate in various online activities: content sharing, tagging, posting status updates, etc. These diverse activities leave behind traces of their social life, providing clues to understand changing social structures. A large body of existing work focuses on extracting cohesive groups based on network topology. But little attention is paid to understanding the changing social structures. In order to help explain the formation of a group, we explore different group-profiling strategies to construct descriptions of a group. This research can assist network navigation, visualization, and analysis, as well as monitoring and tracking the ebbs and tides of different groups in evolving networks. By exploiting information collected from real-world social media sites, extensive experiments are conducted to evaluate group-profiling results. The pros and cons of different group-profiling strategies are analyzed with concrete examples. We also show some potential applications based on group profiling. Interesting findings with discussions are reported.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {15},
numpages = {25},
keywords = {social media, Group profiling, community, social structure, group formation}
}

@article{10.1145/2036264.2036278,
author = {Anagnostopoulos, Aris and Broder, Andrei Z. and Gabrilovich, Evgeniy and Josifovski, Vanja and Riedel, Lance},
title = {Web Page Summarization for Just-in-Time Contextual Advertising},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036278},
doi = {10.1145/2036264.2036278},
abstract = {Contextual advertising is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, often ads need to be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low relevance or high latency or high load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time.Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 6% fraction of the page text can sacrifice only 1%--3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500--600 bytes to the usual request. We also compared our summarization approach, which is based on structural properties of the HTML content of the page, with a more principled one based on one of the standard text summarization tools (MEAD), and found their performance to be comparable.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {14},
numpages = {32},
keywords = {Text classification, text summarization}
}

@article{10.1145/2036264.2036277,
author = {Prettenhofer, Peter and Stein, Benno},
title = {Cross-Lingual Adaptation Using Structural Correspondence Learning},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036277},
doi = {10.1145/2036264.2036277},
abstract = {Cross-lingual adaptation is a special case of domain adaptation and refers to the transfer of classification knowledge between two languages. In this article we describe an extension of Structural Correspondence Learning (SCL), a recently proposed algorithm for domain adaptation, for cross-lingual adaptation in the context of text classification. The proposed method uses unlabeled documents from both languages, along with a word translation oracle, to induce a cross-lingual representation that enables the transfer of classification knowledge from the source to the target language. The main advantages of this method over existing methods are resource efficiency and task specificity.We conduct experiments in the area of cross-language topic and sentiment classification involving English as source language and German, French, and Japanese as target languages. The results show a significant improvement of the proposed method over a machine translation baseline, reducing the relative error due to cross-lingual adaptation by an average of 30% (topic classification) and 59% (sentiment classification). We further report on empirical analyses that reveal insights into the use of unlabeled data, the sensitivity with respect to important hyperparameters, and the nature of the induced cross-lingual word correspondences.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {13},
numpages = {22},
keywords = {cross-lingual adaptation, structural correspondence learning, Cross-language text classification}
}

@article{10.1145/2036264.2036276,
author = {Wang, Jingdong and Hua, Xian-Sheng},
title = {Interactive Image Search by Color Map},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036276},
doi = {10.1145/2036264.2036276},
abstract = {The availability of large-scale images from the Internet has made the research on image search attract a lot of attention. Text-based image search engines, for example, Google/Microsoft Bing/Yahoo! image search engines using the surrounding text, have been developed and widely used. However, they suffer from an inability to search image content. In this article, we present an interactive image search system, image search by color map, which can be applied to, but not limited to, enhance text-based image search. This system enables users to indicate how the colors are spatially distributed in the desired images, by scribbling a few color strokes, or dragging an image and highlighting a few regions of interest in an intuitive way. In contrast to the conventional sketch-based image retrieval techniques, our system searches images based on colors rather than shapes, and we, technically, propose a simple but effective scheme to mine the latent search intention from the user’s input, and exploit the dominant color filter strategy to make our system more efficient. We integrate our system to existing Web image search engines to demonstrate its superior performance over text-based image search. The user study shows that our system can indeed help users conveniently find desired images.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {12},
numpages = {23},
keywords = {color map, intention map, Image search}
}

@article{10.1145/2036264.2036275,
author = {Jiang, Yingying and Tian, Feng and Zhang, Xiaolong (Luke) and Dai, Guozhong and Wang, Hongan},
title = {Understanding, Manipulating and Searching Hand-Drawn Concept Maps},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036275},
doi = {10.1145/2036264.2036275},
abstract = {Concept maps are an important tool to organize, represent, and share knowledge. Building a concept map involves creating text-based concepts and specifying their relationships with line-based links. Current concept map tools usually impose specific task structures for text and link construction, and may increase cognitive burden to generate and interact with concept maps. While pen-based devices (e.g., tablet PCs) offer users more freedom in drawing concept maps with a pen or stylus more naturally, the support for hand-drawn concept map creation and manipulation is still limited, largely due to the lack of methods to recognize the components and structures of hand-drawn concept maps. This article proposes a method to understand hand-drawn concept maps. Our algorithm can extract node blocks, or concept blocks, and link blocks of a hand-drawn concept map by combining dynamic programming and graph partitioning, recognize the text content of each concept node, and build a concept-map structure by relating concepts and links. We also design an algorithm for concept map retrieval based on hand-drawn queries. With our algorithms, we introduce structure-based intelligent manipulation techniques and ink-based retrieval techniques to support the management and modification of hand-drawn concept maps. Results from our evaluation study show high structure recognition accuracy in real time of our method, and good usability of intelligent manipulation and retrieval techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {11},
numpages = {21},
keywords = {intelligent manipulation, retrieval, Hand-drawn concept map, recognition}
}

@article{10.1145/2036264.2036274,
author = {Cioffi-Revilla, Claudio and Rogers, J. Daniel and Hailegiorgis, Atesmachew},
title = {Geographic Information Systems and Spatial Agent-Based Model Simulations for Sustainable Development},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036274},
doi = {10.1145/2036264.2036274},
abstract = {In recent years the interdisciplinary field of Computational Social Science has developed theory and methodologies for building spatial Agent-Based Social Simulation (ABSS) models of human societies that are situated in ecosystems with land cover and climate. This article explains the needs and demand for Geographic Information Systems (GIS) in these types of agent-based models, with an emphasis on models applied to Eastern Africa and Inner Asia and relevance for understanding and analyzing development issues. The models are implemented with the MASON (Multi-Agent Simulator Of Networks and Neighborhoods) system, an open-source simulation environment in the Java language and suitable for developing ABSS models with GIS for representing spatial features.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {10},
numpages = {11},
keywords = {Eastern Africa, Inner Asia, Geographic Information Systems (GIS), Spatial Agent-based Modeling (ABM), computational social science, Multi-agent Simulator of Networks and Neighborhoods (MASON), Multi-agent Systems (MAS)}
}

@article{10.1145/2036264.2036273,
author = {Vu, Thuc and Shoham, Yoav},
title = {Fair Seeding in Knockout Tournaments},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036273},
doi = {10.1145/2036264.2036273},
abstract = {We investigated the existence of fair seeding in knockout tournaments. We define two fairness criteria, both adapted from the literature: envy-freeness and order preservation. We show how to achieve the first criterion in tournaments whose structure is unconstrained, and prove an impossibility result for balanced tournaments. For the second criterion we have a similar result for unconstrained tournaments, but not for the balanced case. We provide instead a heuristic algorithm which we show through experiments to be efficient and effective. This suggests that the criterion is achievable also in balanced tournaments. However, we prove that it again becomes impossible to achieve when we add a weak condition guarding against the phenomenon of tournament dropout.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {9},
numpages = {17},
keywords = {heuristic algorithm, Knockout tournament}
}

@article{10.1145/2036264.2036272,
author = {Gal, Ya’akov and Kraus, Sarit and Gelfand, Michele and Khashan, Hilal and Salmon, Elizabeth},
title = {An Adaptive Agent for Negotiating with People in Different Cultures},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036272},
doi = {10.1145/2036264.2036272},
abstract = {The rapid dissemination of technology such as the Internet across geographical and ethnic lines is opening up opportunities for computer agents to negotiate with people of diverse cultural and organizational affiliations. To negotiate proficiently with people in different cultures, agents need to be able to adapt to the way behavioral traits of other participants change over time. This article describes a new agent for repeated bilateral negotiation that was designed to model and adapt its behavior to the individual traits exhibited by its negotiation partner. The agent’s decision-making model combined a social utility function that represented the behavioral traits of the other participant, as well as a rule-based mechanism that used the utility function to make decisions in the negotiation process. The agent was deployed in a strategic setting in which both participants needed to complete their individual tasks by reaching agreements and exchanging resources, the number of negotiation rounds was not fixed in advance and agreements were not binding. The agent negotiated with human subjects in the United States and Lebanon in situations that varied the dependency relationships between participants at the onset of negotiation. There was no prior data available about the way people would respond to different negotiation strategies in these two countries. Results showed that the agent was able to adopt a different negotiation strategy to each country. Its average performance across both countries was equal to that of people. However, the agent outperformed people in the United States, because it learned to make offers that were likely to be accepted by people, while being more beneficial to the agent than to people. In contrast, the agent was outperformed by people in Lebanon, because it adopted a high reliability measure which allowed people to take advantage of it. These results provide insight for human-computer agent designers in the types of multicultural settings that we considered, showing that adaptation is a viable approach towards the design of computer agents to negotiate with people when there is no prior data of their behavior.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {8},
numpages = {24},
keywords = {Human-agent decision making, cultural modeling}
}

@article{10.1145/2036264.2036271,
author = {Shakarian, Paulo and Subrahmanian, V. S. and Sapino, Maria Luisa},
title = {GAPs: Geospatial Abduction Problems},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036271},
doi = {10.1145/2036264.2036271},
abstract = {There are many applications where we observe various phenomena in space (e.g., locations of victims of a serial killer), and where we want to infer “partner” locations (e.g., the location where the killer lives) that are geospatially related to the observed phenomena. In this article, we define geospatial abduction problems (GAPs for short). We analyze the complexity of GAPs, develop exact and approximate algorithms (often with approximation guarantees) for these problems together with analyses of these algorithms, and develop a prototype implementation of our GAP framework. We demonstrate accuracy of our algorithms on a real world data set consisting of insurgent IED (improvised explosive device) attacks against U.S. forces in Iraq (the observations were the locations of the attacks, while the “partner” locations we were trying to infer were the locations of IED weapons caches).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {7},
numpages = {27},
keywords = {complexity analysis, Abduction, heuristic algorithms}
}

@article{10.1145/2036264.2036270,
author = {Liu, Huan and Nau, Dana},
title = {Introduction},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036270},
doi = {10.1145/2036264.2036270},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {6},
numpages = {1}
}

@article{10.1145/2036264.2036269,
author = {Zhao, Shiwan and Zhou, Michelle X. and Zhang, Xiatian and Yuan, Quan and Zheng, Wentao and Fu, Rongyao},
title = {Who is Doing What and When: Social Map-Based Recommendation for Content-Centric Social Web Sites},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036269},
doi = {10.1145/2036264.2036269},
abstract = {Content-centric social Web sites, such as discussion forums and blog sites, have flourished during the past several years. These sites often contain overwhelming amounts of information that are also being updated rapidly. To help users locate their interests at such sites (e.g., interesting blogs to read or discussion forums to join), researchers have developed a number of recommendation technologies. However, it is difficult to make effective recommendations for new users (a.k.a. the cold start problem) due to a lack of user information (e.g., preferences and interests). Furthermore, the complexity of recommendation algorithms often prevents users from comprehending let alone trusting the recommended results. To tackle these above two challenges, we are building a social map-based recommender system called Pharos. A social map summarizes users’ content-related social behavior over time (e.g., reading, writing, and commenting behavior during the past week) as a set of latent communities. For a given time interval, each community is characterized by the theme of the content being discussed and the key people involved. By discovering, ranking, and displaying the most popular latent communities at different time intervals, Pharos creates a time-sensitive, visual social map of a Web site. This enables new users to obtain a quick overview of the site, alleviating the cold start problem. Furthermore, we use the social map as a context to help explain Pharos-recommended content and people. Users can also interactively explore the social map to locate the content in which they are interested or people that are not being explicitly recommended, compensating for the imperfections in the recommendation algorithms. We have developed several Pharos applications, one of which is deployed within our company. Our preliminary evaluation of the deployed application shows the usefulness of Pharos.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {5},
numpages = {23},
keywords = {visual explanation, social map, Recommender systems, trust, cold start}
}

@article{10.1145/2036264.2036268,
author = {McNally, Kevin and O’Mahony, Michael P. and Coyle, Maurice and Briggs, Peter and Smyth, Barry},
title = {A Case Study of Collaboration and Reputation in Social Web Search},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036268},
doi = {10.1145/2036264.2036268},
abstract = {Although collaborative searching is not supported by mainstream search engines, recent research has highlighted the inherently collaborative nature of many Web search tasks. In this article, we describe HeyStaks, a collaborative Web search framework that is designed to complement mainstream search engines. At search time, HeyStaks learns from the search activities of other users and leverages this information to generate recommendations based on results that others have found relevant for similar searches. The key contribution of this article is to extend the HeyStaks social search model by considering the search expertise, or reputation, of HeyStaks users and using this information to enhance the result recommendation process. In particular, we propose a reputation model for HeyStaks users that utilise the implicit collaboration events that take place between users as recommendations are made and selected. We describe a live-user trial of HeyStaks that demonstrates the relevance of its core recommendations and the ability of the reputation model to further improve recommendation quality. Our findings indicate that incorporating reputation into the recommendation process further improves the relevance of HeyStaks recommendations by up to 40%.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {reputation, Trust, social search, HeyStaks}
}

@article{10.1145/2036264.2036267,
author = {Vasuki, Vishvas and Natarajan, Nagarajan and Lu, Zhengdong and Savas, Berkant and Dhillon, Inderjit},
title = {Scalable Affiliation Recommendation Using Auxiliary Networks},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036267},
doi = {10.1145/2036264.2036267},
abstract = {Social network analysis has attracted increasing attention in recent years. In many social networks, besides friendship links among users, the phenomenon of users associating themselves with groups or communities is common. Thus, two networks exist simultaneously: the friendship network among users, and the affiliation network between users and groups. In this article, we tackle the affiliation recommendation problem, where the task is to predict or suggest new affiliations between users and communities, given the current state of the friendship and affiliation networks. More generally, affiliations need not be community affiliations---they can be a user’s taste, so affiliation recommendation algorithms have applications beyond community recommendation. In this article, we show that information from the friendship network can indeed be fruitfully exploited in making affiliation recommendations. Using a simple way of combining these networks, we suggest two models of user-community affinity for the purpose of making affiliation recommendations: one based on graph proximity, and another using latent factors to model users and communities. We explore the affiliation recommendation algorithms suggested by these models and evaluate these algorithms on two real-world networks, Orkut and Youtube. In doing so, we motivate and propose a way of evaluating recommenders, by measuring how good the top 50 recommendations are for the average user, and demonstrate the importance of choosing the right evaluation strategy. The algorithms suggested by the graph proximity model turn out to be the most effective. We also introduce scalable versions of these algorithms, and demonstrate their effectiveness. This use of link prediction techniques for the purpose of affiliation recommendation is, to our knowledge, novel.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {3},
numpages = {20},
keywords = {Scalability}
}

@article{10.1145/2036264.2036266,
author = {Lipczak, Marek and Milios, Evangelos},
title = {Efficient Tag Recommendation for Real-Life Data},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036266},
doi = {10.1145/2036264.2036266},
abstract = {Despite all of the advantages of tags as an easy and flexible information management approach, tagging is a cumbersome task. A set of descriptive tags has to be manually entered by users whenever they post a resource. This process can be simplified by the use of tag recommendation systems. Their objective is to suggest potentially useful tags to the user. We present a hybrid tag recommendation system together with a scalable, highly efficient system architecture. The system is able to utilize user feedback to tune its parameters to specific characteristics of the underlying tagging system and adapt the recommendation models to newly added content. The evaluation of the system on six real-life datasets demonstrated the system’s ability to combine tags from various sources (e.g., resource content or tags previously used by the user) to achieve the best quality of recommended tags. It also confirmed the importance of parameter tuning and content adaptation. A series of additional experiments allowed us to better understand the characteristics of the system and tagging datasets and to determine the potential areas for further system development.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {2},
numpages = {21},
keywords = {folksonomies, Tag recommendation, hybrid systems, collaborative tagging, broad folksonomies, narrow folksonomies}
}

@article{10.1145/2036264.2036265,
author = {Guy, Ido and Chen, Li and Zhou, Michelle X.},
title = {Introduction},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/2036264.2036265},
doi = {10.1145/2036264.2036265},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {1},
numpages = {2}
}

@article{10.1145/1989734.1989746,
author = {Haigh, Karen Zita and Yaman, Fusun},
title = {RECYCLE: Learning Looping Workflows from Annotated Traces},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989746},
doi = {10.1145/1989734.1989746},
abstract = {A workflow is a model of a process that systematically describes patterns of activity. Workflows capture a sequence of operations, their enablement conditions, and data flow dependencies among them. It is hard to design a complete and correct workflow from scratch, while it is much easier for humans to demonstrate the solution than to state the solution declaratively.This article presents RECYCLE, our approach to learning workflow models from example demonstration traces. RECYCLE captures control flow, data flow, and enablement conditions of an underlying workflow process. Unlike prior work from workflow mining and AI planning literature, (1) RECYCLE can learn from a single demonstration trace with loops, (2) RECYCLE learns both loop and conditional branch structure, and (3) RECYCLE handles data flow among actions.In this article, we describe the phases of RECYCLE's learning algorithm: substructure analysis and node abstraction. To ground the discussion, we present a simplified flight reservation system with some of the important characteristics of the real domains we worked with. We present some results from a patient transport domain.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {42},
numpages = {32},
keywords = {learning from demonstration, learning from traces, process mining, workflow learning, Hierarchical Task Network learning}
}

@article{10.1145/1989734.1989745,
author = {Reddy, Sudhakar Y. and Frank, Jeremy D. and Iatauro, Michael J. and Boyce, Matthew E. and K\"{u}rkl\"{u}, Elif and Ai-Chang, Mitchell and J\'{o}nsson, Ari K.},
title = {Planning Solar Array Operations on the International Space Station},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989745},
doi = {10.1145/1989734.1989745},
abstract = {Flight controllers manage the orientation and modes of eight large solar arrays that power the International Space Station (ISS). The task requires generating plans that balance complex constraints and preferences. These considerations include context-dependent constraints on viable solar array configurations, temporal limits on transitions between configurations, and preferences on which considerations have priority. The Solar Array Constraint Engine (SACE) treats this operations planning problem as a sequence of tractable constrained optimization problems. SACE uses constraint management and automated planning capabilities to reason about the constraints, to find optimal array configurations subject to these constraints and solution preferences, and to automatically generate solar array operations plans. SACE further provides flight controllers with real-time situational awareness and what-if analysis capabilities. SACE is built on the Extensible Universal Remote Operations Planning Architecture (EUROPA) model-based planning system. EUROPA facilitated SACE development by providing model-based planning, built-in constraint reasoning capability, and extensibility. This article formulates the planning problem, explains how EUROPA solves the problem, and provides performance statistics from several planning scenarios. SACE reduces a highly manual process that takes weeks to an automated process that takes tens of minutes.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {41},
numpages = {24},
keywords = {optimization, scheduling, constraint satisfaction, Planning, space mission operations}
}

@article{10.1145/1989734.1989744,
author = {Berry, Pauline M. and Gervasio, Melinda and Peintner, Bart and Yorke-Smith, Neil},
title = {PTIME: Personalized Assistance for Calendaring},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989744},
doi = {10.1145/1989734.1989744},
abstract = {In a world of electronic calendars, the prospect of intelligent, personalized time management assistance seems a plausible and desirable application of AI. PTIME (Personalized Time Management) is a learning cognitive assistant agent that helps users handle email meeting requests, reserve venues, and schedule events. PTIME is designed to unobtrusively learn scheduling preferences, adapting to its user over time. The agent allows its user to flexibly express requirements for new meetings, as they would to an assistant. It interfaces with commercial enterprise calendaring platforms, and it operates seamlessly with users who do not have PTIME. This article overviews the system design and describes the models and technical advances required to satisfy the competing needs of preference modeling and elicitation, constraint reasoning, and machine learning. We further report on a multifaceted evaluation of the perceived usefulness of the system.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {40},
numpages = {22},
keywords = {machine learning, preference modeling, calendaring, Personal assistant agents}
}

@article{10.1145/1989734.1989743,
author = {Ding, Wei and Stepinski, Tomasz F. and Mu, Yang and Bandeira, Lourenco and Ricardo, Ricardo and Wu, Youxi and Lu, Zhenyu and Cao, Tianyu and Wu, Xindong},
title = {Subkilometer Crater Discovery with Boosting and Transfer Learning},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989743},
doi = {10.1145/1989734.1989743},
abstract = {Counting craters in remotely sensed images is the only tool that provides relative dating of remote planetary surfaces. Surveying craters requires counting a large amount of small subkilometer craters, which calls for highly efficient automatic crater detection. In this article, we present an integrated framework on autodetection of subkilometer craters with boosting and transfer learning. The framework contains three key components. First, we utilize mathematical morphology to efficiently identify crater candidates, the regions of an image that can potentially contain craters. Only those regions occupying relatively small portions of the original image are the subjects of further processing. Second, we extract and select image texture features, in combination with supervised boosting ensemble learning algorithms, to accurately classify crater candidates into craters and noncraters. Third, we integrate transfer learning into boosting, to enhance detection performance in the regions where surface morphology differs from what is characterized by the training set. Our framework is evaluated on a large test image of 37,500 \texttimes{} 56,250 m2 on Mars, which exhibits a heavily cratered Martian terrain characterized by nonuniform surface morphology. Empirical studies demonstrate that the proposed crater detection framework can achieve an F1 score above 0.85, a significant improvement over the other crater detection algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {39},
numpages = {22},
keywords = {planetary and space science, feature selection, transfer learning, spatial data mining, Classification}
}

@article{10.1145/1989734.1989742,
author = {Toole, Jameson L. and Eagle, Nathan and Plotkin, Joshua B.},
title = {Spatiotemporal Correlations in Criminal Offense Records},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989742},
doi = {10.1145/1989734.1989742},
abstract = {With the increased availability of rich behavioral datasets, we present a novel application of tools to analyze this information. Using criminal offense records as an example, we employ cross-correlation measures, eigenvalue spectrum analysis, and results from random matrix theory to identify spatiotemporal patterns on multiple scales. With these techniques, we show that most significant correlation exists on the time scale of weeks and identify clusters of neighborhoods whose crime rates are affected simultaneously by external forces.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {38},
numpages = {18},
keywords = {engineering social systems, Big data, computational social science, computational sustainability, criminology}
}

@article{10.1145/1989734.1989741,
author = {Li, Zhenhui and Han, Jiawei and Ji, Ming and Tang, Lu-An and Yu, Yintao and Ding, Bolin and Lee, Jae-Gil and Kays, Roland},
title = {MoveMine: Mining Moving Object Data for Discovery of Animal Movement Patterns},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989741},
doi = {10.1145/1989734.1989741},
abstract = {With the maturity and wide availability of GPS, wireless, telecommunication, and Web technologies, massive amounts of object movement data have been collected from various moving object targets, such as animals, mobile devices, vehicles, and climate radars. Analyzing such data has deep implications in many applications, such as, ecological study, traffic control, mobile communication management, and climatological forecast. In this article, we focus our study on animal movement data analysis and examine advanced data mining methods for discovery of various animal movement patterns. In particular, we introduce a moving object data mining system, MoveMine, which integrates multiple data mining functions, including sophisticated pattern mining and trajectory analysis. In this system, two interesting moving object pattern mining functions are newly developed: (1) periodic behavior mining and (2) swarm pattern mining. For mining periodic behaviors, a reference location-based method is developed, which first detects the reference locations, discovers the periods in complex movements, and then finds periodic patterns by hierarchical clustering. For mining swarm patterns, an efficient method is developed to uncover flexible moving object clusters by relaxing the popularly-enforced collective movement constraints.In the MoveMine system, a set of commonly used moving object mining functions are built and a user-friendly interface is provided to facilitate interactive exploration of moving object data mining and flexible tuning of the mining constraints and parameters. MoveMine has been tested on multiple kinds of real datasets, especially for MoveBank applications and other moving object data analysis. The system will benefit scientists and other users to carry out versatile analysis tasks to analyze object movement regularities and anomalies. Moreover, it will benefit researchers to realize the importance and limitations of current techniques and promote future studies on moving object data mining. As expected, a mastery of animal movement patterns and trends will improve our understanding of the interactions between and the changes of the animal world and the ecosystem and therefore help ensure the sustainability of our ecosystem.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {37},
numpages = {32},
keywords = {swarm pattern, pattern mining, computational sustainability, Moving objects, periodic behavior}
}

@article{10.1145/1989734.1989740,
author = {Mithal, Varun and Garg, Ashish and Boriah, Shyam and Steinbach, Michael and Kumar, Vipin and Potter, Christopher and Klooster, Steven and Castilla-Rubio, Juan Carlos},
title = {Monitoring Global Forest Cover Using Data Mining},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989740},
doi = {10.1145/1989734.1989740},
abstract = {Forests are a critical component of the planet's ecosystem. Unfortunately, there has been significant degradation in forest cover over recent decades as a result of logging, conversion to crop, plantation, and pasture land, or disasters (natural or man made) such as forest fires, floods, and hurricanes. As a result, significant attention is being given to the sustainable use of forests. A key to effective forest management is quantifiable knowledge about changes in forest cover. This requires identification and characterization of changes and the discovery of the relationship between these changes and natural and anthropogenic variables. In this article, we present our preliminary efforts and achievements in addressing some of these tasks along with the challenges and opportunities that need to be addressed in the future. At a higher level, our goal is to provide an overview of the exciting opportunities and challenges in developing and applying data mining approaches to provide critical information for forest and land use management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {36},
numpages = {24},
keywords = {forest cover change, Computational sustainability, land change, remote sensing}
}

@article{10.1145/1989734.1989739,
author = {Ramchurn, Sarvapali D. and Vytelingum, Perukrishnen and Rogers, Alex and Jennings, Nicholas R.},
title = {Agent-Based Homeostatic Control for Green Energy in the Smart Grid},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989739},
doi = {10.1145/1989734.1989739},
abstract = {With dwindling nonrenewable energy reserves and the adverse effects of climate change, the development of the smart electricity grid is seen as key to solving global energy security issues and to reducing carbon emissions. In this respect, there is a growing need to integrate renewable (or green) energy sources in the grid. However, the intermittency of these energy sources requires that demand must also be made more responsive to changes in supply, and a number of smart grid technologies are being developed, such as high-capacity batteries and smart meters for the home, to enable consumers to be more responsive to conditions on the grid in real time. Traditional solutions based on these technologies, however, tend to ignore the fact that individual consumers will behave in such a way that best satisfies their own preferences to use or store energy (as opposed to that of the supplier or the grid operator). Hence, in practice, it is unclear how these solutions will cope with large numbers of consumers using their devices in this way. Against this background, in this article, we develop novel control mechanisms based on the use of autonomous agents to better incorporate consumer preferences in managing demand. These agents, residing on consumers' smart meters, can both communicate with the grid and optimize their owner's energy consumption to satisfy their preferences. More specifically, we provide a novel control mechanism that models and controls a system comprising of a green energy supplier operating within the grid and a number of individual homes (each possibly owning a storage device). This control mechanism is based on the concept of homeostasis whereby control signals are sent to individual components of a system, based on their continuous feedback, in order to change their state so that the system may reach a stable equilibrium. Thus, we define a new carbon-based pricing mechanism for this green energy supplier that takes advantage of carbon-intensity signals available on the Internet in order to provide real-time pricing. The pricing scheme is designed in such a way that it can be readily implemented using existing communication technologies and is easily understandable by consumers. Building upon this, we develop new control signals that the supplier can use to incentivize agents to shift demand (using their storage device) to times when green energy is available. Moreover, we show how these signals can be adapted according to changes in supply and to various degrees of penetration of storage in the system. We empirically evaluate our system and show that, when all homes are equipped with storage devices, the supplier can significantly reduce its reliance on other carbon-emitting power sources to cater for its own shortfalls. By so doing, the supplier reduces the carbon emission of the system by up to 25% while the consumer reduces its costs by up to 14.5%. Finally, we demonstrate that our homeostatic control mechanism is not sensitive to small prediction errors and the supplier is incentivized to accurately predict its green production to minimize costs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {35},
numpages = {28},
keywords = {agentbased control, Agents, electricity, computational sustainability, multiagent systems}
}

@article{10.1145/1989734.1989738,
author = {Patnaik, Debprakash and Marwah, Manish and Sharma, Ratnesh K. and Ramakrishnan, Naren},
title = {Temporal Data Mining Approaches for Sustainable Chiller Management in Data Centers},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989738},
doi = {10.1145/1989734.1989738},
abstract = {Practically every large IT organization hosts data centers---a mix of computing elements, storage systems, networking, power, and cooling infrastructure---operated either in-house or outsourced to major vendors. A significant element of modern data centers is their cooling infrastructure, whose efficient and sustainable operation is a key ingredient to the “always-on” capability of data centers. We describe the design and implementation of CAMAS (Chiller Advisory and MAnagement System), a temporal data mining solution to mine and manage chiller installations. CAMAS embodies a set of algorithms for processing multivariate time-series data and characterizes sustainability measures of the patterns mined. We demonstrate three key ingredients of CAMAS---motif mining, association analysis, and dynamic Bayesian network inference---that help bridge the gap between low-level, raw, sensor streams, and the high-level operating regions and features needed for an operator to efficiently manage the data center. The effectiveness of CAMAS is demonstrated by its application to a real-life production data center managed by HP.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {34},
numpages = {29},
keywords = {chillers, Data centers, frequent episodes, clustering, motifs, sustainability}
}

@article{10.1145/1989734.1989737,
author = {Cattafi, Massimiliano and Gavanelli, Marco and Milano, Michela and Cagnoli, Paolo},
title = {Sustainable Biomass Power Plant Location in the Italian Emilia-Romagna Region},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989737},
doi = {10.1145/1989734.1989737},
abstract = {Biomass power plants are very promising for reducing carbon oxides emissions, because they provide energy with a carbon-neutral process. Biomass comes from trees and vegetables, so they provide a renewable type of energy. However, biomass plants location, along with their provisioning basins, are heavily regulated by economical aspects, often without careful consideration of their environmental footprint. For example, some Italian biomass plants import from overseas palm-tree oil that is economically convenient. However, the energy consumed for the oil transportation is definitely greater than the energy produced by the palm-tree oil burning. In this way biomass power plants turn out to be environmentally inefficient, even if they produce renewable energy.We propose an Integer Linear Programming approach for defining the energy and cost-efficient biomass plant location along with the corresponding provisioning basin. In addition, the model enables to evaluate existing plants and their energy and cost efficiency. Our study is based on real data gathered in the Emilia-Romagna region of Italy.Finally, this optimization tool is just a small part of a wider perspective that is aimed to define decision support tools for the improvement of regional planning and its precise strategic environmental assessment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {33},
numpages = {19},
keywords = {Computational sustainability, facility location}
}

@article{10.1145/1989734.1989736,
author = {Krause, Andreas and Guestrin, Carlos},
title = {Submodularity and Its Applications in Optimized Information Gathering},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989736},
doi = {10.1145/1989734.1989736},
abstract = {Where should we place sensors to efficiently monitor natural drinking water resources for contamination? Which blogs should we read to learn about the biggest stories on the Web? These problems share a fundamental challenge: How can we obtain the most useful information about the state of the world, at minimum cost?Such information gathering, or active learning, problems are typically NP-hard, and were commonly addressed using heuristics without theoretical guarantees about the solution quality. In this article, we describe algorithms which efficiently find provably near-optimal solutions to large, complex information gathering problems. Our algorithms exploit submodularity, an intuitive notion of diminishing returns common to many sensing problems: the more sensors we have already deployed, the less we learn by placing another sensor. In addition to identifying the most informative sensing locations, our algorithms can handle more challenging settings, where sensors need to be able to reliably communicate over lossy links, where mobile robots are used for collecting data, or where solutions need to be robust against adversaries and sensor failures.We also present results applying our algorithms to several real-world sensing tasks, including environmental monitoring using robotic sensors, activity recognition using a built sensing chair, a sensor placement challenge, and deciding which blogs to read on the Web.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {32},
numpages = {20},
keywords = {computational sustainability, blogs, information overload, environmental monitoring, active learning, submodular functions, Information gathering, sensor networks}
}

@article{10.1145/1989734.1989735,
author = {Gomes, Carla and Yang, Qiang},
title = {Introduction to Special Issue on Computational Sustainability},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/1989734.1989735},
doi = {10.1145/1989734.1989735},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jul,
articleno = {31},
numpages = {2}
}

@article{10.1145/1961189.1961202,
author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
title = {Learning to Detect Malicious URLs},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961202},
doi = {10.1145/1961189.1961202},
abstract = {Malicious Web sites are a cornerstone of Internet criminal activities. The dangers of these sites have created a demand for safeguards that protect end-users from visiting them. This article explores how to detect malicious Web sites from the lexical and host-based features of their URLs. We show that this problem lends itself naturally to modern algorithms for online learning. Online algorithms not only process large numbers of URLs more efficiently than batch algorithms, they also adapt more quickly to new features in the continuously evolving distribution of malicious URLs. We develop a real-time system for gathering URL features and pair it with a real-time feed of labeled URLs from a large Web mail provider. From these features and labels, we are able to train an online classifier that detects malicious Web sites with 99% accuracy over a balanced dataset.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {30},
numpages = {24},
keywords = {malicious Web sites, Online learning}
}

@article{10.1145/1961189.1961201,
author = {Ma, Hao and King, Irwin and Lyu, Michael R.},
title = {Learning to Recommend with Explicit and Implicit Social Relations},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961201},
doi = {10.1145/1961189.1961201},
abstract = {Recommender systems have been well studied and developed, both in academia and in industry recently. However, traditional recommender systems assume that all the users are independent and identically distributed; this assumption ignores the connections among users, which is not consistent with the real-world observations where we always turn to our trusted friends for recommendations. Aiming at modeling recommender systems more accurately and realistically, we propose a novel probabilistic factor analysis framework which naturally fuses the users' tastes and their trusted friends' favors together. The proposed framework is quite general, and it can also be applied to pure user-item rating matrix even if we do not have explicit social trust information among users. In this framework, we coin the term social trust ensemble to represent the formulation of the social trust restrictions on the recommender systems. The complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations, while the experimental results show that our method outperforms state-of-the-art approaches.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {29},
numpages = {19},
keywords = {social trust ensemble, Recommender systems, social network, matrix factorization}
}

@article{10.1145/1961189.1961200,
author = {Gasso, Gilles and Pappaioannou, Aristidis and Spivak, Marina and Bottou, L\'{e}on},
title = {Batch and Online Learning Algorithms for Nonconvex Neyman-Pearson Classification},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961200},
doi = {10.1145/1961189.1961200},
abstract = {We describe and evaluate two algorithms for Neyman-Pearson (NP) classification problem which has been recently shown to be of a particular importance for bipartite ranking problems. NP classification is a nonconvex problem involving a constraint on false negatives rate. We investigated batch algorithm based on DC programming and stochastic gradient method well suited for large-scale datasets. Empirical evidences illustrate the potential of the proposed methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {28},
numpages = {19},
keywords = {online learning, DC algorithm, nonconvex SVM, Neyman-Pearson}
}

@article{10.1145/1961189.1961199,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
title = {LIBSVM: A Library for Support Vector Machines},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961199},
doi = {10.1145/1961189.1961199},
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {27},
numpages = {27},
keywords = {Classification LIBSVM optimization regression support vector machines SVM}
}

@article{10.1145/1961189.1961198,
author = {Liu, Zhiyuan and Zhang, Yuzhou and Chang, Edward Y. and Sun, Maosong},
title = {PLDA+: Parallel Latent Dirichlet Allocation with Data Placement and Pipeline Processing},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961198},
doi = {10.1145/1961189.1961198},
abstract = {Previous methods of distributed Gibbs sampling for LDA run into either memory or communication bottlenecks. To improve scalability, we propose four strategies: data placement, pipeline processing, word bundling, and priority-based scheduling. Experiments show that our strategies significantly reduce the unparallelizable communication bottleneck and achieve good load balancing, and hence improve scalability of LDA.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {26},
numpages = {18},
keywords = {latent Dirichlet allocation, Gibbs sampling, Topic models, distributed parallel computations}
}

@article{10.1145/1961189.1961197,
author = {Hsu, Chun-Nan},
title = {Introduction to Special Issue on Large-Scale Machine Learning},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961197},
doi = {10.1145/1961189.1961197},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {25},
numpages = {2}
}

@article{10.1145/1961189.1961196,
author = {Ge, Yong and Xiong, Hui and Zhou, Wenjun and Li, Siming and Sahoo, Ramendra},
title = {Multifocal Learning for Customer Problem Analysis},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961196},
doi = {10.1145/1961189.1961196},
abstract = {In this study, we formalize a multifocal learning problem, where training data are partitioned into several different focal groups and the prediction model will be learned within each focal group. The multifocal learning problem is motivated by numerous real-world learning applications. For instance, for the same type of problems encountered in a customer service center, the problem descriptions from different customers can be quite different. Experienced customers usually give more precise and focused descriptions about the problem. In contrast, inexperienced customers usually provide diverse descriptions. In this case, the examples from the same class in the training data can be naturally in different focal groups. Therefore, it is necessary to identify those natural focal groups and exploit them for learning at different focuses. Along this line, the key development challenge is how to identify those focal groups in the training data. As a case study, we exploit multifocal learning for profiling customer problems. Also, we provide an empirical study about how the performance of multifocal learning is affected by the quality of focal groups. The results on real-world customer problem logs show that multifocal learning can significantly boost the performance of many existing classification algorithms, such as Support Vector Machines (SVMs), for classifying customer problems and there is strong correlation between the quality of focal groups and the learning performance.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {24},
numpages = {22},
keywords = {customer service support, Multi-focal learning}
}

@article{10.1145/1961189.1961195,
author = {Zhang, Richong and Tran, Thomas},
title = {A Helpfulness Modeling Framework for Electronic Word-of-Mouth on Consumer Opinion Platforms},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961195},
doi = {10.1145/1961189.1961195},
abstract = {Electronic Word-of-Mouth (eWOM) is growing exponentially with the rapid development of electronic commerce. As a result, consumers are increasingly crowded by a huge amount of eWOM contents and therefore there is a need to automatically recommend eWOM contents that are helpful to them. Existing helpfulness assessment approaches that deterministically estimate the helpfulness of eWOM contents lack a generative formulation and are limited to the training set that has been voted by many readers. This article presents a rigorous probabilistic framework for inferring the “helpfulness” of eWOM contents which can build a “helpfulness” model from a low number of votes on eWOM contents. Furthermore, we introduce a measurement, “helpfulness” bias, as the benchmark for the “helpfulness” of eWOM documents. We also propose a model that exploits the graphical model and expectation maximization algorithm, under this probabilistic framework, to demonstrate the versatility of our framework. Our algorithm is compared experimentally to other existing helpfulness discovering algorithms and the experimental results show that our framework can effectively model the helpfulness of eWOM contents better than other approaches, and therefore indicate the capability of our framework to recommend helpful eWOMs to potential consumers.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {23},
numpages = {18},
keywords = {recommender systems, Ranking, online product reviews}
}

@article{10.1145/1961189.1961194,
author = {Bonchi, Francesco and Castillo, Carlos and Gionis, Aristides and Jaimes, Alejandro},
title = {Social Network Analysis and Mining for Business Applications},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961194},
doi = {10.1145/1961189.1961194},
abstract = {Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {22},
numpages = {37},
keywords = {networks dynamics and evolution, community structure, influence propagation, viral marketing, Social networks, expert finding}
}

@article{10.1145/1961189.1961193,
author = {Li, Bin and Hoi, Steven C.H. and Gopalkrishnan, Vivekanand},
title = {CORN: Correlation-Driven Nonparametric Learning Approach for Portfolio Selection},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961193},
doi = {10.1145/1961189.1961193},
abstract = {Machine learning techniques have been adopted to select portfolios from financial markets in some emerging intelligent business applications. In this article, we propose a novel learning-to-trade algorithm termed CORrelation-driven Nonparametric learning strategy (CORN) for actively trading stocks. CORN effectively exploits statistical relations between stock market windows via a nonparametric learning approach. We evaluate the empirical performance of our algorithm extensively on several large historical and latest real stock markets, and show that it can easily beat both the market index and the best stock in the market substantially (without or with small transaction costs), and also surpass a variety of state-of-the-art techniques significantly.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {21},
numpages = {29},
keywords = {nonparametric learning, Correlation coefficient, online portfolio selection}
}

@article{10.1145/1961189.1961192,
author = {Huang, Szu-Hao and Lai, Shang-Hong and Tai, Shih-Hsien},
title = {A Learning-Based Contrarian Trading Strategy via a Dual-Classifier Model},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961192},
doi = {10.1145/1961189.1961192},
abstract = {Behavioral finance is a relatively new and developing research field which adopts cognitive psychology and emotional bias to explain the inefficient market phenomenon and some irrational trading decisions. Unlike the experts in this field who tried to reason the price anomaly and applied empirical evidence in many different financial markets, we employ the advanced binary classification algorithms, such as AdaBoost and support vector machines, to precisely model the overreaction and strengthen the portfolio compositions of the contrarian trading strategies. The novelty of this article is to discover the financial time-series patterns through a high-dimensional and nonlinear model which is constructed by integrated knowledge of finance and machine learning techniques. We propose a dual-classifier learning framework to select candidate stocks from the past results of original contrarian trading strategies based on the defined learning targets. Three different feature extraction methods, including wavelet transformation, historical return distribution, and various technical indicators, are employed to represent these learning samples in a 381-dimensional financial time-series feature space. Finally, we construct the classifier models with four different learning kernels and prove that the proposed methods could improve the returns dramatically, such as the 3-year return that improved from 26.79% to 53.75%. The experiments also demonstrate significantly higher portfolio selection accuracy, improved from 57.47% to 66.41%, than the original contrarian trading strategy. To sum up, all these experiments show that the proposed method could be extended to an effective trading system in the historical stock prices of the leading U.S. companies of S&amp;P 100 index.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {20},
numpages = {20},
keywords = {classification, Behavioral finance, machine learning}
}

@article{10.1145/1961189.1961191,
author = {Dhar, Vasant},
title = {Prediction in Financial Markets: The Case for Small Disjuncts},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961191},
doi = {10.1145/1961189.1961191},
abstract = {Predictive models in regression and classification problems typically have a single model that covers most, if not all, cases in the data. At the opposite end of the spectrum is a collection of models, each of which covers a very small subset of the decision space. These are referred to as “small disjuncts.” The trade-offs between the two types of models have been well documented. Single models, especially linear ones, are easy to interpret and explain. In contrast, small disjuncts do not provides as clean or as simple an interpretation of the data, and have been shown by several researchers to be responsible for a disproportionately large number of errors when applied to out-of-sample data. This research provides a counterpoint, demonstrating that a portfolio of “simple” small disjuncts provides a credible model for financial market prediction, a problem with a high degree of noise. A related novel contribution of this article is a simple method for measuring the “yield” of a learning system, which is the percentage of in-sample performance that the learned model can be expected to realize on out-of-sample data. Curiously, such a measure is missing from the literature on regression learning algorithms. Pragmatically, the results suggest that for problems characterized by a high degree of noise and lack of a stable knowledge base it makes sense to reconstruct the portfolio of small rules periodically.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {19},
numpages = {22},
keywords = {time-series prediction, Machine learning, financial markets, predictive modeling}
}

@article{10.1145/1961189.1961190,
author = {Ling, Charles X.},
title = {Introduction to Special Issue on Machine Learning for Business Applications},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/1961189.1961190},
doi = {10.1145/1961189.1961190},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {18},
numpages = {2}
}

@article{10.1145/1899412.1899421,
author = {Bhatt, Chidansh and Kankanhalli, Mohan},
title = {Probabilistic Temporal Multimedia Data Mining},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899421},
doi = {10.1145/1899412.1899421},
abstract = {Existing sequence pattern mining techniques assume that the obtained events from event detectors are accurate. However, in reality, event detectors label the events from different modalities with a certain probability over a time-interval. In this article, we consider for the first time Probabilistic Temporal Multimedia (PTM) Event data to discover accurate sequence patterns. PTM event data considers the start time, end time, event label and associated probability for the sequence pattern discovery. As the existing sequence pattern mining techniques cannot work on such realistic data, we have developed a novel framework for performing sequence pattern mining on probabilistic temporal multimedia event data. We perform probability fusion to resolve the redundancy among detected events from different modalities, considering their cross-modal correlation. We propose a novel sequence pattern mining algorithm called Probabilistic Interval based Event Miner (PIE-Miner) for discovering frequent sequence patterns from interval based events. PIE-Miner has a new support counting mechanism developed for PTM data. Existing sequence pattern mining algorithms have event label level support counting mechanism, whereas we have developed event cluster level support counting mechanism. We discover the complete set of all possible temporal relationships based on Allen's interval algebra. The experimental results showed that the discovered sequence patterns are more useful than the patterns discovered with state-of-the-art sequence pattern mining algorithms.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {17},
numpages = {19},
keywords = {Multimedia datamining, sequence pattern mining, multimodal datamining, cross-modal correlation, probabilistic interval based event mining}
}

@article{10.1145/1899412.1899420,
author = {Liu, Qingzhong and Sung, Andrew H. and Qiao, Mengyu},
title = {Neighboring Joint Density-Based JPEG Steganalysis},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899420},
doi = {10.1145/1899412.1899420},
abstract = {The threat posed by hackers, spies, terrorists, and criminals, etc. using steganography for stealthy communications and other illegal purposes is a serious concern of cyber security. Several steganographic systems that have been developed and made readily available utilize JPEG images as carriers. Due to the popularity of JPEG images on the Internet, effective steganalysis techniques are called for to counter the threat of JPEG steganography. In this article, we propose a new approach based on feature mining on the discrete cosine transform (DCT) domain and machine learning for steganalysis of JPEG images. First, neighboring joint density features on both intra-block and inter-block are extracted from the DCT coefficient array and the absolute array, respectively; then a support vector machine (SVM) is applied to the features for detection. An evolving neural-fuzzy inference system is employed to predict the hiding amount in JPEG steganograms. We also adopt a feature selection method of support vector machine recursive feature elimination to reduce the number of features. Experimental results show that, in detecting several JPEG-based steganographic systems, our method prominently outperforms the well-known Markov-process based approach.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {16},
numpages = {16},
keywords = {JPEG, neighboring joint density, nuero-fuzzy, SVMRFE, classification, steganalysis, SVM, steganography}
}

@article{10.1145/1899412.1899419,
author = {Tong, Xiaofeng and Liu, Jia and Wang, Tao and Zhang, Yimin},
title = {Automatic Player Labeling, Tracking and Field Registration and Trajectory Mapping in Broadcast Soccer Video},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899419},
doi = {10.1145/1899412.1899419},
abstract = {In this article, we present a method to perform automatic player trajectories mapping based on player detection, unsupervised labeling, efficient multi-object tracking, and playfield registration in broadcast soccer videos. Player detector determines the players' positions and scales by combining the ability of dominant color based background subtraction and a boosting detector with Haar features. We first learn the dominant color with accumulate color histogram at the beginning of processing, then use the player detector to collect hundreds of player samples, and learn player appearance codebook by unsupervised clustering. In a soccer game, a player can be labeled as one of four categories: two teams, referee or outlier. The learning capability enables the method to be generalized well to different videos without any manual initialization. With the dominant color and player appearance model, we can locate and label each player. After that, we perform multi-object tracking by using Markov Chain Monte Carlo (MCMC) data association to generate player trajectories. Some data driven dynamics are proposed to improve the Markov chain's efficiency, such as label consistency, motion consistency, and track length, etc. Finally, we extract key-points and find the mapping from an image plane to the standard field model, and then map players' position and trajectories to the field. A large quantity of experimental results on FIFA World Cup 2006 videos demonstrate that this method can reach high detection and labeling precision, reliably tracking in scenes of player occlusion, moderate camera motion and pose variation, and yield promising field registration results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {15},
numpages = {32},
keywords = {Boosting, MCMC, sports video, video analysis, codebook, field registration, multiple player tracking, player labeling}
}

@article{10.1145/1899412.1899418,
author = {Tang, Jinhui and Hong, Richang and Yan, Shuicheng and Chua, Tat-Seng and Qi, Guo-Jun and Jain, Ramesh},
title = {Image Annotation by <i>k</i>NN-Sparse Graph-Based Label Propagation over Noisily Tagged Web Images},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899418},
doi = {10.1145/1899412.1899418},
abstract = {In this article, we exploit the problem of annotating a large-scale image corpus by label propagation over noisily tagged web images. To annotate the images more accurately, we propose a novel kNN-sparse graph-based semi-supervised learning approach for harnessing the labeled and unlabeled data simultaneously. The sparse graph constructed by datum-wise one-vs-kNN sparse reconstructions of all samples can remove most of the semantically unrelated links among the data, and thus it is more robust and discriminative than the conventional graphs. Meanwhile, we apply the approximate k nearest neighbors to accelerate the sparse graph construction without loosing its effectiveness. More importantly, we propose an effective training label refinement strategy within this graph-based learning framework to handle the noise in the training labels, by bringing in a dual regularization for both the quantity and sparsity of the noise. We conduct extensive experiments on a real-world image database consisting of 55,615 Flickr images and noisily tagged training labels. The results demonstrate both the effectiveness and efficiency of the proposed approach and its capability to deal with the noise in the training labels.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {14},
numpages = {15},
keywords = {kNN, label propagation, web image, semi-supervised learning, noisy tags, Sparse graph}
}

@article{10.1145/1899412.1899417,
author = {Wu, Lei and Hoi, Steven C.H. and Jin, Rong and Zhu, Jianke and Yu, Nenghai},
title = {Distance Metric Learning from Uncertain Side Information for Automated Photo Tagging},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899417},
doi = {10.1145/1899412.1899417},
abstract = {Automated photo tagging is an important technique for many intelligent multimedia information systems, for example, smart photo management system and intelligent digital media library. To attack the challenge, several machine learning techniques have been developed and applied for automated photo tagging. For example, supervised learning techniques have been applied to automated photo tagging by training statistical classifiers from a collection of manually labeled examples. Although the existing approaches work well for small testbeds with relatively small number of annotation words, due to the long-standing challenge of object recognition, they often perform poorly in large-scale problems. Another limitation of the existing approaches is that they require a set of high-quality labeled data, which is not only expensive to collect but also time consuming. In this article, we investigate a social image based annotation scheme by exploiting implicit side information that is available for a large number of social photos from the social web sites. The key challenge of our intelligent annotation scheme is how to learn an effective distance metric based on implicit side information (visual or textual) of social photos. To this end, we present a novel “Probabilistic Distance Metric Learning” (PDML) framework, which can learn optimized metrics by effectively exploiting the implicit side information vastly available on the social web. We apply the proposed technique to photo annotation tasks based on a large social image testbed with over 1 million tagged photos crawled from a social photo sharing portal. Encouraging results show that the proposed technique is effective and promising for social photo based annotation tasks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {13},
numpages = {28},
keywords = {Automated photo tagging, uncertain side information, social images, distance metric learning, content-based image retrieval}
}

@article{10.1145/1899412.1899416,
author = {Yu, Jie and Jin, Xin and Han, Jiawei and Luo, Jiebo},
title = {Collection-Based Sparse Label Propagation and Its Application on Social Group Suggestion from Photos},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899416},
doi = {10.1145/1899412.1899416},
abstract = {Online social network services pose great opportunities and challenges for many research areas. In multimedia content analysis, automatic social group recommendation for images holds the promise to expand one's social network through media sharing. However, most existing techniques cannot generate satisfactory social group suggestions when the images are classified independently. In this article, we present novel methods to produce accurate suggestions of suitable social groups from a user's personal photo collection. First, an automatic clustering process is designed to estimate the group similarities, select the optimal number of clusters and categorize the social groups. Both visual content and textual annotations are integrated to generate initial predictions of the group categories for the images. Next, the relationship among images in a user's collection is modeled as a sparse graph. A collection-based sparse label propagation method is proposed to improve the group suggestions. Furthermore, the sparse graph-based collection model can be readily exploited to select the most influential and informative samples for active relevance feedback, which can be integrated with the label propagation process without the need for classifier retraining. The proposed methods have been tested on group suggestion tasks for real user collections and demonstrated superior performance over the state-of-the-art techniques.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {12},
numpages = {21},
keywords = {Social image, active relevance feedback, collection-based sparse label propagation, group recommendation}
}

@article{10.1145/1899412.1899415,
author = {Shao, Yuanlong and Zhou, Yuan and Cai, Deng},
title = {Variational Inference with Graph Regularization for Image Annotation},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899415},
doi = {10.1145/1899412.1899415},
abstract = {Image annotation is a typical area where there are multiple types of attributes associated with each individual image. In order to achieve better performance, it is important to develop effective modeling by utilizing prior knowledge. In this article, we extend the graph regularization approaches to a more general case where the regularization is imposed on the factorized variational distributions, instead of posterior distributions implicitly involved in EM-like algorithms. In this way, the problem modeling can be more flexible, and we can choose any factor in the problem domain to impose graph regularization wherever there are similarity constraints among the instances. We formulate the problem formally and show its geometrical background in manifold learning. We also design two practically effective algorithms and analyze their properties such as the convergence. Finally, we apply our approach to image annotation and show the performance improvement of our algorithm.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {11},
numpages = {21},
keywords = {variational inference, Automatic image annotation, Laplacian regularization, semantic indexing, graph regularization, semi-supervised learning}
}

@article{10.1145/1899412.1899414,
author = {Wang, Meng and Hua, Xian-Sheng},
title = {Active Learning in Multimedia Annotation and Retrieval: A Survey},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899414},
doi = {10.1145/1899412.1899414},
abstract = {Active learning is a machine learning technique that selects the most informative samples for labeling and uses them as training data. It has been widely explored in multimedia research community for its capability of reducing human annotation effort. In this article, we provide a survey on the efforts of leveraging active learning in multimedia annotation and retrieval. We mainly focus on two application domains: image/video annotation and content-based image retrieval. We first briefly introduce the principle of active learning and then we analyze the sample selection criteria. We categorize the existing sample selection strategies used in multimedia annotation and retrieval into five criteria: risk reduction, uncertainty, diversity, density and relevance. We then introduce several classification models used in active learning-based multimedia annotation and retrieval, including semi-supervised learning, multilabel learning and multiple instance learning. We also provide a discussion on several future trends in this research direction. In particular, we discuss cost analysis of human annotation and large-scale interactive multimedia annotation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {10},
numpages = {21},
keywords = {content-based image retrieval, image annotation, model learning, sample selection, Active learning, video annotation}
}

@article{10.1145/1899412.1899413,
author = {Hua, Xian-Sheng and Tian, Qi and Bimbo, Alberto Del and Jain, Ramesh},
title = {Introduction to the Special Issue on Intelligent Multimedia Systems and Technology},
year = {2011},
issue_date = {February 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1899412.1899413},
doi = {10.1145/1899412.1899413},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {9},
numpages = {2}
}

@article{10.1145/1889681.1889689,
author = {Bao, Xinlong and Dietterich, Thomas G.},
title = {FolderPredictor: Reducing the Cost of Reaching the Right Folder},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889689},
doi = {10.1145/1889681.1889689},
abstract = {Helping computer users rapidly locate files in their folder hierarchies is a practical research problem involving both intelligent systems and user interface design. This article reports on FolderPredictor, a software system that can reduce the cost of locating files in hierarchical folders. FolderPredictor applies a cost-sensitive prediction algorithm to the user's previous file access information to predict the next folder that will be accessed. Experimental results show that, on average, FolderPredictor reduces the number of clicks spent on locating a file by 50%. Several variations of the cost-sensitive prediction algorithm are discussed. An experimental study shows that the best algorithm among them is a mixture of the most recently used (MRU) folder and the cost-sensitive predictions. Furthermore, FolderPredictor does not require users to adapt to a new interface, but rather meshes with the existing interface for opening files on the Windows platform.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {8},
numpages = {23},
keywords = {folders, intelligent user interfaces, Activities, user interface, prediction, directories, shortcuts, tasks, intelligent systems, recommendation}
}

@article{10.1145/1889681.1889688,
author = {Wyatt, Danny and Choudhury, Tanzeem and Bilmes, Jeff and Kitts, James A.},
title = {Inferring Colocation and Conversation Networks from Privacy-Sensitive Audio with Implications for Computational Social Science},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889688},
doi = {10.1145/1889681.1889688},
abstract = {New technologies have made it possible to collect information about social networks as they are acted and observed in the wild, instead of as they are reported in retrospective surveys. These technologies offer opportunities to address many new research questions: How can meaningful information about social interaction be extracted from automatically recorded raw data on human behavior? What can we learn about social networks from such fine-grained behavioral data? And how can all of this be done while protecting privacy? With the goal of addressing these questions, this article presents new methods for inferring colocation and conversation networks from privacy-sensitive audio. These methods are applied in a study of face-to-face interactions among 24 students in a graduate school cohort during an academic year. The resulting analysis shows that networks derived from colocation and conversation inferences are quite different. This distinction can inform future research in computational social science, especially work that only measures colocation or employs colocation data as a proxy for conversation networks.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {7},
numpages = {41},
keywords = {mobile sensing, Social networks}
}

@article{10.1145/1889681.1889687,
author = {Ward, Jamie A. and Lukowicz, Paul and Gellersen, Hans W.},
title = {Performance Metrics for Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889687},
doi = {10.1145/1889681.1889687},
abstract = {In this article, we introduce and evaluate a comprehensive set of performance metrics and visualisations for continuous activity recognition (AR). We demonstrate how standard evaluation methods, often borrowed from related pattern recognition problems, fail to capture common artefacts found in continuous AR—specifically event fragmentation, event merging and timing offsets. We support our assertion with an analysis on a set of recently published AR papers. Building on an earlier initial work on the topic, we develop a frame-based visualisation and corresponding set of class-skew invariant metrics for the one class versus all evaluation. These are complemented by a new complete set of event-based metrics that allow a quick graphical representation of system performance—showing events that are correct, inserted, deleted, fragmented, merged and those which are both fragmented and merged. We evaluate the utility of our approach through comparison with standard metrics on data from three different published experiments. This shows that where event- and frame-based precision and recall lead to an ambiguous interpretation of results in some cases, the proposed metrics provide a consistently unambiguous explanation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {6},
numpages = {23},
keywords = {performance evaluation, Activity recognition, metrics}
}

@article{10.1145/1889681.1889686,
author = {Zhou, Yue and Ni, Bingbing and Yan, Shuicheng and Huang, Thomas S.},
title = {Recognizing Pair-Activities by Causality Analysis},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889686},
doi = {10.1145/1889681.1889686},
abstract = {In this article, beyond solo-activity analysis for single object, we study the more complicated pair-activity recognition problem by exploring the relationship between two active objects based on their trajectory clues obtained from video sensor. Our contributions are three-fold. First, we design two sets of features for representing the pair-activities encoded as length-variable trajectory pairs. One set characterizes the strength of causality between two trajectories, for example, the causality ratio and feedback ratio based on the Granger Causality Test (GCT), and another set describes the style of causality between two trajectories, for example, the sampled frequency responses of the digital filter with these two trajectories as the input and output discrete signals respectively. These features along with conventional velocity and position features of a trajectory-pair are essentially of multi-modalities, and may be greatly different in scales and importance. To make full use of them, we then develop a novel feature fusing procedure to learn the coefficients for weighting these features by maximizing the discriminating power measured by weighted correlation. Finally, we collected a pair-activity database of five popular categories, each of which consists of about 170 instances. The extensive experiments on this database validate the effectiveness of the designed features for pair-activity representation, and also demonstrate that the proposed feature fusing procedure significantly boosts the pair-activity classification accuracy.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {5},
numpages = {20},
keywords = {digital filter, causality analysis, Activity analysis, frequency responses}
}

@article{10.1145/1889681.1889685,
author = {Hsu, Jane Yung-Jen and Lian, Chia-Chun and Jih, Wan-Rong},
title = {Probabilistic Models for Concurrent Chatting Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889685},
doi = {10.1145/1889681.1889685},
abstract = {Recognition of chatting activities in social interactions is useful for constructing human social networks. However, the existence of multiple people involved in multiple dialogues presents special challenges. To model the conversational dynamics of concurrent chatting behaviors, this article advocates Factorial Conditional Random Fields (FCRFs) as a model to accommodate co-temporal relationships among multiple activity states. In addition, to avoid the use of inefficient Loopy Belief Propagation (LBP) algorithm, we propose using Iterative Classification Algorithm (ICA) as the inference method for FCRFs. We designed experiments to compare our FCRFs model with two dynamic probabilistic models, Parallel Condition Random Fields (PCRFs) and Hidden Markov Models (HMMs), in learning and decoding based on auditory data. The experimental results show that FCRFs outperform PCRFs and HMMs-like models. We also discover that FCRFs using the ICA inference approach not only improves the recognition accuracy but also takes significantly less time than the LBP inference method.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {4},
numpages = {20},
keywords = {factorial conditional random fields, Chatting activity recognition, loopy belief propagation, iterative classification}
}

@article{10.1145/1889681.1889684,
author = {Farrahi, Katayoun and Gatica-Perez, Daniel},
title = {Discovering Routines from Large-Scale Human Locations Using Probabilistic Topic Models},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889684},
doi = {10.1145/1889681.1889684},
abstract = {In this work, we discover the daily location-driven routines that are contained in a massive real-life human dataset collected by mobile phones. Our goal is the discovery and analysis of human routines that characterize both individual and group behaviors in terms of location patterns. We develop an unsupervised methodology based on two differing probabilistic topic models and apply them to the daily life of 97 mobile phone users over a 16-month period to achieve these goals. Topic models are probabilistic generative models for documents that identify the latent structure that underlies a set of words. Routines dominating the entire group's activities, identified with a methodology based on the Latent Dirichlet Allocation topic model, include “going to work late”, “going home early”, “working nonstop” and “having no reception (phone off)” at different times over varying time-intervals. We also detect routines which are characteristic of users, with a methodology based on the Author-Topic model. With the routines discovered, and the two methods of characterizing days and users, we can then perform various tasks. We use the routines discovered to determine behavioral patterns of users and groups of users. For example, we can find individuals that display specific daily routines, such as “going to work early” or “turning off the mobile (or having no reception) in the evenings”. We are also able to characterize daily patterns by determining the topic structure of days in addition to determining whether certain routines occur dominantly on weekends or weekdays. Furthermore, the routines discovered can be used to rank users or find subgroups of users who display certain routines. We can also characterize users based on their entropy. We compare our method to one based on clustering using K-means. Finally, we analyze an individual's routines over time to determine regions with high variations, which may correspond to specific events.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {3},
numpages = {27},
keywords = {reality mining, Human activity modeling, topic models}
}

@article{10.1145/1889681.1889683,
author = {Zheng, Yu and Xie, Xing},
title = {Learning Travel Recommendations from User-Generated GPS Traces},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889683},
doi = {10.1145/1889681.1889683},
abstract = {The advance of GPS-enabled devices allows people to record their location histories with GPS traces, which imply human behaviors and preferences related to travel. In this article, we perform two types of travel recommendations by mining multiple users' GPS traces. The first is a generic one that recommends a user with top interesting locations and travel sequences in a given geospatial region. The second is a personalized recommendation that provides an individual with locations matching her travel preferences. To achieve the first recommendation, we model multiple users' location histories with a tree-based hierarchical graph (TBHG). Based on the TBHG, we propose a HITS (Hypertext Induced Topic Search)-based model to infer the interest level of a location and a user's travel experience (knowledge). In the personalized recommendation, we first understand the correlation between locations, and then incorporate this correlation into a collaborative filtering (CF)-based model, which predicts a user's interests in an unvisited location based on her locations histories and that of others. We evaluated our system based on a real-world GPS trace dataset collected by 107 users over a period of one year. As a result, our HITS-based inference model outperformed baseline approaches like rank-by-count and rank-by-frequency. Meanwhile, we achieved a better performance in recommending travel sequences beyond baselines like rank-by-count. Regarding the personalized recommendation, our approach is more effective than the weighted Slope One algorithm with a slightly additional computation, and is more efficient than the Pearson correlation-based CF model with the similar effectiveness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {2},
numpages = {29},
keywords = {Location recommendation, GPS trace, collaborative filtering, location history, GeoLife}
}

@article{10.1145/1889681.1889682,
author = {Zhang, Daqing and Philipose, Matthai and Yang, Qiang},
title = {Introduction to the Special Issue on Intelligent Systems for Activity Recognition},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1889681.1889682},
doi = {10.1145/1889681.1889682},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {1},
numpages = {4}
}

@article{10.1145/1869397.1869404,
author = {Cirillo, Marcello and Karlsson, Lars and Saffiotti, Alessandro},
title = {Human-Aware Task Planning: An Application to Mobile Robots},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869404},
doi = {10.1145/1869397.1869404},
abstract = {Consider a house cleaning robot planning its activities for the day. Assume that the robot expects the human inhabitant to first dress, then have breakfast, and finally go out. Then, it should plan not to clean the bedroom while the human is dressing, and to clean the kitchen after the human has had breakfast. In general, robots operating in inhabited environments, like households and future factory floors, should plan their behavior taking into account the actions that will be performed by the humans sharing the same environment. This would improve human-robot cohabitation, for example, by avoiding undesired situations for the human. Unfortunately, current task planners only consider the robot's actions and unexpected external events in the planning process, and cannot accommodate expectations about the actions of the humans.In this article, we present a human-aware planner able to address this problem. Our planner supports alternative hypotheses of the human plan, temporal duration for the actions of both the robot and the human, constraints on the interaction between robot and human, partial goal achievement and, most importantly, the possibility to use observations of human actions in the policy generated for the robot. Our planner has been tested both as a stand-alone component and within a full framework for human-robot interaction in a real environment.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {15},
numpages = {26},
keywords = {human-aware planning, Human-robot interaction}
}

@article{10.1145/1869397.1869403,
author = {Talamadupula, Kartik and Benton, J. and Kambhampati, Subbarao and Schermerhorn, Paul and Scheutz, Matthias},
title = {Planning for Human-Robot Teaming in Open Worlds},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869403},
doi = {10.1145/1869397.1869403},
abstract = {As the number of applications for human-robot teaming continue to rise, there is an increasing need for planning technologies that can guide robots in such teaming scenarios. In this article, we focus on adapting planning technology to Urban Search And Rescue (USAR) with a human-robot team. We start by showing that several aspects of state-of-the-art planning technology, including temporal planning, partial satisfaction planning, and replanning, can be gainfully adapted to this scenario. We then note that human-robot teaming also throws up an additional critical challenge, namely, enabling existing planners, which work under closed-world assumptions, to cope with the open worlds that are characteristic of teaming problems such as USAR. In response, we discuss the notion of conditional goals, and describe how we represent and handle a specific class of them called open world quantified goals. Finally, we describe how the planner, and its open world extensions, are integrated into a robot control architecture, and provide an empirical evaluation over USAR experimental runs to establish the effectiveness of the planning components.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {14},
numpages = {24},
keywords = {Automated planning, search and rescue, robot, planner}
}

@article{10.1145/1869397.1869402,
author = {Benaskeur, Abder Rezak and Kabanza, Froduald and Beaudry, Eric},
title = {CORALS: A Real-Time Planner for Anti-Air Defense Operations},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869402},
doi = {10.1145/1869397.1869402},
abstract = {Forces involved in modern conflicts may be exposed to a variety of threats, including coordinated raids of advanced ballistic and cruise missiles. To respond to these, a defending force will rely on a set of combat resources. Determining an efficient allocation and coordinated use of these resources, particularly in the case of multiple simultaneous attacks, is a very complex decision-making process in which a huge amount of data must be dealt with under uncertainty and time pressure. This article presents CORALS (COmbat Resource ALlocation Support), a real-time planner developed to support the command team of a naval force defending against multiple simultaneous threats. In response to such multiple threats, CORALS uses a local planner to generate a set of local plans, one for each threat considered apart, and then combines and coordinates them into a single optimized, conflict-free global plan. The coordination is performed through an iterative process of plan merging and conflict detection and resolution, which acts as a plan repair mechanism. Such an incremental plan repair approach also allows adapting previously generated plans to account for dynamic changes in the tactical situation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {13},
numpages = {21},
keywords = {anti-air defense operations, decision support, Planning}
}

@article{10.1145/1869397.1869401,
author = {Refanidis, Ioannis and Yorke-Smith, Neil},
title = {A Constraint-Based Approach to Scheduling an Individual's Activities},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869401},
doi = {10.1145/1869397.1869401},
abstract = {The goal of helping to automate the management of an individual's time is ambitious in terms both of knowledge engineering and of the quality of the plans produced by an AI system. Modeling an individual's activities is itself a challenge, due to the variety of activity, constraint, and preference types involved. Activities might be simple or interruptible; they might have fixed or variable durations, constraints over their temporal domains, and binary constraints between them. Activities might require the individual being at specific locations in order, whereas traveling time should be taken into account. Some activities might require exclusivity, whereas others can be overlapped with compatible concurrent activities. Finally, while scheduled activities generate utility for the individual, extra utility might result from the way activities are scheduled in time, individually and in conjunction.This article presents a rigorous, expressive model to represent an individual's activities, that is, activities whose scheduling is not contingent on any other person. Joint activities such as meetings are outside our remit; it is expected that these are arranged manually or through negotiation mechanisms and they are considered as fixed busy times in the individual's calendar. The model, formulated as a constraint optimization problem, is general enough to accommodate a variety of situations. We present a scheduler that operates on this rich model, based on the general squeaky wheel optimization framework and enhanced with domain-dependent heuristics and forward checking. Our empirical evaluation demonstrates both the efficiency and the effectiveness of the selected approach. Part of the work described has been implemented in the SelfPlanner system, a Web-based intelligent calendar application that utilizes Google Calendar.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {12},
numpages = {32},
keywords = {activity modeling, constraints, preferences, intelligent calendar applications, Greedy algorithms}
}

@article{10.1145/1869397.1869400,
author = {Bryce, Daniel and Verdicchio, Michael and Kim, Seungchan},
title = {Planning Interventions in Biological Networks},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869400},
doi = {10.1145/1869397.1869400},
abstract = {Modeling the dynamics of biological processes has recently become an important research topic in computational biology and systems engineering. One of the most important reasons to model a biological process is to enable high-throughput in-silico experiments that attempt to predict or intervene in the process. These experiments can help accelerate the design of therapies through their rapid and inexpensive replication and alteration. While some techniques exist for reasoning with biological processes, few take advantage of the flexible and scalable algorithms popular in AI research. In reasoning about interventions in biological processes, where scalability is crucial for feasible application, we apply AI planning-based search techniques and demonstrate their advantage over existing enumerative methods. We also present a novel formulation of intervention planning that relies on models that characterize and attempt to change the phenotype of a system. We study three biological systems: the yeast cell cycle, a model of the human aging process, and the Wnt5a network governing the metastasis of melanoma in humans. The contribution of our investigation is in demonstrating that: (i) prior approaches, based on dynamic programming, cannot scale as well as heuristic search, and (ii) the newly found scalability enables us to plan previously unknown sequences of interventions that reveal novel and biologically significant responses in the systems which are consistent with biological knowledge in the literature.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {11},
numpages = {26},
keywords = {Planning, search, systems biology}
}

@article{10.1145/1869397.1869399,
author = {Porteous, Julie and Cavazza, Marc and Charles, Fred},
title = {Applying Planning to Interactive Storytelling: Narrative Control Using State Constraints},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869399},
doi = {10.1145/1869397.1869399},
abstract = {We have seen ten years of the application of AI planning to the problem of narrative generation in Interactive Storytelling (IS). In that time planning has emerged as the dominant technology and has featured in a number of prototype systems. Nevertheless key issues remain, such as how best to control the shape of the narrative that is generated (e.g., by using narrative control knowledge, i.e., knowledge about narrative features that enhance user experience) and also how best to provide support for real-time interactive performance in order to scale up to more realistic sized systems. Recent progress in planning technology has opened up new avenues for IS and we have developed a novel approach to narrative generation that builds on this. Our approach is to specify narrative control knowledge for a given story world using state trajectory constraints and then to treat these state constraints as landmarks and to use them to decompose narrative generation in order to address scalability issues and the goal of real-time performance in larger story domains. This approach to narrative generation is fully implemented in an interactive narrative based on the “Merchant of Venice.” The contribution of the work lies both in our novel use of state constraints to specify narrative control knowledge for interactive storytelling and also our development of an approach to narrative generation that exploits such constraints. In the article we show how the use of state constraints can provide a unified perspective on important problems faced in IS.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {planning, Interactive storytelling, agents in games and virtual environments, narrative modeling}
}

@article{10.1145/1869397.1869398,
author = {Chen, Yixin},
title = {Preface to Special Issue on Applications of Automated Planning},
year = {2010},
issue_date = {November 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/1869397.1869398},
doi = {10.1145/1869397.1869398},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {9},
numpages = {3}
}

@article{10.1145/1858948.1858956,
author = {Wang, Meng and Liu, Bo and Hua, Xian-Sheng},
title = {Accessible Image Search for Colorblindness},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858956},
doi = {10.1145/1858948.1858956},
abstract = {This article introduces an intelligent system that accommodates colorblind users in image search. Color plays an important role in the human perception and recognition of images. However, there are about 8% of men and 0.8% of women suffering from colorblindness. We show that the existing image search techniques cannot provide satisfactory results for these users since many images will not be well perceived by them due to the loss of color information. To deal with this difficulty, we introduce a system named Accessible Image Search (AIS) to accommodate these users. Different from the general image search scheme that aims at returning more relevant results, AIS further takes into account the colorblind accessibilities of the returned results, that is, the image qualities in the eyes of colorblind users. The system contains three components: accessibility assessment, accessibility improvement, and color indication. The accessibility assessment component measures the accessibility scores of images, and consequently different reranking methods can be performed to prioritize images with high accessibilities. In the accessibility improvement component, we propose an efficient recoloring algorithm to modify the colors of the images such that they can be better perceived by colorblind users. Color indication aims to indicate the name of the interesting color in an image. We evaluate the introduced system with more than 60 queries and 20 anonymous colorblind users, and the empirical results demonstrate its effectiveness and usefulness.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {8},
numpages = {26},
keywords = {Image search, colorblindness}
}

@article{10.1145/1858948.1858955,
author = {Goolsby, Rebecca},
title = {Social Media as Crisis Platform: The Future of Community Maps/Crisis Maps},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858955},
doi = {10.1145/1858948.1858955},
abstract = {Social media provides the means for creating new communities and for reenergizing old communities. Recently, a new kind of quickly formulated, powerful community has formed as existing social media communities, news organizations, and users have converged in social media spaces to respond to sudden tragedies. This article addresses the ad-hoc crisis community, whith uses the social madia as a crisis platform to generate community crisis maps.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {7},
numpages = {11},
keywords = {Social networking, crisis mapping, maps, Haiti, disaster management, Mumbai, visualization, information tools}
}

@article{10.1145/1858948.1858954,
author = {Roos, Patrick and Carr, J. Ryan and Nau, Dana S.},
title = {Evolution of State-Dependent Risk Preferences},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858954},
doi = {10.1145/1858948.1858954},
abstract = {Researchers have invested much effort in constructing models of the state-dependent (sometimes risk-averse and sometimes risk-prone) nature of human decision making. An important open question is how state-dependent risk behavior can arise and remain prominent in populations. We believe that one part of the answer is the interplay between risk-taking and sequentiality of choices in populations subject to evolutionary population dynamics. To support this hypothesis, we provide simulation and analytical results for evolutionary lottery games, including results on evolutionary stability. We consider a parameterized class of imitation dynamics in which the parameter 0 ≤ α ≤ 1 yields the replicator dynamic with α = 1 and the imitate-the-better dynamic with α = 0. Our results demonstrate that for every population dynamic in this class except for the replicator dynamic, the interplay between risk-taking and sequentiality of choices allows state-dependent risk behavior to have an evolutionary advantage over expected-value maximization.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {6},
numpages = {21},
keywords = {decision theory, population dynamics, Evolutionary games, risk}
}

@article{10.1145/1858948.1858953,
author = {Wu, Fang and Huberman, Bernardo A.},
title = {Opinion Formation under Costly Expression},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858953},
doi = {10.1145/1858948.1858953},
abstract = {Opinions play an important role in trust building and the creation of consensus about issues and products and a number of studies have focused on the design, evaluation, and utilization of online opinion systems. However, little effort has been spent on the dynamic aspects of online opinion formation. In this article, we study the dynamics of online opinion expression by analyzing the temporal evolution of vey large sets of user views and determine that in the course of time, later opinions tend to show a big difference with earlier opinions, which moderates the average opinion to the less extreme. Online posters also tend to disagree with previous opinions when the cost of expression is high.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {5},
numpages = {13},
keywords = {Opinion formation, costly expression}
}

@article{10.1145/1858948.1858952,
author = {Feldman, Michal and Tennenholtz, Moshe},
title = {Structured Coalitions in Resource Selection Games},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858952},
doi = {10.1145/1858948.1858952},
abstract = {We study stability against coalitional deviations in resource selection games where the coalitions have a certain structure. In particular, the agents are partitioned into coalitions, and only deviations by the prescribed coalitions are considered. This is in contrast to the classical concept of strong equilibrium according to which any subset of the agents may deviate. In resource selection games, each agent selects a resource from a set of resources, and its payoff is an increasing (or nondecreasing) function of the number of agents selecting its resource. While it has been shown that a strong equilibrium always exists in resource selection games, a closer look reveals severe limitations to the applicability of the existence result even in the simplest case of two identical resources with increasing cost functions. First, these games do not possess a super strong equilibrium in which a fruitful deviation benefits at least one deviator without hurting any other deviator. Second, a strong equilibrium may not exist when the game is played repeatedly. We prove that for any given partition, there exists a super strong equilibrium for resource selection games of identical resources with increasing cost functions. In addition, we show similar existence results for a variety of other classes of resource selection games. For the case of repeated games, we characterize partitions that guarantee the existence of a strong equilibrium. Together, our work introduces a natural concept, which turns out to lead to positive and applicable results in one of the basic domains studied in the literature.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {4},
numpages = {21},
keywords = {Coalitions, partition, resource selection games, strong equilibrium, repeated games}
}

@article{10.1145/1858948.1858951,
author = {Bainbridge, William Sims},
title = {Virtual Worlds as Cultural Models},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858951},
doi = {10.1145/1858948.1858951},
abstract = {Thirteen gamelike virtual worlds illustrate issues that overlap social science and information science, because they embody rather clear theories of society and culture: World of Warcraft, Lord of the Rings Online, Dark Age of Camelot, Age of Conan, Pirates of the Burning Sea, A Tale in the Desert, Entropia Universe, Anarchy Online, The Matrix Online, Tabula Rasa, EVE Online, Star Trek Online, and Dungeons and Dragons Online. A fourteenth, Star Wars Galaxies, illustrates the possibility that not all virtual worlds embody clear theories. After describing the thirteen, this essay discusses their economic systems, social systems, communication challenges, and the ways in which autonomous agents and semi-autonomous secondary avatars enrich interactive complexity.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {3},
numpages = {21},
keywords = {game, Culture, virtual world}
}

@article{10.1145/1858948.1858950,
author = {Liu, Huan and Nau, Dana},
title = {Introduction to the ACM TIST Special Issue AI in Social Computing and Cultural Modeling},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858950},
doi = {10.1145/1858948.1858950},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {2},
numpages = {2}
}

@article{10.1145/1858948.1858949,
author = {Yang, Qiang},
title = {Introduction to ACM TIST},
year = {2010},
issue_date = {October 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/1858948.1858949},
doi = {10.1145/1858948.1858949},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {1},
numpages = {2}
}

