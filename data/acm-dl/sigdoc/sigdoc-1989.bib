@inproceedings{10.1145/74311.74312,
author = {Ogura, A. and Robertson, J.},
title = {Designing Hypermedia Help Systems: Problems and Issues},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74312},
doi = {10.1145/74311.74312},
abstract = {Over the past few years we have seen a significant increase in hypertext and hypermedia research; the variety of products now available is quite impressive and it seems we are constantly adding newer and even more innovative hypersystems. Clearly, hypermedia is an idea that has captured the imagination of the computer world and many claims have been made for its utility.Hypersystems have been used for many different types of applications. Borrowing Conklin's taxonomy [6], we find hypertexts used as macroliterary systems (large online libraries with machine-supported links), problem exploration tools (problem-solving, designing, authoring aids), browsing systems (small-scale macroliterary systems with an emphasis on ease of use), and “general” hypertext technology (systems designed to allow research on hypertext characteristics as well as on hypertext users). Some notable applications include attempts at a “dynamic book” [18] the “electronic book” [19] and an electronic encyclopedia (Hyperties [14]) and the use of hypertext for collaborative work (gIBIS [7], NoteCards [15], KMS [1]). Some other interesting educational applications include a system to teach Shakespeare [9] and Steinbeck (Grapevine) [4].In the area of hypermedia, we are now going beyond merely combining text and graphics, but combining text, graphics, video and audio [11]. A system developed at Carnegie Mellon University uses video segments combined with on-screen question prompts in a hypermedia system for the teaching of ethics [8]. Students explore a case on issue of the right to die (using the Dax Cowart case), creating their own paths through the video segments until they can form their own decisions. There is also increasing interest in the role of artificial intelligence in hypersystems to aid both browsing and searching functions [2]More relevant to documenters, hypersystems are also being developed not only for information presentation as in a on-line help system, but also for electronic publishing and documentation management (both for writing and information presentation as in document delivery systems) [3] [16] [10]. Hypersystems have also made possible more interactive help systems and tutorials.But along with the increasing popularity of hypersystems, or perhaps because of it, hypersystems have also engendered much controversy. Authors and developers often argue over the definition of hypertext or hypermedia, how to design nodes, how to compute links, what models or maps best aid users, and more generally, whether or not hypersystems can, indeed, solve our information access and retrieval problems. In a critique of hypertext systems, Raskin [12] describes hypertexts as a technology that has “failed to deliver on their promise” citing problems with various technological limitations (storage capacity, speed), social, legal, and economic problems, and more importantly, the lack of a human interface specification. Members of a panel on “A critical assessment of hypertext systems” [5] commented on the pros and cons of imposing structure, user-created links and nodes, and intelligent support systems, as well as optimal chunk sizes of text, search mechanisms, and information presentation. There seems to be no consensus on what hypersystems can or cannot be used for, no agreement on a user interface or even ideal characteristics of an interface, and no clear answers to questions such as “How do we present text to the user?” “How do we prevent readers from getting lost?” “How can we support information search as well as browsing?” and “How can we manage a hypertext help system?”At the Information Technology Center at Carnegie Mellon University, we have already developed a hypertext help system (the Andrew Help System) and have been exploring ways in which to create a more usable system that incorporates multi-media. We surveyed the field the various applications currently available in search of answers to the writing, presentation, and management questions we all ask about any online help system, whether hypertext or not. In the initial search for information, we were seeking ways to make the “right” or “correct” system, but we quickly became mired in the controversies surrounding the technology. There seemed to be no “right” or “correct” that anyone could agree upon. There is even no agreement on the definition of hypertext or hypermedia as yet. We decided, therefore, to step back and first derive some sort of model to work from that we could use to inform our design decisions. The model we will now describe is a taxonomy of issues and problems that have a basis not on application features, but in rhetorical theory. The following discussion will present our rhetorical model of hypersystems and how we have used this model to help us generate the necessary design questions for our own hypermedia help system.Rhetorical situations for HyperHelpAs with any other discussion of hypertext and hypermedia, we require a brief definition of terms. We define Hyper-Help as any online help system that uses hypertext or hypermedia characteristics. These characteristics may include nodes and links (user-generated or computed), databases, multiple windows, multiple media (text, graphics, audio, video), and varying levels of user interaction. The Andrew Help System is a HyperHelp system consisting of a database of files with cross-references that the user may choose to navigate. Andrew Help has a history mechanism that records previous nodes the user has visited, a list of all available files presented in its own panel, a rudimentary search and pattern-match facility, and the ability to open new windows onto new files. Because of the object-oriented nature of the underlying system, Andrew Help has the capability of incorporating graphics and animation with the text as well as link to interactive tutorials.As documenters, we must always consider the particular rhetorical situation we are working in. The rhetorical situation is, broadly speaking, simply the audience for the text: the goals purposes of the reader as well as the writer, and any social effects that may influence the readers' and writers' processes. These issues have also been discussed as the “context” of hypertexts [13].More specifically, we consider the audience's experience and reading levels, their possible reactions to the text and information, their purpose in reading the document as well as how they may read the document, and the effects of presentation technology and design factors on how well the audience reads, learns, searches, etc.. We have various methodologies ranging from computerized metrics to experimental testing for evaluating documentation, but the bottom line question for evaluation is “How well does this document fit the rhetorical situation it will be used in?” Minus the jargon, this would translate to “How usable is the document?” Since HyperHelp systems are still a form of documentation, can we evaluate them based on the same kinds of rhetorical criteria we impose on other types of documents?Working on the assumption that criteria for usability translates across technologies, we started with the question:“How well does a HyperHelp interface/system fit the rhetorical situation it will be used in?”But what is the rhetorical situation for a HyperHelp? We already discussed the reader (and writer's) purposes and environments as being important components of a rhetorical situation. We would also, because of the nature of the media, include technology as a variable in the rhetorical situation as well. Most of the discussion to date has (we believe incorrectly) focused primarily on technology issues. We also added the nature of the discourse and the nature of the information to be presented as important factors to consider in the rhetorical situation.We then used the categories of 1) reader goals, 2) technology, 3) nature of information and discourse to classify all the problems and issues that we encountered in the current literature as well as to generate additional problems and issues that we found through our own experiences. The following is a more in-depth discussion of the categories in our rhetorical model using examples of how we solved some of our own problems or generated useful design decisions based on the model.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {5–11},
numpages = {7},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74313,
author = {Mongeau, D.},
title = {Business Planning in Technical Documentation Organizations},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74313},
doi = {10.1145/74311.74313},
abstract = {Business planning is a process of deciding what an organization will do to be successful, and how it will do it. Business planning can benefit any organization that wants to control its future and to succeed. However, a literature review and some practical experience at AT&amp;T Bell Laboratories suggests that technical documentation organizations have virtually ignored the application of business planning, both as a means of creating their futures and as a means of advancing their profession. The business planning process involves creating an organizational mission; diagnosing the organization's strengths, weaknesses, opportunities, and risks; setting goals; developing objectives and action plans; developing a financial plan; and writing, sharing, and implementing the plan. Following these steps in the Bell Labs Publication Center, we have seen our budget and staff grow and our client base diversify.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {13–19},
numpages = {7},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74314,
author = {Mirel, B. and Allmendinger, L.},
title = {Strategies for Designing Manuals for Hacker Styles of Learning},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74314},
doi = {10.1145/74311.74314},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {21–23},
numpages = {3},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74315,
author = {Johnson, R.},
title = {Rhetoric and Human-Computer Interaction: Investigations into the Writing of User-Centered Documentation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74315},
doi = {10.1145/74311.74315},
abstract = {This paper investigates the need for a user-centered theory of writing documentation. User-centered approaches present new challenges for writers, and it is argued that rhetoric and human-computer interaction are the most appropriate fields for developing a theory to meet those challenges. In addition, applications of the theory are proposed to aid writers in solving common documentation problems.“They perfect nature, and are perfected by experience.” (Francis Bacon. Of Studies, 1625)In Beckett's “Waiting for Godot,” two men spend the entire play waiting for something that never happens. They converse until the end and then depart, having never met up with Godot. Many critics have argued that Beckett's play is a symbol for the despair of twentieth century man: he lives in a world where meaning is always being sought, but can never be found. Another interpretation of this play, however, is that modern man is always waiting - simply waiting for something to happen. It is with this second theme that I would like to draw an analogy to the writing of computer documentation. We, the writers and teachers of computer documentation, are waiting for coherent guidelines which will aid us in our design tasks. We are not at the point of despair (far from that, we are at a point of grasping unprecedented power in our profession), but we, too, are waiting for something to happen.The purpose of this present investigation is to argue that the period of waiting is nearing an end, but that we must seize the moment in a conscious, thoughtful, and open-minded manner for any advances to be fruitful. Too often a field will launch into new applications with tremendous intensity, but because it is done with little reflection as to purpose, the results are smothered by the heat. In addition, because the movement is so quick, there is not time to consider alternative or novel solutions. With these cautions in mind, I propose in the following discussion to investigate how we might move toward a theory of user-centered documentation. To achieve this, we will look at two fields - rhetoric and human-computer interaction (HCI) - to see how they can help in the development of such a theory. To give a conceptual framework to this discussion, let us begin by turning to some of the problems that documentation writing theory faces.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {25–32},
numpages = {8},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74316,
author = {Anderson, S. and Talburt, J.},
title = {A Writing Course in User Documentation for Computer Science Majors},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74316},
doi = {10.1145/74311.74316},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {33–37},
numpages = {5},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74317,
author = {Tanenbaum, J.-A.},
title = {Writing Online Help When Space is Limited},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74317},
doi = {10.1145/74311.74317},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {39–40},
numpages = {2},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74318,
author = {Natchez, M. and Prose, T.},
title = {Creating Effective Hypercard Online Documentation and Training},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74318},
doi = {10.1145/74311.74318},
abstract = {In recent years, the “hypers” have made a strong impact on technical communication. Hypertext and hypermedia applications have shown that it is possible to communicate information in user-defined, non-linear ways, taking advantage of multiple sources and delivery mechanisms. HyperCard is a particular application, written by Bill Atkinson and packaged with every Macintosh computer, that provides a structure for creating an easy-to-use, interactive learning experience that can easily combine text, graphics, sound, animation, and even video-taped segments. Depending on the design of the application, users can proceed sequentially along a predefined path, or jump between topics of interest.For example, rather than giving a student a manual that starts at page one and goes sequentially through the range of material, the instructor or technical writer gathers relevant material, enters it on-line, and provides indexes or menus for students to direct their own learning. Material may be graphic, audio, text-based, animated, interactive, or any combination of the above. This creates a tremendous variety in the learning system and lets the user determine the most relevant path through it.With the advent of this type of application, however, writers, trainers, and program developers face new problems in design. These problems include envisioning the overall design of the application, designing effective mapping techniques, streamlining text, creating consistent screens and metaphors, creating guideposts for the user, and planning the effective use of sound, animation, and graphics. This paper addresses these problems and offers guidelines and solutions. We will cover strategies for planning and development of HyperCard projects, and explore two case studies: one, turning a brochure into a HyperCard presentation; the other, designing a HyperCard troubleshooting guide for mainframe operations. While the examples used focus on HyperCard, the instructional design precepts behind them are applicable to any hypermedia presentation.In the flash and dazzle of the new technology it is important to integrate basic writing and instructional design theory. These are all the more critical because they are often neglected. Right now, when technology is changing so rapidly, we tend to be seduced by the newest and most exciting video laser displays, touch screen presentations, and animated screens, and forget that a poorly designed or executed presentation is equally ineffective in whatever medium it is presented.The first step for any project, HyperCard or not, is to perform a needs assessment. If you are unfamiliar with this term, a needs assessment is the determination of who your audience is and what they need. To perform the assessment, you interview the people who know, you interview the people who are going to need to know, and you determine the best way to get the information from one to the other. Identifying your audience and what they need to learn is the first step in any instructional design, and there are several excellent references on this process listed in the bibliography.Next, you must determine whether a specific project will translate effectively into a HyperCard application. The material that works best in HyperCard is subject matter that can be broken down into fairly discrete segments. On-screen reading does not work for long pages of text, so you have be able to convey meaning in a few paragraphs or less. Can this information be assembled in a series of paragraphs that can be linked together by a road map? If so, HyperCard is a viable alternative. Some examples of information that translates well are troubleshooting procedures, brochures, advertising material, orientations, maps, and job aides. The subject can be large — even encyclopedic — as long as it can be broken down into manageable subject areas. Job aids translate well to HyperCard because typically, you are dealing with particular job functions that you can document and show on-screen in a lively and engaging way. You can convey the information before the user gives up on reading the screen.Marketing presentations can translate into powerful hypermedia presentations, because you are trying to present the features of a product or a service in an eye-catching way. These images can be conveyed more easily with pictures, sound, music, and short bursts of text than they can be on a one-dimensional page that someone is likely to throw into the trash.We have worked on projects to translate reference manuals into HyperCard. These can translate effectively because the essential focus of a reference manual is: first, find the information you need; and second, describe it as briefly as application can be implemented here, as long as text is kept to a maximum of three screens per topic. The reader is not interested in the entire reference manual, but rather in one specific piece of information at a time, and then perhaps related subjects.Another type of presentation that translates well is one that requires the user to move in a more structured way with alternate paths. For example, in troubleshooting you have to go from step one to step two to step x, with the progression depending on the problems encountered and the previous response. It is easy to miss a procedure on a page, or to get confused as to the next step. By putting the troubleshooting into a HyperCard application, where you can click and change from step one to step two, you encourage the user to focus on each section of the presentation separately. You also have a natural decision-based branching mechanism. These enhance the presentation.Projects that do not lend themselves to HyperCard presentation include most scientific articles, theoretical discussions, and abstract concepts. These do not work in the kind of “burst of information” environment that HyperCard provides. Translating an ordinary book to HyperCard generally is not a good idea. Philosophy, detailed technical information, or novels are not natural projects for HyperCard.Once you have determined that a project will benefit from presentation in HyperCard, you should turn your attention to mapping the project and determining how the user is going to access the information. This is the problem of providing structure so that the user does not get lost. You need to keep the structure flexible, but you also need to make your information accessible sequentially when appropriate. It is important to keep in mind that there are many types of learners. The application must be available not only to independent thinking, creative people, but also to those of us who like to go through learning processes in a more structured and orderly fashion.Mapping the application is the first job. It requires a clear overview of your objectives, how you envision the user interacting with your material, and what the nature of the logical links between subjects. The more thorough the mapping process, the more satisfactory the end result. It is here that basic instructional design rules can help. Define your objectives and your methodology. Then map your material in as simple and straightforward a manner as possible. Any “flash” comes later, when you enhance the presentation. Figure 1 shows a sample HyperCard application map.The map shows how the user will access various topics covered in the application. In the example shown, the application includes basic operations troubleshooting procedures and background information on the computer system. There is a startup screen, with an optional introduction to the Macintosh. Then the Main Menu appears. From there, users can select individual topics, returning to the menu at the completion of a topic, or go to an index/glossary, for a definition of a term or concept. They can move to a related procedure, or see how a component fits into the system configuration when appropriate. Access to specific procedures is also available by looking up an error message or symptom or by looking at a map of the system, selecting a component. Users are then directed to the related procedure. This allows access by concept, by error message, by symptom, by visual overview, or by procedure.In addition to this initial map for developing the application, you must also build in maps and guideposts to help users navigate easily to the information they want. These range from the simple (forward and backward arrows, a return symbol to return to the starting menu) to the more complex (specific icons, imbedded maps, menu bars, webbed views). They are all designed to help users determine their current position and where to go next. For example, you might place a “where am I” button in the lower right hand corner of the screen. When users click this button they can get an overview of where they have come from, and where they can go. You can go up or down several levels by clicking further. This is a very effective mapping technique. Another technique is to provide a consistent menu bar that gives simple choices such as: go back one, back to start (start meaning point of entry), and index. An sample of this type of menu bar and index is shown in figure 2:Bookmarks are another excellent technique for helping the users find their way around. A bookmark is a way users can say, “save my place, go to somewhere else, and then return.” A web view of the application is also very helpful. A web view is an asymmetrical picture or web of related topics and where they might lead. It gives the user a feel for how the current text is linked to other topics in the application. Another proven navigation tool is a browser. This is a map of the hard links that you have created so the user can quickly see where the logical connectors are going. The browser enables users to see the direction you had in mind in the design of the application. They can then choose whether or not to follow that path.A filter is a more sophisticated mapping tool. With a filter, users can set conditions or choose conditions from a list, and then view a display of topics that correspond to these criteria. This creates a subset of topics from the data base of available material.Indexes are still an excellent way to find information. Many of on-line help systems use them for that reason and they're also effective for HyperCard. Whatever techniques you choose, well-designed navigation for the user is of critical importance to the success of the application. When you have chosen your approach, you must then map a logical flow from several perspectives. You have to create hard links so that the user who doesn't want to explore without a guide can explore the application in a guided fashion.Specific icons to represent topics or areas are also helpful. For example, in a complex procedure with a section on loading tape drives, on CPU commands, and on disc commands, you might have a representative icon for tapes, CPUs and discs. When users are in the CPU procedure, they could click on the tape icon to return to the tape drive procedure, or on the disc icon to go to a procedure for disc drives.The idea of icons brings us to the question of metaphor in general. In selecting images for icons, graphic illustrations, or concepts, it is important to keep imagery simple, logical, and consistent. At the simplest level, if you decide to use forward and backward arrows, they should always be the same arrows, in the same area of the screen. The appropriate icons should be identified and maintained throughout the application, and they should work in a logical way, a way that makes intuitive sense. For example, if you have chosen the image of a VCR control panel as a navigation tool, then the buttons should function in the same way as a real VCR.Consistency is also important in the layout of the screen…Consistent screen design provides a high comfort level for the user. This does not mean that you should only have one screen pattern for every display, but you might have five or six basic patterns and then add to these, repeat them, vary them, add animation, add sound, change, fade, scroll, etc. However you enhance them, the basic patterns, like the road maps, provide a framework for the user. You should try to have a consistent area for instruction, for feedback, and for navigation, so users always know where to look for specific information.The design of the screen is also very important. It is not possible to cover all the principles of screen design in this presentation; screen design is a field in itself. But there are some basics to keep in mind. First, you should have plenty of white space on your screen, never less than 20% is the rule for the page, never less than 30% is a good rule for on-screen presentations. This helps the user focus on the text.Another basic rule is to use both upper and lower case letters for text; the eye can easily distinguish patterns in letters that are upper and lower case. Use all capitals only for emphasis. X-height of fonts is also important. X-height is the distance from the bottom of a letter to the top of the lower case letter. A font with a low x-height, like Times, is harder to read on-screen than a font with a tall x-height, like Stuttgart. In general, use fonts between 12 and 18 points for best onscreen readability - occasionally larger for titles or, very rarely, smaller for notes. Let's look at several screen examples to illustrate some effective designs. (Screen examples will be shown and discussed here.)Writing style for HyperCard is another important area. Technical writing can be oppressively convoluted and scholarly. This is something you must avoid in HyperCard presentations. Writing for HyperCard must be clear, direct, and pared down. It needs to have a punch to it — active voice, imperative verbs, short sentences, and very specific words. Especially for HyperCard, good writing is clear writing. You should be able to understand the words as quickly as your eye can read them, and text should convey information in as few words as possible.For example, you could change, “You should try to avoid using long, involuted sentences in your presentation” to “use short, clear sentences in your presentation.” When possible, use the font and style capabilities of the Macintosh to pare down your sentences. For example, instead of “If you want to choose open from the file menu, move to the file menu, pull it down, and choose open.” You might write, “To open a file, from the File menu, select Open.” This makes use of fonts to clarify your point and uses about half the number of words.Use plenty of examples - graphic examples where possible. HyperCard lends itself to visual presentations, so exploit this capability. If you can say it in a picture, don't say it with words. Good graphics greatly enhance HyperCard. Any place where a picture will convey an idea, you can use a picture to enhance the text. You can use libraries of existing graphics or scan in custom or existing graphics.The final area we will discuss is the inclusion of animation and sound. The first rule here is “less is better.” Cuteness or pizzazz, which are the basic functions of animation and sound, are wonderful enhancements to presentations. However, they are wearing as a steady diet. There are two very effective ways you can this technique in your presentations. One is to illustrate how to perform an activity. You can use an animated presentation: move to this, press this, and this happens. Another way is to use an animated display for entertainment, to add a little whimsicality to the presentation. You may have seen the demo of a camera with various shutter speeds taking photos with different exposures. This makes a point in a charming way with animation, and is very effective in small doses.In the same way, sound can capture the ear, focus the user, simulate an event, or provide a background. If you are using background music, it should be unobtrusive, and in line with the tone of your presentation. I would never recommend that you use music as a background for an entire presentation, but only for portions. For example, in one of the case studies we are going to look at, we had an automated tour of several topics. During the automated presentation we used a musical background. If you decided not to take what we called the “guided tour,”but to find your way through the presentation yourself, there was no music. The music was there to keep you engaged while things were happening on the screen.With that, we will review the case studies, and talk a little about them. We will use our checklists to evaluate the presentations and their success or lack of success, and learn from the presentations themselves.*In conclusion, I want to summarize the points we have discussed here. We opened this presentation with overall management concerns. We presented guidelines for projects to determine which should or should not be attempted in the HyperCard environment. Once we determined that a project would benefit from presentation in HyperCard, we discussed the steps necessary to plan for and organize the development of this application.We discussed mapping techniques, and methods to keep the user from getting lost or feeling overwhelmed by this new environment and strategies for helping the user navigate through an application. We discussed how to build these keys into the original application design. We discussed the importance of consistency for icons, metaphors, and screen layout, and discussed basic criteria for good screen design.We also explored HyperCard writing techniques, focusing on how to pare the writing down to the minimum necessary to convey meaning. We discussed how to present text in ways that make it memorable in a screen environment, as opposed to designing for the page. Finally, we discussed the use of sound, animation, and graphics, and how these can be used to accent a presentation.We demonstrated two case studies, evaluated them according to a checklist, and discussed user reactions to the case studies, and what was learned from user feedback. Our belief is that thoughtful selection and planning can lead to excellent HyperCard applications.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {41–44},
numpages = {4},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74319,
author = {Embad, A.},
title = {The Relationships between Online Help Systems and Print Documentation: An Empirical Investigation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74319},
doi = {10.1145/74311.74319},
abstract = {Issues addressing the time needed to learn a software system and the effectiveness of the communication between the end-user and a software system have been receiving attention over the past decade [e.g., Emdad, 1988, Pepper, 1981, Way, 1982]. This paper reports on an empirical investigation on the instructional effectiveness of the printed software documentation versus the online help facilities of a software system.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {45–48},
numpages = {4},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74320,
author = {Puscas, M.},
title = {A Survey of Technical Computer Users Resulting in Guidelines for the Development of Technical Computer Documentation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74320},
doi = {10.1145/74311.74320},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {49–65},
numpages = {17},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74321,
author = {Oram, A.},
title = {Sentence First, Verdict Afterward: Finding the Prerequisites for Good Computer Documentation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74321},
doi = {10.1145/74311.74321},
abstract = {Computer documentation reflects the underlying structures and relationships within computer systems. Therefore, successful documentation depends on understanding and interpreting these structures and relationships, not on superficial improvements in writing style, format, presentation philosophy, or technical medium.This paper proposes that the research and writing of documentation be driven by the structure of the software. The paper identifies tasks to be performed on the design side of the software, and on the documentation side.The most formal and technical part of this paper covers the responsibilities of the engineers, and provides writers with a proposal they can present to their reviewers. This section lists the basic categories of features that engineers must cover (flags, counters, identifiers, table entries, and raw data), as well as what to document for each feature. It is the engineers' responsibility to provide a context for each feature on the system, showing how it would be used in real life.Based on this feature-by-feature information, writers must build examples and procedures of gradually increasing complexity. The resulting documents contain immediately applicable information, and are easy to verify and review.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {77–85},
numpages = {9},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74322,
author = {Simpson, M.},
title = {Users Invoked: How Documents Help Readers Assume User Roles},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74322},
doi = {10.1145/74311.74322},
abstract = {A typical and often productive response to writing for documentation audiences is to think in terms of types or genres of documentation—reference manuals, tutorials, “using” manuals, quick reference books and cards, and so on. These categories can help writers think about readers, at least the general purposes for which readers consult documentation.Unfortunately these forms of documentation say little about the many local decisions computer users make as they read documentation. Sticht (1985), Diehl and Mikulecky (1981) and others have described how readers may read to do specific tasks or read to learn material. Sullivan and Flower (1986) show that users may not read a manual or section of a manual in its entirety, and that when they do read, they read to answer questions that arise during a task. The implication of this research is that users' purposes for reading are likely to vary and that these purposes may be determined by work-related tasks or problems which occur during the tasks.A question arises, however, about the role of texts in influencing how readers read computer manuals. If readers' purposes for reading come from outside text—a task or problem stemming from a task—do the text themselves influence the readers? Rhetorical theory suggests one answer: that cues in texts invoke reader roles which readers take on as they read.The theory comes to rhetoric by way of literary criticism and rhetoric. Gibson (1949-50), for instance, argues that tests imply a “mock reader,” an entity distinct from the real reader, by marshaling semantic and syntactic cues in a text. The rhetorician Walter Ong (1977) develops this idea further; he explains that the mock or fictionalized reader is actually a role created in the text and that the notion of reader roles is relevant to all writing, not just fictional writing: The “historian, the scholar or scientist, and the simple letter writer all fictionalize their audiences, casting them in a make-up role and calling on them to play the role assigned” (p. 74). Ede and Lunsford (1984) apply the concept of reader roles to composition theory and acknowledge that it is applicable to such nonfictional forms of writing as academic journal articles, business letters, and student academic writing.Although the concept of reader roles has been applied to several kinds of writing, the concept's relevance to computer documentation is largely unexplored. If computer documentation does invoke reader roles, then the notion of roles needs to be considered by writers as they plan and write documentation. Writers would have to be certain that their documents invoke a role consistent with their intended uses. To explore this issue, I decided to re-examine the results of user protocols made during a test of the Microsoft Works V2.0 documentation at the Microsoft Corporation. Microsoft Works is an integrated PC applications program that contains a word processor, spreadsheet, database, and communications program.A major goal of the original documentation test was to determine how well users could navigate in and use the Works V2.0 alphabetic reference.1 The reference combined the features of a typical reference—for example, an alphabetic arrangement of key topics and commands—with features of user's guide, such as conceptual explanations of topics as well as step-by-step guides to commands and tasks. My re-examination of the data from the Works test was prompted by two questions:What kinds of roles might computer documentation invoke, and what cues might invoke them?When users read computer documentation containing role cues, how do users respond to them?In this paper I will answer these questions and explain how the answers can affect the writing of computer documentation.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {85–92},
numpages = {8},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74323,
author = {Farkas, D.},
title = {Conceptual Foundations for Computer Documentation: New Distinctions for a New Er},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74323},
doi = {10.1145/74311.74323},
abstract = {Concepts are thoughts made clear and distinct by the distinctions we draw at their boundaries. The concept “conifer” comes about when we begin to make a specific distinction about the features of certain trees. If we cannot formulate such a distinction, we do not have the concept.As the computer industry changes, much depends on our ability to formulate new and relevant distinctions and to thereby refocus old concepts and make new ones possible. Otherwise, our overall understanding of our field will diminish and our day-to-day work will in subtle ways become less effective.As documentation specialists, our view of the computer industry is necessarily different from that of those who design systems, manufacture systems, or market systems. Thus, we need to carve up the universe in ways that are most useful for our work. At the same time, of course, we have to understand and use the distinctions made elsewhere in the industry.The purpose of this paper is to point out four traditional distinctions within the computer industry that are not highly serviceable to those engaged in documentation and to describe refinements upon or alternatives to those distinctions. The distinctions are as follows:
Computer systems and noncomputer systemsComputer hardware and softwareDocumentation and interfacePrint and online documentationAs we shall see, the distinction between computer hardware and software has always presented significant conceptual difficulties in the area of documentation. In the case of the other distinctions, the difficulties have come about or have been exacerbated by technological change.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {93–96},
numpages = {4},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74324,
author = {Reid, C.},
title = {A Distributed Architecture for Document Management},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74324},
doi = {10.1145/74311.74324},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {97},
numpages = {1},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74325,
author = {Glass, R.},
title = {Software Maintenance Documentation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74325},
doi = {10.1145/74311.74325},
abstract = {When technical writers speak of software documentation, they usually speak of user manuals. Certainly there is reason for them to. User manuals produced by software people (and I am proud to count myself as a software person!) have often been so bad that a whole industry has sprung up to publish books containing information needed to supplement traditional developer-written user manuals. Technical writers have played a key role in that industry.But there is much more to software technical documentation than user manuals. Concepts are prepared and documented; requirements are derived and written into specifications; designs are created and recorded in design documents; test plans and results are documented in special test documents; and finally the as-built product is described in maintenance manuals [Glass 88].This paper is about software maintenance documentation. Although user manuals have been perhaps the most spectacular failure in software documentation, maintenance manuals may well be the most costly. Software maintenance consumes well over half of the typical software budget [Glass 81]. Of the maintenance tasks, more time is spent on understanding the software than on any other [Fjelsted 79]. The purpose of software maintenance documentation is to help software maintainers with that (enormously expensive) understanding.The state of the practice of software maintenance documentation is abysmal. There are two extremes, neither of which is satisfactory - either no software maintenance documentation is written at all because of schedule and budget pressures during development, or too much documentation is written and as the software product evolves during maintenance the documentation, because of its bulk, is allowed to become out of date and therefore worthless. In either case, the result is that there is no usable documentation to support the software maintainer.This finding is now new. Over 15 years ago [Ridge 73] stated “There is virtual unanimity in the computer industry on the importance of software [maintenance] documentation … despite this fact, it is the first of the cargo to be jettisoned on a floundering project.” The situation has changed little over the years. In [Sasso 89], a software maintainer is quoted as saying “I'm going to assume this is typical maintenance documentation, not worth the paper it's written on.”Fortunately, there are some trends at work that may counter this problem.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {99–101},
numpages = {3},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74326,
author = {McClaren, L.},
title = {Manuals That Meet Market Demands},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74326},
doi = {10.1145/74311.74326},
abstract = {The greatest challenge you, the writer, will face when creating a manual is to design one that is used, a manual that the user loves, a manual with dog-eared pages, yellow highlighter and handwritten notes. Don't you wish your manuals would look as used as the yellow pages do after just one year's use?Yet, writers are also holders of manuals, many of which we do not use. Is there an out-of-date manual tucked into the bottom shelf of your bookcase, dusty with disuse? Do you know if it is even relevant anymore?The business community is littered with beautiful manuals, color-coded headings, amazing graphics—but they're useless, because the information is out of date. Sometimes it seems as though the writer had been trying to create a document like the U.S. Constitution, which has remained virtually unchanged in 200 years. Now there is a document that has stood the test of time. But it required 55 men almost 17 weeks to bring a document of 4,000 words to publication.Should we be striving for such immutability? Are we expecting to create manuals that will stand the test of time? Is this a realistic or even a practical goal? Or should we design our documents from the beginning for change?A manual is not a dead object, created once, then left on the shelf like a vase. It is a living, evolving document, that must be nurtured with new information, with new approaches. Without this evolution, it will die.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {103–107},
numpages = {5},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74327,
author = {Bellin, D.},
title = {Forms Based Documentation to Support Structured Develomemt and C.A.S.E. Implementation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74327},
doi = {10.1145/74311.74327},
abstract = {One of the most important aspects of structured development is the creation and enforcement of standards. Standards define how a given methodology is to be used within your organization. Examples of standards might include
Which forms and other documents must be bundled togetherSteps in the approval processWhen and by whom certain project steps must be doneMaximum size of a module of codeIt is by now common to have some minimal standards for actual program code and program documentation. However, too often only lip service is given to enforcement of organizational standards for system analysis and system design. In my view, documentation and standardization of these development stages is crucial. Despite the current vogue, such documentation standards will not be achieved simply by purchasing and adopting a CASE tool. As a supplement to such a tool, especially at the beginning, a manual forms based method is necessary. Through the use of forms, analysts are provided with an initial set of standards which may be used in your projects. There is a learning curve the development team must go through in order to gain experience using standards. After one or more projects, a decision is often made to modify the initial minimum set of standards to fit the needs of a particular company. This process is similar to that which you must carry out in tailoring the software life cycle model to your own framework. In practice, a manual system reduces the time it takes to learn a new set of tools.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {109–113},
numpages = {5},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74328,
author = {Patterson, D.},
title = {The Myth and Realities of C.A.S.E. for Documentation},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74328},
doi = {10.1145/74311.74328},
abstract = {Hypertext seems to be the major focus of user documentation groups, and even some system documentation people. But system developers, engineers and architects are interested in C.A.S.E. More of our documentation work will be coming from or going into C.A.S.E. solutions and integrated systems, and documentation groups should begin to look very seriously at what C.A.S.E. advertises itself to be, what it is in fact, and what r\^{o}le documentors are likely to play in the face of this touted “software development revolution.”C.A.S.E. is an acronym (and we thought those went out in the 70s) for Computer-Aided Software Engineering. Very little is new here, except the combination. A C.A.S.E. tool is one of dozens of possible pieces of software, techniques, and documents that were developed since the 70s, and which, theoretically, can be combined to create a black box for system generation. The object of the exercise is to put clear, organized, system documentation into the black box, and out comes a working system, for whichever computer (called a platform) you want, with complete user documentation, both on-line and “manual”. The concept should be highly threatening to documentation people, and yet I have not heard a single murmur in the community. The reason, I suspect, is not only that Hypertext is more distracting, being perceived as more glamorous, but also that C.A.S.E., in order to sound modern and revolutionary, has enveloped itself with a great deal of new jargon.The first step of the process, supposedly, needs to be done only the first time you begin using C.A.S.E. Actually, it is such an expensive step, that most firms ignore it, hoping for the quick and dirty answer. Plus \c{c}a change, plus c'est la m\^{e}me chose. In this first step the system architect analyzes the organization, known as “the enterprise”, that wants the computer system. He or she determines the minimum amount of data that the enterprise uses to do its business, and establishes the relations and processes that link pieces of data into meaningful combinations, called “information.” The result is “The Enterprise Model”. A system is conceived as a view of the enterprise model, that is, one department's use of the data and processes that really belong to the enterprise.Those of you who are familiar with this material may realize that I am simplifying some of this, but those of you who are not familiar with it are probably already confused, right? For those who are a trifle bewildered, let me get a bit more specific.We have an analyst who has created a very theoretical model of a corporation that says that a company is just a collection of data, and processes (programs, people, authorizations) that manipulate that data. Since most corporations keep more data than they need, and do many processes at least twice, this model can be useful in cutting waste. Because the object of the effort is to create systems that are clean, simple, and use a database, you can see why you begin to write your first C.A.S.E. system by doing not only a Spring cleaning, but a complete analysis of the foundation of your house!Because this model is so important, and because corporations are always changing, this model must be kept as a large software (and sometimes hardware) system of its own, with a huge database that documents each item of data, each process, and each combination. This work ordinarily takes years, because the data must be reconciled to show that Customer name, and Ship-to name, are in the end actually the same, and that each of them points to that basic data item called name. I have chosen an easy one, but you can imagine. The crucial thing about the model is that it must be right. And since the documentation is going to be used later for creating working systems and documentation, it should be clear and concise.* Some system analysts or software engineers believe that this accuracy means that only they can write this sort of documentation. They completely loose track of the importance of the text for later documents, and for the transfer of knowledge to another group of C.A.S.E. analysts who will build the view or modify it. This is not throw away documentation, and it must be done especially well.Such work is enormously time consuming, as I have already made clear, and so I have seen groups just put in the name of a data item, its length, and its data type (alphanumeric, usually), and ignore description, aliases, and other information not important now, but vital to a succeeding generation of C.A.S.E. tool users.Once the enterprise model is completely documented, system analysis can begin. The system is established as a view, by selecting the data elements it inputs and outputs. If the documentation of the data has been correctly done, all one needs is a list. The system is probably already documented as a high-level process, but the details are designed using “upper C.A.S.E.” or front end C.A.S.E. tools. These are microcomputer based drawing tools for ye olde Demarco and Gane and Sarson diagrams, with a little module at the back that makes a small database of the relations and data items. All of this is fun, so this is usually where less far-thinking firms want to begin using C.A.S.E., and it is probably where there is the largest population of C.A.S.E. tools in the spectrum of possibilities. Diagrams are sexy, and people who can't draw think that this is all just a game, but can be passed off as hard work. For the user, there are screen mock-ups that fit into this process. These are usually part of a “Fourth Generation Language” for one is either hidden, or very obvious, as part of the bottom level of the diagramming technique, creating pseudo-code for the “primitive” level of diagrams.The second-largest number of C.A.S.E. tools is probably in this part of the spectrum. They take the pseudocode, which they tell you is not really programming, but documentation, and the data descriptions, and create some form of brute-force computer code. The object of this exercise is to create something nearly foolproof, that is repeatable. Some code-generators produce COBOL because it is comforting, or C because it is trendy. Supposedly, you put this code on your machine, compile it, or simply copy load modules, depending on the sophistication of the tool, and the whole system is supposed to run first go.What actually happens is that there needs to be real programming done in order to tweak the system. If you have an unusual platform, or one with some unusual devices running on it, or you want to have a real-time system, there must be code written to tailor the brute-force code to fit.Virtually no vendor of C.A.S.E. tools has all of these parts. They may present a unified package by buying the rights to remarket someone else's portion of the spectrum. The big 8 (or are there only four now?) accounting and consulting firms usually just buy the company with the C.A.S.E. tools instead of the marketing rights, but the result is the same: there are some rough edges that need to be smoothed over by more code and perhaps some documentation.All right, I have been describing a system—one made up of hardware for graphics, such as “mouses”, software for diagramming and code generation, and methodology for doing the right steps in the right order. The usual possibilities for documentation are apparent, but what is special about this tool?In some ways there is nothing special about it. It only brings together the collection of problems that we have been working on for a long time, and are still hoping to solve, such as better on-line help, better system's documentation for tools that can be changed as the user desires, and better procedures and forms. In another way, it is very special and important, because the heart of C.A.S.E. is documentation. But the conglomeration of these documentation tools is now being put into the hands of software engineers and system analysts, and marketed as though no writing of prose were necessary.To make my point even clearer, let me explain the usual look and feel of C.A.S.E. products:
The data dictionary, encyclop\ae{}dia, or central repository (I've heard all of these three names, and there may be many others) is just an on-line glossary. The standard names of data are entered, followed by pseudonyms, type (alphanumeric), and length (4 bytes, packed). Following this meager description is some prose to say how the data should be used, how it differs from something similar, if it does, and any other important information to the potential user of this data. If this glossary entry is a composite item, made up of other data items, then there is a list of what it contains, usually provided by the software, e.g. “MAILING LABEL” contains “FULL NAME”, and “ADDRESS”. When you look up ADDRESS you find it contains ADR LINE1, ADR LINE2…POSTAL CODE.
What most of the project spends its time doing is looking at these data entries to decide if this is the one they want, and if it is, connecting it to process entries, or other data entries. For instance, an analyst wants to build a system for keeping an inventory of office furniture (there have been thefts lately), so he needs to find some identifier for the office furniture, and he might just use the catalogue number from the purchase order for this furniture. He them builds a new record that attaches that catalogue number with the name of the employee who has been assigned that desk. The employee name, and the catalogue number exist in the glossary, so he must find them, probably by starting a search through the personnel record description, and the purchase order records. He then copies the data names, and creates his new record containing them. He then attaches the record to his new system name. Sorting and searching requires some good text, preferably brief, and to the point. There are a great many names in any enterprise. Naming data well is a special skill that is best done by someone with a good vocabulary, but only prose will make it clear in the end that this is the right data item, and where it can be copied from.
Sorting and searching is fully 50% of any C.A.S.E.-related, system-development project once the glossary is begun. These encyclop\ae{}dic data bases can, of course, be searched in any order. So here we have not only the usual problems of on-line documentation—how to put it all on a single screen and make it clear—but also the complications of a Hypertext—we know not in what order the information will appear to our reader; and we can give him no clues in terms of position in a book, as to what he might have been expected to read before he began reading here.Once the data has been identified, or more usually, while it is being identified, the children are playing in the Demarco, Gane-Sarson sandbox. Here are diagrams that are good tools for the designer, but which are useless without the glossary, and so are often ignored once the preliminary design phase is done. The circles and boxes ordinarily cannot have the well-designed names shown in Demarco, et al., because these diagrams are being used by the system to generate linkages for use later in the code generator. In some systems comments are allowed, and are usually ignored. The diagrams are the most attractive part of the process, but they are often cumbersome as they extend past the boundaries of the screen, or, when reduced, are too small to read. So while they are updatable and so on, they are employed much less than the glossary over the life cycle of the system.Imposed on all of this development so far is the notion of life cycle. This is some plan that can only partially be imposed by the system, and involves a good deal of good common sense. Everyone, without exception, hated the 1970s system development methodologies that were books of long-winded aphorisms followed by forms to copy a thousand times and fill out, one for each of the one thousand data items to be used in this system. And yet behind all these forms and writing down of mother's reminders to wash behind your ears, there was some important stuff, which cannot be done away with.
Most C.A.S.E. tools try to gloss over the methodology by giving you a diagram, and telling you to wing it, hoping that you have learned enough in a course on the subject to do the right thing. Teaching planning, good habits and good models on which to base standards is still a job for writing, film or some other, non-software medium. All C.A.S.E. tools give lip-service to the methodology. None addresses it squarely for someone who has never used a C.A.S.E. tool before. The bulk would be bad psychology; the exposure to the real length of time all of this will take, and the effort involved would kill sales.Screen mock-ups begin pretty early on. Supposedly this is where user testing begins. The usual 4GL method of mocking up screens is integrated with the glossary and the diagramming tools to put the right data with the right length of field on the screen. Text for prompts may be taken from the glossary. It is a rare firm that sees this testing as the first opportunity for Usability Testing of the documentation and user interface, because the C.A.S.E. myth says that documentation will come as part and parcel of the system development process. This means that the documentation that users will be expected to see will be the prose in the glossary—if anyone could be bothered to write it.
Although there is nothing to stop an enlightened system developer from hiring a psychologist to help in the development of the user interface, or a good word monger to put the mot juste in the prompt to keep the screen clear, and easy to use, yet not condescending, there is also nothing in the C.A.S.E. methodology to promote the use of experts here beside the user, who knows his system best.
Here I may add that in addition to the myth of C.A.S.E. there is something of a myth that the users knows what they want, meaning what they need. The reality is that they know what they like, and that may just be the best of whatever they have seen so far. Shown something better, they usually ask for the system to be redesigned, and the cyclical part of that life cycle comes into play. Most C.A.S.E. tools recognize the cyclical problem, and try to take care of it here in the screen development. Much time may be wasted here because only software engineers and users are involved rather than experts in user interfaces.Once the screens are accepted, then coding begins. Of course this is not supposed to be programming, just the creation of pseudo-code, that last element in the Yourdon, Demarco, Gane-Sarson methodology. Actually it is programming in some version of a Fourth Generation Language, with some Third Generation features to take care of “platform” dependencies. Since this coding is part of the C.A.S.E. product no system documentation is needed over and above the glossary and diagramming. Actually, since the tools are usually separate, either there is some “reverse engineering” to take the code back to the glossary, or the code is really undocumented. As we know from many published papers, 4GLs are not easily interpreted without the help of documents.Once the generated code is merged with the platform-dependent modules, the code is supposed to be running. Actually, patches may well be required. There is practically no way to document the patches neatly in any of the glossary or diagramming niches, except in text.Some C.A.S.E. tools, which I will carefully not name, then claim that it is possible to use all the documentation that has been accumulated thus far to generate the user documentation automatically. What happens is a long print out of the help screens, which are, in turn, the text from the glossary. Reams of text with SGML tags is technically documentation, but of the sordid kind that I, for one, would like to see abolished. There can never be a point to documentation built out of homogenous blocks of object descriptions. There is no place for insight. If the C.A.S.E. tool permits the addition of text for prefaces and the like, then the documentation becomes ancillary to the construction of the system, and all the usual problems of keeping the documentation current will reappear.Having sat through what must seem like a diatribe on the design of C.A.S.E. tools, let me say that I like them, and believe that they are exciting additions to system development. What I decry is the promotion of them as minimizing the need for experts, such as documentation and user interface specialists, and even good programmers who can code in assembler. It is a myth that a few good software engineers, and a few sterling users will necessarily build great systems with C.A.S.E. tools in practically no time.Let me highlight the r\^{o}le of documentation experts in C.A.S.E.I perhaps have omitted something, and you will see it, and comment on it. Documentation, it seems to be me, is made more valuable because of C.A.S.E. tools, and requires more skill than is now given to the training of software engineers and analysts. Most computer science programmes do not give any instruction in writing skills for documentation, and throw the wrong kind of professional into the breech of C.A.S.E.While I have every sympathy for the marketers of C.A.S.E. tools who want to emphasize the good design, the coherence of systems' and corporate goals, and the benefits of simplified maintenance, there is a myth here that all of this can be done without very many people, and those few are system engineers and users. The truth is that a great deal of the value of C.A.S.E. lies in particularly well constructed documentation, and good programming skills in a high-level language. There is no simple black box that eliminates the need for talent, for clear thinking, for programming and for maintenance. The only thing that may have been eliminated is the coding of simple tasks in COBOL. None of the documentation is eliminated, rather adroitly constructed texts and supplementary materials are more important than ever before. C.A.S.E. tools require the skills of experts to use, and they challenge us to do much further research to bring the promise of C.AS.E. to fruition.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {115–119},
numpages = {5},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74329,
author = {Hinds, A.},
title = {Maintaining Multiple Versions of a Document},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74329},
doi = {10.1145/74311.74329},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {121–124},
numpages = {4},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74330,
author = {Black, J. E.},
title = {Scribe Support in GNU Emacs},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74330},
doi = {10.1145/74311.74330},
abstract = {Think of Scribe1 as a compiler for a document description language; Scribe is NOT an editor. Scribe is a “high level” document processing system, or a “composition engine” which permits users to deal with documentation at a higher level of abstraction than is possible with “word-processors” or “page processors.” One of the methods used to provide this higher level of abstraction is the separation2 of the “form” or “layout” of the document from the “content;” thus avoiding distraction of the document's author with superfluous details of document format. This frees the writer to concentrate on the content of a document, rather than its format.With the increasing popularity of WYSIWYG3 style editors, which are more properly described as “page processing” systems; fewer people are willing to insert the type of “mark-up” commands required to properly use a “document processing” system such as Scribe. Described herein are a set of support functions, written in a dialect of LISP, which provide assistance to the Scribe user during the preparation and composition of documents. These support functions provide “short-cuts” for insertion of Scribe mark-up, as well as certain features useful during composition and maintenance of large documents. Collectively, these support functions are called “Scribe Mode” and are written to be used with the “GNU Emacs” editor.4 GNU Emacs is known to run under the Unix5 and VAX/VMS6 operating systems, and various versions have been observed to operate on a wide variety of host computers, and other operating systems.},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {125–135},
numpages = {11},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74331,
author = {Leonard, D. and Waller, A. L.},
title = {Usability Planning for End-User Training},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74331},
doi = {10.1145/74311.74331},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {137–142},
numpages = {6},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

@inproceedings{10.1145/74311.74332,
author = {Ballard, F.},
title = {Executable Documentation: Testing the Documentation, Documenting the Testing},
year = {1989},
isbn = {089791337X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74311.74332},
doi = {10.1145/74311.74332},
booktitle = {Proceedings of the 7th Annual International Conference on Systems Documentation},
pages = {143–146},
numpages = {4},
location = {Pittsburg, Pennsylvania, USA},
series = {SIGDOC '89}
}

