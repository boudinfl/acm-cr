@inproceedings{10.1145/10563.10564,
author = {Wiecha, Charlie and Henrion, Max},
title = {Separating Content from Form: A Language for Formatting on-Line Documentation and Dialog},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10564},
doi = {10.1145/10563.10564},
abstract = {Recent research has demonstrated the advantages of separating management of the user interface from the application program. A user interface system should integrate access to on-line help and documentation with other dialog for interacting with the program into a uniform environment. We describe such a user interface management system, called ICE, with emphasis on its facilities for authoring networks of frames containing help information and menus for interacting with the application program. Authors can write help and dialog using a language similar to the SCRIBE document processing system, widely used at CMU and elsewhere. But instead of generating hardcopy documents for different printing devices, ICE produces interactive “softcopy” documents, consisting of a network of frames combining documentation and interface.In ICE the screen layout of frames and the style of interaction is specified in a format file which is separate from the dialog file that contains the text to appear in the frames. This separation allows the dialog author to write the text without having to worry much about its precise appearance on the screen. The display designer can specify the actual format independently. The same text can be formatted in different ways to make use of different display devices and to allow experimentation with alternative formats and styles of interaction.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {1–7},
numpages = {7},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10565,
author = {McKee, John B},
title = {Computer User Manuals in Print: Do They Have a Future?},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10565},
doi = {10.1145/10563.10565},
abstract = {What sort of a role will the printed page play in the computer user manuals of the future? I believe that print does have a future in this area, but not perhaps the future we might have foreseen five years ago. At that time nothing was less controversial than the viability of print as a medium of documentation. That viability is in question now, and to show how the questioning developed I propose to examine its beginnings in the recent past. I shall then go into some detail on how the controversy about print is being maintained at present. Finally, I shall explain how I feel the controversy is likely to be resolved, by making four predictions about the future of computer user manuals, and those whose job it is to produce them.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {8–14},
numpages = {7},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10566,
author = {Smith, Richard H},
title = {ADL—A Documentation Language},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10566},
doi = {10.1145/10563.10566},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {15–17},
numpages = {3},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10567,
author = {Ouchi, Miheko L},
title = {Software Maintenance Documentation},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10567},
doi = {10.1145/10563.10567},
abstract = {This work aims at presenting a model for assisting in the development of Information System for software maintenance documentation.It seeks to emphasize the optimization of the functions related to the documentation, by employing automatic processes.In this study the system of documentation is based on a Data Dictionary structure, adequately designed for the organized storage of the information needed for the execution of the maintenance activities.Forms of the information input, updating and retrieval have been analysed with the intention of utilizing the basic software of support, as well as to consider the possibilities of the Dictionary interface with the Operating System file, throughout the modification of the application software.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {18–23},
numpages = {6},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10568,
author = {Callender, E D and Yamamoto, Y and Childs, D B and Farney, A M},
title = {Documentation Generation from a PSA Database},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10568},
doi = {10.1145/10563.10568},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {24–33},
numpages = {10},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10569,
author = {Schilling, Peter E and Wizzard, John T},
title = {Evolution of Program Documentation through a Long-Term Project},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10569},
doi = {10.1145/10563.10569},
abstract = {This paper is a case study with some recommendations. Over the course of a four-year (twenty person-year) project, the method of documenting programs changed from using separate files containing input data for a documentation lister, to placing all of the program documentation in the source code and using an extraction program when separate documentation was needed. The history and reasons for this evolution are described. The project revealed the usefulness of documentation tools including program templates, a good cross-reference lister, in-line documentation, and the extraction program.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {34–43},
numpages = {10},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10570,
author = {Barnett, Michael P},
title = {BASIC and DOS Jobstreams in IBM PC Software Documentation},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10570},
doi = {10.1145/10563.10570},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {44–54},
numpages = {11},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10571,
author = {Solem, Ann},
title = {Designing Computer Documentation That Will Be Used: Understanding Computer User Attitudes},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10571},
doi = {10.1145/10563.10571},
abstract = {At the Los Alamos National Laboratory we have over 15 years of experience in designing computer documentation for computer users of the Los Alamos Integrated Computing Network (ICN), which includes five major operating systems. Currently there are over 6,000 users of the ICN: programmers, scientists, technicians, managers, technical editors, and secretaries. They can choose from more than 1,000 printed documents and a variety of online information sources.Because of this plethora of information, many users do not know where to find the information they need. And, after finding the right document, they may be dissatisfied by the way the information is presented.Over the last few years we studied our computer users and their needs for information. Our objective was to develop a model that could be used to organize the mountain of computer documentation that is needed for complex computer networks.In this paper we present the results of this study.We find that user attitudes toward information acquisition fall into three categories: (1) no time to learn, (2) want to learn, and (3) know what I want.No-time-to-learn users generally are using the computer as a tool and are trying to accomplish something in their own discipline. They need Quick Steps documentation that shows them what to do without explanation.Want-to-learn users have the time and inclination to learn a computing topic in a methodical way. They need Learning documentation that teaches the use of computers by developing an understanding of computing concepts and providing a foundation for generalization.Know-what-I-want users generally are knowledgeable in a specific computing area and simply want to look up particular information. They need Reference documentation.We find that these user attitudes significantly determine documentation needs, even more so than do experience or background. We reject the significance of alternative factors, including casual vs. expert; programmer vs. nonprogrammer; scientist vs. manager vs. secretary; and inexperienced vs. experienced. These factors tend to pigeonhole people and fail to recognize that a user can have different documentation needs at different times of each day.The types of documentation resulting from this understanding of our computer users are (1) Catalog, (2) Quick Steps, (3) Learning, and (4) Reference.Catalog documentation helps all users select the systems and documentation they need. It compares system capabilities in tabular and textual forms. It may describe computing concepts that users must understand in order to make their selections.Quick Steps documentation shows no-time-to-learn users what to do without explaining it. There is little reading and there are lots of examples. Development of Quick Steps documentation is appropriate for systems that are used by many people.Learning documentation teaches the use of computers by developing an understanding of computing concepts and providing a foundation for generalization. There is lots to read and try. Learning information can be provided in a variety of forms, including tutorial manuals, computer-assisted instruction, and live or videotaped courses. It is useful for want-to-learn users who have the time and inclination to learn a computing topic in a methodical way. Development of Learning documentation is appropriate for systems that are frequently used and where in-depth understanding by many people would be useful.Reference documentation can be provided in a variety of forms, including comprehensive reference manuals, quick-reference cards, online help, and glossaries. It is useful for know-what-I-want users who generally are knowledgeable in a specific computing area and simply want to look up particular information. All computing topics require Reference documentation.Additional documentation, such as news about changed computing capabilities and public information for laymen, is also required.We believe that it is necessary to consider computer user attitudes in order to design documentation that will be used.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {55–56},
numpages = {2},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10572,
author = {Angerstein, Paula},
title = {Better Quality through Better Indexing},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10572},
doi = {10.1145/10563.10572},
abstract = {Readers of technical documentation generally agree that the information in those manuals is only as good as the ease with which they can find it. An informative and accurate index is one of the best tools for helping the reader find information quickly and easily. Yet indexes are one of the most neglected areas of technical documentation, in part because the tools used for creating indexes have not kept pace with other document creation tools.This paper discusses the qualities of a good index, and how different index creation tools can hinder or contribute to achieving those qualities. The method developed at Burroughs, which provides capabilities for generating high-quality indexes easily, is described.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {57–60},
numpages = {4},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10573,
author = {Patterson, Diana and Hallgren, Chris},
title = {Documentation's Recognition Problem: What Can We Do about It?},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10573},
doi = {10.1145/10563.10573},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {61},
numpages = {1},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10574,
author = {McArthur, Gregory R},
title = {If Writers Can't Program and Programmers Can't Write, Who's Writing User Documentation?},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10574},
doi = {10.1145/10563.10574},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {62–70},
numpages = {9},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10575,
author = {Worona, Steven},
title = {An Informal Overview of CUINFO (Cornell's Computer-Based Bulletin Board)},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10575},
doi = {10.1145/10563.10575},
abstract = {Judith Turner's article on CUINFO (The Chronicle of Higher Education, July 10, 1985) resulted in a number of requests for more information. This note is intended to answer the most frequently asked questions. For more details, please get in touch with me at 401 Uris Hall, Cornell University, Ithaca, NY 14853; (607)256-4981; SLW@CORNELLA.BITNET.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {71–77},
numpages = {7},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10576,
author = {Borenstein, Nathaniel S},
title = {Help Texts vs. Help Mechanisms: A New Mandate for Documentation Writers},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10576},
doi = {10.1145/10563.10576},
abstract = {To compare different methods of accessing and presenting on-line help, controlled experiments were conducted. Several different help systems were compared, including a natural language help system and a human tutor. The results indicate that, while varying the help mechanism may have some effect on learning, its importance is greatly overshadowed by the simple quality of the help texts being presented. Even for on-line help, good writing seems to be the most important part of helping the user, far more important than elaborate or sophisticated mechanisms.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {78–83},
numpages = {6},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10577,
author = {Kahn, Russell L},
title = {Improving Systems Documentation Using an Online Copy Editor},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10577},
doi = {10.1145/10563.10577},
abstract = {For the last three years I have been using Writers Workbench, a UNIX text-editing tool, to edit computer documentation. In this article I outline my experiences using the system, noting both the advantages to explore and pitfalls to avoid in using this tool. Writers Workbench is especially useful for improving a writer's basic skills—punctuation, spelling, and grammar. When used effectively, Writers Workbench can cut down on wordiness and improve the consistency of a manual. It can help in the creation of a table of contents, index, glossary, and bibliography and in checking readability. Furthermore, by creating user-defined dictionaries, authors or editors can customize the tool to fit their purposes and styles. However, Writers Workbench is not good at catching problems relating to organization, focus, and clarity.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {84–87},
numpages = {4},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10578,
author = {Chisholm, Richard M},
title = {New Metaphors for Understanding the New Machines},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10578},
doi = {10.1145/10563.10578},
abstract = {Though frequently misunderstood and dismissed as irrelevant ornamentation, metaphors are useful tools for writers in the computer industry. Metaphors are especially useful for presenting information about new technologies because they help readers grasp whole concepts immediately and because they illuminate concepts that are difficult to communicate otherwise. It is important to distinguish this use of metaphors from their use in literature and advertisement.Participants in the workshop will follow procedures for investigating the suitability of several metaphors. They will analyze the power and appropriateness of metaphors currently used in the computer industry by applying seven criteria: Is the metaphor useful? understood? close? illuminating? acceptable? economical? memorable?},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {88–96},
numpages = {9},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10579,
author = {Cross, John A},
title = {A Paperless Environment for Group Effort in Documentation Development},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10579},
doi = {10.1145/10563.10579},
abstract = {Research into the effect of word processing on writing and a paperless environment for the submission and grading of student assignments has led to considerations for new technology to support the development of system documentation. We assert that a relatively paperless system which integrates the concepts of word processing, electronic mail, computer conferencing and the HANDIN paperless system for student assignments (Cross and Wolfe, 1985) has potential for facilitating the process of developing system documentation. A conceptual framework and specific system features are presented. Research goals and methodologies are briefly considered.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {97–101},
numpages = {5},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10580,
author = {Weizenbaum, Pm},
title = {Creating a Campus On-Line News System},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10580},
doi = {10.1145/10563.10580},
abstract = {Information Systems, MIT's campus-wide computing service organization, recently reorganized and strengthened its resources. Out of this recent effort came the decision to explore several ways of reporting on the expanded range of systems and services we offer.One service that central computing facilities must provide is timely notice of changes to the supported systems. This paper presents the design and implementation of Information Systems' “On-Line News System”, which keeps users updated about changes in the wide variety of services offered by Information Systems.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {102–107},
numpages = {6},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10581,
author = {Hallgren, Chris},
title = {Factors Affecting Readability},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10581},
doi = {10.1145/10563.10581},
abstract = {No one has found a way to really help writers create readable prose. Robert Gunning developed a method for calculating the 'Fog Index' and Rudolph Flesch worked out more than one formula for measuring the simplicity of writing. By one of Flesch's formulas (the one without personal pronouns), Ronald S. Lemos in the February, 1985 issue of Communications of the ACM (CACM) was able to prove that CACM required two less years of school to read than Datamation. Statistics can prove anything. I have no idea what Sophomore in High School could read the CACM cover to cover and even understand most of it.Flesch's book 'The Art of Plain Talk' was given to me at a Yourdon Systems Analysis course. The Instructor handed it to each of us, saying something like “read this and you'll be a manager in no time” (supposedly, management is handed to the least efficient person who can also write well). The book is full of examples, mostly journalistic, showing how good writers evoke human interest. Of course, these writers had human events, thoughts and feelings as their focal points, not software, I doubt whether any of the graduates of that week ever used Flesch as a reference for grading their own documentation. How would Bernard Shaw have documented software? Or Mingus played it? This paper addresses these burning issues.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {108–109},
numpages = {2},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10582,
author = {Burdett, Paul S},
title = {Documentation: Effective AND Literate},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10582},
doi = {10.1145/10563.10582},
abstract = {The purpose of this paper is to show how documentation can be literate, in a stylistic sense, and still be effective. Literate prose is a powerful tool that, when properly used in computer documentation, can take advantage of the full power of the English language. This does not mean that all computer documentation must or can read like a Nobel Prize novel, but neither does it have to read like a military cryptogram. A happy medium — founded on healthy grammar and syntax, and following the logic of the software being documented — is a good AND obtainable goal.The roots of the problem of literate documentation lie in a common complaint from software users: “The program looked great in the store, but the documentation is so awful that I can't get it to work!” This is particularly acute in the field of educational software — but applies across the board — because the product cannot fulfill its purpose nor its potential unless it can be effectively employed. The problem is usually caused by an excessively technical focus, and/or poor composition (that obscures the information). A person should not have to be a computer scientist or a programmer to use software; that would defeat the purpose of the product. On the other hand, the user should not need to be a philologist to discover the author's intent. In short, documentation we compose needs to be truly “user-friendly.”The major obstacle to producing effective documentation is selecting the appropriate conceptual approach. The two main tendencies when tackling the problem are both extreme: often, there is either over emphasis on academic modes of expression, or on the technical aspects. These are often inappropriate as people want beginning words and concepts when they are “beginners.” They will enjoy high-tech once they grasp the basics; they want to how-to, not why.This “how-to” label is a critical issue in developing a good conceptual approach to documentation. In the field of software documentation, the “technical writing” mindset can often derail what would otherwise have been an effective product.Far too many people are seriously misled by the term “technical” writing and it is unfortunate that the word has been so broadly applied. The use of “technical” conjures a dominant notion that, in our case here, the writer must tell the reader all there is to know about hardware and programming. If the purpose of the document is to explain such things, then that is certainly fine and can legitimately be called “technical writing.”However, the technical details are often precisely what the non-technical user does NOT need to know; most really do not care about the electronic complexities, they just want their program to work. Hence, they need “practical” documentation: that is, documentation that tells them how to perform a function or set of functions so they can do whatever it is that they want to do (one might also wish to define it as being task-oriented or task-specific rather than technology-oriented). So, the rule of thumb should be to tell them NO MORE than what is necessary to use the program.There is also an economic bonus attached to practicality. Supporting documentation that is unnecessarily abstruse will deter program use, and it is altogether likely that further offerings would consequently not sell. Keeping it “sweet and simple,” as they say, also keeps it saleable.The term “practical writing” is preferrable to “technical writing” because the term conceptually moves away from a nuts-and-bolts orientation toward a “how-to” approach.Despite the fact that computers have been with us for some time and that personal computers are now the rage, the average user still feels somewhat intimidated by the machine and its accoutrements. Therefore, they certainly do not need to be further put off by the software they wish to use. Any computer magazine or journal you can select will show the current over-emphasis on hardware, diskettes, and their capabilities. However, a corresponding under-emphasis on the accompanying documentation exists. For example, in five recent issues of BYTE magazine, there were 134 feature articles of which only TWO would be of any use to documentation writers. That is rather sad.As the educational software market grows by leaps and bounds,1 and more and more companies become involved in similar enterprises for internal training purposes, it is imperative that not only the programs themselves be impressive and intellectually stimulating, but the accompanying documentation must really SUPPORT the program. Steve Halpern, vice-president of Classroom Consortia Media, Inc. (Staten Island, NY), addressed the problem — for both programs AND documentation — thus: “The computer should be a non-threatening tool to help the teacher. So software needs to be free of problems, of any kind of sophisticated codes that have to be put in to make it work. It should have all kinds of error-trapping that lets it do what it's supposed to do, which is reinforce learning and teach concepts.”2 The computer should not be designed as a teacher replacement but, rather, to be a challenging teaching aid and learning tool. Consequently, the accompanying documentation needs to be much more than merely a textbook supplement or user's manual. Rather, it needs to be a bit of both, which is a difficult marriage to be sure, but necessary. We who produce documentation of this sort (and others) MUST be sensitive to the genuine needs of those who will be using the programs and their supporting material. This is to say that we must seriously consider our potential audience and resist the temptation to just “crank out” a document. The maxim “haste makes waste” certainly applies here.Kathy Yakal, Editorial Assistant for COMPUTE'S! PC &amp; PCjr magazine, has said, “Courseware should be enjoyable, open-ended, and exciting. And humorous. Intriguing. Authentically pleasing. It needs to be easy to use. It must present concrete demonstrations of abstract ideas whenever possible. Provide constructive feedback for errors. Be accompanied with clear and complete manuals.”3 Equally important, it must also teach by its own example: which means that it should not include the kind of grammatical errors that are displayed in the foregoing quotation. The writer must be dedicated to producing documentation that is just as grammatically sound as it is accurate because poorly expressed information is worth little or nothing.Consequently, to produce a document that will function as an effective information base, the writer must walk a tough but necessary tightrope between two extremes. Information must be clearly expressed, and there is just no excuse for ignoring the rules of good English composition.To produce good documentation, we need to be very sensitive to writing as a mechanical process and the strategies employed by writers to present the information necessary to fulfill the required task. One might have all the best intentions of blending the academic and technical approaches, but you know the destination of the road that is pved with good intentions and those intentions go nowhere unless the writer produces really balanced copy.Even talented writers need training to produce consistently effective documentation. The preferred method is formal classroom instruction where the individual can learn various strategies and tactics and gain some intensive practice in the writing and production craft. However, as anyone in business knows, the time necessary for this process is often a luxury. Short company-sponsored workshops can be of assistance, but time is money and money is a precious commodity.With what, then, are we left? With the right tools, a lot can be done on one's own to become a more effective communicator. There are several sources in the marketplace that can be valuable as self-study materials as well as important job aids.4 They will certainly not replace a formal course, but they can at least serve as useful on-the-job training materials.Let us consider a pedagogical tool I devised while teaching tech writing at the College of Staten Island-CUNY that graphically displays the relationships between the five principles of practical writing: organization, clarity, exposition, accuracy and validity. I rather egotistically call it “Burdett's Pentagon.”The arrows are all double-headed simply because the mutual interdependencies are critical to the strength of the whole structure.ORGANIZATION: Organization is the basis of a document. Without proper logical structure, a document will fall flat on its face. Consequently, when planning a document, organize it according to the canons of logic as displayed in a normal syllogism (a word equation).This is an oversimplification, but pursuing this tack will keep a writer properly focused on the necessary steps. It is amazing how much documentation falters because the information is not presented in a logical sequence, and there is nothing more frustrating to a reader than to have to keep questioning the author's meaning.EXPOSITION: Exposition is text production and all its appurtenant features: proper grammar and syntax, effective vocabulary, appropriate reading level, and so on. This is where the writer's style will have the most direct impact and a talented writer should have little trouble. For those with average skills, the task will be more onerous. But, if they follow the syllogistic format, they will at least be able to produce satisfactory copy that will be logically consistent and do the job.CLARITY: Clarity is the connection between organization, exposition and accuracy. If the organization is not clear, the exposition will be muddied. If the data are not clear, then the points one is trying to make will be correspondingly obtuse. For our purposes, clarity is more a psychic requirement: that is, always keep the principle in the front of one's mind as a constant check on the work being performed.ACCURACY: Accuracy applies to the data or other information used in one's document. Quite simply, if the information is not accurate, the whole thing falls apart regardless how well organized or well expressed it might be.VALIDITY: Statisticians will be familiar with the concept of validity5: that is, accuracy itself is not all that is required for good documentation. In addition, we must ensure that the information used pertains to the points we are trying to make. It may well be accurate, but if it is not germane to our subject, the exposition will then be meaningless.It is fairly clear that if any one element of the pentagon is weak, the whole document will then be unsound. Devices such as this simple tool are often helpful to talented writers who may produce poor documentation because they are harried and harassed.English is a very rich tongue that, when properly employed, has tremendous potential for accurately and effectively transmitting instructions and information. Documentation writers should be particularly sensitive to the necessity of employing proper grammar and should utilize appropriate vocabulary because these documents (especially educational software) also teach by example. For instance, I cannot count the number of arguments I have had about the spelling of “through.” I consistently heard lamentations like: “But the road signs say 'thru' traffic, so…it must be OK.” Another example is the roaring battle I once had with another writer who actually said, “What difference does it make if the grammar is bad? It's a science program!” Regardless the subject, there is absolutely no excuse, especially in education programs but also in general, for sloppy exposition.Therefore, when composing documentation, let's be tough on ourselves and do it right, or don't do it at all. One's exposition is often sloppy because one has developed sloppy habits. Therefore, develop good habits, do things the proper way, use the full power of the language, and one's documentation will be exemplary. If we recall the pentagon, poor exposition can adversely affect the clarity of the document and obscure the information that one is trying to convey. Let's not take shortcuts just because it is easy. Let's not invent words (as “prioritize”) when the language already has a perfectly good word that sounds much less jarring (as “rank” or “order”).6 However, at present, there is a lot of documentation on the market that has such faulty punctuation, infantile vocabulary, excessive jargon, poor expression, that it would seem that few writers care whether they set a good example or not.Therefore, to be literate in one's composition is ALSO to be effective: instructions and information will be easy to understand, the documentation will thus mesh well with the program and will succeed in molding a comfortable computer-software-human unit. That's being truly “user-friendly,” literate and effective.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {110–113},
numpages = {4},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10583,
author = {Talburt, John},
title = {The Flesch Index: An Easily Programmable Readability Analysis Algorithm},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10583},
doi = {10.1145/10563.10583},
abstract = {This paper is an exposition of an algorithm for text analysis that can be of value to writers and documentalists. The simplicity of this algorithm allows it to be easily programmed on most computer systems. The author has successfully implemented this test as a function within a text editing system written in RPG II. Included in this paper is a sample program written for the VAX 11/780 in PL/I.In 1949 Dr. Rudolph Flesch published a book titled “The Art of Readable Writing.” In this book, he described a manual method of reading ease analysis. This method was to analyze text samples of about 100 words. Each sample is assigned a readability index based upon the average number of syllables per word and the average number of words per sentence. This Flesch Index is designed so that most scores range from 0 to 100. Only college graduates are supposed to follow prose in the 0 - 30 range. Scores of 50 -60 are high-school level and 90 - 100 should be readable by fourth graders.Though crude, since it is designed simply to reward short words and sentences, the index is useful. It gives a basic, objective idea of how hard prose is to wade through. This test has been used by some state insurance commissions to enforce the readability of policies.Flesch's algorithm was automated in the early 1970s by the Service Research Group of the General Motors Corporation. The program, called GM-STAR (General Motors Simple Test Approach for Readability) was used so that shop manuals could be made more readable. GM-STAR was originally written in BASIC language. The key to this program is a very simple algorithm to count the number of syllables in a word. In general the text analysis portion of the program uses the following rules:
Periods, explanation points, question marks, colons and semi-colons count as end-of-sentence marks.Each group of continuous non-blank characters counts as a word.Each vowel (a, e, i, o, u, y) in a word counts as one syllable subject to the following sub-rules:
Ignore final -ES, -ED, -E (except for -LE)Words of three letters or less count as one syllableConsecutive vowels count as one syllable.Although there are many exceptions to these rules, it works in a remarkable number of cases.The Flesch Index (F) for a given text sample is calculated from three statistics;according to the following formula: F = 206.835 - 1.015 \texttimes{} (W/N) - 84.6 × (L/W).The Grade Level Equivalent (G) of the Flesch Index is given by the following table:A PL/I program that implements this algorithm is listed below along with sample output. For simplicity, this program assumes all letters are in upper case. Processing text with lower case letters can be accomplished by either modifying the program to test for lower case as well as upper case, or by preprocessing the text sample to translate all letters to upper case. There are a multitude of other refinements and amenities that can be added to the basic analysis. Among these are:
Nothing which characters are considered sentence terminators.Ignoring periods that are used for abbreviations rather than sentence terminators.Ignoring word connecting hyphens in compound words.Noting which character groups should probably be spelled out, such as numerals and dollar amounts.Sharpening the syllable counting routine to detect exceptional cases.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {114–122},
numpages = {9},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10584,
author = {Perlman, Gary},
title = {Multilingual Programming: Coordinating Programs, User Interfaces, on-Line Help and Documentation},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10584},
doi = {10.1145/10563.10584},
abstract = {The high cost of software is not due to the difficulty of coding, but in recoding and redocumenting software. This can be better understood when one considers how many expressions of the same ideas must be constructed and coordinated. Program code and comments, user interface and on-line help, and a variety of off-line documents, all must be consistent. A solution to the coordination problem is presented in this paper. Multilingual programming is a method of developing software that uses a database of information to generate multiple target languages like commented program code, user interface languages, and text formatting languages.The method begins with an analysis of a domain to determine key attributes. These are used to describe particular problems in the domain and the description is stored in a database. Attributes in the database are inserted in templates of idioms in a variety of target languages to generate solutions to the original problem. Because each of these solutions is based on the same source database of information, the solutions (documents, programs, etc.) are consistent. If the information changes, the change is made in the database and propagated to all solutions. Conversely, if the form of a solution must change, then only the templates change. In sum, the method saves much effort for updates of documents and programs that must be coordinated by designing for redesign.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {123–129},
numpages = {7},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10585,
author = {Patterson, Diana},
title = {The Impact of Technology on Publishing through the Ages to You},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10585},
doi = {10.1145/10563.10585},
abstract = {This paper presents various technological developments from the Rosetta Stone to the Apple MacIntosh Computer. With the advent of each change the quality of the product decays significantly, and is only restored to something near the glory of the past after a long period requiring much creative endeavor.This paper was presented as a slide presentation with extemporaneous comments.Let me remind you that you are a publisher. Whether you publish only within your corporation, or to a larger market, you are disseminating information relating to computer software. You may publish on paper, in the usual sort of user's guide or procedure manual, or you may publish online in the form of help screen data dictionaries, or video text. You may be publishing conference notices, newsletters, and memos. All of these are publishing activities, and as a publisher you should be aware of publishing technology and its impact on your readers.Let me also remind you that “technology” did not spring up 10 years ago, or with the advent of the Twentieth century, or even with the industrial revolution. Technology is, after all, a word derived from the ancient Greek techne, meaning art or skill, and logia, meaning study or science. Further, computer technology did not spring up fully grown, like Athena from the head of Zeus, despite the implications of computer salesmen. Computer technology is just one more step in the history of ideas. It is not necessarily the pinnacle of that history either.Look at the average piece of paper that crosses your desk. More than likely, it comes from a word processing system. You are probably happy with it because it was bold face as required. Fancy formats to accommodate illustrations were no trouble. Even graphic enhancement of the Pharaoh's face was part of the job, if requested.The advent of papyrus, a less durable, but lighter writing medium than stone, created the word processing pool. In the scriptorium, Egyptian artists would make copies of the original texts, probably personalized for the class of audience, so that, while the papyrus could not be posted for long in the market square for all to see, copies could be sent by runner to a wider range of villages for more people to read.The problem with the papyrus technology was the limited supply of papyrus. It did not grow in cooler climates where civilization seems to have crept. Thus developed the parchment technology. Parchment is made from split and pounded sheep skin. For large documents, such as maps, the whole sheep skin was stretched on a rack, rather like an embroidery frame, or drum surface, and the colours and inks were applied while the surface was taut. For the more ordinary reading material, the skins were sewn together, and cut into long narrow, continuous forms, and stored on sticks, forming scrolls, or volumes. The advantages and disadvantages of the scroll format are known to many computer users. Eventually the scroll format was replaced with a side-ways columnar format, rather more like the “screen-full” of text used by many computer systems.The next advance was to fold the scroll side-ways to create pages with a column on each surface. This developed into the book, or “codex”, that valuable random-access storage system for information which was such an improvement over the sequential system. With the advent of the page, came the possibility of the index. The codex was a Roman invention, but the index is still not used in a great number of books.The medieval codex contains a spectacular use of ink, and pigment technology, combined with the surfacing technology of the parchment. After more than a thousand years pages are still white, with easy to generate, and compared to a manual typewriter, the result probably looks pretty good. The proceeding paper that you are reading is not the better sort of computer output—quite deliverately so, but it is pretty usual, and considered acceptable.Look at the average computer screen that you use. It is usually over-crowded, backlit in green, white, or amber. The lettering is often terribly distorted monospaced type which is very hard to read for hours on end. In fact, we don't usually read screens, we recognize patterns on them. These pieces of paper, and these screens are the signs of our advanced technology.Now I am going to ask you to consider the technological changes that have preceded the usual paper manuals and screens that we use in our offices: before Macdraw before Macwrite in “Old English”; before all those computer graphics posters, and colour graphics terminals.The first publishing technology I want you to consider is the Rosetta Stone. Next time you are in the British Museum take another look at this monumental tribute to publishing. The Rosetta Stone is a multilingual document, in clear, proportionally-spaced type. There was only one, but it was a published document because it was placed in a public place as part of a large edifice, and therefore everyone of any importance would visit this place and have a chance to read it in the language of his choice.The next important technological change in publishing came with the development of papyrus as a writing surface. This was not an overnight success, I'm sure. It must have taken a long time to develop the right sort of combination of surface and inks to produce the types of papyrus documents we have preserved. They have drawings in colour which accompany the text, usually written in columnar form, in a good clear lettering, proportionally spaced, and still legible, and flexible after 5-10 millennia. Print-outs were available in full colour, with multi-colour texts, variable fonts, and brilliant, full colour illustrations. Texts are proportionally spaced, there are multi-lingual pages with varying column widths to suit the needs of the translator or commentator. Very often the texts are justified to both left and right margins, with column balancing as required, and specially tailored illustrations. Ornamental headings and first letters of text include references to the owner of the book, so that the illustration of a man's book would include his coat of arms, or his name, so that his copy was not like any other.There are texts of this technology created as special graphical jokes, or multi-media presentations, e.g., the parable of the loaves and fishes written in the shape of two very realistic fishes with the words forming the scales. There is a story of an ostrich with the text circling around until is runs into the bird's neck, and thus into the ground. Changing fonts seems to have been no trouble: Greek and Latin, French and German can appear on a single page, in large and small type, with bold face, italic, and illumination. Even the length of dashes can be altered especially to justify the lines. Mixing red and black inks on the same page did not even take a special press run. It was all part of the page if you desired it.The next technological advance of real note was Gutenberg's movable type which could be put into a press to allow many copies of a text to be produced with a total reduction in time. Gutenberg's first type was carved hardwood, which would deteriorate after a few books. So while the Bible he printed looked like the best handwriting of the day (German style handwriting, or “black letter” which the layman calls “Old English”). The advantage of Gutenberg's contribution was a lessening of costs of books, and the proliferation of important texts. The disadvantage was that the printing process was only an improvement in the printing of the text, or in specially carved wood block pictures. The paintings that could be done in many colours disappeared, because these paints did not work in a printing press which required ink. There were some attempts at hand-colouring pictures whose outlines had been done on the press, but these soon stopped. This phenomenon is symptomatic of technological advancement. Once we think that we have achieved a method that takes less time, we resent mixing the old technology with the new. I have seen it in the computer age quite strongly. Our printing process is now nearly universally offset printing rather than Gutenberg's letterpress. We can photographically reproduce any drawing, in colour or black and white. But the computerized publisher seems to resent mixing the computer-generated letters with hand drawings, because somehow, if you cannot do the whole business on computer, you are losing some of the advantage of the technological advance. The hell with the way it looks. The feeling among these people seems to be that it is better to have boxes made out of dashes and vertical bars than a hand drawing with some rounded corners to indicate a screen or keys on the keyboard. In just such a way the hand painting of printed black and white line drawing was left undone, and we lost colour in book illustration for about 500 years. Even when it was finally re-introduce
d, the pictures and text were never integrated as they has been in the manuscript days, and now pictures go in blocks, or pages especially set aside for doing screened black and white photos, line drawings, or full colour separations.It took about 300 hundred years for the quality of printing to come into its own, with the progressive simplification of type making it clear, clean, and legible, and sufficiently flexible to allow for the fish jokes of yesteryear. And at about the same time there was another technological change that put the quality of texts back into the stone age. That invention was the typewriter. There is no doubt that the typewriter was a wonderful tool, replacing wax tables, paper notes, and eventually, writings with small circulation (meaning the circulation of the number of carbons that could be forced into the typewriter). The quality of the result was abysmal compared to typeset texts, but the convenience of the transcription by skilled scribes, allowed people to overlook the poor quality. It was 50 to 75 years before typewriters became advanced enough to even-up the darkness of the letters, and eventually produce a sharp image. Another ten years passed before it became possible to get crisp letters with film ribbons, and then proportionally spaced type.There are a few other technological changes about this time which provided even worse images, such as court-stenographic machines and calculators, but we will skip rapidly over them once you have a picture of that style of output in your mind.Now we come to the advance that you have been waiting for: the computer revolution. The first computer printing is much older than the micro-computer, and was in all uppercase, with bad inking, and often type that varied in position in two directions: it was out of line both up and down, and side to side. But with the introduction of the microcomputer, and the recognition of word processing as a worthwhile activity for a computer, the quality of type has gone up. There are computerized typesetters, which create text about as nice as type done in the 18th century. It is not as nice as the monotype of the 20th century, but not as bad as the cheap type of the 19th century. There is typewriter quality output—known as “letter quality”—which looks like the best typewriters because it uses exactly the same mechanism. And at the lower end there are several varieties of type created with dots. The 300 dot per inch laser printer looks like Gutenberg operating on some old type, but because it is in a Times Roman face, it is considered much better than the unpretentious letter quality monospaced type. Then, horror of horrors, there is enlarged Mactype, which looks like it was cut out of felt with pinking shears in any cookie-cutter shape from black letter to Helvetica. It's so much worse than anything ever produced before in the history of publishing, that I cannot understand why anyone thinks that this was a technological advance. Then there is the dot-matrix printer, which is ineffable.I cannot give you the impression that I am a Luddite, because I have produced this paper using a word processing system on the ineffable printer. What I am trying to tell you is that there is still much work to be done to achieve the quality that computer publishing may achieve. But frankly this technological advance may be easy, but it's not good quality. Consider some of the achievements of the past, when people were trying to prove what they could do by hand, and work toward hose goals in beauty of design, legibility, and appropriateness. Do not let anyone tell you that civilization began in 1949 with the invention of the digital computer. Louis XIV was rather more civilized than you or I will ever be.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {130–133},
numpages = {4},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10586,
author = {Cleary, Judi},
title = {Merging Text and Graphics},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10586},
doi = {10.1145/10563.10586},
abstract = {The capability to merge graphics and text into a consolidated document can greatly enhance communication. Even simple graphics such as boxes and arrows can help organize ideas and make information easier to understand. It is still disturbing to see the number of manuals that describe computer graphics systems that do not include even one graphic image!In the phototypesetting world, the capability to merge graphics and text has been available for some time, but only recently have the components for less costly systems become available. This paper will discuss a segmented system in use in a scientific R&amp;D environment for including graphics into documents.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {134–138},
numpages = {5},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10587,
author = {Smith, Sandra B},
title = {Dynamic Screens and Static Paper},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10587},
doi = {10.1145/10563.10587},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {139–145},
numpages = {7},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10588,
author = {Segelken, Roger},
title = {A Case History of a Computer Media Event—Introducing a Supercomputer Center},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10588},
doi = {10.1145/10563.10588},
abstract = {I've been asked to tell you how Cornell attempted to explain to the rest of the world the establishment here of one of four national centers for advanced computing — the supercomputer facility formally known as the Center for Theory and Simulation in Science and Engineering and nicknamed the Theory Center. This is the center that was founded this spring with part of the 200 million dollars that the National Science Foundation is allocating in the federal government's supercomputer initiative. That's a polite way of saying, we want to “Pearl Harbor” the Japanese before they do it to us in yet another area of technology.In brief, the Cornell Theory Center will be receiving something in the neighborhood of 30 million dollars from the National Science Foundation and another 30 million from IBM, in equipment and support, over the next three years to build and operate a production supercomputer facility — a sort of jumbo jet of supercomputers — and to conduct research in experimental supercomputer configurations — a program that could be thought of as the X-15 of computing. The Theory Center is still seeking additional industrial support; another $100 million would be a nice round number. Even without the industrial support, this is the largest single research program at Cornell.I'd like to describe the preparation — the groundwork — that went into this public information effort, as much as two years in advance. We'll go into the gory detail of what went wrong in our announcement, and some of the things that went right, not so many thanks to us. We'll take a look at how the news media covered an event like this — in particular television news — and I'll tell you why the hardest part of our job, as public information practitioners, is still ahead us.Let's start with who am I and what am I doing at a conference of computer documentation people? In a way, we're in the same business. We're supposed to be explaining computers and computing to people. Your people — your public — can be assumed to be receptive to computing. Or at least they're using it. The general public includes lots of people like myself who are still on the fringes of the computer revolution. They've been involved in a few skirmishes, maybe not even wounded yet, but they're not sure whose side they're on. They know that “user friendly” isn't good enough. They're not ready to learn a new language to speak to a machine. “The damn things are in the U.S. of A. Let them learn to speak American”.Even the millions of folks who have bought personal computers share a healthy suspicion of computing. Big computers are the ones the IRS uses to lose your tax returns. Big computers are the ones in government weapons labs. Big computers still can't predict the weather. Big computers are the ones, when they make mistakes, you can't argue with.That has something to do with why the idea of super computing — large scale computing — is not so easy to sell. “Bigger is better” went out with tailfins on cars. Now, if you're going to be bigger, or bigger just to be faster, there had better be a good reason for it.We didn't realize all of this, however, when ken Wilson won the Nobel Prize for physics in 1982, and immediately began talking about building supercomputers. I guess we were just glad to hear a theoretical physicist talk about something besides “the deep and hitherto unperceived analogies between the phenomena revealed by phase transitions and certain aspects of elementary particle physics.”The public's first inkling that there would be something called a Theory Center began when Ken Wilson stood up at a press conference that October morning, about four hours after being notified he was a Nobel Prize winner, and said: “I'm working at the national policy level to get people to realize the importance of computers as they become very much more powerful than they are today.” He said that just one field alone—theoretical physics — needed computing support to the tune of $100 million a year. He said, “I hope the prestige of the prize will help me get people — not necessarily to give $100 million — but to look carefully at the problems I've been discussing and to see if we can't get them worked out.”And from that day on, Cornell began promoting Kenneth G. Wilson — and I'm not ashamed of that word, promotion — and capitalizing on his fame. After all, you're only the reigning Nobel Prize winner for 365 days, and then someone else's phone starts ringing off the hook.Now Wilson was already serving on government panels to advise on the future of large scale computing and he had been knocking on the doors of executive suites in big business and industry, trying to convince the movers and shakers that American industry needed supercomputers and that the computer industry wasn't going to make very many of them until there was a demonstrated market and the best showroom, if you will, for supercomputers would be the universities where potential customers could come and “kick the tires” of the latest models.Then suddenly, Nobel Prize winner Wilson was the most prominent member of those government panels. Receptionists would say, “Let me show you right in, Dr. Wilson.” We interviewed Wilson for a Cornell publication a month later and he said, “There is nothing that comes close to providing the kind of forum that the Nobel Prize provides. With the kinds of problems I'm dealing with, with the kinds of barriers I face, anything short of the Nobel Prize doesn't mean very much.”We took the text of that interview — which talked about new uses for computer modeling and simulation and some schemes for parallel processing — and we sent it to about a dozen key business writers and science writers and editors around the country with a note saying, “Keep your eye on this guy. He knows more about supercomputing than anyone else in the country.”Now we didn't know whether that was true at the time. We just sort of became convinced of it.In the meantime, Ken Wilson was stepping up his activities in behalf of supercomputers. He was visiting more industries and getting more involved in advising government policy. When a report or a recommendation came out, if his name wasn't on it, people would ask his opinion. When the Japanese moved a little closer to making some big advance, people would ask Ken Wilson what he thought the U.S. should do. And once you get cited in The New York Times as a “leading expert” then you are one, and everyone else wants to know what you think. He was invited to write lots of articles and give talks on supercomputers and “the Japanese challenge.” He became “Mr. Supercomputer.”We don't claim all the credit for his fame. A lot, maybe most, of the effort was on Wilson's part. We just did everything we could to keep him in the public eye. When he and IBM and some other industries and the National Bureau of Standards sponsored a conference on large scale computing in Washington, we promoted it, even though it had next to nothing to do with Cornell. He became one of about a dozen almost-celebrity professors at Cornell. The only person more quoted, day in and out, was Carl Sagan.After Wilson had convinced the Washington establishment and the people holding the purse strings to spend some big money on scientific supercomputing, he had to step back from the role of neutral adviser and apply for some of the money himself. And somewhere along the line, the Theory Center became the Theory and Simulation Center, and it was to be for engineering and not just science. Must be someone figured out that there's a reason why the Fortune 500 doesn't include companies called International Business Theories or General Theoretical Motors. The co-investigators in the proposal to the National Science Foundation were Wilson; Dr. Kenneth King, who is also a physicist by background and is the computer czar for Cornell; and Ravi Sudan, also a physicist and an engineer who runs a lab for plasma fusion studies.During the time the proposal to the federal government was being reviewed — for months — we couldn't say much about the Theory Center. It's considered bad form to discuss something you're certain you will get. And if you don't get it, you look really silly.So instead, we concentrated on one little phase of the Theory Center, one that was already going on. This was the so-called GIBBS Project, an attempt by Wilson and some of the computer scientists here to create an entirely new scientific programming language to replace FORTRAN. We asked the public relations firm that represents the College of Engineering, of which computer science is a part, to push the GIBBS Project and they tried. It got some attention in the trade press and in places Like Science magazine, not too bad for something that wasn't hatched yet. The Theory Center, itself, wasn't real for a long time either. The Cornell faculty had given its consent and so had the University Board of Trustees. But Cornell's President, Frank Rhodes, wouldn't allow it to be established until Wilson could show some evidence of funding. They had an office with a name on the door and some furniture and a couple of people, but they didn't exist as far as Cornell University was concerned. We took to calling them the Theoretical Theory Center.We also started planning how we would announce the center when it was funded, which everyone said it would be, except that was a secret. We started preparing with the National Science Foundation's public information people to make an announcement. They told us they were afraid of a leak, ahead of the official announcement, and it could come from Congress. We thought they meant congressmen from California or Illinois or someplace. Surely, no elected official from New York would engage in something as sleazy as pork barreling, then spill the beans. Remember, I told you something would go wrong…In our brainstorming sessions, Ken Wilson made a demand that caused some snickers and mumblings of “Boy, is he naive.” He wanted to create the impression in the public mind that all of upstate New York was the next up-and-coming high technology region in the country. That all the isolated high tech areas like Rochester and Schenectady and Syracuse could be working together, rather than in competition, and that they could be linked electronically, by the Digital Thruway. The next Silicon Valley! And he wanted that impression created and established before the Theory Center was established, so that it would seem to be another piece fitting into the high tech picture. So we tossed around some names. Everything new has to have a catchy name. If New York City was the Big Apple, upstate could be the Silicon Apple. Then someone pointed out that gallium arsenide was the next hot semiconductor material and maybe we should be the Gallium Arsenide Apple. But that sounded too much like something the wicked witch would give Snow White. We talked about how the state could become involved. We tried to point out that impressions of prosperity and high tech environments aren't created overnight. Nobody knew they loved New York until millions of dollars worth of jingles and bumper stickers and billboards told them so.I guess we sensed a few inferiority complexes showing in these men who were about to pull off an astounding achievement — to persuade the federal government and the biggest computer company in the world to risk tens of millions of dollars. I remember Ken King telling of a telephone call he had just received from an acquaintance at another university who said, “Congratulations, but you got the booby prize.” He meant that Cornell — although it hadn't been announced yet — would be the fourth last-minute center funded by the government, and that we had to team up with a company that didn't even make supercomputers to do it.We tried to point out that Cornell didn't need to apologize for being the odd man in or out or wherever, because we had the element of surprise on our side. Everyone would want to know why the government was designating a private university in the middle of nowhere as a national center. We said that a couple of times, then shut up, We thought we still had three months to prepare for the announcement.At one point, some thought was given to hiring the same public relations firm that represents the manufacturer of the array processors the Theory Center uses, Floating Point Systems, to represent Cornell as well. They talked a Lot about “building understanding” which is something that p.r. people are big on. “Building understanding” is p.r. shorthand for building understanding of my point of view and convincing you of it. The firm wanted $40,000 to make the announcement of the Theory Center, and that was just to the trade press. It occurred to us that for $40,000 we could parachute Ken Wilson to the roof of every one of the top 100 newspapers in the country to personally hand a news release to the editor. We told them we'd think about it. We thought we still had two months to prepare for an announcement, sometime in the middle of April.In the meantime, we began preparing background information on the supercomputer center. We did a story saying that supercomputing will benefit American business, that “the advanced power of supercomputing and the research discoveries it makes possible promise to improve the entire corporate production cycle, from conception of a product through manufacturing to distribution.”We did a story saying that the marriage of supercomputing and three-dimensional, real-time computer graphics would be the greatest advance in communication since cavemen started painting on walls.We prepared a background piece saying that “Cornell University is a promising Location for a national, advanced scientific computing center because of its experience in operating highly successful interdisciplinary centers for the benefit of the scientific research community.” And we took the opportunity to brag about the Cornell Manufacturing Engineering and Productivity Program (COMEPP) and the Cornell High Energy Synchrotron Source (CHESS) and the Materials Science Center and the National Research and Resource Facility for Submicron Structures (which spells NRRFSS) and the Cornell Biotechnology Institute and the Semiconductor Research Corporation Center of Excellence in Microscience and Technology (which doesn't spell anything).We did another story saying the research uses of the supercomputer will range from “the study of galaxies to subatomic particles, from the motion of drifting continents to the movement of toxic wastes.”We wrote a general news release on the announcement, Leaving blanks for the amount of money and the number of years and the actual date of the announcement. We thought we still had a month to get ready.We solicited statement of congratulations from New York Governor Mario Cuomo and from the congressional delegation from this part of the state and from IBM vice president Jack Keuhler.When we were writing our news releases, by the way, we had to be careful not to mention IBM in the same breath — or even the same paragraph — with the word supercomputers. That directive came down from on high at IBM. IBM was not in the supercomputer business. Never had been, never will be. We couldn't even say the 3084QX would be a building block of a supercomputer. IBM was just giving us $30 million because they like us.We prepared biographies of all key personnel involved — all the way from Cornell President Frink Rhodes, who doesn't know anything about computers but who is able, with a little prompting, to speak eloquently on any issue and thank people for giving us money — down through all the vice presidents of: the university and provosts to the principal investigators in the Theory Center grant to the people who will really run the facility.Then we sat back and waited. Until February 20th, a Wednesday, when the NSF told us the announcement would be made the following Monday morning, in Washington. That didn't bother us. We were ready. We decided to schedule not one, not two, but three simultaneous press conferences. We would send Ken Wilson to the NSF press conference in Washington, along with a couple people from our office to straighten his tie and tuck in his shirt tails. We sent President Rhodes and Vice Provost King and Professor Ravi Sudan to New York for a press conference at Cornell Medical College. And we kept one Cornell vice president and one provost and the executive director of Theory Center, Bill Schrader, and the head of Theorynet, Alison Brown, for a simultaneous press conference in Ithaca.When we announce a press conference, we are very cagey. We try not to give away very much of the story — just enough to entice people to turn out. There's a reason for this. If you give away the story and if it's worth anything, the news people — being in a very competitive business — will try to run with it, and spoil your announcement. It happens every time. So we said something Like: Cornell University, the National Science Foundation and a major manufacturer of computing equipment will announce the start of a $60 million cooperative research venture at 10:30 a.m. Monday, February 25, at the following locations: …. Then we swore everybody to secrecy, everybody who might get calls Late at night or even be likely to talk in their sleep. There was one exception to that: We lined up an interview with Ken Wilson and The New York Times for Monday morning, just preceding the scheduled announcement. Then we sat back and waited.Imagine our surprise, on Saturday morning, February 23, to start receiving calls from news media all over New York State: “Could someone comment on the D'Amato announcement?” The D'Amato announcement? They read from a press release: “Senator Alfonse M. D'Amato (R, NY) is pleased to announce that Cornell University will receive at least $30 million and possibly up to $60 million from the National Science Foundation and up to $35 million from International Business Machines Corp. to do fundamental research from America's next generation of supercomputers.” Senator D'Amato said the grants would make Cornell one of the leading institutions in the country.That surprised us a little. We thought we were already a Leading institution. We are among the top four or five or six research universities in the country, and our academic reputation isn't too shabby, either.It was obvious what Sen. D'Amato was up to. He serves on one or two committees that some time in the past had reviewed the NSF proposals, probably voted for an appropriation when it seemed that some of the money might go to his home state, and placed a note in his future file: Break this on a slow news day and take credit for it.We tried to persuade D'Amato and his staff to back off, to correct his misinformation, to join us in a joint announcement on Monday and to shut up in the meantime. No go. He said the NSF had given him the green light to make the announcement. The NSF was furious. They refused to confirm that Cornell would even get a nickel, they started an investigation to determine how the leak occurred, and they blamed Cornell for prompting Sen. D'Amato to jump the gun and spoil the announcement for the other three centers around the country. The congressmen in the House of Representatives who really had gone to bat for Cornell on this one weren't too happy either. But they knew that D'Amato had a reputation for this sort of thing.Of course the news was in the Sunday papers all around New York the next day. That kind of money, even if the figures are incorrect, gets people's attention. The stories said that Cornell officials refused to comment, except to say that Sen. D'Amato was mistaken, and that an announcement would be made on Monday. It looked like we had something to hide. Or at least that we were caught off guard.Which we weren't. We had even given Ken Wilson media training. We had put him in front of our own TV cameras. We had dry runs of press conferences. We asked him the toughest, the stupidest, the most repetitive questions we could think of. We tried to teach him to look at the camera, not to fidget or play with parts of his clothing, not to look to the heavens or into his pocket for answers. In short, to give the impression that every question he gets is the most perceptive and original he has ever heard, deserving of a sensitive, profound answer that he just thought of. We weren't trying to make a Carl Sagan of him, just to help him come across as the intelligent person that he really is.Then we packed everybody off to their respective press conferences. All in all, they went pretty well, considering that practically nobody came to the New York City press conference and that the University of Illinois beat the pants off us in the Washington conference. The problem at the NSF press conference wag Larry Smarr, the Illinois astrophysicist, who was savvy enough to jump up and answer questions that weren't directed to anybody in particular. (We made a note to train Wilson to do that the next time.) Larry Smarr told a little story that any of the supercomputer people could have told but he thought of it first, and it's been quoted everywhere since then. He said that supercomputers are so scarce in this country that in order to do his research he had to go to Germany to find time on a machine — a machine that was made in this country. No big deal, but it was the kind of anecdote that writers love and lots of them used it in their stories.We knew our homework was paying off, however, when we saw that Ken Wilson was quoted on the front page of The New York Times, even before they mentioned the other places that have supercomputer centers. Larry Smarr had a snappier quote, but it was in the second section of the paper, and besides, IBM was quoted as saying that Cornell's approach to building supercomputers was the only one IBM would consider exploring, not that IBM was particularly interested in making supercomputers, of course.You have before you copies of some of the newspaper and magazine clips that appeared over the next few days. There have been lots more since. I'd like to play for you some tapes from local television stations that covered the Ithaca press conference. They go from bad to worse. We in the public information business collect these things to try to figure out how the information that we thought was presented so clearly gets so screwed up on the six o'clock news. Later I'll play a segment from a public affairs show on a Local PBS station, and then a piece that one of the national networks did — a really first rate job. Those of you who live in major media centers will be educated on what small town television is like. Those of you from small towns can sympathize.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {146–160},
numpages = {15},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

@inproceedings{10.1145/10563.10589,
author = {Andr\'{e}sen, Karen E},
title = {From Pencils and Paste-Ups to VDTs and the Integrated Page: Some Thoughts on the State-of-the-Art},
year = {1986},
isbn = {0897911865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/10563.10589},
doi = {10.1145/10563.10589},
abstract = {A few spring seasons ago one of my professors in graduate school was deep in a depression. In response to a departmental decision to install word processors, his desire to put pencil to paper had died and his passion for instilling a healthy respect for the English language in his students had waned. The following fall, however, he was a changed man - the depression had vanished. When I asked what had brought about this change, he said he had spent the summer in the “eighteenth century,” alias the museum school at Cooperstown. There, he had spent long hours drafting with a quill pen, setting type blocks by hand, and cranking sheets of rag paper through a press - producing documents from start to finish by his own hand. He had recaptured his vitality and his perspective by returning to the basics, and now, he thought, he was ready to take on the spectre of the computer.Many professionals in our field are experiencing the same desperation that my professor faced. However, they do not have the option of returning to a school for antiquities — they are being pushed into the future. Today, writers are being told to put aside their pencils, and to work online. Writers are complaining that the new documentation processes are removing them too far from their words, and that the process is contributing to the defamation of the natural beauty of our language. They fear that the art of technical writing and the craft of producing a document are being undermined by the new technology. They wonder how to maintain the integrity of the language when the words of that language can't be touched anymore - when they are composed not by human hand but by a sequence of dots in a matrix, and when they are not laid to boards with wax but synthesized by a computer that integrates text and graphics and appear on a clean, odorless printout. They are both intimidated by the new systems and feel resentful of the machine that separates them from their words.This separation conflict can be addressed on three levels. First, evaluating the advantages and disadvantages of computer-aided publishing can help ease the crisis of craft, that is, the idea that the documentation process is out of human control. Second, recognizing the advantages of the new medium - in our case, computer code - can help ease the crisis of change, that is, the perception that the new technology is overwhelming, and that resistance is the best line of defense against it. Third, accepting the idea that online development is not merely word processing, and realizing that your strength, skill, and art cannot be compromised by a keyboard can solve the crisis of art - that is, the idea that computer technology can deface the beauty of your words.To address the first concern - that of a revolutionary approach to production, let us examine a case in point. At Bell Communications Research, Inc. (Bellcore), a plan for electronic publishing has just been instituted. Within two years, we expect to write online, create graphics online, compose pages that integrate those words and graphics online, impose our publications' standards on those pages online, and publish documents composed of those pages online. Instead of sending mechanicals to a human printer, we will send computer code to a laser printer. In short, our publication portfolios will be in binary instead of binders.There are few technological obstacles to overcome to implement this system. One great advantage is that our form of computer-aided publishing (CAP) frees us from the traditional serial method of production - writing, editing, illustration, typeset, paste-up, and printing. That process is lengthy and labor-intensive, and each step is dependent on the one that preceded it. If, for instance, the decision is made after paste-up to drop an illustration, the document must be reformatted from the point of the illustration. The writer must return to the text to make sure that references and the folio list are changed. The proofreader must ensure that the writer has caught all the changes. The compositor must reformat the document. And, if the change occurred after the blueline was produced, the negatives must be re-shot. Each person's efforts are cumulative and pyramidal. If one block is imperfect, the end product may be structurally deficient.This takes not only time but money. Given an average cost per page of 20 to 50 dollars a page for new mechanicals, this can easily add hundreds of dollars to the cost of composition. Although CAP has its own costs, they are considerably less than the price of manual composition. For example, a 20-page document with four schematics costs approximately 450 dollars on our CAP system. This cost includes text entry, creation of the graphics, proofreading, formatting, composing, and typesetting. The “boards” (repro copy) are ready in less than three days. And, cost savings continue through the “printing” process. If the document is electronically transmitted, a negligible cost is incurred for XOPR, or whatever means of data transfer is used. Finally, whatever costs are normally associated with the laser printer are levied. If the document is printed at a print shop, the costs are about two-thirds of normal because negatives are not needed. Negatives are not necessary because the copy is not mounted on boards, and boards are not necessary because all paste-up is done electronically. CAP merges text and graphics. In either case, substantial savings can be realized. Given the cumulative effect these sorts of changes have on document price, CAP is particularly appealing to those who have to pay for the material - in our case, the regional telephone operating companies.The companies - our owners - also want to be in control of the number of documents they receive. This is an added feature of our CAP process. With “on-demand” printing, once a document is ready for “distribution” to the field, the code is transmitted to VAX systems in 14 major metropolitan areas. Each VAX system is connected to a Xerox 9700 printer. With computer code and their own laser printers (which can be used for purposes other than printing the documents) they do not have to pay for print overruns. They know exactly how many copies they need and only that number is printed. Consequently, they have more control over the cost of “printing” and distribution. They simply do it themselves. What could be better?From the writer's perspective, some things could be better. And these concerns revolve around the notion of a crisis of change. Attuning oneself to the radical changes embedded in this process takes more than a little doing. Although it is not a difficult task to type on a computer keyboard (instead of a typewriter), or learn which buttons to press to format a document (instead of dictating specs to a compositor), it is stressful not to have the option to return to paper and pencil. It is challenging to face the consequence of technology: you must address the future to address your clients. And, although this message may seem far-fetched for the publication industry, it is not. The time has come when industry wants its money spent wisely and efficiently. Computer-aided publication must follow on the heels of computer-aided design and computer-aided engineering.The benefits of accepting and employing technological change have historical precedent. Think back to the middle of the fifteenth century. Johann Gutenberg's invention of printing with movable type was more a modification of existing technology than a totally new concept, but it had enormous impact. Writers and philosophers who took advantage of the ability to disseminate their ideas through print instead of manuscript were at a great advantage in terms of how great an audience could be addressed. In fact, the humanist movement might have been short-lived had it not been for the relatively widespread availability of classical writings. We might never have had the joy of Erasmus' In Praise of Folly or Sir Thomas More's Utopia had it not been for their realization that circulation of exact copies of their writings led to not only increased discourse but new attitudes. They contributed not only to the world of ideas, but the idea that the world could get information in a more timely, accurate, and consistent form.Advancing in time, the same statement can be linked to those who fought the ideological battles that laid the way for the American Revolution. Through their rhetoric they inflamed more hearts and brought them to the edge of revolution than was justified by the turn of events. The printing and pamphleteering business boomed. The “audience” responded. Suffice to say, they used the latest in technology to address critical issues. By controlling the presses, they had a clear advantage. The rest, as they say, is history.These historical examples should not be viewed in isolation, for today great opportunities exist to use the tools and technology to expand our audience base and to reach out to those that standard print technology might not affect. The possibilities of sending text over television (broadband), over FM radio (narrowband), through microwave frequencies, or through satellites or over cable television are endless. The dream of videotext is now a reality. One- and two-way transmissions can be sent over the telephone or through a cable. Technical capabilities are no longer a hindrance. Rather, they should be seen as a boon.Computer-aided publishing can also be a boon to the writer as concerns the third crisis I want to address - that of the artistry of writing. As technology has advanced, and as writers must deal with new vocabularies and constructions, many feel the result is a decline in the style and presentation of writing. One of my colleagues described it as “our loss of autonomy in a world of presentation - with no latitude for creativity and individuality,” and wondered how “yesterday's writer is accepted into today's non-verbal, passive, and highly graphic world of communications.” These are legitimate concerns. Good examples of bad English abound in all the communication media today.Using CAP to its fullest advantage is one way to combat the deterioration of the language. For example, the speed and ease of editing allows for more editing iterations. And although a document will not necessarily improve during each editing cycle, very often the quality does improve the more the document is worked on. And, quality is the heart of this concern. Computers can do nothing to intuitively aid or deter the writer in his or her work - they are merely tools, but tools with many attachments that can ease the process. Those basic attachments, such as programs that check spelling, that count how many times and in which contexts words have been used, that identify certain types of phrase structures, and that format documents automatically are aids that should not be neglected. Additionally, the ease with which drafts can be circulated, commented on, and returned to a writer with comments contribute to better writing. From my terminal, for example, I can transmit drafts to experts on telecommunications in Washington, San Francisco, St. Louis, and Seattle within minutes, and receive their comments within hours.Furthermore, CAP permits the use of more exotic technology. The links to graphics software, which afford the writer the luxury of a choice of formats and presentation, allow for experimentation in how to effectively present illustrative material. Along those same lines, the writer also can dally with fonts and type size (although this is sometimes limited by the constraints of the printer that will ultimately produce the physical document). In sum, electronic peripherals are a means through which the writer can attack the problem of producing good writing in a world in which bad writing abounds. They offer the opportunity to the writer to fine-tune a document in relatively short period of time (with the aid of software “helpers” if desired), and to support that writing through interactive design and execution of graphics and format.These three perspectives on CAP - how to deal with the concerns of how the process has changed our craft, how to use it most effectively, and how we may maintain our standards in light of the revolutionary change - are central to our work. We cannot dismiss them. Whereas computer-aided publishing is an awesome concept, and one that threatens certain types of crafts, the advantages in speed (reduction of composition time results in increase in creativity time), control (over all phases of the production as well as the quality of the writing), and audience (the opportunity to address a greater audience both more directly and through a medium that is more the norm than the exception) are overwhelming. CAP should not be viewed as an obstacle, but as a gateway that, once opened, reveals a wealth of opportunity. I believe that even my professor in graduate school, light-hearted as he was over preparing a document by his own hand, would agree with me. When I last saw him a few months ago, I had to wrench him away from his current passion - composing a book on eighteenth century printing. His medium? His new loves in life - a personal computer, word processing software, and a printer.},
booktitle = {Proceedings of the 4th Annual International Conference on Systems Documentation},
pages = {161–164},
numpages = {4},
location = {Ithaca, New York, USA},
series = {SIGDOC '85}
}

