@inproceedings{10.1145/3405962.3405989,
author = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis},
title = {Scalable Machine Learning on High-Dimensional Vectors: From Data Series to Deep Network Embeddings},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405989},
doi = {10.1145/3405962.3405989},
abstract = {There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to analyze very large collections of static and streaming sequences (a.k.a. data series), predominantly in real-time. Examples of such applications come from Internet of Things installations, neuroscience, astrophysics, and a multitude of other scientific and application domains that need to apply machine learning techniques for knowledge extraction. It is not unusual for these applications, for which similarity search is a core operation, to involve numbers of data series in the order of hundreds of millions to billions, which are seldom analyzed in their full detail due to their sheer size. Such application requirements have driven the development of novel similarity search methods that can facilitate scalable analytics in this context. At the same time, a host of other methods have been developed for similarity search of high-dimensional vectors in general. All these methods are now becoming increasingly important, because of the growing popularity and size of sequence collections, as well as the growing use of high-dimensional vector representations of a large variety of objects (such as text, multimedia, images, audio and video recordings, graphs, database tables, and others) thanks to deep network embeddings. In this work, we review recent efforts in designing techniques for indexing and analyzing massive collections of data series, and argue that they are the methods of choice even for general high-dimensional vectors. Finally, we discuss the challenges and open research problems in this area.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {1–6},
numpages = {6},
keywords = {similarity search, time series, high-dimensional vectors, machine learning, deep learning, data series, embeddings},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405977,
author = {G\'{o}mez-Suta, Manuela and Echeverry-Correa, Juli\'{a}n D. and Soto-Mej\'{\i}a, Jos\'{e} A.},
title = {Semi-Automatic Extraction and Validation of Concepts in Ontology Learning from Texts in Spanish},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405977},
doi = {10.1145/3405962.3405977},
abstract = {The construction of ontologies from texts in Spanish is a challenge since this language lacks conceptual databases to validate abstract ontology structures as concepts and relations between them. The preceding generates the necessity of using manual evaluation by human experts; carrying high expenses that limit the calibration of algorithm parameters and large-scale evaluations. This document presents a proposal to evaluate abstract ontology structures through the task of semantic clustering of documents, without the expensive necessity of using manual evaluation or conceptual databases. The proposal is not only affordable but also applicable to model data and domains that lack structured knowledge resources. The experiments lead to the extraction and validation of the ontology structures from texts in Spanish regarding the domain of the Colombian armed conflict.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {7–16},
numpages = {10},
keywords = {Spanish, concepts, Ontology learning, evaluation},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405973,
author = {El Achkar, Charbel and At\'{e}chian, Talar},
title = {Supporting Music Pattern Retrieval and Analysis: An Ontology-Based Approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405973},
doi = {10.1145/3405962.3405973},
abstract = {Analyzing music notations is found useful for musicology purposes. This can be applied by retrieving semantic information from digitally annotated music scores. In this paper, we propose an ontology that structures the knowledge extraction process of a music pattern analysis algorithm. In addition to mandatory elements that describe music scores, the proposed ontology relies on contextual elements and attributes for pattern analysis. The ontology then supports the semantic information retrieval and analysis processes of music score contents. We illustrate the whole mechanism by explaining the workflow of the ontology integrated inside a music encoding platform for eastern music.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {17–20},
numpages = {4},
keywords = {Ontology, MEI, pattern analysis},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405983,
author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
title = {The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405983},
doi = {10.1145/3405962.3405983},
abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {21–32},
numpages = {12},
keywords = {queries, aviation, services, datasets, ontology},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405984,
author = {Kirsh, Ilan and Joy, Mike},
title = {Splitting the Web Analytics Atom: From Page Metrics and KPIs to Sub-Page Metrics and KPIs},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405984},
doi = {10.1145/3405962.3405984},
abstract = {Web analytics Key Performance Indicators (KPIs) are important metrics used to evaluate websites and web pages against objectives. The power of KPIs is in their simplicity. Every web page can be assessed by numeric KPI values, which can be easily calculated, compared, and tracked over time. KPIs highlight the strengths and weaknesses of individual web pages and significantly help in maintaining, improving, and optimizing websites. Current web analytics metrics and KPIs, in academic studies as well as in commercial tools, relate to entire websites and web pages. This paper advocates extending KPIs use to sub-page elements, such as paragraphs, as an effective way to refine knowledge and leverage web analytics capabilities. We discuss the potential and challenges of sub-page web analytics and define a framework for calculating sub-page metrics from accumulated in-page user activity data, such as mouse and keyboard events. Then we propose potential KPIs that may be effective in highlighting the strengths and weaknesses of individual page parts, such as paragraphs. We use web usage data from a sample website to demonstrate these ideas. This study is the first step towards sub-page web analytics metrics and KPIs. Further work is required in order to gain more knowledge about potential KPIs that are introduced in this work, as well as to explore new methods, metrics, and KPIs.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {33–43},
numpages = {11},
keywords = {Educational Technology, Javascript, AJAX, Web Content, Analytics Tools, Web Pages, Web Users, Web Usage Mining, Web Activity, Web Analytics, Online Learning, Paragraphs, Browsers, Websites, Data Mining},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405998,
author = {Dimitriadis, Ilias and Poiitis, Marinos and Faloutsos, Christos and Vakali, Athena},
title = {TRIAGE: Temporal Twitter Attribute Graph Patterns},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405998},
doi = {10.1145/3405962.3405998},
abstract = {Given a node-attributed network of Twitter users, can we capture their posting behavior over time and identify patterns that could probably describe, model or predict their activity? Based on the assumption that the posts of these users are topic-specific, can we identify temporal connectivity patterns that emerge from the use of specific attributes? More challengingly, are there any particular attribute usage patterns which indicate an inherent anomaly either for users or attributes? Our study attempts to provide solid answers to all the above questions, extending previous work on other social networks and attribute types. We propose TRIAGE, a pipeline of methods which: (a) identify temporal behavioral patterns in individual attribute distributions, (b) model the temporal evolution of attribute induced graphs and (c) detect irregular attributes and users based on the patterns identified earlier; More specifically, we model the attribute distributions using the log-Odds ratio, we provide explanations with respect to the attribute induced subgraph patterns and we observe the structural differences of attribute induced subgraphs based on these patterns. Experimental results show that: most of the individual attribute distributions remain stable over time following mostly power laws norm; the temporal evolution of attribute induced graphs obey certain laws and deviations are outliers; finally, we discover that we can indeed identify the structure of each subgraph, based on the emerging patterns. Real dataset experiments on 50K Twitter users activities and attributes has successfully proven that TRIAGE has effectively identified Twitter user and attribute behavioral patterns and can identify irregular activities for users and anomalous graph structures for attribute induced subgraphs.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {44–53},
numpages = {10},
keywords = {Twitter, Anomaly Detection, Graph mining, Network Modelling, Social networks},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405988,
author = {Weichselbraun, Albert and H\"{o}rler, Sandro and Hauser, Christian and Havelka, Anina},
title = {Classifying News Media Coverage for Corruption Risks Management with Deep Learning and Web Intelligence},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405988},
doi = {10.1145/3405962.3405988},
abstract = {A substantial number of international corporations have been affected by corruption. The research presented in this paper introduces the Integrity Risks Monitor, an analytics dashboard that applies Web Intelligence and Deep Learning to english and german-speaking documents for the task of (i) tracking and visualizing past corruption management gaps and their respective impacts, (ii) understanding present and past integrity issues, (iii) supporting companies in analyzing news media for identifying and mitigating integrity risks.Afterwards, we discuss the design, implementation, training and evaluation of classification components capable of identifying English documents covering the integrity topic of corruption. Domain experts created a gold standard dataset compiled from Anglo-American media coverage on corruption cases that has been used for training and evaluating the classifier. The experiments performed to evaluate the classifiers draw upon popular algorithms used for text classification such as Na\"{\i}ve Bayes, Support Vector Machines (SVM) and Deep Learning architectures (LSTM, BiLSTM, CNN) that draw upon different word embeddings and document representations. They also demonstrate that although classical machine learning approaches such as Na\"{\i}ve Bayes struggle with the diversity of the media coverage on corruption, state-of-the art Deep Learning models perform sufficiently well in the project's context.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {54–62},
numpages = {9},
keywords = {Web Intelligence, Deep Neural Networks, Text Analytics, Word Embeddings, Text Classification, Corruption Risk Management},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405979,
author = {Kumara, Indika and Vasileiou, Zoe and Meditskos, Georgios and Tamburri, Damian A. and Van Den Heuvel, Willem-Jan and Karakostas, Anastasios and Vrochidis, Stefanos and Kompatsiaris, Ioannis},
title = {Towards Semantic Detection of Smells in Cloud Infrastructure Code},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405979},
doi = {10.1145/3405962.3405979},
abstract = {Automated deployment and management of Cloud applications relies on descriptions of their deployment topologies, often referred to as Infrastructure Code. As the complexity of applications and their deployment models increases, developers inadvertently introduce software smells to such code specifications, for instance, violations of good coding practices, modular structure, and more. This paper presents a knowledge-driven approach enabling developers to identify the aforementioned smells in deployment descriptions. We detect smells with SPARQL-based rules over pattern-based OWL 2 knowledge graphs capturing deployment models. We show the feasibility of our approach with a prototype and three case studies.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {63–67},
numpages = {5},
keywords = {OWL 2, Infrastructure Code Smells, Deployment, Defects, Infrastructure Code, TOSCA, Cloud Computing},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405986,
author = {Knani, Samia and Ayadi, Nadia Yacoubi},
title = {A Holistic Approach for Semantic Interpretation of Relational Web Tables},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405986},
doi = {10.1145/3405962.3405986},
abstract = {The Web contains vast amounts of semi-structured data in the form of HTML tables found on Web pages which may serve for various applications. One prominent application, which is often referred to Semantic Table Interpretation, is to exploit the semantics of a widely recognized knowledge bases (KB) by matching tabular data, including column headers and cell contents, to semantically rich descriptions of classes, entities and properties in Web KBs. In this paper, we focus on relational tables which are valuable sources of facts about real-world entities (persons, locations, organizations, etc.) and we propose a robust and efficient approach for bridging the gap between millions of Web tables and large-scale Knowledge graphs such as DBpedia. Our approach is holistic and fully unsupervised for semantic interpretation of Web tables based on the DBpedia Knowledge graph. Our approach covers three phases that heavily rely on word and entity pre-trained embeddings to uncover semantics of Web tables. Our experimental evaluation is conducted using the T2D gold standard corpus. Our results are very promising compared to several existing approaches of annotation in web tables.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {68–77},
numpages = {10},
keywords = {Semantic embeddings, Semantic Table Interpretation, Entity Linking},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405966,
author = {Fu, Bo and Steichen, Ben and McBride, Alexandra},
title = {Tumbling to Succeed: A Predictive Analysis of User Success in Interactive Ontology Visualization},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405966},
doi = {10.1145/3405962.3405966},
abstract = {Ontology visualization is an important component in the support of human-ontology interaction, as it amplifies cognition and offloads cognitive efforts to the human perceptual system. While a significant amount of research efforts has focused on designing and developing various visual layouts and improve performance of large-scale visualizations, the differences in user preferences and cognitive abilities have been largely overlooked. This provides an opportunity to investigate ways to potentially provide more personalized visual support in human-ontology interaction. To this end, this paper demonstrates successful predictions on an individual user's likelihood to succeed in a given task, based on this person's gaze data collected during interaction. Specifically, we show several statistically significant predictions against a baseline classifier when inferring users' success before a given task is actually completed. Moreover, we present results showing that accurate predictions of user success can be achieved early on during user interaction, such as after just a few minutes in some cases. These findings suggest there are ample opportunities throughout various stages of human-ontology interaction where the underlying visual system may adapt in real time to the user's visual needs to provide the most appropriate visualization with the overall goal of possibly increasing user success in a given task.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {78–87},
numpages = {10},
keywords = {Applied Machine Learning, Adaptive Ontology Visualization, Tumbling Window, Eye Tracking, Predictive User Analysis},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405970,
author = {Yamamoto, Yukio and Ishikawa, Hiroshi},
title = {Data Management in Japanese Planetary Explorations for Big Data Era},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405970},
doi = {10.1145/3405962.3405970},
abstract = {The data obtained by planetary explorations has various aspects such as decision making during an ongoing mission, anomaly detection for spacecraft safety, data archives for scientific analysis, and attractive snapshots for outreach. Each aspect requires each data formats and processing techniques. In this paper, we discuss changes in the environment surrounding planetary explorations and the handling of big data on computers. As a result, for the long-term preservation of scientific data, there must be standards and a community to endorse the standards. After standards, each community prepares the analysis tools. Furthermore, scientists need to make efforts not only in standardization but also in ensuring the quality of science. For highly informative data in recent years, the processing of data archives requires information science experts. Also, data providers or distributors should define data policies to clarify data usages to users. Finally, scientific analysis of cloud-based architecture due to big data and computer resources.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {88–90},
numpages = {3},
keywords = {Planetary Data System, planetary exploration, SPICE},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405993,
author = {Takahashi, Munenori and Endo, Masaki and Ohno, Shigeyoshi and Hirota, Masaharu and Ishikawa, Hiroshi},
title = {Automatic Detection Method of Tourist Spots Using SNS},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405993},
doi = {10.1145/3405962.3405993},
abstract = {Tourism information collection using the web has become popular in recent years. Moreover, tourists are increasingly using the web to obtain tourist information. Particularly because of the spread of social network services (SNSs), various tourism information is available. Various studies are being conducted using Twitter, which is one of SNS. A low-cost moving average method using geotagged tweets posted location information has been proposed to estimate the best time (peak period) for phenological observation. Geotagged tweets are also useful for estimating and acquiring local tourist information in real time, as a social sensor, because the information can reflect real-world situations. We have been working on, we are pursuing an estimation of the best time to view cherry blossoms. Our earlier studies have improved methods of estimating cherry blossom viewing times. The research so far can estimate the spot that the user knows. However, we cannot estimate the cherry blossoms that the users do not know. Therefore, a user requires a system that is independent of the amount of knowledge. It is possible to provide useful information to all users. We propose a prototype system that estimates the best time without prior knowledge of tourist destinations. In the early stages, the purpose is to use tweets to find spots already featured in magazines and the web. As described herein, we detected spots automatically using a geotagged tweet by visualization with a heat map and setting conditions. The proposed method achieved it in about 80%.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {91–96},
numpages = {6},
keywords = {Mining, Spot detection, Sightseeing, SNS},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405991,
author = {Lin, Jhih-Yu and Wen, Shu-Mei and Hirota, Masaharu and Araki, Tetsuya and Ishikawa, Hiroshi},
title = {A Method for Ranking Tourist Attractions Based on Geo-Tagged Photographs and Image Quality Assessment},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405991},
doi = {10.1145/3405962.3405991},
abstract = {Recently, tourism has become a development emphasis for many countries because international tourism can bring huge revenues; it can also positively affect increased long-run economic growth. However, in this era of complex information, it is hard to get integrated tourist information on the Internet. Consequently, tourists might spend a lot of time to search and compare different information and then decided their travel itinerary. To deal with this issue, we propose a formula for ranking tourist attractions by analyzing geo-tagged photographs on Flickr in this paper. In this way, tourists can save their time to find their interest tourist attractions readily. Moreover, our proposed method includes different aspects such as image quality assessment (IQA), the sentiment of comment, and the popularity of tourist attraction which can evaluate the attractive level of tourist attraction. Especially, we provide different ranking results for local residents and foreign visitors.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {97–103},
numpages = {7},
keywords = {Popularity of tourist attraction, Flickr, Image quality assessment, Geo-tagged photographs},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405990,
author = {Miyata, Yasushi and Ishikawa, Hiroshi},
title = {Concept Drift Detection on Data Stream for Revising DBSCAN Cluster},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405990},
doi = {10.1145/3405962.3405990},
abstract = {Data stream mining of IoT data can help operators immediately isolate causes of equipment alarms. The challenge, however, is how to keep the classifiers high-purity (i.e., keep data of the same class in the right cluster) while dealing with the concept drifting ascribed to differences between alarm models and entities. We propose continuously revising the classification model in accordance with the data distribution and trend changes. Evaluations showed there was no purity deterioration for oscillation condition data with a drifting rate of 1%. This result demonstrates that our approach can help operators improve their decision making.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {104–110},
numpages = {7},
keywords = {Concept Drift, data stream, DBSCAN, power grid, clustering},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405992,
author = {Toyoshima, Takuma and Endo, Masaki and Kikuchi, Takuo and Ohno, Shigeyoshi and Ishikawa, Hiroshi},
title = {Estimating Deflation Representing People Spreading in Stream Data},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405992},
doi = {10.1145/3405962.3405992},
abstract = {With the expanded use of social media such as Twitter in recent years, it has become easy to add various information such as location data using mobile devices. Using those data, one can observe the real world without using physical sensors. Therefore, social media have high operational value as social sensors. As described herein, we aim to support decision-making for people who intend to visit a specific place at which an event or some trouble recently occurred. After proposing a method of real-time extraction of data reflecting a burst state showing people's concentration, their inactivity, and continuous flow and dispersion, we confirm the method's effectiveness.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {111–116},
numpages = {6},
keywords = {Twitter, Burst structure, real time analysis, Deflation structure, social sensor},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405965,
author = {Exp\'{o}sito-Ventura, Marta and Ruip\'{e}rez-Valiente, Jos\'{e} A. and Forn\'{e}, Jordi},
title = {Measuring Online Advertising Viewability and Analyzing Its Variability Across Different Dimensions},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405965},
doi = {10.1145/3405962.3405965},
abstract = {Many of the current online business base completely their revenue models in earnings from online advertisement. A problematic fact is that according to Google more than half of display ads are not being seen. The International Advertising Bureau (IAB) has defined a viewable impression as an impression that at least 50% of its pixels are rendered in the viewport during at least one continuous second. Although there is agreement on this definition for measuring viewable impressions in the industry, there is no systematic methodologies on how it should be implemented or the trustworthiness of these implementations. In fact, the Media Rating Council (MRC) announced that there are inconsistencies across multiple reports attempting to measure this metric. For this reason, we select a subset of implementations to track viewable impressions and we perform a case study by implementing them in a webpage registered in the worldwide ad-network ExoClick in order to see their results on different dimensions. Our results show that the Intersection Observer API is the implementation that detects more viewable impressions and that there are significant viewability differences depending on the banner location on the website. Finally, we also propose an ensemble viewability method that proves to be able to detect a higher number of viewable impressions.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {117–122},
numpages = {6},
keywords = {web measurements, data mining, Viewability, online advertising},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405967,
author = {Usher, James and Dondio, Pierpaolo},
title = {BREXIT Election: Forecasting a Conservative Party Victory through the Pound Using ARIMA and Facebook's Prophet},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405967},
doi = {10.1145/3405962.3405967},
abstract = {On the 30th October, 2019, the markets watched as British Prime Minister, Boris Johnson, took a massive political gamble to call a general election to break the Withdrawal Agreement stalemate in the House of Commons to "Get BREXIT Done". The pound had been politically sensitive owing to BREXIT uncertainty. With the polls indicating a Conservative win on 4th December, 2019, the margin of victory could be observed through increases in the pound. The outcome of a Conservative party victory would benefit the pound by removing the current market turbulence. We look to provide a short-term forecast of the pound. Our approach focuses on modelling the GBP/EUR and GBP/USD Fx from the inception of BREXIT referendum talks from the 1st January, 2016 to the conclusion of the BREXIT election on the 12th December, 2019, focusing on forecasted increases in the pound from the 4th December, 2019. We construct two machine learning models in the form of an Auto Regressive Integrated Moving Average (ARIMA) financial time series and an additive regression financial time series using Facebook's Prophet to investigate the hypothesis that the polls prediction of a Conservative victory could be validated by forecasted increases in the pound. The efficiency of the forecasted models was then tested based on MAPE and MSE criteria. Our results found that the ARIMA and Prophet models were effective and proficient in forecasting the polls prediction on the 4th December, 2019 of a Conservative win by validation of forecasted increases in the pound. The ARIMA (4,1,0) model resulted in forecasts with the lowest MAPE and MAE.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {123–128},
numpages = {6},
keywords = {Time series forecasting, Facebook Prophet, BREXIT Election, ARIMA},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405982,
author = {Kirsh, Ilan},
title = {Directions and Speeds of Mouse Movements on a Website and Reading Patterns: A Web Usage Mining Case Study},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405982},
doi = {10.1145/3405962.3405982},
abstract = {Mouse activity is known as an important indicator of user attention and interest on a web page. Many modern commercial web analytics services record and report mouse activity of users on websites. The position of the mouse cursor on the screen is the main source of information, as studies show a correlation between the cursor position during mouse activity and the user's eye gaze. This study focuses on mouse movement directions and speeds, and what they indicate, rather than on the mouse cursor position. Statistical analysis of mouse movements on a technical-educational website, which was selected for this study, sheds light on several interesting patterns. For example, most mouse movements in the examined usage data are either approximately horizontal or approximately vertical, horizontal mouse movements are more frequent than vertical mouse movements, and horizontal movements to the left and to the right are not equivalent in terms of moving time and speed. As this study shows, these statistical findings are related to the reading patterns and behaviors of web users. Associating mouse movements with text reading may potentially highlight content that most users tend to skip, and therefore, might not interest the website audience, and content that many readers read more than once or slowly, meaning it is possibly unclear. This could be useful in locating issues in textual content, in websites in general, and especially in online learning and educational technology applications.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {129–138},
numpages = {10},
keywords = {Mouse Movement, Web Usage Mining, Text Reading Patterns, Reading Behaviors, Human-Computer Interaction, Web Pages, Websites, Online Learning, Web Analytics, Education Technology},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405975,
author = {Dalecke, Sandor and Karlsen, Randi},
title = {Designing Dynamic and Personalized Nudges},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405975},
doi = {10.1145/3405962.3405975},
abstract = {Nudging is about influencing people to make decisions that are beneficial to society and individuals. We are in particular concerned with using nudges to cause a behavioral change for persons, where healthier or environmentally friendlier behavior may be the goal. As people make more and more decisions in a digital context, digital nudging has steadily become more relevant. With today's technology, it is feasible to dynamically generate highly personalized nudges, using information on the person receiving the nudge, such as their intention and the situation they are in. This paper presents a new nudge model, designed with personalization in mind. We propose to use personal and situational data to generate the most suitable nudge designed from nudge components. The presented nudges should be transparent, helpful and effective to the user.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {139–148},
numpages = {10},
keywords = {personalization, dynamic nudge design, Digital nudging},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405987,
author = {Luberg, Ago and Pindis, Jakob and Tammet, Tanel},
title = {Sights, Titles and Tags: Mining a Worldwide Photo Database for Sightseeing},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405987},
doi = {10.1145/3405962.3405987},
abstract = {The paper focuses on calculating suitable place names and descriptive tags for large photo collections of visually interesting sights. The core dataset analyzed contains 45 million crowd-sourced geotagged pictures of the Panoramio database. We present several methods for analysis along with machine learning experiments for tag recommendation and suggest a manually built taxonomy of tag categories, based on the analysis of most widely used taglike words in the photo titles, along with their popularities. The methods, selected tags and the taxonomy can be used for building different tourism applications for visually interesting sights.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {149–158},
numpages = {10},
keywords = {popularity analysis, photo tagging, crowd-sourced mapping, POI categorization},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405985,
author = {Rusnachenko, Nicolay and Loukachevitch, Natalia},
title = {Attention-Based Neural Networks for Sentiment Attitude Extraction Using Distant Supervision},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405985},
doi = {10.1145/3405962.3405985},
abstract = {In the sentiment attitude extraction task, the aim is to identify «attitudes» - sentiment relations between entities mentioned in text. In this paper, we provide a study on attention-based context encoders in the sentiment attitude extraction task. For this task, we adapt attentive context encoders of two types: (1) feature-based; (2) self-based. In our study, we utilize the corpus of Russian analytical texts RuSentRel and automatically constructed news collection RuAttitudes for enriching the training set. We consider the problem of attitude extraction as two-class (positive, negative) and three-class (positive, negative, neutral) classification tasks for whole documents. Our experiments1 with the RuSentRel corpus show that the three-class classification models, which employ the RuAttitudes corpus for training, result in 10% increase and extra 3% by F1, when model architectures include the attention mechanism. We also provide the analysis of attention weight distributions in dependence on the term type.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {159–168},
numpages = {10},
keywords = {distant supervision, sentiment analysis, attention-based models, relation extraction},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405969,
author = {Johannessen, Erlend and Karlsen, Randi},
title = {Incremental Information Retrieval: Finding Obscured Information in Internet Search},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405969},
doi = {10.1145/3405962.3405969},
abstract = {When searching the internet e.g. for a person, solution to a problem, or some topic of interest, the wanted outcome is usually specific answers. The result quality for this kind of search is reasonably precise, most of the time we get the answers we need. However, searching a second or third time with the same query, the outcome seems to be minor variations on the same results.So what if the search for information is of a different nature, more like exploring. A typical case would be when a person has a hobby, and time after time wants to search for information about it. Very soon all the quickly accessed information has already been seen, and is not that interesting in the context of new information.This paper presents an approach to Incremental Information Retrieval, where each repeated search with a given query, will provide the user with previously obscured (i.e. unseen) results. We have implemented a prototype system, called IIR, where we demonstrate and test our approach. The system targets situations where users have a continuous information need, that cannot be satisfied through a single search on the Internet, but where the user may want to see new results on the same subject over a period of days, months, or even years. A detailed description of the IIR system and results of our tests are presented.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {169–177},
numpages = {9},
keywords = {Persistent Information Retrieval, Online search, Incremental Information Retrieval},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405981,
author = {Usher, James and Dondio, Pierpaolo},
title = {BREXIT: Psychometric Profiling the Political Salubrious through Machine Learning: Predicting Personality Traits of Boris Johnson through Twitter Political Text},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405981},
doi = {10.1145/3405962.3405981},
abstract = {Whilst the CIA have been using psychometric profiling for decades, Cambridge Analytica showed that peoples psychological characteristics can be accurately predicted from their digital footprints, such as their Facebook or Twitter accounts. To exploit this form of psychological assessment from digital footprints, we propose machine learning methods for assessing political personality from Twitter. We have extracted the tweet content of Prime Minster Boris Johnsons Twitter account and built three predictive personality models based on his Twitter political content. We use a Multi-Layer Perceptron Neural network, a Naive Bayes multinomial model and a Support Machine Vector model to predict the OCEAN model which consists of the Big Five personality factors from a sample of 3355 political tweets. The approach vectorizes political tweets, then it learns word vector representations as embeddings from spaCy that are then used to feed a supervised learner classifier. We demonstrate the effectiveness of the approach by measuring the quality of the predictions for each trait per model from a classification algorithm. Our findings show that all three models compute the personality trait "Openness" with the Support Machine Vector model achieving the highest accuracy. "Extraversion" achieved the second highest accuracy personality score by the Multi-Layer Perceptron neural network and Support Machine Vector model.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {178–183},
numpages = {6},
keywords = {OCEAN, Big Five Personality, Social Media, BREXIT},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405976,
author = {Mirkin, Boris and Frolov, Dmitry and Vlasov, Alex and Nascimento, Susana and Fenner, Trevor},
title = {A Hybrid Approach to Interpretable Analysis of Research Paper Collections},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405976},
doi = {10.1145/3405962.3405976},
abstract = {We define and find a most specific generalization of a fuzzy set of topics assigned to leaves of the rooted tree of a taxonomy. This generalization lifts the set to a "head subject" in the higher ranks of the taxonomy, that is supposed to "tightly" cover the query set, possibly bringing in some errors, both "gaps" and "offshoots". Our method involves two more automated analysis techniques: a fuzzy clustering method, FADDIS, involving both additive and spectral properties, and a purely structural string-to-text relevance measure based on suffix trees annotated by frequencies. We apply this to extract research tendencies from two collections of research papers: (a) about 18000 research papers published in Springer journals on data science for 20 years, and (b) about 27000 research papers retrieved from Springer and Elsevier journals in response to data science related queries. We consider a taxonomy of Data Science based on the Association for Computing Machinery Classification of Computing System (ACM-CCS 2012). Our findings allow us to make some comments on the tendencies of research that cannot be derived by using more conventional techniques.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {184–189},
numpages = {6},
keywords = {generalization, hybrid approach, fuzzy cluster, annotated suffix tree, research tendency},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405963,
author = {Vystr\v{c}ilov\'{a}, Michaela and Pe\v{s}ka, Ladislav},
title = {Lyrics or Audio for Music Recommendation?},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405963},
doi = {10.1145/3405962.3405963},
abstract = {Music recommender systems (RS) aim to aid people with finding relevant enjoyable music without having to sort through the enormous amount of available content. Music RS often rely on collaborative filtering methods, which however limits predicting capabilities in cold-start situations or for users who deviate from main-stream music preferences. Therefore, this paper evaluates various content-based music recommendation methods that may be used in combination with collaborative filtering to overcome such issues. Specifically, the paper focuses on the ability of lyrics-based embedding methods such as tf-idf, word2vec or bert to estimate songs similarity compared to state-of-the-art audio and meta-data based embeddings. Results indicate that both audio and lyrics methods perform similarly, which may favor lyrics-based approaches due to the much simpler processing. We also show that although lyrics-based methods do not outperform meta-data based approaches, they provide much more diverse, yet reasonably relevant recommendations, which is suitable in exploration-oriented music RS.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {190–194},
numpages = {5},
keywords = {music recommender systems, song lyrics, embeddings},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405964,
author = {Peeters, Ralph and Primpeli, Anna and Wichtlhuber, Benedikt and Bizer, Christian},
title = {Using Schema.Org Annotations for Training and Maintaining Product Matchers},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405964},
doi = {10.1145/3405962.3405964},
abstract = {Product matching is a central task within e-commerce applications such as price comparison portals and online market places. State-of-the-art product matching methods achieve F1 scores above 0.90 using deep learning techniques combined with huge amounts of training data (e.g &gt; 100K pairs of offers). Gathering and maintaining such large training corpora is costly, as it implies labeling pairs of offers as matches or non-matches. Acquiring the ability to be good at product matching thus means a major investment for an e-commerce company. This paper shows that the manual labeling of training data for product matching can be replaced by relying exclusively on schema.org annotations gathered from the public Web. We show that using only schema.org data for training, we are able to achieve F1 scores between 0.92 and 0.95 depending on the product category. As new products appear everyday, it is important that matching models can be maintained with justifiable effort. In order to give practical advice on how to maintain matching models, we compare the performance of deep learning and traditional matching models on unseen products and experiment with different fine-tuning and re-training strategies for model maintenance, again using only schema.org annotations as training data. Finally, as using the public Web as distant supervision carries inherent noise, we evaluate deep learning and traditional matching models with regards to their label-noise resistance and show that deep learning is able to deal with the amounts of identifier-noise found in schema.org annotations.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {195–204},
numpages = {10},
keywords = {schema.org, distant supervision, product matching, deep learning, e-commerce},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405980,
author = {Seraj, Saeed and Pavlidis, Michalis and Polatidis, Nikolaos},
title = {A Novel Dataset for Fake Android Anti-Malware Detection},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405980},
doi = {10.1145/3405962.3405980},
abstract = {Today in the world people are able to get all types of Android applications (apps) from the app store or various sources over the Internet. A large number of apps is being produced daily, some of which are infected with malware. Thus, the use of anti-malware identification tools is essential. At the same time, a number of attackers who exploit a number of anti-malwares have been doing obtaining information from mobile phones in various ways, such as decompiling or infecting anti-malware. Therefore, in this paper, we developed a classification dataset from collected anti-malware data looking for fraudulent anti-malware products. Additionally, we applied various machine learning algorithms and we propose a combination of algorithms which provides high accuracy over various evaluation tests, showing that our approach is both practical and effective.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {205–209},
numpages = {5},
keywords = {Anti-malware, Fake anti-malware detection, Android, Malware, Machine learning, Cyber security},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405968,
author = {Steuber, Florian and Schoenfeld, Mirco and Rodosek, Gabi Dreo},
title = {Topic Modeling of Short Texts Using Anchor Words},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405968},
doi = {10.1145/3405962.3405968},
abstract = {We present Archetypal LDA or short A-LDA, a topic model tailored to short texts containing "semantic anchors" which convey a certain meaning or implicitly build on discussions beyond their mere presence. A-LDA is an extension to Latent Dirichlet Allocation in that we guide the process of topic inference by these semantic anchors as seed words to the LDA. We identify these seed words unsupervised from the documents and evaluate their co-occurrences using archetypal analysis, a geometric approximation problem that aims for finding k points that best approximate the data set's convex hull. These so called archetypes are considered as latent topics and used to guide the LDA. We demonstrate the effectiveness of our approach using Twitter, where semantic anchor words are the hashtags assigned to tweets by users. In direct comparison to LDA, A-LDA achieves 10-13% better results. We find that representing topics in terms of hashtags corresponding to calculated archetypes alone already results in interpretable topics and the model's performance peaks for seed confidence values ranging from 0.7 to 0.9.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {210–219},
numpages = {10},
keywords = {short text, topic modeling, data mining, text mining, archetypal analysis},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405978,
author = {Lentschat, Martin and Buche, Patrice and Dibie-Barthelemy, Juliette and Roche, Mathieu},
title = {SciPuRe: A New Representation of Textual Data for Entity Identification from Scientific Publications},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405978},
doi = {10.1145/3405962.3405978},
abstract = {Retrieving entities associated with experimental data in the textual content of scientific documents faces numbers of challenges. One of them is the assessment of the extracted entities for further process, especially the identification of false positives. We present in this paper SciPuRe (Scientific Publication Representation): a new representation of entities.The extraction process presented in this paper is driven by an Ontological and Terminological Resource (OTR). It is applied to the extraction of entities associated with food packaging permeabilities, that can be symbolic (e.g. the Packaging "low density polyethylene") or quantitative (e.g. the Temperature "25", "°C" or the H20_Permeability "4.34 * 10-3", "cm3 μm-2 d-1 kPa"). A representation of each entity, composed of a set of features, is built during the extraction process. These features can be gathered in three categories: Ontological, Lexical and Structural. The features of SciPuRe are used to compute Relevance scores that consider the different information available for each entity extracted. Such Relevance scores inform the usefulness of SciPuRe and can then be used to rank the extraction results and discard false positives.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {220–226},
numpages = {7},
keywords = {Ontological and Terminological Resource, Web Scientific Documents, Information Retrieval, Relevance ranking, Data Representation},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405994,
author = {Soller, Sebastian and Kranz, Matthias and Hoelzl, Gerold},
title = {Adaptive Error Prediction for Production Lines with Unknown Dependencies},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405994},
doi = {10.1145/3405962.3405994},
abstract = {Forecasting or predicting errors can dramatically reduce the downtime of machines in industrial settings and even allow to take counteractions long before the error affects the production system. A forecast system to predict upcoming critical values for identical production lines under different environmental circumstances is proposed. We focus on errors that result in multiple erroneous work pieces. These error patterns need manual corrections by a machine controller. An analysis of the system observed gathered the information about the types of errors that are observable. 30% of errors are measurement errors or single faulty work-pieces which are not influenced by previous work-pieces and do not show any indication to preceding work-pieces. These errors do not need any type of action by the machine controller. 70% of the observed errors are continuous system deviations which lead to multiple erroneous work-pieces in order or a high percentage of erroneous work-pieces in an observed time frame.We observe multiple production lines which consist of identical machines and produce the same product type. For the forecast of errors, we use the ARIMA, Holt and Holt-Winter method. Each production line and product type combination showed different results for the different forecast methods. We implemented a dynamic system that automatically detects the seasonality and trend of the specific combination to assign a correct forecast method and model. For 40 combinations of production line and product type the holt-winter algorithm performed best for 14, the holt-winter without seasonal or trend component performed best for 13 combinations and the holt-winter with only a trend component performed best for 10 setups. 3 combinations did not have a distinct best method for all observed results. By selecting the correct forecast methods, we were able to boost the forecast accuracy for the overall system over each single forecast method.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {227–234},
numpages = {8},
keywords = {maintenance prediction, time series forecast, real-time system, data mining, seasonality analysis},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405972,
author = {Elizabeth, Varkey Merlyn and Singh, Khurmi Yashpreet and Anupam, Garg},
title = {Design and Development of Modular Industrial Robot Kit (Merlyn TRN-1) for Classroom Training in STEM and ROBOTICS: Real Time Intelligent Training System},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405972},
doi = {10.1145/3405962.3405972},
abstract = {Merlyn TRN-1 is a set of precision machined parts so designed that they can be configured into interlinked linear and rotary axis as per your choice. The kit contains all the necessary mechanical structural elements, motors, motor driver electronics, microprocessor controllers and software. Each of them can be individually changed to upgrade your capacity and requirement. The modularity is an integral part of the kit. Merlyn TRN-1 modular design, high precision rugged parts with innovative tolerances and versatile interconnection of parts allow the user to add axis as per their design.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {235–237},
numpages = {3},
keywords = {Automation, High precision, Higher education, Industrial Automation, Robotics, STEM, Modularity, Student Robotics Training},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405996,
author = {Da\v{s}i\'{c}, Dejan and Vu\v{c}eti\'{c}, Miljan and Peri\'{c}, Miroslav and Beko, Marko and Stankovi\'{c}, Milo\v{s}},
title = {Cooperative Multi-Agent Reinforcement Learning for Spectrum Management in IoT Cognitive Networks},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405996},
doi = {10.1145/3405962.3405996},
abstract = {The paper investigates the applications of cooperative Multi-Agent Reinforcement Learning (MARL) schemes to Cognitive Radio Networking (CRN), which in turn can facilitate spectrum utilization for wireless (ad hoc) networks within the Internet of Things (IoT). These schemes provide the ability of wireless transceivers to learn the optimal control and configuration in unknown environmental and application conditions, exploiting potential for cooperation among spectrum secondary users. An overview of the existing MARL approaches to the CRN is provided, with an analysis of their advantages and weaknesses compared to the rest of CRN approaches. We argue that in typical CRN practical scenarios including IoT systems, it is of essential importance that the cooperative algorithms are completely decentralized and distributed, having also a capability that the agents/nodes together can successfully calculate the optimal strategy even if the individual agents cannot. Hence, we propose a new scheme for cooperative spectrum sensing and selection within CRN, based on an adaptation of a recently proposed cooperative MARL scheme, provide detailed analysis of its properties and potential performance, indicating its superiority compared to the existing schemes.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {238–247},
numpages = {10},
keywords = {Internet of Things, Multi-Agent Reinforcement Learning, Joint Spectrum Sensing and Channel Selection, Cognitive Radio Networking},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405974,
author = {Lewis, Rory},
title = {Rough Set &amp; Riemannian Covariance Matrix Theory for Mining the Multidimensionality of Artificial Consciousness},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405974},
doi = {10.1145/3405962.3405974},
abstract = {This paper presents a means to analyze the multidimensionality of human consciousness as it interacts with the brain by utilizing Rough Set Theory and Riemannian Covariance Matrices. We mathematically define the infantile state of a robot's operating system running artificial consciousness, which operates mutually exclusively to the operating system for its AI and locomotor functions.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {248–251},
numpages = {4},
keywords = {Riemannian Theory, Rough Sets, Artificial Consciousness},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405971,
author = {Mahammed, Nadir and Bennabi, Souad and Fahsi, Mahmoud},
title = {Optimizing Business Process Designs with a Multiple Population Genetic Algorithm},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405971},
doi = {10.1145/3405962.3405971},
abstract = {This article discusses a multi-objective business process optimization. The authors present an approach for an evolutionary combinatorial multi-objective optimization of business process designs with a specified genetic algorithm based on multiple populations. The results show that the optimization approach is capable of producing a satisfactory number of optimized designs alternatives.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {252–254},
numpages = {3},
keywords = {business process, multiple population, multi-criteria optimization, genetic algorithm},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3405962.3405995,
author = {Kneis, Bryan and Zhang, Wenhao},
title = {3D Face Recognition Using Photometric Stereo and Deep Learning},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405995},
doi = {10.1145/3405962.3405995},
abstract = {Illumination variance is one of the largest real-world problems when deploying face recognition systems. Over the last few years much work has gone into the development of novel 3D face recognition methods to overcome this issue. Photometric stereo is a well-established 3D reconstruction technique capable of recovering the normals and albedo of a surface. Although it provides a way to obtain 3D data, the amount of training data available captured using photometric stereo often does not provide sufficient modelling capacity for training state-of-the-art feature extractors, such as deep convolutional neural networks, from scratch.In this work we present a novel approach to utilising the lighting apparatus commonly used for photometric stereo to synthesise data that can act as a biometric. Combining this with deep learning techniques not only did we achieve near state-of-the-art results, but it gave insight into the possibility of using photometric stereo without the need of reconstruction. This could not only simplify the face recognition process but avoid unnecessary error that may arise from reconstruction.Additionally, we utilise the active lighting from photometric stereo to evaluate the effect of illumination on face recognition. We compare our method to the state-of-the-art 3D methods and discuss potential use cases for our system.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {255–261},
numpages = {7},
keywords = {Photometric Stereo, Deep Learning, Face Recognition},
location = {Biarritz, France},
series = {WIMS 2020}
}

