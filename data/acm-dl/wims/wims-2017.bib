@inproceedings{10.1145/3102254.3102255,
author = {Cima, Gianluca and Lenzerini, Maurizio and Poggi, Antonella},
title = {Semantic Technology for Open Data Publishing},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102255},
doi = {10.1145/3102254.3102255},
abstract = {After years of focus on technologies for big data storing and processing, many observers are pointing out that making sense of big data cannot be done without suitable tools for conceptualizing, preparing, and integrating data (see http://www.dbta.com/). Research in the last years has shown that taking into account the semantics of data is crucial for devising powerful data integration solutions. In this work we focus on a specific paradigm for semantic data integration, named "Ontology-Based Data Access" (OBDA), proposed in [1--4]. According to such paradigm, the client of the information system is freed from being aware of how data and processes are structured in concrete resources (databases, software programs, services, etc.), and interacts with the system by expressing her queries and goals in terms of a conceptual representation of the domain of interest, called ontology. More precisely, a system realizing the vision of OBDA is constituted by three components:• The ontology, whose goal is to provide a formal, clean and high level representation of the domain of interest, and constitutes the component with which the clients of the system (both humans and software programs) interact.• The data source layer, representing the existing data sources in the information system, which are managed by the processes and services operating on their data.• The mapping between the two layers, which is an explicit representation of the relationship between the data sources and the ontology, and is used to translate the operations on the ontology (e.g., query answering) in terms of concrete actions on the data sources.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {1},
numpages = {1},
keywords = {ontology-based data access, open data},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102256,
author = {Pastor, Oscar},
title = {A WIMS Perspective for Understanding the Human Genome: The Role of Conceptual Modeling},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102256},
doi = {10.1145/3102254.3102256},
abstract = {Our strong capability of conceptualization makes us, human beings, different from any other species in our planet1. Conceptual modelers should apply in the right direction such fascinating capability to make it play an essential role to face the big challenge of understanding life in general, the human genome in particular. Considering the huge amount of unstructured genome data that every day are published in Internet and the need of elaborating a sound and structured knowledge from them, Web Intelligence, Mining and Semantic (WIMS) approaches becomes essential alternatives to assess that an effective data mining and data semantics understanding is achieved in order to identify the relevant information that is needed to link conceptual models and life.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {2},
numpages = {1},
keywords = {information and knowledge extraction, web mining, web-based health- and bio- information systems, conceptual modeling},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102257,
author = {Cal\'{\i}, Andrea},
title = {Querying and Searching the Deep Web},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102257},
doi = {10.1145/3102254.3102257},
abstract = {The term Deep Web (sometimes also called Hidden Web) [2, 5, 8] refers to the data content that is accessible through Web pages, typically via HTML forms, but is not available on static pages for indexing by search engines. Deep Web data reside in databases and are made available dynamically, as Web pages, upon a specific search or query. An example is when we query a Yellow Pages website: the generated output is the result of a query posed on an underlying database, and is not stored as static pages; such output can normally be represented in relational form.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {3},
numpages = {1},
keywords = {deep web, query processing, information integration},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102258,
author = {Tagarelli, Andrea},
title = {Exploring Lurking Behaviors in Online Social Networks},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102258},
doi = {10.1145/3102254.3102258},
abstract = {Lurking in an online social network (OSN) refers to the behavior shown by silent users in the crowd, i.e., users who do not significantly take an active role in the interaction with other members of the OSN. Lurkers typically gain benefit from information produced by others, though their presence is legitimated, expected or even welcome. Lurkers might hold potential social capital, because they acquire knowledge from the OSN, therefore it is desirable to encourage them to more actively participate. Lurking behavior analysis has been long studied in social science and human-computer interaction fields, but it has also matured over the last few years in social network analysis and mining. This tutorial introduces the principles, models, and methods for the characterization and analysis of lurkers in OSNs, and discusses current and emerging applications related to the engagement of lurkers.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {4},
numpages = {1},
keywords = {behavior analysis, lurking, user engagement, ranking, social networks, influence maximization},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102259,
author = {Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe},
title = {Evolutionary Mining of Relaxed Dependencies from Big Data Collections},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102259},
doi = {10.1145/3102254.3102259},
abstract = {Many modern application contexts, especially those related to the semantic Web, advocate for automatic techniques capable of extracting relationships between semi-structured data, for several purposes, such as the identification of inconsistencies or patterns of semantically related data, query rewriting, and so forth. One way to represent such relationships is to use relaxed functional dependencies (rfds), since they can embed approximate matching paradigms to compare unstructured data, and admit the possibility of exceptions for them. To this end, thresholds might need to be specified in order to limit the similarity degree in approximate comparisons or the occurrence of exceptions. Thanks to the availability of huge amount of data, including unstructured data available on the Web, nowadays it is possible to automatically discover rfds from data. However, due to the many different combinations of similarity and exception thresholds, the discovery process has an exponential complexity. Thus, it is vital devising proper optimization strategies, in order to make the discovery process feasible. To this end, in this paper, we propose a genetic algorithm to discover rfds from data, also providing an empirical evaluation demonstrating its effectiveness.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {5},
numpages = {10},
keywords = {genetic algorithm, discovery from data, functional dependency},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102263,
author = {Bernardi, Mario Luca and Cimitile, Marta and Martinelli, Fabio and Mercaldo, Francesco},
title = {A Time Series Classification Approach to Game Bot Detection},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102263},
doi = {10.1145/3102254.3102263},
abstract = {Online games consumers are strongly grown in the last years attracted by the always higher quality of the games and the more effective gaming infrastructures. The increasing of on line games market is also concurrent to the diffusion of game bots that allow to automatize malicious tasks obtaining some rewards with respect to the other game players (the game bots user increases personal benefits and popularity with low effort). Given the interest of game developers to preserve game equity and player satisfaction, the topic of game bots detection is becoming very critical and consists to distinguish between game bots and human players behaviour. This paper describes an approach to the online role player games bot detection based on time series classification used to discriminate between human and game bots behavioral features. In this paper an application of the proposed approach in a real role player game is reported.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {6},
numpages = {11},
keywords = {game bot detection, machine learning, time series},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102275,
author = {Ardimento, Pasquale and Dinapoli, Andrea},
title = {Knowledge Extraction from On-Line Open Source Bug Tracking Systems to Predict Bug-Fixing Time},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102275},
doi = {10.1145/3102254.3102275},
abstract = {1For large scale software systems, many bugs can be reported over a long period of time. For software quality assurance and software project management, it is important to assign adequate resources to resolve the reported bug. An important issue concerning assignment is the ability to predict bug-fixing time because it can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we propose a model that can predict the bug-fixing time using the text information extracted from Bugzilla, an on-line open source Bug Tracking System (BTS). We perform an empirical investigation for the bugs of Novell, OpenOffice and LiveCode, three open source projects using Bugzilla. Proposed model is based on historical data stored on the BTS. For each bug-report we build a classification model to predict the time of its resolution, as slow or fast. In this work we used, as classifier, Support Vector Machine (SVM) but different classifier can be easily used. Our model, differently from existing work reported in the literature, selects all and only the attributes useful for prediction and filters appropriately attributes for the test-set. Experimental results show the model is effective. In the future, we will use and compare other different classification method to select the best one for a specific data-set.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {7},
numpages = {9},
keywords = {bug-fix time prediction, bug triage, support vector machine (SVM), supervised learning, text categorization},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102260,
author = {Karim, Farah and Mami, Mohamed Nadjib and Vidal, Maria-Esther and Auer, S\"{o}ren},
title = {Large-Scale Storage and Query Processing for Semantic Sensor Data},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102260},
doi = {10.1145/3102254.3102260},
abstract = {Nowadays, there is a rapid increase in the number of sensor data produced by a wide variety of devices and sensors. Collections of sensor data can be semantically described using ontologies, e.g., the Semantic Sensor Network (SSN) ontology. Albeit semantically enriched, the volume of semantic sensor data is considerably larger than raw sensor data. Moreover, some measurement values can be observed several times, and a large number of repeated facts can be generated. We devise a compact or factorized representation of semantic sensor data, where repeated values are represented only once. To scale up to large datasets, tabular representation is utilized to store and manage factorized semantic sensor data using Big data technologies. We empirically study the effectiveness of the proposed factorized representation of semantic sensor data, and the impact of factorizing semantic sensor data on query processing. Furthermore, we evaluate the effects of storing RDF factorized data on state-of-the-art RDF engines and in the proposed tabular-based representation. Results suggest that factorization techniques empower storage and query processing of sensor data, and execution time can be reduced by up to two orders of magnitude.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {8},
numpages = {12},
keywords = {data factorization, linked sensor data, query execution},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102264,
author = {Jamil, Hasan M.},
title = {Efficient Top-k Shortest Path Query Processing in Sparse Graph Databases},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102264},
doi = {10.1145/3102254.3102264},
abstract = {While internet graphs are usually dense as a whole, localized sub-graphs are often sparse. In particular, in social networks, the graphs corresponding to users' friends, connections and interactions with others are almost always sparse relative to the entire social graph of which they are a part. In such sparse graphs, reachability queries may suffer unnecessary performance losses if generalized reachability algorithms are used. Classical shortest path queries are among those that incur such losses.Yen's classical top-k shortest path algorithm has a worst case bound of O(kn(m + n log n)) for k shortest paths where n is the number of nodes and m is the number of edges. In this paper, our goal is to leverage recent development in graph reachability research to propose a pre-computed reachability index in order to reduce the cost by a factor of O(n) needed to discover each of k single-source shortest paths. We experimentally show that as k approaches the number of possible paths, our algorithm outperforms contemporary methods. In particular, we demonstrate using both simulated and real life data set that the path discovery in our algorithm converges significantly faster than the many variants of Yen's method when the k approximation condition is met.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {9},
numpages = {10},
keywords = {sparse graphs, social nets, shortest path, reachability, indexing, graph databases},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102277,
author = {Cima, Gianluca and De Giacomo, Giuseppe and Lenzerini, Maurizio and Poggi, Antonella},
title = {On the SPARQL Metamodeling Semantics Entailment Regime for OWL 2 QL Ontologies},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102277},
doi = {10.1145/3102254.3102277},
abstract = {OWL 2 QL is the profile of OWL 2 targeted to Ontology-Based Data Access (OBDA) scenarios, where large amount of data are to be accessed, and thus answering conjunctive queries over data is the main task. However, this task is quite restrained wrt the classical KR Ask-and-Tell framework based on querying the whole theory, not only facts (data). If we use SPARQL as query language, we get much closer to this ideal. Indeed, SPARQL queries over OWL 2 QL, under the so-called Direct Semantics Entailment Regime, may comprise any assertion expressible in the language, i.e., both ABox atoms and TBox atoms, including inequalities expressed by means of DifferentIndividuals. Nevertheless this regime is hampered by the assumption that variables in queries need to be typed, meaning that the same variable cannot occur in positions of different types, e.g., both in class and individual position (punning). In this paper we dismiss this limiting assumption by resorting to a recent meta modeling semantics and show that query answering in the resulting entailment regime is polynomially compilable into Datalog (and hence PTIME wrt both TBox and ABox).},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {10},
numpages = {6},
keywords = {OWL 2, datalog, ontology-based data access},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102262,
author = {Di Crescenzo, Cristiano and Gavazzi, Giulia and Legnaro, Giacomo and Troccoli, Elena and Bordino, Ilaria and Gullo, Francesco},
title = {HERMEVENT: A News Collection for Emerging-Event Detection},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102262},
doi = {10.1145/3102254.3102262},
abstract = {News portals and microblogging platforms have become people's medium of choice for breaking news and unexpected events, thanks to their ability to provide directions and useful information more timely and more effectively than official communication channels. This has caused a flourishing of research on event detection in social-media streams. However, this research is severely affected by the scarcity of publicly-available test collections, which are needed to build proper evaluation mechanisms.In this paper we introduce a new test collection for event detection, which we dub HERMEVENT. The dataset includes a large-scale dump of tweets and news articles from a list of major Italian newspapers, spanning a time interval of approximately 3 months in 2016/2017. From this dump we extracted a set of temporal graphs with different semantic and temporal granularity. To demonstrate the good quality of our data collection, we run two state-of-the-art algorithms that detect emerging events by extracting dense sub-graphs from a temporal graph. We conduct an editorial evaluation of the events discovered by the two algorithms on a set of 780 stories, achieving an accuracy of 75% in detecting real-world events. We make the text dump, the graphs and the editorial judgements freely available. We believe that this new dataset can be a really useful contribution to support research on event detection.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {10},
keywords = {emerging events, entity networks, web information retrieval, event detection, entity linking, data collection, web mining, graph mining},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102261,
author = {Helmich, Ji\v{r}\'{\i} and Poto\v{c}ek, Tobi\'{a}\v{s} and Kl\'{\i}mek, Jakub and Ne\v{c}ask\'{y}, Martin},
title = {Towards Easier Visualization of Linked Data for Lay Users},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102261},
doi = {10.1145/3102254.3102261},
abstract = {There are lots of Linked Open Data (LOD) datasets published today. However, the possibilities of their consumption are very limited and certainly not suitable for lay users. A lay user is often insufficiently shielded from the RDF format, e.g. when facing resource IRIs. Also, he needs to be a domain expert to understand the published datasets as they are published in a form which provides as much information as possible. Typically, a middle man is needed to interpret the data for the lay users and to create an understandable view. However, due to the lack of LOD enabled tools, the data gets converted to a lesser data format such as CSV, XML or JSON and therefore looses its semantics. In this paper, we identify and formalize the current problems related to publishing Linked Data based visualizations and we propose a method of configuring views for lay users. Moreover, we present a tool, LinkedPipes Visualization Assistant (LPVA), which experimentally implements the proposed method. We also evaluate the implementation and present two experiments based on real world data.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {12},
numpages = {9},
keywords = {linked data, RDF, data presentation, data visualization},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102265,
author = {Lynden, Steven},
title = {Analysis of Semantic URLs to Support Automated Linking of Structured Data on the Web},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102265},
doi = {10.1145/3102254.3102265},
abstract = {A growing amount of structured data can be found embedded in web pages using formats such as RDFa, JSON-LD and Microdata. Although such data is indexed by search engines and sometimes replicated in centralised knowledge bases, application scenarios exist in which there is a need to discover such data on-the-fly, for example when using the follow-your-nose principle of accessing Linked Open Data, or in applications where the velocity at which data changes can result in centralised repositories being out of date. In this paper we demonstrate two complementary techniques for aiding such applications by analysing URLs. Firstly, we demonstrate that machine learning can be of benefit in predicting, from previously encountered URLs, the likelihood of encountering structured data in an unseen URL. This can be applied within applications that encounter large number of possible URLs to dereference, and must implement some priority scheme to choose relevant URLs. Secondly, we demonstrate that association rule mining can be of use in linking existing resources in a knowledge base, such as DBpedia, to URLs that follow common schemes, such as Semantic (search engine friendly) URLs.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {13},
numpages = {6},
keywords = {linked data, semantic URLs, structured data},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102268,
author = {Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.},
title = {An Innovative Majority Voting Mechanism in Interactive Social Network Clustering},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102268},
doi = {10.1145/3102254.3102268},
abstract = {We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {social networks, majority, vote, graph clustering},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102282,
author = {Azzouza, Noureddine and Akli-Astouati, Karima and Oussalah, Amira and Bachir, Samy Ait},
title = {A Real-Time Twitter Sentiment Analysis Using an Unsupervised Method},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102282},
doi = {10.1145/3102254.3102282},
abstract = {With the evolution of Social Networks (SNs) such as Twitter, millions of users can interacting, sharing interests, activities, contents or exchanging experiences and opinions. Sharing opinions is an active research topic in the framework of sentiment analysis and opinion mining.In this paper, we present a real-time implementation of a system that can discover and track opinions on Twitter using Apache Storm tool. Our system provides multiple opinions' representations through dynamic graphic visualizations. We use an unsupervised machine learning technique, to analyze opinions and detect tweets polarity. By doing so, we can recommend relevant keywords regarding the main topic of interest.Results using SemEval Datasets, show that our solution outperforms the existing ones, by recording better scores.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {15},
numpages = {10},
keywords = {real-time, Twitter sentiment analysis, unsupervised method, opinion mining},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102283,
author = {Ben-Lhachemi, Nada and Nfaoui, El Habib},
title = {An Extended Spreading Activation Technique for Hashtag Recommendation in Microblogging Platforms},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102283},
doi = {10.1145/3102254.3102283},
abstract = {Microblogging platforms such as Twitter have become immensely popular. Users of these platforms use hashtags to gather similar posts under one umbrella. However, the percentage of tweets containing hashtags is still small, and hashtag use is very heterogeneous because users are free to create their own hashtags. Therefore, users might spend considerable time searching for appropriate hashtags for their posts in real time. Thus, there is a need for a powerful and effective hashtag recommendation system to assist users. Recent publications have described methods to address this need from different viewpoints, but these methods are based on traditional approaches to classification and tag recommendation that are successful for long texts but not for short texts, such as tweets. The existing methods use a syntactic mechanism, which has limitations; for example, they do not consider blogger preferences in the choice of hashtags. In this paper, we introduce an approach to hashtag recommendation in microblogging platforms that is based on our extended version of the spreading activation technique and uses several semantic knowledge bases to fill the gap between the semantic context of tweets and the semantic meaning of hashtags. The proposed approach semantically annotates the current tweet and the most popular hashtags in a recent period, after which it constructs a semantic network using DBpedia. We propose the use of an extended version of the spreading activation technique to calculate the semantic similarities of tweets and hashtags and then recommend the top k suitable hashtags to the blogger. The experimental results obtained from two Twitter datasets collected in 2012 and 2016 demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {16},
numpages = {8},
keywords = {DBpedia, semantic similarity, microblogging platforms, recommender system, hashtag, spreading activation},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102272,
author = {Weichselbraun, Albert and Kuntschik, Philipp},
title = {Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102272},
doi = {10.1145/3102254.3102272},
abstract = {Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {12},
keywords = {named entity linking, linked data quality, semantic technologies, applications, information extraction, mitigation strategies},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102273,
author = {Colacino, Vincenzo Giuseppe and Po, Laura},
title = {Managing Road Safety through the Use of Linked Data and Heat Maps},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102273},
doi = {10.1145/3102254.3102273},
abstract = {Road traffic injuries are a critical public health challenge that requires valuable efforts for effective and sustainable prevention. Worldwide, an estimated 1.2 million people are killed in road crashes each year and as many as 50 million are injured. An analysis of data provided by authoritative sources can be a valuable source for understanding which are the most critical points on the road network. The aim of this paper is to discover data about road accidents in Italy and to provide useful visualization for improving road safety. Starting from the annual report of road accidents of the Automobile Club of Italy, we transform the original data into an RDF dataset according to the Linked Open Data principles and connect it to external datasets. Then, an integration with Open Street Map allows to display the accident data on a map. Here, the final user is able to identify which road sections are most critical based on the number of deaths, injuries or accidents.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {18},
numpages = {8},
keywords = {linked open data, geographic visualization, OWL, visualization, map, RDF, road fatalities, LOD, road accident rate, heat map, data integration},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102266,
author = {Celestini, Alessandro and Guarino, Stefano},
title = {Design, Implementation and Test of a Flexible Tor-Oriented Web Mining Toolkit},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102266},
doi = {10.1145/3102254.3102266},
abstract = {Searching and retrieving information from the Web is a primary activity needed to monitor the development and usage of Web resources. Possible benefits include improving user experience (e.g. by optimizing query results) and enforcing data/user security (e.g. by identifying harmful websites). Motivated by the lack of ready-to-use solutions, in this paper we present a flexible and accessible toolkit for structure and content mining, able to crawl, download, extract and index resources from the Web. While being easily configurable to work in the "surface" Web, our suite is specifically tailored to explore the Tor dark Web, i.e. the ensemble of Web servers composing the world's most famous darknet. Notably, the toolkit is not just a Web scraper, but it includes two mining modules, respectively able to prepare content to be fed to an (external) semantic engine, and to reconstruct the graph structure of the explored portion of the Web. Other than discussing in detail the design, features and performance of our toolkit, we report the findings of a preliminary run over Tor, that clarify the potential of our solution.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {19},
numpages = {10},
keywords = {tor web graph, dark web},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102278,
author = {Elsayed, Ahmed and Eldin, Ahmed Sharaf and Elzanfaly, Doaa S. and Kholeif, Sherif},
title = {ConteSaG: Context-Based Keyword Search over Multiple Heterogeneous Graph-Modeled Data},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102278},
doi = {10.1145/3102254.3102278},
abstract = {Following the NoSQL (Not Only SQL) movement, more research work and applications are looking towards graph databases for their dynamic schema and natural representation of complex data1. In order to access/ search graph data in a single source, a number of search methods have been proposed in the literature. These methods range from graph query languages, pattern queries, template/form based search, to keyword search. One key challenge of these methods is the expressiveness and ease of use trade-off for query formulation. Formulating queries becomes more challenging when querying multiple heterogeneous data sources and when users are unaware of the structure of the underlying data. This paper reviews various methods proposed in the literature to query graph modeled data in two different settings; namely, single source and multiple heterogeneous data sources. Furthermore, the paper proposes ConteSaG, a technique for transparently querying multiple heterogeneous data sources. ConteSaG employs graph database to represent data residing in local sources with no need to create complex global schema or even to upfront integrate all data in a central source. Moreover, ConteSaG provides a context-based keyword search over the graph representations. Context-based keyword search allows users to search multiple data sources by determining the context of search terms without the need to have complete knowledge about the structure of data in the local sources or writing queries in a specific query language.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {20},
numpages = {6},
keywords = {graph databases, heterogeneous data, context keyword search},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102279,
author = {Cochez, Michael and Ristoski, Petar and Ponzetto, Simone Paolo and Paulheim, Heiko},
title = {Biased Graph Walks for RDF Graph Embeddings},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102279},
doi = {10.1145/3102254.3102279},
abstract = {Knowledge Graphs have been recognized as a valuable source for background information in many data mining, information retrieval, natural language processing, and knowledge extraction tasks. However, obtaining a suitable feature vector representation from RDF graphs is a challenging task. In this paper, we extend the RDF2Vec approach, which leverages language modeling techniques for unsupervised feature extraction from sequences of entities. We generate sequences by exploiting local information from graph substructures, harvested by graph walks, and learn latent numerical representations of entities in RDF graphs. We extend the way we compute feature vector representations by comparing twelve different edge weighting functions for performing biased walks on the RDF graph, in order to generate higher quality graph embeddings. We evaluate our approach using different machine learning, as well as entity and document modeling benchmark data sets, and show that the naive RDF2Vec approach can be improved by exploiting Biased Graph Walks.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {21},
numpages = {12},
keywords = {graph embeddings, linked open data, data mining},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102280,
author = {Collarana, Diego and Galkin, Mikhail and Traverso-Rib\'{o}n, Ignacio and Vidal, Maria-Esther and Lange, Christoph and Auer, S\"{o}ren},
title = {MINTE: Semantically Integrating RDF Graphs},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102280},
doi = {10.1145/3102254.3102280},
abstract = {The nature of the RDF data model allows for numerous descriptions of the same entity. For example, different RDF vocabularies may be utilized to describe pharmacogenomic data, and the same drug or gene is represented by different RDF graphs in DBpedia or Drug-bank. To provide a unified representation of the same real-world entity, RDF graphs need to be semantically integrated. Semantic integration requires the management of knowledge encoded in RDF vocabularies to determine the relatedness of different RDF representations of the same entity, e.g., axiomatic definition of vocabulary properties or resource equivalences. We devise MINTE, an integration technique that relies on both: knowledge stated in RDF vocabularies and semantic similarity measures to merge semantically equivalent RDF graphs, i.e., graphs corresponding to the same real-world entity. MINTE follows a two-fold approach to solve the problem of integrating RDF graphs. In the first step, MINTE implements a 1--1 weighted perfect matching algorithm to identify semantically equivalent RDF entities in different graphs. Then, MINTE relies on different fusion policies to merge triples from these semantically equivalent RDF entities. We empirically evaluate the performance of MINTE on data from DBpedia, Wiki-data, and Drugbank. The experimental results suggest that MINTE is able to accurately integrate semantically equivalent RDF graphs.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {22},
numpages = {11},
keywords = {RDF graph, fusion policy, semantic data integration, equivalent RDF molecules},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102276,
author = {Greco, Luca and Ritrovato, Pierluigi and Vento, Mario},
title = {Advanced Video Analytics: An Ontology-Based Approach},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102276},
doi = {10.1145/3102254.3102276},
abstract = {During the last decades the interest in the development of surveillance systems capable of autonomously performing video analytics task has become prominent in the scientific community, both for real time analysis and post event forensics. In this paper, we propose a novel video analytics framework where the output of a tracking algorithm on a sequence captured from a video surveillance camera is semantically annotated according to a custom ontology, allowing advanced analytics functionalities. Our approach is based on semantic web standards that guarantee wide interoperability.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {23},
numpages = {6},
keywords = {semantic web, event recognition, video analytics},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102284,
author = {Kachroudi, Marouen and Diallo, Gayo and Yahia, Sadok Ben},
title = {On the Composition of Large Biomedical Ontologies Alignment},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102284},
doi = {10.1145/3102254.3102284},
abstract = {Ontology alignment process is seen as a key mechanism in order to reduce heterogeneity and linking diverse data masses and ontologies arising in the semantic Web. In such a large infrastructures and environments, it is inconceivable to assume that all ontologies dealing with a particular knowledge domain are aligned in pairs. Moreover, the high performance of the alignment techniques is closely related to two major factors, i.e., time consumption and resource machine limitations. Indeed, good quality alignments are valuable and it would be appropriate to harness. From this statement, this paper introduces a new indirect ontology alignment method. Indeed, the proposed method treats biomedical ontologies alignments and implements a strategy of indirect ontology alignment based on a smart and efficient direct alignments composition and reuse. The core of the proposed method process relies on the alignment algebra that governs the composition of semantic relations and confidence values. Results obtained after extensive and detailed carried experiments are very encouraging and highlight many useful insights about the new proposed method.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {24},
numpages = {10},
keywords = {alignment algebra, semantic web, large biomedical ontologies, interoperability, indirect ontology alignment},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102285,
author = {Bayoudhi, Leila and Sassi, Najla and Jaziri, Wassim},
title = {OWL 2 DL Ontology Inconsistencies Prediction},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102285},
doi = {10.1145/3102254.3102285},
abstract = {When using ontology in dynamic environments, we should adapt it accordingly to follow the new requirements. Ontology should remain in a consistent state after changes. Otherwise, ontology inconsistency would be propagated to the dependent artifacts and may engender serious errors. This issue is addressed in this paper, by proposing an a priori repair action to prevent inconsistencies when updating OWL 2 DL ontologies. Predictive algorithms are defined to foresee the potential inconsistencies and to keep the ontology logically consistent, free of syntactical invalidities and style issues after each change.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {25},
numpages = {8},
keywords = {inconsistency prevention, predictive algorithms, OWL 2 DL ontology},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102270,
author = {Cameranesi, Marco and Diamantini, Claudia and Genga, Laura and Potena, Domenico},
title = {Students' Careers Analysis: A Process Mining Approach},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102270},
doi = {10.1145/3102254.3102270},
abstract = {University degrees are typically organized in courses with prerequisites among them. If prerequisite are not mandatory, students are left free to attend courses and take exams in almost any order. While favoring flexible organization of the work by students, this practice can also lead to unstructured learning practices and to performance issues. In this paper we propose to take a process-oriented view of students' careers and analyze them by process mining techniques. Results provide us with some evidence of typical patterns followed by students and of the advantages of adopting structured learning practices.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {26},
numpages = {7},
keywords = {students' career analysis, process mining, educational mining},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102288,
author = {Cerquitelli, Tania and Di Corso, Evelina and Ventura, Francesco and Chiusano, Silvia},
title = {Data Miners' Little Helper: Data Transformation Activity Cues for Cluster Analysis on Document Collections},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102288},
doi = {10.1145/3102254.3102288},
abstract = {In this paper we propose a new self-learning engine to streamline the analytics process, as it enables analysts to mine massive data repositories with minimal user intervention. In the context of cluster analysis on a collection of documents this new system, named SELF-DATA (SELF-learning DAta TrAnsformation), suggests to the analyst how to configure the whole mining process for a given dataset. SELF-DATA relies on an engine exploring different data weighting schemas (e.g., normalized term frequencies) and data transformation methods (e.g., PCA) before applying the cluster analysis, evaluating and comparing solutions through different quality indices (e.g., weighted Silhouette), and presenting the k-top solutions to the analyst. SELF-DATA will also include a knowledge base storing results of experiments on previously processed datasets, and a classification algorithm trained on the knowledge base content to forecast the best configuration for the whole mining process for an unexplored dataset. The first development of SELF-DATA running on Apache Spark has been validated on 5 collections of documents. Experimental results highlight that TF-IDF and logarithmic entropy are effective to measure item relevance with sparse datasets, and the LSI method outperforms PCA with a large dictionary.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {27},
numpages = {6},
keywords = {big data framework, data transformation method, parameter-free technique, data weighting function, text mining},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102286,
author = {Bergamaschi, Sonia and Cappelli, Andrea and Circiello, Antonio and Varone, Marco},
title = {Conditional Random Fields with Semantic Enhancement for Named-Entity Recognition},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102286},
doi = {10.1145/3102254.3102286},
abstract = {We propose a novel Named Entity Recognition (NER) system based on a machine learning technique and a semantic network. The NER system is able to exploit the advantages of semantic information, coming from Expert System proprietary technology Cogito. NER is a task of Natural Language Processing (NLP) which consists in detecting, from an unformatted text source and classify Named Entities (NE), i.e. real-world entities that can be denoted with a rigid designator. To address this problem, the chosen approach is a combination of machine learning and deep semantic processing. The machine learning method used is Conditional Random Fields (CRF).CRF is particularly suitable for the task because it analyzes an input sequence of tokens considering it as a whole, instead of one item at a time. CRF has been trained not only with classical information, available after a simple computation or anyway with little effort, but with the addition of semantic information. Semantic information is obtained with Sensigrafo and Semantic Disambiguator, which are the proprietary semantic network and semantic engine of Expert System, respectively. The results are encouraging, as we can experimentally prove the improvements in the NER task obtained by exploiting semantics, in particular when the training data size decreases.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {28},
numpages = {7},
keywords = {named entity recognition, NLP, cogito, CRF},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102267,
author = {Francolino, Vincenzo},
title = {Application of Computer-Supported Content Analysis on Twitter: A Case Study on Swiss National Media Criticism},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102267},
doi = {10.1145/3102254.3102267},
abstract = {Media and media content are of considerable social relevance. However, the systematic and automated extraction of media content is hardly a subject in the communication and media studies. This also applies to the potential use of computer-assisted content analysis as an alternative or complement to manual coding in a media science context. My project aims to address this research gap and reduce labor-intensive processes by using machine learning methods and pooled computing resources to automatically extract topic related data from Twitter. As research topic the project intents to analyze the status quo of Swiss national media-criticism. This project combines the two research fields computer linguistics applied to media studies and examines the extent to which an automated process using a Na\"{\i}ve-Bayes algorithm can adequately identify content containing Swiss national media-criticism. To answer this question the results are validated by means of manual content analysis.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {29},
numpages = {3},
keywords = {machine learning, web intelligence, na\"{\i}ve-bayes, domain knowledge, media criticism, computer-aided content analysis, twitter, tweets, media analytics, classifier, algorithm},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102269,
author = {Gitis, V. G. and Derendyaev, A. B. and Pirogov, S. A. and Spokoiny, V. G. and Yurkov, E. F.},
title = {Earthquake Prediction Using the Fields Estimated by an Adaptive Algorithm},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102269},
doi = {10.1145/3102254.3102269},
abstract = {We suggest a new approach to the estimation of the parameters of in-homogeneous spatio-temporal marked point fields. It is based on the method of adaptive weights smoothing (AWS). We construct a generalized version of AWS algorithm for calculating spatial and spatio-temporal fields of density, mean values and the correlation (fractal) dimension. The algorithm is used to evaluate the seismic process parameter fields from earthquake catalogues. We use these estimates to predict strong earthquakes. We compare the results of forecasting based on the generalized AWS algorithm and on the kernel smoothing procedure. The AWS-based forecasting was observed to surpass the forecasting using the kernel estimates.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {30},
numpages = {8},
keywords = {marked point fields, earthquake prediction, adaptive weights smoothing (AWS), earthquake catalogue, GIS GeoTime 3},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102287,
author = {Aghamirkarimi, Dara and Lemire, Daniel},
title = {Discovering the Smart Forests with Virtual Reality},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102287},
doi = {10.1145/3102254.3102287},
abstract = {By analogy with the rise of the Internet of Things in cities and the emergence of "smart cities", we advocate the concept of a "smart forest". Such a constantly and deeply monitored forest can both maximize wood production and help combat climate change. However, as we work toward the acquisition of terabytes of sensor data, using inexpensive Internet-of-Things technology, we face an age-old data warehousing and Big Data challenge: how do we equip the decision makers and the scientists so that they can make sense of this deluge of data? We propose to go beyond conventional geographical information systems (GIS) and to leverage a concurrent technological trend: afordable virtual reality. We discuss upcoming challenges and opportunities.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {31},
numpages = {6},
keywords = {internet of things, big data, virtual reality},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102290,
author = {Anelli, Vito W. and Bellini, Vito and Cal\'{\i}, Andrea and De Santis, Giuseppe and di Noia, Tommaso and di Sciascio, Eugenio},
title = {Querying Deep Web Data Sources as Linked Data},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102290},
doi = {10.1145/3102254.3102290},
abstract = {The Deep Web is constituted by dynamically generated pages, usually requested through HTML forms; it is notoriously difficult to query and to search, as its pages are obviously non-indexable. Recently, Deep Web data have been made accessible through RESTful services that return information usually structured in JSON or XML format. We propose techniques to make the Deep Web available in the Linked Data Cloud, and we study algorithms for processing queries posed in a transparent way on the Linked Data, providing answers based on the underlying Deep Web sources. We present a software prototype that exposes RESTful services as Linked Data datasets thus allowing a smoother semantic integration of different structured information sources in a global data and knowledge space.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {32},
numpages = {7},
keywords = {linked data, RESTful services, SPARQL, deep web},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102291,
author = {Cal\`{\i}, Andrea and Straccia, Umberto},
title = {Integration of Deep Web Sources: A Distributed Information Retrieval Approach},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102291},
doi = {10.1145/3102254.3102291},
abstract = {The Deep Web consists of those structured data that are available as dynamically generated pages, typically requested through HTML forms. Deep Web pages cannot be indexed by search engines, and are notoriously difficult to query and integrate due to the limited access that they offer. We propose a novel framework for integrating Deep Web sources by means of a mediated schema that represent the underlying, distributed sources. Our goal is to compute answers to queries posed on the mediated schema. To this aim, we propose the use of techniques from the area of Distributed Information Retrieval. We discuss a novel approach to automated sampling, size estimation and selection of Deep Web sources, as well as a technique for merging result lists.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {33},
numpages = {4},
keywords = {integration of deep web sources: a distributed information retrieval approach},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102281,
author = {Jamil, Hasan M. and Rivero, Carlos R.},
title = {A Novel Model for Distributed Big Data Service Composition Using Stratified Functional Graph Matching},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102281},
doi = {10.1145/3102254.3102281},
abstract = {A significant number of current industrial applications rely on web services. A cornerstone task in these applications is discovering a suitable service that meets the threshold of some user needs. Then, those services can be composed to perform specific functionalities. We argue that the prevailing approach to compose services based on the "all or nothing" paradigm is limiting and leads to exceedingly high rejection of potentially suitable services. Furthermore, contemporary models do not allow "mix and match" composition from atomic services of different composite services when binary matching is not possible or desired. In this paper, we propose a new model for service composition based on "stratified graph summarization" and "service stitching". We discuss the limitations of existing approaches with a motivating example, present our approach to overcome these limitations, and outline a possible architecture for service composition from atomic services. Our thesis is that, with the advent of Big Data, our approach will reduce latency in service discovery, and will improve efficiency and accuracy of matchmaking and composition of services.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {34},
numpages = {8},
keywords = {graph summarization, graph matching, web services, service stitching, semantic graph matching, service discovery},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3102254.3102271,
author = {Cuzzocrea, Alfredo and Loia, Vincenzo and Tommasetti, Aurelio},
title = {Big-Data-Driven Innovation for Enterprises: Innovative Big Value Paradigms for next-Generation Digital Ecosystems},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102271},
doi = {10.1145/3102254.3102271},
abstract = {Among the various interpretations and meanings of the well-known Vs (Volume, Velocity, Variety) of Big Data, V as Value represents the most significant and critical innovation for enterprises, which are a well-known case of digital ecosystems. The key issue for big enterprise data consists in extracting knowledge for creating new value and innovation for the target enterprise. Therefore, the big data analytics phase plays a critical role to this end. Following these considerations, in this paper we provide the following three contributions: (i) an overview of most relevant proposals in the context of big data innovation for enterprises; (ii) a reference architecture for supporting advanced big data analytics over big enterprise data; (iii) a discussion on future challenges in the context of big data innovation for enterprises.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {35},
numpages = {5},
keywords = {big data analytics, big value, big data, big data prediction, big enterprise data, digital ecosystems},
location = {Amantea, Italy},
series = {WIMS '17}
}

