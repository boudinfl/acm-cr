@inproceedings{10.1145/2912845.2912853,
author = {Su, Xiang and Gilman, Ekaterina and Wetz, Peter and Riekki, Jukka and Zuo, Yifei and Lepp\"{a}nen, Teemu},
title = {Stream Reasoning for the Internet of Things: Challenges and Gap Analysis},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912853},
doi = {10.1145/2912845.2912853},
abstract = {The Internet of Things (IoT) is not only about interconnecting embedded devices to the Internet, but also about providing knowledge on such devices and what they sense from the physical world. One focus of IoT is put on extracting actionable knowledge and providing value-added services by means of reasoning techniques. Stream reasoning techniques offer a promising solution for processing dynamic, heterogeneous, and volume data for IoT. In this article, we identify the challenges for utilizing stream reasoners from the IoT point of view, review the landscape of stream reasoning techniques, and examine their capabilities to meet the challenges of IoT. Moreover, we present an experimental IoT system implementing stream reasoning and perform a gap analysis to evaluate stream reasoners. Finally, based on the analysis, we suggest several recommendations for future development of stream reasoners in order to overcome the identified gaps.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {1},
numpages = {10},
keywords = {dynamics, stream reasoning, Semantic Web, gap analysis},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912856,
author = {Hajj-Hassan, H. and Arnaud, N. and Castelltort, A. and Drapeau, L. and Laurent, A. and Lobry, O. and Khater, C.},
title = {Multimapping Design of Complex Sensor Data in Environmental Observatories},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912856},
doi = {10.1145/2912845.2912856},
abstract = {Environmental resources (e.g., air quality, water quantity) are needed to understand fundamental questions such as global change. Such resources are often collected from sensors, including humans acting as sensors. Tools have emerged to manage such data in the form of time series and, in particular, the Sensor Observation Service (SOS) which offers a framework based on predefined relational database schema. Environmental observatories can be built using such frameworks, allowing to address specific key scientific questions by collecting and sharing large-scale environmental data. However, the strict schema of SOS database makes it difficult to integrate some data that cannot be directly mapped to the schema. Guidelines and best practices are offered in the literature in order to reuse standards from the Semantic Web but they do not cover all needs. In particular, they do not help to reflect the fact that a single environmental database can lead to several SOS models. Since being aware of these multiple possibilities is crucial for a better use of the observatories, we argue that some extensions of the existing works are required. In this paper, we thus propose an extension of existing vocabularies to achieve this goal. Our contribution is illustrated on the real case of the Lebanese-French O-LiFE environmental observatory.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {2},
numpages = {10},
keywords = {O-LiFE, Environmental Data, SSN Ontology, Observatory, Sensor Observation Service, Multiple mapping},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912852,
author = {Viana, Paula and Soares, M\'{a}rcio},
title = {A Hybrid Recommendation System for News in a Mobile Environment},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912852},
doi = {10.1145/2912845.2912852},
abstract = {Over the last few years consumption of news articles has shifted more and more from the written versions towards the web. Mobile devices, which became more powerful, with larger screens and connected to the Internet, have had a great influence on this paradigm change. A critical problem associated to online news is related to the fact that the large number of daily articles can be overwhelming to the users. Recommendation services can largely improve the efficiency and accuracy of acquired information. These systems are designed to filter critical news, key events and meaningful items that might be of interest to a reader.In this paper, a news recommendation system in a mobility scenario is presented. The implemented recommendation system combines content-based and georeferenced recommendation techniques. Recommendations are supported by short-term and long-term user profiles created implicitly and considering also the mobile device geolocation. The final recommendation list is obtained by combining recommendations provided by the different recommendation approaches. To evaluate the performance of the solution, a user study was conducted. Results indicate that the quality of the recommendations is acknowledged by the test users. The system was integrated in a mobile application of a Portuguese newspaper (P\'{u}blico) in the context of the project Pglobal.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {3},
numpages = {9},
keywords = {user profiles, hybrid recommendation system, mobile application, geolocation, Online news},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912862,
author = {Othman, Rania and Belkaroui, Rami and Faiz, Rim},
title = {Customer Opinion Summarization Based on Twitter Conversations},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912862},
doi = {10.1145/2912845.2912862},
abstract = {As Twitter gains popularity, millions of messages are generated every day via this platform allowing people to communicate with each other through daily chatter and public conversations. This social content is usually considered more subjective than professional articles involving a lot of opinions and thoughts. This has stimulated many companies to use tweets to keep track and have general overview on their customer opinions about the brand. The conversational element of Twitter is of particular interest to the marketing community. However, most studies on summarizing customer opinions through Twitter, have so far employed single tweets rather than considering the whole conversations. As tweets are limited to 140 characters and usually written in an informal way, it is frequently hard to detect the exact meaning of the tweet when taken separately. In this paper, we propose a new approach for customer opinion summarization based on Twitter conversations named COSTwiC. We employ a new conversation-based method that employs conversation interactions to effectively extract the different product features as well as the polarity of the conversation messages. Experimentations show promising results. In particular, we have proved that incorporating conversation structure in the opinion summarization task contributes to improving system performance.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {4},
numpages = {10},
keywords = {Twitter, Conversations, User interactions, Opinion summarization},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912854,
author = {Andersen, Per-Arne and Kr\r{a}kevik, Christian and Goodwin, Morten and Yazidi, Anis},
title = {Adaptive Task Assignment in Online Learning Environments},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912854},
doi = {10.1145/2912845.2912854},
abstract = {With the increasing popularity of online learning, intelligent tutoring systems are regaining increased attention. In this paper, we introduce adaptive algorithms for personalized assignment of learning tasks to student so that to improve his performance in online learning environments. As main contribution of this paper, we propose a a novel Skill-Based Task Selector (SBTS) algorithm which is able to approximate a student's skill level based on his performance and consequently suggest adequate assignments. The SBTS is inspired by the class of multi-armed bandit algorithms. However, in contrast to standard multi-armed bandit approaches, the SBTS aims at acquiring two criteria related to student learning, namely: which topics should the student work on, and what level of difficulty should the task be. The SBTS centers on innovative reward and punishment schemes in a task and skill matrix based on the student behaviour.To verify the algorithm, the complex student behaviour is modelled using a neighbour node selection approach based on empirical estimations of a students learning curve. The algorithm is evaluated with a practical scenario from a basic java programming course. The SBTS is able to quickly and accurately adapt to the composite student competency --- even with a multitude of student models.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {5},
numpages = {10},
keywords = {Intelligent Tutoring System, Online Learning, Adaptive Learning},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2936809,
author = {Gunes, Omer},
title = {Aspect Term and Opinion Target Extraction from Web Product Reviews Using Semi-Markov Conditional Random Fields with Word Embeddings as Features},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2936809},
doi = {10.1145/2912845.2936809},
abstract = {Descriptions and reviews for products abound on the web and characterise the corresponding products through their aspects. Extracting these aspects is essential to better understand these descriptions, e.g., for comparing or recommending products. Current pattern-based aspect extraction approaches focus on flat patterns extracting flat sets of adjective-noun pairs. Aspects also have crucial importance on sentiment classification in which sentiments are matched with aspect-level expressions. A preliminary step in both aspect extraction and aspect based sentiment analysis is to detect aspect terms and opinion targets. In this paper, we propose a sequential learning approach to extract aspect terms and opinion targets from opinionated documents. For the first time, we use semi-markov conditional random fields for this task and we incorporate word embeddings as features into the learning process. We get comparative results on the benchmark datasets for the subtask of aspect term extraction in SemEval-2014 Task 4 and the subtask of opinion target extraction in SemEval-2015 Task 12. Our results show that word embeddings improve the detection accuracy for aspect terms and opinion targets.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {6},
numpages = {5},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912850,
author = {Stanchev, Lubomir},
title = {Creating a Probabilistic Graph for WordNet Using Markov Logic Network},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912850},
doi = {10.1145/2912845.2912850},
abstract = {The paper shows how to create a probabilistic graph for WordNet. A node is created for every word and phrase in WordNet. An edge between two nodes is labeled with the probability that a user that is interested in the source concept will also be interested in the destination concept. For example, an edge with weight 0.3 between "canine" and "dog" indicates that there is a 30% probability that a user who searches for "canine" will be interested in results that contain the word "dog". We refer to the graph as probabilistic because we enforce the constraint that the sum of the weights of all the edges that go out of a node add up to one. Structural (e.g., the word "canine" is a hypernym (i.e., kind of) of the word "dog") and textual (e.g., the word "canine" appears in the textual definition of the word "dog") data from WordNet is used to create a Markov logic network, that is, a set of first order formulas with probabilities. The Markov logic network is then used to compute the weights of the edges in the probabilistic graph. We experimentally validate the quality of the data in the probabilistic graph on two independent benchmarks: Miller and Charles and WordSimilarity-353.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {7},
numpages = {12},
keywords = {Probabilistic Graph, Semantic Search, Markov Logic Network, WordNet},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912855,
author = {Berrahou, Soumia Lilia and Buche, Patrice and Dibie, Juliette and Roche, Mathieu},
title = {Xart System: Discovering and Extracting Correlated Arguments of n-Ary Relations from Text},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912855},
doi = {10.1145/2912845.2912855},
abstract = {In this paper, we present Xart system based on a hybrid method using data mining approaches and syntactic analysis to automatically discover and extract relevant information modeled as n-ary relations from text. A n-ary relation links a studied object with its features considered as several arguments. Our work focuses on extracting quantitative arguments associated with their attributes, i.e. a numerical value and a unit of measure, in order to populate a domain Ontological and Terminological Resource (OTR) with new instances. The proposed method relies on the discovery of correlated arguments in text using sequential pattern mining and the OTR. Then, those Ontological Sequential Patterns (OSP) are enriched with specific syntactic relations in order to construct Ontological Linguistic Sequential Patterns (OLSP) where the arguments are expressed according to different levels of term abstraction (term, grammatical category and concept). We have made concluding experiments on a corpus from food packaging domain where relevant data to be extracted are experimental results on packagings. We have been able to extract up to 4 correlated arguments with a F-measure from 0.6 to 0.8.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {8},
numpages = {12},
keywords = {Data mining, Sequential pattern, N-ary relation, Information extraction, Syntactic analysis, Ontology, Lingusitic pattern, Argument, Quantitative data},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912868,
author = {Lee, Kyungyup Daniel and Han, Kyungah and Myaeng, Sung-Hyon},
title = {Capturing Word Choice Patterns with LDA for Fake Review Detection in Sentiment Analysis},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912868},
doi = {10.1145/2912845.2912868},
abstract = {The usefulness of user-generated online reviews is hampered by fake reviews, often produced by clandestinely sponsored reviewers. Detecting fake reviews is a difficult task even for laypeople, and this has also been the case for previous automatic detection approaches, which have only had a limited success. Earlier studies showed that people who tell lies or write deceptive reviews tend to select words unnaturally. We propose a novel approach to detecting fake reviews by applying a topic modeling method based on Latent Dirichlet Allocation (LDA). A unique contribution of this paper is to explicate some latent aspects of fake and truthful reviews by means of "topics" that are not necessarily subject areas but related to the word choice patterns reflecting behavioral and linguistic characteristics of the fake review writers. We constructed a labeled dataset based on Yelp and demonstrated that the proposed approach helps identifying unique aspects of fake and truthful reviews, which has a potential to improving the performance of the fake review detection task. The experimental result shows that our proposed method yields better performance than that of state-of-the-art methods for small size categories in our dataset.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {9},
numpages = {7},
keywords = {topic modeling, Sentiment analysis, word choice patterns, fake review detection, LDA},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912873,
author = {Jean, Pierre-Antoine and Harispe, S\'{e}bastien and Ranwez, Sylvie and Bellot, Patrice and Montmain, Jacky},
title = {Uncertainty Detection in Natural Language: A Probabilistic Model},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912873},
doi = {10.1145/2912845.2912873},
abstract = {Designing approaches able to automatically detect uncertain expressions within natural language is central to design efficient models based on text analysis, in particular in domains such as question-answering, approximate reasoning, knowledge-based population. This article proposes an overview of several contributions and classifications defining the concept of uncertainty expressions in natural language, and the related detection methods that have been proposed so far. A new supervised and generic approach is next introduced for this specific task; it is based on the statistical analysis of multiple lexical and syntactic features used to characterize sentences through vector-based representations that can be analyzed by proven classification methods. The global performance of our approach is demonstrated and discussed with regard to various dimensions of uncertainty and text specificities.This method is available for download at https://github.com/pajean/uncertaintyDetection.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {10},
numpages = {10},
keywords = {Uncertainty detection, Binary classification, Supervised model},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912870,
author = {Surroca, Guillaume and Lemoisson, Philippe and Jonquet, Clement and Cerri, Stefano A.},
title = {Subjective and Generic Distance in ViewpointS: An Experiment on WordNet},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912870},
doi = {10.1145/2912845.2912870},
abstract = {We first briefly recall the ViewpointS knowledge representation formalism and discuss the genericity it enables in terms of semantic distance computation. ViewpointS enables representation and storage of individual viewpoints in a shared knowledge graph. Knowledge providers (i.e., agents) express their individual opinions by emitting viewpoints on the semantic similarity or proximity between resources of the knowledge graph which can either be agents, documents (i.e., knowledge supports) or concepts (i.e., descriptors). In this paper, we benchmark the ViewpointS approach against other classic semantic distances (graph based or information content based) on a WordNet experiment. Our goal is to demonstrate the value of keeping the subjectivity of the represented knowledge, while having a generic approach that can handle any kind of knowledge and compute similarity between any kinds of objects.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {6},
keywords = {Subjective Knowledge Representation, Knowledge Graphs, Semantic similarity, Semantic distances, WordNet},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912846,
author = {Watanabe, Toyohide and Kanamaru, Masanori},
title = {Knowledge Acquisition/Utilization Model and PDC Framework for Managing Contexts in Research Activity},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912846},
doi = {10.1145/2912845.2912846},
abstract = {The research activity relates directly to knowledge-oriented work: interpretation of current working situation with the existing knowledge; reproduction/refinement of knowledge derived from the existing knowledge, reconstruction of the existing knowledge with newly-interpreted/newly-added knowledge, etc. In this paper, we address the knowledge acquisition/utilization process in research activity and design KAU model in knowledge acquisition/utilization process. In addition, we propose a PDC framework to promote effectively the working process under our KAU model. Our PDC framework can explain the stepwise stage structure for knowledge acquisition/utilization with the activity context. We show that our framework is effective and the usability of prototype system are successful through the experiments, as the concluding remark in this paper.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {12},
numpages = {12},
keywords = {knowledge management, activity network, activity context, PDC model, Knowledge acquisition/utilization, knowledge network},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912847,
author = {Annane, Amina and Emonet, Vincent and Azouaou, Fai\c{c}al and Jonquet, Clement},
title = {Multilingual Mapping Reconciliation between English-French Biomedical Ontologies},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912847},
doi = {10.1145/2912845.2912847},
abstract = {Even if multilingual ontologies are now more common, for historical reasons, in the biomedical domain, many ontologies or terminologies have been translated from one natural language to another resulting in two potentially aligned ontologies but with their own specificity (e.g., format, developers, and versions). Most often, there is no formal representation of the translation links between translated ontologies and original ones and those mappings are not formally available as linked data. However, these mappings are very important for the interoperability and the integration of multilingual biomedical data. In this paper, we propose an approach to represent translation mappings between ontologies based on the NCBO BioPortal format. We have reconciled more than 228K mappings between ten English ontologies hosted on NCBO BioPortal and their French translations. Then, we have stored both the translated ontologies and mappings on a French customized version of the platform, called the SIFR BioPortal, making the whole thing available in RDF. Reconciling the mappings turned more complex than expected because the translations are rarely exactly the same than the original ontologies as discussed in this paper.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {13},
numpages = {12},
keywords = {ontology localization, semantic web, biomedical ontologies, linked data, mapping reconciliation, ontology alignment, multilingual mapping, BioPortal, Ontology repository},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912861,
author = {Melo, Andr\'{e} and Paulheim, Heiko and V\"{o}lker, Johanna},
title = {Type Prediction in RDF Knowledge Bases Using Hierarchical Multilabel Classification},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912861},
doi = {10.1145/2912845.2912861},
abstract = {Large Semantic Web knowledge bases are often noisy, incorrect, and incomplete with respect to type information. Automatic type prediction can help reduce such incompleteness, and, as previous works show, statistical methods are well-suited for this kind of data. Since most Semantic Web knowledge bases come with an ontology defining a type hierarchy, in this paper, we rephrase the type prediction problem as a hierarchical multilabel classification problem. We propose SLCN, a modification of the local classifier per node approach, which performs feature selection, instance sampling, and class balancing for each local classifier. Our approach improves scalability, facilitating its application on large Semantic Web datasets with high-dimensional feature and label spaces. We compare the performance of our proposed method with a state-of-the-art type prediction approach and popular hierarchical multilabel classifiers, and report on experiments with large-scale RDF datasets.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {10},
keywords = {Hierarchical Multilabel Classification, Type Prediction, Knowledge Base},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912848,
author = {Beretta, Valentina and Harispe, S\'{e}bastien and Ranwez, Sylvie and Mougenot, Isabelle},
title = {How Can Ontologies Give You Clue for Truth-Discovery? An Exploratory Study},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912848},
doi = {10.1145/2912845.2912848},
abstract = {The main aim of truth-finding methods is to identify the most reliable and trustworthy data among a set of facts. Since existing methods assume a single true value, they cannot deal with numerous real-world use cases in which a set of true values exists for a given fact, even for functional predicate (e.g. Picasso is born in M\`{a}laga and in Spain). This paper studies how traditional truth-finding methods can be adapted to this setting. After introducing a new definition of true value and discussing associated implications, we propose an approach that can be used to identify true values among a set of non-conflicting claims; it takes advantage of belief functions to incorporate knowledge about value relationships in the form of a partial ordering of claimed values. By reducing the error rate up to 30% adapting classical approaches, the effectiveness and suitability of our proposal is clearly highlighted through empirical experiments performed on DBpedia.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {15},
numpages = {12},
keywords = {Belief Functions, Ontology, Source Trustworthiness, Truth Discovery},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912866,
author = {Kim, Jinho and Myaeng, Sung-Hyon},
title = {Discovering Relations to Augment a Web-Scale Knowledge Base Constructed from the Web},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912866},
doi = {10.1145/2912845.2912866},
abstract = {We propose a method for automatically discovering new knowledge from a knowledge base consisting of entity-relation-entity triples, constructed automatically from the Web. This method infers new relations between entity pairs by exploiting the structure of massive graphs, which can be obtained by linking existing entity-relation-entity triples. After identifying an unlinked entity pair likely to have a strong association from the graphs and a connected component around it, the method learns a new relation name from a similar structure. More precisely, it retrieves other connected components containing a similar structure and finds a new relation between the entities. The process can be considered a simple form of analogical reasoning, followed by identifying an additional relation in the searched connected component in the knowledge base. We used two datasets to evaluate the performance of this method. The ReVerb dataset, a large-scale knowledge base constructed from an Open Information Extraction method, was used to demonstrate its scalability and accuracy in comparison with SHERLOCK and other baselines. Our evaluation with the YAGO dataset showed the proposed method is applicable to other types of knowledge bases that are not from the Open Information Extraction paradigm.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {16},
numpages = {12},
keywords = {Open Knowledge Acquisition, Knowledge base expansion, Implicit relation Inference, Open Information Extraction},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912867,
author = {Chortaras, Alexandros and Grau, Bernardo Cuenca and Stamou, Giorgos and Stoilos, Giorgos},
title = {Reformulating Ontological Queries Using Materialised Rewritings},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912867},
doi = {10.1145/2912845.2912867},
abstract = {Query rewriting is a prominent technique in ontology-based data access (OBDA) applications. Roughly speaking, a rewriting of a query Q w.r.t. an ontology is another query Q' that can be directly evaluated over the data without further reference to the input ontology. In this paper, we observe that many OBDA applications could significantly benefit from precomputing rewritings for certain queries. For example, in query optimisation, materialised rewritings of frequently asked queries can be used to speed up the query reformulation process. Moreover, in systems where users have different levels of access to information, materialised rewritings for the views assigned to each user can be exploited to obtain the set of answers to the input query derivable from the assigned views. Consequently, we investigate the problem of reformulating a query given a set of materialised rewritings and present a practical algorithm. Subsequently, we use our approach to design a fully fledged query rewriting algorithm which can exploit materialised rewritings to speed up the rewriting process. Our experimental results confirm the potential of our technique in practice.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {8},
keywords = {OWL, Semantic Web, Description Logics, Ontologies, Materialised Rewritings},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912859,
author = {Lynden, Steven and Yui, Makoto and Matono, Akiyoshi and Nakamura, Akihito and Ogawa, Hirotaka and Kojima, Isao},
title = {Optimising Coverage, Freshness and Diversity in Live Exploration-Based Linked Data Queries},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912859},
doi = {10.1145/2912845.2912859},
abstract = {Centralised indexes and distributed query federation-based approaches towards executing queries over distributed Linked Open Data are currently limited when it comes to providing complete coverage and up-to-date results. However, live exploration-based query execution, in accordance with the Linked Open Data publishing principles, dereferences Internationalised Resource Identifiers (IRI)s on the fly in order to provide results from Linked Data anywhere on the Web. We propose and investigate similarity search-based strategies for dereferencing IRIs during live exploration-based querying in order to maximise user criteria of coverage, freshness and diversity within a limited execution time, in contrast to existing approaches which may provide complete results but within response times that are too high to be useful within many practical applications. Results are presented from a set of sample queries comparing the IRI selection strategies with existing approaches showing that coverage, freshness and diversity can be improved by up to 30%.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {18},
numpages = {12},
keywords = {Database technologies for Linked Data, Searching and Ranking Linked Data},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912857,
author = {Thakkar, Harsh and Endris, Kemele M. and Gimenez-Garcia, Jose M. and Debattista, Jeremy and Lange, Christoph and Auer, S\"{o}ren},
title = {Are Linked Datasets Fit for Open-Domain Question Answering? A Quality Assessment},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912857},
doi = {10.1145/2912845.2912857},
abstract = {The current decade is a witness to an enormous explosion of data being published on the Web as Linked Data to maximise its reusability. Answering questions that users speak or write in natural language is an increasingly popular application scenario for Web Data, especially when the domain of the questions is not limited to a domain where dedicated curated datasets exist, like in medicine. The increasing use of Web Data in this and other settings has highlighted the importance of assessing its quality. While quite some work has been done with regard to assessing the quality of Linked Data, only few efforts have been dedicated to quality assessment of linked data from the question answering domain's perspective. From the linked data quality metrics that have so far been well documented in the literature, we have identified those that are most relevant for QA. We apply these quality metrics, implemented in the Luzzu framework, to subsets of two datasets of crucial importance to open domain QA -- DBpedia and Wikidata -- and thus present the first assessment of the quality of these datasets for QA. From these datasets, we assess slices covering the specific domains of restaurants, politicians, films and soccer players. The results of our experiments suggest that for most of these domains, the quality of Wikidata with regard to the majority of relevant metrics is higher than that of DBpedia.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {19},
numpages = {12},
keywords = {Data Quality, Wikidata, DBpedia, Linked Open Data, Quality assessment, Question Answering},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912864,
author = {Diefenbach, Dennis and Usbeck, Ricardo and Singh, Kamal Deep and Maret, Pierre},
title = {A Scalable Approach for Computing Semantic Relatedness Using Semantic Web Data},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912864},
doi = {10.1145/2912845.2912864},
abstract = {Computing semantic relatedness is an essential operation for many natural language processing (NLP) tasks, such as Entity Linking (EL) and Question Answering (QA). It is still challenging to find a scalable approach to compute the semantic relatedness using Semantic Web data. Hence, we present for the first time an approach to pre-compute the semantic relatedness between the instances, relations, and classes of an ontology, such that they can be used in real-time applications.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {20},
numpages = {9},
keywords = {Scalability, Semantic Web, Semantic Relatedness},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912872,
author = {Chifu, Adrian-Gabriel and Fournier, S\'{e}bastien},
title = {SegChain: Towards a Generic Automatic Video Segmentation Framework, Based on Lexical Chains of Audio Transcriptions},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912872},
doi = {10.1145/2912845.2912872},
abstract = {With the advances in multimedia broadcasting through a rich variety of channels and with the vulgarization of video production, it becomes essential to be able to provide reliable means of retrieving information within videos, not only the videos themselves. Research in this area has been widely focused on the context of TV news broadcasts, for which the structure itself provides clues for story segmentation. The systematic employment of these clues would lead to thematically driven systems that would not be easily adaptable in the case of videos of other types. The systems are therefore dependent on the type of videos for which they have been designed. In this paper we aim at introducing SegChain, a generic unsupervised framework for story segmentation, based on lexical chains from transcriptions. SegChain takes into account the topic changes by perceiving the fluctuations of the most frequent terms throughout the video.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {21},
numpages = {8},
keywords = {Transcriptions, Video retrieval, Lexical chains, Story segmentation},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912863,
author = {Violos, John and Tserpes, Konstantinos and Psomakelis, Evangelos and Psychas, Konstantinos and Varvarigou, Theodora},
title = {Sentiment Analysis Using Word-Graphs},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912863},
doi = {10.1145/2912845.2912863},
abstract = {The Word-Graph Sentiment Analysis Method is proposed to identify the sentiment that expressed in a microblog document using the sequence of the words that contains. The sequence of the words can be represented using graphs in which graph similarity metrics and classification algorithms can be applied to produce sentiment predictions. Experiments that were carried out with this method in a Twitter dataset validate the proposed model and allow us to further understand the metrics and the criteria that can be applied in words-graphs to predict the sentiment disposition of short, microblog documents.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {22},
numpages = {9},
keywords = {Sentiment analysis, vector classification, graph similarity metrics, word graph representation model},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912860,
author = {Cortez, Paulo and Oliveira, Nuno and Ferreira, Jo\~{a}o Peixoto},
title = {Measuring User Influence in Financial Microblogs: Experiments Using StockTwits Data},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912860},
doi = {10.1145/2912845.2912860},
abstract = {In this paper, we study the effect of graph structure user influence measures in financial social media. In particular, we explore rich and recent data, composed of 1.2 million StockTwits messages, from June 2010 to March 2013. These data allow the creation of social network graphs by considering direct active interactions (retweets, shares or replies). Using such graphs and a realistic rolling windows evaluation, we analyzed four user influence measures (indegree, betweenness, page rank and posts) under two criteria: Percentage of Quality Users (PQU), as manually labeled by StockTwits; and the daily sentiment correlation between top lists of influential users and other users. The sentiment was based on a StockTwits labeled dataset and assessed in terms of three selections: overall sentiment (ALL) and filtered by two major technological companies (Apple -- AAPL and Google -- GOOG).Promising results were obtained, with several top lists presenting PQU values higher than 80% and correlations higher than 0.6. Overall, the best results were achieved by the page rank and posts measures.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {23},
numpages = {10},
keywords = {User Influence, Microblogging Data, Stock Markets, Sentiment Analysis, Social Networks},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912874,
author = {Bouchlaghem, Rihab and Elkhelifi, Aymen and Faiz, Rim},
title = {A Machine Learning Approach For Classifying Sentiments in Arabic Tweets},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912874},
doi = {10.1145/2912845.2912874},
abstract = {Nowadays, sentiment analysis methods become more and more popular especially with the proliferation of social media platform users number. In the same context, this paper presents a sentiment analysis approach which can faithfully translate the sentimental orientation of Arabic Twitter posts, based on a novel data representation and machine learning techniques. The proposed approach applied a wide range of features: lexical, surface-form, syntactic, etc. We also made use of lexicon features inferred from two Arabic sentiment words lexicons. To build our supervised sentiment analysis system, we use several standard classification methods (Support Vector Machines, K-Nearest Neighbour, Na\"{\i}ve Bayes, Decision Trees, Random Forest) known by their effectiveness over such classification issues.In our study, Support Vector Machines classifier outperforms other supervised algorithms in Arabic Twitter sentiment analysis. Via an ablation experiments, we show the positive impact of lexicon based features on providing higher prediction performance.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {24},
numpages = {6},
keywords = {Modern Standard Arabic, Sentiment analysis, Twitter, Arabic sentiment lexicon, Supervised classification},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912871,
author = {Planti\'{e}, Michel and Niang, Mouhamadou},
title = {Complementarity of Persons Sharing Properties in Social Networks},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912871},
doi = {10.1145/2912845.2912871},
abstract = {Our previous work focused on the unified community detection in networks of people: social networks, communities of actors, etc. shown as generally bipartite graph. In this article. We define therefore the notion of complementarity between the vertices of a bipartite graph. We use for it the concepts of entropy and mutual information. We show the usefulness of such an approach and the value of the approach by an experiment on a well known example.Complementarity in social networks is an interesting approach to identify cohesion in groups of persons. Our previous works studied a first approach of complementarity in networks represented as bipartite graphs: social networks, communities of actors, etc. In this paper we try to respond to semantic complementarity problems that arise as soon as one wishes to associate people in order to best fulfil a goal. We compare several approaches of complementarity to find the most appropriate technique. In some definitions of complementarity, the problem is viewed as close to a classical research: find transversals in hypergraphs, with however differences in final goals. To validate our approach, we apply and compare our methods on well known graphs and real data whose sizes are very different: from small graphs to very large graphs.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {25},
numpages = {6},
keywords = {Community detection, Social networks, Nash Equilibrium},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912851,
author = {Ruohonen, Jukka and Hyrynsalmi, Sami and Lepp\"{a}nen, Ville},
title = {Exploring the Use of Deprecated PHP Releases in the Wild Internet: Still a LAMP Issue?},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912851},
doi = {10.1145/2912845.2912851},
abstract = {Many web sites utilize deprecated software products that are no longer maintained by the associated software producers. This paper explores the question of whether an existing big data collection can be used to predict the likelihood of deprecated PHP releases based on different abstract components in modern web deployment stacks. Building on web intelligence, software security, and data-based industry rationales, the question is examined by focusing on the most popular domains in the contemporary web-facing Internet. Logistic regression is used for classification. Although statistical classification performance is modest, the results indicate that deprecated PHP releases are associated with Linux and other open source software components. Geographical variation is small. Besides these results, the paper contributes to the web intelligence research by evaluating the feasibility of existing big data collections for mass-scale fingerprinting.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {26},
numpages = {12},
keywords = {web crawling, release engineering, patching, cyber security},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912869,
author = {Luyen, LE Ngoc and Tireau, Anne and Venkatesan, Aravind and Neveu, Pascal and Larmande, Pierre},
title = {Development of a Knowledge System for Big Data: Case Study to Plant Phenotyping Data},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912869},
doi = {10.1145/2912845.2912869},
abstract = {In the recent years, the data deluge in many areas of scientific research brings challenges in the treatment and improvement of agricultural data. Research in bioinformatics field does not outside this trend. This paper presents some approaches aiming to solve the Big Data problem by combining the increase in semantic search capacity on existing data in the plant research laboratories. This helps us to strengthen user experiments on the data obtained in this research by infering new knowledge. To achieve this, there exist several approaches having different characteristics and using different platforms. Nevertheless, we can summarize it in two main directions: the query re-writing and data transformation to RDF graphs. In reality, we can solve the problem from origin of increasing capacity on semantic data with triplets. Thus, data transformation to RDF graphs direction was chosen to work on the practical part. However, the synchronization data in the same format is required before processing the triplets because our current data are heterogeneous. The data obtained for triplets are larger that regular triplestores could manage. So we evaluate some of them thus we can compare the benefits and drawbacks of each and choose the best system for our problem.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {27},
numpages = {9},
keywords = {NoSQL, Inference, Reasoning, Benchmark, Knowledge base, Big Data, Triplestore, xR2RML, Ontology, SPARQL},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912849,
author = {Waqar, Muhammad and Rafiei, Davood},
title = {Characterizing Users in an Online Classified Ad Network},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912849},
doi = {10.1145/2912845.2912849},
abstract = {Unlike online social networking sites (e.g. Twitter and Facebook) which are heavily used for disseminating content and sharing information between users and shopping sites (e.g. Ebay) where buyers and sellers are reviewed, the flow of information between users such as buyers and sellers in a classified ad network is very limited. Characterizing users or assigning them to some classes in one such network is challenging due to the sparsity of the data about users, the vague separation of user classes and sometimes the tendency of users to hide or misrepresent their profile information.In this paper, we study the information revealed in the ads posted to an online classified ads site and analyze the behaviour of users posting those ads; our study is conducted using data collected from Kijiji over a year. We study the problem in the context of one specific task where we seek to detect if a user posting an ad belongs to one of the two classes business and non-business, based on the ads the user has posted. We study an approach based on user profiling, where given statistics on how an ad mentions terms and features from a class profile, the affinity of an ad (and subsequently a user) to a particular class is determined. We report the effectiveness of this approach in detecting user classes solely based on the information revealed in their ads and study the impact of the profile size on the accuracy. In the absence of labeled training data, we show that a simple bootstrapping technique with only a few n-grams as a seed set can give nearly good results in terms of F-measure. We further report our experiments on characterizing the collective behavior of users in posting ads and some of the distinctive usage patterns that emerge.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {28},
numpages = {9},
keywords = {classified ads, user modeling, social networks, user profiling},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912875,
author = {Kastrati, Zenun and Yayilgan, Sule Yildirim and Hjelsvold, Rune},
title = {Automatically Enriching Domain Ontologies for Document Classification},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912875},
doi = {10.1145/2912845.2912875},
abstract = {The ontology-based document classification approach relies on the content meanings of a given domain exploited and captured using the ontologies of this particular domain. Domain ontologies consist of a set of concepts and relations which links these concepts. However, they often do not provide an in-depth coverage of concepts thereby limiting their use in some subdomain applications. Therefore, the techniques for enhancing ontologies, particularly ontology enrichment, have emerged as an essential prerequisite for ontology-based applications. In this paper, we propose a new objective metric called SEMCON to enrich the domain ontology with new terms. To achieve this, SEMCON combines semantic as well as contextual information of terms within the text documents. Experiments are conducted to demonstrate the applicability of the proposed model and the obtained results from the funding domain show that document classification achieved better performance using the enriched ontology in contrast to using the baseline ontology.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {29},
numpages = {4},
keywords = {Contextual information, Document classification, Ontology enrichment, SEMCON, Semantic information},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912877,
author = {Manon, Quintana},
title = {Inbenta Semantic Search Engine: A Search Engine Inspired by the Meaning-Text Theory},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912877},
doi = {10.1145/2912845.2912877},
abstract = {Due to the widespread digitalization of documents, the need to build sophisticated search engines that can adapt to the particular way users ask questions and provide quick and efficient access to information is becoming increasingly relevant.To cope with this reality, INBENTA has developed an intelligent search engine, called the Inbenta Semantic Search Engine (ISSE). ISSE's two main tasks are analyzing users' queries and finding the most appropriate answer to those questions in a knowledge-base. To carry out these tasks, INBENTA's software solution relies on Meaning-Text Theory, which focuses on the lexicon and semantics.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {30},
numpages = {3},
keywords = {lexical function, lexicon, Search engine, semantics, Meaning-Text Theory, Natural language processing},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912878,
author = {Albahli, Saleh and Melton, Austin},
title = {RDF Data Management: A Survey of RDBMS-Based Approaches},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912878},
doi = {10.1145/2912845.2912878},
abstract = {This paper surveys approaches and up-to-date information of RDF data management and then categorizes them into four main RDF storages. Then, the survey restricts the discussion to those methods that solve RDF data management using a RDBMS, since it gives better performance and query optimization as a result of the large quantity of work required to induce relational query efficiency and also the scalability of its storage comes into play, with respect to scalability and various characteristics of performance.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {31},
numpages = {4},
keywords = {RDF, Information Retrieval, Reasoning, RDF Data Management, NoSQL, Ontology, Data Partition, Triple Store},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912879,
author = {Akremi, Houda and Zghal, Sami and Diallo, Gayo},
title = {Modeling of Uncertainty: Fuzzification of Medical Ontology},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912879},
doi = {10.1145/2912845.2912879},
abstract = {In this paper, we propose a new method of ontology fuzzification which is able to analyzing data imperfection. In general, the constituents of an ontology are, as all data from the real world, characterized by aspects of inaccuracies and uncertainties. These imperfections of ontologies are the result of a vague and imprecise linguistic description, provided by dmain experts. They are broken down into two categories: the uncertainty, and the imprecision. Indeed, the theories of vagueness and uncertainty form the basis to support these two aspects. Thus, the purpose of our paper is to propose an approach of ontologies fuzzification which therefore takes into account these two aspects. The approach is illustrated in the medical domain.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {32},
numpages = {4},
keywords = {medical ontology, Classical ontology, fuzzification of ontologies, fuzzy ontology},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912880,
author = {Rafi, Muhammad and Sharif, Muhammad Naveed and Arshad, Waleed and Mohsin, Sheharyar and Rafay, Habibullah},
title = {Multi-Layer Semantics Based Document Clustering},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912880},
doi = {10.1145/2912845.2912880},
abstract = {Document clustering is an unsupervised machine learning method that separates a large subject heterogeneous collection (Document Base or Corpus) into smaller, more manageable subject homogeneous collections (clusters). Traditional method of document clustering uses features like words, sequence, phrases, etc. These features are independent to each other and do not cater semantics. In order to perform semantic viable clustering, we believe that the problem of document clustering has two main components: (1) to represent the document in such a form that it inherently captures semantics of the text. This may also help to reduce dimensionality of the document and (2) to define a similarity measure based on the semantic representation such that it assigns higher numerical values to document pairs which have higher semantic relationship. In this paper, we propose a representation of document, based on three distinct layers: these are lexical, syntactic and semantic layers. We believe that these three layers are essential to ensure semantics into document meta descriptor. Using these three layer's features we propose a similarity function for performing document clustering. We performed an extensive series of experiments on standard text mining data sets with external clustering evaluations like: F-Measure and Purity.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {33},
numpages = {4},
keywords = {Semantics, Text Mining, Document Clustering, Similarity Measure},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912881,
author = {Samuel, John and Rey, Christophe},
title = {Generic Web Service Wrapper for Mediation Based Data Warehousing},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912881},
doi = {10.1145/2912845.2912881},
abstract = {The growing availability of specialized web services targeting only a particular niche has resulted in the use of multiple web services by enterprises for their daily activities. It is significantly difficult for resource-crunched small enterprises to write applications to integrate with each and every dependent web service. Overcoming the missing wide scale adoption of machine readable standards (WSDL, WADL, SAWSDL), we explore the capability to integrate with numerous web services using only the basic web standards (HTTP, JSON, XML, XSD, XSLT) and declarative languages (SQL, datalog queries). In this paper, we specifically focus on the role of a generic web service wrapper to support this declarative approach to describe, query and extract enterprise data from web services.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {34},
numpages = {4},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912882,
author = {Arfaoui, Olfa and Hidri, Minyar Sassi},
title = {Ontology-Based Query Refinement for XML Information Retrieval},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912882},
doi = {10.1145/2912845.2912882},
abstract = {The characteristics of XML (eXtensible Markup Language) documents have favored the need to develop specific and flexible querying systems while taking into account the coexistence of both structural and content information. The ultimate goal of these systems is to respond to different user expectations which tend to return appropriate answers to their preferences. However, people have often insufficient knowledge about XML data structure and contents, thus frequently obtaining empty answers or having to reformulate the queries several times. To solve this problem, we propose an ontology-based query refinement model for semi-structured information retrieval. It consists in reformulating a query by adding attributes from domain ontologies extracted from XML schemas.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {35},
numpages = {4},
keywords = {Query refinement, XML, Information Retrieval, Ontology},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

@inproceedings{10.1145/2912845.2912883,
author = {Ollagnier, Ana\"{\i}s and Fournier, S\'{e}bastien and Bellot, Patrice},
title = {A Supervised Approach for Detecting Allusive Bibliographical References in Scholarly Publications},
year = {2016},
isbn = {9781450340564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2912845.2912883},
doi = {10.1145/2912845.2912883},
abstract = {Exploiting the links between content is crucial in recommendation approaches. In the case of a scientific article library, bibliographic references serve as a major link source. Among them, some are explicit references as we can find at the end of articles or books, while other references are scattered in the text or in the footnotes, according to a more or less strong implicit degree. We propose to focus on the detection of this type of references that we call allusive, in scientific articles from the field of Human and Social Sciences. To overcome the inherent difficulties raised by such reference detection, we present a method which aims at (i) identifying paragraphs that contain references via a classification process and (ii) at applying CCRFs (Cascaded Conditional Random Field) in order to detect more accurately the bibliographic entries and consequently annotate their contents.},
booktitle = {Proceedings of the 6th International Conference on Web Intelligence, Mining and Semantics},
articleno = {36},
numpages = {4},
keywords = {bibliographical references detection, cascaded conditional random field, supervised classification},
location = {N\^{\i}mes, France},
series = {WIMS '16}
}

