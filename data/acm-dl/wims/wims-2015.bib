@inproceedings{10.1145/2797115.2797119,
author = {Diakidis, Georgios and Karna, Despoina and Fasarakis-Hilliard, Dimitris and Vogiatzis, Dimitrios and Paliouras, George},
title = {Predicting the Evolution of Communities in Social Networks},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797119},
doi = {10.1145/2797115.2797119},
abstract = {We studied the predictability of community evolution in on-line social networks as a supervised learning task with sequential and non-sequential classifiers. Communities that are formed in on-line social networks as a result of user interaction evolve over time. Structural, content and contextual features as well as the previous states of a community are considered as the features that are involved in the task of community evolution. The evolution phenomena we try to predict are the continuation, shrinking, growth and dissolution. The evolution labels stem from a community tracker that provided the background truth. We have obtained interesting results on a set from Twitter.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {1},
numpages = {6},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797131,
author = {Crampes, Michel and Planti\'{e}, Michel},
title = {Overlapping Community Detection Optimization and Nash Equilibrium},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797131},
doi = {10.1145/2797115.2797131},
abstract = {Community detection in social networks is the focus of many algorithms. Recent methods aimed at optimizing the so-called modularity function proceed by maximizing relations within communities while minimizing inter-community relations. However, given the NP-completeness of the problem, these algorithms are heuristics that do not guarantee an optimum. In this paper, we introduce a new algorithm along with a function that takes an approximate solution and modifies it in order to reach an optimum. This reassignment function is considered a `potential function' and becomes a necessary condition to asserting that the computed optimum is indeed a Nash Equilibrium. We also use this function to simultaneously show two detection and visualization modes: partitioned and overlaped communities, of great value in revealing interesting features in a social network. Our approach is successfully illustrated through several experiments on either real unipartite, multipartite or directed graphs of medium and large-sized datasets.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {2},
numpages = {10},
keywords = {Social networks, Community detection, Nash Equilibrium},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797134,
author = {Belkaroui, Rami and Faiz, Rim},
title = {Towards Events Tweet Contextualization Using Social Influence Model and Users Conversations},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797134},
doi = {10.1145/2797115.2797134},
abstract = {Nowadays, microblogging sites have completely changed the manner in which people communicate and share information. They are among the most relevant source of knowledge where information is created, exchanged and transformed, as witnessed by the important number of their users and their activities during events or campaigns like the terror attack in Paris in 2015. On Twitter, users post messages (called tweets) in real time about events, natural disasters, news, etc. Tweets are short messages that do not exceed 140 characters. Due to this limitation, an individual tweet it's rarely self-content. However, user cannot effectively understand or consume information.In order, to make tweet understandable to a reader, it is therefore necessary to know their context. In fact, on Twitter, context can be derived from users interactions, content streams and friendship. Given that there are rich user interactions on Twitter. In this paper, we propose an approach for tweet contextualization task which combines different types of signals from social users interactions to provide automatically information that explains the tweet. In addition, our approach aims to help users to satisfy any contextual information need. To evaluate our approach, we construct a reference summary by asking assessors to manually select the most informative tweets as a summary. Our experimental results based on this editorial data set offers interesting results and help ensure that context summaries contain adequate correlating information with the given tweet.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {3},
numpages = {9},
keywords = {user conversations, Events Tweet contextualization, Twitter Context Tree, Tweet understanding, Tweet influence},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797117,
author = {Alaya, Nourh\`{e}ne and Yahia, Sadok Ben and Lamolle, Myriam},
title = {What Makes Ontology Reasoning so Arduous? Unveiling the Key Ontological Features},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797117},
doi = {10.1145/2797115.2797117},
abstract = {Reasoning with ontologies is one of the core fields of research in Description Logics. A variety of efficient reasoner with highly optimized algorithms have been developed to allow inference tasks on expressive ontology languages such as OWL(DL). However, reasoner reported computing times have exceeded and sometimes fall behind the expected theoretical values. From an empirical perspective, it is not yet well understood, which particular aspects in the ontology are reasoner performance degrading factors. In this paper, we conducted an investigation about state of art works that attempted to portray potential correlation between reasoner empirical behaviour and particular ontological features. These works were analysed and then broken down into categories. Further, we proposed a set of ontology features covering a broad range of structural and syntactic ontology characteristics. We claim that these features are good indicators of the ontology hardness level against reasoning tasks. In order to assess the worthiness of our proposals, we adopted a supervised machine learning approach. Features served as the bases to learn predictive models of reasoners robustness. These models was trained for 6 well known reasoners and using their evaluation results during the ORE'2014 competition. Our prediction models showed a high accuracy level which witness the effectiveness of our set of features.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {4},
numpages = {12},
keywords = {Description Logic, Supervised Machine Learning, Reasoner, Ontology Features, Ontology},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797127,
author = {Booshehri, Meisam and Luksch, Peter},
title = {An Ontology Enrichment Approach by Using DBpedia},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797127},
doi = {10.1145/2797115.2797127},
abstract = {Over the past decade, an increasing number of methods have been proposed for (semi-) automatic generation of ontology from text. However, the ontology generated by these methods usually does not meet the needs of many reasoning-based applications in different domains since most of these methods aim at generating inexpressive ontologies e.g. bare taxonomies. In this paper, a new ontology enrichment approach is proposed in which Web of Linked Data (in particular, DBpedia as one of the huge Linked Data datasets) is used as background knowledge beside text in order to recognize new ontological relations, specifically object properties, for ontology enrichment. In other words, this enrichment approach can be considered as a post-processing step for the "Relations" layer (i.e. the fifth layer) in Ontology Learning Stack, aiming at recommending new object properties to the ontology engineers enabling them to create much more expressive ontologies. This is actually a complementary approach to our recent approach towards adding Linked Data to ontology learning layers where we aimed at improving the functions associated to the "Synonyms" layer, the "Concept Formation" layer and the "Concept Hierarchy" layer of ontology learning stack. In order to evaluate the approach, a customized experimental design is introduced called the "Pseudo Gold Standard based Ontology Evaluation" in which the results obtained by a human expert are compared against those obtained automatically. Finally, the experimental results showed a satisfactory improvement in learning object properties.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {5},
numpages = {11},
keywords = {non-taxonomic relations, Ontology Enrichment, Web of Linked Data, Object Properties, Pseudo Gold Standard based Ontology Evaluation, Ontology learning from text},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797133,
author = {Santipantakis, Georgios and Kotis, Konstantinos I. and Vouros, George A.},
title = {Ontology-Based Data Integration for Event Recognition in the Maritime Domain},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797133},
doi = {10.1145/2797115.2797133},
abstract = {Recent environmental disasters at sea have highlighted the need for efficient maritime surveillance and incident management. Currently, maritime navigation technology automatically provides real time data from vessels, which together with historical data, can be processed in an integrated way to detect complex events and support decision making. Ontology-Based Data Access (OBDA) frameworks, can be employed to access data towards this effort. However the heterogeneity of data in disparate sources make data integration a challenging task. In this paper we report on our efforts to implement a scalable system for integrating data from disparate data sources by means of existing OBDA frameworks and distributed E -- SHIQ knowledge bases, towards supporting complex event recognition. We present two versions of the implemented system, and the lessons learned from this effort.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {6},
numpages = {11},
keywords = {Ontology based data access, event recognition, data integration},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797123,
author = {Karanam, Saraschandra and van Oostendorp, Herre and Sanchiz, Myl\`{e}ne and Chevalier, Aline and Chin, Jessie and Fu, Wai Tat},
title = {Modeling and Predicting Information Search Behavior},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797123},
doi = {10.1145/2797115.2797123},
abstract = {This paper looks at two limitations of cognitive models of web-navigation: first, they do not account for the entire process of information search and second, they do not account for the differences in search behavior caused by aging. To address these limitations, data from an experiment in which two types of information search tasks (simple and difficult), presented to both young and old participants was used. We found that in general difficult tasks demand significantly more time, significantly more clicks, significantly more reformulations and are answered significantly less accurately than simple tasks. Older persons inspect the search engine result pages significantly longer, produce significantly fewer reformulations with difficult tasks than younger persons, and are significantly more accurate than younger persons with simple tasks. We next used a cognitive model of web-navigation called CoLiDeS to predict which search engine result a user would choose to click. Old participants were found to click more often only on search engine results with high semantic similarity with the query. Search engine results generated by old participants were of higher semantic similarity value (computed w.r.t the query) than those generated by young participants only in the second cycle. Match between model-predicted clicks and actual user clicks was found to be significantly higher for difficult tasks compared to simple tasks. Potential improvements in enhancing the modeling and its applications are discussed.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {7},
numpages = {12},
keywords = {Cognitive Factors, Aging, Modeling, Information Search},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797122,
author = {Papadakis, Ioannis and Apostolatos, Ioannis and Apostolou, Dimitris},
title = {A LOD-Based, Query Construction and Refinement Service for Web Search Engines},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797122},
doi = {10.1145/2797115.2797122},
abstract = {Nowadays, search engines are the obvious way of finding information on the web. However, there are times when users are forced to engage themselves in long and tedious search sessions during which they have to process their initial query a number of times until they come up with results that satisfy their information needs.This paper proposes a query construction and refinement service that aids users during their engagement with a large scale web search engine. As a proof of concept, GContext is presented and accordingly evaluated as an implementation of the proposed service. GContext integrates various sources of the lod-cloud within the environment of a large scale web search engine.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {8},
numpages = {8},
keywords = {Linked Data, Synonyms, Semantic Web, Ambiguous Words, Large-scale Search Engines},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797120,
author = {Gro\ss{}mann, Benjamin and Todor, Alexandru and Paschke, Adrian},
title = {Improving Semantic Search through Entity-Based Document Ranking},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797120},
doi = {10.1145/2797115.2797120},
abstract = {Traditional keyword-based IR approaches take into account the document context only in a limited manner. In our paper we present a novel document ranking approach based on the semantic relationships between named entities. In the first step we annotate all documents with named entities from a knowledge base (for example people, places and organisations). In the next step these annotations in combination with the relationships from the knowledge base are used to rank documents in order to perform a semantic search. Documents that contain the specific named entity that was searched for as well as other strongly related entities, receive a higher ranking. The inclusion of the document context in the ranking approach achieves a higher precision in the Top-K results.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {9},
numpages = {12},
keywords = {semantic search, relation ranking, document ranking, named entity recognition, information retrieval},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797118,
author = {Ritze, Dominique and Lehmberg, Oliver and Bizer, Christian},
title = {Matching HTML Tables to DBpedia},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797118},
doi = {10.1145/2797115.2797118},
abstract = {Millions of HTML tables containing structured data can be found on the Web. With their wide coverage, these tables are potentially very useful for filling missing values and extending cross-domain knowledge bases such as DBpedia, YAGO, or the Google Knowledge Graph. As a prerequisite for being able to use table data for knowledge base extension, the HTML tables need to be matched with the knowledge base, meaning that correspondences between table rows/columns and entities/schema elements of the knowledge base need to be found. This paper presents the T2D gold standard for measuring and comparing the performance of HTML table to knowledge base matching systems. T2D consists of 8 700 schema-level and 26 100 entity-level correspondences between the WebDataCommons Web Tables Corpus and the DBpedia knowledge base. In contrast related work on HTML table to knowledge base matching, the Web Tables Corpus (147 million tables), the knowledge base, as well as the gold standard are publicly available. The gold standard is used afterward to evaluate the performance of T2K Match, an iterative matching method which combines schema and instance matching. T2K Match is designed for the use case of matching large quantities of mostly small and narrow HTML tables against large cross-domain knowledge bases. The evaluation using the T2D gold standard shows that T2K Match discovers table-to-class correspondences with a precision of 94%, row-to-entity correspondences with a precision of 90%, and column-to-property correspondences with a precision of 77%.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {10},
numpages = {6},
keywords = {tables matching, knowledge base extension, html tables},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797132,
author = {Krieger, Katrin and Schneider, Jens and Nywelt, Christian and R\"{o}sner, Dietmar},
title = {Creating Semantic Fingerprints for Web Documents},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797132},
doi = {10.1145/2797115.2797132},
abstract = {With Semantic Web technologies and Linked Data datasets we are able to not only retrieve the textual content of a document but also to automatically create formal semantic descriptions of its content. In this paper we present a Linked Data-based approach to automatically generate semantic fingerprints for Web documents. Our approach exploits the structured information in Linked Data datasets to derive an explicit semantic description of a Web resource. A two-stage evaluation of the implementation of the presented approach shows its feasibility and robustness.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {6},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797125,
author = {Belmouhcine, Abdelbadie and Benkhalifa, Mohammed},
title = {Implicit Links Based Web Page Representation for Web Page Classification},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797125},
doi = {10.1145/2797115.2797125},
abstract = {With the rapid growth of the web's size, web page classification becomes more prominent. The representation way of a web page and contextual features used for this representation have both an impact on the classification's performance. Thus, finding an adequate representation of web pages is essential for a better web page classification. In this paper, we propose a web page representation based on the structure of the implicit graph built using implicit links extracted from the query-log. In this representation, we represent web pages using their textual contents along with their neighbors as features instead of using features of their neighbors. When two or more web pages in the implicit graph share the same direct neighbors and belong to the same class ci, it is most likely that every other web page, having the same immediate neighbors, will belong to the same class ci. We propose two kinds of web page representations: Boolean Neighbor Vector (BNV) and Weighted Neighbor Vector (WNV). In BNV, we supplement the feature vector, which represents the textual content of a web page, by a Boolean vector. This vector represents the target web page's neighbors and shows whether a web page is a direct neighbor of the target web page or not. In WNV, we supplement the feature vector, which represents the textual content of a web page, by a weighted vector. This latter represents the target web page's neighbors and shows strengths of relations between the target web page and its neighbors. We conduct experiments using four classifiers: SVM (Support Vector Machine), NB (Naive Bayes), RF (Random Forest) and KNN (K-Nearest Neighbors) on two subsets of ODP (Open Directory Project). Results show that: (1) the proposed representation helps obtain better classification results when using SVM, NB, RF and KNN for both Bag of Words (BW) and 5-gram representations. (2) The performances based on BNV are better than those based on WNV.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {12},
numpages = {11},
keywords = {query-log, web page classification, implicit links, web mining, web page representation},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797130,
author = {Subramanian, Asha and Srinivasa, Srinath and Kumar, Pavan and Vignesh, S.},
title = {Semantic Integration of Structured Data Powered by Linked Open Data},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797130},
doi = {10.1145/2797115.2797130},
abstract = {Recent advances in open data have resulted in vast amounts of tabular datasets containing valuable, actionable information to several stakeholders. However, information pertaining to any given entity is fragmented across several arbitrarily structured tables. There is a pressing need for semantic integration of such disparate datasets to enable deeper forms of inference and intelligence. This task is challenging because not only such datasets have no overarching schematic framework, there is also no overarching thematic framework. The datasets need not be about any one specific topic or theme. Hence, there is no one specific ontology onto which the datasets can be mapped. In this work we address the issue of mapping arbitrarily structured tabular data to one or more existing ontologies from the Linked Open Data Cloud (LOD) or abducing a new ontology around subsets of such tables. The overall objectives of this work called "Inferencing in the Large" aims to go further than this, to enrich mapped ontologies with inferencing rules and enable the use of semantic reasoners.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {13},
numpages = {6},
keywords = {LOD, Ontology, Concept Abduction, Semantic Web},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797126,
author = {Bayerl, Sebastian and Granitzer, Michael},
title = {Bacon: Linked Data Integration Based on the RDF Data Cube Vocabulary},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797126},
doi = {10.1145/2797115.2797126},
abstract = {Discovering and integrating relevant real-live datasets are essential tasks, when it comes to handling Linked Data. Similar to Data Warehousing approaches, Linked Data can be prepared to enable sophisticated data analysis. The developed open source framework bacon enables interactive and crowed-sourced Data Integration on Linked Data (Linked Data Integration), utilizing the RDF Data Cube Vocabulary and the semantic properties of Linked Open Data. Discovering suitable datasets on-the-fly in local or remote repositories sets up the ensuing integration process. Based on well-known Data Warehousing processes, the semantic nature of the data is taken into account to handle and merge RDF Data Cubes. To do so, structure and content of the cubes must be analyzed and processed. A similarity measure has been developed to find similarly structured cubes. The user is offered a graphical interface, where he can search for suitable cubes and modify their structure based on semantic properties. This process is fostered by a set of automated suggestions to support inexperienced users and also domain experts.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {6},
keywords = {Linked Data, Data Integration, Data Discovery, Data Fusion},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797124,
author = {Meusel, Robert and Bizer, Christian and Paulheim, Heiko},
title = {A Web-Scale Study of the Adoption and Evolution of the Schema.Org Vocabulary over Time},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797124},
doi = {10.1145/2797115.2797124},
abstract = {Promoted by major search engines, schema.org has become a widely adopted standard for marking up structured data in HTML web pages. In this paper, we use a series of large-scale Web crawls to analyze the evolution and adoption of schema.org over time. The availability of data from different points in time for both the schema and the websites deploying data allows for a new kind of empirical analysis of standards adoption, which has not been possible before. To conduct our analysis, we compare different versions of the schema.org vocabulary to the data that was deployed on hundreds of thousands of Web pages at different points in time. We measure both top-down adoption (i.e., the extent to which changes in the schema are adopted by data providers) as well as bottom-up evolution (i.e., the extent to which the actually deployed data drives changes in the schema). Our empirical analysis shows that both processes can be observed.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {15},
numpages = {11},
keywords = {Data Space Profiling, Microdata, schema.org, Standardization, Adoption},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797116,
author = {Sivaramakrishnan, Aravind and Krishnamachari, Madhusudhan and Balasubramanian, Vidhya},
title = {Recommending Customizable Products: A Multiple Choice Knapsack Solution},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797116},
doi = {10.1145/2797115.2797116},
abstract = {Recommender systems have become very prominent over the past decade. Methods such as collaborative filtering and knowledge based recommender systems have been developed extensively for non-customizable products. However, as manufacturers today are moving towards customizable products to satisfy customers, the need of the hour is customizable product recommender systems. Such systems must be able to capture customer preferences and provide recommendations that are both diverse and novel. This paper proposes an approach to building a recommender system that can be adapted to customizable products such as desktop computers and home theater systems. The Customizable Product Recommendation problem is modeled as a special case of the Multiple Choice Knapsack Problem, and an algorithm is proposed to generate desirable product recommendations in real-time. The performance of the proposed system is then evaluated.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {16},
numpages = {10},
keywords = {Customization, Recommender System, Customizable Products},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797128,
author = {Peska, Ladislav and Vojtas, Peter},
title = {Using Linked Open Data in Recommender Systems},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797128},
doi = {10.1145/2797115.2797128},
abstract = {In this paper, we present our work in progress on using LOD data to enhance recommending on existing e-commerce sites. We imagine a situation of e-commerce website employing content-based or hybrid recommendation. Such recommending algorithms need relevant object attributes to produce useful recommendations. However, on some domains, usable attributes may be difficult to fill in manually and yet accessible from LOD cloud.A pilot study was conducted on the domain of secondhand bookshops. In this domain, recommending is extraordinary difficult because of high ratio between objects and users, lack of significant attributes and limited availability of items. Both collaborative filtering and content-based recommendation applicability is questionable under this conditions. We queried both Czech and English language edition of DBPedia in order to receive additional information about objects (books) and used various recommending algorithms to learn user preferences. Our approach is general and can be applied on other domains as well.Proposed methods were tested in an off-line recommending scenario with promising results; however there are a lot of challenges for the future work including more complex algorithm analysis, improving SPARQL queries or improving DBPedia matching rules and resource identification.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {17},
numpages = {6},
keywords = {e-commerce, DBPedia, CBMF, Recommender systems, Linked Open Data, VSM, content-based attributes},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797129,
author = {Kitazawa, Takuya and Sugiyama, Masahide},
title = {User Modeling in Folksonomies: Relational Clustering and Tag Weighting},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797129},
doi = {10.1145/2797115.2797129},
abstract = {This paper proposes a user-modeling method for folksonomic data. Since data mining of folksonomic data is difficult due to their complexity, significant amounts of preprocessing are usually required. To catch sketchy characteristics of such complex data, our method employs two steps: (1) using the infinite relational model (IRM) to perform relational clustering of a folksonomic data set, and (2) using tag-weighting to extract the characteristics of each user cluster. As an experimental evaluation, we applied our method to real-world data from one of the most popular social bookmarking services in Japan. Our user-modeling method successfully extracted semantically clustered user models, thus demonstrating that relational data analysis has promise for mining folksonomic data. In addition, we developed the user-model-based filtering algorithm (UMF), which evaluates the user models by their resource recommendations. The F-measure was higher than that of random recommendation, and the running time was much shorter than that of collaborative-filtering-based top-n recommendation.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {18},
numpages = {6},
keywords = {Folksonomy, Recommendation, User Modeling},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

@inproceedings{10.1145/2797115.2797121,
author = {Paphitou, Athina C. and Constantinou, Stella and Kapitsaki, Georgia M.},
title = {SensoMan: Remote Management of Context Sensors},
year = {2015},
isbn = {9781450332934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797115.2797121},
doi = {10.1145/2797115.2797121},
abstract = {Sensor networks that collect data from the environment can be utilized in the development of different context-aware applications bringing in sight the need for data collection, management and distribution. Boards with microcontrollers are gaining on acceptance and popularity in the latest years mainly for educational and research purposes. Utilizing the information available via sensors connected to these platforms requires the presence of adequate infrastructure for the management of the sensor system, in order to retrieve information and control its use. In this work, we present the prototype of our sensor management system SensoMan that manages a collection of sensors spread in the environment connected to specific boards. The proposed system can be extended with more sensors and combined with different applications for the efficient use of the sensor data in context-aware applications. In this paper we present the architecture of SensoMan and its main modules.},
booktitle = {Proceedings of the 5th International Conference on Web Intelligence, Mining and Semantics},
articleno = {19},
numpages = {6},
keywords = {Sensor management, Raspberry Pi, Mobile computing, Context, Arduino},
location = {Larnaca, Cyprus},
series = {WIMS '15}
}

