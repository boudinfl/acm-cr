@inproceedings{10.1145/1150402.1150404,
author = {Stankovic, John A.},
title = {Self-Organizing Wireless Sensor Networks in Action},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150404},
doi = {10.1145/1150402.1150404},
abstract = {Wireless sensor networks (WSN) composed of large numbers of small devices that self-organize are being investigated for a wide variety of applications. Two key advantages of these networks over more traditional sensor networks are that they can be dynamically and quickly deployed, and that they can provide fine-grained sensing. Applications, such as emergency response to natural or manmade disasters, detection and tracking, and fine grained sensing of the environment are key examples of applications that can benefit from these types of WSN. Current research for these systems is widespread. However, many of the proposed solutions are developed with simplifying assumptions about wireless communication and the environment, even though the realities of wireless communication and environmental sensing are well known. Many of the solutions are evaluated only by simulation. In this talk I describe a fully implemented system consisting of a suite of more than 30 synthesized protocols. The system supports a power aware surveillance, tracking and classification application running on 203 XSM motes and evaluated in a realistic, large-area environment. Technical details and evaluations are presented. I end with a discussion of opportunities and problems for data mining related to WSN.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150405,
author = {Moore, Andrew},
title = {New Cached-Sufficient Statistics Algorithms for Quickly Answering Statistical Questions},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150405},
doi = {10.1145/1150402.1150405},
abstract = {This talk is about recent work on new ways to exploit preprocessed views of data tables for tractably solving big statistical queries. We'll describe deployments of these new algorithms in the realms of detecting killer asteroids and unnatural disease outbreaks.In recent years, several groups have looked at methods for pre-storing general sufficient statistics of the data in spatial data structures such as kd-trees and ball-trees so that both frequentist and Bayesian statistical operations become fast for large datasets. In this talk we will look at two other classes of optimization required in important statistical queries.The first involves iterating over all spatial regions (big and small). The second involves detection of tracks from noisy intermittent observations separated far apart in time and space. We will also discuss the implications that have arisen from making these operations tractable. We will focus particularly onDetecting all asteroids in the solar system larger than Pittsburgh's Cathedral of Learning (data to be collected over 2006-2010). Early detection of emerging diseases based on national monitoring of health-related transactions..},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {2},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150406,
author = {Agrawal, Rakesh},
title = {Next Frontier},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150406},
doi = {10.1145/1150402.1150406},
abstract = {This talk is about the next frontier in knowledge discovery and data mining.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {3},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150408,
author = {Achtert, Elke and B\"{o}hm, Christian and Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Zimek, Arthur},
title = {Deriving Quantitative Models for Correlation Clusters},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150408},
doi = {10.1145/1150402.1150408},
abstract = {Correlation clustering aims at grouping the data set into correlation clusters such that the objects in the same cluster exhibit a certain density and are all associated to a common arbitrarily oriented hyperplane of arbitrary dimensionality. Several algorithms for this task have been proposed recently. However, all algorithms only compute the partitioning of the data into clusters. This is only a first step in the pipeline of advanced data analysis and system modelling. The second (post-clustering) step of deriving a quantitative model for each correlation cluster has not been addressed so far. In this paper, we describe an original approach to handle this second step. We introduce a general method that can extract quantitative information on the linear dependencies within a correlation clustering. Our concepts are independent of the clustering model and can thus be applied as a post-processing step to any correlation clustering algorithm. Furthermore, we show how these quantitative models can be used to predict the probability distribution that an object is created by these models. Our broad experimental evaluation demonstrates the beneficial impact of our method on several applications of significant practical importance.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {4–13},
numpages = {10},
keywords = {cluster description, data mining, cluster model, correlation clustering, clustering},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150409,
author = {Agarwal, Alekh and Chakrabarti, Soumen and Aggarwal, Sunny},
title = {Learning to Rank Networked Entities},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150409},
doi = {10.1145/1150402.1150409},
abstract = {Several algorithms have been proposed to learn to rank entities modeled as feature vectors, based on relevance feedback. However, these algorithms do not model network connections or relations between entities. Meanwhile, Pagerank and variants find the stationary distribution of a reasonable but arbitrary Markov walk over a network, but do not learn from relevance feedback. We present a framework for ranking networked entities based on Markov walks with parameterized conductance values associated with the network edges. We propose two flavors of conductance learning problems in our framework. In the first setting, relevance feedback comparing node-pairs hints that the user has one or more hidden preferred communities with large edge conductance, and the algorithm must discover these communities. We present a constrained maximum entropy network flow formulation whose dual can be solved efficiently using a cutting-plane approach and a quasi-Newton optimizer. In the second setting, edges have types, and relevance feedback hints that each edge type has a potentially different conductance, but this is fixed across the whole network. Our algorithm learns the conductances using an approximate Newton method.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {14–23},
numpages = {10},
keywords = {conductance matrix, maximum entropy, network flow, pagerank},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150410,
author = {Agarwal, Deepak and McGregor, Andrew and Phillips, Jeff M. and Venkatasubramanian, Suresh and Zhu, Zhengyuan},
title = {Spatial Scan Statistics: Approximations and Performance Study},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150410},
doi = {10.1145/1150402.1150410},
abstract = {Spatial scan statistics are used to determine hotspots in spatial data, and are widely used in epidemiology and biosurveillance. In recent years, there has been much effort invested in designing efficient algorithms for finding such "high discrepancy" regions, with methods ranging from fast heuristics for special cases, to general grid-based methods, and to efficient approximation algorithms with provable guarantees on performance and quality.In this paper, we make a number of contributions to the computational study of spatial scan statistics. First, we describe a simple exact algorithm for finding the largest discrepancy region in a domain. Second, we propose a new approximation algorithm for a large class of discrepancy functions (including the Kulldorff scan statistic) that improves the approximation versus run time trade-off of prior methods. Third, we extend our simple exact and our approximation algorithms to data sets which lie naturally on a grid or are accumulated onto a grid. Fourth, we conduct a detailed experimental comparison of these methods with a number of known methods, demonstrating that our approximation algorithm has far superior performance in practice to prior methods, and exhibits a good performance-accuracy trade-off.All extant methods (including those in this paper) are suitable for data sets that are modestly sized; if data sets are of the order of millions of data points, none of these methods scale well. For such massive data settings, it is natural to examine whether small-space streaming algorithms might yield accurate answers. Here, we provide some negative results, showing that any streaming algorithms that even provide approximately optimal answers to the discrepancy maximization problem must use space linear in the input.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {24–33},
numpages = {10},
keywords = {discrepancy, spatial scan statistics, Kulldorff scan statistic},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150411,
author = {Anagnostopoulos, Aris and Vlachos, Michail and Hadjieleftheriou, Marios and Keogh, Eamonn and Yu, Philip S.},
title = {Global Distance-Based Segmentation of Trajectories},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150411},
doi = {10.1145/1150402.1150411},
abstract = {This work introduces distance-based criteria for segmentation of object trajectories. Segmentation leads to simplification of the original objects into smaller, less complex primitives that are better suited for storage and retrieval purposes. Previous work on trajectory segmentation attacked the problem locally, segmenting separately each trajectory of the database. Therefore, they did not directly optimize the inter-object separability, which is necessary for mining operations such as searching, clustering, and classification on large databases. In this paper we analyze the trajectory segmentation problem from a global perspective, utilizing data aware distance-based optimization techniques, which optimize pairwise distance estimates hence leading to more efficient object pruning. We first derive exact solutions of the distance-based formulation. Due to the intractable complexity of the exact solution, we present anapproximate, greedy solution that exploits forward searching of locally optimal solutions. Since the greedy solution also imposes a prohibitive computational cost, we also put forward more light weight variance-based segmentation techniques, which intelligently "relax" the pairwise distance only in the areas that affect the least the mining operation.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {34–43},
numpages = {10},
keywords = {DNA visualization, data simplification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150412,
author = {Backstrom, Lars and Huttenlocher, Dan and Kleinberg, Jon and Lan, Xiangyang},
title = {Group Formation in Large Social Networks: Membership, Growth, and Evolution},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150412},
doi = {10.1145/1150402.1150412},
abstract = {The processes by which communities come together, attract new members, and develop over time is a central research issue in the social sciences - political movements, professional organizations, and religious denominations all provide fundamental examples of such communities. In the digital domain, on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal. However, the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved: what are the structural features that influence whether individuals will join communities, which communities will grow rapidly, and how do the overlaps among pairs of communities change over time.Here we address these questions using two large sources of data: friendship links and community membership on LiveJournal, and co-authorship and conference publications in DBLP. Both of these datasets provide explicit user-defined communities, where conferences serve as proxies for communities in DBLP. We study how the evolution of these communities relates to properties such as the structure of the underlying social networks. We find that the propensity of individuals to join communities, and of communities to grow rapidly, depends in subtle ways on the underlying network structure. For example, the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community, but also crucially by how those friends are connected to one another. We use decision-tree techniques to identify the most significant structural determinants of these properties. We also develop a novel methodology for measuring movement of individuals between communities, and show how such movements are closely aligned with changes in the topics of interest within the communities.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {44–54},
numpages = {11},
keywords = {diffusion of innovations, on-line communities, social networks},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150413,
author = {Barbar\'{a}, Daniel and Domeniconi, Carlotta and Rogers, James P.},
title = {Detecting Outliers Using Transduction and Statistical Testing},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150413},
doi = {10.1145/1150402.1150413},
abstract = {Outlier detection can uncover malicious behavior in fields like intrusion detection and fraud analysis. Although there has been a significant amount of work in outlier detection, most of the algorithms proposed in the literature are based on a particular definition of outliers (e.g., density-based), and use ad-hoc thresholds to detect them. In this paper we present a novel technique to detect outliers with respect to an existing clustering model. However, the test can also be successfully utilized to recognize outliers when the clustering information is not available. Our method is based on Transductive Confidence Machines, which have been previously proposed as a mechanism to provide individual confidence measures on classification decisions. The test uses hypothesis testing to prove or disprove whether a point is fit to be in each of the clusters of the model. We experimentally demonstrate that the test is highly robust, and produces very few misdiagnosed points, even when no clustering information is available. Furthermore, our experiments demonstrate the robustness of our method under the circumstances of data contaminated by outliers. We finally show that our technique can be successfully applied to identify outliers in a noisy data set for which no information is available (e.g., ground truth, clustering structure, etc.). As such our proposed methodology is capable of bootstrapping from a noisy data set a clean one that can be used to identify future outliers.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {55–64},
numpages = {10},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150414,
author = {B\"{o}hm, Christian and Faloutsos, Christos and Pan, Jia-Yu and Plant, Claudia},
title = {Robust Information-Theoretic Clustering},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150414},
doi = {10.1145/1150402.1150414},
abstract = {How do we find a natural clustering of a real world point set, which contains an unknown number of clusters with different shapes, and which may be contaminated by noise? Most clustering algorithms were designed with certain assumptions (Gaussianity), they often require the user to give input parameters, and they are sensitive to noise. In this paper, we propose a robust framework for determining a natural clustering of a given data set, based on the minimum description length (MDL) principle. The proposed framework, Robust Information-theoretic Clustering (RIC), is orthogonal to any known clustering algorithm: given a preliminary clustering, RIC purifies these clusters from noise, and adjusts the clusterings such that it simultaneously determines the most natural amount and shape (subspace) of the clusters. Our RIC method can be combined with any clustering technique ranging from K-means and K-medoids to advanced methods such as spectral clustering. In fact, RIC is even able to purify and improve an initial coarse clustering, even if we start with very simple methods such as grid-based space partitioning. Moreover, RIC scales well with the data set size. Extensive experiments on synthetic and real world data sets validate the proposed RIC framework.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {65–75},
numpages = {11},
keywords = {data summarization, noise-robustness, parameter-free data mining, clustering},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150415,
author = {Brickell, Justin and Shmatikov, Vitaly},
title = {Efficient Anonymity-Preserving Data Collection},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150415},
doi = {10.1145/1150402.1150415},
abstract = {The output of a data mining algorithm is only as good as its inputs, and individuals are often unwilling to provide accurate data about sensitive topics such as medical history and personal finance. Individuals maybe willing to share their data, but only if they are assured that it will be used in an aggregate study and that it cannot be linked back to them. Protocols for anonymity-preserving data collection provide this assurance, in the absence of trusted parties, by allowing a set of mutually distrustful respondents to anonymously contribute data to an untrusted data miner.To effectively provide anonymity, a data collection protocol must be collusion resistant, which means that even if all dishonest respondents collude with a dishonest data miner in an attempt to learn the associations between honest respondents and their responses, they will be unable to do so. To achieve collusion resistance, previously proposed protocols for anonymity-preserving data collection have quadratically many communication rounds in the number of respondents, and employ (sometimes incorrectly) complicated cryptographic techniques such as zero-knowledge proofs.We describe a new protocol for anonymity-preserving, collusion resistant data collection. Our protocol has linearly many communication rounds, and achieves collusion resistance without relying on zero-knowledge proofs. This makes it especially suitable for data mining scenarios with a large number of respondents.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {76–85},
numpages = {10},
keywords = {data mining, privacy, anonymity},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150416,
author = {Buehrer, Gregory and Parthasarathy, Srinivasan and Ghoting, Amol},
title = {Out-of-Core Frequent Pattern Mining on a Commodity PC},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150416},
doi = {10.1145/1150402.1150416},
abstract = {In this work we focus on the problem of frequent itemset mining on large, out-of-core data sets. After presenting a characterization of existing out-of-core frequent itemset mining algorithms and their drawbacks, we introduce our efficient, highly scalable solution. Presented in the context of the FPGrowth algorithm, our technique involves several novel I/O-conscious optimizations, such as approximate hash-based sorting and blocking, and leverages recent architectural advancements in commodity computers, such as 64-bit processing. We evaluate the proposed optimizations on truly large data sets,up to 75GB, and show they yield greater than a 400-fold execution time improvement. Finally, we discuss the impact of this research in the context of other pattern mining challenges, such as sequence mining and graph mining.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {86–95},
numpages = {10},
keywords = {secondary memory, out of core, pattern mining, data mining, itemsets},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150417,
author = {Calders, Toon and Goethals, Bart and Jaroszewicz, Szymon},
title = {Mining Rank-Correlated Sets of Numerical Attributes},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150417},
doi = {10.1145/1150402.1150417},
abstract = {We study the mining of interesting patterns in the presence of numerical attributes. Instead of the usual discretization methods, we propose the use of rank based measures to score the similarity of sets of numerical attributes. New support measures for numerical data are introduced, based on extensions of Kendall's tau, and Spearman's Footrule and rho. We show how these support measures are related. Furthermore, we introduce a novel type of pattern combining numerical and categorical attributes. We give efficient algorithms to find all frequent patterns for the proposed support measures, and evaluate their performance on real-life datasets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {96–105},
numpages = {10},
keywords = {numerical, rank correlation, data mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150418,
author = {Chen, Jin and Hsu, Wynne and Lee, Mong Li and Ng, See-Kiong},
title = {NeMoFinder: Dissecting Genome-Wide Protein-Protein Interactions with Meso-Scale Network Motifs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150418},
doi = {10.1145/1150402.1150418},
abstract = {Recent works in network analysis have revealed the existence of network motifs in biological networks such as the protein-protein interaction (PPI) networks. However, existing motif mining algorithms are not sufficiently scalable to find meso-scale network motifs. Also, there has been little or no work to systematically exploit the extracted network motifs for dissecting the vast interactomes.We describe an efficient network motif discovery algorithm, NeMoFinder, that can mine meso-scale network motifs that are repeated and unique in large PPI networks. Using NeMoFinder, we successfully discovered, for the first time, up to size-12 network motifs in a large whole-genome S. cerevisiae (Yeast) PPI network. We also show that such network motifs can be systematically exploited for indexing the reliability of PPI data that were generated via highly erroneous high-throughput experimental methods.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {106–115},
numpages = {10},
keywords = {protein-protein interaction network, graph mining, network motif},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150419,
author = {Davis, Jason V. and Dhillon, Inderjit S.},
title = {Estimating the Global Pagerank of Web Communities},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150419},
doi = {10.1145/1150402.1150419},
abstract = {Localized search engines are small-scale systems that index a particular community on the web. They offer several benefits over their large-scale counterparts in that they are relatively inexpensive to build, and can provide more precise and complete search capability over their relevant domains. One disadvantage such systems have over large-scale search engines is the lack of global PageRank values. Such information is needed to assess the value of pages in the localized search domain within the context of the web as a whole. In this paper, we present well-motivated algorithms to estimate the global PageRank values of a local domain. The algorithms are all highly scalable in that, given a local domain of size n, they use O(n) resources that include computation time, bandwidth, and storage. We test our methods across a variety of localized domains, including site-specific domains and topic-specific domains. We demonstrate that by crawling as few as n or 2n additional pages, our methods can give excellent global PageRank estimates.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {116–125},
numpages = {10},
keywords = {stochastic complementation, page rank, Markov chain},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150420,
author = {Ding, Chris and Li, Tao and Peng, Wei and Park, Haesun},
title = {Orthogonal Nonnegative Matrix T-Factorizations for Clustering},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150420},
doi = {10.1145/1150402.1150420},
abstract = {Currently, most research on nonnegative matrix factorization (NMF)focus on 2-factor $X=FG^T$ factorization. We provide a systematicanalysis of 3-factor $X=FSG^T$ NMF. While it unconstrained 3-factor NMF is equivalent to it unconstrained 2-factor NMF, itconstrained 3-factor NMF brings new features to it constrained 2-factor NMF. We study the orthogonality constraint because it leadsto rigorous clustering interpretation. We provide new rules for updating $F,S, G$ and prove the convergenceof these algorithms. Experiments on 5 datasets and a real world casestudy are performed to show the capability of bi-orthogonal 3-factorNMF on simultaneously clustering rows and columns of the input datamatrix. We provide a new approach of evaluating the quality ofclustering on words using class aggregate distribution andmulti-peak distribution. We also provide an overview of various NMF extensions andexamine their relationships.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {126–135},
numpages = {10},
keywords = {tri-factorization, multi-peak distribution, nonnegative matrix factorization (NMF), clustering, orthogonal factorization},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150421,
author = {Fan, Wei and McCloskey, Joe and Yu, Philip S.},
title = {A General Framework for Accurate and Fast Regression by Data Summarization in Random Decision Trees},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150421},
doi = {10.1145/1150402.1150421},
abstract = {Predicting the values of continuous variable as a function of several independent variables is one of the most important problems for data mining. A very large number of regression methods, both parametric and nonparametric, have been proposed in the past. However, since the list is quite extensive and many of these models make rather explicit, strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options, choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial, sometimes frustrating, task for data mining practitioners. Choosing the inappropriate methodology can have rather disappointing results. This issue is against the general utility of data mining software. For example,linear regression methods are straightforward and well-understood. However, since the linear assumption is very strong, its performance is compromised for complicated non-linear problems. Kernel-based methods perform quite well if the kernel functions are selected correctly. In this paper, we propose a straightforward approach based on summarizing the training data using an ensemble of random decisions trees. It requires very little knowledge from the user, yet is applicable to every type of regression problem that we are currently aware of. We have experimented on a wide range of problems including those that parametric methods performwell, a large selection of benchmark datasets for nonparametric regression, as well as highly non-linear stochastic problems. Our results are either significantly better than or identical to many approaches that are known to perform well on these problems.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {136–146},
numpages = {11},
keywords = {regression, decision trees, random},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150422,
author = {Fan, Wei and Davidson, Ian},
title = {Reverse Testing: An Efficient Framework to Select amongst Classifiers under Sample Selection Bias},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150422},
doi = {10.1145/1150402.1150422},
abstract = {One of the most important assumptions made by many classification algorithms is that the training and test sets are drawn from the same distribution, i.e., the so-called "stationary distribution assumption" that the future and the past data sets are identical from a probabilistic standpoint. In many domains of real-world applications, such as marketing solicitation, fraud detection, drug testing, loan approval, sub-population surveys, school enrollment among others, this is rarely the case. This is because the only labeled sample available for training is biased in different ways due to a variety of practical reasons and limitations. In these circumstances, traditional methods to evaluate the expected generalization error of classification algorithms, such as structural risk minimization, ten-fold cross-validation, and leave-one-out validation, usually return poor estimates of which classification algorithm, when trained on biased dataset, will be the most accurate for future unbiased dataset, among a number of competing candidates. Sometimes, the estimated order of the learning algorithms' accuracy could be so poor that it is not even better than random guessing. Therefore,a method to determine the most accurate learner is needed for data mining under sample selection bias for many real-world applications. We present such an approach that can determine which learner will perform the best on an unbiased test set, given a possibly biased training set, in a fraction of the computational cost to use cross-validation based approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {147–156},
numpages = {10},
keywords = {classification, stationary distribution, sample selection bias, cross-validation},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150423,
author = {Forman, George},
title = {Quantifying Trends Accurately despite Classifier Error and Class Imbalance},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150423},
doi = {10.1145/1150402.1150423},
abstract = {This paper promotes a new task for supervised machine learning research: quantification - the pursuit of learning methods for accurately estimating the class distribution of a test set, with no concern for predictions on individual cases. A variant for cost quantification addresses the need to total up costs according to categories predicted by imperfect classifiers. These tasks cover a large and important family of applications that measure trends over time.The paper establishes a research methodology, and uses it to evaluate several proposed methods that involve selecting the classification threshold in a way that would spoil the accuracy of individual classifications. In empirical tests, Median Sweep methods show outstanding ability to estimate the class distribution, despite wide disparity in testing and training conditions. The paper addresses shifting class priors and costs, but not concept drift in general.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {157–166},
numpages = {10},
keywords = {text mining, cost quantification, quantification, classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150424,
author = {Gionis, Aristides and Mannila, Heikki and Mielik\"{a}inen, Taneli and Tsaparas, Panayiotis},
title = {Assessing Data Mining Results via Swap Randomization},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150424},
doi = {10.1145/1150402.1150424},
abstract = {The problem of assessing the significance of data mining results on high-dimensional 0-1 data sets has been studied extensively in the literature. For problems such as mining frequent sets and finding correlations, significance testing can be done by, e.g., chi-square tests, or many other methods. However, the results of such tests depend only on the specific attributes and not on the dataset as a whole. Moreover, the tests are more difficult to apply to sets of patterns or other complex results of data mining. In this paper, we consider a simple randomization technique that deals with this shortcoming. The approach consists of producing random datasets that have the same row and column margins with the given dataset, computing the results of interest on the randomized instances, and comparing them against the results on the actual data. This randomization technique can be used to assess the results of many different types of data mining algorithms, such as frequent sets, clustering, and rankings. To generate random datasets with given margins, we use variations of a Markov chain approach, which is based on a simple swap operation. We give theoretical results on the efficiency of different randomization methods, and apply the swap randomization method to several well-known datasets. Our results indicate that for some datasets the structure discovered by the data mining algorithms is a random artifact, while for other datasets the discovered structure conveys meaningful information.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {167–176},
numpages = {10},
keywords = {significance testing, 0-1 data, randomization tests, swaps},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150425,
author = {Hashimoto, Kosuke and Aoki-Kinoshita, Kiyoko F. and Ueda, Nobuhisa and Kanehisa, Minoru and Mamitsuka, Hiroshi},
title = {A New Efficient Probabilistic Model for Mining Labeled Ordered Trees},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150425},
doi = {10.1145/1150402.1150425},
abstract = {Mining frequent patterns is a general and important issue in data mining. Complex and unstructured (or semi-structured) datasets have appeared in major data mining applications, including text mining, web mining and bioinformatics. Mining patterns from these datasets is the focus of many of the current data mining approaches. We focus on labeled ordered trees, typical datasets of semi-structured data in data mining, and propose a new probabilistic model and its efficient learning scheme for mining labeled ordered trees. The proposed approach significantly improves the time and space complexity of an existing probabilistic modeling for labeled ordered trees, while maintaining its expressive power. We evaluated the performance of the proposed model, comparing it with that of the existing model, using synthetic as well as real datasets from the field of glycobiology. Experimental results showed that the proposed model drastically reduced the computation time of the competing model, keeping the predictive power and avoiding overfitting to the training data. Finally, we assessed our results using the proposed model on real data from a variety of biological viewpoints, verifying known facts in glycobiology.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {177–186},
numpages = {10},
keywords = {probabilistic models, expectation - maximization, labeled ordered trees, maximum likelihood},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150426,
author = {Hoi, Steven C. H. and Lyu, Michael R. and Chang, Edward Y.},
title = {Learning the Unified Kernel Machines for Classification},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150426},
doi = {10.1145/1150402.1150426},
abstract = {Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper, we propose a novel general framework of learning the Unified Kernel Machines (UKM) from both labeled and unlabeled data. Our proposed framework integrates supervised learning, semi-supervised kernel learning, and active learning in a unified solution. In the suggested framework, we particularly focus our attention on designing a new semi-supervised kernel learning method, i.e., Spectral Kernel Learning (SKL), which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework, we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions (KLR), i.e., Unified Kernel Logistic Regression (UKLR). We evaluate our proposed UKLR classification scheme in comparison with traditional solutions. The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {187–196},
numpages = {10},
keywords = {spectral kernel learning, semi-supervised learning, kernel machines, kernel logistic regressions, active learning, classification, unsupervised kernel design, supervised learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150427,
author = {Horv\'{a}th, Tam\'{a}s and Ramon, Jan and Wrobel, Stefan},
title = {Frequent Subgraph Mining in Outerplanar Graphs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150427},
doi = {10.1145/1150402.1150427},
abstract = {In recent years there has been an increased interest in algorithms that can perform frequent pattern discovery in large databases of graph structured objects. While the frequent connected subgraph mining problem for tree datasets can be solved in incremental polynomial time, it becomes intractable for arbitrary graph databases. Existing approaches have therefore resorted to various heuristic strategies and restrictions of the search space, but have not identified a practically relevant tractable graph class beyond trees. In this paper, we define the class of so called tenuous outerplanar graphs, a strict generalization of trees, develop a frequent subgraph mining algorithm for tenuous outerplanar graphs that works in incremental polynomial time, and evaluate the algorithm empirically on the NCI molecular graph dataset.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {197–206},
numpages = {10},
keywords = {frequent pattern discovery, graph mining, computational chemistry},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150428,
author = {Ihler, Alexander and Hutchins, Jon and Smyth, Padhraic},
title = {Adaptive Event Detection with Time-Varying Poisson Processes},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150428},
doi = {10.1145/1150402.1150428},
abstract = {Time-series of count data are generated in many different contexts, such as web access logging, freeway traffic monitoring, and security logs associated with buildings. Since this data measures the aggregated behavior of individual human beings, it typically exhibits a periodicity in time on a number of scales (daily, weekly,etc.) that reflects the rhythms of the underlying human activity and makes the data appear non-homogeneous. At the same time, the data is often corrupted by a number of bursty periods of unusual behavior such as building events, traffic accidents, and so forth. The data mining problem of finding and extracting these anomalous events is made difficult by both of these elements. In this paper we describe a framework for unsupervised learning in this context, based on a time-varying Poisson process model that can also account for anomalous events. We show how the parameters of this model can be learned from count time series using statistical estimation techniques. We demonstrate the utility of this model on two datasets for which we have partial ground truth in the form of known events, one from freeway traffic data and another from building access data, and show that the model performs significantly better than a non-probabilistic, threshold-based technique. We also describe how the model can be used to investigate different degrees of periodicity in the data, including systematic day-of-week and time-of-day effects, and make inferences about the detected events (e.g., popularity or level of attendance). Our experimental results indicate that the proposed time-varying Poisson model provides a robust and accurate framework for adaptively and autonomously learning how to separate unusual bursty events from traces of normal human activity.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {207–216},
numpages = {10},
keywords = {event detection, poisson, Markov modulated},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150429,
author = {Joachims, Thorsten},
title = {Training Linear SVMs in Linear Time},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150429},
doi = {10.1145/1150402.1150429},
abstract = {Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s &lt;&lt; N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {217–226},
numpages = {10},
keywords = {training algorithms, ordinal regression, ROC-area, support vector machines (SVM), large-scale problems},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150430,
author = {Ke, Yiping and Cheng, James and Ng, Wilfred},
title = {Mining Quantitative Correlated Patterns Using an Information-Theoretic Approach},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150430},
doi = {10.1145/1150402.1150430},
abstract = {Existing research on mining quantitative databases mainly focuses on mining associations. However, mining associations is too expensive to be practical in many cases. In this paper, we study mining correlations from quantitative databases and show that it is a more effective approach than mining associations. We propose a new notion of Quantitative Correlated Patterns (QCPs), which is founded on two formal concepts, mutual information and all-confidence. We first devise a normalization on mutual information and apply it to QCP mining to capture the dependency between the attributes. We further adopt all-confidence as a quality measure to control, at a finer granularity, the dependency between the attributes with specific quantitative intervals. We also propose a supervised method to combine the consecutive intervals of the quantitative attributes based on mutual information, such that the interval combining is guided by the dependency between the attributes. We develop an algorithm, QCoMine, to efficiently mine QCPs by utilizing normalized mutual information and all-confidence to perform a two-level pruning. Our experiments verify the efficiency of QCoMine and the quality of the QCPs.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {227–236},
numpages = {10},
keywords = {information-theoretic approach, mutual information, quantitative databases, correlated patterns},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150431,
author = {Knobbe, Arno J. and Ho, Eric K. Y.},
title = {Maximally Informative K-Itemsets and Their Efficient Discovery},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150431},
doi = {10.1145/1150402.1150431},
abstract = {In this paper we present a new approach to mining binary data. We treat each binary feature (item) as a means of distinguishing two sets of examples. Our interest is in selecting from the total set of items an itemset of specified size, such that the database is partitioned with as uniform a distribution over the parts as possible. To achieve this goal, we propose the use of joint entropy as a quality measure for itemsets, and refer to optimal itemsets of cardinality k as maximally informative k-itemsets. We claim that this approach maximises distinctive power, as well as minimises redundancy within the feature set. A number of algorithms is presented for computing optimal itemsets efficiently.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {237–244},
numpages = {8},
keywords = {feature selection, binary data, subgroup discovery, joint entropy, maximally informative k-itemsets, information theory},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150432,
author = {Koren, Yehuda and North, Stephen C. and Volinsky, Chris},
title = {Measuring and Extracting Proximity in Networks},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150432},
doi = {10.1145/1150402.1150432},
abstract = {Measuring distance or some other form of proximity between objects is a standard data mining tool. Connection subgraphs were recently proposed as a way to demonstrate proximity between nodes in networks. We propose a new way of measuring and extracting proximity in networks called "cycle free effective conductance"(CFEC). Our proximity measure can handle more than two endpoints, directed edges, is statistically well-behaved, and produces an effectiveness score for the computed subgraphs. We provide an efficien talgorithm. Also, we report experimental results and show examples for three large network data sets: a telecommunications calling graph, the IMDB actors graph, and an academic co-authorship network.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {245–255},
numpages = {11},
keywords = {escape probability, random walks, proximity, proximity subgraph, cycle-free escape probability, connection subgraph},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150433,
author = {Kumar, Ravi and Punera, Kunal and Tomkins, Andrew},
title = {Hierarchical Topic Segmentation of Websites},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150433},
doi = {10.1145/1150402.1150433},
abstract = {In this paper, we consider the problem of identifying and segmenting topically cohesive regions in the URL tree of a large website. Each page of the website is assumed to have a topic label or a distribution on topic labels generated using a standard classifier. We develop a set of cost measures characterizing the benefit accrued by introducing a segmentation of the site based on the topic labels. We propose a general framework to use these measures for describing the quality of a segmentation; we also provide an efficient algorithm to find the best segmentation in this framework. Extensive experiments on human-labeled data confirm the soundness of our framework and suggest that a judicious choice of cost measures allows the algorithm to perform surprisingly accurate topical segmentations.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {257–266},
numpages = {10},
keywords = {website hierarchy, facility location, KL-distance, gain ratio, website segmentation, classification, tree partitioning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150434,
author = {Latecki, Longin Jan and Sobel, Marc and Lakaemper, Rolf},
title = {New EM Derived from Kullback-Leibler Divergence},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150434},
doi = {10.1145/1150402.1150434},
abstract = {We introduce a new EM framework in which it is possible not only to optimize the model parameters but also the number of model components. A key feature of our approach is that we use nonparametric density estimation to improve parametric density estimation in the EM framework. While the classical EM algorithm estimates model parameters empirically using the data points themselves, we estimate them using nonparametric density estimates.There exist many possible applications that require optimal adjustment of model components. We present experimental results in two domains. One is polygonal approximation of laser range data, which is an active research topic in robot navigation. The other is grouping of edge pixels to contour boundaries, which still belongs to unsolved problems in computer vision.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {267–276},
numpages = {10},
keywords = {Kullback-Leibler divergence, EM, expectation maximization},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150435,
author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
title = {Workload-Aware Anonymization},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150435},
doi = {10.1145/1150402.1150435},
abstract = {Protecting data privacy is an important problem in microdata distribution. Anonymization algorithms typically aim to protect individual privacy, with minimal impact on the quality of the resulting data. While the bulk of previous work has measured quality through one-size-fits-all measures, we argue that quality is best judged with respect to the workload for which the data will ultimately be used.This paper provides a suite of anonymization algorithms that produce an anonymous view based on a target class of workloads, consisting of one or more data mining tasks, as well as selection predicates. An extensive experimental evaluation indicates that this approach is often more effective than previous anonymization techniques.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {277–286},
numpages = {10},
keywords = {predictive modeling, privacy, anonymity, data recoding},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150436,
author = {Li, Ping and Hastie, Trevor J. and Church, Kenneth W.},
title = {Very Sparse Random Projections},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150436},
doi = {10.1145/1150402.1150436},
abstract = {There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {287–296},
numpages = {10},
keywords = {random projections, sampling, rates of convergence},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150437,
author = {Liu, Bing and Zhao, Kaidi and Benkler, Jeffrey and Xiao, Weimin},
title = {Rule Interestingness Analysis Using OLAP Operations},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150437},
doi = {10.1145/1150402.1150437},
abstract = {The problem of interestingness of discovered rules has been investigated by many researchers. The issue is that data mining algorithms often generate too many rules, which make it very hard for the user to find the interesting ones. Over the years many techniques have been proposed. However, few have made it to real-life applications. Since August 2004, we have been working on a major application for Motorola. The objective is to find causes of cellular phone call failures from a large amount of usage log data. Class association rules have been shown to be suitable for this type of diagnostic data mining application. We were also able to put several existing interestingness methods to the test, which revealed some major shortcomings. One of the main problems is that most existing methods treat rules individually. However, we discovered that users seldom regard a single rule to be interesting by itself. A rule is only interesting in the context of some other rules. Furthermore, in many cases, each individual rule may not be interesting, but a group of them together can represent an important piece of knowledge. This led us to discover a deficiency of the current rule mining paradigm. Using non-zero minimum support and non-zero minimum confidence eliminates a large amount of context information, which makes rule analysis difficult. This paper proposes a novel approach to deal with all of these issues, which casts rule analysis as OLAP operations and general impression mining. This approach enables the user to explore the knowledge space to find useful knowledge easily and systematically. It also provides a natural framework for visualization. As an evidence of its effectiveness, our system, called Opportunity Map, based on these ideas has been deployed, and it is in daily use in Motorola for finding actionable knowledge from its engineering and other types of data sets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {297–306},
numpages = {10},
keywords = {OLAP, general impressions, diagnostic data mining, interestingness analysis, class association rules},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150438,
author = {Loekito, Elsa and Bailey, James},
title = {Fast Mining of High Dimensional Expressive Contrast Patterns Using Zero-Suppressed Binary Decision Diagrams},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150438},
doi = {10.1145/1150402.1150438},
abstract = {Patterns of contrast are a very important way of comparing multi-dimensional datasets. Such patterns are able to capture regions of high difference between two classes of data, and are useful for human experts and the construction of classifiers. However, mining such patterns is particularly challenging when the number of dimensions is large. This paper describes a new technique for mining several varieties of contrast pattern, based on the use of Zero-Suppressed Binary Decision Diagrams (ZBDDs), a powerful data structure for manipulating sparse data. We study the mining of both simple contrast patterns, such as emerging patterns, and more novel and complex contrasts, which we call disjunctive emerging patterns. A performance study demonstrates our ZBDD technique is highly scalable, substantially improves on state of the art mining for emerging patterns and can be effective for discovering complex contrasts from datasets with thousands of attributes.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {307–316},
numpages = {10},
keywords = {contrast patterns, disjunctive emerging patterns, zero-suppressed binary decision diagrams},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150439,
author = {Long, Bo and Wu, Xiaoyun and Zhang, Zhongfei (Mark) and Yu, Philip S.},
title = {Unsupervised Learning on K-Partite Graphs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150439},
doi = {10.1145/1150402.1150439},
abstract = {Various data mining applications involve data objects of multiple types that are related to each other, which can be naturally formulated as a k-partite graph. However, the research on mining the hidden structures from a k-partite graph is still limited and preliminary. In this paper, we propose a general model, the relation summary network, to find the hidden structures (the local cluster structures and the global community structures) from a k-partite graph. The model provides a principal framework for unsupervised learning on k-partite graphs of various structures. Under this model, we derive a novel algorithm to identify the hidden structures of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures. Experiments on both synthetic and real datasets demonstrate the promise and effectiveness of the proposed model and algorithm. We also establish the connections between existing clustering approaches and the proposed model to provide a unified view to the clustering approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {317–326},
numpages = {10},
keywords = {k-partite graph, clustering, unsupervised learning, Bregman divergence, relation summary network},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150440,
author = {Mahoney, Michael W. and Maggioni, Mauro and Drineas, Petros},
title = {Tensor-CUR Decompositions for Tensor-Based Data},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150440},
doi = {10.1145/1150402.1150440},
abstract = {Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more indices, we develop a tensor-based extension of the matrix CUR decomposition. The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others. In this case, the tensor-CUR decomposition approximately expresses the original data tensor in terms of a basis consisting of underlying subtensors that are actual data elements and thus that have natural interpretation in terms ofthe processes generating the data. In order to demonstrate the general applicability of this tensor decomposition, we apply it to problems in two diverse domains of data analysis: hyperspectral medical image analysis and consumer recommendation system analysis. In the hyperspectral data application, the tensor-CUR decomposition is used to compress the data, and we show that classification quality is not substantially reduced even after substantial data compression. In the recommendation system application, the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor, and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {327–336},
numpages = {10},
keywords = {recommendation system analysis, tensor CUR, CUR decomposition, hyperspectral image analysis},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150441,
author = {Mei, Qiaozhu and Xin, Dong and Cheng, Hong and Han, Jiawei and Zhai, ChengXiang},
title = {Generating Semantic Annotations for Frequent Patterns with Context Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150441},
doi = {10.1145/1150402.1150441},
abstract = {As a fundamental data mining task, frequent pattern mining has widespread applications in many different domains. Research in frequent pattern mining has so far mostly focused on developing efficient algorithms to discover various kinds of frequent patterns, but little attention has been paid to the important nextstep - interpreting the discovered frequent patterns. Although some recent work has studied the compression and summarization of frequent patterns, the proposed techniques can only annotate a frequent pattern with non-semantical information (e.g. support), which provides only limited help for a user to understand the patterns.In this paper, we propose the novel problem of generating semantic annotations for frequent patterns. The goal is to annotate a frequent pattern with in-depth, concise, and structured information that can better indicate the hidden meanings of the pattern. We propose a general approach to generate such anannotation for a frequent pattern by constructing its context model, selecting informative context indicators, and extracting representative transactions and semantically similar patterns. This general approach has potentially many applications such as generating a dictionary-like description for a pattern, finding synonym patterns, discovering semantic relations, and summarizing semantic classes of a set of frequent patterns. Experiments on different datasets show that our approach is effective in generating semantic pattern annotations.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {337–346},
numpages = {10},
keywords = {pattern annotation, pattern semantic analysis, pattern context, frequent pattern},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150442,
author = {Mielik\"{a}inen, Taneli and Terzi, Evimaria and Tsaparas, Panayiotis},
title = {Aggregating Time Partitions},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150442},
doi = {10.1145/1150402.1150442},
abstract = {Partitions of sequential data exist either per se or as a result of sequence segmentation algorithms. It is often the case that the same timeline is partitioned in many different ways. For example, different segmentation algorithms produce different partitions of the same underlying data points. In such cases, we are interested in producing an aggregate partition, i.e., a segmentation that agrees as much as possible with the input segmentations. Each partition is defined as a set of continuous non-overlapping segments of the timeline. We show that this problem can be solved optimally in polynomial time using dynamic programming. We also propose faster greedy heuristics that work well in practice. We experiment with our algorithms and we demonstrate their utility in clustering the behavior of mobile-phone users and combining the results of different segmentation algorithms on genomic sequences.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {347–356},
numpages = {10},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150443,
author = {Rattigan, Matthew J. and Maier, Marc and Jensen, David},
title = {Using Structure Indices for Efficient Approximation of Network Properties},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150443},
doi = {10.1145/1150402.1150443},
abstract = {Statistics on networks have become vital to the study of relational data drawn from areas such as bibliometrics, fraud detection, bioinformatics, and the Internet. Calculating many of the most important measures - such as betweenness centrality, closeness centrality, and graph diameter-requires identifying short paths in these networks. However, finding these short paths can be intractable for even moderate-size networks. We introduce the concept of a network structure index (NSI), a composition of (1) a set of annotations on every node in the network and (2) a function that uses the annotations to estimate graph distance between pairs of nodes. We present several varieties of NSIs, examine their time and space complexity, and analyze their performance on synthetic and real data sets. We show that creating an NSI for a given network enables extremely efficient and accurate estimation of a wide variety of network statistics on that network.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {357–366},
numpages = {10},
keywords = {knowledge discovery in graphs, social network analysis, network structure index, centrality},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150444,
author = {Rosales, R\'{o}mer and Fung, Glenn},
title = {Learning Sparse Metrics via Linear Programming},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150444},
doi = {10.1145/1150402.1150444},
abstract = {Calculation of object similarity, for example through a distance function, is a common part of data mining and machine learning algorithms. This calculation is crucial for efficiency since distances are usually evaluated a large number of times, the classical example being query-by-example (find objects that are similar to a given query object). Moreover, the performance of these algorithms depends critically on choosing a good distance function. However, it is often the case that (1) the correct distance is unknown or chosen by hand, and (2) its calculation is computationally expensive (e.g., such as for large dimensional objects). In this paper, we propose a method for constructing relative-distance preserving low-dimensional mapping (sparse mappings). This method allows learning unknown distance functions (or approximating known functions) with the additional property of reducing distance computation time. We present an algorithm that given examples of proximity comparisons among triples of objects (object i is more like object j than object k), learns a distance function, in as few dimensions as possible, that preserves these distance relationships. The formulation is based on solving a linear programming optimization problem that finds an optimal mapping for the given dataset and distance relationships. Unlike other popular embedding algorithms, this method can easily generalize to new points, does not have local minima, and explicitly models computational efficiency by finding a mapping that is sparse, i.e. one that depends on a small subset of features or dimensions. Experimental evaluation shows that the proposed formulation compares favorably with a state-of-the art method in several publicly available datasets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {367–373},
numpages = {7},
keywords = {convex optimization, dimensionality reduction, relative distance constraints, metric learning, linear projections, linear programming},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150445,
author = {Sun, Jimeng and Tao, Dacheng and Faloutsos, Christos},
title = {Beyond Streams and Graphs: Dynamic Tensor Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150445},
doi = {10.1145/1150402.1150445},
abstract = {How do we find patterns in author-keyword associations, evolving over time? Or in Data Cubes, with product-branch-customer sales information? Matrix decompositions, like principal component analysis (PCA) and variants, are invaluable tools for mining, dimensionality reduction, feature selection, rule identification in numerous settings like streaming data, text, graphs, social networks and many more. However, they have only two orders, like author and keyword, in the above example.We propose to envision such higher order data as tensors,and tap the vast literature on the topic. However, these methods do not necessarily scale up, let alone operate on semi-infinite streams. Thus, we introduce the dynamic tensor analysis (DTA) method, and its variants. DTA provides a compact summary for high-order and high-dimensional data, and it also reveals the hidden correlations. Algorithmically, we designed DTA very carefully so that it is (a) scalable, (b) space efficient (it does not need to store the past) and (c) fully automatic with no need for user defined parameters. Moreover, we propose STA, a streaming tensor analysis method, which provides a fast, streaming approximation to DTA.We implemented all our methods, and applied them in two real settings, namely, anomaly detection and multi-way latent semantic indexing. We used two real, large datasets, one on network flow data (100GB over 1 month) and one from DBLP (200MB over 25 years). Our experiments show that our methods are fast, accurate and that they find interesting patterns and outliers on the real datasets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {374–383},
numpages = {10},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150446,
author = {Tang, Lei and Zhang, Jianping and Liu, Huan},
title = {Acclimatizing Taxonomic Semantics for Hierarchical Content Classification},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150446},
doi = {10.1145/1150402.1150446},
abstract = {Hierarchical models have been shown to be effective in content classification. However, we observe through empirical study that the performance of a hierarchical model varies with given taxonomies; even a semantically sound taxonomy has potential to change its structure for better classification. By scrutinizing typical cases, we elucidate why a given semantics-based hierarchy does not work well in content classification, and how it could be improved for accurate hierarchical classification. With these understandings, we propose effective localized solutions that modify the given taxonomy for accurate hierarchical classification. We conduct extensive experiments on both toy and real-world data sets, report improved performance and interesting findings, and provide further analysis of algorithmic issues such as time complexity, robustness, and sensitivity to the number of features.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {384–393},
numpages = {10},
keywords = {hierarchical classification, taxonomy adjustment, hierarchical modeling, text classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150447,
author = {Tao, Yufei and Xiao, Xiaokui and Zhou, Shuigeng},
title = {Mining Distance-Based Outliers from Large Databases in Any Metric Space},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150447},
doi = {10.1145/1150402.1150447},
abstract = {Let R be a set of objects. An object o ∈ R is an outlier, if there exist less than k objects in R whose distances to o are at most r. The values of k, r, and the distance metric are provided by a user at the run time. The objective is to return all outliers with the smallest I/O cost.This paper considers a generic version of the problem, where no information is available for outlier computation, except for objects' mutual distances. We prove an upper bound for the memory consumption which permits the discovery of all outliers by scanning the dataset 3 times. The upper bound turns out to be extremely low in practice, e.g., less than 1% of R. Since the actual memory capacity of a realistic DBMS is typically larger, we develop a novel algorithm, which integrates our theoretical findings with carefully-designed heuristics that leverage the additional memory to improve I/O efficiency. Our technique reports all outliers by scanning the dataset at most twice (in some cases, even once), and significantly outperforms the existing solutions by a factor up to an order of magnitude.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {394–403},
numpages = {10},
keywords = {metric data, outlier, mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150448,
author = {Tong, Hanghang and Faloutsos, Christos},
title = {Center-Piece Subgraphs: Problem Definition and Fast Solutions},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150448},
doi = {10.1145/1150402.1150448},
abstract = {Given Q nodes in a social network (say, authorship network), how can we find the node/author that is the center-piece, and has direct or indirect connections to all, or most of them? For example, this node could be the common advisor, or someone who started the research area that the Q nodes belong to. Isomorphic scenarios appear in law enforcement (find the master-mind criminal, connected to all current suspects), gene regulatory networks (find the protein that participates in pathways with all or most of the given Q proteins), viral marketing and many more.Connection subgraphs is an important first step, handling the case of Q=2 query nodes. Then, the connection subgraph algorithm finds the b intermediate nodes, that provide a good connection between the two original query nodes.Here we generalize the challenge in multiple dimensions: First, we allow more than two query nodes. Second, we allow a whole family of queries, ranging from 'OR' to 'AND', with 'softAND' in-between. Finally, we design and compare a fast approximation, and study the quality/speed trade-off.We also present experiments on the DBLP dataset. The experiments confirm that our proposed method naturally deals with multi-source queries and that the resulting subgraphs agree with our intuition. Wall-clock timing results on the DBLP dataset show that our proposed approximation achieve good accuracy for about 6:1 speedup.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {404–413},
numpages = {10},
keywords = {center-piece subgraph, goodness score, KAND},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150449,
author = {Wang, Ke and Fung, Benjamin C. M.},
title = {Anonymizing Sequential Releases},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150449},
doi = {10.1145/1150402.1150449},
abstract = {An organization makes a new release as new information become available, releases a tailored view for each data request, releases sensitive information and identifying information separately. The availability of related releases sharpens the identification of individuals by a global quasi-identifier consisting of attributes from related releases. Since it is not an option to anonymize previously released data, the current release must be anonymized to ensure that a global quasi-identifier is not effective for identification. In this paper, we study the sequential anonymization problem under this assumption. A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose. We introduce the lossy join, a negative property in relational database design, as a way to hide the join relationship among releases, and propose a scalable and practical solution.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {414–423},
numpages = {10},
keywords = {generalization, classification, privacy, k-anonymity, sequential release},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150450,
author = {Wang, Xuerui and McCallum, Andrew},
title = {Topics over Time: A Non-Markov Continuous-Time Model of Topical Trends},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150450},
doi = {10.1145/1150402.1150450},
abstract = {This paper presents an LDA-style topic model that captures not only the low-dimensional structure of data, but also how the structure changes over time. Unlike other recent work that relies on Markov assumptions or discretization of time, here each topic is associated with a continuous distribution over timestamps, and for each generated document, the mixture distribution over topics is influenced by both word co-occurrences and the document's timestamp. Thus, the meaning of a particular topic can be relied upon as constant, but the topics' occurrence and correlations change significantly over time. We present results on nine months of personal email, 17 years of NIPS research papers and over 200 years of presidential state-of-the-union addresses, showing improved topics, better timestamp prediction, and interpretable trends.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {424–433},
numpages = {10},
keywords = {topic modeling, graphical models, temporal analysis},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150451,
author = {Webb, Geoffrey I.},
title = {Discovering Significant Rules},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150451},
doi = {10.1145/1150402.1150451},
abstract = {In many applications, association rules will only be interesting if they represent non-trivial correlations between all constituent items. Numerous techniques have been developed that seek to avoid false discoveries. However, while all provide useful solutions to aspects of this problem, none provides a generic solution that is both flexible enough to accommodate varying definitions of true and false discoveries and powerful enough to provide strict control over the risk of false discoveries. This paper presents generic techniques that allow definitions of true and false discoveries to be specified in terms of arbitrary statistical hypothesis tests and which provide strict control over the experiment wise risk of false discoveries.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {434–443},
numpages = {10},
keywords = {association rules, rule discovery, statistics},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150452,
author = {Xin, Dong and Cheng, Hong and Yan, Xifeng and Han, Jiawei},
title = {Extracting Redundancy-Aware Top-k Patterns},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150452},
doi = {10.1145/1150402.1150452},
abstract = {Observed in many applications, there is a potential need of extracting a small set of frequent patterns having not only high significance but also low redundancy. The significance is usually defined by the context of applications. Previous studies have been concentrating on how to compute top-k significant patterns or how to remove redundancy among patterns separately. There is limited work on finding those top-k patterns which demonstrate high-significance and low-redundancy simultaneously.In this paper, we study the problem of extracting redundancy-aware top-k patterns from a large collection of frequent patterns. We first examine the evaluation functions for measuring the combined significance of a pattern set and propose the MMS (Maximal Marginal Significance) as the problem formulation. The problem is known as NP-hard. We further present a greedy algorithm which approximates the optimal solution with performance bound O(log k) (with conditions on redundancy), where k is the number of reported patterns. The direct usage of redundancy-aware top-k patterns is illustrated through two real applications: disk block prefetch and document theme extraction. Our method can also be applied to processing redundancy-aware top-k queries in traditional database.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {444–453},
numpages = {10},
keywords = {significance, redundancy, pattern extraction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@dataset{10.1145/review-1150402.1150452_R41381,
author = {Papadopoulos, Apostolos N},
title = {Review ID:R41381 for DOI: 10.1145/1150402.1150452},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/review-1150402.1150452_R41381}
}

@inproceedings{10.1145/1150402.1150453,
author = {Ye, Jieping and Wang, Tie},
title = {Regularized Discriminant Analysis for High Dimensional, Low Sample Size Data},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150453},
doi = {10.1145/1150402.1150453},
abstract = {Linear and Quadratic Discriminant Analysis have been used widely in many areas of data mining, machine learning, and bioinformatics. Friedman proposed a compromise between Linear and Quadratic Discriminant Analysis, called Regularized Discriminant Analysis (RDA), which has been shown to be more flexible in dealing with various class distributions. RDA applies the regularization techniques by employing two regularization parameters, which are chosen to jointly maximize the classification performance. The optimal pair of parameters is commonly estimated via cross-validation from a set of candidate pairs. It is computationally prohibitive for high dimensional data, especially when the candidate set is large, which limits the applications of RDA to low dimensional data.In this paper, a novel algorithm for RDA is presented for high dimensional data. It can estimate the optimal regularization parameters from a large set of parameter candidates efficiently. Experiments on a variety of datasets confirm the claimed theoretical estimate of the efficiency, and also show that, for a properly chosen pair of regularization parameters, RDA performs favorably in classification, in comparison with other existing classification methods.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {454–463},
numpages = {10},
keywords = {dimensionality reduction, quadratic discriminant analysis, cross-validation, regularization},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150454,
author = {Yu, Shipeng and Yu, Kai and Tresp, Volker and Kriegel, Hans-Peter and Wu, Mingrui},
title = {Supervised Probabilistic Principal Component Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150454},
doi = {10.1145/1150402.1150454},
abstract = {Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {464–473},
numpages = {10},
keywords = {dimensionality reduction, supervised projection, semi-supervised projection, principal component analysis},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150455,
author = {Zhang, Dell and Lee, Wee Sun},
title = {Extracting Key-Substring-Group Features for Text Classification},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150455},
doi = {10.1145/1150402.1150455},
abstract = {In many text classification applications, it is appealing to take every document as a string of characters rather than a bag of words. Previous research studies in this area mostly focused on different variants of generative Markov chain models. Although discriminative machine learning methods like Support Vector Machine (SVM) have been quite successful in text classification with word features, it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features. In this paper, we propose to partition all substrings into statistical equivalence groups, and then pick those groups which are important (in the statistical sense) as features (named key-substring-group features) for text classification. In particular, we propose a suffix tree based algorithm that can extract such features in linear time (with respect to the total number of characters in the corpus). Our experiments on English, Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {474–483},
numpages = {10},
keywords = {text classification, machine learning, suffix tree, text mining, feature extraction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150456,
author = {Zhao, Qiankun and Liu, Tie-Yan and Bhowmick, Sourav S. and Ma, Wei-Ying},
title = {Event Detection from Evolution of Click-through Data},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150456},
doi = {10.1145/1150402.1150456},
abstract = {Previous efforts on event detection from the web have focused primarily on web content and structure data ignoring the rich collection of web log data. In this paper, we propose the first approach to detect events from the click-through data, which is the log data of web search engines. The intuition behind event detection from click-through data is that such data is often event-driven and each event can be represented as a set ofquery-page pairs that are not only semantically similar but also have similar evolution pattern over time. Given the click-through data, in our proposed approach, we first segment it into a sequence of bipartite graphs based on theuser-defined time granularity. Next, the sequence of bipartite graphs is represented as a vector-based graph, which records the semantic and evolutionary relationships between queries and pages. After that, the vector-based graph is transformed into its dual graph, where each node is a query-page pair that will be used to represent real world events. Then, the problem of event detection is equivalent to the problem of clustering the dual graph of the vector-based graph. The clustering process is based on a two-phase graph cut algorithm. In the first phase, query-page pairs are clustered based on thesemantic-based similarity such that each cluster in the result corresponds to a specific topic. In the second phase, query-page pairs related to the same topic are further clustered based on the evolution pattern-based similarity such that each cluster is expected to represent a specific event under the specific topic. Experiments with real click-through data collected from a commercial web search engine show that the proposed approach produces high quality results.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {484–493},
numpages = {10},
keywords = {click-through data, dynamic web, event detection, evolution pattern},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150457,
author = {Zhu, Jun and Nie, Zaiqing and Wen, Ji-Rong and Zhang, Bo and Ma, Wei-Ying},
title = {Simultaneous Record Detection and Attribute Labeling in Web Data Extraction},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150457},
doi = {10.1145/1150402.1150457},
abstract = {Recent work has shown the feasibility and promise of template-independent Web data extraction. However, existing approaches use decoupled strategies - attempting to do data record detection and attribute labeling in two separate phases. In this paper, we show that separately extracting data records and attributes is highly ineffective and propose a probabilistic model to perform these two tasks simultaneously. In our approach, record detection can benefit from the availability of semantics required in attribute labeling and, at the same time, the accuracy of attribute labeling can be improved when data records are labeled in a collective manner. The proposed model is called Hierarchical Conditional Random Fields. It can efficiently integrate all useful features by learning their importance, and it can also incorporate hierarchical interactions which are very important for Web data extraction. We empirically compare the proposed model with existing decoupled approaches for product information extraction, and the results show significant improvements in both record detection and attribute labeling.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {494–503},
numpages = {10},
keywords = {web page segmentation, hierarchical conditional random fields, data record detection, attribute labeling, conditional random fields},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150459,
author = {Abe, Naoki and Zadrozny, Bianca and Langford, John},
title = {Outlier Detection by Active Learning},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150459},
doi = {10.1145/1150402.1150459},
abstract = {Most existing approaches to outlier detection are based on density estimation methods. There are two notable issues with these methods: one is the lack of explanation for outlier flagging decisions, and the other is the relatively high computational requirement. In this paper, we present a novel approach to outlier detection based on classification, in an attempt to address both of these issues. Our approach isbased on two key ideas. First, we present a simple reduction of outlier detection to classification, via a procedure that involves applying classification to a labeled data set containing artificially generated examples that play the role of potential outliers. Once the task has been reduced to classification, we then invoke a selective sampling mechanism based on active learning to the reduced classification problem. We empirically evaluate the proposed approach using a number of data sets, and find that our method is superior to other methods based on the same reduction to classification, but using standard classification methods. We also show that it is competitive to the state-of-the-art outlier detection methods in the literature based on density estimation, while significantly improving the computational complexity and explanatory power.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {504–509},
numpages = {6},
keywords = {ensemble method, outlier detection, active learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150460,
author = {Aggarwal, Charu C. and Pei, Jian and Zhang, Bo},
title = {On Privacy Preservation against Adversarial Data Mining},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150460},
doi = {10.1145/1150402.1150460},
abstract = {Privacy preserving data processing has become an important topic recently because of advances in hardware technology which have lead to widespread proliferation of demographic and sensitive data. A rudimentary way to preserve privacy is to simply hide the information in some of the sensitive fields picked by a user. However, such a method is far from satisfactory in its ability to prevent adversarial data mining. Real data records are not randomly distributed. As a result, some fields in the records may be correlated with one another. If the correlation is sufficiently high, it may be possible for an adversary to predict some of the sensitive fields using other fields.In this paper, we study the problem of privacy preservation against adversarial data mining, which is to hide a minimal set of entries so that the privacy of the sensitive fields are satisfactorily preserved. In other words, even by data mining, an adversary still cannot accurately recover the hidden data entries. We model the problem concisely and develop an efficient heuristic algorithm which can find good solutions in practice. An extensive performance study is conducted on both synthetic and real data sets to examine the effectiveness of our approach.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {510–516},
numpages = {7},
keywords = {data mining, association rules, privacy preservation},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inbook{10.1145/1150402.1150461,
author = {Arunasalam, Bavani and Chawla, Sanjay},
title = {CCCS: A Top-down Associative Classifier for Imbalanced Class Distribution},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150461},
abstract = {In this paper we propose CCCS, a new algorithm for classification based on association rule mining. The key innovation in CCCS is the use of a new measure, the "Complement Class Support (CCS)" whose application results in rules which are guaranteed to be positively correlated. Furthermore, the anti-monotonic property that CCS possesses has very different semantics vis-a-vis the traditional support measure. In particular, "good" rules have a low CCS value. This makes CCS an ideal measure to use in conjunction with a top-down algorithm. Finally, the nature of CCS allows the pruning of rules without the setting of any threshold parameter! To the best of our knowledge this is the first threshold-free algorithm in association rule mining for classification.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {517–522},
numpages = {6}
}

@inproceedings{10.1145/1150402.1150462,
author = {Berger-Wolf, Tanya Y. and Saia, Jared},
title = {A Framework for Analysis of Dynamic Social Networks},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150462},
doi = {10.1145/1150402.1150462},
abstract = {Finding patterns of social interaction within a population has wide-ranging applications including: disease modeling, cultural and information transmission, and behavioral ecology. Social interactions are often modeled with networks. A key characteristic of social interactions is their continual change. However, most past analyses of social networks are essentially static in that all information about the time that social interactions take place is discarded. In this paper, we propose a new mathematical and computational framework that enables analysis of dynamic social networks and that explicitly makes use of information about when social interactions occur.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {523–528},
numpages = {6},
keywords = {dynamic social networks, algorithms, disease spread},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150463,
author = {Bhattacharya, Indrajit and Getoor, Lise and Licamele, Louis},
title = {Query-Time Entity Resolution},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150463},
doi = {10.1145/1150402.1150463},
abstract = {The goal of entity resolution is to reconcile database references corresponding to the same real-world entities. Given the abundance of publicly available databases where entities are not resolved, we motivate the problem of quickly processing queries that require resolved entities from such 'unclean' databases. We propose a two-stage collective resolution strategy for processing queries. We then show how it can be performed on-the-fly by adaptively extracting and resolving those database references that are the most helpful for resolving the query. We validate our approach on two large real-world publication databases where we show the usefulness of collective resolution and at the same time demonstrate the need for adaptive strategies for query processing. We then show how the same queries can be answered in real time using our adaptive approach while preserving the gains of collective resolution.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {529–534},
numpages = {6},
keywords = {relations, entity resolution, query, adaptive},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150464,
author = {Buciluundefined, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
title = {Model Compression},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150464},
doi = {10.1145/1150402.1150464},
abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {535–541},
numpages = {7},
keywords = {supervised learning, model compression},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150465,
author = {Burke, Robin and Mobasher, Bamshad and Williams, Chad and Bhaumik, Runa},
title = {Classification Features for Attack Detection in Collaborative Recommender Systems},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150465},
doi = {10.1145/1150402.1150465},
abstract = {Collaborative recommender systems are highly vulnerable to attack. Attackers can use automated means to inject a large number of biased profiles into such a system, resulting in recommendations that favor or disfavor given items. Since collaborative recommender systems must be open to user input, it is difficult to design a system that cannot be so attacked. Researchers studying robust recommendation have therefore begun to identify types of attacks and study mechanisms for recognizing and defeating them. In this paper, we propose and study different attributes derived from user profiles for their utility in attack detection. We show that a machine learning classification approach that includes attributes derived from attack models is more successful than more generalized detection algorithms previously studied.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {542–547},
numpages = {6},
keywords = {attack detection, robustness, collaborative filtering, recommender systems},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150466,
author = {Carvalho, Vitor R. and Cohen, William W.},
title = {Single-Pass Online Learning: Performance, Voting Schemes and Online Feature Selection},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150466},
doi = {10.1145/1150402.1150466},
abstract = {To learn concepts over massive data streams, it is essential to design inference and learning methods that operate in real time with limited memory. Online learning methods such as perceptron or Winnow are naturally suited to stream processing; however, in practice multiple passes over the same training data are required to achieve accuracy comparable to state-of-the-art batch learners. In the current work we address the problem of training an on-line learner with a single passover the data. We evaluate several existing methods, and also propose a new modification of Margin Balanced Winnow, which has performance comparable to linear SVM. We also explore the effect of averaging, a.k.a. voting, on online learning. Finally, we describe how the new Modified Margin Balanced Winnow algorithm can be naturally adapted to perform feature selection. This scheme performs comparably to widely-used batch feature selection methods like information gain or Chi-square, with the advantage of being able to select features on-the-fly. Taken together, these techniques allow single-pass online learning to be competitive with batch techniques, and still maintain the advantages of on-line learning.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {548–553},
numpages = {6},
keywords = {winnow, averaging, online learning, voting},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150467,
author = {Chakrabarti, Deepayan and Kumar, Ravi and Tomkins, Andrew},
title = {Evolutionary Clustering},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150467},
doi = {10.1145/1150402.1150467},
abstract = {We consider the problem of clustering data over time. An evolutionary clustering should simultaneously optimize two potentially conflicting criteria: first, the clustering at any point in time should remain faithful to the current data as much as possible; and second, the clustering should not shift dramatically from one timestep to the next. We present a generic framework for this problem, and discuss evolutionary versions of two widely-used clustering algorithms within this framework: k-means and agglomerative hierarchical clustering. We extensively evaluate these algorithms on real data sets and show that our algorithms can simultaneously attain both high accuracy in capturing today's data, and high fidelity in reflecting yesterday's clustering.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {554–560},
numpages = {7},
keywords = {agglomerative, temporal evolution, k-means, clustering},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150468,
author = {Gionis, Aristides and Mannila, Heikki and Puolam\"{a}ki, Kai and Ukkonen, Antti},
title = {Algorithms for Discovering Bucket Orders from Data},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150468},
doi = {10.1145/1150402.1150468},
abstract = {Ordering and ranking items of different types are important tasks in various applications, such as query processing and scientific data mining. A total order for the items can be misleading, since there are groups of items that have practically equal ranks.We consider bucket orders, i.e., total orders with ties. They can be used to capture the essential order information without overfitting the data: they form a useful concept class between total orders and arbitrary partial orders. We address the question of finding a bucket order for a set of items, given pairwise precedence information between the items. We also discuss methods for computing the pairwise precedence data.We describe simple and efficient algorithms for finding good bucket orders. Several of the algorithms have a provable approximation guarantee, and they scale well to large datasets. We provide experimental results on artificial and a real data that show the usefulness of bucket orders and demonstrate the accuracy and efficiency of the algorithms.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {561–566},
numpages = {6},
keywords = {partial order, bucket order, ranking, ordering},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150469,
author = {Guo, Hongyu and Viktor, Herna L.},
title = {Mining Relational Data through Correlation-Based Multiple View Validation},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150469},
doi = {10.1145/1150402.1150469},
abstract = {Commercial relational databases currently store vast amounts of real-world data. The data within these relational repositories are represented by multiple relations, which are inter-connected by means of foreign key joins. The mining of such interrelated data poses a major challenge to the data mining community. Unfortunately, traditional data mining algorithms usually only explore one relation, the so-called target relation, thus excluding crucial knowledge embedded in the related so-called background relations. In this paper, we propose a novel approach for classifying relational such domains. This strategy employs multiple views to capture crucial information not only from the target relation, but also from related relations. This information is integrated into the relational mining process. The framework presented here, firstly, explore the relational domain to partition its features space into multiple subsets. Subsequently, these subsets are used to construct multiple uncorrelated views, based on a novel correlation-based view validation method, against the target concept. Finally, the knowledge possessed by multiple views are incorporated into a meta-learning mechanism to augment one another. Based on this framework, a wide range of conventional data mining methods can be applied to mine relational databases. Our experiments on benchmark real-world data sets show that the proposed method achieves promising results both in terms of overall accuracy obtained and run time, when compared with two other relational data mining approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {567–573},
numpages = {7},
keywords = {multi-relational data mining, relational database, classification, multi-view learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150470,
author = {Iwata, Tomoharu and Saito, Kazumi and Yamada, Takeshi},
title = {Recommendation Method for Extending Subscription Periods},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150470},
doi = {10.1145/1150402.1150470},
abstract = {Online stores providing subscription services need to extend user subscription periods as long as possible to increase their profits. Conventional recommendation methods recommend items that best coincide with user's interests to maximize the purchase probability, which does not necessarily contribute to extend subscription periods. We present a novel recommendation method for subscription services that maximizes the probability of the subscription period being extended. Our method finds frequent purchase patterns in the long subscription period users, and recommends items for a new user to simulate the found patterns. Using survival analysis techniques, we efficiently extract information from the log data for finding the patterns. Furthermore, we infer user's interests from purchase histories based on maximum entropy models, and use the interests to improve the recommendations. Since a longer subscription period is the result of greater user satisfaction, our method benefits users as well as online stores. We evaluate our method using the real log data of an online cartoon distribution service for cell-phone in Japan.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {574–579},
numpages = {6},
keywords = {subscription service, maximum entropy, recommendation, survival analysis, user purchase pattern},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150471,
author = {Jank, Wolfgang and Shmueli, Galit and Wang, Shanshan},
title = {Dynamic, Real-Time Forecasting of Online Auctions via Functional Models},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150471},
doi = {10.1145/1150402.1150471},
abstract = {We propose a dynamic model for forecasting price in online auctions. One of the key features of our model is that it operates during the live-auction, which makes it different from previous approaches that only consider static models. Our model is also different with respect to how information about price is incorporated. While one part of the model is based on the more traditional notion of an auction's price-level, another part incorporates its dynamics in the form of a price's velocity and acceleration. In that sense, it incorporates key features of a dynamic environment such as an online auction. The use of novel functional data methodology allows us to measure, and subsequently include, dynamic price characteristics. We illustrate our model on a diverse set of eBay auctions across many different book categories. We find significantly higher prediction accuracy compared to standard approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {580–585},
numpages = {6},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150472,
author = {Jaroszewicz, Szymon},
title = {Polynomial Association Rules with Applications to Logistic Regression},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150472},
doi = {10.1145/1150402.1150472},
abstract = {A new class of associations (polynomial itemsets and polynomial association rules) is presented which allows for discovering nonlinear relationships between numeric attributes without discretization. For binary attributes, proposed associations reduce to classic itemsets and association rules. Many standard association rule mining algorithms can be adapted to finding polynomial itemsets and association rules. We applied polynomial associations to add non-linear terms to logistic regression models. Significant performance improvement was achieved over stepwise methods, traditionally used in statistics, with comparable accuracy.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {586–591},
numpages = {6},
keywords = {association rules, continuous attributes},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150473,
author = {Jiang, Nan and Gruenwald, Le},
title = {CFI-Stream: Mining Closed Frequent Itemsets in Data Streams},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150473},
doi = {10.1145/1150402.1150473},
abstract = {Mining frequent closed itemsets provides complete and condensed information for non-redundant association rules generation. Extensive studies have been done on mining frequent closed itemsets, but they are mainly intended for traditional transaction databases and thus do not take data stream characteristics into consideration. In this paper, we propose a novel approach for mining closed frequent itemsets over data streams. It computes and maintains closed itemsets online and incrementally, and can output the current closed frequent itemsets in real time based on users' specified thresholds. Experimental results show that our proposed method is both time and space efficient, has good scalability as the number of transactions processed increases and adapts very rapidly to the change in data streams.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {592–597},
numpages = {6},
keywords = {frequent closed itemsets, association rules, data stream},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150474,
author = {K\"{o}nig, Arnd Christian and Brill, Eric},
title = {Reducing the Human Overhead in Text Categorization},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150474},
doi = {10.1145/1150402.1150474},
abstract = {Many applications in text processing require significant human effort for either labeling large document collections (when learning statistical models) or extrapolating rules from them (when using knowledge engineering). In this work, we describe away to reduce this effort, while retaining the methods' accuracy, by constructing a hybrid classifier that utilizes human reasoning over automatically discovered text patterns to complement machine learning. Using a standard sentiment-classification dataset and real customer feedback data, we demonstrate that the resulting technique results in significant reduction of the human effort required to obtain a given classification accuracy. Moreover, the hybrid text classifier also results in a significant boost in accuracy over machine-learning based classifiers when a comparable amount of labeled data is used.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {598–603},
numpages = {6},
keywords = {support vector machines, active learning, classification, text classification, text mining, machine learning, supervised learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150475,
author = {Kumar, Deept and Ramakrishnan, Naren and Helm, Richard F. and Potts, Malcolm},
title = {Algorithms for Storytelling},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150475},
doi = {10.1145/1150402.1150475},
abstract = {We formulate a new data mining problem called it storytelling as a generalization of redescription mining. In traditional redescription mining, we are given a set of objects and a collection of subsets defined over these objects. The goal is to view the set system as a vocabulary and identify two expressions in this vocabulary that induce the same set of objects. Storytelling, on the other hand, aims to explicitly relate object sets that are disjoint (and hence, maximally dissimilar) by finding a chain of (approximate) redescriptions between the sets. This problem finds applications in bioinformatics, for instance, where the biologist is trying to relate a set of genes expressed in one experiment to another set, implicated in a different pathway. We outline an efficient storytelling implementation that embeds the CART wheels redescription mining algorithm in an A* search procedure, using the former to supply next move operators on search branches to the latter. This approach is practical and effective for mining large datasets and, at the same time, exploits the structure of partitions imposed by the given vocabulary. Three application case studies are presented: a study of word overlaps in large English dictionaries, exploring connections between genesets in a bioinformatics dataset, and relating publications in the PubMed index of abstracts.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {604–610},
numpages = {7},
keywords = {storytelling, redescription, data mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150476,
author = {Kumar, Ravi and Novak, Jasmine and Tomkins, Andrew},
title = {Structure and Evolution of Online Social Networks},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150476},
doi = {10.1145/1150402.1150476},
abstract = {In this paper, we consider the evolution of structure within large online social networks. We present a series of measurements of two such networks, together comprising in excess of five million people and ten million friendship links, annotated with metadata capturing the time of every event in the life of the network. Our measurements expose a surprising segmentation of these networks into three regions: singletons who do not participate in the network; isolated communities which overwhelmingly display star structure; and a giant component anchored by a well-connected core region which persists even in the absence of stars.We present a simple model of network growth which captures these aspects of component structure. The model follows our experimental results, characterizing users as either passive members of the network; inviters who encourage offline friends and acquaintances to migrate online; and linkers who fully participate in the social evolution of the network.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {611–617},
numpages = {7},
keywords = {social networks, stars, graph evolution, graph mining, small-world phenomenon},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150477,
author = {Laur, Sven and Lipmaa, Helger and Mielik\"{a}inen, Taneli},
title = {Cryptographically Private Support Vector Machines},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150477},
doi = {10.1145/1150402.1150477},
abstract = {We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms, give private classification protocols and private polynomial kernel computation protocols. The new protocols return their outputs - either the kernel value, the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants. We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier. The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {618–624},
numpages = {7},
keywords = {kernel methods, privacy preserving data mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150478,
author = {Lauw, Hady W. and Lim, Ee-Peng and Wang, Ke},
title = {Bias and Controversy: Beyond the Statistical Deviation},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150478},
doi = {10.1145/1150402.1150478},
abstract = {In this paper, we investigate how deviation in evaluation activities may reveal bias on the part of reviewers and controversy on the part of evaluated objects. We focus on a 'data-centric approach' where the evaluation data is assumed to represent the 'ground truth'. The standard statistical approaches take evaluation and deviation at face value. We argue that attention should be paid to the subjectivity of evaluation, judging the evaluation score not just on 'what is being said' (deviation), but also on 'who says it' (reviewer) as well as on 'whom it is said about' (object). Furthermore, we observe that bias and controversy are mutually dependent, as there is more bias if there is higher deviation on a less controversial object. To address this mutual dependency, we propose a reinforcement model to identify bias and controversy. We test our model on real-life data to verify its applicability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {625–630},
numpages = {6},
keywords = {evaluation, bias, controversy, social network},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150479,
author = {Leskovec, Jure and Faloutsos, Christos},
title = {Sampling from Large Graphs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150479},
doi = {10.1145/1150402.1150479},
abstract = {Given a huge real graph, how can we derive a representative sample? There are many known algorithms to compute interesting measures (shortest paths, centrality, betweenness, etc.), but several of them become impractical for large graphs. Thus graph sampling is essential.The natural questions to ask are (a) which sampling method to use, (b) how small can the sample size be, and (c) how to scale up the measurements of the sample (e.g., the diameter), to get estimates for the large graph. The deeper, underlying question is subtle: how do we measure success?.We answer the above questions, and test our answers by thorough experiments on several, diverse datasets, spanning thousands nodes and edges. We consider several sampling methods, propose novel methods to check the goodness of sampling, and develop a set of scaling laws that describe relations between the properties of the original and the sample.In addition to the theoretical contributions, the practical conclusions from our work are: Sampling strategies based on edge selection do not perform well; simple uniform random node selection performs surprisingly well. Overall, best performing methods are the ones based on random-walks and "forest fire"; they match very accurately both static as well as evolutionary graph patterns, with sample sizes down to about 15% of the original graph.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {631–636},
numpages = {6},
keywords = {scaling laws, graph mining, graph sampling},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150480,
author = {Liu, Jinze and Zhang, Qi and Wang, Wei and McMillan, Leonard and Prins, Jan},
title = {Clustering Pair-Wise Dissimilarity Data into Partially Ordered Sets},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150480},
doi = {10.1145/1150402.1150480},
abstract = {Ontologies represent data relationships as hierarchies of possibly overlapping classes. Ontologies are closely related to clustering hierarchies, and in this article we explore this relationship in depth. In particular, we examine the space of ontologies that can be generated by pairwise dissimilarity matrices. We demonstrate that classical clustering algorithms, which take dissimilarity matrices as inputs, do not incorporate all available information. In fact, only special types of dissimilarity matrices can be exactly preserved by previous clustering methods. We model ontologies as a partially ordered set (poset) over the subset relation. In this paper, we propose a new clustering algorithm, that generates a partially ordered set of clusters from a dissimilarity matrix.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {637–642},
numpages = {6},
keywords = {PoCluster, dissimilarity, clustering, poset},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150481,
author = {Maniyar, Dharmesh M. and Nabney, Ian T.},
title = {Visual Data Mining Using Principled Projection Algorithms and Information Visualization Techniques},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150481},
doi = {10.1145/1150402.1150481},
abstract = {We introduce a flexible visual data mining framework which combines advanced projection algorithms from the machine learning domain and visual techniques developed in the information visualization domain. The advantage of such an interface is that the user is directly involved in the data mining process. We integrate principled projection algorithms, such as generative topographic mapping (GTM) and hierarchical GTM (HGTM), with powerful visual techniques, such as magnification factors, directional curvatures, parallel coordinates and billboarding, to provide a visual data mining framework. Results on a real-life chemoinformatics dataset using GTM are promising and have been analytically compared with the results from the traditional projection methods. It is also shown that the HGTM algorithm provides additional value for large datasets. The computational complexity of these algorithms is discussed to demonstrate their suitability for the visual data mining framework.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {643–648},
numpages = {6},
keywords = {visual data mining, probabilistic projection algorithms, information visualization techniques},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150482,
author = {Mei, Qiaozhu and Zhai, ChengXiang},
title = {A Mixture Model for Contextual Text Mining},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150482},
doi = {10.1145/1150402.1150482},
abstract = {Contextual text mining is concerned with extracting topical themes from a text collection with context information (e.g., time and location) and comparing/analyzing the variations of themes over different contexts. Since the topics covered in a document are usually related to the context of the document, analyzing topical themes within context can potentially reveal many interesting theme patterns. In this paper, we generalize some of these models proposed in the previous work and we propose a new general probabilistic model for contextual text mining that can cover several existing models as special cases. Specifically, we extend the probabilistic latent semantic analysis (PLSA) model by introducing context variables to model the context of a document. The proposed mixture model, called contextual probabilistic latent semantic analysis (CPLSA) model, can be applied to many interesting mining tasks, such as temporal text mining, spatiotemporal text mining, author-topic analysis, and cross-collection comparative analysis. Empirical experiments show that the proposed mixture model can discover themes and their contextual variations effectively.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {649–655},
numpages = {7},
keywords = {clustering, theme pattern, mixture model, contextual text mining, context, EM algorithm},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150483,
author = {Merugu, Srujana and Rosset, Saharon and Perlich, Claudia},
title = {A New Multi-View Regression Approach with an Application to Customer Wallet Estimation},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150483},
doi = {10.1145/1150402.1150483},
abstract = {Motivated by the problem of customer wallet estimation, we propose a new setting for multi-view regression, where we learn a completely unobserved target (in our case, customer wallet) by modeling it as a "central link" in a directed graphical model, connecting multiple sets of observed variables. The resulting conditional independence allows us to reduce the maximum discriminative likelihood estimation problem to a convex optimization problem for exponential linear models. We show that under certain modeling assumptions, in particular, when there exist two conditionally independent views and the noise is Gaussian, this problem can be reduced to a single least squares regression. Thus, for this specific, but widely applicable setting, the "unsupervised" multi-view problem can be solved via a simple supervised learning approach. This reduction also allows us to test the statistical independence assumptions underlying the graphical model and perform variable selection. We demonstrate the effectiveness of our approach on our motivating problem of customer wallet estimation and on simulation data.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {656–661},
numpages = {6},
keywords = {regression, multi-view learning, bayesian networks},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150484,
author = {Messaoud, Riadh Ben and Boussaid, Omar and Rabas\'{e}da, Sabine Loudcher},
title = {Efficient Multidimensional Data Representations Based on Multiple Correspondence Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150484},
doi = {10.1145/1150402.1150484},
abstract = {In the On Line Analytical Processing (OLAP) context, exploration of huge and sparse data cubes is a tedious task which does not always lead to efficient results. In this paper, we couple OLAP with the Multiple Correspondence Analysis (MCA) in order to enhance visual representations of data cubes and thus, facilitate their interpretations and analysis. We also provide a quality criterion to measure the relevance of obtained representations. The criterion is based on a geometric neighborhood concept and a similarity metric between cells of a data cube. Experimental results on real data proved the interest and the efficiency of our approach.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {662–667},
numpages = {6},
keywords = {homogeneity criterion, data representation, MCA, test-values, data cubes, OLAP},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150485,
author = {Moerchen, Fabian},
title = {Algorithms for Time Series Knowledge Mining},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150485},
doi = {10.1145/1150402.1150485},
abstract = {Temporal patterns composed of symbolic intervals are commonly formulated with Allen's interval relations originating in temporal reasoning. This representation has severe disadvantages for knowledge discovery. The Time Series Knowledge Representation (TSKR) is a new hierarchical language for interval patterns expressing the temporal concepts of coincidence and partial order. We present effective and efficient mining algorithms for such patterns based on itemset techniques. A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation and speed up the algorithms. On a real data set a concise set of TSKR patterns can explain the underlying temporal phenomena, whereas the patterns found with Allen's relations are far more numerous yet only explain fragments of the data.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {668–673},
numpages = {6},
keywords = {interval patterns, knowledge discovery, time series},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150486,
author = {Nath, J. Saketha and Bhattacharyya, C. and Murty, M. N.},
title = {Clustering Based Large Margin Classification: A Scalable Approach Using SOCP Formulation},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150486},
doi = {10.1145/1150402.1150486},
abstract = {This paper presents a novel Second Order Cone Programming (SOCP) formulation for large scale binary classification tasks. Assuming that the class conditional densities are mixture distributions, where each component of the mixture has a spherical covariance, the second order statistics of the components can be estimated efficiently using clustering algorithms like BIRCH. For each cluster, the second order moments are used to derive a second order cone constraint via a Chebyshev-Cantelli inequality. This constraint ensures that any data point in the cluster is classified correctly with a high probability. This leads to a large margin SOCP formulation whose size depends on the number of clusters rather than the number of training data points. Hence, the proposed formulation scales well for large datasets when compared to the state-of-the-art classifiers, Support Vector Machines (SVMs). Experiments on real world and synthetic datasets show that the proposed algorithm outperforms SVM solvers in terms of training time and achieves similar accuracies.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {674–679},
numpages = {6},
keywords = {Gaussian mixture models, scalability, BIRCH, large margin classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150487,
author = {Newman, David and Chemudugunta, Chaitanya and Smyth, Padhraic},
title = {Statistical Entity-Topic Models},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150487},
doi = {10.1145/1150402.1150487},
abstract = {The primary purpose of news articles is to convey information about who, what, when and where. But learning and summarizing these relationships for collections of thousands to millions of articles is difficult. While statistical topic models have been highly successful at topically summarizing huge collections of text documents, they do not explicitly address the textual interactions between who/where, i.e. named entities (persons, organizations, locations) and what, i.e. the topics. We present new graphical models that directly learn the relationship between topics discussed in news articles and entities mentioned in each article. We show how these entity-topic models, through a better understanding of the entity-topic relationships, are better at making predictions about entities.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {680–686},
numpages = {7},
keywords = {text modeling, entity recognition, topic modeling},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150488,
author = {Palatin, Noam and Leizarowitz, Arie and Schuster, Assaf and Wolff, Ran},
title = {Mining for Misconfigured Machines in Grid Systems},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150488},
doi = {10.1145/1150402.1150488},
abstract = {Grid systems are proving increasingly useful for managing the batch computing jobs of organizations. One well-known example is Intel, whose internally developed NetBatch system manages tens of thousands of machines. The size, heterogeneity, and complexity of grid systems make them very difficult, however, to configure. This often results in misconfigured machines, which may adversely affect the entire system.We investigate a distributed data mining approach for detection of misconfigured machines. Our Grid Monitoring System (GMS) non-intrusively collects data from all sources (log files, system services, etc.) available throughout the grid system. It converts raw data to semantically meaningful data and stores this data on the machine it was obtained from, limiting incurred overhead and allowing scalability. Afterwards, when analysis is requested, a distributed outliers detection algorithm is employed to identify misconfigured machines. The algorithm itself is implemented as a recursive workflow of grid jobs. It is especially suited to grid systems, in which the machines might be unavailable most of the time and often fail altogether.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {687–692},
numpages = {6},
keywords = {system monitoring, distributed data mining, grid systems, grid information system, outliers detection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150489,
author = {Pan, Jia-Yu and Balan, Andr\'{e} G. R. and Xing, Eric P. and Traina, Agma Juci Machado and Faloutsos, Christos},
title = {Automatic Mining of Fruit Fly Embryo Images},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150489},
doi = {10.1145/1150402.1150489},
abstract = {We present FEMine, an automatic system for image-based gene expression analysis. We perform experiments on the largest publicly available collection of Drosophila ISH (in situ hybridization) images, showing that our FEMine system achieves excellent performance in classification, clustering, and content-based image retrieval. The major innovation of FEMine is the use of automatically discovered latent spatial "themes" of gene expressions, LGEs, in the whole-embryo context, as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {693–698},
numpages = {6},
keywords = {gene expression, drosophila, independent component analysis, embryonic image analysis},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150490,
author = {Park, Seung-Taek and Pennock, David and Madani, Omid and Good, Nathan and DeCoste, Dennis},
title = {Na\"{\i}ve Filterbots for Robust Cold-Start Recommendations},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150490},
doi = {10.1145/1150402.1150490},
abstract = {The goal of a recommender system is to suggest items of interest to a user based on historical behavior of a community of users. Given detailed enough history, item-based collaborative filtering (CF) often performs as well or better than almost any other recommendation method. However, in cold-start situations - where a user, an item, or the entire system is new - simple non-personalized recommendations often fare better. We improve the scalability and performance of a previous approach to handling cold-start situations that uses filterbots, or surrogate users that rate items based only on user or item attributes. We show that introducing a very small number of simple filterbots helps make CF algorithms more robust. In particular, adding just seven global filterbots improves both user-based and item-based CF in cold-start user, cold-start item, and cold-start system settings. Performance is better when data is scarce, performance is no worse when data is plentiful, and algorithm efficiency is negligibly affected. We systematically compare a non-personalized baseline, user-based CF, item-based CF, and our bot-augmented user- and item-based CF algorithms using three data sets (Yahoo! Movies, MovieLens, and EachMovie) with the normalized MAE metric in three types of cold-start situations. The advantage of our "na\"{\i}ve filterbot" approach is most pronounced for the Yahoo! data, the sparsest of the three data sets.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {699–705},
numpages = {7},
keywords = {na\"{\i}ve filterbots, collaborative filtering, cold start, recommender systems, performance analysis, hybrid content and collaborative filtering, robustness},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150491,
author = {Spiliopoulou, Myra and Ntoutsi, Irene and Theodoridis, Yannis and Schult, Rene},
title = {MONIC: Modeling and Monitoring Cluster Transitions},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150491},
doi = {10.1145/1150402.1150491},
abstract = {There is much recent work on detecting and tracking change in clusters, often based on the study of the spatiotemporal properties of a cluster. For the many applications where cluster change is relevant, among them customer relationship management, fraud detection and marketing, it is also necessary to provide insights about the nature of cluster change: Is a cluster corresponding to a group of customers simply disappearing or are its members migrating to other clusters? Is a new emerging cluster reflecting a new target group of customers or does it rather consist of existing customers whose preferences shift? To answer such questions, we propose the framework MONIC for modeling and tracking of cluster transitions. Our cluster transition model encompasses changes that involve more than one cluster, thus allowing for insights on cluster change in the whole clustering. Our transition tracking mechanism is not based on the topological properties of clusters, which are only available for some types of clustering, but on the contents of the underlying data stream. We present our first results on monitoring cluster transitions over the ACM digital library.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {706–711},
numpages = {6},
keywords = {temporal analysis, data streams, clusters, cluster transitions, cluster change detection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150492,
author = {Suchanek, Fabian M. and Ifrim, Georgiana and Weikum, Gerhard},
title = {Combining Linguistic and Statistical Analysis to Extract Relations from Web Documents},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150492},
doi = {10.1145/1150402.1150492},
abstract = {The World Wide Web provides a nearly endless source of knowledge, which is mostly given in natural language. A first step towards exploiting this data automatically could be to extract pairs of a given semantic relation from text documents - for example all pairs of a person and her birthdate. One strategy for this task is to find text patterns that express the semantic relation, to generalize these patterns, and to apply them to a corpus to find new pairs. In this paper, we show that this approach profits significantly when deep linguistic structures are used instead of surface text patterns. We demonstrate how linguistic structures can be represented for machine learning, and we provide a theoretical analysis of the pattern matching approach. We show the benefits of our approach by extensive experiments with our prototype system LEILA.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {712–717},
numpages = {6},
keywords = {relation extraction, pattern matching, machine learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150493,
author = {Tan, Bin and Shen, Xuehua and Zhai, ChengXiang},
title = {Mining Long-Term Search History to Improve Search Accuracy},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150493},
doi = {10.1145/1150402.1150493},
abstract = {Long-term search history contains rich information about a user's search preferences, which can be used as search context to improve retrieval performance. In this paper, we study statistical language modeling based methods to mine contextual information from long-term search history and exploit it for a more accurate estimate of the query language model. Experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. The best performance is achieved when using clickthrough data of past searches that are related to the current query.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {718–723},
numpages = {6},
keywords = {query expansion, search history, context},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150494,
author = {Tsang, Ivor W. and Kocsor, Andras and Kwok, James T.},
title = {Efficient Kernel Feature Extraction for Massive Data Sets},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150494},
doi = {10.1145/1150402.1150494},
abstract = {Maximum margin discriminant analysis (MMDA) was proposed that uses the margin idea for feature extraction. It often outperforms traditional methods like kernel principal component analysis (KPCA) and kernel Fisher discriminant analysis (KFD). However, as in other kernel methods, its time complexity is cubic in the number of training points m, and is thus computationally inefficient on massive data sets. In this paper, we propose an (1+ε)2-approximation algorithm for obtaining the MMDA features by extending the core vector machines. The resultant time complexity is only linear in m, while its space complexity is independent of m. Extensive comparisons with the original MMDA, KPCA, and KFD on a number of large data sets show that the proposed feature extractor can improve classification accuracy, and is also faster than these kernel-based methods by more than an order of magnitude.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {724–729},
numpages = {6},
keywords = {extraction, SVM, kernel feature, scalability},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150495,
author = {Wang, Chao and Parthasarathy, Srinivasan},
title = {Summarizing Itemset Patterns Using Probabilistic Models},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150495},
doi = {10.1145/1150402.1150495},
abstract = {In this paper, we propose a novel probabilistic approach to summarize frequent itemset patterns. Such techniques are useful for summarization, post-processing, and end-user interpretation, particularly for problems where the resulting set of patterns are huge. In our approach items in the dataset are modeled as random variables. We then construct a Markov Random Fields (MRF) on these variables based on frequent itemsets and their occurrence statistics. The summarization proceeds in a level-wise iterative fashion. Occurrence statistics of itemsets at the lowest level are used to construct an initial MRF. Statistics of itemsets at the next level can then be inferred from the model. We use those patterns whose occurrence can not be accurately inferred from the model to augment the model in an iterative manner, repeating the procedure until all frequent itemsets can be modeled. The resulting MRF model affords a concise and useful representation of the original collection of itemsets. Extensive empirical study on real datasets show that the new approach can effectively summarize a large number of itemsets and typically significantly outperforms extant approaches.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {730–735},
numpages = {6},
keywords = {Markov random field, itemset pattern summarization, probabilistic graphical model},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150496,
author = {Wang, Haixun and Yin, Jian and Pei, Jian and Yu, Philip S. and Yu, Jeffrey Xu},
title = {Suppressing Model Overfitting in Mining Concept-Drifting Data Streams},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150496},
doi = {10.1145/1150402.1150496},
abstract = {Mining data streams of changing class distributions is important for real-time business decision support. The stream classifier must evolve to reflect the current class distribution. This poses a serious challenge. On the one hand, relying on historical data may increase the chances of learning obsolete models. On the other hand, learning only from the latest data may lead to biased classifiers, as the latest data is often an unrepresentative sample of the current class distribution. The problem is particularly acute in classifying rare events, when, for example, instances of the rare class do not even show up in the most recent training data. In this paper, we use a stochastic model to describe the concept shifting patterns and formulate this problem as an optimization one: from the historical and the current training data that we have observed, find the most-likely current distribution, and learn a classifier based on the most-likely distribution. We derive an analytic solution and approximate this solution with an efficient algorithm, which calibrates the influence of historical data carefully to create an accurate classifier. We evaluate our algorithm with both synthetic and real-world datasets. Our results show that our algorithm produces accurate and efficient classification.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {736–741},
numpages = {6},
keywords = {data streams, classifier, classifier ensemble, concept drift},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150497,
author = {Wedig, Steve and Madani, Omid},
title = {A Large-Scale Analysis of Query Logs for Assessing Personalization Opportunities},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150497},
doi = {10.1145/1150402.1150497},
abstract = {Query logs, the patterns of activity left by millions of users, contain a wealth of information that can be mined to aid personalization. We perform a large-scale study of Yahoo! search engine logs, tracking 1.35 million browser-cookies over a period of 6 months. We define metrics to address questions such as 1) How much history is available?, 2) How do users' topical interests vary, as reflected by their queries?, and 3) What can we learn from user clicks? We find that there is significantly more expected history for the user of a randomly picked query than for a randomly picked user. We show that users exhibit consistent topical interests that vary between users. We also see that user clicks indicate a variety of special interests. Our findings shed light on user activity and can inform future personalization efforts.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {742–747},
numpages = {6},
keywords = {user history, query logs, user interests, categorization, clustering, personalization},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150498,
author = {Wei, Li and Keogh, Eamonn},
title = {Semi-Supervised Time Series Classification},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150498},
doi = {10.1145/1150402.1150498},
abstract = {The problem of time series classification has attracted great interest in the last decade. However current research assumes the existence of large amounts of labeled training data. In reality, such data may be very difficult or expensive to obtain. For example, it may require the time and expertise of cardiologists, space launch technicians, or other domain specialists. As in many other domains, there are often copious amounts of unlabeled data available. For example, the PhysioBank archive contains gigabytes of ECG data. In this work we propose a semi-supervised technique for building time series classifiers. While such algorithms are well known in text domains, we will show that special considerations must be made to make them both efficient and effective for the time series domain. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, handwritten documents, and video datasets. The experimental results demonstrate that our approach requires only a handful of labeled examples to construct accurate classifiers.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {748–753},
numpages = {6},
keywords = {time series, semi-supervised learning, classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150499,
author = {Wong, Raymond Chi-Wing and Li, Jiuyong and Fu, Ada Wai-Chee and Wang, Ke},
title = {(α, k)-Anonymity: An Enhanced k-Anonymity Model for Privacy Preserving Data Publishing},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150499},
doi = {10.1145/1150402.1150499},
abstract = {Privacy preservation is an important issue in the release of data for mining purposes. The k-anonymity model has been introduced for protecting individual identification. Recent studies show that a more sophisticated model is necessary to protect the association of individuals to sensitive information. In this paper, we propose an (α, k)-anonymity model to protect both identifications and relationships to sensitive information in data. We discuss the properties of (α, k)-anonymity model. We prove that the optimal (α, k)-anonymity problem is NP-hard. We first presentan optimal global-recoding method for the (α, k)-anonymity problem. Next we propose a local-recoding algorithm which is more scalable and result in less data distortion. The effectiveness and efficiency are shown by experiments. We also describe how the model can be extended to more general case.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {754–759},
numpages = {6},
keywords = {data publishing, privacy preservation, anonymity, data mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150500,
author = {Wu, Gang and Chang, Edward and Chen, Yen Kuang and Hughes, Christoper},
title = {Incremental Approximate Matrix Factorization for Speeding up Support Vector Machines},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150500},
doi = {10.1145/1150402.1150500},
abstract = {Traditional decomposition-based solutions to Support Vector Machines (SVMs) suffer from the widely-known scalability problem. For example, given a one-million training set, it takes about six days for SVMLight to run on a Pentium-4 sever with 8G-byte memory. In this paper, we propose an incremental algorithm, which performs approximate matrix-factorization operations, to speed up SVMs. Two approximate factorization schemes, Kronecker and incomplete Cholesky, are utilized in the primal-dual interior-point method (IPM) to directly solve the quadratic optimization problem in SVMs. We found out that a coarse approximate algorithm enjoys good speedup performance but may suffer from poor training accuracy. Conversely, a fine-grained approximate algorithm enjoys good training quality but may suffer from long training time. We subsequently propose an incremental training algorithm, which uses the approximate IPM solution of a coarse factorization to initialize the IPM of a fine-grained factorization. Extensive empirical studies show that our proposed incremental algorithm with approximate factorizations substantially speeds up SVM training while maintaining high training accuracy. In addition, we show that our proposed algorithm is highly parallelizable on an Intel dual-coreprocessor.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {760–766},
numpages = {7},
keywords = {matrix factorization, support vector machines, interior-point method},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150501,
author = {Wu, Mingxi and Jermaine, Christopher},
title = {Outlier Detection by Sampling with Accuracy Guarantees},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150501},
doi = {10.1145/1150402.1150501},
abstract = {An effective approach to detecting anomalous points in a data set
is distance-based outlier detection. This paper describes a simple
sampling algorithm to effciently detect distance-based outliers in
domains where each and every distance computation is very
expensive. Unlike any existing algorithms, the sampling algorithm
requires a xed number of distance computations and can return good
results with accuracy guarantees. The most computationally
expensive aspect of estimating the accuracy of the result is
sorting all of the distances computed by the sampling algorithm.
The experimental study on two expensive domains as well as ten
additional real-life datasets demonstrates both the effciency and
effectiveness of the sampling algorithm in comparison with the
state-of-the-art algorithm and there liability of the accuracy
guarantees.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {767–772},
numpages = {6},
keywords = {ensemble method, active learning, outlier detection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150502,
author = {Xin, Dong and Shen, Xuehua and Mei, Qiaozhu and Han, Jiawei},
title = {Discovering Interesting Patterns through User's Interactive Feedback},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150502},
doi = {10.1145/1150402.1150502},
abstract = {In this paper, we study the problem of discovering interesting patterns through user's interactive feedback. We assume a set of candidate patterns (ie, frequent patterns) has already been mined. Our goal is to help a particular user effectively discover interesting patterns according to his specific interest. Without requiring a user to explicitly construct a prior knowledge to measure the interestingness of patterns, we learn the user's prior knowledge from his interactive feedback. We propose two models to represent a user's prior: the log linear model and biased belief model. The former is designed for item-set patterns, whereas the latter is also applicable to sequential and structural patterns. To learn these models, we present a two-stage approach, progressive shrinking and clustering, to select sample patterns for feedback. The experimental results on real and synthetic data sets demonstrate the effectiveness of our approach.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {773–778},
numpages = {6},
keywords = {pattern discovery, interactive feedback},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150503,
author = {Xiong, Hui and Wu, Junjie and Chen, Jian},
title = {K-Means Clustering versus Validation Measures: A Data Distribution Perspective},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150503},
doi = {10.1145/1150402.1150503},
abstract = {K-means is a widely used partitional clustering method. While there are considerable research efforts to characterize the key features of K-means clustering, further investigation is needed to reveal whether and how the data distributions can have the impact on the performance of K-means clustering. Indeed, in this paper, we revisit the K-means clustering problem by answering three questions. First, how the "true" cluster sizes can make impact on the performance of K-means clustering? Second, is the entropy an algorithm-independent validation measure for K-means clustering? Finally, what is the distribution of the clustering results by K-means? To that end, we first illustrate that K-means tends to generate the clusters with the relatively uniform distribution on the cluster sizes. In addition, we show that the entropy measure, an external clustering validation measure, has the favorite on the clustering algorithms which tend to reduce high variation on the cluster sizes. Finally, our experimental results indicate that K-means tends to produce the clusters in which the variation of the cluster sizes, as measured by the Coefficient of Variation(CV), is in a specific range, approximately from 0.3 to 1.0.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {779–784},
numpages = {6},
keywords = {coefficient of variation (CV), K-means clustering, entropy},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150504,
author = {Xu, Jian and Wang, Wei and Pei, Jian and Wang, Xiaoyuan and Shi, Baile and Fu, Ada Wai-Chee},
title = {Utility-Based Anonymization Using Local Recoding},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150504},
doi = {10.1145/1150402.1150504},
abstract = {Privacy becomes a more and more serious concern in applications involving microdata. Recently, efficient anonymization has attracted much research work. Most of the previous methods use global recoding, which maps the domains of the quasi-identifier attributes to generalized or changed values. However, global recoding may not always achieve effective anonymization in terms of discernability and query answering accuracy using the anonymized data. Moreover, anonymized data is often for analysis. As well accepted in many analytical applications, different attributes in a data set may have different utility in the analysis. The utility of attributes has not been considered in the previous methods.In this paper, we study the problem of utility-based anonymization. First, we propose a simple framework to specify utility of attributes. The framework covers both numeric and categorical data. Second, we develop two simple yet efficient heuristic local recoding methods for utility-based anonymization. Our extensive performance study using both real data sets and synthetic data sets shows that our methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. Furthermore, our utility-based method can boost the quality of analysis using the anonymized data.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–790},
numpages = {6},
keywords = {k-anonymity, data mining, local recoding, privacy preservation, utility},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150505,
author = {Yoo, Illhoi and Hu, Xiaohua and Song, Il-Yeol},
title = {Integration of Semantic-Based Bipartite Graph Representation and Mutual Refinement Strategy for Biomedical Literature Clustering},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150505},
doi = {10.1145/1150402.1150505},
abstract = {We introduce a novel document clustering approach that overcomes those problems by combining a semantic-based bipartite graph representation and a mutual refinement strategy. The primary contributions of this paper are the following. First, we introduce a new representation of documents using a bipartite graph between documents and co-occurrence concepts in the documents. Second, we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering results. Third, through the experiments on MEDLINE documents, we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering methods. Our approach improves on the average 29.5 cluster quality and 26.3 clustering reliability, in terms of misclassification index, over Bisecting K-means with the best parameters.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {791–796},
numpages = {6},
keywords = {bipartite graph representation, ontology, mutual refinement strategy, document clustering},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150506,
author = {Zeng, Zhiping and Wang, Jianyong and Zhou, Lizhu and Karypis, George},
title = {Coherent Closed Quasi-Clique Discovery from Large Dense Graph Databases},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150506},
doi = {10.1145/1150402.1150506},
abstract = {Frequent coherent subgraphs can provide valuable knowledge about the underlying internal structure of a graph database, and mining frequently occurring coherent subgraphs from large dense graph databases has been witnessed several applications and received considerable attention in the graph mining community recently. In this paper, we study how to efficiently mine the complete set of coherent closed quasi-cliques from large dense graph databases, which is an especially challenging task due to the downward-closure property no longer holds. By fully exploring some properties of quasi-cliques, we propose several novel optimization techniques, which can prune the unpromising and redundant sub-search spaces effectively. Meanwhile, we devise an efficient closure checking scheme to facilitate the discovery of only closed quasi-cliques. We also develop a coherent closed quasi-clique mining algorithm, <b>Cocain</b>1 Thorough performance study shows that Cocain is very efficient and scalable for large dense graph databases.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {797–802},
numpages = {6},
keywords = {quasi-clique, coherent subgraph, graph mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150507,
author = {Zhang, Minghua and Hsu, Wynne and Lee, Mong Li},
title = {Mining Progressive Confident Rules},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150507},
doi = {10.1145/1150402.1150507},
abstract = {Many real world objects have states that change over time. By tracking the state sequences of these objects, we can study their behavior and take preventive measures before they reach some undesirable states. In this paper, we propose a new kind of pattern called progressive confident rules to describe sequences of states with an increasing confidence that lead to a particular end state. We give a formal definition of progressive confident rules and their concise set. We devise pruning strategies to reduce the enormous search space. Experiment result shows that the proposed algorithm is efficient and scalable. We also demonstrate the application of progressive confident rules in classification.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {803–808},
numpages = {6},
keywords = {progressive confident, classification, sequence},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150508,
author = {Zhang, Sheng and Chakrabarti, Amit and Ford, James and Makedon, Fillia},
title = {Attack Detection in Time Series for Recommender Systems},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150508},
doi = {10.1145/1150402.1150508},
abstract = {Recent research has identified significant vulnerabilities in recommender systems. Shilling attacks, in which attackers introduce biased ratings in order to influence future recommendations, have been shown to be effective against collaborative filtering algorithms. We postulate that the distribution of item ratings in time can reveal the presence of a wide range of shilling attacks given reasonable assumptions about their duration. To construct a time series of ratings for an item, we use a window size of k to group consecutive ratings for the item into disjoint windows and compute the sample average and sample entropy in each window. We derive a theoretically optimal window size to best detect an attack event if the number of attack profiles is known. For practical applications where this number is unknown, we propose a heuristic algorithm that adaptively changes the window size. Our experimental results demonstrate that monitoring rating distributions in time series is an effective approach for detecting shilling attacks.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {809–814},
numpages = {6},
keywords = {time series, recommender systems, shilling attacks, anomaly detection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150509,
author = {Zhang, Shichao and Chen, Feng and Wu, Xindong and Zhang, Chengqi},
title = {Identifying Bridging Rules between Conceptual Clusters},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150509},
doi = {10.1145/1150402.1150509},
abstract = {A bridging rule in this paper has its antecedent and action from different conceptual clusters. We first design two algorithms for mining bridging rules between clusters in a database, and then propose two non-linear metrics for measuring the interestingness of bridging rules. Bridging rules can be distinct from association rules (or frequent itemsets). This is because (1) bridging rules can be generated by infrequent itemsets that are pruned in association rule mining; and (2) bridging rules are measured by the importance that includes the distance between two conceptual clusters, whereas frequent itemsets are measured by only the support.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {815–820},
numpages = {6},
keywords = {bridging rule, outlier, clustering, entropy, association rule},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150510,
author = {Zhang, Tong and Popescul, Alexandrin and Dom, Byron},
title = {Linear Prediction Models with Graph Regularization for Web-Page Categorization},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150510},
doi = {10.1145/1150402.1150510},
abstract = {We present a risk minimization formulation for learning from both text and graph structures which is motivated by the problem of collective inference for hypertext document categorization. The method is based on graph regularization formulated as a well-formed convex optimization problem. We present numerical algorithms for our formulation, and show that such combination of local text features and link information can lead to improved predictive accuracy.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {821–826},
numpages = {6},
keywords = {collective inference, semi-supervised learning, regularization, graph and relational learning, document classification},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150511,
author = {Zhao, Lizhuang and Zaki, Mohammed J. and Ramakrishnan, Naren},
title = {BLOSOM: A Framework for Mining Arbitrary Boolean Expressions},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150511},
doi = {10.1145/1150402.1150511},
abstract = {We introduce a novel framework, called BLOSOM, for mining (frequent) boolean expressions over binary-valued datasets. We organize the space of boolean expressions into four categories: pure conjunctions, pure disjunctions, conjunction of disjunctions, and disjunction of conjunctions. We focus on mining the simplest expressions the minimal generators for each class. We also propose a closure operator for each class that yields closed boolean expressions. BLOSOM efficiently mines frequent boolean expressions by utilizing a number of methodical pruning techniques. Experiments showcase the behavior of BLOSOM, and an application study on a real dataset is also given.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {827–832},
numpages = {6},
keywords = {boolean expression, closed itemsets, data mining, minimal generator},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150513,
author = {Jonas, Jeff},
title = {Introducing Perpetual Analytics},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150513},
doi = {10.1145/1150402.1150513},
abstract = {Common strategies to liberate an organization's information assets for situational awareness frequently rely on infrastructure components such as data integration, enterprise search, federation, data warehousing, and so on. And while these traditional platforms enable analysts to get better and faster answers to their queries, the next big advance will change this paradigm. Users cannot be expected to formulate and ask every smart question every day. And to escape this impractical and un-scalable model, the new paradigm will involve technologies where "the data finds the data" and "relevance finds the user." Perpetual Analytics describes a class of application whereby enterprise context is assembled, in real-time, on data streams as fast as operational systems record observations. Context construction is a "data finds the data" activity which enables events of interest to be streamed to subscribers. In this talk, I will talk at some depth about the dynamics of such systems including scalability and sustainability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {833},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150514,
author = {Kahn, William},
title = {Capital One's Statistical Problems: Our Top Ten List},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150514},
doi = {10.1145/1150402.1150514},
abstract = {Capital One is a highly quantitatively driven diversified financial services firm. As such, we make broad and deep use of the entire repertory of highly quantitative techniques. This talk will present our top ten statistical problems. Indeed, one of them has, as a sub point, the data mining dimension, but it will likely be useful for data miners to see how their research needs to complement, and fit into, the entire range of hard statistical issues.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {834},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150515,
author = {McCallum, Andrew},
title = {Information Extraction, Data Mining and Joint Inference},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150515},
doi = {10.1145/1150402.1150515},
abstract = {Although information extraction and data mining appear together in many applications, their interface in most current systems would better be described as serial juxtaposition than as tight integration. Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually not aware of the emerging patterns and regularities in the database. Data mining methods begin from a populated database, and are often unaware of where the data came from, or its inherent uncertainties. The result is that the accuracy of both suffers, and accurate mining of complex text sources has been beyond reach.In this talk I will describe work in probabilistic models that perform joint inference across multiple components of an information processing pipeline in order to avoid the brittle accumulation of errors. After briefly introducing conditional random fields, I will describe recent work in information extraction leveraging factorial state representations, entity resolution, and transfer learning, as well as scalable methods of inference and learning. I'll close with some recent work on probabilistic models for social network analysis, and a demonstration of Rexa.info, a new research paper search engine.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {835},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150516,
author = {Cavaretta, Michael},
title = {Data Mining Challenges in the Automotive Domain},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150516},
doi = {10.1145/1150402.1150516},
abstract = {Automotive companies, such as Ford Motor Company, have no shortage of large databases with abundant opportunities for cost reduction and revenue enhancement. The Data Mining Group at Ford has worked in the areas of Quality, Customer Satisfaction and Warranty Analytics for close to ten years. In this time, we have developed a number of methods for building systems to help the business. One area of particular success has been in warranty analysis. While traditional hazard analysis has been applied at Ford for a number of years, we have used techniques from other industries (e.g. retail), as well as text mining to view warranty analytics in a new way. However, our success has been tempered by serious challenges particularly in the areas of data understanding, computing meaningful aggregations and implementation. Case studies from the automobile industry (warranty, quality, forecasting, etc.) as well as from other industries will be used.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {836},
numpages = {1},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150518,
author = {Bi, Jinbo and Periaswamy, Senthil and Okada, Kazunori and Kubota, Toshiro and Fung, Glenn and Salganicoff, Marcos and Rao, R. Bharat},
title = {Computer Aided Detection via Asymmetric Cascade of Sparse Hyperplane Classifiers},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150518},
doi = {10.1145/1150402.1150518},
abstract = {This paper describes a novel classification method for computer aided detection (CAD) that identifies structures of interest from medical images. CAD problems are challenging largely due to the following three characteristics. Typical CAD training data sets are large and extremely unbalanced between positive and negative classes. When searching for descriptive features, researchers often deploy a large set of experimental features, which consequently introduces irrelevant and redundant features. Finally, a CAD system has to satisfy stringent real-time requirements.This work is distinguished by three key contributions. The first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfactory false positive rates. The second is the incorporation of feature computational costs in a linear program formulation that allows the feature selection process to take into account different evaluation costs of various features. The third is a boosting algorithm derived from column generation optimization to effectively solve the proposed cascade linear programs.We apply the proposed approach to the problem of detecting lung nodules from helical multi-slice CT images. Our approach demonstrates superior performance in comparison against support vector machines, linear discriminant analysis and cascade AdaBoost. Especially, the resulting detection system is significantly sped up with our approach.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {837–844},
numpages = {8},
keywords = {sparse solutions, mathematical programming, support vector machines, cascading classification, computer aided detection},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150519,
author = {Castano, Rebecca and Mazzoni, Dominic and Tang, Nghia and Greeley, Ron and Doggett, Thomas and Cichy, Ben and Chien, Steve and Davies, Ashley},
title = {Onboard Classifiers for Science Event Detection on a Remote Sensing Spacecraft},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150519},
doi = {10.1145/1150402.1150519},
abstract = {Typically, data collected by a spacecraft is downlinked to Earth and preprocessed before any analysis is performed. We have developed classifiers that can be used onboard a spacecraft to identify high priority data for downlink to Earth, providing a method for maximizing the use of a potentially bandwidth limited downlink channel. Onboard analysis can also enable rapid reaction to dynamic events, such as flooding, volcanic eruptions or sea ice break-up.Four classifiers were developed to identify cryosphere events using hyperspectral images. These classifiers include a manually constructed classifier, a Support Vector Machine (SVM), a Decision Tree and a classifier derived by searching over combinations of thresholded band ratios. Each of the classifiers was designed to run in the computationally constrained operating environment of the spacecraft. A set of scenes was hand-labeled to provide training and testing data. Performance results on the test data indicate that the SVM and manual classifiers outperformed the Decision Tree and band-ratio classifiers with the SVM yielding slightly better classifications than the manual classifier.The manual and SVM classifiers have been uploaded to the EO-1 spacecraft and have been running onboard the spacecraft for over a year. Results of the onboard analysis are used by the Autonomous Sciencecraft Experiment (ASE) of NASA's New Millennium Program onboard EO-1 to automatically target the spacecraft to collect follow-on imagery. The software demonstrates the potential for future deep space missions to use onboard decision making to capture short-lived science events.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {845–851},
numpages = {7},
keywords = {constrained processing environment, classification, support vector machine},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150520,
author = {Forman, George and Kirshenbaum, Evan and Suermondt, Jaap},
title = {Pragmatic Text Mining: Minimizing Human Effort to Quantify Many Issues in Call Logs},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150520},
doi = {10.1145/1150402.1150520},
abstract = {We discuss our experiences in analyzing customer-support issues from the unstructured free-text fields of technical-support call logs. The identification of frequent issues and their accurate quantification is essential in order to track aggregate costs broken down by issue type, to appropriately target engineering resources, and to provide the best diagnosis, support and documentation for most common issues. We present a new set of techniques for doing this efficiently on an industrial scale, without requiring manual coding of calls in the call center. Our approach involves (1) a new text clustering method to identify common and emerging issues; (2) a method to rapidly train large numbers of categorizers in a practical, interactive manner; and (3) a method to accurately quantify categories, even in the face of inaccurate classifications and training sets that necessarily cannot match the class distribution of each new month's data. We present our methodology and a tool we developed and deployed that uses these methods for tracking ongoing support issues and discovering emerging issues at HP.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {852–861},
numpages = {10},
keywords = {text mining, quantification, text classification, log processing, supervised machine learning, applications},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150521,
author = {Hettich, Seth and Pazzani, Michael J.},
title = {Mining for Proposal Reviewers: Lessons Learned at the National Science Foundation},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150521},
doi = {10.1145/1150402.1150521},
abstract = {In this paper, we discuss a prototype application deployed at the U.S. National Science Foundation for assisting program directors in identifying reviewers for proposals. The application helps program directors sort proposals into panels and find reviewers for proposals. To accomplish these tasks, it extracts information from the full text of proposals both to learn about the topics of proposals and the expertise of reviewers. We discuss a variety of alternatives that were explored, the solution that was implemented, and the experience in using the solution within the workflow of NSF.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {862–871},
numpages = {10},
keywords = {similarity functions, information retrieval, clustering, keyword extraction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150522,
author = {Liu, Chao and Chen, Chen and Han, Jiawei and Yu, Philip S.},
title = {GPLAG: Detection of Software Plagiarism by Program Dependence Graph Analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150522},
doi = {10.1145/1150402.1150522},
abstract = {Along with the blossom of open source projects comes the convenience for software plagiarism. A company, if less self-disciplined, may be tempted to plagiarize some open source projects for its own products. Although current plagiarism detection tools appear sufficient for academic use, they are nevertheless short for fighting against serious plagiarists. For example, disguises like statement reordering and code insertion can effectively confuse these tools. In this paper, we develop a new plagiarism detection tool, called GPLAG, which detects plagiarism by mining program dependence graphs (PDGs). A PDG is a graphic representation of the data and control dependencies within a procedure. Because PDGs are nearly invariant during plagiarism, GPLAG is more effective than state-of-the-art tools for plagiarism detection. In order to make GPLAG scalable to large programs, a statistical lossy filter is proposed to prune the plagiarism search space. Experiment study shows that GPLAG is both effective and efficient: It detects plagiarism that easily slips over existing tools, and it usually takes a few seconds to find (simulated) plagiarism in programs having thousands of lines of code.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {872–881},
numpages = {10},
keywords = {program dependence graph, software plagiarism detection, graph mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150523,
author = {Moerchen, Fabian and Mierswa, Ingo and Ultsch, Alfred},
title = {Understandable Models Of Music Collections Based on Exhaustive Feature Generation with Temporal Statistics},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150523},
doi = {10.1145/1150402.1150523},
abstract = {Data mining in large collections of polyphonic music has recently received increasing interest by companies along with the advent of commercial online distribution of music. Important applications include the categorization of songs into genres and the recommendation of songs according to musical similarity and the customer's musical preferences. Modeling genre or timbre of polyphonic music is at the core of these tasks and has been recognized as a difficult problem. Many audio features have been proposed, but they do not provide easily understandable descriptions of music. They do not explain why a genre was chosen or in which way one song is similar to another. We present an approach that combines large scale feature generation with meta learning techniques to obtain meaningful features for musical similarity. We perform exhaustive feature generation based on temporal statistics and train regression models to summarize a subset of these features into a single descriptor of a particular notion of music. Using several such models we produce a concise semantic description of each song. Genre classification models based on these semantic features are shown to be better understandable and almost as accurate as traditional methods.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {882–891},
numpages = {10},
keywords = {genre classification, meta learning, feature generation, logistic regression, music mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150524,
author = {Zhao, Kaidi and Liu, Bing and Benkler, Jeffrey and Xiao, Weimin},
title = {Opportunity Map: Identifying Causes of Failure - a Deployed Data Mining System},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150524},
doi = {10.1145/1150402.1150524},
abstract = {In this paper, we report a deployed data mining application system for Motorola. Originally, its intended use was for identifying causes of cellular phone failures, but it has been found to be useful for many other engineering data sets as well. For this report, the case study is a dataset containing cellular phone call records. This data set is like any dataset used in classification applications, i.e., with a set of attributes which can be continuous or discrete, and a discrete class attribute. In our application, the classes are normally ended calls, calls which failed to setup, and calls which failed while in progress. However, the task is not to predict any failure, but to identify possible causes that resulted in failures. Then, engineering efforts may focus on improvements that can be made to the phones. In the course of the project, various classification techniques, e.g., decision trees, na\"{\i}ve Bayesian classification and SVM were tried. However, the results were unsatisfactory. After several demonstrations and interaction with domain experts, we finally designed and implemented an effective approach to perform the task. The final system is based on class association rules, general impressions and visualization. The system has been deployed and is in regular use at Motorola. In this paper, we first describe our experiences with some existing classification systems and discuss why they are not suitable for the task. We then present our techniques. As an illustration, we show several visualization screens in the case study, which reveal some important knowledge. Due to confidentiality, we will not give specifics but only present a general discussion about the results.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {892–901},
numpages = {10},
keywords = {general impressions, visual data mining, class association rules},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150526,
author = {Agichtein, Eugene and Zheng, Zijian},
title = {Identifying "Best Bet" Web Search Results by Mining Past User Behavior},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150526},
doi = {10.1145/1150402.1150526},
abstract = {The top web search result is crucial for user satisfaction with the web search experience. We argue that the importance of the relevance at the top position necessitates special handling of the top web search result for some queries. We propose an effective approach of leveraging millions of past user interactions with a web search engine to automatically detect "best bet" top results preferred by majority of users. Interestingly, this problem can be more effectively addressed with classification than using state-of-the-art general ranking methods. Furthermore, we show that our general machine learning approach achieves precision comparable to a heavily tuned, domain-specific algorithm, with significantly higher coverage. Our experiments over millions of user interactions for thousands of queries demonstrate the effectiveness and robustness of our techniques.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {902–908},
numpages = {7},
keywords = {user behavior mining, web search ranking, web usage mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150527,
author = {Caruana, Rich and Elhawary, Mohamed and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Fink, Daniel and Hochachka, Wesley M. and Kelling, Steve},
title = {Mining Citizen Science Data to Predict Orevalence of Wild Bird Species},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150527},
doi = {10.1145/1150402.1150527},
abstract = {The Cornell Laboratory of Ornithology's mission is to interpret and conserve the earth's biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and potentially are very noisy. The ecologists are interested in identifying which features have the strongest effect on the distribution and abundance of bird species as well as describing the forms of these relationships. We show how data mining can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {909–915},
numpages = {7},
keywords = {sensitivity analysis, partial dependence function, model inspection, decision trees, bagging, attribute importance},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150528,
author = {Etienne, Julien and Wachmann, Bernd and Zhang, Lei},
title = {A Component-Based Framework for Knowledge Discovery in Bioinformatics},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150528},
doi = {10.1145/1150402.1150528},
abstract = {Motivation: In the field of bioinformatics there is an emerging need to integrate all knowledge discovery steps into a standardized modular framework. Indeed, component-based development can significantly enhance reusability and productivity for short timeline projects with a small team. We present Interactive Knowledge Discovery and Data mining (iKDD), an application framework written in Java that was specifically designed for these purposes.Results: iKDD consists of a component-based architecture and a web-based tool for pre-clinical research and prototype development. The platform provides an intuitive and consistent interface to create and maintain components, e.g., data structures, algorithms and utilities to load, save and visualize data and pipelines. The rich-featured tool supplies database connectivity, workflow processing and rapid prototype building. The architecture was carefully designed using an object-oriented approach that respects crucial goals: usability, openness, robustness and functionality, especially in the abstraction and description of the components, which distinguishes it from other packages. iKDD is well-suited to serve as a public repository of components, to run scientific experiments with a high-level of reproducibility, and also to rapidly build prototypes. This paper describes the general architecture, and demonstrates through examples the ease by which a complex scenario implementation can be facilitated with iKDD.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {916–921},
numpages = {6},
keywords = {data mining, workflow, bioinformatics},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150529,
author = {Gao, Byron J. and Griffith, Obi L. and Ester, Martin and Jones, Steven J. M.},
title = {Discovering Significant OPSM Subspace Clusters in Massive Gene Expression Data},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150529},
doi = {10.1145/1150402.1150529},
abstract = {Order-preserving submatrixes (OPSMs) have been accepted as a biologically meaningful subspace cluster model, capturing the general tendency of gene expressions across a subset of conditions. In an OPSM, the expression levels of all genes induce the same linear ordering of the conditions. OPSM mining is reducible to a special case of the sequential pattern mining problem, in which a pattern and its supporting sequences uniquely specify an OPSM cluster. Those small twig clusters, specified by long patterns with naturally low support, incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes, which are common in today's gene expression analysis. However, it is in particular interest of biologists to reveal such small groups of genes that are tightly coregulated under many conditions, and some pathways or processes might require only two genes to act in concert. In this paper, we introduce the KiWi mining framework for massive datasets, that exploits two parameters k and w to provide a biased testing on a bounded number of candidates, substantially reducing the search space and problem scale, targeting on highly promising seeds that lead to significant clusters and twig clusters. Extensive biological and computational evaluations on real datasets demonstrate that KiWi can effectively mine biologically meaningful OPSM subspace clusters with good efficiency and scalability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {922–928},
numpages = {7},
keywords = {scalability, twig cluster, gene expression data, subspace clustering, order-preserving submatrix},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150530,
author = {Ling, Charles X. and Sheng, Victor S. and Bruckhaus, Tilmann and Madhavji, Nazim H.},
title = {Maximum Profit Mining and Its Application in Software Development},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150530},
doi = {10.1145/1150402.1150530},
abstract = {While most software defects (i.e., bugs) are corrected and tested as part of the lengthy software development cycle, enterprise software vendors often have to release software products before all reported defects are corrected, due to deadlines and limited resources. A small number of these defects will be escalated by customers and they must be resolved immediately by the software vendors at a very high cost. In this paper, we develop an Escalation Prediction (EP) system that mines historic defect report data and predict the escalation risk of the defects for maximum net profit. More specifically, we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning. We then apply and compare several well-known cost-sensitive learning approaches for EP. Our experiments suggest that the cost-sensitive decision tree is the best method for producing the highest positive net profit and comprehensible results. The EP system has been deployed successfully in the product group of an enterprise software vendor.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {929–934},
numpages = {6},
keywords = {escalation prediction, data mining, cost-sensitive learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150531,
author = {Mierswa, Ingo and Wurst, Michael and Klinkenberg, Ralf and Scholz, Martin and Euler, Timm},
title = {YALE: Rapid Prototyping for Complex Data Mining Tasks},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150531},
doi = {10.1145/1150402.1150531},
abstract = {KDD is a complex and demanding task. While a large number of methods has been established for numerous problems, many challenges remain to be solved. New tasks emerge requiring the development of new methods or processing schemes. Like in software development, the development of such solutions demands for careful analysis, specification, implementation, and testing. Rapid prototyping is an approach which allows crucial design decisions as early as possible. A rapid prototyping system should support maximal re-use and innovative combinations of existing methods, as well as simple and quick integration of new ones.This paper describes Yale, a free open-source environment forKDD and machine learning. Yale provides a rich variety of methods whichallows rapid prototyping for new applications and makes costlyre-implementations unnecessary. Additionally, Yale offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool. Following the paradigm of visual programming eases the design of processing schemes. While the graphical user interface supports interactive design, the underlying XML representation enables automated applications after the prototyping phase.After a discussion of the key concepts of Yale, we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization. These case studies cover tasks like feature engineering, text mining, data stream mining and tracking drifting concepts, ensemble methods and distributed data mining. This variety of applications is also reflected in a broad user base, we counted more than 40,000 downloads during the last twelve months.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {935–940},
numpages = {6},
keywords = {result visualization, rapid prototyping, multimedia mining, feature construction, distributed data mining, data stream mining, audio and text mining, data pre-processing, KDD system},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150532,
author = {Virdhagriswaran, Sankar and Dakin, Gordon},
title = {Camouflaged Fraud Detection in Domains with Complex Relationships},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150532},
doi = {10.1145/1150402.1150532},
abstract = {We describe a data mining system to detect frauds that are camouflaged to look like normal activities in domains with high number of known relationships. Examples include accounting fraud detection for rating and investment, insider attacks on corporate networks, and health care insurance fraud. Our goal is to help analysts who are overwhelmed with information about companies or on-line system access logs or insurance claims to focus their attentions on features that cause damage in the future. We focused on accounting fraud where the task is to detect the subset of companies that were potentially committing accounting fraud within the total population of public companies that file quarterly and annual filings with the Securities and Exchange Commission (SEC). Using (a) Representation of changes, (b) A mix of decision tree learning, locally weighted logistic regression, k-means clustering, and constant regression in a two phase pipe line, we developed models that rank companies based on the probability of forecasting future damaging performance. The learned models were tested extensively over four years with public data available from SEC filings and private data available from rating companies and investment firms. Cross validation experiments and analyst based validation of private experiments were found to show that the approach performed as well as or better than domain experts and discovered new relationships that domain experts did not use on a regular basis. Finally, the detections preceded public knowledge of such problems by six to eighteen months.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {941–947},
numpages = {7},
keywords = {on-line intrusion analysis, accounting fraud},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150533,
author = {Yan, Lian and Baldasare, Patrick},
title = {Beyond Classification and Ranking: Constrained Optimization of the ROI},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150533},
doi = {10.1145/1150402.1150533},
abstract = {Classification has been commonly used in many data mining projects in the financial service industry. For instance, to predict collectability of accounts receivable, a binary class label is created based on whether a payment is received within a certain period. However, optimization of the classifier does not necessarily lead to maximization of return on investment (ROI), since maximization of the true positive rate is often different from maximization of the collectable amount which determines the ROI under a fixed budget constraint. The typical cost sensitive learning does not solve this problem either since it involves an unknown opportunity cost due to the budget constraint. Learning the ranks of collectable amount would ultimately solve the problem, but it tries to tackle an unnecessarily difficult problem and often results in poorer results for our specific target. We propose a new algorithm that uses gradient descent to directly optimize the related monetary measure under the budget constraint and thus maximizes the ROI. By comparison with several classification, regression, and ranking algorithms, we demonstrate the new algorithm's substantial improvement of the financial impact on our clients in the financial service industry.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {948–953},
numpages = {6},
keywords = {neural networks, constrained optimization, return on investment},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@inproceedings{10.1145/1150402.1150535,
author = {Piatetsky-Shapiro, Gregory and Grossman, Robert and Djeraba, Chabane and Feldman, Ronen and Getoor, Lise and Zaki, Mohammed},
title = {Is There a Grand Challenge or X-Prize for Data Mining?},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150535},
doi = {10.1145/1150402.1150535},
abstract = {This panel will discuss possible exciting and motivating Grand Challenge problems for Data Mining, focusing on bioinformatics, multimedia mining, link mining, text mining, and web mining.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {954–956},
numpages = {3},
keywords = {data mining, link mining, text mining, X-prize, image mining, multimedia mining, bioinformatics, web mining, grand challenge, video mining},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

