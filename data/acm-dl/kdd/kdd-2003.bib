@inproceedings{10.1145/956750.956752,
author = {Gray, Jim},
title = {On-Line Science: The World-Wide Telescope as a Prototype for the New Computational Science},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956752},
doi = {10.1145/956750.956752},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {3},
numpages = {1},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956753,
author = {Koller, Daphne},
title = {Statistical Learning from Relational Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956753},
doi = {10.1145/956750.956753},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {4},
numpages = {1},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956754,
author = {Weigend, Andreas S.},
title = {Analyzing Customer Behavior at Amazon.Com},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956754},
doi = {10.1145/956750.956754},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {5},
numpages = {1},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956756,
author = {Aggarwal, Charu C.},
title = {Towards Systematic Design of Distance Functions for Data Mining Applications},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956756},
doi = {10.1145/956750.956756},
abstract = {Distance function computation is a key subtask in many data mining algorithms and applications. The most effective form of the distance function can only be expressed in the context of a particular data domain. It is also often a challenging and non-trivial task to find the most effective form of the distance function. For example, in the text domain, distance function design has been considered such an important and complex issue that it has been the focus of intensive research over three decades. The final design of distance functions in this domain has been reached only by detailed empirical testing and consensus over the quality of results provided by the different variations. With the increasing ability to collect data in an automated way, the number of new kinds of data continues to increase rapidly. This makes it increasingly difficult to undertake such efforts for each and every new data type. The most important aspect of distance function design is that since a human is the end-user for any application, the design must satisfy the user requirements with regard to effectiveness. This creates the need for a systematic framework to design distance functions which are sensitive to the particular characteristics of the data domain. In this paper, we discuss such a framework. The goal is to create distance functions in an automated waywhile minimizing the work required from the user. We will show that this framework creates distance functions which are significantly more effective than popularly used functions such as the Euclidean metric.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {9–18},
numpages = {10},
keywords = {user interaction, data mining, distance functions},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956757,
author = {Banerjee, Arindam and Dhillon, Inderjit and Ghosh, Joydeep and Sra, Suvrit},
title = {Generative Model-Based Clustering of Directional Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956757},
doi = {10.1145/956750.956757},
abstract = {High dimensional directional data is becoming increasingly important in contemporary applications such as analysis of text and gene-expression data. A natural model for multi-variate directional data is provided by the von Mises-Fisher (vMF) distribution on the unit hypersphere that is analogous to the multi-variate Gaussian distribution in Rd. In this paper, we propose modeling complex directional data as a mixture of vMF distributions. We derive and analyze two variants of the Expectation Maximization (EM) framework for estimating the parameters of this mixture. We also propose two clustering algorithms corresponding to these variants. An interesting aspect of our methodology is that the spherical kmeans algorithm (kmeans with cosine similarity) can be shown to be a special case of both our algorithms. Thus, modeling text data by vMF distributions lends theoretical validity to the use of cosine similarity which has been widely used by the information retrieval community. As part of experimental validation, we present results on modeling high-dimensional text and gene-expression data as a mixture of vMF distributions. The results indicate that our approach yields superior clusterings especially for difficult clustering tasks in high-dimensional spaces.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {19–28},
numpages = {10},
keywords = {von Mises-Fisher, EM, directional data, mixtures, clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956758,
author = {Bay, Stephen D. and Schwabacher, Mark},
title = {Mining Distance-Based Outliers in near Linear Time with Randomization and a Simple Pruning Rule},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956758},
doi = {10.1145/956750.956758},
abstract = {Defining outliers by their distance to neighboring examples is a popular approach to finding unusual examples in a data set. Recently, much work has been conducted with the goal of finding fast algorithms for this task. We show that a simple nested loop algorithm that in the worst case is quadratic can give near linear time performance when the data is in random order and a simple pruning rule is used. We test our algorithm on real high-dimensional data sets with millions of examples and show that the near linear scaling holds over several orders of magnitude. Our average case analysis suggests that much of the efficiency is because the time to process non-outliers, which are the majority of examples, does not depend on the size of the data set.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {29–38},
numpages = {10},
keywords = {distance-based operations, diskbased algorithms, outliers, anomaly detection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956759,
author = {Bilenko, Mikhail and Mooney, Raymond J.},
title = {Adaptive Duplicate Detection Using Learnable String Similarity Measures},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956759},
doi = {10.1145/956750.956759},
abstract = {The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {39–48},
numpages = {10},
keywords = {string edit distance, data cleaning, distance metric learning, record linkage, SVM applications, trained similarity measures},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956760,
author = {Bolton, Richard J. and Adams, Niall M.},
title = {An Iterative Hypothesis-Testing Strategy for Pattern Discovery},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956760},
doi = {10.1145/956750.956760},
abstract = {Pattern discovery has emerged as a direct result of increased data storage and analytic capabilities available to the data analyst. Without a massive amount of data, we do not have the evidence to support the discovery of the local deterministic structures that we call patterns. As such, pattern discovery is one of the few areas of data mining that cannot be considered simply as a 'scaling-up' of current statistical methodology to analyze large data sets. However, the philosophies of hypothesis testing and modeling in traditional statistics do lend themselves to forming a framework for pattern discovery, and we can also draw from ideas relating to outlier discovery and residual analysis to discover patterns. We illustrate an iterative strategy in a statistical framework by way of its application to one simulated and two real data sets.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {49–58},
numpages = {10},
keywords = {uncertainty, statistical models, data mining, residual analysis, outlier detection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956761,
author = {Br\"{o}nnimann, Herv\'{e} and Chen, Bin and Dash, Manoranjan and Haas, Peter and Scheuermann, Peter},
title = {Efficient Data Reduction with EASE},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956761},
doi = {10.1145/956750.956761},
abstract = {A variety of mining and analysis problems --- ranging from association-rule discovery to contingency table analysis to materialization of certain approximate datacubes --- involve the extraction of knowledge from a set of categorical count data. Such data can be viewed as a collection of "transactions," where a transaction is a fixed-length vector of counts. Classical algorithms for solving count-data problems require one or more computationally intensive passes over the entire database and can be prohibitively slow. One effective method for dealing with this ever-worsening scalability problem is to run the algorithms on a small sample of the data. We present a new data-reduction algorithm, called EASE, for producing such a sample. Like the FAST algorithm introduced by Chen et al., EASE is especially designed for count data applications. Both EASE and FAST take a relatively large initial random sample and then deterministically produce a subsample whose "distance" --- appropriately defined --- from the complete database is minimal. Unlike FAST, which obtains the final subsample by quasi-greedy descent, EASE uses epsilon-approximation methods to obtain the final subsample by a process of repeated halving. Experiments both in the context of association rule mining and classical χ2 contingency-table analysis show that EASE outperforms both FAST and simple random sampling, sometimes dramatically.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {59–68},
numpages = {10},
keywords = {OLAP, sampling, count dataset, frequency estimation, association rules, data streams},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956762,
author = {Casali, Alain and Cicchetti, Rosine and Lakhal, Lotfi},
title = {Extracting Semantics from Data Cubes Using Cube Transversals and Closures},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956762},
doi = {10.1145/956750.956762},
abstract = {In this paper we propose a lattice-based approach intended for extracting semantics from datacubes: borders of version spaces for supervised classification, closed cube lattice to summarize the semantics of datacubes w.r.t. COUNT, SUM, and covering graph of the quotient cube as a visualization tool of minimal multidimensional associations. With this intention, we introduce two novel concepts: the cube transversals and the cube closures over the cube lattice of a categorical database relation. We propose a levelwise merging algorithm for mining minimal cube transversals with a single database scan. We introduce the cube connection, show that it is a Galois connection and derive a closure operator over the cube lattice. Using cube transversals and closures, we define a new characterization of boundary sets which provide a condensed representation of version spaces used to enhance supervised classification. The algorithm designed for computing such borders improves the complexity of previous proposals. We also introduce the concept of closed cube lattice and show that it is isomorph to on one hand the Galois lattice and on the other hand the quotient cube w.r.t. COUNT, SUM. Proposed in [16], the quotient cube is a succinct summary of a datacube preserving the Rollup/Drilldown semantics. We show that the quotient cube w.r.t. COUNT, SUM and the closed cube lattice have a similar expression power but the latter has the smallest possible size. Finally we focus on the multidimensional association issue and introduce the covering graph of the quotient cube which provides the user with a visualization tool of minimal multidimensional associations.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {69–78},
numpages = {10},
keywords = {version spaces, hypergraph transversals, algorithm, datacubes, lattices, closures},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956763,
author = {Chudova, Darya and Gaffney, Scott and Mjolsness, Eric and Smyth, Padhraic},
title = {Translation-Invariant Mixture Models for Curve Clustering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956763},
doi = {10.1145/956750.956763},
abstract = {In this paper we present a family of algorithms that can simultaneously align and cluster sets of multidimensional curves defined on a discrete time grid. Our approach uses the Expectation-Maximization (EM) algorithm to recover both the mean curve shapes for each cluster, and the most likely shifts, offsets, and cluster memberships for each curve. We demonstrate how Bayesian estimation methods can improve the results for small sample sizes by enforcing smoothness in the cluster mean curves. We evaluate the methodology on two real-world data sets, time-course gene expression data and storm trajectory data. Experimental results show that models that incorporate curve alignment systematically provide improvements in predictive power and within-cluster variance on test data sets. The proposed approach provides a non-parametric, computationally efficient, and robust methodology for clustering broad classes of curve data.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {79–88},
numpages = {10},
keywords = {EM, mixture model, alignment, transformation invariance, curve clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956764,
author = {Dhillon, Inderjit S. and Mallela, Subramanyam and Modha, Dharmendra S.},
title = {Information-Theoretic Co-Clustering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956764},
doi = {10.1145/956750.956764},
abstract = {Two-dimensional contingency or co-occurrence tables arise frequently in important applications such as text, web-log and market-basket data analysis. A basic problem in contingency table analysis is co-clustering: simultaneous clustering of the rows and columns. A novel theoretical formulation views the contingency table as an empirical joint probability distribution of two discrete random variables and poses the co-clustering problem as an optimization problem in information theory---the optimal co-clustering maximizes the mutual information between the clustered random variables subject to constraints on the number of row and column clusters. We present an innovative co-clustering algorithm that monotonically increases the preserved mutual information by intertwining both the row and column clusterings at all stages. Using the practical example of simultaneous word-document clustering, we demonstrate that our algorithm works well in practice, especially in the presence of sparsity and high-dimensionality.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {89–98},
numpages = {10},
keywords = {mutual information, information theory, co-clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956765,
author = {Eirinaki, M. and Vazirgiannis, M. and Varlamis, I.},
title = {SEWeP: Using Site Semantics and a Taxonomy to Enhance the Web Personalization Process},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956765},
doi = {10.1145/956750.956765},
abstract = {Web personalization is the process of customizing a Web site to the needs of each specific user or set of users, taking advantage of the knowledge acquired through the analysis of the user's navigational behavior. Integrating usage data with content, structure or user profile data enhances the results of the personalization process. In this paper, we present SEWeP, a system that makes use of both the usage logs and the semantics of a Web site's content in order to personalize it. Web content is semantically annotated using a conceptual hierarchy (taxonomy). We introduce C-logs, an extended form of Web usage logs that encapsulates knowledge derived from the link semantics. C-logs are used as input to the Web usage mining process, resulting in a broader yet semantically focused set of recommendations.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {99–108},
numpages = {10},
keywords = {Web mining, concept hierarchies, Web personalization, semantic annotation of Web content},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956766,
author = {El-Hajj, Mohammad and Za\"{\i}ane, Osmar R.},
title = {Inverted Matrix: Efficient Discovery of Frequent Items in Large Datasets in the Context of Interactive Mining},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956766},
doi = {10.1145/956750.956766},
abstract = {Existing association rule mining algorithms suffer from many problems when mining massive transactional datasets. One major problem is the high memory dependency: either the gigantic data structure built is assumed to fit in main memory, or the recursive mining process is too voracious in memory resources. Another major impediment is the repetitive and interactive nature of any knowledge discovery process. To tune parameters, many runs of the same algorithms are necessary leading to the building of these huge data structures time and again. This paper proposes a new disk-based association rule mining algorithm called Inverted Matrix, which achieves its efficiency by applying three new ideas. First, transactional data is converted into a new database layout called Inverted Matrix that prevents multiple scanning of the database during the mining phase, in which finding frequent patterns could be achieved in less than a full scan with random access. Second, for each frequent item, a relatively small independent tree is built summarizing co-occurrences. Finally, a simple and non-recursive mining process reduces the memory requirements as minimum candidacy generation and counting is needed. Experimental studies reveal that our Inverted Matrix approach outperform FP-Tree especially in mining very large transactional databases with a very large number of unique items. Our random access disk-based approach is particularly advantageous in a repetitive and interactive setting.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {109–118},
numpages = {10},
keywords = {association rules, inverted matrix, COFI-tree, frequent patterns mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956767,
author = {Etzioni, Oren and Tuchinda, Rattapoom and Knoblock, Craig A. and Yates, Alexander},
title = {To Buy or Not to Buy: Mining Airfare Data to Minimize Ticket Purchase Price},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956767},
doi = {10.1145/956750.956767},
abstract = {As product prices become increasingly available on the World Wide Web, consumers attempt to understand how corporations vary these prices over time. However, corporations change prices based on proprietary algorithms and hidden variables (e.g., the number of unsold seats on a flight). Is it possible to develop data mining techniques that will enable consumers to predict price changes under these conditions?This paper reports on a pilot study in the domain of airline ticket prices where we recorded over 12,000 price observations over a 41 day period. When trained on this data, Hamlet --- our multi-strategy data mining algorithm --- generated a predictive model that saved 341 simulated passengers $198,074 by advising them when to buy and when to postpone ticket purchases. Remarkably, a clairvoyant algorithm with complete knowledge of future prices could save at most $320,572 in our simulation, thus HAMLET's savings were 61.8% of optimal. The algorithm's savings of $198,074 represents an average savings of 23.8% for the 341 passengers for whom savings are possible. Overall, HAMLET saved 4.4% of the ticket price averaged over the entire set of 4,488 simulated passengers. Our pilot study suggests that mining of price data available over the web has the potential to save consumers substantial sums of money per annum.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {119–128},
numpages = {10},
keywords = {Web mining, price mining, Internet, airline price prediction},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956768,
author = {Gionis, Aristides and Kujala, Teija and Mannila, Heikki},
title = {Fragments of Order},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956768},
doi = {10.1145/956750.956768},
abstract = {High-dimensional collections of 0--1 data occur in many applications. The attributes in such data sets are typically considered to be unordered. However, in many cases there is a natural total or partial order ≺ underlying the variables of the data set. Examples of variables for which such orders exist include terms in documents, courses in enrollment data, and paleontological sites in fossil data collections. The observations in such applications are flat, unordered sets; however, the data sets respect the underlying ordering of the variables. By this we mean that if A ≺ B ≺ C are three variables respecting the underlying ordering ≺, and both of variables A and C appear in an observation, then, up to noise levels, variable B also appears in this observation. Similarly, if A1 ≺ A2 ≺ … ≺ Al-1 ≺ Ai is a longer sequence of variables, we do not expect to see many observations for which there are indices i &lt; j &lt; k such that Ai and Ak occur in the observation but Aj does not.In this paper we study the problem of discovering fragments of orders of variables implicit in collections of unordered observations. We define measures that capture how well a given order agrees with the observed data. We describe a simple and efficient algorithm for finding all the fragments that satisfy certain conditions. We also discuss the sometimes necessary postprocessing for selecting only the best fragments of order. Also, we relate our method with a sequencing approach that uses a spectral algorithm, and with the consecutive ones problem. We present experimental results on some real data sets (author lists of database papers, exam results data, and paleontological data).},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {129–136},
numpages = {8},
keywords = {novel data mining algorithms, consecutive ones property, discovering hidden orderings, spectral analysis of data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956769,
author = {Kempe, David and Kleinberg, Jon and Tardos, \'{E}va},
title = {Maximizing the Spread of Influence through a Social Network},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956769},
doi = {10.1145/956750.956769},
abstract = {Models for the processes by which ideas and influence propagate through a social network have been studied in a number of domains, including the diffusion of medical and technological innovations, the sudden and widespread adoption of various strategies in game-theoretic settings, and the effects of "word of mouth" in the promotion of new products. Recently, motivated by the design of viral marketing strategies, Domingos and Richardson posed a fundamental algorithmic problem for such social network processes: if we can try to convince a subset of individuals to adopt a new product or innovation, and the goal is to trigger a large cascade of further adoptions, which set of individuals should we target?We consider this problem in several of the most widely studied models in social network analysis. The optimization problem of selecting the most influential nodes is NP-hard here, and we provide the first provable approximation guarantees for efficient algorithms. Using an analysis framework based on submodular functions, we show that a natural greedy strategy obtains a solution that is provably within 63% of optimal for several classes of models; our framework suggests a general approach for reasoning about the performance guarantees of algorithms for these types of influence problems in social networks.We also provide computational experiments on large collaboration networks, showing that in addition to their provable guarantees, our approximation algorithms significantly out-perform node-selection heuristics based on the well-studied notions of degree centrality and distance centrality from the field of social networks.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {137–146},
numpages = {10},
keywords = {social networks, viral marketing, approximation algorithms, diffusion of innovations},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956770,
author = {Koyut\"{u}rk, Mehmet and Grama, Ananth},
title = {PROXIMUS: A Framework for Analyzing Very High Dimensional Discrete-Attributed Datasets},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956770},
doi = {10.1145/956750.956770},
abstract = {This paper presents an efficient framework for error-bounded compression of high-dimensional discrete attributed datasets. Such datasets, which frequently arise in a wide variety of applications, pose some of the most significant challenges in data analysis. Subsampling and compression are two key technologies for analyzing these datasets. PROXIMUS provides a technique for reducing large datasets into a much smaller set of representative patterns, on which traditional (expensive) analysis algorithms can be applied with minimal loss of accuracy. We show desirable properties of PROXIMUS in terms of runtime, scalability to large datasets, and performance in terms of capability to represent data in a compact form. We also demonstrate applications of PROXIMUS in association rule mining. In doing so, we establish PROXIMUS as a tool for preprocessing data before applying computationally expensive algorithms or as a tool for directly extracting correlated patterns. Our experimental results show that use of the compressed data for association rule mining provides excellent precision and recall values (near 100%) across a range of support thresholds while reducing the time required for association rule mining drastically.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {147–156},
numpages = {10},
keywords = {non-orthogonal matrix decompositions, semi-discrete decomposition, compressing discrete-valued vectors},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956771,
author = {Pampalk, Elias and Goebl, Werner and Widmer, Gerhard},
title = {Visualizing Changes in the Structure of Data for Exploratory Feature Selection},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956771},
doi = {10.1145/956750.956771},
abstract = {Using visualization techniques to explore and understand high-dimensional data is an efficient way to combine human intelligence with the immense brute force computation power available nowadays. Several visualization techniques have been developed to study the cluster structure of data, i.e., the existence of distinctive groups in the data and how these clusters are related to each other. However, only few of these techniques lend themselves to studying how this structure changes if the features describing the data are changed. Understanding this relationship between the features and the cluster structure means understanding the features themselves and is thus a useful tool in the feature extraction phase.In this paper we present a novel approach to visualizing how modification of the features with respect to weighting or normalization changes the cluster structure. We demonstrate the application of our approach in two music related data mining projects.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {157–166},
numpages = {10},
keywords = {high-dimensional data, interactive data mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956772,
author = {Perlich, Claudia and Provost, Foster},
title = {Aggregation-Based Feature Invention and Relational Concept Classes},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956772},
doi = {10.1145/956750.956772},
abstract = {Model induction from relational data requires aggregation of the values of attributes of related entities. This paper makes three contributions to the study of relational learning. (1) It presents a hierarchy of relational concepts of increasing complexity, using relational schema characteristics such as cardinality, and derives classes of aggregation operators that are needed to learn these concepts. (2) Expanding one level of the hierarchy, it introduces new aggregation operators that model the distributions of the values to be aggregated and (for classification problems) the differences in these distributions by class. (3) It demonstrates empirically on a noisy business domain that more-complex aggregation methods can increase generalization performance. Constructing features using target-dependent aggregations can transform relational prediction tasks so that well-understood feature-vector-based modeling algorithms can be applied successfully.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {167–176},
numpages = {10},
keywords = {propositionalization, constructive induction, relational learning, aggregation, feature construction},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956773,
author = {Sarawagi, Sunita and Chakrabarti, Soumen and Godbole, Shantanu},
title = {Cross-Training: Learning Probabilistic Mappings between Topics},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956773},
doi = {10.1145/956750.956773},
abstract = {Classification is a well-established operation in text mining. Given a set of labels A and a set DA of training documents tagged with these labels, a classifier learns to assign labels to unlabeled test documents. Suppose we also had available a different set of labels B, together with a set of documents DB marked with labels from B. If A and B have some semantic overlap, can the availability of DB help us build a better classifier for A, and vice versa? We answer this question in the affirmative by proposing cross-training: a new approach to semi-supervised learning in presence of multiple label sets. We give distributional and discriminative algorithms for cross-training and show, through extensive experiments, that cross-training can discover and exploit probabilistic relations between two taxonomies for more accurate classification.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {177–186},
numpages = {10},
keywords = {semi-supervised multi-task learning, document classification, support vector machines, EM},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956774,
author = {Sripada, Somayajulu G. and Reiter, Ehud and Hunter, Jim and Yu, Jin},
title = {Generating English Summaries of Time Series Data Using the Gricean Maxims},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956774},
doi = {10.1145/956750.956774},
abstract = {We are developing technology for generating English textual summaries of time-series data, in three domains: weather forecasts, gas-turbine sensor readings, and hospital intensive care data. Our weather-forecast generator is currently operational and being used daily by a meteorological company. We generate summaries in three steps: (a) selecting the most important trends and patterns to communicate; (b) mapping these patterns onto words and phrases; and (c) generating actual texts based on these words and phrases. In this paper we focus on the first step, (a), selecting the information to communicate, and describe how we perform this using modified versions of standard data analysis algorithms such as segmentation. The modifications arose out of empirical work with users and domain experts, and in fact can all be regarded as applications of the Gricean maxims of Quality, Quantity, Relevance, and Manner, which describe how a cooperative speaker should behave in order to help a hearer correctly interpret a text. The Gricean maxims are perhaps a key element of adapting data analysis algorithms for effective communication of information to human users, and should be considered by other researchers interested in communicating data to human users.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {187–196},
numpages = {10},
keywords = {time series data, summarization, natural language processing, Gricean maxims},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956775,
author = {Tantrum, Jeremy and Murua, Alejandro and Stuetzle, Werner},
title = {Assessment and Pruning of Hierarchical Model Based Clustering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956775},
doi = {10.1145/956750.956775},
abstract = {The goal of clustering is to identify distinct groups in a dataset. The basic idea of model-based clustering is to approximate the data density by a mixture model, typically a mixture of Gaussians, and to estimate the parameters of the component densities, the mixing fractions, and the number of components from the data. The number of distinct groups in the data is then taken to be the number of mixture components, and the observations are partitioned into clusters (estimates of the groups) using Bayes' rule. If the groups are well separated and look Gaussian, then the resulting clusters will indeed tend to be "distinct" in the most common sense of the word - contiguous, densely populated areas of feature space, separated by contiguous, relatively empty regions. If the groups are not Gaussian, however, this correspondence may break down; an isolated group with a non-elliptical distribution, for example, may be modeled by not one, but several mixture components, and the corresponding clusters will no longer be well separated. We present methods for assessing the degree of separation between the components of a mixture model and between the corresponding clusters. We also propose a new clustering method that can be regarded as a hybrid between model-based and nonparametric clustering. The hybrid clustering algorithm prunes the cluster tree generated by hierarchical model-based clustering. Starting with the tree corresponding to the mixture model chosen by the Bayesian Information Criterion, it progressively merges clusters that do not appear to correspond to different modes of the data density.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {197–205},
numpages = {9},
keywords = {model-based clustering, nonparametric clustering, unimodality, density estimation},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956776,
author = {Vaidya, Jaideep and Clifton, Chris},
title = {Privacy-Preserving <i>k</i>-Means Clustering over Vertically Partitioned Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956776},
doi = {10.1145/956750.956776},
abstract = {Privacy and security concerns can prevent sharing of data, derailing data mining projects. Distributed knowledge discovery, if done correctly, can alleviate this problem. The key is to obtain valid results, while providing guarantees on the (non)disclosure of data. We present a method for k-means clustering when different sites contain different attributes for a common set of entities. Each site learns the cluster of each entity, but learns nothing about the attributes at other sites.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {206–215},
numpages = {10},
keywords = {privacy},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956777,
author = {Vlachos, Michail and Hadjieleftheriou, Marios and Gunopulos, Dimitrios and Keogh, Eamonn},
title = {Indexing Multi-Dimensional Time-Series with Support for Multiple Distance Measures},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956777},
doi = {10.1145/956750.956777},
abstract = {Although most time-series data mining research has concentrated on providing solutions for a single distance function, in this work we motivate the need for a single index structure that can support multiple distance measures. Our specific area of interest is the efficient retrieval and analysis of trajectory similarities. Trajectory datasets are very common in environmental applications, mobility experiments, video surveillance and are especially important for the discovery of certain biological patterns. Our primary similarity measure is based on the Longest Common Subsequence (LCSS) model, that offers enhanced robustness, particularly for noisy data, which are encountered very often in real world applications. However, our index is able to accommodate other distance measures as well, including the ubiquitous Euclidean distance, and the increasingly popular Dynamic Time Warping (DTW). While other researchers have advocated one or other of these similarity measures, a major contribution of our work is the ability to support all these measures without the need to restructure the index. Our framework guarantees no false dismissals and can also be tailored to provide much faster response time at the expense of slightly reduced precision/recall. The experimental results demonstrate that our index can help speed-up the computation of expensive similarity measures such as the LCSS and the DTW.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {216–225},
numpages = {10},
keywords = {trajectories, longest common subsequence, dynamic time warping},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956778,
author = {Wang, Haixun and Fan, Wei and Yu, Philip S. and Han, Jiawei},
title = {Mining Concept-Drifting Data Streams Using Ensemble Classifiers},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956778},
doi = {10.1145/956750.956778},
abstract = {Recently, mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection, target marketing, network intrusion detection, etc. Conventional knowledge discovery tools are facing two challenges, the overwhelming volume of the streaming data, and the concept drifts. In this paper, we propose a general framework for mining concept-drifting data streams using weighted ensemble classifiers. We train an ensemble of classification models, such as C4.5, RIPPER, naive Beyesian, etc., from sequential chunks of the data stream. The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time-evolving environment. Thus, the ensemble approach improves both the efficiency in learning the model and the accuracy in performing classification. Our empirical study shows that the proposed methods have substantial advantage over single-classifier approaches in prediction accuracy, and the ensemble framework is effective for a variety of classification models.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {226–235},
numpages = {10},
keywords = {classifier ensemble, classifier, concept drift, data streams},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956779,
author = {Wang, Jianyong and Han, Jiawei and Pei, Jian},
title = {CLOSET+: Searching for the Best Strategies for Mining Frequent Closed Itemsets},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956779},
doi = {10.1145/956750.956779},
abstract = {Mining frequent closed itemsets provides complete and non-redundant results for frequent pattern analysis. Extensive studies have proposed various strategies for efficient frequent closed itemset mining, such as depth-first search vs. breadthfirst search, vertical formats vs. horizontal formats, tree-structure vs. other data structures, top-down vs. bottom-up traversal, pseudo projection vs. physical projection of conditional database, etc. It is the right time to ask "what are the pros and cons of the strategies?" and "what and how can we pick and integrate the best strategies to achieve higher performance in general cases?"In this study, we answer the above questions by a systematic study of the search strategies and develop a winning algorithm CLOSET+. CLOSET+ integrates the advantages of the previously proposed effective strategies as well as some ones newly developed here. A thorough performance study on synthetic and real data sets has shown the advantages of the strategies and the improvement of CLOSET+ over existing mining algorithms, including CLOSET, CHARM and OP, in terms of runtime, memory usage and scalability.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {236–245},
numpages = {10},
keywords = {mining methods and algorithms, association rules, frequent closed itemsets},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956780,
author = {Wang, Ke and Jiang, Yuelong and Lakshmanan, Laks V. S.},
title = {Mining Unexpected Rules by Pushing User Dynamics},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956780},
doi = {10.1145/956750.956780},
abstract = {Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest. In this paper, we study three important issues that have been previously ignored in mining unexpected rules. First, the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario, in addition to the knowledge itself. Second, the prior knowledge should be considered right from the start to focus the search on unexpected rules. Third, the unexpectedness of a rule depends on what other rules the user has seen so far. Thus, only rules that remain unexpected given what the user has seen should be considered interesting. We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {246–255},
numpages = {10},
keywords = {subjective interestingness, unexpected rule, association rule},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956781,
author = {Webb, Geoffrey I. and Butler, Shane and Newlands, Douglas},
title = {On Detecting Differences between Groups},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956781},
doi = {10.1145/956750.956781},
abstract = {Understanding the differences between contrasting groups is a fundamental task in data analysis. This realization has led to the development of a new special purpose data mining technique, contrast-set mining. We undertook a study with a retail collaborator to compare contrast-set mining with existing rule-discovery techniques. To our surprise we observed that straightforward application of an existing commercial rule-discovery system, Magnum Opus, could successfully perform the contrast-set-mining task. This led to the realization that contrast-set mining is a special case of the more general rule-discovery task. We present the results of our study together with a proof of this conclusion.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {256–265},
numpages = {10},
keywords = {contrast-set discovery, retailing, rule discovery},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956782,
author = {White, Scott and Smyth, Padhraic},
title = {Algorithms for Estimating Relative Importance in Networks},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956782},
doi = {10.1145/956750.956782},
abstract = {Large and complex graphs representing relationships among sets of entities are an increasingly common focus of interest in data analysis---examples include social networks, Web graphs, telecommunication networks, and biological networks. In interactive analysis of such data a natural query is "which entities are most important in the network relative to a particular individual or set of individuals?" We investigate the problem of answering such queries in this paper, focusing in particular on defining and computing the importance of nodes in a graph relative to one or more root nodes. We define a general framework and a number of different algorithms, building on ideas from social networks, graph theory, Markov models, and Web graph analysis. We experimentally evaluate the different properties of these algorithms on toy graphs and demonstrate how our approach can be used to study relative importance in real-world networks including a network of interactions among September 11th terrorists, a network of collaborative research in biotechnology among companies and universities, and a network of co-authorship relationships among computer science researchers.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {266–275},
numpages = {10},
keywords = {social networks, graphs, PageRank, relative importance, Markov chains},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956783,
author = {Wu, Xintao and Barbar\'{a}, Daniel and Ye, Yong},
title = {Screening and Interpreting Multi-Item Associations Based on Log-Linear Modeling},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956783},
doi = {10.1145/956750.956783},
abstract = {Association rules have received a lot of attention in the data mining community since their introduction. The classical approach to find rules whose items enjoy high support (appear in a lot of the transactions in the data set) is, however, filled with shortcomings. It has been shown that support can be misleading as an indicator of how interesting the rule is. Alternative measures, such as lift, have been proposed. More recently, a paper by DuMouchel et al. proposed the use of all-two-factor loglinear models to discover sets of items that cannot be explained by pairwise associations between the items involved. This approach, however, has its limitations, since it stops short of considering higher order interactions (other than pairwise) among the items. In this paper, we propose a method that examines the parameters of the fitted loglinear models to find all the significant association patterns among the items. Since fitting loglinear models for large data sets can be computationally prohibitive, we apply graph-theoretical results to divide the original set of items into components (sets of items) that are statistically independent from each other. We then apply loglinear modeling to each of the components and find the interesting associations among items in them. The technique is experimentally evaluated with a real data set (insurance data) and a series of synthetic data sets. The results show that the technique is effective in finding interesting associations among the items involved.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {276–285},
numpages = {10},
keywords = {association rule, graphical model, log-linear model},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956784,
author = {Yan, Xifeng and Han, Jiawei},
title = {CloseGraph: Mining Closed Frequent Graph Patterns},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956784},
doi = {10.1145/956750.956784},
abstract = {Recent research on pattern discovery has progressed form mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in bioinformatics, Web exploration, and etc. However, mining large graph patterns in challenging due to the presence of an exponential number of frequent subgraphs. Instead of mining all the subgraphs, we propose to mine closed frequent graph patterns. A graph g is closed in a database if there exists no proper supergraph of g that has the same support as g. A closed graph pattern mining algorithm, CloseGraph, is developed by exploring several interesting pruning methods. Our performance study shows that CloseGraph not only dramatically reduces unnecessary subgraphs to be generated but also substantially increases the efficiency of mining, especially in the presence of large graph patterns.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {286–295},
numpages = {10},
keywords = {frequent graph, graph representation, canonical label, closed pattern},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956785,
author = {Yi, Lan and Liu, Bing and Li, Xiaoli},
title = {Eliminating Noisy Information in Web Pages for Data Mining},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956785},
doi = {10.1145/956750.956785},
abstract = {A commercial Web page typically contains many information blocks. Apart from the main content blocks, it usually has such blocks as navigation panels, copyright and privacy notices, and advertisements (for business purposes and for easy user access). We call these blocks that are not the main content blocks of the page the noisy blocks. We show that the information contained in these noisy blocks can seriously harm Web data mining. Eliminating these noises is thus of great importance. In this paper, we propose a noise elimination technique based on the following observation: In a given Web site, noisy blocks usually share some common contents and presentation styles, while the main content blocks of the pages are often diverse in their actual contents and/or presentation styles. Based on this observation, we propose a tree structure, called Style Tree, to capture the common presentation styles and the actual contents of the pages in a given Web site. By sampling the pages of the site, a Style Tree can be built for the site, which we call the Site Style Tree (SST). We then introduce an information based measure to determine which parts of the SST represent noises and which parts represent the main contents of the site. The SST is employed to detect and eliminate noises in any Web page of the site by mapping this page to the SST. The proposed technique is evaluated with two data mining tasks, Web page clustering and classification. Experimental results show that our noise elimination technique is able to improve the mining results significantly.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {296–305},
numpages = {10},
keywords = {noise detection, noise elimination, Web mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956786,
author = {Yu, Hwanjo and Yang, Jiong and Han, Jiawei},
title = {Classifying Large Data Sets Using SVMs with Hierarchical Clusters},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956786},
doi = {10.1145/956750.956786},
abstract = {Support vector machines (SVMs) have been promising methods for classification and regression analysis because of their solid mathematical foundations which convery several salient properties that other methods hardly provide. However, despite the prominent properties of SVMs, they are not as favored for large-scale data mining as for pattern recognition or machine learning because the training complexity of SVMs is highly dependent on the size of a data set. Many real-world data mining applications involve millions or billions of data records where even multiple scans of the entire data are too expensive to perform. This paper presents a new method, Clustering-Based SVM (CB-SVM), which is specifically designed for handling very large data sets. CB-SVM applies a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples that carry the statistical summaries of the data such that the summaries maximize the benefit of learning the SVM. CB-SVM tries to generate the best SVM boundary for very large data sets given limited amount of resources. Our experiments on synthetic and real data sets show that CB-SVM is highly scalable for very large data sets while also generating high classification accuracy.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {306–315},
numpages = {10},
keywords = {support vector machines, hierarchical cluster},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956787,
author = {Zaki, Mohammed J. and Aggarwal, Charu C.},
title = {XRules: An Effective Structural Classifier for XML Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956787},
doi = {10.1145/956750.956787},
abstract = {XML documents have recently become ubiquitous because of their varied applicability in a number of applications. Classification is an important problem in the data mining domain, but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words. Such techniques ignore a significant amount of information hidden inside the documents. In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents. Such a technique is more capable of finding the classification characteristics of documents. In addition, the technique can also be extended to cost sensitive classification. We show the effectiveness of the method with respect to other classifiers. We note that the methodology discussed in this paper is applicable to any kind of semi-structured data.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {316–325},
numpages = {10},
keywords = {classification, tree mining, XML/Semi-structured data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956788,
author = {Zaki, Mohammed J. and Gouda, Karam},
title = {Fast Vertical Mining Using Diffsets},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956788},
doi = {10.1145/956750.956788},
abstract = {A number of vertical mining algorithms have been proposed recently for association mining, which have shown to be very effective and usually outperform horizontal approaches. The main advantage of the vertical format is support for fast frequency counting via intersection operations on transaction ids (tids) and automatic pruning of irrelevant data. The main problem with these approaches is when intermediate results of vertical tid lists become too large for memory, thus affecting the algorithm scalability.In this paper we present a novel vertical data representation called Diffset, that only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. We show that diffsets drastically cut down the size of memory required to store intermediate results. We show how diffsets, when incorporated into previous vertical mining methods, increase the performance significantly.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {326–335},
numpages = {10},
keywords = {diffsets, association rule mining, frequent itemsets},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956789,
author = {Zhu, Yunyue and Shasha, Dennis},
title = {Efficient Elastic Burst Detection in Data Streams},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956789},
doi = {10.1145/956750.956789},
abstract = {Burst detection is the activity of finding abnormal aggregates in data streams. Such aggregates are based on sliding windows over data streams. In some applications, we want to monitor many sliding window sizes simultaneously and to report those windows with aggregates significantly different from other periods. We will present a general data structure for detecting interesting aggregates over such elastic windows in near linear time. We present applications of the algorithm for detecting Gamma Ray Bursts in large-scale astrophysical data. Detection of periods with high volumes of trading activities and high stock price volatility is also demonstrated using real time Trade and Quote (TAQ) data from the New York Stock Exchange (NYSE). Our algorithm beats the direct computation approach by several orders of magnitude.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {336–345},
numpages = {10},
keywords = {elastic burst, data stream},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956791,
author = {Ali, Kamal and Ketchpel, Steven P.},
title = {Golden Path Analyzer: Using Divide-and-Conquer to Cluster Web Clickstreams},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956791},
doi = {10.1145/956750.956791},
abstract = {This paper describes a novel algorithm and deployed system Golden Path Analyzer (GPA) that analyzes clickstreams of people trying to complete the same task on a website. It finds the shortest, successful paths taken by users - 'golden paths' - and uses these as seeds for clickstream clusters. Other users are assigned to a cluster if their clickstream is a supersequence of the golden path. The advantages of this approach are that the resulting clusters are easily comprehended, they are few in number, correspond to semantically different strategies used by the users, and jointly partition all the clickstreams. GPA's key contribution over prior work in process funnels is that by not excluding users that make diversions from the golden path, GPA is able to assign more users to fewer clusters. Another key contribution is to use actual full clickstreams as cluster seeds to which supersequences of other users are added. Golden paths correspond to complete clickstreams that are based on actual user page transitions. GPA is particularly useful for site designers to improve processes such as shopping, returns and registration. Its analyses identify which web pages cause many users to deviate from a golden path, which links distract users and the percentage of users taking each golden path. GPA has demonstrated value on more than twenty client projects in diverse industries.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {349–358},
numpages = {10},
keywords = {clustering, divide-and-conquer, Web-mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956792,
author = {Fram, David M. and Almenoff, June S. and DuMouchel, William},
title = {Empirical Bayesian Data Mining for Discovering Patterns in Post-Marketing Drug Safety},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956792},
doi = {10.1145/956750.956792},
abstract = {Because of practical limits in characterizing the safety profiles of therapeutic products prior to marketing, manufacturers and regulatory agencies perform post-marketing surveillance based on the collection of adverse reaction reports ("pharmacovigilance").The resulting databases, while rich in real-world information, are notoriously difficult to analyze using traditional techniques. Each report may involve multiple medicines, symptoms, and demographic factors, and there is no easily linked information on drug exposure in the reporting population. KDD techniques, such as association finding, are well-matched to the problem, but are difficult for medical staff to apply and interpret.To deploy KDD effectively for pharmacovigilance, Lincoln Technologies and GlaxoSmithKline collaborated to create a webbased safety data mining web environment. The analytical core is a high-performance implementation of the MGPS (Multi-Item Gamma Poisson Shrinker) algorithm described previously by DuMouchel and Pregibon, with several significant extensions and enhancements. The environment offers an interface for specifying data mining runs, a batch execution facility, tabular and graphical methods for exploring associations, and drilldown to case details. Substantial work was involved in preparing the raw adverse event data for mining, including harmonization of drug names and removal of duplicate reports.The environment can be used to explore both drug-event and multi-way associations (interactions, syndromes). It has been used to study age/gender effects, to predict the safety profiles of proposed combination drugs, and to separate contributions of individual drugs to safety problems in polytherapy situations.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {359–368},
numpages = {10},
keywords = {association rules, data mining, empirical Bayes methods, post-marketing surveillance, pharmacovigilance},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956793,
author = {Ho, Tu Bao and Nguyen, Trong Dung and Kawasaki, Saori and Le, Si Quang and Nguyen, Dung Duc and Yokoi, Hideto and Takabayashi, Katsuhiko},
title = {Mining Hepatitis Data with Temporal Abstraction},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956793},
doi = {10.1145/956750.956793},
abstract = {The hepatitis temporal database collected at Chiba university hospital between 1982--2001 was recently given to challenge the KDD research. The database is large where each patient corresponds to 983 tests represented as sequences of irregular timestamp points with different lengths. This paper presents a temporal abstraction approach to mining knowledge from this hepatitis database. Exploiting hepatitis background knowledge and data analysis, we introduce new notions and methods for abstracting short-term changed and long-term changed tests. The abstracted data allow us to apply different machine learning methods for finding knowledge part of which is considered as new and interesting by medical doctors.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {369–377},
numpages = {9},
keywords = {temporal abstraction, medicaldata mining, hepatitis data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956794,
author = {Jensen, David and Rattigan, Matthew and Blau, Hannah},
title = {Information Awareness: A Prospective Technical Assessment},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956794},
doi = {10.1145/956750.956794},
abstract = {Recent proposals to apply data mining systems to problems in law enforcement, national security, and fraud detection have attracted both media attention and technical critiques of their expected accuracy and impact on privacy. Unfortunately, the majority of technical critiques have been based on simplistic assumptions about data, classifiers, inference procedures, and the overall architecture of such systems. We consider these critiques in detail, and we construct a simulation model that more closely matches realistic systems. We show how both the accuracy and privacy impact of a hypothetical system could be substantially improved, and we discuss the necessary and sufficient conditions for this improvement to be achieved. This analysis is neither a defense nor a critique of any particular system concept. Rather, our model suggests alternative technical designs that could mitigate some concerns, but also raises more specific conditions that must be met for such systems to be both accurate and socially desirable.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {378–387},
numpages = {10},
keywords = {ranking classifiers, technology assessment, TIA, information awareness, privacy, collective classification, relational data mining, social network analysis, iterative classification},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956795,
author = {Last, Mark and Friedman, Menahem and Kandel, Abraham},
title = {The Data Mining Approach to Automated Software Testing},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956795},
doi = {10.1145/956750.956795},
abstract = {In today's industry, the design of software tests is mostly based on the testers' expertise, while test automation tools are limited to execution of pre-planned tests only. Evaluation of test outputs is also associated with a considerable effort by human testers who often have imperfect knowledge of the requirements specification. Not surprisingly, this manual approach to software testing results in heavy losses to the world's economy. The costs of the so-called "catastrophic" software failures (such as Mars Polar Lander shutdown in 1999) are even hard to measure. In this paper, we demonstrate the potential use of data mining algorithms for automated induction of functional requirements from execution data. The induced data mining models of tested software can be utilized for recovering missing and incomplete specifications, designing a minimal set of regression tests, and evaluating the correctness of software outputs when testing new, potentially flawed releases of the system. To study the feasibility of the proposed approach, we have applied a novel data mining algorithm called Info-Fuzzy Network (IFN) to execution data of a general-purpose code for solving partial differential equations. After being trained on a relatively small number of randomly generated input-output examples, the model constructed by the IFN algorithm has shown a clear capability to discriminate between correct and faulty versions of the program.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {388–396},
numpages = {9},
keywords = {input-output analysis, finite element solver, automated software testing, info-fuzzy networks, regression testing},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956796,
author = {Lawrence, Richard D. and Hong, Se June and Cherrier, Jacques},
title = {Passenger-Based Predictive Modeling of Airline No-Show Rates},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956796},
doi = {10.1145/956750.956796},
abstract = {Airlines routinely overbook flights based on the expectation that some fraction of booked passengers will not show for each flight. Accurate forecasts of the expected number of no-shows for each flight can increase airline revenue by reducing the number of spoiled seats (empty seats that might otherwise have been sold) and the number of involuntary denied boardings at the departure gate. Conventional no-show forecasting methods typically average the no-show rates of historically similar flights, without the use of passenger-specific information.We develop two classes of models to predict cabin-level no-show rates using specific information on the individual passengers booked on each flight. The first of these models computes the no-show probability for each passenger, using both the cabin-level historical forecast and the extracted passenger features as explanatory variables. This passenger-level model is implemented using three different predictive methods: a C4.5 decision-tree, a segmented Naive Bayes algorithm, and a new aggregation method for an ensemble of probabilistic models. The second cabin-level model is formulated using the desired cabin-level no-show rate as the response variable. Inputs to this model include the predicted cabin-level no-show rates derived from the various passenger-level models, as well as simple statistics of the features of the cabin passenger population. The cabin-level model is implemented using either linear regression, or as a direct probability model with explicit incorporation of the cabin-level no-show rates derived from the passenger-level model outputs.The new passenger-based models are compared to a conventional historical model, using train and evaluation data sets taken from over 1 million passenger name records. Standard metrics such as lift curves and mean-square cabin-level errors establish the improved accuracy of the passenger-based models over the historical model. All models are also evaluated using a simple revenue model, and it is shown that the cabin-level passenger-based model can produce between 0.4% and 3.2% revenue gain over the conventional model, depending on the revenue-model parameters.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {397–406},
numpages = {10},
keywords = {classification, airline overbooking, no-show forecasting, model aggregation, data mining, probabilistic estimation, predictive modeling},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956797,
author = {Piatetsky-Shapiro, Gregory and Khabaza, Tom and Ramaswamy, Sridhar},
title = {Capturing Best Practice for Microarray Gene Expression Data Analysis},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956797},
doi = {10.1145/956750.956797},
abstract = {Analyzing gene expression data from microarray devices has many important application in medicine and biology, but presents significant challenges to data mining. Microarray data typically has many attributes (genes) and few examples (samples), making the process of correctly analyzing such data difficult to formulate and prone to common mistakes. For this reason it is unusually important to capture and record good practices for this form of data mining. This paper presents a process for analyzing microarray data, including pre-processing, gene selection, randomization testing, classification and clustering; this process is captured with "Clementine Application Templates". The paper describes the process in detail and includes three case studies, showing how the process is applied to 2-class classification, multi-class classification and clustering analyses for publicly available microarray datasets.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {407–415},
numpages = {9},
keywords = {data mining process, Clementine, microarrays, application template, gene expression},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956798,
author = {Rao, R. Bharat and Sandilya, Sathyakama and Niculescu, Radu Stefan and Germond, Colin and Rao, Harsha},
title = {Clinical and Financial Outcomes Analysis with Existing Hospital Patient Records},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956798},
doi = {10.1145/956750.956798},
abstract = {Existing patient records are a valuable resource for automated outcomes analysis and knowledge discovery. However, key clinical data in these records is typically recorded in unstructured form as free text and images, and most structured clinical information is poorly organized. Time-consuming interpretation and analysis is required to convert these records into structured clinical data. Thus, only a tiny fraction of this resource is utilized. We present REMIND, a Bayesian Framework for Reliable Extraction and Meaningful Inference from Nonstructured Data. REMIND integrates and blends the structured and unstructured clinical data in patient records to automatically created high-quality structured clinical data. This structuring allows existing patient records to be mined for quality assurance, regulatory compliance, and to relate financial and clinical factors. We demonstrate REMIND on two medical applications: (a) Extract "recurrence", the key outcome for measuring treatment effectiveness, for colon cancer patients (ii) Extract key diagnoses and complications for acute myocardial infarction (heart attack) patients, and demonstrate the impact of these clinical factors on financial outcomes.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {416–425},
numpages = {10},
keywords = {HMMs, Bayes Nets, temporal reasoning, data mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956799,
author = {Sahoo, R. K. and Oliner, A. J. and Rish, I. and Gupta, M. and Moreira, J. E. and Ma, S. and Vilalta, R. and Sivasubramaniam, A.},
title = {Critical Event Prediction for Proactive Management in Large-Scale Computer Clusters},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956799},
doi = {10.1145/956750.956799},
abstract = {As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {426–435},
numpages = {10},
keywords = {critical event prediction, large-scale clusters, system event log},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956800,
author = {She, Rong and Chen, Fei and Wang, Ke and Ester, Martin and Gardy, Jennifer L. and Brinkman, Fiona S. L.},
title = {Frequent-Subsequence-Based Prediction of Outer Membrane Proteins},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956800},
doi = {10.1145/956750.956800},
abstract = {A number of medically important disease-causing bacteria (collectively called Gram-negative bacteria) are noted for the extra "outer" membrane that surrounds their cell. Proteins resident in this membrane (outer membrane proteins, or OMPs) are of primary research interest for antibiotic and vaccine drug design as they are on the surface of the bacteria and so are the most accessible targets to develop new drugs against. With the development of genome sequencing technology and bioinformatics, biologists can now deduce all the proteins that are likely produced in a given bacteria and have attempted to classify where proteins are located in a bacterial cell. However such protein localization programs are currently least accurate when predicting OMPs, and so there is a current need for the development of a better OMP classifier. Data mining research suggests that the use of frequent patterns has good performance in aiding the development of accurate and efficient classification algorithms. In this paper, we present two methods to identify OMPs based on frequent subsequences and test them on all Gram-negative bacterial proteins whose localizations have been determined by biological experiments. One classifier follows an association rule approach, while the other is based on support vector machines (SVMs). We compare the proposed methods with the state-of-the-art methods in the biological domain. The results demonstrate that our methods are better both in terms of accurately identifying OMPs and providing biological insights that increase our understanding of the structures and functions of these important proteins.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {436–445},
numpages = {10},
keywords = {support vector machine, classification, subcellular localization, outer membrane protein, association rule},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956801,
author = {Steinbach, Michael and Tan, Pang-Ning and Kumar, Vipin and Klooster, Steven and Potter, Christopher},
title = {Discovery of Climate Indices Using Clustering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956801},
doi = {10.1145/956750.956801},
abstract = {To analyze the effect of the oceans and atmosphere on land climate, Earth Scientists have developed climate indices, which are time series that summarize the behavior of selected regions of the Earth's oceans and atmosphere. In the past, Earth scientists have used observation and, more recently, eigenvalue analysis techniques, such as principal components analysis (PCA) and singular value decomposition (SVD), to discover climate indices. However, eigenvalue techniques are only useful for finding a few of the strongest signals. Furthermore, they impose a condition that all discovered signals must be orthogonal to each other, making it difficult to attach a physical interpretation to them. This paper presents an alternative clustering-based methodology for the discovery of climate indices that overcomes these limitiations and is based on clusters that represent regions with relatively homogeneous behavior. The centroids of these clusters are time series that summarize the behavior of the ocean or atmosphere in those regions. Some of these centroids correspond to known climate indices and provide a validation of our methodology; other centroids are variants of known indices that may provide better predictive power for some land areas; and still other indices may represent potentially new Earth science phenomena. Finally, we show that cluster based indices generally outperform SVD derived indices, both in terms of area weighted correlation and direct correlation with the known indices.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {446–455},
numpages = {10},
keywords = {mining scientific data, clustering, singular value decomposition, time series, earth science data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956802,
author = {Weiss, Sholom M. and Buckley, Stephen J. and Kapoor, Shubir and Damgaard, S\o{}ren},
title = {Knowledge-Based Data Mining},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956802},
doi = {10.1145/956750.956802},
abstract = {We describe techniques for combining two types of knowledge systems: expert and machine learning. Both the expert system and the learning system represent information by logical decision rules or trees. Unlike the classical views of knowledge-base evaluation or refinement, our view accepts the contents of the knowledge base as completely correct. The knowledge base and the results of its stored cases will provide direction for the discovery of new relationships in the form of newly induced decision rules. An expert system called SEAS was built to discover sales leads for computer products and solutions. The system interviews executives by asking questions, and based on the responses, recommends products that may improve a business' operations. Leveraging this expert system, we record the results of the interviews and the program's recommendations. The very same data stored by the expert system is used to find new predictive rules. Among the potential advantages of this approach are (a) the capability to spot new sales trends and (b) the substitution of less expensive probabilistic rules that use database data instead of interviews.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {456–461},
numpages = {6},
keywords = {sales leads, decision rule induction, expert systems},
location = {Washington, D.C.},
series = {KDD '03}
}

@inbook{10.1145/956750.956803,
author = {Wu, Yi-Leh and Goh, King-Shy and Li, Beitao and You, Huaxing and Chang, Edward Y.},
title = {The Anatomy of a Multimodal Information Filter},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956803},
abstract = {The proliferation of objectionable information on the Internet has reached a level of serious concern. To empower end-users with the choice of blocking undesirable and offensive websites, we propose a multimodal information filter, named MORF. In this paper, we present MORF's core components: its confidence-based classifier, a Cross-bagging ensemble scheme, and multimodal classification algorithm. Empirical studies and initial statistics collected from the MORF filters deployed at sites in the U.S. and Asia show that MORF is both efficient and effective, due to our classification methods.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {462–471},
numpages = {10}
}

@inproceedings{10.1145/956750.956805,
author = {Argamon, Shlomo and \v{S}ari\'{c}, Marin and Stein, Sterling S.},
title = {Style Mining of Electronic Messages for Multiple Authorship Discrimination: First Results},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956805},
doi = {10.1145/956750.956805},
abstract = {This paper considers the use of computational stylistics for performing authorship attribution of electronic messages, addressing categorization problems with as many as 20 different classes (authors). Effective stylistic characterization of text is potentially useful for a variety of tasks, as language style contains cues regarding the authorship, purpose, and mood of the text, all of which would be useful adjuncts to information retrieval or knowledge-management tasks. We focus here on the problem of determining the author of an anonymous message, based only on the message text. Several multiclass variants of the Winnow algorithm were applied to a vector representation of the message texts to learn models for discriminating different authors. We present results comparing the classification accuracy of the different approaches. The results show that stylistic models can be accurately learned to determine an author's identity.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {475–480},
numpages = {6},
keywords = {electronic communication, computational stylistics, authorship attribution, text mining, text categorization},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956806,
author = {Bhatnagar, Raj and Kurra, Goutham and Niu, Wen},
title = {Mining High Dimensional Data for Classifier Knowledge},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956806},
doi = {10.1145/956750.956806},
abstract = {We present in this paper the problem of discovering sets of attribute-value pairs in high dimensional data sets that are of interest not because of co-occurrence alone, but due to their value in serving as cores for potential classifiers of clusters. We present our algorithm in the context of a gene-expression dataset. Gene expression data, in most situations, is insufficient for clustering algorithms and any statistical inference because for 6000+ genes, typically only 10s and at most 100s of data points become available. It is difficult to use statistical techniques to design a classifier for such immensely under-specified data. The observed data, though statistically, insufficient contains some information about the domain. Our goal is to discover as much information about all potential classifiers as possible from the data and then summarize this knowledge. This summarization provides insights into the composition of potential classifiers. We present here algorithms and methods for mining a high dimensional data set, exemplified by a gene expression data set, for mining such information.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {481–486},
numpages = {6},
keywords = {pattern recognition, high dimensional data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956807,
author = {Chang, Joong Hyuk and Lee, Won Suk},
title = {Finding Recent Frequent Itemsets Adaptively over Online Data Streams},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956807},
doi = {10.1145/956750.956807},
abstract = {A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Consequently, the knowledge embedded in a data stream is more likely to be changed as time goes by. Identifying the recent change of a data stream, specially for an online data stream, can provide valuable information for the analysis of the data stream. In addition, monitoring the continuous variation of a data stream enables to find the gradual change of embedded knowledge. However, most of mining algorithms over a data stream do not differentiate the information of recently generated transactions from the obsolete information of old transactions which may be no longer useful or possibly invalid at present. This paper proposes a data mining method for finding recent frequent itemsets adaptively over an online data stream. The effect of old transactions on the mining result of the data steam is diminished by decaying the old occurrences of each itemset as time goes by. Furthermore, several optimization techniques are devised to minimize processing time as well as main memory usage. Finally, the proposed method is analyzed by a series of experiments.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {487–492},
numpages = {6},
keywords = {recent frequent itemsets, pruning of itemsets, decay mechanism, data stream, delayed-insertion},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956808,
author = {Chiu, Bill and Keogh, Eamonn and Lonardi, Stefano},
title = {Probabilistic Discovery of Time Series Motifs},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956808},
doi = {10.1145/956750.956808},
abstract = {Several important time series data mining problems reduce to the core task of finding approximately repeated subsequences in a longer time series. In an earlier work, we formalized the idea of approximately repeated subsequences by introducing the notion of time series motifs. Two limitations of this work were the poor scalability of the motif discovery algorithm, and the inability to discover motifs in the presence of noise.Here we address these limitations by introducing a novel algorithm inspired by recent advances in the problem of pattern discovery in biosequences. Our algorithm is probabilistic in nature, but as we show empirically and theoretically, it can find time series motifs with very high probability even in the presence of noise or "don't care" symbols. Not only is the algorithm fast, but it is an anytime algorithm, producing likely candidate motifs almost immediately, and gradually improving the quality of results over time.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {493–498},
numpages = {6},
keywords = {time series, data mining, motifs, randomized algorithms},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956809,
author = {Cohen, William W. and Wang, Richard and Murphy, Robert F.},
title = {Understanding Captions in Biomedical Publications},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956809},
doi = {10.1145/956750.956809},
abstract = {From the standpoint of the automated extraction of scientific knowledge, an important but little-studied part of scientific publications are the figures and accompanying captions. Captions are dense in information, but also contain many extra-grammatical constructs, making them awkward to process with standard information extraction methods. We propose a scheme for "understanding" captions in biomedical publications by extracting and classifying "image pointers" (references to the accompanying image). We evaluate a number of automated methods for this task, including hand-coded methods, methods based on existing learning techniques, and methods based on novel learning techniques. The best of these methods leads to a usefully accurate tool for caption-understanding, with both recall and precision in excess of 94% on the most important single class in a combined extraction/classification task.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {499–504},
numpages = {6},
keywords = {bioinformatics, boosting, information extraction},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956810,
author = {Du, Wenliang and Zhan, Zhijun},
title = {Using Randomized Response Techniques for Privacy-Preserving Data Mining},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956810},
doi = {10.1145/956750.956810},
abstract = {Privacy is an important issue in data mining and knowledge discovery. In this paper, we propose to use the randomized response techniques to conduct the data mining computation. Specially, we present a method to build decision tree classifiers from the disguised data. We conduct experiments to compare the accuracy of our decision tree with the one built from the original undisguised data. Our results show that although the data are disguised, our method can still achieve fairly high accuracy. We also show how the parameter used in the randomized response techniques affects the accuracy of the results.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {505–510},
numpages = {6},
keywords = {decision tree, data mining, privacy, security},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956811,
author = {DuMouchel, William and Agarwal, Deepak K.},
title = {Applications of Sampling and Fractional Factorial Designs to Model-Free Data Squashing},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956811},
doi = {10.1145/956750.956811},
abstract = {The concept of "data squashing" was introduced by DuMouchel et al [4] as a method of summarizing massive data sets that preserves statistical relationships among variables. The idea is to create a smaller data set that allows statistical modeling to take place using in-memory algorithms, and to preserve the modeling results more accurately than would a same-size random sample from the massive data set. This research attempts to avoid several limitations of previous approaches to data squashing. Our method avoids the curse of dimensionality by a double use of principal components transformations that makes computing time linear in the number of cases and quadratic in the number of variables. Categorical and continuous variables are smoothly integrated. Because the binning is based on principal components, which are uncorrelated, we can use fractional factorial designs that sample less than one point per bin. We also investigate various weighting schemes for the squashed sample to see whether matching moments or matching subregion data counts is more effective. Finally, previous work required the specification of a statistical model, either to perform the squashing algorithm or to compare the worth of different squashing methods. Our approach to evaluation is model free and does not even require the specification of variables as responses or predictors. Instead, we develop a chi-squared like measure of accuracy to compare the closeness of various discrete densities (the squashed data sets) to the discrete massive data set.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {511–516},
numpages = {6},
keywords = {stratified sampling, data squashing, fractional factorial design, summary of massive datasets},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956812,
author = {Fradkin, Dmitriy and Madigan, David},
title = {Experiments with Random Projections for Machine Learning},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956812},
doi = {10.1145/956750.956812},
abstract = {Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {517–522},
numpages = {6},
keywords = {dimensionality reduction, random projection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956813,
author = {Gama, Jo\~{a}o and Rocha, Ricardo and Medas, Pedro},
title = {Accurate Decision Trees for Mining High-Speed Data Streams},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956813},
doi = {10.1145/956750.956813},
abstract = {In this paper we study the problem of constructing accurate decision tree models from data streams. Data streams are incremental tasks that require incremental, online, and any-time learning algorithms. One of the most successful algorithms for mining data streams is VFDT. In this paper we extend the VFDT system in two directions: the ability to deal with continuous data and the use of more powerful classification techniques at tree leaves. The proposed system, VFDTc, can incorporate and classify new information online, with a single scan of the data, in time constant per example. The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets. This is relevant due to the any-time property. We study the behaviour of VFDTc in different problems and demonstrate its utility in large and medium data sets. Under a bias-variance analysis we observe that VFDTc in comparison to C4.5 is able to reduce the variance component.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {523–528},
numpages = {6},
keywords = {incremental decision trees, data streams, functional leaves},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956814,
author = {Guha, Sudipto and Gunopulos, D. and Koudas, Nick},
title = {Correlating Synchronous and Asynchronous Data Streams},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956814},
doi = {10.1145/956750.956814},
abstract = {In a variety of modern mining applications, data are commonly viewed as infinite time ordered data streams rather as finite data sets stored on disk. This view challenges fundamental assumptions commonly made in the context of several data mining algorithms.In this paper, we study the problem of identifying correlations between multiple data streams. In particular, we propose algorithms capable of capturing correlations between multiple continuous data streams in a highly efficient and accurate manner. Our algorithms and techniques are applicable in the case of both synchronous and asynchronous data streaming environments. We capture correlations between multiple streams using the well known technique of Singular Value Decomposition (SVD). Correlations between data items, and the SVD technique in particular, have been repeatedly utilized in an off-line (non stream) data mining problems, for example forecasting, approximate query answering, and data reduction.We propose a methodology based on a combination of dimensionality reduction and sampling to make the SVD technique suitable for a data stream context. Our techniques are approximate, trading accuracy with performance, and we analytically quantify this tradeoff. We present a through experimental evaluation, using both real and synthetic data sets, from a prototype implementation of our technique, investigating the impact of various parameters in the accuracy of the overall computation. Our results indicate, that correlations between multiple data streams can be identified very efficiently and accurately. The algorithms proposed herein, are presented as generic tools, with a multitude of applications on data stream mining problems.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {529–534},
numpages = {6},
keywords = {approximate computation, singular value decomposition, data streams},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956815,
author = {G\"{u}nd\"{u}z, \c{S}ule and \"{O}zsu, M. Tamer},
title = {A Web Page Prediction Model Based on Click-Stream Tree Representation of User Behavior},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956815},
doi = {10.1145/956750.956815},
abstract = {Predicting the next request of a user as she visits Web pages has gained importance as Web-based activity increases. Markov models and their variations, or models based on sequence mining have been found well suited for this problem. However, higher order Markov models are extremely complicated due to their large number of states whereas lower order Markov models do not capture the entire behavior of a user in a session. The models that are based on sequential pattern mining only consider the frequent sequences in the data set, making it difficult to predict the next request following a page that is not in the sequential pattern. Furthermore, it is hard to find models for mining two different kinds of information of a user session. We propose a new model that considers both the order information of pages in a session and the time spent on them. We cluster user sessions based on their pair-wise similarity and represent the resulting clusters by a click-stream tree. The new user session is then assigned to a cluster based on a similarity measure. The click-stream tree of that cluster is used to generate the recommendation set. The model can be used as part of a cache prefetching system as well as a recommendation model.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {535–540},
numpages = {6},
keywords = {graph based clustering, two dimensional sequential model, Web usage mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956816,
author = {Hopcroft, John and Khan, Omar and Kulis, Brian and Selman, Bart},
title = {Natural Communities in Large Linked Networks},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956816},
doi = {10.1145/956750.956816},
abstract = {We are interested in finding natural communities in large-scale linked networks. Our ultimate goal is to track changes over time in such communities. For such temporal tracking, we require a clustering algorithm that is relatively stable under small perturbations of the input data. We have developed an efficient, scalable agglomerative strategy and applied it to the citation graph of the NEC CiteSeer database (250,000 papers; 4.5 million citations). Agglomerative clustering techniques are known to be unstable on data in which the community structure is not strong. We find that some communities are essentially random and thus unstable while others are natural and will appear in most clusterings. These natural communities will enable us to track the evolution of communities over time.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {541–546},
numpages = {6},
keywords = {large linked networks, hierarchical agglomerative clustering, stability, natural communities},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956817,
author = {Houle, Michael E.},
title = {Navigating Massive Data Sets via Local Clustering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956817},
doi = {10.1145/956750.956817},
abstract = {This paper introduces a scalable method for feature extraction and navigation of large data sets by means of local clustering, where clusters are modeled as overlapping neighborhoods. Under the model, intra-cluster association and external differentiation are both assessed in terms of a natural confidence measure. Minor clusters can be identified even when they appear in the intersection of larger clusters. Scalability of local clustering derives from recent generic techniques for efficient approximate similarity search. The cluster overlap structure gives rise to a hierarchy that can be navigated and queried by users. Experimental results are provided for two large text databases.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {547–552},
numpages = {6},
keywords = {nearest neighbor, association, confidence, soft clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956818,
author = {Hsu, Wynne and Dai, Jing and Lee, Mong Li},
title = {Mining Viewpoint Patterns in Image Databases},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956818},
doi = {10.1145/956750.956818},
abstract = {The increasing number of image repositories has made image mining an important task because of its potential in discovering useful image patterns from a large set of images. In this paper, we introduce the notion of viewpoint patterns for image databases. Viewpoint patterns refer to patterns that capture the invariant relationships of one object from the point of view of another object. These patterns are unique and significant in images because the absolute positional information of objects for most images is not important, but rather, it is the relative distance and orientation of the objects from each other that is meaningful. We design a scalable and efficient algorithm to discover such viewpoint patterns. Experiments results on various image sets demonstrate that viewpoint patterns are meaningful and interesting to human users.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {553–558},
numpages = {6},
keywords = {image database, image mining, spatial relationship},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956819,
author = {Jermaine, Christopher},
title = {Playing Hide-and-Seek with Correlations},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956819},
doi = {10.1145/956750.956819},
abstract = {We present a method for very high-dimensional correlation analysis. The method relies equally on rigorous search strategies and on human interaction. At each step, the method conservatively "shaves off" a fraction of the database tuples and attributes, so that most of the correlations present in the data are not affected by the decomposition. Instead, the correlations become more obvious to the user, because they are hidden in a much smaller portion of the database. This process can be repeated iteratively and interactively, until only the most important correlations remain.The main technical difficulty of the approach is figuring out how to "shave off" part of the database so as to preserve most correlations. We develop an algorithm for this problem that has a polynomial running time and guarantees result quality.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {559–564},
numpages = {6},
keywords = {data mining, correlations, association rules, minimum cut},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956820,
author = {Jiang, Daxin and Pei, Jian and Zhang, Aidong},
title = {Interactive Exploration of Coherent Patterns in Time-Series Gene Expression Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956820},
doi = {10.1145/956750.956820},
abstract = {Discovering coherent gene expression patterns in time-series gene expression data is an important task in bioinformatics research and biomedical applications. In this paper, we propose an interactive exploration framework for mining coherent expression patterns in time-series gene expression data. We develop a novel tool, coherent pattern index graph, to give users highly confident indications of the existences of coherent patterns. To derive a coherent pattern index graph, we devise an attraction tree structure to record the genes in the data set and summarize the information needed for the interactive exploration. We present fast and scalable algorithms to construct attraction trees and coherent pattern index graphs from gene expression data sets. We conduct an extensive performance study on some real data sets to verify our design. The experimental results strongly show that our approach is more effective than the state-of-the-art methods in mining real gene expression data, and is scalable in mining large data sets.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {565–570},
numpages = {6},
keywords = {coherent patterns, bioinformatics, gene expression data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956821,
author = {Jin, Ruoming and Agrawal, Gagan},
title = {Efficient Decision Tree Construction on Streaming Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956821},
doi = {10.1145/956750.956821},
abstract = {Decision tree construction is a well studied problem in data mining. Recently, there has been much interest in mining streaming data. Domingos and Hulten have presented a one-pass algorithm for decision tree construction. Their work uses Hoeffding inequality to achieve a probabilistic bound on the accuracy of the tree constructed.In this paper, we revisit this problem. We make the following two contributions: 1) We present a numerical interval pruning (NIP) approach for efficiently processing numerical attributes. Our results show an average of 39% reduction in execution times. 2) We exploit the properties of the gain function entropy (and gini) to reduce the sample size required for obtaining a given bound on the accuracy. Our experimental results show a 37% reduction in the number of data instances required.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {571–576},
numpages = {6},
keywords = {streaming data, sampling, decision tree},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956822,
author = {Joshi, Sachindra and Agrawal, Neeraj and Krishnapuram, Raghu and Negi, Sumit},
title = {A Bag of Paths Model for Measuring Structural Similarity in Web Documents},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956822},
doi = {10.1145/956750.956822},
abstract = {Structural information (such as layout and look-and-feel) has been extensively used in the literatuce for extraction of interesting or relevant data, efficient storage, and query optimization. Traditionally, tree models (such as DOM trees) have been used to represent structural information, especially in the case of HTML and XML documents. However, computation of structural similarity between documents based on the tree model is computationally expensive. In this paper, we propose an alternative scheme for representing the structural information of documents based on the paths contained in the corresponding tree model. Since the model includes partial information about parents, children and siblings, it allows us to define a new family of meaningful (and at the same time computationally simple) structural similarity measures. Our experimental results based on the SIGMOD XML data set as well as HTML document collections from ibm.com, dell.com, and amazon.com show that the representation is powerful enough to produce good clusters of structurally similar pages.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {577–582},
numpages = {6},
keywords = {semi-structured documents, structural similarity},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956823,
author = {Kamishima, Toshihiro},
title = {Nantonac Collaborative Filtering: Recommendation Based on Order Responses},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956823},
doi = {10.1145/956750.956823},
abstract = {A recommender system suggests the items expected to be preferred by the users. Recommender systems use collaborative filtering to recommend items by summarizing the preferences of people who have tendencies similar to the user preference. Traditionally, the degree of preference is represented by a scale, for example, one that ranges from one to five. This type of measuring technique is called the semantic differential (SD) method. Web adopted the ranking method, however, rather than the SD method, since the SD method is intrinsically not suited for representing individual preferences. In the ranking method, the preferences are represented by orders, which are sorted item sequences according to the users' preferences. We here propose some methods to recommed items based on these order responses, and carry out the comparison experiments of these methods.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {583–588},
numpages = {6},
keywords = {recommender system, order, collaborative filtering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956824,
author = {Koren, Yehuda and Harel, David},
title = {A Two-Way Visualization Method for Clustered Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956824},
doi = {10.1145/956750.956824},
abstract = {We describe a novel approach to the visualization of hierarchical clustering that superimposes the classical dendrogram over a fully synchronized low-dimensional embedding, thereby gaining the benefits of both approaches. In a single image one can view all the clusters, examine the relations between them and study many of their properties. The method is based on an algorithm for low-dimensional embedding of clustered data, with the property that separation between all clusters is guaranteed, regardless of their nature. In particular, the algorithm was designed to produce embeddings that strictly adhere to a given hierarchical clustering of the data, so that every two disjoint clusters in the hierarchy are drawn separately.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {589–594},
numpages = {6},
keywords = {dendrogram, hierarchical clustering, information visualization},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956825,
author = {Leung, Kelvin T. and Parker, D. Stott},
title = {Empirical Comparisons of Various Voting Methods in Bagging},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956825},
doi = {10.1145/956750.956825},
abstract = {Finding effective methods for developing an ensemble of models has been an active research area of large-scale data mining in recent years. Models learned from data are often subject to some degree of uncertainty, for a variety of resoans. In classification, ensembles of models provide a useful means of averaging out error introduced by individual classifiers, hence reducing the generalization error of prediction.The plurality voting method is often chosen for bagging, because of its simplicity of implementation. However, the plurality approach to model reconciliation is ad-hoc. There are many other voting methods to choose from, including the anti-plurality method, the plurality method with elimination, the Borda count method, and Condorcet's method of pairwise comparisons. Any of these could lead to a better method for reconciliation.In this paper, we analyze the use of these voting methods in model reconciliation. We present empirical results comparing performance of these voting methods when applied in bagging. These results include some surprises, and among other things suggest that (1) plurality is not always the best voting method; (2) the number of classes can affect the performance of voting methods; and (3) the degree of dataset noise can affect the performance of voting methods. While it is premature to make final judgments about specific voting methods, the results of this work raise interesting questions, and they open the door to the application of voting theory in classification theory.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {595–600},
numpages = {6},
keywords = {voting, model reconciliation, ensemble classification},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956826,
author = {Liu, Bing and Grossman, Robert and Zhai, Yanhong},
title = {Mining Data Records in Web Pages},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956826},
doi = {10.1145/956750.956826},
abstract = {A large amount of information on the Web is contained in regularly structured objects, which we call data records. Such data records are important because they often present the essential information of their host pages, e.g., lists of products or services. It is useful to mine such data records in order to extract information from them to provide value-added services. Existing automatic techniques are not satisfactory because of their poor accuracies. In this paper, we propose a more effective technique to perform the task. The technique is based on two observations about data records on the Web and a string matching algorithm. The proposed technique is able to mine both contiguous and non-contiguous data records. Our experimental results show that the proposed technique outperforms existing techniques substantially.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {601–606},
numpages = {6},
keywords = {Web data records, Web mining, Web information integration},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956827,
author = {Liu, Guimei and Lu, Hongjun and Lou, Wenwu and Yu, Jeffrey Xu},
title = {On Computing, Storing and Querying Frequent Patterns},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956827},
doi = {10.1145/956750.956827},
abstract = {Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns. However, frequent pattern mining remains a time-consuming process, especially for very large datasets. It is therefore desirable to adopt a "mining once and using many times" strategy. Unfortunately, there has been little work reported on managing and organizing a large set of patterns for future use. In this paper, we propose a disk-based data structure, CFP-tree (Condensed Frequent Pattern Tree), for organizing frequent patterns discovered from transactional databases. In addition to an efficient algorithm for CFP-tree construction, we also developed algorithms to efficiently support two important types of queries, namely queries with minimum support constraints and queries with item constraints, against the stored patterns, as these two types of queries are basic building blocks for complex frequent pattern related mining tasks. Comprehensive experimental study has been conducted to demonstrate the effectiveness of CFP-tree and efficiency of related algorithms.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {607–612},
numpages = {6},
keywords = {frequent pattern mining, data mining and data warehousing},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956828,
author = {Ma, Junshui and Perkins, Simon},
title = {Online Novelty Detection on Temporal Sequences},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956828},
doi = {10.1145/956750.956828},
abstract = {In this paper, we present a new framework for online novelty detection on temporal sequences. This framework include a mechanism for associating each detection result with a confidence value. Based on this framework, we develop a concrete online detection algorithm, by modeling the temporal sequence using an online support vector regression algorithm. Experiments on both synthetic and real world data are performed to demonstrate the promising performance of our proposed detection algorithm.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {613–618},
numpages = {6},
keywords = {novelty detection, online algorithm, support vector regression, anomaly detection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956829,
author = {Morinaga, Satoshi and Yamanishi, Kenji and Takeuchi, Jun-ichi},
title = {Distributed Cooperative Mining for Information Consortia},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956829},
doi = {10.1145/956750.956829},
abstract = {We consider the situation where a number of agents are distributed and each of them collects a data sequence generated according to an unknown probability distribution. Here each of the distributions is specified by common parameters and individual parameters e.g., a normal distribution with an identical mean and a different variance. Here we introduce a notion of an information consortium, which is a framework where the agents cannot show raw data to one another, but they like to enjoy significant information gain for estimating the respective distributions. Such an information consortium has recently received much interest in a broad range of areas including financial risk management, ubiquitous network mining, etc. In this paper we are concerned with the following three issues: 1) how to design a collaborative strategy for agents to estimate the respective distributions in the information consortium, 2) characterizing when each agent has a benefit in terms of information gain for estimating its distribution or information loss for predicting future data, and 3) charracterizing how much benefit each agent obtains. In this paper we yield a statistical formulation of information consortia and solve all of the above three problems for a general form of probability distributions. Specifically we propose a basic strategy for cooperative estimation and derive a necessary and sufficient condition for each agent to have a significant benefit.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {619–624},
numpages = {6},
keywords = {information consortium, distributed mining, collective mining, estimation, statistical model},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956830,
author = {Neville, Jennifer and Jensen, David and Friedland, Lisa and Hay, Michael},
title = {Learning Relational Probability Trees},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956830},
doi = {10.1145/956750.956830},
abstract = {Classification trees are widely used in the machine learning and data mining communities for modeling propositional data. Recent work has extended this basic paradigm to probability estimation trees. Traditional tree learning algorithms assume that instances in the training data are homogenous and independently distributed. Relational probability trees (RPTs) extend standard probability estimation trees to a relational setting in which data instances are heterogeneous and interdependent. Our algorithm for learning the structure and parameters of an RPT searches over a space of relational features that use aggregation functions (e.g. AVERAGE, MODE, COUNT) to dynamically propositionalize relational data and create binary splits within the RPT. Previous work has identified a number of statistical biases due to characteristics of relational data such as autocorrelation and degree disparity. The RPT algorithm uses a novel form of randomization test to adjust for these biases. On a variety of relational learning tasks, RPTs built using randomization tests are significantly smaller than other models and achieve equivalent, or better, performance.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {625–630},
numpages = {6},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956831,
author = {Noble, Caleb C. and Cook, Diane J.},
title = {Graph-Based Anomaly Detection},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956831},
doi = {10.1145/956750.956831},
abstract = {Anomaly detection is an area that has received much attention in recent years. It has a wide variety of applications, including fraud detection and network intrusion detection. A good deal of research has been performed in this area, often using strings or attribute-value data as the medium from which anomalies are to be extracted. Little work, however, has focused on anomaly detection in graph-based data. In this paper, we introduce two techniques for graph-based anomaly detection. In addition, we introduce a new method for calculating the regularity of a graph, with applications to anomaly detection. We hypothesize that these methods will prove useful both for finding anomalies, and for determining the likelihood of successful anomaly detection within graph-based data. We provide experimental results using both real-world network intrusion data and artificially-created data.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {631–636},
numpages = {6},
keywords = {data mining, graph regularity, anomaly detection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956832,
author = {Pan, Feng and Cong, Gao and Tung, Anthony K. H. and Yang, Jiong and Zaki, Mohammed J.},
title = {Carpenter: Finding Closed Patterns in Long Biological Datasets},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956832},
doi = {10.1145/956750.956832},
abstract = {The growth of bioinformatics has resulted in datasets with new characteristics. These datasets typically contain a large number of columns and a small number of rows. For example, many gene expression datasets may contain 10,000-100,000 columns but only 100-1000 rows.Such datasets pose a great challenge for existing (closed) frequent pattern discovery algorithms, since they have an exponential dependence on the average row length. In this paper, we describe a new algorithm called CARPENTER that is specially designed to handle datasets having a large number of attributes and relatively small number of rows. Several experiments on real bioinformatics datasets show that CARPENTER is orders of magnitude better than previous closed pattern mining algorithms like CLOSET and CHARM.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {637–642},
numpages = {6},
keywords = {closed pattern, row enumeration, frequent pattern},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956833,
author = {Peter, William and Chiochetti, John and Giardina, Clare},
title = {New Unsupervised Clustering Algorithm for Large Datasets},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956833},
doi = {10.1145/956750.956833},
abstract = {A fast and accurate unsupervised clustering algorithm has been developed for clustering very large datasets. Though designed for very large volumes of geospatial data, the algorithm is general enough to be used in a wide variety of domain applications. The number of computations the algorithm requires is ~ O(N), and thus faster than hierarchical algorithms. Unlike the popular K-means heuristic, this algorithm does not require a series of iterations to converge to a solution. In addition, this method does not depend on initialization of a given number of cluster representatives, and so is insensitive to initial conditions. Being unsupervised, the algorithm can also "rank" each cluster based on density. The method relies on weighting a dataset to grid points on a mesh, and using a small number of rule-based agents to find the high density clusters. This method effectively reduces large datasets to the size of the grid, which is usually many orders of magnitude smaller. Numerical experiments are shown that demonstrate the advantages of this algorithm over other techniques.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {643–648},
numpages = {6},
keywords = {geospatial data, data streaming, large datasets, clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956834,
author = {Sequeira, Karlton and Zaki, Mohammed and Szymanski, Boleslaw and Carothers, Christopher},
title = {Improving Spatial Locality of Programs via Data Mining},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956834},
doi = {10.1145/956750.956834},
abstract = {In most computer systems, page fault rate is currently minimized by generic page replacement algorithms which try to model the temporal locality inherent in programs. In this paper, we propose two algorithms, one greedy and the other stochastic, designed for program specific code restructuring as a means of increasing spatial locality within a program. Both algorithms effectively decrease average working set size and hence the page fault rate. Our methods are more effective than traditional approaches due to use of domain information. We illustrate the efficacy of our algorithms on actual data mining algorithms.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {649–654},
numpages = {6},
keywords = {program locality, code restructuring, page clustering},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956835,
author = {Tang, Chun and Zhang, Aidong and Pei, Jian},
title = {Mining Phenotypes and Informative Genes from Gene Expression Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956835},
doi = {10.1145/956750.956835},
abstract = {Mining microarray gene expression data is an important research topic in bioinformatics with broad applications. While most of the previous studies focus on clustering either genes or samples, it is interesting to ask whether we can partition the complete set of samples into exclusive groups (called phenotypes) and find a set of informative genes that can manifest the phenotype structure. In this paper, we propose a new problem of simultaneously mining phenotypes and informative genes from gene expression data. Some statistics-based metrics are proposed to measure the quality of the mining results. Two interesting algorithms are developed: the heuristic search and the mutual reinforcing adjustment method. We present an extensive performance study on both real-world data sets and synthetic data sets. The mining results from the two proposed methods are clearly better than those from the previous methods. They are ready for the real-world applications. Between the two methods, the mutual reinforcing adjustment method is in general more scalable, more effective and with better quality of the mining results.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {655–660},
numpages = {6},
keywords = {informative genes, bioinformatics, phenotype, array data},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956836,
author = {Tao, Feng and Murtagh, Fionn and Farid, Mohsen},
title = {Weighted Association Rule Mining Using Weighted Support and Significance Framework},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956836},
doi = {10.1145/956750.956836},
abstract = {We address the issues of discovering significant binary relationships in transaction datasets in a weighted setting. Traditional model of association rule mining is adapted to handle weighted association rule mining problems where each item is allowed to have a weight. The goal is to steer the mining focus to those significant relationships involving items with significant weights rather than being flooded in the combinatornal explosion of insignificant relationships. We identify the challenge of using weights in the iterative process of generating large itemsets. The problem of invalidation of the "downward closure property" in the weighted setting is solved by using an improved model of weighted support measurements and exploiting a "weighted downward closure property". A new algorithm called WARM (Weighted Association Rule Mining) is developed based on the improved model. The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on simulated datasets.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {661–666},
numpages = {6},
keywords = {weighted support, weighted downward closure property, WARM algorithm, significant relationship, Weighted Association Rule Mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956837,
author = {Teoh, Soon Tee and Ma, Kwan-Liu},
title = {PaintingClass: Interactive Construction, Visualization and Exploration of Decision Trees},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956837},
doi = {10.1145/956750.956837},
abstract = {Decision trees are commonly used for classification. We propose to use decision trees not just for classification but also for the wider purpose of knowledge discovery, because visualizing the decision tree can reveal much valuable information in the data. We introduce PaintingClass, a system for interactive construction, visualization and exploration of decision trees. PaintingClass provides an intuitive layout and convenient navigation of the decision tree. PaintingClass also provides the user the means to interactively construct the decision tree. Each node in the decision tree is displayed as a visual projection of the data. Through actual examples and comparison with other classification methods, we show that the user can effectively use PaintingClass to construct a decision tree and explore the decision tree to gain additional knowledge.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {667–672},
numpages = {6},
keywords = {information visualization, visual data mining, decision trees, interactive visualization, classification},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956838,
author = {Tsamardinos, Ioannis and Aliferis, Constantin F. and Statnikov, Alexander},
title = {Time and Sample Efficient Discovery of Markov Blankets and Direct Causal Relations},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956838},
doi = {10.1145/956750.956838},
abstract = {Data Mining with Bayesian Network learning has two important characteristics: under conditions learned edges between variables correspond to casual influences, and second, for every variable T in the network a special subset (Markov Blanket) identifiable by the network is the minimal variable set required to predict T. However, all known algorithms learning a complete BN do not scale up beyond a few hundred variables. On the other hand, all known sound algorithms learning a local region of the network require an exponential number of training instances to the size of the learned region.The contribution of this paper is two-fold. We introduce a novel local algorithm that returns all variables with direct edges to and from a target variable T as well as a local algorithm that returns the Markov Blanket of T. Both algorithms (i) are sound, (ii) can be run efficiently in datasets with thousands of variables, and (iii) significantly outperform in terms of approximating the true neighborhood previous state-of-the-art algorithms using only a fraction of the training size required by the existing methods. A fundamental difference between our approach and existing ones is that the required sample depends on the generating graph connectivity and not the size of the local region; this yields up to exponential savings in sample relative to previously known algorithms. The results presented here are promising not only for discovery of local causal structure, and variable selection for classification, but also for the induction of complete BNs.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {673–678},
numpages = {6},
keywords = {robust and scalable statistical methods, novel data mining algorithms, Bayesian Networks},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956839,
author = {Yu, Hang and Chang, Ee-Chien},
title = {Distributed Multivariate Regression Based on Influential Observations},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956839},
doi = {10.1145/956750.956839},
abstract = {Large-scale data sets are sometimes logically and physically distributed in separate databases. The issues of mining these data sets are not just their sizes, but also the distributed nature. The complication is that communicating all the data to a central database would be too slow. To reduce communication costs, one could compress the data during transmission. Another method is random sampling. We propose an approach for distributed multivariate regression based on sampling and discuss its relationship with the compression method. The central idea is motivated by the observation that, although communication is limited, each individual site can still scan and process all the data it holds. Thus it is possible for the site to communicate only influential samples without seeing data in other sites. We exploit this observation and derive a method that provides tradeoff between communication cost and accuracy. Experimental results show that it is better than the compression method and random sampling.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {679–684},
numpages = {6},
keywords = {sampling, learning curve, multivariate linear regression, distributed data mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956840,
author = {Yu, Lei and Liu, Huan},
title = {Efficiently Handling Feature Redundancy in High-Dimensional Data},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956840},
doi = {10.1145/956750.956840},
abstract = {High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {685–690},
numpages = {6},
keywords = {redundancy, high-dimensional data, feature selection},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956842,
author = {Alonso, Rafael and Bloom, Jeffrey A. and Li, Hua and Basu, Chumki},
title = {An Adaptive Nearest Neighbor Search for a Parts Acquisition EPortal},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956842},
doi = {10.1145/956750.956842},
abstract = {One of the major hurdles in maintaining long-lived electronic systems is that electronic parts become obsolete, no longer available from the original suppliers. When this occurs, an engineer is tasked with resolving the problem by finding a replacement that is "as similar as possible" to the original part. The current approach involves a laborious manual search through several electronic portals and data books. The search is difficult because potential replacements may differ from the original and from each other by one or more parameters. Worse still, the cumbersome nature of this process may cause the engineers to miss appropriate solutions amid the many thousands of parts listed in industry catalogs.In this paper, we address this problem by introducing the notion of a parametric "distance" between electronic components. We use this distance to search a large parts data set and recommend likely replacements. Recommendations are based on an adaptive nearest-neighbor search through the parametric data set. For each user, we learn how to scale the axes of the feature space in which the nearest neighbors are sought. This allows the system to learn each user's judgment of the phrase "as similar as possible."},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {693–698},
numpages = {6},
keywords = {user profiling, k-nearest neighbor classification, adaptive search, query by example},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956843,
author = {Barry, Philip and Zhang, Jianping and McDonald, Mary},
title = {Architecting a Knowledge Discovery Engine for Military Commanders Utilizing Massive Runs of Simulations},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956843},
doi = {10.1145/956750.956843},
abstract = {The Marine Corps' Project Albert seeks to model complex phenomenon by observing the behavior of relatively simple simulations over thousands of runs. A rich data base is developed by running the simulations thousands of times, varying the agent and scenario input parameters as well as the random seeds. Exploring this result space may provide significant insight into nonlinear, surprising, and emergent behaviors. Capturing these results can provide a path for making the results usable for decision support to a military commander. This paper presents two data mining approaches, rule discovery and Bayesian networks, for analyzing the Albert simulation data. The first approach generates rules from the data and then uses them to create descriptive model. The second generates Bayesian Networks which provide a quantitative belief model for decision support. Both of these approaches as well as the Project Albert simulations are framed in the context of a system architecture for decision support.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {699–704},
numpages = {6},
keywords = {rule discovery, agent-based simulations, Bayesian Networks},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956844,
author = {Dasu, Tamraparni and Vesonder, Gregg T. and Wright, Jon R.},
title = {Data Quality through Knowledge Engineering},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956844},
doi = {10.1145/956750.956844},
abstract = {Traditionally, data quality programs have acted as a preprocessing stage to make data suitable for a data mining or analysis operation. Recently, data quality concepts have been applied to databases that support business operations such as provisioning and billing. Incorporating business rules that drive operations and their associated data processes is critically important to the success of such projects. However, there are many practical complications. For example, documentation on business rules is often meager. Rules change frequently. Domain knowledge is often fragmented across experts, and those experts do not always agree. Typically, rules have to be gathered from subject matter experts iteratively, and are discovered out of logical or procedural sequence, like a jigsaw puzzle. Our approach is to impement business rules as constraints on data in a classical expert system formalism sometimes called production rules. Our system works by allowing good data to pass through a system of constraints unchecked. Bad data violate constraints and are flagged, and then fed back after correction. Constraints are added incrementally as better understanding of the business rules is gained. We include a real-life case study.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {705–710},
numpages = {6},
keywords = {static and dynamic constraints, data quality, business operations databases},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956845,
author = {Lau, Gloria T. and Law, Kincho H. and Wiederhold, Gio},
title = {Similarity Analysis on Government Regulations},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956845},
doi = {10.1145/956750.956845},
abstract = {Government regulations are semi-structured text documents that are often voluminous, heavily cross-referenced between provisions and even ambiguous. Multiple sources of regulations lead to difficulties in both understanding and complying with all applicable codes. In this work, we propose a framework for regulation management and similarity analysis. An online repository for legal documents is created with the help of text mining tool, and users can access regulatory documents either through the natural hierarchy of provisions or from a taxonomy generated by knowledge engineers based on concepts. Our similarity analysis core identifies relevant provisions and brings them to the user's attention, and this is performed by utilizing both the hierarchical and referential structures of regulations to provide a better comparison between provisions. Preliminary results show that our system reveals hidden similarities that are not apparent between provisions based on node content comparisons.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {711–716},
numpages = {6},
keywords = {regulations, similarity analysis, legal informatics, text mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956846,
author = {Mayer, Uwe F. and Sarkissian, Armand},
title = {Experimental Design for Solicitation Campaigns},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956846},
doi = {10.1145/956750.956846},
abstract = {Data mining techniques are routinely used by fundraisers to select those prospects from a large pool of candidates who are most likely to make a financial contribution. These techniques often rely on statistical models based on trial performance data. This trial performance data is typically obtained by soliciting a smaller sample of the possible prospect pool. Collecting this trial data involves a cost; therefore the fundraiser is interested in keeping the trial size small while still collecting enough data to build a reliable statistical model that will be used to evaluate the remainder of the prospects.We describe an experimental design approach to optimally choose the trial prospects from an existing large pool of prospects. Prospects are clustered to render the problem practically tractable. We modify the standard D-optimality algorithm to prevent repeated selection of the same prospect cluster, since each prospect can only be solicited at most once.We assess the benefits of this approach on the KDD-98 data set by comparing the performance of the model based on the optimal trial data set with that of a model based on a randomly selected trial data set of equal size.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {717–722},
numpages = {6},
keywords = {solicitation campaign, data collection, experimental design},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956847,
author = {Otey, M. and Parthasarathy, S. and Ghoting, A. and Li, G. and Narravula, S. and Panda, D.},
title = {Towards NIC-Based Intrusion Detection},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956847},
doi = {10.1145/956750.956847},
abstract = {We present and evaluate a NIC-based network intrusion detection system. Intrusion detection at the NIC makes the system potentially tamper-proof and is naturally extensible to work in a distributed setting. Simple anomaly detection and signature detection based models have been implemented on the NIC firmware, which has its own processor and memory. We empirically evaluate such systems from the perspective of quality and performance (bandwidth of acceptable messages) under varying conditions of host load. The preliminary results we obtain are very encouraging and lead us to believe that such NIC-based security schemes could very well be a crucial part of next generation network security systems.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {723–728},
numpages = {6},
keywords = {network security, network interface cards, network intrusion detection, NICs, data mining},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956848,
author = {Perng, Chang-Shing and Thoenen, David and Grabarnik, Genady and Ma, Sheng and Hellerstein, Joseph},
title = {Data-Driven Validation, Completion and Construction of Event Relationship Networks},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956848},
doi = {10.1145/956750.956848},
abstract = {Event management is a focal point in building and maintaining high quality information infrastructures. We have witnessed the shift of the paradigm of event management in practice from root cause analysis (RCA) to action-oriented analysis (AOA). IBM has developed a pioneer event management methodology (EMD) based on the AOA paradigm and applied it to more than two hundred production sites with success. Foreseeably, more and more event management professionals will apply AOA in different incarnations in building proactive management facilities. By that, building correct and effective Event Relationship Networks (ERNs) becomes the dominating activity in AOA service design process. Currently, the quality of ERNs and the cost of building them largely depend on the knowledge of domain experts. We believe that we can utilize historical event logs in shortening the ERNs design process and perfecting the quality of ERNs. In this paper, we describe in detail how to apply this data-driven approach in ERN validation, completion and construction.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {729–734},
numpages = {6},
keywords = {proactive management, action oriented-analysis, ERN Construction, root cause analysis, event relationship networks, ERN Validation, event management methodology},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956849,
author = {Pratt, Kevin B. and Tschapek, Gleb},
title = {Visualizing Concept Drift},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956849},
doi = {10.1145/956750.956849},
abstract = {We describe a visualization technique that uses brushed, parallel histograms to aid in understanding concept drift in multidimensional problem spaces. This technique illustrates the relationship between changes in distributions of multiple antecedent feature values and the outcome distribution. We can also observe effects on the relative utilization of predictive rules. Our parallel histogram technique solves the over-plotting difficulty of parallel coordinate graphs and the difficulty of comparing distributions of brushed and original data. We demonstrate our technique's usefulness in understanding concept drifts in power demand and stock investment returns.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {735–740},
numpages = {6},
keywords = {brushing, parallel coordinate graph, visualization, concept drift, parallel histogram},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956850,
author = {Shimazu, Keiko and Momma, Atsuhito and Furukawa, Koichi},
title = {Experimental Study of Discovering Essential Information from Customer Inquiry},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956850},
doi = {10.1145/956750.956850},
abstract = {This paper reports the result of our experimental study on a new method of applying an association rule miner to discover useful information from customer inquiry database in a call center of a company. It has been claimed that association rule mining is not suited for text mining. To overcome this problem, we propose (1) to generate sequential data set of words with dependency structure from the Japanese text database, and (2) to employ a new method for extracting meaningful association rules by applying a new rule selection criterion. Each inquiry in the sequential data was represented as a list of word pairs, each of which consists of a verb and its dependent noun. The association rules were induced regarding each pair of words as an item. The rule selection criterion comes from our principle that we put heavier weights to co-occurrence of multiple items more than single item occurrence. We regarded a rule important if the existence of the items in the rule body significantly affects the occurrence of the item in the rule head. The selected rules were then categorized to form meaningful information classes. With this method, we succeeded in extracting useful information classes from the text database, which were not acquired by only simple keyword retrieval. Also, inquiries with multiple aspects were properly classified into corresponding multiple categories.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {741–746},
numpages = {6},
keywords = {data mining, text mining, prior confidence, association rule, syntactic dependency, posterior confidence},
location = {Washington, D.C.},
series = {KDD '03}
}

@inproceedings{10.1145/956750.956851,
author = {Zhang, Zhongfei (Mark) and Salerno, John J. and Yu, Philip S.},
title = {Applying Data Mining in Investigating Money Laundering Crimes},
year = {2003},
isbn = {1581137370},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956750.956851},
doi = {10.1145/956750.956851},
abstract = {In this paper, we study the problem of applying data mining to facilitate the investigation of money laundering crimes (MLCs). We have identified a new paradigm of problems --- that of automatic community generation based on uni-party data, the data in which there is no direct or explicit link information available. Consequently, we have proposed a new methodology for Link Discovery based on Correlation Analysis (LDCA). We have used MLC group model generation as an exemplary application of this problem paradigm, and have focused on this application to develop a specific method of automatic MLC group model generation based on timeline analysis using the LDCA methodology, called CORAL. A prototype of CORAL method has been implemented, and preliminary testing and evaluations based on a real MLC case data are reported. The contributions of this work are: (1) identification of the uni-party data community generation problem paradigm, (2) proposal of a new methodology LDCA to solve for problems in this paradigm, (3) formulation of the MLC group model generation problem as an example of this paradigm, (4) application of the LDCA methodology in developing a specific solution (CORAL) to the MLC group model generation problem, and (5) development, evaluation, and testing of the CORAL prototype in a real MLC case data.},
booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {747–752},
numpages = {6},
keywords = {CORAL, clustering, community generation, Link Discovery based on Correlation Analysis (LDCA), histogram, MLC Group Models, timeline analysis, Money Laundering Crimes (MLCs), uni-party data, bi-party data},
location = {Washington, D.C.},
series = {KDD '03}
}

