@inproceedings{10.1145/3249713,
author = {Bonifati, A.},
title = {Session Details: Keynote Address},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249713},
doi = {10.1145/3249713},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183552,
author = {Amer-Yahia, Sihem},
title = {The Power of Structured Data on the Web},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183552},
doi = {10.1145/1183550.1183552},
abstract = {I will talk about two major roles that structured data is playing on the Web today. The first one is the ability to capture users' intent and display more relevant advertisements thereby reaching the long tail of users and advertisers. Nowadays, ads are treated as text and can only be searched in a limited way. The second one is related to enabling community search on the Web 2.0. Nowadays, users of Flickr, YouTube and Yahoo! Groups supply structured content, organize it, and search it. I will discuss how structure-awareness when representing users, advertisements and communities offers a much better alternative to existing search.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {1–2},
numpages = {2},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/3249714,
author = {Fundulaki, I.},
title = {Session Details: XML Data Management and P2P},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249714},
doi = {10.1145/3249714},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183554,
author = {Wang, Fusheng and Zhou, Xin and Zaniolo, Carlo},
title = {Bridging Relational Database History and the Web: The XML Approach},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183554},
doi = {10.1145/1183550.1183554},
abstract = {The preservation of digital artifacts represents an unanswered challenge for the modern information society: XML and its query languages provide an effective environment to address this challenge because of their ability to support temporal information and queries, and make it easy to publish database history to the Web. In this paper, we focus on the problem of preserving, publishing, and querying efficiently the history of a relational database. Past research on temporal databases revealed the difficulty of achieving satisfactory solutions using flat relational tables and SQL. Here we show that the problem can be solved using (a) XML to support temporally grouped representations of the database history, and (b) XQuery to express powerful temporal queries on such representations. Furthermore, the approach is quite general and it can be used to preserve and query the history of multi-version XML documents. Then we turn to the problem of efficient implementation, and we investigate alternative approaches, including (i) XML DBMS, (ii) shredding XML into relational tables and using SQL/XML on these tables, (iii) SQL:2003 nested tables, and iv) OR-DBMS extended with XML support. These experiments suggest that a combination of temporal XML views and physical relational tables provides the best approach for managing temporal database information.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {3–10},
numpages = {8},
keywords = {xquery, temporal database, temporal query, XML database, temporal grouping},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183555,
author = {Bry, Fran\c{c}ois and Furche, Tim and Linse, Benedikt and Schroeder, Andreas},
title = {Efficient Evaluation of N-Ary Conjunctive Queries over Trees and Graphs},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183555},
doi = {10.1145/1183550.1183555},
abstract = {N-ary conjunctive queries, i.e., queries with any number of answer variables, are the formal core of many Web query languages including XSLT, XQuery, SPARQL, and Xcerpt. Despite a considerable body of research on the optimization of such queries over tree-shaped XML data, little attention has been paid so far to efficient access to graph-shaped XML, RDF, or Topic Maps. We propose the first evaluation technique for n-ary conjunctive queries that applies to both tree- and graph-shaped data and retains the same complexity as the best known approaches that are restricted to tree-shaped data only. Furthermore, the approach treats tree and graph-shaped queries uniformly without sacrificing evaluation complexity on the restricted query class. The core of the evaluation technique is based on dynamic programming using a memoization data structure, called "memoization matrix". It can be populated and consumed in different ways. For each of population and consumption, we propose two resp. three algorithms each having their own advantages. The complexity of the algorithms is compared analytically and experimentally.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {11–18},
numpages = {8},
keywords = {semi-structured data, XML, query evaluation and optimization, memoization, RDF, conjunctive queries},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183556,
author = {Mandreoli, Federica and Martoglia, Riccardo and Sassatelli, Simona and Penzo, Wilma},
title = {SRI: Exploiting Semantic Information for Effective Query Routing in a PDMS},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183556},
doi = {10.1145/1183550.1183556},
abstract = {The huge amount of data available from Internet information sources has focused much attention on the sharing of distributed information through Peer Data Management Systems (PDMSs). In a PDMS, peers have a schema on their local data, and they are related each other through semantic mappings that can be defined between their own schemas.Querying a PDMS means either flooding the network with messages to all peers or take advantage of a routing mechanism to reformulate a query only on the best peers selected according to some given criteria. As reformulations may lead to semantic approximations, we deem that such approximations can be exploited for locating the semantically best directions to forward a query to.In this paper, we propose a distributed index mechanism where each peer is provided with a Semantic Routing Index (SRI) for routing queries effectively. A fuzzy-oriented model for SRI is presented where operations for creating and maintaining SRIs are well-founded. In addition, we show how SRIs can be employed in the query processing phase with the aim of reducing the space of reformulations. Finally, we conduct a series of meaningful experiments showing the effectiveness of the proposed approach.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {19–26},
numpages = {8},
keywords = {data-sharing P2P systems, query routing, semantics},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/3249715,
author = {Mitra, P.},
title = {Session Details: Web Ranking and Classification},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249715},
doi = {10.1145/3249715},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183558,
author = {Raschid, Louiqa and Wu, Yao and Lee, Woei-Jyh and Vidal, Mar\'{\i}a Esther and Tsaparas, Panayiotis and Srinivasan, Padmini and Sehgal, Aditya Kumar},
title = {Ranking Target Objects of Navigational Queries},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183558},
doi = {10.1145/1183550.1183558},
abstract = {Web navigation plays an important role in exploring public interconnected data sources such as life science data. A navigational query in the life science graph produces a result graph which is a layered directed acyclic graph (DAG). Traversing the result paths in this graph reaches a target object set (TOS). The challenge for ranking the target objects is to provide recommendations that re ect the relative importance of the retrieved object, as well as its relevance to the specific query posed by the scientist. We present a metric layered graph PageRank (lgPR) to rank target objects based on the link structure of the result graph. LgPR is a modification of PageRank; it avoids random jumps to respect the path structure of the result graph. We also outline a metric layered graph ObjectRank (lgOR) which extends the metric ObjectRank to layered graphs. We then present an initial evaluation of lgPR. We perform experiments on a real-world graph of life sciences objects from NCBI and report on the ranking distribution produced by lgPR. We compare lgPR with PageRank. In order to understand the characteristics of lgPR, an expert compared the Top K target objects (publications in the PubMed source) produced by lgPR and a word-based ranking method that uses text features extracted from an external source (such as Entrez Gene) to rank publications.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {27–34},
numpages = {8},
keywords = {ranking, pagerank, navigational query, link analysis},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183559,
author = {Lindemann, Christoph and Littig, Lars},
title = {Coarse-Grained Classification of Web Sites by Their Structural Properties},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183559},
doi = {10.1145/1183550.1183559},
abstract = {In this paper, we identify and analyze structural properties which reflect the functionality of a Web site. These structural properties consider the size, the organization, the composition of URLs, and the link structure of Web sites. Opposed to previous work, we perform a comprehensive measurement study to delve into the relation between the structure and the functionality of Web sites. Our study focuses on five of the most relevant functional classes, namely Academic, Blog, Corporate, Personal, and Shop. It is based upon more than 1,400 Web sites composed of 7 million crawled and 47 million known Web pages. We present a detailed statistical analysis which provides insight into how structural properties can be used to distinguish between Web sites from different functional classes. Building on these results, we introduce a content-independent approach for the automated coarse-grained classification of Web sites. A na\"{\i}ve Bayesian classifier with advanced density estimation yields a precision of 82% and recall of 80% for the classification of Web sites into the considered classes.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {35–42},
numpages = {8},
keywords = {web structure mining, search engines, web site classification, web mining, na\"{\i}ve bayesian classification, web measurement},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183560,
author = {Nelson, Michael L. and Smith, Joan A. and del Campo, Ignacio Garcia},
title = {Efficient, Automatic Web Resource Harvesting},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183560},
doi = {10.1145/1183550.1183560},
abstract = {There are two problems associated with conventional web crawling techniques: a crawler cannot know if all resources at a non-trivial web site have been discovered and crawled ("the counting problem") and the human-readable format of the resources are not always suitable for machine processing ("the representation problem"). We introduce an approach that solves these two problems by implementing support for both the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) and MPEG-21 Digital Item Declaration Language (DIDL) into the web server itself. We present the Apache module "mod_oai", which can be used to address the counting problem by listing all valid URIs at a web server and efficiently discovering updates and additions on subsequent crawls. Our experiments indicated comparable performance for initial crawls, and dramatic increases in update speed mod_oaican also be used to address the representation problem by providing "preservation ready" versions of web resources aggregated with their respective forensic metadata in MPEG-21 DIDL format.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {43–50},
numpages = {8},
keywords = {web crawling, OAI-PMH, mod_oai},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/3249716,
author = {Geerts, F.},
title = {Session Details: Web Resource Crawling and Searching},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249716},
doi = {10.1145/3249716},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183562,
author = {Desai, Ronak and Yang, Qi and Wu, Zonghuan and Meng, Weiyi and Yu, Clement},
title = {Identifying Redundant Search Engines in a Very Large Scale Metasearch Engine Context},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183562},
doi = {10.1145/1183550.1183562},
abstract = {For a given set of search engines, a search engine is redundant if its searchable contents can be found from other search engines in this set. In this paper, we propose a method to identify redundant search engines in a very large-scale metasearch engine context. The general problem is equivalent to an NP hard problem -- the set-covering problem. Due to the large number of search engines that need to be considered and the large sizes of these search engines, approximate solutions must be developed. In this paper, we propose a general methodology to tackle this problem and within the context of this methodology, we propose several new heuristic algorithms for solving the set-covering problem.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {51–58},
numpages = {8},
keywords = {redundant search engine identification, set-covering problem},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183563,
author = {Chirita, Paul - Alexandru and Firan, Claudiu S. and Nejdl, Wolfgang},
title = {Pushing Task Relevant Web Links down to the Desktop},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183563},
doi = {10.1145/1183550.1183563},
abstract = {Searching the web has become a task in many people's work, without which subsequent tasks would be hard to carry out or even impossible. But as people tend to have less time for querying the web or even for searching their personal computer for information they need, it becomes common to skip information gathering activities like trying to find useful resources on the web because of the "effort" it takes to query a web search engine. In this paper we propose to use software agents that collect useful web specific related information which would otherwise not be viewed at all. More specifically, we present two new algorithms to automatically search the web and recommend URLs relevant to user's current work, defined through his or her active personal desktop documents. Our experiments show our proposed algorithms, Sentence Selection and Lexical Compounds, to yield significant improvement over simple Term Frequency based web query generation, which we used as a baseline.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {59–66},
numpages = {8},
keywords = {user profile, personalized web search, document summarization, just-in-time information retrieval},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183564,
author = {McCown, Frank and Smith, Joan A. and Nelson, Michael L.},
title = {Lazy Preservation: Reconstructing Websites by Crawling the Crawlers},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183564},
doi = {10.1145/1183550.1183564},
abstract = {Backup of websites is often not considered until after a catastrophic event has occurred to either the website or its webmaster. We introduce "lazy preservation" -- digital preservation performed as a result of the normal operation of web crawlers and caches. Lazy preservation is especially suitable for third parties; for example, a teacher reconstructing a missing website used in previous classes. We evaluate the effectiveness of lazy preservation by reconstructing 24 websites of varying sizes and composition using Warrick, a web-repository crawler. Because of varying levels of completeness in any one repository, our reconstructions sampled from four different web repositories: Google (44%), MSN (30%), Internet Archive (19%) and Yahoo (7%). We also measured the time required for web resources to be discovered and cached (10-103 days) as well as how long they remained in cache after deletion (7-61 days).},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {67–74},
numpages = {8},
keywords = {recovery, digital preservation, search engine, cached resources},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/3249717,
author = {McCown, F.},
title = {Session Details: Web Organization},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3249717},
doi = {10.1145/3249717},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183566,
author = {Jaiswal, Anuj R. and Giles, C. Lee and Mitra, Prasenjit and Wang, James Z.},
title = {An Architecture for Creating Collaborative Semantically Capable Scientific Data Sharing Infrastructures},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183566},
doi = {10.1145/1183550.1183566},
abstract = {Increasingly, scientists are seeking to collaborate and share data among themselves. Such sharing is can be readily done by publishing data on the World-Wide Web. Meaningful querying and searching on such data depends upon the availability of accurate and adequate metadata that describes the data and the sources of the data. In this paper, we outline the architecture of an implemented cyber-infrastructure for chemistry that provides tools for users to upload datasets and their metadata to a database. Our proposal combines a two level metadata system with a centralized database repository and analysis tools to create an effective and capable data sharing infrastructure. Our infrastructure is extensible in that it can handle data in different formats and allows different analytic tools to be plugged in.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {75–82},
numpages = {8},
keywords = {architecture for cyber-infrastructures, inter-operation, research dataset integration, scientific databases},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

@inproceedings{10.1145/1183550.1183567,
author = {Nambiar, Ullas and Ludaescher, Bertram and Lin, Kai and Baru, Chaitan},
title = {The GEON Portal: Accelerating Knowledge Discovery in the Geosciences},
year = {2006},
isbn = {1595935258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183550.1183567},
doi = {10.1145/1183550.1183567},
abstract = {Geoscience studies produce data from various observations, experiments, and simulations at an enormous rate. With proliferation of applications and data formats, the geoscience research community faces many challenges in effectively managing and sharing resources and in efficiently integrating and analyzing the data. In this paper, we discuss how this challenge is being addressed by the GEON Portal, a Web based distributed resource management system that provides integrated access to data and tools needed for knowledge discovery in the geosciences. Unlike previous data management efforts that were either data-driven or application-driven, the GEON Portal provides facilities for efficient sharing, discovery and integration of both data and services that use geoscience data. We identify the challenges involved in managing geoscientific resources and provide solutions that exploit the syntactic, semantic, temporal and spatial metadata associated with the resources. One of our goals is to provide some insight into the challenges involved in providing a comprehensive scientific data management solution based on our experiences with geoscientific data.},
booktitle = {Proceedings of the 8th Annual ACM International Workshop on Web Information and Data Management},
pages = {83–90},
numpages = {8},
keywords = {data integration, metadata, geoinformatics, semantic search},
location = {Arlington, Virginia, USA},
series = {WIDM '06}
}

