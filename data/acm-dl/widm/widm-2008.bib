@inproceedings{10.1145/3247202,
author = {Polyzotis, Neoklis},
title = {Session Details: Data Mining and Clustering},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247202},
doi = {10.1145/3247202},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
numpages = {1},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458504,
author = {Hu, Meishan and Sun, Aixin and Lim, Ee-Peng},
title = {Event Detection with Common User Interests},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458504},
doi = {10.1145/1458502.1458504},
abstract = {In this paper, we aim at detecting events of common user interests from huge volume of user-generated content. The degree of interest from common users in an event is evidenced by a significant surge of event-related queries issued to search for documents (e.g., news articles, blog posts) relevant to the event. Taking the stream of queries from users and the stream of documents as input, our proposed framework seamlessly integrates the two streams into a single stream of query profiles. A query profile is a set of documents matching a query at a given time. With the single stream of query profiles, the well-studied techniques in event detection (e.g., incremental clustering) could be easily applied. In our experiments using real data collected from Blog and News search engines respectively, the proposed technique achieved very high event detection accuracy.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
keywords = {query profile, blog, event detection, popular queries},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458505,
author = {Senellart, Pierre and Mittal, Avin and Muschick, Daniel and Gilleron, R\'{e}mi and Tommasi, Marc},
title = {Automatic Wrapper Induction from Hidden-Web Sources with Domain Knowledge},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458505},
doi = {10.1145/1458502.1458505},
abstract = {We present an original approach to the automatic induction of wrappers for sources of the hidden Web that does not need any human supervision. Our approach only needs domain knowledge expressed as a set of concept names and concept instances. There are two parts in extracting valuable data from hidden-Web sources: understanding the structure of a given HTML form and relating its fields to concepts of the domain, and understanding how resulting records are represented in an HTML result page. For the former problem, we use a combination of heuristics and of probing with domain instances; for the latter, we use a supervised machine learning technique adapted to tree-like information on an automatic, imperfect, and imprecise, annotation using the domain knowledge. We show experiments that demonstrate the validity and potential of the approach.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {9–16},
numpages = {8},
keywords = {wrapper, probing, web service, deep web, form, hidden web, information extraction, invisible web},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458506,
author = {Shnaiderman, Lila and Shmueli, Oded and Bordawekar, Rajesh},
title = {PIXSAR: Incremental Reclustering of Augmented XML Trees},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458506},
doi = {10.1145/1458502.1458506},
abstract = {XML is one of the primary encoding schemes for data and knowledge. We investigate incremental physical data clustering in systems that store XML documents using a native format. We formulate the XML clustering problem as an augmented (with sibling edges) tree partitioning problem and propose the PIXSAR (Practical Incremental XML Sibling Augmented Reclustering) algorithm for incrementally clustering XML documents. We show the fundamental importance of workload-driven dynamically rearranging storage.PIXSAR incrementally executes reclustering operations on selected subgraphs of the global augmented document tree. The subgraphs are implied by significant changes in the workload. As the workload changes, PIXSAR incrementally djusts the XML data layout so as to better fit the workload. PIXSAR's main parameters are the radius, in pages, of the augmented portion to be reclustered and the way reclustering is triggered. We briefly explore some of the effects of indexes; a full treatment of indexes is the subject of another paper.We use an experimental data clustering system that includes a fast disk simulator and File System simulator for storing native XML data. We use a novel method for 'exporting' the Saxon query processor into our setting. Experimental results indicate that using PIXSAR significantly reduces the number of page faults (counting ALL page faults incurred while querying the document as well as maintenance operations) thereby resulting in improved query performance.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {17–24},
numpages = {8},
keywords = {XML, reclustering, incremental},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458507,
author = {Zhu, Jianhan},
title = {A Study of the Relationship between Ad Hoc Retrieval and Expert Finding in Enterprise Environment},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458507},
doi = {10.1145/1458502.1458507},
abstract = {Ad hoc retrieval returns a ranked list of documents in response to a search query, while expert finding returns a ranked list of people in response to an expertise request in the form of a search query, e.g., "information retrieval". In current state of the art expert finding approaches, ad hoc retrieval is a key component for locating documents relevant to the expertise request. While ad hoc retrieval has been well researched in information retrieval, no previous work has been carried out on the effects of document retrieval in expert finding. The main contribution of this paper is that we are the first to study the effect of document retrieval in expert finding via a background smoothing parameter in a language modeling approach and two document features, namely, anchor text and indegree. Our research gives insight into how to design an effective approach for both ad hoc retrieval and expert finding in enterprise environment. Our experiments on the TREC (Text REtrieval Conference) 2007 Enterprise Track CSIRO (Australian Commonwealth Scientific and Research Organization) dataset shows that background smoothing helps improve ad hoc retrieval but does not help or even hurt expert finding, anchor text helps expert finding but hurt ad hoc retrieval when weighted high, and indegree helps expert finding but does not help improve ad hoc retrieval significantly.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {25–30},
numpages = {6},
keywords = {ad hoc retrieval, expert finding, language models},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458508,
author = {Huang, Shen and Wu, Xiaoyuan and Bolivar, Alvaro},
title = {The Effect of Title Term Suggestion on E-Commerce Sites},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458508},
doi = {10.1145/1458502.1458508},
abstract = {Most E-Commerce websites rely on title keyword search to accurately retrieve the items for sale in a particular category. We have found that the titles of many items on eBay are shortened or not very specific, which leads to ineffective results when searched. One possible solution is to recommend the sellers relevant and informative terms for title expansion without any change of search function. The related technique has been explored in previous work such as query expansion and keyword suggestion. In this paper, we study the effect of term suggestion on title-based search. A frequently used approach, co-occurrence, is tested on a dataset collected from eBay website (www.ebay.com). Besides, for suggestion algorithm, we take into account three particular features in our application scenario, including concept term, description relevance and chance-to-be viewed. Although the experiments are conducted on eBay data, we believe that considering E-Commerce particularities will help us to customize the suggestion according to the requirements of web commerce.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {31–38},
numpages = {8},
keywords = {term suggestion, e-commerce, keyword suggestion},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/3247203,
author = {Chan, Chee-Yong},
title = {Session Details: System Issues},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247203},
doi = {10.1145/3247203},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
numpages = {1},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458510,
author = {Klein, Martin and Nelson, Michael L.},
title = {A Comparison of Techniques for Estimating IDF Values to Generate Lexical Signatures for the Web},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458510},
doi = {10.1145/1458502.1458510},
abstract = {For bounded datasets such as the TREC Web Track the computation of term frequency (TF) and inverse document frequency (IDF) is not difficult. However, since IDF cannot be directly calculated for the entire web, it must be estimated. We see a need to estimate accurate IDF values to generate TF-IDF based lexical signatures (LSs) of web pages. Future applications for generating such LSs require a real time IDF computation. Therefore we conducted a comparison study of different methods to estimate IDF values of web pages. Our objective is to investigate how accurate these estimation methods are compared to the a baseline. We use the Google N-grams as our baseline and compare it against two IDF estimation techniques which are based on: 1) a "local universe" consisting of textual content and the according document frequencies from copies of URLs from the Internet Archive and 2) "screen scraping", a technique to query the Google web interface for document frequencies. We found a term overlap of 70 to 80% between the results of the two methods and the baseline. We further discovered a great agreement in rank correlation of TF-IDF ranked terms between our methods. Kendall τ is approximately 0.8 and the M-Score (penalizing discordances in higher ranks) is even higher, it peaks at well above 0.9. These preliminary results lead us to the conclusion that both methods are appropriate for creating accurate IDF values for web pages.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {39–46},
numpages = {8},
keywords = {lexical signature, inverse document frequency},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458511,
author = {Marin, Mauricio and Paredes, Rodrigo and Bonacic, Carolina},
title = {High-Performance Priority Queues for Parallel Crawlers},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458511},
doi = {10.1145/1458502.1458511},
abstract = {Large scale data centers for crawlers are able to maintain a very large number of active http connections in order to download as fast as possible the usually huge number of web pages from given sections of the WWW. This generates a continuous stream of new URLs of documents to be downloaded and it is clear that the associated work-load can only be served efficiently with proper parallel computing techniques. The incoming new URLs have to be organized by a priority measure in order to download the most relevant documents first. Efficiently managing them along with other synchronization issues such as URLs downloaded by different processing nodes forming a cluster of computers are the matters of this paper. We propose efficient and scalable strategies which consider intra-node multi-core multi-threading on an inter-nodes distributed memory environment, including efficient use of secondary memory.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {47–54},
numpages = {8},
keywords = {priority queues, parallel and distributed computing},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458512,
author = {Garcia-Alvarado, Carlos and Ordonez, Carlos},
title = {Information Retrieval from Digital Libraries in SQL},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458512},
doi = {10.1145/1458502.1458512},
abstract = {Information retrieval techniques have been traditionally exploited outside of relational database systems, due to storage overhead, the complexity of programming them inside the database system, and their slow performance in SQL implementations. This project supports the idea that searching and querying digital libraries with information retrieval models in relational database systems can be performed with optimized SQL queries and User-Defined Functions. In our research, we propose several techniques divided into two phases: storing and retrieving. The storing phase includes executing document pre-processing, stop-word removal and term extraction, and the retrieval phase is implemented with three fundamental IR models: the popular Vector Space Model, the Okapi Probabilistic Model, and the Dirichlet Prior Language Model. We conduct experiments using article abstracts from the DBLP bibliography and the ACM Digital Library. We evaluate several query optimizations, compare the on-demand and the static weighting approaches, and we study the performance with conjunctive and disjunctive queries with the three ranking models. Our prototype proved to have linear scalability and a satisfactory performance with medium-sized document collections. Our implementation of the Vector Space Model is competitive with the two other models.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {55–62},
numpages = {8},
keywords = {documents, SQL, query optimization, ranking},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458513,
author = {Doka, Katerina and Tsoumakos, Dimitrios and Koziris, Nectarios},
title = {HiPPIS: An Online P2P System for Efficient Lookups on d-Dimensional Hierarchies},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458513},
doi = {10.1145/1458502.1458513},
abstract = {In this paper we describe HiPPIS, a system that enables efficient storage and on-line querying of multidimensional data organized into concept hierarchies and dispersed over a network. Our scheme utilizes an adaptive algorithm that automatically adjusts the level of indexing according to the granularity of the incoming queries, without assuming any prior knowledge of the workload. Efficient roll-up and drill-down operations take place in order to maximize the performance by minimizing query flooding. Extensive experimental evaluations show that, on top of the advantages that a distributed storage offers, our method answers the large majority of incoming queries, both point and aggregate ones, without flooding the network. At the same time, it manages to preserve the hierarchical nature of data. These characteristics are maintained even after sudden shifts in the workload.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {63–70},
numpages = {8},
keywords = {distributed hash table, data warehousing, concept hierarchies},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458514,
author = {Karnstedt, Marcel and Sattler, Kai-Uwe and Ha\ss{}, Michael and Hauswirth, Manfred and Sapkota, Brahmananda and Schmidt, Roman},
title = {Approximating Query Completeness by Predicting the Number of Answers in DHT-Based Web Applications},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458514},
doi = {10.1145/1458502.1458514},
abstract = {Due to the rapid development of theWeb, applications based on the P2P paradigm gain more and more interest. Recently, such systems start to evolve to adopt standard database functionalities in terms of complex query processing support. This goes far beyond simple key lookups, as provided by standard DHT systems, which makes estimating the completeness of query answers a crucial challenge. In this paper, we discuss the semantics of completeness for complex queries in P2P database systems and propose methods based on the notion of routing graphs for estimating the number of expected query answers. Further, we discuss probabilistic guarantees for the estimated values and evaluate the proposed methods through an implemented system.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {71–78},
numpages = {8},
keywords = {query processing, query completeness, probabilistic guarantees, dht},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/3247204,
author = {Polyzotis, Neoklis},
title = {Session Details: Web 2.0 and Social Networks},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247204},
doi = {10.1145/3247204},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
numpages = {1},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458516,
author = {Kinsella, Sheila and Budura, Adriana and Skobeltsyn, Gleb and Michel, Sebastian and Breslin, John G. and Aberer, Karl},
title = {From Web 1.0 to Web 2.0 and Back -: How Did Your Grandma Use to Tag?},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458516},
doi = {10.1145/1458502.1458516},
abstract = {We consider the applicability of terms extracted from anchortext as a source of Web page descriptions in the form of tags. With a relatively simple and easy-to-use method, we show that anchortext significantly overlaps with tags obtained from the popular tagging portal del.icio.us. Considering the size and diversity of the user community potentially involved in social tagging, this observation is rather surprising. Furthermore, we show by an evaluation using human-created relevance assessments the general suitability of the anchortext tag generation in terms of user-perceived precision values. The awareness of this easy-to-obtain source of tags could trigger the rise of new tagging portals pushed by this automatic bootstrapping process or be applied in already existing portals to increase the number of tags per page by merely looking at the anchortext which exists anyway.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {79–86},
numpages = {8},
keywords = {tag prediction, anchortext, social tagging, Web 2.0},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458517,
author = {Abiteboul, Serge and Greenshpan, Ohad and Milo, Tova},
title = {Modeling the Mashup Space},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458517},
doi = {10.1145/1458502.1458517},
abstract = {We introduce a formal model for capturing the notion of mashup in its globality. The basic component in our model is the mashlet. A mashlet may query data sources, import other mashlets, use external Web services, and specify complex interaction patterns between its components. A mashlet state is modeled by a set of relations and its logic specified by datalog-style active rules. We are primarily concerned with changes in a mashlet state relations and rules. The interactions with users and other applications, as well as the consequent effects on the mashlets composition and behavior, are captured by streams of changes. The model facilitates dynamic mashlets composition, interaction and reuse, and captures the fundamental behavioral aspects of mashups.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {87–94},
numpages = {8},
keywords = {information, integration, model, data, mashups, web},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458518,
author = {Biancalana, Claudio and Micarelli, Alessandro and Squarcella, Claudio},
title = {<i>Nereau</i>: A Social Approach to Query Expansion},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458518},
doi = {10.1145/1458502.1458518},
abstract = {Classical query expansion techniques can be roughly divided into two groups: the statistical approach, which consists of the selection of top-ranked terms from relevant sources based on co-occurrence values, and the semantic approach, where candidate terms are chosen based on their meaning. In this paper we present a novel approach, in which the classical cooccurrence matrix is enhanced with metadata retrieved from social bookmarking services in order to overcome its lack of semantic attributes. The implemented system, named Nereau, combines methods from the areas of Information Retrieval and Social Network Analysis: given the original query, our system performs multiple expansions and presents results divided into categories. We use a new approach to web personalization based on user collaboration sharing of information about web documents. Our evaluation results are encouraging and suggest that personalization based on social bookmarking and tagging is a useful addition to web toolset and that the subject is worth further research, in particular with regard to increasing popularity of social and collaborative services in the WWW today.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {95–102},
numpages = {8},
keywords = {personalized web search, keyword co-occurrences, social bookmarking, keyword exraction, query expansion},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458519,
author = {Park, Jaehui and Fukuhara, Tomohiro and Ohmukai, Ikki and Takeda, Hideaki and Lee, Sang-goo},
title = {Web Content Summarization Using Social Bookmarks: A New Approach for Social Summarization},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458519},
doi = {10.1145/1458502.1458519},
abstract = {An increasing number of Web applications are allowing users to play more active roles for enriching the source content. The enriched data can be used for various applications such as text summarization, opinion mining and ontology creation. In this paper, we propose a novel Web content summarization method that creates a text summary by exploiting user feedback (comments and tags) in a social bookmarking service. We had manually analyzed user feedback in several representative social services including del.icio.us, Digg, YouTube, and Amazon.com. We found that (1) user comments in each social service have its own characteristics with respect to summarization, and (2) a tag frequency rank does not necessarily represent its usefulness for summarization. Based on these observations, we conjecture that user feedback in social bookmarking services is more suitable for summarization than other type of social services. We implemented prototype system called SSNote that analyzes tags and user comments in del.icio.us, and extracts summaries. Performance evaluations of the system were conducted by comparing its output summary with manual summaries generated by human evaluators. Experimental results show that our approach highlights the potential benefits of user feedback in social bookmarking services.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {103–110},
numpages = {8},
keywords = {social summarizartion, user feedback, social bookmarking service},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458520,
author = {Fersini, Elisabetta and Messina, Enza and Archetti, Francesco},
title = {Granular Modeling of Web Documents: Impact on Information Retrieval Systems},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458520},
doi = {10.1145/1458502.1458520},
abstract = {One of the most important tasks in Information Retrieval (IR) is related to web page information extraction and processing. It is a common approach to consider a web page as an atomic unit and to model its textual content as a "bag-of-words". However, this kind of representation does not reflect how people perceive a web page. A granular document representation, in terms of semantic objects, can help in identifying semantic areas of a web page and using them for different IR goals. In this paper we use a granular representation to define a new metric for evaluating semantic object importance and to enhance the performance of IR systems. In particular we show that this new metric can be used not only for classification goals, in which instances are assumed as independent and identically distributed, but also to gauge the strength of relationship between hypertextual documents and exploit this information for improving page ranking performance.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {111–118},
numpages = {8},
keywords = {web page ranking, visual layout analysis, document classification, relational granular document modeling},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/3247205,
author = {Chan, Chee-Yong},
title = {Session Details: Ranking and Similarity Search},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3247205},
doi = {10.1145/3247205},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
numpages = {1},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458522,
author = {Shao, Bo and Li, Tao and Ogihara, Mitsunori},
title = {Quantify Music Artist Similarity Based on Style and Mood},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458522},
doi = {10.1145/1458502.1458522},
abstract = {Music artist similarity has been an active research topic in music information retrieval for a long time since it is especially useful for music recommendation and organization. However, it is a difficult problem. The similarity varies significantly due to different artistic aspects considered and most importantly, it is hard to quantify. In this paper, we propose a new framework for quantifying artist similarity. In the framework, we focus on style and mood aspects of artists whose descriptions are extracted from the authoritative information available at the All Music Guide website. We then generate style--mood joint taxonomies using hierarchical co-clustering algorithm, and quantify the semantic similarities between the style/mood terms based on the taxonomy structure and the positions of these terms in the taxonomies. Finally we calculate the artist similarities according to all the style/mood terms used to describe them. Experiments are conducted to show the effectiveness of our framework.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {119–124},
numpages = {6},
keywords = {music artist similarity, similarity quantification, hierarchical co-clustering},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458523,
author = {Giannopoulos, Giorgos and Dalamagas, Theodore and Eirinaki, Magdalini and Sellis, Timos},
title = {Boosting the Ranking Function Learning Process Using Clustering},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458523},
doi = {10.1145/1458502.1458523},
abstract = {As the Web continuously grows, the results returned by search engines are too many to review. Lately, the problem of personalizing the ranked result list based on user feedback has gained a lot of attention. Such approaches usually require a big amount of user feedback on the results, which is used as training data. In this work, we present a method that overcomes this issue by exploiting all search results, both rated and unrated, in order to train a ranking function. Given a small initial set of user feedback for some search results, we first perform clustering on all results returned by the search. Based on the clusters created, we extend the initial set of rated results, including new, unrated results. Then, we use a popular training method (Ranking SVM) to train a ranking function using the expanded set of results. The experiments show that our method approximates sufficiently the results of an "ideal" system where all results of each query should be rated in order to be used as training data, something that is not feasible in a real-world scenario.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {125–132},
numpages = {8},
keywords = {search engine, clickthrough data, training, relevance judgement, clustering, ranking},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458524,
author = {Sun, Yang and Li, Huajing and Councill, Isaac G. and Huang, Jian and Lee, Wang-Chien and Giles, C. Lee},
title = {Personalized Ranking for Digital Libraries Based on Log Analysis},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458524},
doi = {10.1145/1458502.1458524},
abstract = {Given the exponential increase of indexable context on the Web, ranking is an increasingly difficult problem in information retrieval systems. Recent research shows that implicit feedback regarding user preferences can be extracted from web access logs in order to increase ranking performance. We analyze the implicit user feedback from access logs in the CiteSeer academic search engine and show how site structure can better inform the analysis of clickthrough feedback providing accurate personalized ranking services tailored to individual information retrieval systems. Experiment and analysis shows that our proposed method is more accurate on predicting user preferences than any non-personalized ranking methods when user preferences are stable over time. We compare our method with several non-personalized ranking methods including ranking SVMlight as well as several ranking functions specific to the academic document domain. The results show that our ranking algorithm can reach 63.59% accuracy in comparison to 50.02% for ranking SVMlight and below 43% for all other single feature ranking methods. We also show how the derived personalized ranking vectors can be employed for other ranking-related purposes such as recommendation systems.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {133–140},
numpages = {8},
keywords = {web usage mining, personalized ranking},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458525,
author = {Pon, Raymond K. and C\'{a}rdenas, Alfonso F. and Buttler, David J.},
title = {Online Selection of Parameters in the Rocchio Algorithm for Identifying Interesting News Articles},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458525},
doi = {10.1145/1458502.1458525},
abstract = {We show that users have different reading behavior when evaluating the interestingness of articles, calling for different parameter configurations for information retrieval algorithms for different users. Better recommendation results can be made if parameters for common information retrieval algorithms, such as the Rocchio algorithm, are learned dynamically instead of being statically fixed a priori. By dynamically learning good parameter configurations, Rocchio can adapt to differences in user behavior among users. We show that by adaptively learning online the parameters of a simple retrieval algorithm, similar recommendation performance can be achieved as more complex algorithms or algorithms that require extensive fine-tuning. Also we have also shon that online parameter learning can yield 10% better results than best performing filter from the TREC11 adaptive filter task.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {141–148},
numpages = {8},
keywords = {personalization, news filtering, news recommendation},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

@inproceedings{10.1145/1458502.1458526,
author = {Blanco, Lorenzo and Crescenzi, Valter and Merialdo, Paolo and Papotti, Paolo},
title = {Supporting the Automatic Construction of Entity Aware Search Engines},
year = {2008},
isbn = {9781605582603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1458502.1458526},
doi = {10.1145/1458502.1458526},
abstract = {Several web sites deliver a large number of pages, each publishing data about one instance of some real world entity, such as an athlete, a stock quote, a book. Although it is easy for a human reader to recognize these instances, current search engines are unaware of them. Technologies for the Semantic Web aim at achieving this goal; however, so far they have been of little help in this respect, as semantic publishing is very limited.We have developed a method to automatically search on the web for pages that publish data representing an instance of a certain conceptual entity. Our method takes as input a small set of sample pages: it automatically infers a description of the underlying conceptual entity and then searches the web for other pages containing data representing the same entity. We have implemented our method in a system prototype, which has been used to conduct several experiments that have produced interesting results.},
booktitle = {Proceedings of the 10th ACM Workshop on Web Information and Data Management},
pages = {149–156},
numpages = {8},
keywords = {resource discovery, web exploration, entity aware search engines},
location = {Napa Valley, California, USA},
series = {WIDM '08}
}

