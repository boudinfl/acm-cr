@inproceedings{10.1145/502932.502934,
author = {Nie, Zaiqing and Kambhampati, Subbarao and Nambiar, Ullas and Vaddi, Sreelakshmi},
title = {Mining Source Coverage Statistics for Data Integration},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502934},
doi = {10.1145/502932.502934},
abstract = {Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition there are no effective approaches for learning the needed statistics. The key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable. Naive approaches can become infeasible very quickly. In this paper we present a set of connected techniques that estimate the coverage and overlap statistics while keeping the needed statistics tightly under control. Our approach uses a hierarchical classification of the queries, and threshold based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We describe the details of our method, and present preliminary experimental results showing the feasibility of the approach.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502935,
author = {Mobasher, Bamshad and Dai, Honghua and Luo, Tao and Nakagawa, Miki},
title = {Effective Personalization Based on Association Rule Discovery from Web Usage Data},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502935},
doi = {10.1145/502932.502935},
abstract = {To engage visitors to a Web site at a very early stage (i.e., before registration or authentication), personalization tools must rely primarily on clickstream data captured in Web server logs. The lack of explicit user ratings as well as the sparse nature and the large volume of data in such a setting poses serious challenges to standard collaborative filtering techniques in terms of scalability and performance. Web usage mining techniques such as clustering that rely on offline pattern discovery from user transactions can be used to improve the scalability of collaborative filtering, however, this is often at the cost of reduced recommendation accuracy. In this paper we propose effective and scalable techniques for Web personalization based on association rule discovery from usage data. Through detailed experimental evaluation on real usage data, we show that the proposed methodology can achieve better recommendation effectiveness, while maintaining a computational advantage over direct approaches to collaborative filtering such as the k-nearest-neighbor strategy.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {9–15},
numpages = {7},
keywords = {web usage mining, association rules, personalization, collaborative filtering},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502937,
author = {Arcieri, F. and Melideo, G. and Nardelli, E. and Talamo, M.},
title = {Keeping Coherence among Web Sources},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502937},
doi = {10.1145/502932.502937},
abstract = {With the increasing popularity of the Web the problem of maintaining coherence of information contained in a vast collection of semantically related web pages has become a significant challenge. A fundamental issue is that an object exposed on a web page is independently and autonomously managed by the organizational unit responsible for its production and controlling the way it changes, but the same object is also needed (and visible) at web pages managed by different and independent organizational units. The aim is therefore to define a flexible but effective mechanism to automatically keep the representations of the same piece of information aligned in the various web pages.In this paper we discuss a formal model supporting this goal, which derives from practical experiences and from their formalization in the area of supporting coherence maintenance in the underlying legacy databases of cooperating organizations.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {16–23},
numpages = {8},
keywords = {information systems integration, coherence maintenance, data interoperability},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502938,
author = {Chidlovskii, Boris},
title = {Automatic Repairing of Web Wrappers},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502938},
doi = {10.1145/502932.502938},
abstract = {We study the problem of automatic repairing of wrappers for Web information providers. Majority of Web wrappers use "hooks'' or "landmarks'' to find and extract relevant information from Web pages and such wrappers often become inoperable when the page structure is changed. The solution we propose in this paper extends conventional forward wrappers with alternative classifiers built using content features of extracted information and wrappers processing pages backward. We report some preliminary results of the information extraction recovery and wrapper repairing for a set of real Web provider changes.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {24–30},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502939,
author = {Khan, Latifur and Rao, Yan},
title = {A Performance Evaluation of Storing XML Data in Relational Database Management Systems},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502939},
doi = {10.1145/502932.502939},
abstract = {XML is an emerging standard for the representation and exchange of Internet data. Along with document type definition (DTD), XML permits the execution of a collection of queries, using XPath to identify data in XML documents. In this paper we examine how XML data can be stored and queried using a standard relational database management system (RDBMS). For this, we propose a technique for automatic mapping from an XML document to relations within the RDBMS. We demonstrate that our novel approach preserves the nested structure of the XML documents. By hiding database details we devise a seamless, transparent framework for user access to XML data. In order to achieve this, we propose a novel mechanism for translating an XPath query into an SQL statement. Furthermore, we propose efficient techniques for the construction of an XML document on the fly from the result set of the SQL statement. We also present findings in terms of query response time on the comparative performance of different techniques for the construction of an XML document on the fly.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {31–38},
numpages = {8},
keywords = {XML, XPath, DTD, relational DBMS, SQL},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502941,
author = {Berfield, Alan and Simons, Bill and Chrysanthis, Panos K. and Pruhs, Kirk},
title = {Better Client OFF Time Prediction to Improve Performance in Web Information Systems},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502941},
doi = {10.1145/502932.502941},
abstract = {Prefetching is a potential technique for reducing latency in Web information Systems. However, it has been shown that the burstiness of standard prefetching can drastically increase network congestion, and can even increase, rather than decrease, average user perceived latency. Accurate OFF time, the idle periods between user requests, prediction potentially allows the document to be downloaded at an even rate over the OFF time, which can ameliorate the burstiness, and significantly improve both network congestion and average user perceived latency. Yet accurate prediction of such OFF times has been difficult to achieve. This paper examines the use of two machine-learning techniques, namely, neural networks and genetic algorithms, for OFF time prediction. Our performance evaluation results show that these techniques provide better accuracy than those previously reported, with an average increase of twice the correlation. Our results also show that document type is the best predictor of OFF time. Further, our functions can be tailored to favor underpredictions, which would have less negative effects on the overall network than overpredictions.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {39–46},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502942,
author = {Mao, Yun and Chen, Kang and Wang, Dongsheng and Zheng, Weimin},
title = {Cluster-Based Online Monitoring System of Web Traffic},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502942},
doi = {10.1145/502932.502942},
abstract = {Web traffic has been increasing and evolving rapidly in recent years. It is important to measure the volume and characteristic of such dominant traffic to understand large-scale user access pattern and analyze performance of Web applications. Among the common methods of Web measurements, the passive way using packet monitoring is more advantageous since it provides comprehensive information and is transparent to end-users. However, the throughput of current packet monitoring system is limited by the bandwidth of network adapters. Computational capacity and buffer size are also potential performance bottlenecks for monitoring high-speed links. This paper proposes a cluster-based online monitoring system, which generates rich logs by packet sniffing for Web analysis in a cluster environment. Powered by cluster computing technologies, the system achieves high performance and availability. Other techniques, such as online reconstruction in memory and kernel packet filter, are also adopted to improve the system performance. The experimental results indicate that it is feasible to trace in high-speed links with the cluster-based system.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {47–53},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502943,
author = {Cacheda, Fidel and Vi\~{n}a, Angel},
title = {Superimposing Codes Representing Hierarchical Information in Web Directories},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502943},
doi = {10.1145/502932.502943},
abstract = {In this article we describe how superimposed coding can be used to represent hierarchical information, which is especially useful in categorized information retrieval systems (for example, Web directories). Superimposed coding have been widely used in signature files in a rigid manner, but our approach is more flexible and powerful. The categorization is based on a directed acyclic graph and each document is assigned to one or more nodes, using superimposed coding we represent the categorization information of each document in a signature. In this paper we explain the superimposed coding theory and how this coding technique can be applied to more flexible environments. Furthermore, we realize an exhaustive analysis of the important factors that have repercussions on the performance of the system. Finally we expose the conclusions obtained from this article.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {54–60},
numpages = {7},
keywords = {hierarchical information, superimposed codes, indexing techniques, information retrieval in WWW},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502945,
author = {Rapela, Joaquin},
title = {Automatically Combining Ranking Heuristics for HTML Documents},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502945},
doi = {10.1145/502932.502945},
abstract = {Current search engines use several criteria or heuristics to rank HTML documents. HTML ranking heuristics need to be combined into a ranking function that given a text query returns a ranked list of HTML documents. The standard approach is to build a weighted average by manually estimating the importance of every heuristic and assigning a weight proportional to the estimated importance. In the current paper we apply an automatic method for combining HTML ranking heuristics. Using recall/precision evaluations we study the performance of the automatic method and using collections of HTML documents with different characteristics we show that the automatic method finds weights tailored to specific characteristics of each document collection},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {61–67},
numpages = {7},
keywords = {HTML, HTML ranking heuristics, information retrieval, HTML ranking, WWW, automatic combination},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502946,
author = {Su, Hong and Kuno, Harumi and Rundensteiner, Elke A.},
title = {Automating the Transformation of XML Documents},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502946},
doi = {10.1145/502932.502946},
abstract = {The advent of web services that use XML-based message exchanges has spurred many efforts that address issues related to inter-enterprise service electronic commerce interactions. Currently emerging standards and technologies enable enterprises to describe and advertise their own Web Services and to discover and determine how to interact with services fronted by other businesses. However, these technologies do not address the problem of how to reconcile structural differences between similar types of documents supported by different enterprises. Transformations between such documents must thus be created manually on a case-by-case basis. In this paper, we explore the problem of how to automate the transformation of XML E-business documents. We develop an integrated solution that automates as much as possible all steps of the document transformation process. One, we propose a set of schema transformation operations that establish semantic relationships between two XML document schemas. Two, we define a model that allows us to compare the cost of performing these operations. Three, we introduce an algorithm that discovers an efficient sequence of operations for transforming a source document schema into a target document schema based on our cost model. The operation sequence then is used to generate an equivalent XSLT transformation script. Experimental results indicate that our algorithm can satisfactorily discover acceptable transformations.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {68–75},
numpages = {8},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

@inproceedings{10.1145/502932.502947,
author = {Eguchi, Koji and Sugita, Shigeki},
title = {Automatically Editing Book Reviews on the Web},
year = {2001},
isbn = {1581134444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/502932.502947},
doi = {10.1145/502932.502947},
abstract = {A large amount of book information lies scattered on the Web. It is written in non-standard forms, as is other Web information, and this imposes a heavy load on a user browsing search results. We propose an automatic editing method that assists users to retrieve book information, especially book reviews scattered on the Web. Our proposed system retrieves the bibliographic information of a user-specified book using a library catalog database. Using this, it retrieves book reviews on the Web, which are then automatically edited using some heuristic rules for segment extraction, filtering and sorting according to a semantic likelihood of their being book reviews, and are finally presented in tables to the users. We implemented a prototype system and performed a preliminary evaluation of its effectiveness by experiment.},
booktitle = {Proceedings of the 3rd International Workshop on Web Information and Data Management},
pages = {76–81},
numpages = {6},
location = {Atlanta, Georgia, USA},
series = {WIDM '01}
}

