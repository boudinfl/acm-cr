@inproceedings{10.1145/319759.319764,
author = {Barish, Greg and DiPasquo, Dan and Knoblock, Craig A. and Minton, Steven},
title = {An Efficient Plan Execution System for Information Management Agents},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319764},
doi = {10.1145/319759.319764},
abstract = {Recent work on information integration has yielded novel and efficient solutions for gathering data from the World Wide Web. However, there has been little attention given to the problem of providing information management capabilities that closely model how people interact with the web in productive ways - not only collecting information, but monitoring web sites for new or updated data, sending notifications based on the results, building reports, creating local repositories of information, and so on. These needs are unique to the dynamic nature of information in a networked environment. In this paper, we describe Theseus, an efficient plan execution system for information management agents. Through its plan language, Theseus supports a number of capabilities which enable practical information management, including repeated and periodic query execution, conditional plan declarations, query result aggregation, and flexible communication of results. The Theseus executor system focuses on efficiency, with support for data pipelining, and dataflow-based, event driven parallel execution. With Theseus, users can automate the complex but practical ways in which they interact with the web, for both information gathering and management.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {1–5},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319766,
author = {Mattox, David and Seligman, Len and Smith, Ken},
title = {Rapper: A Wrapper Generator with Linguistic Knowledge},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319766},
doi = {10.1145/319759.319766},
abstract = {Database management systems are becoming available for semistructured data, however, these tools cannot be used on many real-world data sources (e.g., most web sites) in their native form. Often, wrappers are needed to extract information and organize it into a graph structure that makes explicit the concepts users want to query and update. This paper presents a new approach to wrapper generation that exploits linguistic knowledge. The approach produces a more fine-grained parse of sources with natural language text than previous efforts. The resulting graph structured databases answer queries that could not be formulated in database produced by prior generated wrappers. In addition, our approach may be more robust in the face of slight variations in word choice and order. We discuss a prototype implementation, lessons learned to date, evaluation issues, and future research directions.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {6–11},
numpages = {6},
keywords = {discovering structure, wrapper generation, web databases, semistructured data, information extraction},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319767,
author = {Ikeji, Augustine Chidi and Fotouhi, Farshad},
title = {An Adaptive Real-Time Web Search Engine},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319767},
doi = {10.1145/319759.319767},
abstract = {The Internet provides a wealth of information scattered all over the world. The fact that the information may be located anywhere makes it both convenient for placing information on the Web and difficult for others to find. Conventional search engines can only locate information that is in their search index and users do not have much choice in limiting or expanding the search parameters. Some web pages like those for news services change frequently and will not work well with index based search engines because the indexed information may become obsolete at any moment.We are proposing an efficient algorithm for finding information on the Web that gives the user greater control over the search path and what to search for. Unlike the conventional techniques, our algorithm does not use an index and works in real time. We save on space, we can search any part of the Internet indicated by the user, and since the search is in real time, the result will be current.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {12–16},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319769,
author = {Hohenstein, Uwe and Ebert, Andreas},
title = {Automatic Migration of Files into Relational Databases},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319769},
doi = {10.1145/319759.319769},
abstract = {In order to provide database-like features for files, particularly for searching in Web data, one solution is to migrate file data into a relational database. Having stored the data, the capabilities of SQL can be used for querying, provided, the data has been given some structure. To this end, an adapter must be implemented that converts data from files into the database. This paper proposes a specification-based automation for this procedure: Given some descriptive specification of file contents, those file adapters are generated. An adequate specification language provides powerful concepts to describe the contents of files. In contrast to similar work, directory structures are taken into account because they often contain useful semantics.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {17–21},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319770,
author = {Bertino, Elisa and Castano, Silvana and Ferrari, Elena and Mesiti, Marco},
title = {Controlled Access and Dissemination of XML Documents},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319770},
doi = {10.1145/319759.319770},
abstract = {XML (eXtensible Markup Language) is becoming the most relevant standardization effort in the area of document representation through markup languages. Through XML, it is possible to define complex documents, containing information at different degrees of sensitivity. Moreover, the processes of document exchange and acquisition, which can be very frequent in Web-based information systems, are simplified and standardized. In this scenario, there is a strong need for policies to control and regulate the access and dissemination of XML documents. In the paper, we discuss main protection requirements posed by XML documents and we present a set of authorization and dissemination policies that enable both a controlled access to XML documents in a given source and the exchange of XML documents across different sources.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {22–27},
numpages = {6},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319773,
author = {Wood, Peter T.},
title = {Optimising Web Queries Using Document Type Definitions},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319773},
doi = {10.1145/319759.319773},
abstract = {A document type definition (DTD) D defines the structure of elements permitted in any web document valid with respect to D. From a given DTD D we show how to derive a number of simple structural constraints which are implied by D. Using a relational abstraction of web databases, we consider a class of conjunctive queries which retrieve elements from web documents stored in a database D. For simplicity, we assume that all documents in D are valid with respect to the same DTDD. The main contribution of the paper is the use of the constraints derived from D to optimise conjunctive queries on D by removing redundant conjuncts. The relational abstraction allows us to show that the constraints derived from a DTD are equivalent to tuple-generating and equality-generating dependencies which hold on D. Having done so, we can use the chase algorithm to show equivalence between a query and its reduced form.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {28–32},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319774,
author = {Ashish, Naveen and Knoblock, Craig A. and Shahabi, Cyrus},
title = {Selectively Materializing Data in Mediators by Analyzing Source Structure, Query Distribution and Maintenance Cost},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319774},
doi = {10.1145/319759.319774},
abstract = {We present an approach to selecting data to materialize in Web based information mediators by analyzing multiple factors. An issue in building Web based information mediators is how to improve the query response time given the high response time for retrieving data from remote Web sources. We had earlier presented a framework for optimizing the performance of information mediators by selectively materializing data. In this paper we describe our approach for automatically selecting the portion of data that must be materialized by analyzing a combination of several factors, namely the distribution of user queries, the structure of sources and the update cost.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {33–37},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319777,
author = {Wei, Zu-Kuan and Oh, Young-Hwan and Lee, Jae-Dong and Kim, Jae-Hong and Park, Dong-Sun and Lee, Young-Geol and Bae, Hae-Young},
title = {Efficient Spatial Data Transmission in Web-Based GIS},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319777},
doi = {10.1145/319759.319777},
abstract = {This paper has proposed a new method of efficient spatial data transmission in the client-side Web-Based GIS (Geographic Information System) which handles large-size spatial geographic information on the Internet. The basic idea is that, firstly, a largesize map is divided into several parts, where each part is called a “Tile”, according to appropriate division granularity. Secondly, when an user requires a certain region in the map at client side, the GIS server only transmits spatial data in the tiles which overlap with the requested region. And the received data are stored in client local machine for reuse. Here a method of tile-division and tile-query processing has been provided. Comparing with traditional client-side Web-Based GIS, the performance improvement is achieved by the usage of the proposed method.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {38–42},
numpages = {5},
keywords = {Web-based GIS, data transmission, spatial DB},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319781,
author = {Garofalakis, Minos N. and Rastogi, Rajeev and Seshadri, S. and Shim, Kyuseok},
title = {Data Mining and the Web: Past, Present and Future},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319781},
doi = {10.1145/319759.319781},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {43–47},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319782,
author = {Yi, Jeonghee and Sundaresan, Neel},
title = {Mining the Web for Acronyms Using the Duality of Patterns and Relations},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319782},
doi = {10.1145/319759.319782},
abstract = {The Web is a rich source of information, but this information is scattered and hidden in the diversity of web pages. Search engines are windows to the web. However, the current search engines, designed to identify pages with specified phrases have very limited power. For example, they cannot search for phrases related in a particular way (e.g. books and their authors).In this paper we present a solution for identifying a set of inter-related information on the web using the duality concept. Duality problems arise when one tries to identify a pair of inter-related phrases such as (book, author), (name, email) or (acronym, expansion) relations. We propose a solution to this problem that iteratively refines mutually dependent approximations to their identifications. Specifically, we iteratively refine i) pairs of phrases related in a specific way, and ii) the patterns of their occurrences in web pages, i.e. the ways in which the related phrases are marked in the pages. We cast light on the general solution of the duality problems in the web by concentrating on one paradigmatic duality problem i.e. identifying (acronym, expansion) pairs in terms of the patterns of their occurrences in the web pages. The solution to this problem involves two mutually dependent duality problems of 1) the duality between the related pairs and their patterns, and 2) the duality between the related pairs and the acronym formulation rules.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {48–52},
numpages = {5},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319785,
author = {Faisal, Adil and Shahabi, Cyrus and McLaughlin, Margaret and Betz, Frederick},
title = {INsite: Introduction to a Generic Paradigm for Interpreting User-Web Space Interaction},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319785},
doi = {10.1145/319759.319785},
abstract = {INsite is a heuristic-based implementation to provide consistent tracking, analysis and visualization of users' interactions with a generic web site. Our research has immediate applicability in such disparate fields as Business, E-commerce, Distance Education, Entertainment and Management for capturing individual and collective profiles of customers, learners and employees. INsite can identify trends and changes in user(s) behavior (interests) by monitoring their online interactions. It has a three-tier architecture for tracking, analysis and visualization. First, a remote agent transparently tracks user-navigation-paths within a site. Second, a unique Connectivity Matrix (CM) Model (a set of Connectivity Matrices) represents each path (and cluster of paths). Third, the user-web site interaction, thus translated to a finite number of CM-Models, is readily visualized by graphically representing the member matrices of the models. Each member matrix of a representative CM-Model captures a single navigational attribute. Our dimensionally static approach to path and cluster representation by the Connectivity Matrices can reduce the complexity of analysis by several orders. Consequently, we employ a new paradigm for dynamic clustering that leverages on the unique CM-Model of representation.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {53–58},
numpages = {6},
keywords = {Web space, visualization, Web navigation analysis, user profile, Web mining, traffic analysis, WWW},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319789,
author = {Lee, Chung-Hong and Yang, Hsin-Chang},
title = {A Web Text Mining Approach Based on Self-Organizing Map},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319789},
doi = {10.1145/319759.319789},
abstract = {Web text mining is a new issue in the knowledge discovery research field. It is aimed to help people discover knowledge from large quantities of semi-structured or unstructured text in the web. Several approaches, including some pure and hybrid information retrieval (IR) methods, have been proposed to tackle such an issue. Among these approaches, combining the Self-Organizing Map (SOM) method with the principles of the vectorspace model, appears to be a promising alternative for the traditional purely IR-based methods in this problem domain. In this paper, a novel SOM-based method using a Chinese corpus for web text mining is presented. The SOM is used to generate two maps, namely the word cluster map and the document cluster map, which reveal the relationships among words and documents respectively. The search process incorporates these two maps and effectively finds the relevant documents according to the keywords specified in the query. The conceptually associated web documents are found not only by the specific keywords but the relevant words found by the word cluster map.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {59–62},
numpages = {4},
keywords = {document clustering, text data mining, self-organizing map},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

@inproceedings{10.1145/319759.319792,
author = {Joshi, Karuna P. and Joshi, Anupam and Yesha, Yelena and Krishnapuram, Raghu},
title = {Warehousing and Mining Web Logs},
year = {1999},
isbn = {1581132212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/319759.319792},
doi = {10.1145/319759.319792},
abstract = {Analyzing Web Logs for usage and access trends can not only provide important information to web site developers and administrators, but also help in creating adaptive web sites. While there are many existing tools that generate fixed reports from web logs, they typically do not allow ad-hoc analysis queries. Moreover, such tools cannot discover hidden patterns of access embedded in the access logs. We describe a relational OLAP (ROLAP) approach for creating a web-log warehouse. This is populated both from web logs, as well as the results of mining web logs. We also present a web based ad-hoc tool for analytic queries on the warehouse. We discuss the design criteria that influenced our choice of dimensions, facts and data granularity, and present the results from analyzing and mining the logs.},
booktitle = {Proceedings of the 2nd International Workshop on Web Information and Data Management},
pages = {63–68},
numpages = {6},
keywords = {ad hoc analysis, user interface, Web logs, Web mining, clustering},
location = {Kansas City, Missouri, USA},
series = {WIDM '99}
}

