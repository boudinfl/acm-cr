@inproceedings{10.1145/3250818,
author = {Marx, Maarten},
title = {Session Details: XML and Semi-Structured Data},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250818},
doi = {10.1145/3250818},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316904,
author = {Coelho, Jorge and Florido, Mario},
title = {XCentric: Logic Programming for XML Processing},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316904},
doi = {10.1145/1316902.1316904},
abstract = {Here we present the logic-programming language XCentric, discuss design issues, and show its adequacy for XML processing. Distinctive features of XCentric are a powerful unification algorithm for terms with functors of arbitrary arity (which correspond closely to XML documents) and a rich type language that uses operators such as repetition (*), alternation, etc, as types allowing a compact representation of terms with functors with an arbitrary number of arguments (closely related to standard type languages for XML). This new form of unification together with an appropriate use of types yields a substantial degree of flexibility in programming.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
keywords = {XML processing, logic programming},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316905,
author = {Ronen, Royi and Shmueli, Oded},
title = {Evaluation of Datalog Extended with an XPath Predicate},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316905},
doi = {10.1145/1316902.1316905},
abstract = {XPathL is a logical language for processing XML and relational data. The language is essentially Datalog augmented with a new type of predicate, XPath Expression Predicate, which is dedicated to XPath processing. It may be used as an intermediate target language for higher level constructs within a programming language, or embedded directly within a programming language.Two approaches for processing XPathL queries are presented. The Static approach uses an XPath processor, prior to execution, without considering data binding. The On-Demand approach uses the XPath processor based on known bindings to variables at run-time.We constructed a prototype for experimenting with these approaches. The prototype provides a platform for leveraging the wealth of knowledge in Datalogprocessing for the purpose of integrative querying of both XML and relational data. The prototype also provides a platform to investigate the interaction between processors for arbitrary file-type-specific predicates within one query (in this paper the interaction between XML and relational file types is investigated). In that, XPathL takes a loosely-coupled approach towards data access, which fits well with web data.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {9–16},
numpages = {8},
keywords = {datalog, XML and relational processing, XML},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316906,
author = {Vinson, Alexander R. and Heuser, Carlos A. and da Silva, Altigran S. and de Moura, Edleno S.},
title = {An Approach to XML Path Matching},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316906},
doi = {10.1145/1316902.1316906},
abstract = {In applications that accomplish XML data integration and XML instance querying, the problem of XML path matching plays a central role. This paper presents an approach for matching XML paths that consists of (1) PathSim, a similarity function specifically designed for matching XML paths and (2) a set of pre-processing functions to be applied to XML paths that are to be compared by a similarity function. The reported experiments demonstrate that PathSim achieves matches of higher quality than a similarity function for XML paths found in literature. The experiments further show that matches of higher quality are achieved when the proposed pre-processing functions are employed.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {17–24},
numpages = {8},
keywords = {data integration, approximate matching, xpath},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316907,
author = {Mesquita, Filipe and Barbosa, Denilson and Cortez, Eli and da Silva, Altigran S.},
title = {FleDEx: Flexible Data Exchange},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316907},
doi = {10.1145/1316902.1316907},
abstract = {We propose a lightweight framework for data exchange that is suitable for non-expert and casual users sharing data on the Web or through peer-to-peer systems. Unlike previous work, we consider a simplistic data model and schema formalism that are suitable for describing typical online data, and propose algorithms for mapping such schemas as well as for translating the corresponding instances. Our solution requires minimal overhead and setup costs compared to existing data exchange systems, making it very attractive in the Web data exchange setting. We report experimental results indicating that our method works well with real Web data from various domains.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {25–32},
numpages = {8},
keywords = {data exchange, XML, web data management},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/3250819,
author = {Polyzotis, Neoklis},
title = {Session Details: P2P and System Design Issues},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250819},
doi = {10.1145/3250819},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316909,
author = {Kurasawa, Hisashi and Wakaki, Hiromi and Takasu, Atsuhiro and Adachi, Jun},
title = {Data Allocation Scheme Based on Term Weight for P2P Information Retrieval},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316909},
doi = {10.1145/1316902.1316909},
abstract = {Many Peer-to-Peer information retrieval systems that use a global index have already been proposed that can retrieve documents relevant to a query. Since documents are allocated to peers regardless of the query, the system needs to connect many peers to gather the relevant documents. We propose a new data allocation scheme for P2P information retrieval that we call Concordia. Concordia uses a node to allocate a document based on the weight of each term in the document to efficiently assemble all the documents relevant to a query from the P2P Network. Moreover, the node encodes the binary data of a document with an erasure code, and Concordia produces an efficient redundancy for counteracting node failures.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {33–40},
numpages = {8},
keywords = {data allocating, distributed IR, peer-to-peer information systems},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316910,
author = {Abiteboul, Serge and Marinoiu, Bogdan},
title = {Distributed Monitoring of Peer to Peer Systems},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316910},
doi = {10.1145/1316902.1316910},
abstract = {In this paper, we are concerned with the distributed monitoring of P2P systems. We introduce the P2P Monitor system and a new declarative language, namely P2PML, for specifying monitoring tasks. A P2PML subscription is compiled into a distributed algebraic plan which is described using algebra over XML streams. The operators of this algebra are first alerters in charge of detecting specific events and acting as stream sources. Other operators process the streams or publish them.We introduce a filter for streams of XML documents that scales by processing first simple conditions and then, if still needed, evaluating complex queries. We also show how particular tasks can be supported by identifying subtasks that are already provided by existing streams.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {41–48},
numpages = {8},
keywords = {peer to peer systems, stream processing, databases, web services, active documents, distributed data management},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316911,
author = {Gounaris, Anastasios and Yfoulis, Christos and Sakellariou, Rizos and Dikaiakos, Marios D.},
title = {Self-Optimizing Block Transfer in Web Service Grids},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316911},
doi = {10.1145/1316902.1316911},
abstract = {Nowadays, Web Services (WSs) play an increasingly important role in Web data management solutions, since they offer a practical solution for accessing and manipulating data sources spanning administrative domains. Nevertheless, they are notoriously slow and transferring large data volumes across WSs becomes the main bottleneck in such WS-based applications. This paper deals with the problem of minimizing at runtime, in a self-managing way, the datatransfer cost of a WS encapsulating a data source. To reducethe transfer cost, the data volume is typically divided intoblocks. In this case, response time exhibits a quadratic-like, non-linear behavior with regards to the block size; as such, minimizing the transfer cost entails finding the optimum block size. This situation is encountered in several systems, such as WS Management Systems (WSMSs) for DBMS-like data management over wide area service-based networks, and WSs for accessing and integrating traditional DBMSs. The main challenges in this problem include (i) the unavailability of an analytical model; (ii) the presence of noise, which incurs local minima; (iii) the volatility of the environment, which results into a moving optimum operating point; and (iv) the requirements for fast convergence to the optimal size of the request from the side of the client rather than of the server, and for low overshooting. This paper presents two novel solutions for detecting the optimum block size during data transmission, thus yielding lower response times. The solutions are inspired by the broader areas of runtime optimization and switching extremum control. They incorporate heuristics to avoid local optimal points, and address all the afore-mentioned challenges. The effectiveness andeffciency of the solutions is verified through empirical evaluation in real cases.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {49–56},
numpages = {8},
keywords = {autonomic computing, web service performance, service-oriented data management, self-optimization, data grids},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316912,
author = {Marin, Mauricio and Gomez, Carlos},
title = {Load Balancing Distributed Inverted Files},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316912},
doi = {10.1145/1316902.1316912},
abstract = {This paper present a comparison of scheduling algorithms applied to the context of load balancing the query traffic on distributed inverted files. We implemented a number of algorithms taken from the literature. We propose a novel method to formulate the cost of query processing so that these algorithms can be used to schedule queries onto processors. We avoid measuring load balance at the search engine side because this can lead to imprecise evaluation. Our method is based on the simulation of a bulk-synchronous parallel computer at the broker machine side. This simulation determines an optimal way of processing the queries and provides a stable baseline upon which both the broker and search engine can tune their operation in accordance with the observed query traffic. We conclude that the simplest load balancing heuristics are good enough to achieve efficient performance. Our method can be used in practice by broker machines to schedule queries efficiently onto the cluster processors of search engines.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {57–64},
numpages = {8},
keywords = {parallel and distributed computing, inverted files},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/3250820,
author = {Lee, Dongwon},
title = {Session Details: Personalization},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250820},
doi = {10.1145/3250820},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316914,
author = {Lee, Jongwuk and You, Gae-won and Sohn, IkChan and Hwang, Seung-won and Ko, Kwangil and Lee, Zino},
title = {Supporting Personalized Top-k Skyline Queries Using Partial Compressed Skycube},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316914},
doi = {10.1145/1316902.1316914},
abstract = {As near-infinite amount of data are becoming accessible on the Web, it is getting more and more important to support intelligent query mechanisms, to help each user to identify the ideal results of manageable size. As such mechanism, skyline queries have gained a lot of attention lately for its intuitive query formulation. This intuitiveness, however, has a side-effect of generating too many results, especially for high-dimensional data, to satisfy a wide range of user's needs. Our goal is to support personalized skyline queries as identifying "truly interesting" objects based on user-specific preference and retrieval size k. While this problem has been studied previously, the proposed solution identifies top-k results by navigating a "skycube", which incurs exponential storage overhead to data dimensionality and excessive one-time computational overhead for skycube construction. In contrast, we develop novel techniques to significantly reduce both storage and computation overhead. Our extensive evaluation results validate this framework on both real-life and synthetic data.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {65–72},
numpages = {8},
keywords = {personalization, skyline queries, ranking},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316915,
author = {Nakamura, Satoshi and Yamamoto, Takehiro and Tanaka, Katsumi},
title = {Toward Editable Web Browser: Edit-and-Propagate Operation for Web Browsing},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316915},
doi = {10.1145/1316902.1316915},
abstract = {This paper proposes a novel technique of browsing Web pages called "Edit-and-Propagate" operation based browsing, where edit operation is regarded as the third operation for interacting with the WWW after conventional clicking and scrolling towards realizing the Editable Web Browser. Our method enables users to delete/emphasize any portion of a browsed Web page at any time and modifies the page by propagating the edit operation. For example, the user can easily delete almost any uninteresting portion of a Web page merely by deleting an example of an unwanted portion. While browsing a Web search result page, the user can rerank search results by deleting an unwanted term or by emphasizing an important term. In this paper, we describe the concept of "Edit-and-Propagate" based browsing, and the implementation of our prototypes. Then we describe the results of our evaluation, which demonstrate the usefulness of our system.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {73–80},
numpages = {8},
keywords = {editable web browser, web, user interface, reranking, filtering},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316916,
author = {Dalamagas, Theodore and Bouros, Panagiotis and Galanis, Theodore and Eirinaki, Magdalini and Sellis, Timos},
title = {Mining User Navigation Patterns for Personalizing Topic Directories},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316916},
doi = {10.1145/1316902.1316916},
abstract = {Topic directories are popular means of organizing information resources in the web. In this work, we introduce a methodology for personalizing topic directories. The key feature of our methodology is that the personalization is based on the mining of navigation patterns extracted from previous user visits. These patterns, expressed in the form of visited categories and retrieved resources, represent the navigation behaviour and interests of different users or user groups. Our work provides a set of mining tasks for user navigation patterns and a set of personalization tasks that customize the organization of the topic directory according to these patterns for certain user groups.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {81–88},
numpages = {8},
keywords = {personalization, topic directories, navigation patterns, sequential patterns},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316917,
author = {Ban, Zhijie and Gu, Zhimin and Jin, Yu},
title = {An Online PPM Prediction Model for Web Prefetching},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316917},
doi = {10.1145/1316902.1316917},
abstract = {Web prefetching is a primary means to reduce user access latency. An important amount of work can be found by the use of PPM (Prediction by Partial Match) for modeling and predicting user request patterns in the open literature. However, in general, existing PPM models are constructed off-line. It is highly desirable to perform the online update of the PPM model incrementally because user request patterns may change over time. We present an online PPM model to capture the changing patterns and fit the memory. This model is implemented based on a noncompact suffix tree. Our model only keeps the most recent W requests using a sliding window. To further improve the prefetching performance, we make use of maximum entropy principle to model for the outgoing probability distributions of nodes. Our prediction model combines entropy, prediction accuracy rate and the longest match rule. A performance evaluation is presented using real web logs. Trace-driven simulation results show our PPM prediction model can provide significant improvements over previously proposed models.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {89–96},
numpages = {8},
keywords = {web prefetching, PPM, noncompact suffix tree, entropy},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/3250821,
author = {Eirinaki, Magdalini},
title = {Session Details: Mining Knowledge from Web Data},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250821},
doi = {10.1145/3250821},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316919,
author = {Schuth, Anne and Marx, Maarten and de Rijke, Maarten},
title = {Extracting the Discussion Structure in Comments on News-Articles},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316919},
doi = {10.1145/1316902.1316919},
abstract = {Several on-line daily newspapers offer readers the opportunity to directly comment on articles. In the Netherlands this feature is used quite often and the quality (grammatically and content-wise) is surprisingly high. We develop techniques to collect, store, enrichand analyze these comments. After giving a high-level overview of the Dutch 'commentosphere' we zoom in on extracting the discussion structure found in flat comment threads; people not only comment on the news article, they also heavily comment on other comments, resembling discussion fora. We show how techniques from information retrieval, natural language processing and machine learning can be used to extract the 'reacts-on' relation between comments with high precision and recall.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {97–104},
numpages = {8},
keywords = {web mining, web data extraction},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316920,
author = {Gibson, John and Wellner, Ben and Lubar, Susan},
title = {Adaptive Web-Page Content Identification},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316920},
doi = {10.1145/1316902.1316920},
abstract = {Identifying which parts of a Web-page contain target content (e.g., the portion of an online news page that contains the actual article) is a significant problem that must be addressed for many Web-based applications. Most approaches to this problem involve crafting hand-tailored rules or scripts to extract the content, customized separately for particular Web sites. Besides requiring considerable time and effort to implement, hand-built extraction routines are brittle: they fail to properly extract content in some cases and break when the structure of a site's Web-pages changes. In this work we treat the problem of identifying content as a sequence labeling problem, a common problem structure in machine learning and natural language processing. Using a Conditional Random Field sequence labeling model, we correctly identify the content portion of web-pages anywhere from 80-97% of the time depending on experimental factors such as ensuring the absence of duplicate documents and application of the model against unseen sources.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {105–112},
numpages = {8},
keywords = {sequence labeling, conditional random fields, maximum entropy markov models, content identification},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316921,
author = {Horie, Ikumi and Yamaguchi, Kazunori and Kashiwabara, Kenji},
title = {Pattern Detection from Web Using AFA Set Theory},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316921},
doi = {10.1145/1316902.1316921},
abstract = {Recurring patterns of the same link structure can often be observed on Web sites. Patterns are important for site administrators in revising Web sites but are difficult to find empirically. We propose a method for detecting such patterns.The first step in the method is viewing a Web site as a directed graph, and identifying pages that have the same substructure by the Anti-Foundation Axiom (AFA). The AFA is a non-standard set theory that allows for a circular structure. The pages identified by AFA are divided into connected components. Then, meaningful sets of pages are selected as patterns by using the Galois lattice of the binary relation between the pages and the connected components.We apply our method to three actual Web sites and succeed in detecting patterns within the target sites.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {113–120},
numpages = {8},
keywords = {web graph, non-well-founded set theory, AFA, common substructure, link analysis},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316922,
author = {Elmacioglu, Ergin and Kan, Min-Yen and Lee, Dongwon and Zhang, Yi},
title = {Web Based Linkage},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316922},
doi = {10.1145/1316902.1316922},
abstract = {When a variety of names are used for the same real-world entity, the problem of detecting all such variants has been known as the (record) linkage or entity resolution problem. In this paper, toward this problem, we propose a novel approach that uses the Web as the collective knowledge source in addition to contents of entities. Our hypothesis is that if an entity e1 is a duplicate of another entity e2, and if e1 frequently appears together with information I on the Web, then e2 may appear frequently with I on the Web. By using search engines, we analyze the frequency, URLs, or contents of the returned web pages to capture the information I of an entity. Extensive experiments verify that our hypothesis holds in many real settings, and the idea of using the Web as the additional source for the linkage problem is promising. Our proposal shows 51% (on average) and 193% (at best) improvement in precision/recall compared to a baseline approach.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {121–128},
numpages = {8},
keywords = {record linkage, entity resolution},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/3250822,
author = {Fundulaki, Irini},
title = {Session Details: Web Metadata and Search},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3250822},
doi = {10.1145/3250822},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316924,
author = {Nunes, S\'{e}rgio and Ribeiro, Cristina and David, Gabriel},
title = {Using Neighbors to Date Web Documents},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316924},
doi = {10.1145/1316902.1316924},
abstract = {Time has been successfully used as a feature in web information retrieval tasks. In this context, estimating a document's inception date or last update date is a necessary task. Classic approaches have used HTTP header fields to estimate a document's last update time. The main problem with this approach is that it is applicable to a small part of web documents. In this work, we evaluate an alternative strategy based on a document's neighborhood. Using a random sample containing 10,000 URLs from the Yahoo! Directory, we study each document's links and media assets to determine its age. If we only consider isolated documents, we are able to date 52% of them. Including the document's neighborhood, we are able to estimate the date of more than 86% of the same sample. Also, we find that estimates differ significantly according to the type of neighbors used. The most reliable estimates are based on the document's media assets, while the worst estimates are based on incoming links. These results are experimentally evaluated with a real world application using different datasets.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {129–136},
numpages = {8},
keywords = {link analysis, web information retrieval, web dynamics},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316925,
author = {Jatowt, Adam and Kawai, Yukiko and Tanaka, Katsumi},
title = {Detecting Age of Page Content},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316925},
doi = {10.1145/1316902.1316925},
abstract = {Web pages often contain objects created at different times. The information about the age of such objects may provide useful context for understanding page content and may serve many potential uses. In this paper, we describe a novel concept for detecting approximate creation dates of content elements in Web pages. Our approach is based on dynamically reconstructing page histories using data extracted from external sources - Web archives and efficiently searching inside them to detect insertion dates of content elements. We discuss various issues involving the proposed approach and demonstrate the example of an application that enhances browsing the Web by inserting annotations with temporal metadata into page content on user request.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {137–144},
numpages = {8},
keywords = {age detection, web archive, metadata, document annotation},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316926,
author = {Hu, Meiqun and Lim, Ee-Peng and Sun, Aixin and Lauw, Hady Wirawan and Vuong, Ba-Quy},
title = {On Improving Wikipedia Search Using Article Quality},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316926},
doi = {10.1145/1316902.1316926},
abstract = {Wikipedia is presently the largest free-and-open online encyclopedia collaboratively edited and maintained by volunteers. While Wikipedia offers full-text search to its users, the accuracy of its relevance-based search can be compromised by poor quality articles edited by non-experts and inexperienced contributors. In this paper, we propose a framework that re-ranks Wikipedia search results considering article quality. We develop two quality measurement models, namely Basic and P<scp>eer</scp>R<scp>eview</scp>, to derive article quality based on co-authoring data gathered from articles' edit history. Compared withWikipedia's full-text search engine, Google and Wikiseek, our experimental results showed that (i) quality-only ranking produced by P<scp>eer</scp>R<scp>eview</scp> gives comparable performance to that of Wikipedia and Wikiseek; (ii) P<scp>eer</scp>R<scp>eview</scp> combined with relevance ranking outperforms Wikipedia's full-text search significantly, delivering search accuracy comparable to Google.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {145–152},
numpages = {8},
keywords = {collaborative authoring, quality-aware search, wikipedia},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

@inproceedings{10.1145/1316902.1316927,
author = {Lages, Alexandre G. and Pirmez, Luci and Pires, Paulo F. and Delicato, Fl\'{a}via C.},
title = {Satya: A Reputation-Based Approach for Service Discovery and Selection in Service Oriented Architectures},
year = {2007},
isbn = {9781595938299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1316902.1316927},
doi = {10.1145/1316902.1316927},
abstract = {We present SATYA, a system that computes a reputation value for Web service providers in order to enhance the service discovery and selection process increasing reliability in SOA transactions. In this work, objective values of service evaluations supplied by monitoring entities are used along with subjective evaluations supplied by service consumers. The objective and subjective values are compared in order to: (i) validate subjective evaluations; (ii) minimize the degree of subjectivity of computed reputation values; and (iii) discover consumers' preferences in terms of QoS metrics. By assigning Web services a trustable reputation value, SATYA enhances the service descriptions provided by registries with additional information to be used during the service discovery phase.},
booktitle = {Proceedings of the 9th Annual ACM International Workshop on Web Information and Data Management},
pages = {153–160},
numpages = {8},
keywords = {web service discovery, web services, quality of service},
location = {Lisbon, Portugal},
series = {WIDM '07}
}

