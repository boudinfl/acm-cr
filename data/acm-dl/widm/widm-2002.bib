@inproceedings{10.1145/584931.584934,
author = {Kane, Bintou and Su, Hong and Rundensteiner, Elke A.},
title = {Consistently Updating XML Documents Using Incremental Constraint Check Queries},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584934},
doi = {10.1145/584931.584934},
abstract = {When updating a valid XML document, an efficient yet light-weight mechanism is needed to determine if the up-date would invalidate the document. Towards this goal, we developed a framework called SAXE, we first analyzed the constraints expressed in XML schema specifications and establish constraint rules that must be observed for an XML document to conform to a given XML Schema. We then classify the rules as relevant for a given update case, that is, we show the minimal set of rules that must be checked to guarantee the safety for each given update primitive. Next, we illustrate that this set of incremental constraint checks can be specified using generic XQuery expressions composed of three components. These components are (1) XML schema meta-queries to retrieve any constraint knowledge potentially relevant to the given update, (2) retrieval of specific characteristics from the to-be-modified XML, and (3) lastly an analysis of information collected about the XML schema and the affected XML document to determine validity of the update. As proof of concept, we have established a library of these generic XQuery constraint checks for the type-related constraints. The key idea of SAXE is to rewrite each XQuery update into a safe XML Query by extending it with appropriate constraint check subqueries. This enhanced XML This enhanced XML update query can then safely be executed using any existing XQuery engine that supports updates - thus turning any update engine automatically into an incremental constraint-check engine. In order to verify the feasibility of our approach, we have implemented a prototype system SAXE that generates safe XQuery updates. Our experimental evaluation assesses the overhead of rewriting, as well as the relative performance of our loosely-coupled incremental constraint check approach against the more traditional first-change-document and then revalidate-it approach.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
keywords = {XML schema, XML update, XQuery},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584935,
author = {Manghi, Paolo and Simeoni, Fabio and Lievens, David and Connor, Richard},
title = {Hybrid Applications over XML: Integrating the Procedural and Declarative Approaches},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584935},
doi = {10.1145/584931.584935},
abstract = {We discuss the design of a quasi-statically typed language for XML in which data may be associated with different structures and different algebras in different scopes, whilst preserving identity. In declarative scopes, data are trees and may be queried with the full flexibility associated with XML query algebras. In procedural scopes, data have more conventional structures, such as records and sets, and can be manipulated with the constructs normally found in mainstream languages.For its original form of structural polymorphism, the language offers integrated support for the development of hybrid applications over XML, where data change form to reflect programming expectations and enable their enforcement.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {9–14},
numpages = {6},
keywords = {advanced web applications, XML programming, web data integration, XML querying, XML high-level bindings},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584936,
author = {Zhang, Xin and Pielech, Bradford and Rundesnteiner, Elke A.},
title = {Honey, I Shrunk the XQuery! An XML Algebra Optimization Approach},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584936},
doi = {10.1145/584931.584936},
abstract = {A lot of work is being done in the database community on mapping of XML data into and out of relational database systems, specifically, the query processing over such data using XQuery. We discuss our solution, the XML Algebra Tree (XAT), as part of our larger XML management system called Rainbow.Rainbow uses XQuery to describe the loading and extracting of XML data into relational systems and also for the execution of queries against pre-defined XML views of that stored data. The XML algebra tree of the query against those views is merged with the queries that define the views to form a larger tree. Because the XML formatting operators are interleaved with the computation operators, this XAT must then be optimized before being translated into one or more SQL statements that can be executed on the database. SQL translation is composed of computation pushdown and SQL generation.The computation pushdown splits the tree into the XML-specific and SQL-doable operators, which is then going to be converted into SQL statements. However, the XAT after computation pushdown may contain unreferenced columns or unused operators. Leaving these operators in the tree will create unnecessarily large SQL statements and will slow down the overall execution.Our main contributions to XML query processing, outlined in this paper, are threefold. One, we describe an algebra based on XATs for modeling XQuery expressions. Two, we propose rewriting rules to optimize XQueries by XAT operator cancel out. Lastly, we show a cutting algorithm to remove unreferenced columns and operators from the trees. We have fully implemented the techniques discussed in this paper in the Rainbow system. A preliminary experimental study compares the performance of execution before and after operator cancel out and cutting.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {15–22},
numpages = {8},
keywords = {relational, optimization, XQuery, XML, operator, algebra},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584937,
author = {Amer-Yahia, Sihem and Srivastava, Divesh},
title = {A Mapping Schema and Interface for XML Stores},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584937},
doi = {10.1145/584931.584937},
abstract = {Most XML storage efforts have focused on mapping documents to relational databases. Mapping choices range from storing documents verbatim to shredding documents into relations in various ways. These choices are usually hard-coded into each storage system which makes sharing loading and querying utilities and exchanging information between different XML storage systems hard. To address these issues, we designed MXM and IMXM, a mapping schema and an interface API to define and query XML-to-relational mappings.A mapping is expressed as an instance of MXM. MXM is declarative, concise and captures most existing XML-to-relational mappings. Mappings can be expressed for documents for which no schema information is provided or documents that conform to either a DTD or an XML Schema. IMXM is an interface that allows querying of information contained in a MXM mapping. IMXM is designed as a library of functions which makes it easy to use inside any utility or application that needs to gain access to the XML-to-relational mapping. MXM is extensible and can incorporate new XML-to-relational mappings. We implemented a prototype to define mappings as instances of MXM and generate a repository of meta information on the XML and the relational data and the mapping choices. We implemented IMXM on top of this repository and used it for generating a relational schema and loading XML documents.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {23–30},
numpages = {8},
keywords = {XML storage/loading/publishing, XML-to-relational mapping},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584939,
author = {Oates, Tim and Bhat, Vinay and Shanbhag, Vishal},
title = {Using Latent Semantic Analysis to Find Different Names for the Same Entity in Free Text},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584939},
doi = {10.1145/584931.584939},
abstract = {A common problem faced when gathering information from the web is the use of different names to refer to the same entity. For example, the city in India referred to as Bombay in some documents may be referred to as Mumbai in others because its name officially changed from the former to the latter in 1995. Multiplicity of names can cause relevant documents to be missed by search engines. Our goal is to develop an automated system that discovers additional names for an entity given just one of its names. Latent semantic analysis (LSA) is generally thought to be well-suited for this task (Berry &amp; Fierro 1996). We demonstrate empirically that under a broad range of circumstances LSA performs poorly, and describe a two-stage algorithm based on LSA that performs significantly better.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {31–35},
numpages = {5},
keywords = {latent semantic analysis, alias discovery, free text},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584940,
author = {Yang, Yingchen and Luk, Wo-Shun},
title = {A Framework for Web Table Mining},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584940},
doi = {10.1145/584931.584940},
abstract = {Web table mining is about information extraction from tables published inside web pages as HTML texts. Most previous work on this subject makes use of the tags to discover components of the table. Our work treats web as a distinct publication media, in two ways. We argue that new types of table format have been developed specially for the web. We also argue that the visual cues embedded within the HTML text, are utilized by the authors to direct the viewer on how to read the contents contained a web table properly. We develop a framework for comprehensively analyzing the structural aspects of a web table, within which rules are devised to process and extract attribute-value pairs from the table. This approach to web table mining is validated by good experimental results.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {36–42},
numpages = {7},
keywords = {web pages, table mining, information extraction, data extraction},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584941,
author = {Imafuji, Noriko and Kitsuregawa, Masaru},
title = {Effects of Maximum Flow Algorithm on Identifying Web Community},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584941},
doi = {10.1145/584931.584941},
abstract = {In this paper, we describe the effects of using maximum flow algorithm on extracting web community from the web. A web community is a set of web pages having a common topic. Since the web can be recognized as a graph that consists of nodes and edges that represent web pages and hyperlinks respectively, so far various graph theoretical approaches have been proposed to extract web communities from the web graph. The method of finding a web community using maximum flow algorithm was proposed by NEC Research Institute in Princeton two years ago. However the properties of web communities derived by this method have been seldom known. To examine the effects of this method, we selected 30 topics randomly and experimented using Japanese web archives crawled in 2000. Through these experiments, it became clear that the method has both advantages and disadvantages. We will describe some strategies to use this method effectively. Moreover, by using same topics, we examined another method that is based on complete bipartite graphs. We compared the web communities obtained by those methods and analyzed those characteristics.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {43–48},
numpages = {6},
keywords = {maximum-flow algorithm, web graph, web community},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584942,
author = {Wang, Jidong and Chen, Zheng and Tao, Li and Ma, Wei-Ying and Wenyin, Liu},
title = {Ranking User's Relevance to a Topic through Link Analysis on Web Logs},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584942},
doi = {10.1145/584931.584942},
abstract = {Computing the web-user's relevance to a give topic is an important task for any personalization service on the Web. Since the interest and preference of a web-user are revealed in his Web browsing history, in this paper we develop a novel approach that utilizes Web logs to compute the relevance of a web-user to a given query. In contrast to traditional methods that are purely based on textual analysis, our approach calculates the web-user's relevance through link analysis under a unified framework where the importance of web-pages and web-users mutually reinforce each other in an iterative way. The experimental results show that our approach has achieved 53 of accuracy when ranking the web-user's relevance to a search topic.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {49–54},
numpages = {6},
keywords = {link analysis, web usage mining, web mining},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584932,
author = {Lawrence, Steve},
title = {Improving Access to Scientific Literature},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584932},
doi = {10.1145/584931.584932},
abstract = {CiteSeer (also known as ResearchIndex) is a digital library of scientific literature that aims to improve communication and progress in science. CiteSeer features include automatic metadata extraction, autonomous citation indexing, graph analysis, citation context extraction, and related document computation. This talk covers the design, implementation, and operation of CiteSeer.Steve Lawrence is a Senior Research Scientist at NEC Research Institute, Princeton, NJ. His research interests include information retrieval and machine learning. Dr. Lawrence has published over 50 papers in these areas, including articles in Science, Nature, CACM, and IEEE Computer. He has been interviewed by over 100 news organizations including the New York Times, Wall Street Journal, Washington Post, Reuters, Associated Press, CNN, MSNBC, BBC, and NPR. Hundreds of articles about his research have appeared worldwide in over 10 different languages.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {55},
numpages = {1},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584944,
author = {Rezgui, Abdelmounaam and Ouzzani, Mourad and Bouguettaya, Athman and Medjahed, Brahim},
title = {Preserving Privacy in Web Services},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584944},
doi = {10.1145/584931.584944},
abstract = {Web services are increasingly being adopted as a viable means to access Web-based applications. This has been enabled by the tremendous standardization effort to describe, advertise, discover, and invoke Web services. Digital government (DG) is a major application domain for Web services. It aims at improving government-citizen interactions using information and communication technologies. Government agencies collect, store, process, and share information about millions of citizens who have different preferences regarding their privacy. This naturally raises a number of legal and technical issues that must be addressed to preserve citizens' privacy through the control of the information flow amongst different entities (users, Web services, DBMSs). Solutions addressing this issue are still in their infancy. They consist, essentially, of enforcing privacy by law or by self-regulation. In this paper, we propose a new technical approach for preserving privacy in government Web services. Our design is based on digital privacy credentials, data filters and mobile privacy preserving agents. This work aims at establishing the feasibility and provable reliability of technology-based privacy preserving solutions for Web service infrastructures.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {56–62},
numpages = {7},
keywords = {mobile agents, privacy, digital government, web services},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584945,
author = {Nanopoulos, Alexandros and Manolopoulos, Yannis and Zakrzewicz, Maciej and Morzy, Tadeusz},
title = {Indexing Web Access-Logs for Pattern Queries},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584945},
doi = {10.1145/584931.584945},
abstract = {In this paper, we develop a new indexing method for large web access-logs. We are concerned with pattern queries, which advocate the search for access sequences that contain certain query patterns. This kind of queries find applications in processing web-log mining results (e.g., finding typical/atypical access-sequences). The proposed method focuses on scalability to web-logs' sizes. For this reason, we examine the gains due to signature-trees, which can further improve the scalability to very large web-logs. Experimental results illustrate the superiority of the proposed method.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {63–68},
numpages = {6},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584946,
author = {Lage, Juliano Palmieri and da Silva, Altigran S. and Golgher, Paulo B. and Laender, Alberto H. F.},
title = {Collecting Hidden Weeb Pages for Data Extraction},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584946},
doi = {10.1145/584931.584946},
abstract = {As the Web grows, more and more data has become available under dynamic forms of publication, such as a legacy database accessed by an HTML form (the so called Hidden Web). In situations such as this, integration of this data relies more and more on the fast generation of page fetching agents. As a result, there is an increasing need for tools that can help the user to generate such agents. In this paper, we describe an approach to automatically generating agents to collect hidden Web pages that uses a pre-existing data repository for identifying the contents of these pages and takes the advantage of some regularities that can be found among Web sites. To demonstrate the effectiveness of our approach, we discuss the results of a number of experiments carried out with sites from different domains. We also dicuss how such regularities among sites can be formalized.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {69–75},
numpages = {7},
keywords = {collecting agents, hidden web, navigation patterns},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584947,
author = {Chen, Li and Wang, Song and Cash, Elizabeth and Ryder, Burke and Hobbs, Ian and Rundensteiner, Elke A.},
title = {A Fine-Grained Replacement Strategy for XML Query Cache},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584947},
doi = {10.1145/584931.584947},
abstract = {Caching popular queries and reusing results of previously computed queries is one important query optimization technique, especially in modern distributed environments such as the WWW. Based on the recent proliferation of XML data and the emergence of the XQuery language, we are thus developing a query- based caching system for XQuery queries, called ACE-XQ. ACE-XQ applies innovative query containment and rewriting strategies to answer incoming user queries based on the cached XQueries, whenever possible, instead of accessing remote XML data sources.To manage the space of the cache, a straightforward application of traditional replacement strategies would correspond to removing a complete cached query and its derived XML document as a whole when space needs to be freed. This coarse granularity however does not match well with the typical access pattern of web searches where new queries often partially overlap with cached queries.In this paper, we propose a novel replacement strategy appropriate for such query-based XML caching systems. In particular, we collect user access statistics at the granularity of the XML path structure instead of the complete XML query regions. We then apply a more fine-grained replacement strategy that purges XML fragments off a cached region instead of the whole XML document and accordingly adjusts the query descriptor. This may better capture the user access patterns since more frequently used XML document fragments are likely to remain in the cache while other less beneficial parts are purged. This approach has been implemented in our ACE-XQ System. Preliminary experiment results illustrate the performance improvement achievable by our fine-grained replacement strategy over the one which replaces a whole XML view at a time when the cache size is relatively large.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {76–83},
numpages = {8},
keywords = {XQuery, query containment, XML, cache replacement strategy, query rewriting, semantic caching},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584949,
author = {Ginsburg, Mark},
title = {The Catacomb Project: Building a User-Centered Portal the Conversational Way},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584949},
doi = {10.1145/584931.584949},
abstract = {Enterprise computing is marked by large-scale information systems, such as databases, document management, and groupware that present significant obstacles to consistent cross-application use: dissimilar user interfaces, incompatible security schemes, and the undesirable property of serving only parts of the user community (islands of use) and accessing only some of the enterprise knowledge assets (islands of information).World Wide Web (WWW) architectures do not solve this problem directly. WWW software components are combinable in many clever ways but until recently there were no specific efforts to solve the enterprise computing problems of islands of use and information.The situation is changing now with nascent efforts to architect Web Portal systems with small software modules, for example with Java "portlets" or the Python-based Zope framework [15]. These are program-centric approaches to coalesce information sources and unify the query interface without inherent user modeling. This paper discusses in detail an alternative: a user-centered, conversational portal which extends the ALICE chatbot technology platform and links the user conveniently to information resources, such as Web Services, with specialized query routing. The approach offers scalability, extensibility, and coordination between end-users and developers. A university Intranet implementation, the Catacomb system, is presented and discussed to illustrate the advantages of the conversational Web portal approach.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {84–87},
numpages = {4},
keywords = {ALICE, portal design, conversational portal, query routing},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584950,
author = {EL-Sayed, Maged and Wang, Ling and Ding, Luping and Rundensteiner, Elke A.},
title = {An Algebraic Approach for Incremental Maintenance of Materialized XQuery Views},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584950},
doi = {10.1145/584931.584950},
abstract = {Modern data sources, including structural and semi-structural sources, often export XML views over base data, and at times may materialize their views by storing the XML query result to provide faster data access. It is typically more efficient to maintain a view by incrementally propagating the base changes to the view than by re-computing it from scratch. Techniques for the incremental maintenance of relational views have been extensively studied in the literature. However, the maintenance of views created using XQuery is as of now unexplored. In this paper we propose an algebraic approach for incremental XQuery view maintenance. In our approach, an update to the XML source is transformed into a set of well defined update primitives which are propagated through the XML algebra tree. This algebraic update propagation process generates incremental update primitives to be applied to the result view. We briefly discuss our XQuery view maintenance system implementation. Our experiments confirm that incremental view maintenance is indeed faster than re-computation.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {88–91},
numpages = {4},
keywords = {XML query algebra, maintenance, XML, incremental view},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584951,
author = {Liu, Zehua and Ng, Wee Keong and Li, Feifei and Lim, Ee-Peng},
title = {A Visual Tool for Building Logical Data Models of Websites},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584951},
doi = {10.1145/584931.584951},
abstract = {Information sources over the WWW contain a large amount of data organized according to different interests and values. Thus, it is important that facilities are there to enable users to extract information of interest in a simple and effective manner. To do this, We propose the Wiccap Data Model, an XML data model that maps Web information sources into commonly perceived logical models, so that information can be extracted automatically according to users' interests. To accelerate the creation of data models, we have implemented a visual tool, called the Mapping Wizard, to facilitate and automate the process of producing Wiccap Data Models. Using the tool, the time required to construct a logical data model for a given website is significantly reduced.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {92–95},
numpages = {4},
keywords = {web data model, web information extraction, supervised wrapper generation},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584952,
author = {Sun, Aixin and Lim, Ee-Peng and Ng, Wee-Keong},
title = {Web Classification Using Support Vector Machine},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584952},
doi = {10.1145/584931.584952},
abstract = {In web classification, web pages from one or more web sites are assigned to pre-defined categories according to their content. Since web pages are more than just plain text documents, web classification methods have to consider using other context features of web pages, such as hyperlinks and HTML tags. In this paper, we propose the use of Support Vector Machine (SVM) classifiers to classify web pages using both their text and context feature sets. We have experimented our web classification method on the WebKB data set. Compared with earlier Foil-Pilfs method on the same data set, our method has been shown to perform very well. We have also shown that the use of context features especially hyperlinks can improve the classification performance significantly.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {96–99},
numpages = {4},
keywords = {SVM, web classification, web mining},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584953,
author = {Fu, Yueyu and Bauer, Travis and Mostafa, Javed and Palakal, Mathew and Mukhopadhyay, Snehasis},
title = {Concept Extraction and Association from Cancer Literature},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584953},
doi = {10.1145/584931.584953},
abstract = {There is a large and growing body of web accessible biomedical literature. As this body of electronic literature grows, so does the possibility that document analysis techniques can be used to automatically extract useful biomedical information from them, particularly in the discovery of key concepts dealing with genes, proteins, drugs, and diseases and associations among these concepts. VCGS (Vocabulary Cluster Generating System) was designed to automatically extract and determine associations among tokens from a subset of biomedical literature namely cancer. Such information has notable potential to automate database construction in biomedicine, instead of relying on experts' analysis. This paper reports on the mechanisms for automatically generating clusters of tokens. A formal evaluation of the system, based on a subset of 5338 Pubmed titles and abstracts, has been conducted against the Swiss-Prot database in which the associations among concepts are entered by experts by hand.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {100–103},
numpages = {4},
keywords = {web data mining, web information extraction},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

@inproceedings{10.1145/584931.584954,
author = {Xu, Xuebiao and Jones, Andrew C. and Gray, W. Alex and Fiddian, Nick J. and White, Richard J. and Bisby, Frank A.},
title = {Design and Performance Evaluation of a Web-Based Multi-Tier Federated System for a Catalogue of Life},
year = {2002},
isbn = {1581135939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/584931.584954},
doi = {10.1145/584931.584954},
abstract = {In the SPICE (SPecies 2000 Interoperability Co-ordination Environment) project, we are designing and evaluating a web-based multi-tier federated system, intended as a scalable infrastructure for a globally distributed federated database of biological knowledge. It is designed to harness specialist expertise on classification of individual groups of organisms to form a 'catalogue of life' of high taxonomic quality, and to solve problems of heterogeneity, scale and component database unreliability while providing a reconfigurable, flexible, cost-effective Internet online gateway to the distributed catalogue. This paper outlines our design and design rationale, explaining how our multi-tier federated approach makes maintenance of a consistent classification easier. We present the conceptual architecture and several intelligent agents prototyped to improve system performance with respect to scalability, stability and adaptability, and discuss the evaluation of this system. Our most important finding is that a knowledge based search using suitable caching techniques will effectively reduce the response time unless the core system is excessively loaded.},
booktitle = {Proceedings of the 4th International Workshop on Web Information and Data Management},
pages = {104–107},
numpages = {4},
keywords = {caching, federated information system, biodiversity informatics, multi-tier},
location = {McLean, Virginia, USA},
series = {WIDM '02}
}

