@inproceedings{10.1145/1031453.1031456,
author = {Hedley, Y. L. and Younas, M. and James, A. and Sanderson, M.},
title = {A Two-Phase Sampling Technique for Information Extraction from Hidden Web Databases},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031456},
doi = {10.1145/1031453.1031456},
abstract = {Hidden Web databases maintain a collection of specialised documents, which are dynamically generated in response to users' queries. However, the documents are generated by Web page templates, which contain information that is irrelevant to queries. This paper presents a Two-Phase Sampling (2PS) technique that detects templates and extracts query-related information from the sampled documents of a database. In the first phase, 2PS queries databases with terms contained in their search interface pages and the subsequently sampled documents. This process retrieves a required number of documents. In the second phase, 2PS detects Web page templates in the sampled documents in order to extract information relevant to queries. We test 2PS on a number of real-world Hidden Web databases. Experimental results demonstrate that 2PS effectively eliminates irrelevant information contained in Web page templates and generates terms and frequencies with improved accuracy.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
keywords = {document sampling, information extraction, hidden web databases},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248509,
author = {Kan, M.-Y.},
title = {Session Details: Web Crawling and Exploration},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248509},
doi = {10.1145/3248509},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031457,
author = {Fontes, Augusto de Carvalho and Silva, F\'{a}bio Soares},
title = {SmartCrawl: A New Strategy for the Exploration of the Hidden Web},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031457},
doi = {10.1145/1031453.1031457},
abstract = {The way current search engines work leaves a large amount of information available in the World Wide Web outside their catalogues. This is due to the fact that crawlers work by following hyperlinks and a few other references and ignore HTML forms. In this paper, we propose a search engine prototype that can retrieve information behind HTML forms by automatically generating queries for them. We describe the architecture, some implementation details and an experiment that proves that the information is not in fact indexed by current search engines.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {9–15},
numpages = {7},
keywords = {search engine, label extraction, hidden web},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031458,
author = {Liu, Hongyu and Milios, Evangelos and Janssen, Jeannette},
title = {Probabilistic Models for Focused Web Crawling},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031458},
doi = {10.1145/1031453.1031458},
abstract = {A Focused crawler must use information gleaned from previously crawled page sequences to estimate the relevance of a newly seen URL. Therefore, good performance depends on powerful modelling of context as well as the current observations. Probabilistic models, such as Hidden Markov Models(HMMs) and Conditional Random Fields(CRFs), can potentially capture both formatting and context. In this paper, we present the use of HMM for focused web crawling, and compare it with Best-First strategy. Furthermore, we discuss the concept of using CRFs to overcome the difficulties with HMMs and support the use of many, arbitrary and overlapping features. Finally, we describe a design of a system applying CRFs for focused web crawling, that is currently being implemented.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {16–22},
numpages = {7},
keywords = {focused crawling, web graph, hidden Markov models, conditional random fields, world wide web},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031460,
author = {Iacob, Ionut E. and Dekhtyar, Alex},
title = {Parsing Concurrent XML},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031460},
doi = {10.1145/1031453.1031460},
abstract = {Concurrent markup hierarchies appear often in document-centric XML documents, as a result of different XML elements having overlapping scopes. They require significantly different approach to management and maintenance. Management of XML documents composed of concurrent markup has been mostly studied by the document processing community and has attracted attention of computer scientists only recently. In this paper we discuss the architecture of an XML parser for concurrent XML. This parser uses a GODDAG data structure in place of traditional DOM Tree to store concurrent markup on top of the document content and provides a DOM-like API that allows software developers of tools working with concurrent XML documents to use it instead of parsing each individual component with a traditional DOM XML parser. The paper describes the architecture of the parser, data structures and algorithms used and the DOM-like API.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {23–30},
numpages = {8},
keywords = {DOM, GODDAG, overlapping markup, concurrent XML},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031461,
author = {Amer-Yahia, Sihem and Du, Fang and Freire, Juliana},
title = {A Comprehensive Solution to the XML-to-Relational Mapping Problem},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031461},
doi = {10.1145/1031453.1031461},
abstract = {The use of relational database management systems (RDBMSs) to store and query XML data has attracted considerable interest with a view to leveraging their powerful and reliable data management services. Due to the mismatch between the relational and XML data models, it is necessary to first shred and load the XML data into relational tables, and then btranslate XML queries over the original data into equivalent SQL queries over the mapped tables. Although there is a rich literature on XML-relational storage, none of the existing solutions addresses all the storage problems in a single framework. Works on mapping strategies often have little or no details about query translation, and proposals for query translation often target a specific mapping strategy. XML-storage solutions provided by RDBMS also have limitations. Notably, they are tied to a specific backend and use proprietary mapping languages, which not only may require a steep learning curve, but often are unable to express certain desirable mappings.In order to address these limitations, we developed ShreX, a XML-to-relational mapping framework and system that provides the first comprehensive and end-to-end solution to the relational storage of XML data. Mappings in ShreX are defined through annotations to an XML Schema. The use of XML Schema simplifies the mapping process, since it does not require users to master a new specialized mapping language. The use of annotations allows mapping choices to be combined in many different ways. As a result, ShreX not only supports all the mapping strategies proposed in the literature, but also new useful strategies that had not been considered previously. ShreX provides generic (and automatic) document shredding and query translation capabilities; and it is portable --- its mapping specifications are independent of the database backend.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {31–38},
numpages = {8},
keywords = {XML storage, XML shredding, relational databases, mapping techniques},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031462,
author = {Zou, Qinghua and Liu, Shaorong and Chu, Wesley W.},
title = {Ctree: A Compact Tree for Indexing XML Data},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031462},
doi = {10.1145/1031453.1031462},
abstract = {In this paper, we propose a novel compact tree (Ctree) for XML indexing, which provides not only concise path summaries at the group level but also detailed child-parent links at the element level. Group level mapping allows efficient pruning of a large search space while element level mapping provides fast access to the parent of an element. Due to the tree nature of XML data and queries, such fast child-to-parent access is essential for efficient XML query processing. Using group-based element reference, Ctree enables the clustering of inverted lists according to groups, which provides efficient join between inverted lists and structural index group extents. Our experiments reveal that Ctree is efficient for processing both single-path and branching queries with various value predicates.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {39–46},
numpages = {8},
keywords = {path summary, XQuery evaluation, XML index, Ctree, value index},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031454,
author = {Giles, C. Lee},
title = {Next Generation CiteSeer},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031454},
doi = {10.1145/1031453.1031454},
abstract = {CiteSeer, a computer and information science search engine and digital library, has been a radical departure for scientific document access and analysis. With nearly 700,000 documents, it has sometimes two million page views a day making it one of the most popular document access engines in science. CiteSeer is also portable, having been extended to ebusiness (eBizSearch) and more recently to academic business documents (SMEALSearch). CiteSeer is based on two features: actively acquiring new documents and automatic tagging and linking of metadata information inherent in an academic document's syntactic structure. Why is CiteSeer so popular? We discuss this and methods for providing new tagged metadata such as institutions and acknowledgements, new data resources and services and the issues in automation. We then discuss the next generation of CiteSeer.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {47},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248510,
author = {Deckhtyar, A.},
title = {Session Details: XML and Semistructured Data Querying},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248510},
doi = {10.1145/3248510},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031464,
author = {Bonifati, Angela and Matrangolo, Ugo and Cuzzocrea, Alfredo and Jain, Mayank},
title = {XPath Lookup Queries in P2P Networks},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031464},
doi = {10.1145/1031453.1031464},
abstract = {We address the problem of querying XML data over a P2P network. In P2P networks, the allowed kinds of queries are usually exact-match queries over file names. We discuss the extensions needed to deal with XML data and XPath queries. A single peer can hold a whole document or a partial/complete fragment of the latter. Each XML fragment/document is identified by a distinct path expression, which is encoded in a distributed hash table. Our framework differs from content-based routing mechanisms, biased towards finding the most relevant peers holding the data. We perform fragments placement and enable fragments lookup by solely exploiting few path expressions stored on each peer. By taking advantage of quasi-zero replication of global catalogs, our system supports fast full and partial XPath querying. To this purpose, we have extended the Chord simulator and performed an experimental evaluation of our approach.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {48–55},
numpages = {8},
keywords = {XML querying, XPath, distributed XML indexes, P2P networks},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031465,
author = {Dorneles, Carina F. and Heuser, Carlos A. and Lima, Andrei E. N. and da Silva, Altigran Soares and de Moura, Edleno Silva},
title = {Measuring Similarity between Collection of Values},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031465},
doi = {10.1145/1031453.1031465},
abstract = {In this paper, we propose a set of similarity metrics for manipulating collections of values occuring in XML documents.Following the data model presented in TAX algebra, we treat an XML element as a labeled ordered rooted tree. Consider that XML nodes can be either atomic, i.e, they may contain single values such as short character strings, date, etc, or complex, i.e., nested structures that contain other nodes, we propose two types of similarity metrics: MAVs, for atomic nodes and MCVs, for complex nodes. In the first case, we suggest the use of several application domain dependent metrics. In the second case, we define metrics for complex values that are structure dependent, and can be distinctly applied for it and collections of values. We also present experiments showing the effectiveness of our method.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {56–63},
numpages = {8},
keywords = {imprecise queries, XML, similarity functions, vague queries},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031466,
author = {Kantere, Verena and Tsoumakos, Dimitrios and Roussopoulos, Nick},
title = {Querying Structured Data in an Unstructured P2P System},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031466},
doi = {10.1145/1031453.1031466},
abstract = {Peer-to-Peer networking has become a major research topic over the last few years. Sharing of structured data in such decentralized environments is a challenging problem, especially in the absence of a global schema. The standard practice of answering a query that is consecutively rewritten along the propagation path often results in significant loss of information. In this paper, we present an adaptive and bandwidth-efficient solution to the problem in the context of an unstructured, purely decentralized system. Our method allows peers to individually choose which rewritten version of a query to answer and discover information-rich sources left hidden otherwise. Utilizing normal query traffic only, we describe how efficient query routing and clustering of peers can be used to produce high quality answers. Simulation results show that our technique is both effective and bandwidth-efficient in a variety of workloads and network sizes.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {64–71},
numpages = {8},
keywords = {query reformulation, peer-to-peer, structured data},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248511,
author = {Meng, W.},
title = {Session Details: Web Personalization},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248511},
doi = {10.1145/3248511},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031468,
author = {Eirinaki, Magdalini and Lampos, Charalampos and Paulakis, Stratos and Vazirgiannis, Michalis},
title = {Web Personalization Integrating Content Semantics and Navigational Patterns},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031468},
doi = {10.1145/1031453.1031468},
abstract = {The amounts of information residing on web sites make users' navigation a hard task. To address this problem, web sites provide recommendations to the end users, based on similar users' navigational patterns mined from past visits. In this paper we introduce a recommendation method, which integrates usage data recorded in web logs, and the conceptual relationships between web documents. In the proposed framework, the usage-oriented URI representation of web pages and users' behavior is augmented with content-based semantics expressed using domain-ontology terms. Since the number of multilingual web sites is constantly increasing, we also propose an automatic method for uniformly characterizing a web site's documents using a common vocabulary. Both methods are integrated in the semantic web personalization system SEWeP.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {72–79},
numpages = {8},
keywords = {semantic web personalization, web content semantics, concept hierarchies, semantic web mining},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031469,
author = {Albanese, Massimiliano and Picariello, Antonio and Sansone, Carlo and Sansone, Lucio},
title = {Web Personalization Based on Static Information and Dynamic User Behavior},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031469},
doi = {10.1145/1031453.1031469},
abstract = {The explosive growth of the web is at the basis of the great interest into web usage mining techniques in both commercial and research areas. In this paper, a web personalization strategy based on pattern recognition techniques is presented. This strategy takes into account both static information, by means of classical clustering algorithms, and dynamic behavior of a user, proposing a novel and effective re-classification algorithm. Experiments have been carried out in order to validate our approach and evaluate the proposed algorithm.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {80–87},
numpages = {8},
keywords = {web personalization, web usage mining, clustering},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031470,
author = {Kim, Dong-Ho and Atluri, Vijayalakshmi and Bieber, Michael and Adam, Nabil and Yesha, Yelena},
title = {A Clickstream-Based Collaborative Filtering Personalization Model: Towards a Better Performance},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031470},
doi = {10.1145/1031453.1031470},
abstract = {In recent years, clickstream-based Web personalization models for collaborative filtering recommendation have received much attention mainly due to their scalability [10,16,19]. The common personalization models are the Markov model, (sequential) association rule, and clustering. These models have shown strengths and weaknesses in their performance: for instance, the Markov model has higher precision and lower recall than (sequential) association rule and clustering, and vice versa [22]. In order to address the trade-off relationship of precision and recall, some study has combined two or more different models [22] or applied multi-order models [24,27]. The performance increases by these models, however, are at best marginal and still there is room for improving the performance because of their first order (one model type) application in making recommendation. We propose a new hybrid model for improving the performance, especially recall. The proposed hybrid model applies four prediction models - the Markov model, sequential association rule, association rule, and a default model [1,17] - in tandem in their precision order. We evaluated our model with Web usage data, and the result is promising.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {88–95},
numpages = {8},
keywords = {personalization model, performance, collaborative filtering recommendation, Markov model, default model, clustering, recall, F measure, precision, clickstream, hybrid model, association rule, sequential association rule},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248512,
author = {Mitra, P.},
title = {Session Details: Web Searching},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248512},
doi = {10.1145/3248512},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031472,
author = {Al-Kamha, Reema and Embley, David W.},
title = {Grouping Search-Engine Returned Citations for Person-Name Queries},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031472},
doi = {10.1145/1031453.1031472},
abstract = {We present a technique to group search-engine returned citations for person-name queries, such that the search-engine returned citations in each group belong to the same person. To group the returned citations, we use a multi-faceted approach that considers evidence from three facets: (1) attributes, (2) links, and (3) page similarity. Based on the three facets, we construct a relatedness confidence matrix for pairs of citations. We then merge pairs whose matching confidence value is above an empirically determined threshold. Experimental results from the implementation of our multi-faceted approach are promising.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {96–103},
numpages = {8},
keywords = {multi-faceted approach, queries, person-name, citation grouping, search engines},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031473,
title = {WISE-Cluster: Clustering e-Commerce Search Engines Automatically},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031473},
doi = {10.1145/1031453.1031473},
abstract = {In this paper, we propose a new approach to automatically clustering e-commerce search engines (ESEs) on the Web such that ESEs in the same cluster sell similar products. This allows an e-commerce metasearch engine (comparison shopping system) to be built over the ESEs for each cluster. Our approach performs the clustering based on the features available on the interface page (i.e. the Web page containing the search form or interface) of each ESE. Special features that are utilized include the number of links, the number of images, terms appearing in the search form and normalized price terms. Our experimental results based on nearly 300 ESEs indicate that this approach can achieve good results.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {104–111},
numpages = {8},
keywords = {search engine categorization, document clustering},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031474,
author = {Khoussainov, Rinat and Kushmerick, Nicholas},
title = {Specialisation Dynamics in Federated Web Search},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031474},
doi = {10.1145/1031453.1031474},
abstract = {Organising large-scale Web information retrieval systems into hierarchies of topic-specific search resources can improve both the quality of results and the efficient use of computing resources. A promising way to build such systems involves federations of topic-specific search engines in decentralised search environments. Most of the previous research concentrated on various technical aspects of such environments (e.g. routing of search queries or merging of results from multiple sources). We focus on organisational dynamics: what happens to topical specialisation of search engines in the absence of centralised control, when each engine makes individual and self-interested decisions on its service parameters? We investigate this question in a computational economics framework, where search providers compete for user queries by choosing what topics to index. We provide a formalisation of the competition problem and then analyse theoretically and empirically the specialisation dynamics of such systems.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {112–119},
numpages = {8},
keywords = {topic specialisation, federated web search, competition},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248513,
author = {Milios, E.},
title = {Session Details: Web Mining and Clustering},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248513},
doi = {10.1145/3248513},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031476,
author = {Moro, Rodrigo Giacomini and Galante, Renata de Matos and Heuser, Carlos Alberto},
title = {A Version Model for Supporting Adaptation of Web Pages},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031476},
doi = {10.1145/1031453.1031476},
abstract = {Maintenance of large Web sites is a complex task, similar in some sense to software maintenance. Content should be separated from the formatting rules, allowing independent development and maintenance of both parts. As in software maintenance, version management is important in order to separate stable versions from versions under development. Further a version model that allows alternative versions may be used to support adaptation and personalization of Web content and formatting. To fulfill this request, this paper presents a Web server infrastructure to accomplish version control and to support adaptation of Web pages in a way that is transparent to the user. A version model that separates page content and formatting in distinct components is provided. A configuration is also proposed and it is responsible for generating dynamic pages.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {120–127},
numpages = {8},
keywords = {configurations, hyperdocument model, web adaptation, versions},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031477,
author = {El-Sayed, Maged and Ruiz, Carolina and Rundensteiner, Elke A.},
title = {FS-Miner: Efficient and Incremental Mining of Frequent Sequence Patterns in Web Logs},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031477},
doi = {10.1145/1031453.1031477},
abstract = {Mining frequent patterns is an important component of many prediction systems. One common usage in web applications is the mining of users' access behavior for the purpose of predicting and hence pre-fetching the web pages that the user is likely to visit.In this paper we introduce an efficient strategy for discovering frequent patterns in sequence databases that requires only two scans of the database. The first scan obtains support counts for subsequences of length two. The second scan extracts potentially frequent sequences of any length and represents them as a compressed frequent sequences tree structure (FS-tree). Frequent sequence patterns are then mined from the FS-tree. Incremental and interactive mining functionalities are also facilitated by the FS-tree. As part of this work, we developed the FS-Miner, a system that discovers frequent sequences from web log files. The FS-Miner has the ability to adapt to changes in users' behavior over time, in the form of new input sequences, and to respond incrementally without the need to perform full re-computation. Our system also allows the user to change the input parameters (e.g., minimum support and desired pattern size) interactively without requiring full re-computation in most cases.We have tested our system comparing it against two other algorithms from the literature. Our experimental results show that our system scales up linearly with the size of the input database. Furthermore, it exhibits excellent adaptability to support threshold decreases. We also show that the incremental update capability of the system provides significant performance advantages over full re-computation even for relatively large update sizes.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {128–135},
numpages = {8},
keywords = {incremental mining, prediction, frequent patterns, sequence mining, prefetching, web usage mining, traversal patterns, web logs},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031478,
author = {Lee, Chee How and Kan, Min-Yen and Lai, Sandra},
title = {Stylistic and Lexical Co-Training for Web Block Classification},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031478},
doi = {10.1145/1031453.1031478},
abstract = {Many applications which use web data extract information from a limited number of regions on a web page. As such, web page division into blocks and the subsequent block classification have become a preprocessing step. We introduce PARCELS, an open-source, co-trained approach that performs classification based on separate stylistic and lexical views of the web page. Unlike previous work, PARCELS performs classification on fine-grained blocks. In addition to table-based layout, the system handles real-world pages which feature layout based on divisions and spans as well as stylistic inference for pages using cascaded style sheets. Our evaluation shows that the co-training process results in a reduction of 28.5% in error rate over a single-view classifier and that our approach is comparable to other state-of-the-art systems.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {136–143},
numpages = {8},
keywords = {web page block classification, lexical and stylistic learners, co-training, PARCELS, web page division},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/3248514,
author = {Heuser, C.},
title = {Session Details: User Interfaces and Services},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3248514},
doi = {10.1145/3248514},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
numpages = {1},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031480,
author = {Nelson, Michael L. and Bollen, Johan and Calhoun, JoAnne R. and Mackey, Calvin E.},
title = {User Evaluation of the NASA Technical Report Server Recommendation Service},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031480},
doi = {10.1145/1031453.1031480},
abstract = {We present the user evaluation of two recommendation server methodologies implemented for the NASA Technical Report Server (NTRS). One methodology for generating recommendations uses log analysis to identify co-retrieval events on full-text documents. For comparison, we used the Vector Space Model (VSM) as the second methodology. We calculated cosine similarities and used the top 10 most similar documents (based on metadata) as "recommendations". We then ran an experiment with NASA Langley Research Center (LaRC) staff members to gather their feedback on which method produced the most "quality" recommendations. We found that in most cases VSM outperformed log analysis of co-retrievals. However, analyzing the data revealed the evaluations might have been structurally biased in favor of the VSM generated recommendations. We explore some possible methods for combining log analysis and VSM generated recommendations and suggest areas of future work.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {144–151},
numpages = {8},
keywords = {digital libraries, user evaluation, recommendation servers},
location = {Washington DC, USA},
series = {WIDM '04}
}

@inproceedings{10.1145/1031453.1031481,
author = {Jaromczyk, Jerzy W. and Kowaluk, Miroslaw and Moore, Neil},
title = {A Web Interface to Image-Based Concurrent Markup Using Image Maps},
year = {2004},
isbn = {1581139780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1031453.1031481},
doi = {10.1145/1031453.1031481},
abstract = {Image-based electronic editions (IBEEs) encode manuscripts using markup based on digitized images of the manuscript page. Due to the complex nature of editorial annotations, the resulting markup rarely forms a hierarchical structure---that is, ranges often overlap. To support web-based access to these editions, it is desirable to synchronize the display of markup and related satellite data with the corresponding portions of the image. This goal can be supported by image maps, which associate regions of an image with hyperlinks and actions. These links allow the user to view and manage editorial information and multimedia content associated with the markup.In this paper we present an efficient algorithm for constructing a mapping from overlapping or intersecting markup to regions of an image. This mapping satisfies four properties: (1) Regions are located near the corresponding text. (2) Regions corresponding to nested markup elements are themselves nested. (3) Regions corresponding to intersecting markup elements themselves intersect. (4) If region R contains a collection C of other regions, then R is strictly larger than the union of the regions in C.The fourth property above ensures spatial separation of the regions corresponding to nested markup. An image map satisfying these properties provides a convenient and intuitive linking between image coordinates and the corresponding markup ranges.},
booktitle = {Proceedings of the 6th Annual ACM International Workshop on Web Information and Data Management},
pages = {152–159},
numpages = {8},
keywords = {client-side image maps, computational geometry, concurrent markup, image-based electronic editions, voronoi diagram},
location = {Washington DC, USA},
series = {WIDM '04}
}

