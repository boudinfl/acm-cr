@inproceedings{10.1145/956699.956701,
author = {Meng, Xiaofeng and Hu, Dongdong and Li, Chen},
title = {Schema-Guided Wrapper Maintenance for Web-Data Extraction},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956701},
doi = {10.1145/956699.956701},
abstract = {Extracting data from Web pages using wrappers is a fundamental problem arising in a large variety of applications of vast practical interests. There are two main issues relevant to Web-data extraction, namely wrapper generation and wrapper maintenance. In this paper, we propose a novel schema-guided approach to the problem of automatic wrapper maintenance. It is based on the observation that despite various page changes, many important features of the pages are preserved, such as syntactic patterns, annotations, and hyperlinks of the extracted data items. Our approach uses these preserved features to identify the locations of the desired values in the changed pages, and repair wrappers correspondingly by inducing semantic blocks from the HTML tree. Our intensive experiments on real Web sites show that the proposed approach can effectively maintain wrappers to extract desired data with high accuracies.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {1–8},
numpages = {8},
keywords = {maintenance, wrapper, web, schema, extraction},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956702,
author = {Davulcu, H. and Koduri, S. and Nagarajan, S.},
title = {Datarover: A Taxonomy Based Crawler for Automated Data Extraction from Data-Intensive Websites},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956702},
doi = {10.1145/956699.956702},
abstract = {The advent of e-commerce has created a trend that brought thousands of catalogs online. Most of these websites are "taxonomy-directed". A Web site is said to be "taxonomy-directed" if it contains at least one taxonomy for organizing its contents and it presents the instances belonging to a category in a regular fashion. This paper describes the DataRover system, which can automatically crawl and extract products from taxonomy-directed online catalogs. DataRover utilizes heuristic rules to discover the structural regularities among: taxonomy segments, list-of-product and single-product pages and it uses these regularities to turn the online catalogs into a database of categorized products without the need for user interaction or the wrapper maintenance burden. We provide experimental results to demonstrate the efficacy of the DataRover and point to its current limitations.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {9–14},
numpages = {6},
keywords = {web data extraction, web data integration, web annotation},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956703,
author = {Crescenzi, Valter and Merialdo, Paolo and Missier, Paolo},
title = {Fine-Grain Web Site Structure Discovery},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956703},
doi = {10.1145/956699.956703},
abstract = {Several techniques have been recently proposed to automatically derive web wrappers, i.e., programs that extract data from HTML pages, and transform them into a more structured format, typically in XML syntax. These techniques automatically induce a wrapper from a set of sample pages that share a common HTML template. An open issue, however, is how to collect suitable classes of sample pages to feed the wrapper inducer. Presently, the pages are chosen manually.In this paper, we tackle the problem of automatically discovering the main classes of pages offered by a site by exploring only a small, representative, portion of it. The web site model we propose describes the structure of the site as a graph whose nodes are classes of pages that share a common structure, and whose edges represent links among instances of the page classes. Using this model, we have developed an algorithm that accepts the url of an entry point to the target web site, visits a limited portion of the site, and produces an accurate model of the site structure. We also report on preliminary experiments performed on actual web sites, that have produced encouraging results.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {15–22},
numpages = {8},
keywords = {clustering, web modeling, wrapper induction, information extraction, web information systems},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956705,
author = {Liu, Jixue and Vincent, Millist and Liu, Chengfei},
title = {Local XML Functional Dependencies},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956705},
doi = {10.1145/956699.956705},
abstract = {Keys and functional dependencies play a fundamental role in relational databases where they are used in integrity enforcement and in database design. Similarly, these constraints will play a fundamental role in XML and recently keys and functional dependencies in XML have been defined. In this paper we extend the previous definition of functional dependencies in XML to local functional dependencies in XML. Local functional dependencies (LFDs) are functional dependencies which hold only in a certain part of an XML document and not in the whole document. We also define, and prove correct, axioms for reasoning about the implication of LFDs in XML. Finally, we examine the relationship between LFDs and keys and show that the recently introduced concept of a relative key is a special case of a LFD.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {23–28},
numpages = {6},
keywords = {XML, local functional dependency, relative keys},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956706,
author = {Grandi, Fabio and Mandreoli, Federica and Tiberio, Paolo and Bergonzini, Marco},
title = {A Temporal Data Model and Management System for Normative Texts in XML Format},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956706},
doi = {10.1145/956699.956706},
abstract = {In this paper,we present the results of an on-going research activity concerning the temporal management of normative texts in XML format. In particular, four temporal dimensions (publication,validity,efficacy and transaction times) are used to correctly represent the evolution of norms in time and their resulting versioning. Hence, we introduce a multiversion data model based on XML schema and de?ne basic mechanisms for the management of norm texts. Finally, we describe a prototype management system which has been implemented and evaluated.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {29–36},
numpages = {8},
keywords = {temporal XML, legal information systems},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956707,
author = {Vieira, Humberto and Ruberg, Gabriela and Mattoso, Marta},
title = {<b><i>XVerter</i></b>: Querying XML Data with OR-DBMS},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956707},
doi = {10.1145/956699.956707},
abstract = {Storage techniques and queries over XML databases are being widely studied. Most works store XML documents in traditional DBMSs in order to take advantage of a well established technology and also to store both structured data and XML data within a single system. This work proposes a translation mechanism to execute queries expressed on XQuery on top of XML documents that are stored in an object DBMS using the DOM implementation in disk. Rules for automatic translation from XQuery to SQL3 are presented, where an object-based representation of XML documents is exploited.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {37–44},
numpages = {8},
keywords = {DOM, SQL3, XSLT, XML, XQuery, object DBMS},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956709,
author = {Lacroix, Zo\'{e} and Boucelma, Omar and Essid, Mehdi},
title = {The Biological Integration System},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956709},
doi = {10.1145/956699.956709},
abstract = {The access and exploitation of integrated Web data repositories and applications is critical for life science. Biologists design protocols that typically rely on complex query pipelines accessing various biological Web resources (data sources and tools) to constitute data sets for analysis and mining. Web integration platforms are needed to allow biologists to access, manipulate and analyze electronic biological data. The design of integration architectures to support life science addresses specific issues. Biological resources are highly heterogeneous: not only they differ by their data representation, but they also offer radically different query capabilities. In this paper we present the Biological Integration System (BIS) that focuses on the data integration aspects while addressing the integration of query capabilities available at the sources. Typically, biological Web data sources provide complex query and analysis capabilities such as sequence similarity search engines, that need to be integrated as well as the data. We introduce the notion of derived wrappers that capture additional query capabilities to either compensate capabilities lacking at a source, or to adjust an existing capability in order to make it homogeneous with similar capabilities, wrapped at other sources.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {45–49},
numpages = {5},
keywords = {web, bioinformatics, mediation, heterogeneous resources, data integration, application integration},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956710,
author = {Song, Min and Song, Il-Yeol and Hu, Xiaohua},
title = {KPSpotter: A Flexible Information Gain-Based Keyphrase Extraction System},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956710},
doi = {10.1145/956699.956710},
abstract = {To tackle the issue of information overload, we present an Information Gain-based KeyPhrase Extraction System, called KPSpotter. KPSpotter is a flexible web-enabled keyphrase extraction system, capable of processing various formats of input data, including web data, and generating the extraction model as well as the list of keyphrases in XML. In KPSpotter, the following two features were selected for training and extracting keyphrases: 1) TF*IDF and 2) Distance from First Occurrence. Input training and testing collections were processed in three stages: 1) Data Cleaning, 2) Data Tokenizing, and 3) Data Discretizing. To measure the system performance, the keyphrases extracted by KPSpotter are compared with the ones that the authors assigned. Our experiments show that the performance of KPSpotter was evaluated to be equivalent to KEA, a well-known keyphrase extraction system. KPSpotter, however, is differentiated from other extraction systems in the followings: First, KPSpotter employs a new keyphrase extraction technique that combines the Information Gain data mining measure and several Natural Language Processing techniques such as stemming and case-folding. Second, KPSpotter is able to process various types of input data such as XML, HTML, and unstructured text data and generate XML output. Third, the user can provide input data and execute KPSpotter through the Internet. Fourth, for efficiency and performance reason, KPSpotter stores candidate keyphrases and its related information such as frequency and stemmed form into an embedded database management system.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {50–53},
numpages = {4},
keywords = {information extraction, summarization, text mining},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956711,
author = {Carro, Silvio Antonio and Scharcanski, Jacob and Valdeni de Lima, Jos\'{e}},
title = {MedISeek: A Web Based Diffusion System for Medical Visual Information},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956711},
doi = {10.1145/956699.956711},
abstract = {This paper presents a new metadata model to describe and retrieve medical visual information, such as images and their diagnoses, using the Web. The classes of this model allow to describe medical images of different modalities, including their properties, components and relationships. This model supports the international classification of diseases and related health problems (i.e. ICD-10)[1], and it has been used as a base for the implementation of the MedISeek (Medical Image Seek) prototype. This system allows authorized users to describe, store and retrieve medical images and their associated diagnostic information on the web for fast information exchange. Thus, this paper proposes a persistent structure for relational databases to storage and retrieve medical visual information based on the proposed the metadata model. A description of the prototype structure also is provided.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {54–57},
numpages = {4},
keywords = {metadata, medical images, visual information retrieval, web, RDF},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956712,
author = {Patel, Chintan and Supekar, Kaustubh and Lee, Yugyung and Park, E. K.},
title = {OntoKhoj: A Semantic Web Portal for Ontology Searching, Ranking and Classification},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956712},
doi = {10.1145/956699.956712},
abstract = {The goal of the next generation Web is to build virtual communities, wherein software agents and people can work in cooperation by sharing knowledge. To achieve this goal, the emerging Semantic Web community has proposed ontologies to express knowledge in a machine understandable way. The process of building and maintaining ontologies, which is known as Ontology Engineering, presents unique challenges. These challenges are related to lack of trustworthy and authoritative knowledge sources and absence of a centralized repository to locate ontologies to be reused. In this paper, we propose a Semantic Web portal, called OntoKhoj that is designed to simplify the Ontology Engineering process. The methodology in developing OntoKhoj is based on algorithms used for searching, aggregating, ranking and classifying ontologies in Semantic Web. The proposed OntoKhoj would 1) allow agents and ontology engineers to retrieve trustworthy, authoritative knowledge, and 2) expedite the process of ontology engineering through extensive reuse of ontologies. We have implemented the OntoKhoj portal and further validated our system on the real ontological data in the Semantic Web.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {58–61},
numpages = {4},
keywords = {ranking, semantic web, searching, classification},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956713,
author = {Ruckhaus, Edna and Vidal, Mar\'{\i}a-Esther},
title = {XWebSOGO: An Ontology Language to Describe and Query Web Sources},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956713},
doi = {10.1145/956699.956713},
abstract = {Ontologies play an important role in the Semantic Web. Currently, several Web ontology languages have been proposed and there has been a substantial effort towards the formalization of different knowledge domains. However, it is difficult to identify the ontologies for a specific domain and their annotated Web sources.XWebSOGO is a Web ontology language modeled after RDF and RDF Schema, that allows users to specify a domain in terms of its relevant entities' properties, the services that can be implemented on these entities, their query capabilities and the links that can connect them. In addition, XWebSOGO allows users to describe Web sources in terms of certain domain ontologies. Finally, XWebSOGO can be used to discover ontologies or Web sources that meet certain properties. XWebSOGOQL is a language used to query ontologies and annotated sources. An XWebSOGOQL query can be seen as a conjunctive query, where predicates are defined in terms of XWebSOGO basic concepts. In this paper, we define XWebSOGO and provide a formal definition of a query in XWebSOGOQL.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {62–65},
numpages = {4},
keywords = {meta-data, web query languages, ontologies, semantic web},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956715,
author = {Adami, Giordano and Avesani, Paolo and Sona, Diego},
title = {Clustering Documents in a Web Directory},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956715},
doi = {10.1145/956699.956715},
abstract = {Hierarchical categorization of documents is a task receiving growing interest due to the widespread proliferation of topic hierarchies for text documents. The worst problem of hierarchical supervised classifiers is their high demand in terms of labeled examples, whose amount is related to the number of topics in the taxonomy. Hence, bootstrapping a huge hierarchy with a proper set of labeled examples is a critical issue. In this paper, we propose some solutions for the bootstrapping problem, implicitly or explicitly using a taxonomy definition: a baseline approach where documents are classified according to class labels, and two clustering approaches, where training is constrained by the a-priori knowledge of the taxonomy structure, both at terminological and topological level. In particular, we propose the TaxSOM model, that clusters a set of documents in a predefined hierarchy of classes, directly exploiting the knowledge of both their topological organization and their lexical description. Experimental evaluation was performed on a set of taxonomies taken from the Google Web directory.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {66–73},
numpages = {8},
keywords = {digital libraries, constrained clustering, TaxSOM, knowledge management, web directories, text categorization, taxonomy bootstrapping process, k-means},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956716,
author = {G\'{e}ry, Mathias and Haddad, Hatem},
title = {Evaluation of Web Usage Mining Approaches for User's next Request Prediction},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956716},
doi = {10.1145/956699.956716},
abstract = {Analysis of Web server logs is one of the important challenge to provide Web intelligent services.In this paper, we describe a framework for a recommender system that predicts the user's next requests based on their behaviour discovered from Web Logs data. We compare results from three usage mining approaches: association rules, sequential rules and generalised sequential rules. We use two selection rules criteria: highest confidence and last-subsequence. Experiments are performed on three collections of real usage data: one from an Intranet Web site and two from an Internet Web site.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {74–81},
numpages = {8},
keywords = {evaluation, frequent sequences, frequent generalised sequences, association rules, web usage mining},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956717,
author = {Jespersen, S\o{}ren and Pedersen, Torben Bach and Thorhauge, Jesper},
title = {Evaluating the Markov Assumption for Web Usage Mining},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956717},
doi = {10.1145/956699.956717},
abstract = {Web usage mining concerns the discovery of common browsing patterns, i.e., pages requested in sequence, from web logs. To cope with the enormous amounts of data, several aggregated structures based on statistical models of web surfing have appeared, e.g., the Hypertext Probabilistic Gramma(HPG) model [2]. These techniques typically rely on the Markov assumption with history depth n, i.e., it is assumed that the next requested page is only dependent on the last n pages visited. This is not always valid, i.e. false browsing patterns may be discovered. However, to our knowledge there has been no systematic study of the validity of the Markov assumption wrt. web usage mining and the resulting quality of the mined browsing patterns.In this paper we systematically investigate the quality of browsing patterns mined from structures based on the Markov assumption. Formal measures of quality, based on the closeness of the mined patterns to the true traversal patterns, are defined and an extensive experimental evaluation is performed, based on two substantial real-world data sets. The results indicate that a large number of rules must be considered to achieve high quality, that long rules are generally more distorted than shorter rules and that the model yield knowledge of a higher quality when applied to more random usage patterns. Thus we conclude that Markov-based structures for web usage mining are best suited for tasks demanding less accuracy such as pre-fetching, personalization, and targeted ads.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {82–89},
numpages = {8},
keywords = {web usage mining, information quality, markov models},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956719,
author = {Carvalho, Joyce C. P. and da Silva, Altigran S.},
title = {Finding Similar Identities among Objects from Multiple Web Sources},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956719},
doi = {10.1145/956699.956719},
abstract = {When integrating data from multiple Web sources, objects can exist in different formats and structures, making it difficult to identify those that can be matched together. In this paper, we propose an identification approach to finding similar identities among objects from multiple Web sources. In this approach, object identification works like the relational join operation where a similarity function takes the place of the equality condition. This similarity function is based on information retrieval techniques. Our approach differs from others in the literature since it can be used to identify objects more complexly structured (e.g., XML documents) and not only objects with a flat structure such as relations. The effectiveness of our approach is demonstrated by experimental results with real Web data sources from different domains, that reach precision levels above 75%.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {90–93},
numpages = {4},
keywords = {similarity, web data integration},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956720,
author = {Wan, Jacky W. W. and Dobbie, Gillian},
title = {Extracting Association Rules from XML Documents Using XQuery},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956720},
doi = {10.1145/956699.956720},
abstract = {Data mining is generally considered the extraction and analysis of information from databases. With the rapid growth of XML data available online, mining XML data from the web is becoming important. In support of this trend, several encouraging attempts at developing methods for mining XML data have been proposed. However, efficiency and simplicity are still a barrier for further development In this paper, we show that any XML document can be mined for association rules using only the query language XQuery without any pre-processing or post-processing.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {94–97},
numpages = {4},
keywords = {association rule mining, XML, XQuery, Apriori},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956721,
author = {Grahne, G\"{o}sta and Kiricenko, Victoria},
title = {Partial Answers in Information Integration Systems},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956721},
doi = {10.1145/956699.956721},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {98–101},
numpages = {4},
keywords = {incomplete information, information integration, query rewriting using views},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956722,
author = {L\'{o}sio, Bernadette Farias and Salgado, Ana Carolina and do R\^{e}go Galvundefinedo, Luciano},
title = {Conceptual Modeling of XML Schemas},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956722},
doi = {10.1145/956699.956722},
abstract = {XML has become the standard format for representing structured and semi-structured data on the Web. To describe the structure and content of XML data, several XML schema languages have been proposed. Although being very useful for validating XML documents, an XML schema is not suitable for tasks requiring knowledge about the semantics of the represented data. For such tasks it is better to use a conceptual schema. This paper presents an extension of the Entity Relationship (ER) model, called X-Entity, for conceptual modeling of XML schemas. We also present the process of converting a schema, defined in the XML Schema language, to an X-Entity schema. The conversion process is based on a set of rules that consider element declarations and type definitions and generates the corresponding conceptual elements. Such representation provides a cleaner description for XML schemas by focusing only on semantically relevant concepts. The X Entity model has been used in the context of a Web data integration system with the goal of providing a concise and semantic description for local schemas defined in XML Schema.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {102–105},
numpages = {4},
keywords = {XML, ER model, data integration, XML Schema},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956723,
author = {Groppe, Sven and B\"{o}ttcher, Stefan},
title = {XPath Query Transformation Based on XSLT Stylesheets},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956723},
doi = {10.1145/956699.956723},
abstract = {Whenever XML data must be shared by heterogeneous applications, transformations between different application-specific XML formats are necessary. The state-of-the-art method transforms entire XML documents from one application format into another e.g. by using an XSLT stylesheet, so that each application can work locally on its preferred format. In our approach, we use an XSLT stylesheet in order to transform a given XPath query such that we retrieve and transform only that part of the XML document which is sufficient to answer the given query. Among other things, our approach avoids problems of replication, saves processing time and in distributed scenarios, transportation costs.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {106–110},
numpages = {5},
keywords = {XPath, XSLT, query rewriting, query transformation},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956725,
author = {Cesarano, Carmine and d'Acierno, Antonio and Picariello, Antonio},
title = {An Intelligent Search Agent System for Semantic Information Retrieval on the Internet},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956725},
doi = {10.1145/956699.956725},
abstract = {In this paper we describe a prototype system for information retrieval on the Internet. Our idea is that the Web has to be searched both semantically and syntactically. In order to automatically categorize the web pages on the fly we propose a novel approach based on ontology and semantic networks and we describe a prototype system based on the Intelligent Agent Paradigm. Preliminary experiments are shown and discussed while describing open problems and on-going research.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {111–117},
numpages = {7},
keywords = {semantic network, information retrieval, ontology, web agents},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956726,
author = {Becker, J\"{o}rg and Brelage, Christian and Klose, Karsten and Thygs, Michael},
title = {Conceptual Modeling of Semantic Navigation Structures: The MoSeNa-Approach},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956726},
doi = {10.1145/956699.956726},
abstract = {At the present time, several shortcomings prevent the more effective use and more intense application of web information systems. Recent developments that are subsumed by the term Semantic Web aim to solve these problems. The inherent idea behind these approaches is the annotation of data with metadata, in order to enhance automated processing and the use of ontologies to describe data semantically. However, the emergence of the Semantic Web raises new issues (e.g. significantly higher complexity) for web information system engineers. From an information system engineering perspective, conceptual modeling is an approach for dealing with high complexity and diversified project teams. On the one hand, conceptual models facilitate the (semi)automated generation of information systems. On the other, they foster communication between project team members and provide a sound basis for information system development.In this paper, we present an approach to modeling semantic navigation structures. This approach aims at modeling complex, role-based and integrated navigation structures for structured and semi-structured data. Since navigation via links is the only way for users to interact with the web information system and, to a large extent, determines their ability to retrieve the desired information, a proper definition of these navigation structures is essential for a successful web information system. Comprehensible and useful navigation structures can be derived from existing structures and hierarchies in organizations. Thus, an alignment of information systems with the information needs of users, can be achieved. Furthermore, our approach supports emergent technologies for the Semantic Web.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {118–125},
numpages = {8},
keywords = {semantic navigation structures, semantic web, conceptual modeling, web-based information-system, MoSeNa, information system engineering, SocKS},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956727,
author = {Nambiar, Ullas and Kambhampati, Subbarao},
title = {Answering Imprecise Database Queries: A Novel Approach},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956727},
doi = {10.1145/956699.956727},
abstract = {A growing number of databases especially those published on the Web are becoming available to external users. Users of these databases are provided simple form-based query interfaces that hide the underlying schematic details. Constrained by the expressiveness of the query interface users often have difficulty in articulating a precise query over the database. Supporting  imprecise queries over such systems would allow users to quickly find relevant answers without iteratively refining their queries. For databases to support imprecise queries they must provide answers that closely match the query constraints. In this paper we focus on answering imprecise user queries without changing the existing database system. We propose to support imprecise queries over a database by identifying a set of related precise queries that provide answers that are relevant to the user given query. We present a domain independent approach based on information retrieval techniques to estimate the distance between queries. To demonstrate the utility and usefulness of our approach we perform usability tests and provide results.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {126–133},
numpages = {8},
keywords = {relational database, imprecise queries, similarity, query},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956729,
author = {Bai, Qingyuan and Hong, Jun and McTear, Michael F.},
title = {Query Rewriting Using Views in the Presence of Inclusion Dependencies},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956729},
doi = {10.1145/956699.956729},
abstract = {Query rewriting using views is an essential issue in data integration. A number of algorithms, e.g., the bucket algorithm, the inverse rules algorithm, the SVB algorithm and the MiniCon algorithm, have been proposed to address this issue. These algorithms can be divided into two categories: bucket-based algorithms and inverse rule-based algorithms. Some inverse rule-based algorithms have considered the problem of query rewriting in the presence of inclusion dependencies. However, there has been no bucket-base algorithm so far for the problem. All the previous bucket-based algorithms may miss query rewritings in the presence of inclusion dependencies. In this paper, we extend the MiniCon algorithm to the presence of inclusion dependencies. In the MiniCon algorithm, a view can be used in a non-redundant rewriting of a query only if at least one subgoal in the query is covered by a subgoal in the view. In the presence of inclusion dependencies, when no subgoal in a view directly covers the query subgoal we can apply the chase procedure and rule to the subgoals of the query or view that contains the chase reachable subgoals to get a revised query or view. The condition required by the MiniCon algorithm is then satisfied. We can therefore avoid the problem of missing rewritings with the previous bucket-based algorithms. We prove that our extended algorithm can find the maximally-contained rewriting of a conjunctive query using a set of conjunctive views in the presence of inclusion dependencies. Our extension of the MiniCon algorithm does not involve a significant increase in computational complexity and our new algorithm remains scalable.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {134–138},
numpages = {5},
keywords = {data integration systems, inclusion dependencies, query rewriting using views},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956730,
author = {Ponte Vidal, V\^{a}nia Maria and Casanova, Marco Antonio and da Silva Araujo, Valdiana},
title = {Generating Rules for Incremental Maintenance of XML View of Relational Data},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956730},
doi = {10.1145/956699.956730},
abstract = {This work addresses the problem of incremental maintenance of XML views defined on top of relational data. In order to incrementally maintain a XML view, event-condition-rules should be specified for the relational source. Such rules are responsible for correctly modifying the XML view content in order to reflect changes made to the base source.This work proposes an approach where incremental view maintenance rules are derived from view correspondence assertions, which specify relationships between the view schema and the base source schema. The adoption of correspondence assertions also facilitates the task of formally proving that the derived rules correctly maintain the view.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {139–146},
numpages = {8},
keywords = {relational database, incremental view maintenance, XML},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

@inproceedings{10.1145/956699.956731,
author = {El-Sayed, Maged and Dimitrova, Katica and Rundensteiner, Elke A.},
title = {Efficiently Supporting Order in XML Query Processing},
year = {2003},
isbn = {1581137257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/956699.956731},
doi = {10.1145/956699.956731},
abstract = {Query processing over XML data sources has emerged as a popular topic. XML is an ordered data model and XQuery expressions return results that have a well-defined order. However little work on how order is supported in XML query processing has been done to date. In this paper we study the challenges related to handling order in the XML context, namely challenges imposed by the XML data model, by the variety of distinct XML operators and by incremental view maintenance. We have proposed an efficient solution that addresses these issues. We use a key encoding for XML nodes that supports both node identity and node order. We have designed order encoding rules based on the XML algebraic query execution data model and on node encodings that does not require any actual sorting for intermediate results during execution. Our approach supports more efficient incremental view maintenance as it makes most XML operators distributive with respect to bag union. Our approach is implemented in the context of Rainbow [25], an XML data management system developed at WPI. We prove the correctness of our order encoding approach, namely that it ensures order handling for query processing and for view maintenance. We also show, through experiments, that the overhead of maintaining order in our approach is indeed neglectible.},
booktitle = {Proceedings of the 5th ACM International Workshop on Web Information and Data Management},
pages = {147–154},
numpages = {8},
keywords = {XML query, XML data management systems, algebra, query, XQuery, order in XML},
location = {New Orleans, Louisiana, USA},
series = {WIDM '03}
}

